#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.806885719299316 2.0209035873413086
CurrentTrain: epoch  0, batch     0 | loss: 11.8277893Losses:  10.058280944824219 1.6070730686187744
CurrentTrain: epoch  0, batch     1 | loss: 11.6653538Losses:  8.891439437866211 1.723284125328064
CurrentTrain: epoch  0, batch     2 | loss: 10.6147232Losses:  10.728900909423828 2.056593418121338
CurrentTrain: epoch  0, batch     3 | loss: 12.7854939Losses:  9.68425464630127 1.3864614963531494
CurrentTrain: epoch  0, batch     4 | loss: 11.0707159Losses:  9.879255294799805 1.4510657787322998
CurrentTrain: epoch  0, batch     5 | loss: 11.3303213Losses:  9.265579223632812 1.2829498052597046
CurrentTrain: epoch  0, batch     6 | loss: 10.5485287Losses:  10.407770156860352 1.3826189041137695
CurrentTrain: epoch  0, batch     7 | loss: 11.7903891Losses:  9.131004333496094 1.0719326734542847
CurrentTrain: epoch  0, batch     8 | loss: 10.2029371Losses:  9.051100730895996 1.2267416715621948
CurrentTrain: epoch  0, batch     9 | loss: 10.2778425Losses:  10.353130340576172 1.3168007135391235
CurrentTrain: epoch  0, batch    10 | loss: 11.6699314Losses:  10.179758071899414 1.0001615285873413
CurrentTrain: epoch  0, batch    11 | loss: 11.1799192Losses:  8.640018463134766 1.3923242092132568
CurrentTrain: epoch  0, batch    12 | loss: 10.0323429Losses:  9.502699851989746 1.386412262916565
CurrentTrain: epoch  0, batch    13 | loss: 10.8891125Losses:  8.33444881439209 1.1968051195144653
CurrentTrain: epoch  0, batch    14 | loss: 9.5312538Losses:  8.942155838012695 0.9227887392044067
CurrentTrain: epoch  0, batch    15 | loss: 9.8649445Losses:  8.55031681060791 0.7867404222488403
CurrentTrain: epoch  0, batch    16 | loss: 9.3370571Losses:  8.95673942565918 1.2020167112350464
CurrentTrain: epoch  0, batch    17 | loss: 10.1587563Losses:  9.471296310424805 1.195138692855835
CurrentTrain: epoch  0, batch    18 | loss: 10.6664352Losses:  9.294438362121582 1.440649390220642
CurrentTrain: epoch  0, batch    19 | loss: 10.7350874Losses:  9.199715614318848 1.1669310331344604
CurrentTrain: epoch  0, batch    20 | loss: 10.3666468Losses:  8.974342346191406 1.344535231590271
CurrentTrain: epoch  0, batch    21 | loss: 10.3188772Losses:  8.285776138305664 1.3172883987426758
CurrentTrain: epoch  0, batch    22 | loss: 9.6030645Losses:  7.945834159851074 0.8727055788040161
CurrentTrain: epoch  0, batch    23 | loss: 8.8185396Losses:  8.945735931396484 1.2910326719284058
CurrentTrain: epoch  0, batch    24 | loss: 10.2367687Losses:  8.722225189208984 1.2286572456359863
CurrentTrain: epoch  0, batch    25 | loss: 9.9508820Losses:  8.569812774658203 1.1492359638214111
CurrentTrain: epoch  0, batch    26 | loss: 9.7190485Losses:  8.688121795654297 1.224942922592163
CurrentTrain: epoch  0, batch    27 | loss: 9.9130650Losses:  7.9387617111206055 1.341252326965332
CurrentTrain: epoch  0, batch    28 | loss: 9.2800140Losses:  7.8366522789001465 1.2860745191574097
CurrentTrain: epoch  0, batch    29 | loss: 9.1227264Losses:  9.041580200195312 1.4086600542068481
CurrentTrain: epoch  0, batch    30 | loss: 10.4502401Losses:  8.721630096435547 1.5019259452819824
CurrentTrain: epoch  0, batch    31 | loss: 10.2235565Losses:  9.769115447998047 0.9789026975631714
CurrentTrain: epoch  0, batch    32 | loss: 10.7480183Losses:  8.252460479736328 1.3089826107025146
CurrentTrain: epoch  0, batch    33 | loss: 9.5614433Losses:  9.410575866699219 1.3492827415466309
CurrentTrain: epoch  0, batch    34 | loss: 10.7598591Losses:  8.806713104248047 1.3558748960494995
CurrentTrain: epoch  0, batch    35 | loss: 10.1625881Losses:  8.260212898254395 1.3357858657836914
CurrentTrain: epoch  0, batch    36 | loss: 9.5959988Losses:  9.358680725097656 0.7922186851501465
CurrentTrain: epoch  0, batch    37 | loss: 10.1508999Losses:  9.015254020690918 1.0805375576019287
CurrentTrain: epoch  0, batch    38 | loss: 10.0957918Losses:  8.264853477478027 1.118933916091919
CurrentTrain: epoch  0, batch    39 | loss: 9.3837872Losses:  8.505093574523926 0.882178544998169
CurrentTrain: epoch  0, batch    40 | loss: 9.3872719Losses:  6.959911823272705 0.9726548790931702
CurrentTrain: epoch  0, batch    41 | loss: 7.9325666Losses:  8.097368240356445 1.1172471046447754
CurrentTrain: epoch  0, batch    42 | loss: 9.2146149Losses:  8.037067413330078 0.9528589844703674
CurrentTrain: epoch  0, batch    43 | loss: 8.9899263Losses:  8.047201156616211 1.0870088338851929
CurrentTrain: epoch  0, batch    44 | loss: 9.1342096Losses:  8.73844051361084 1.1091328859329224
CurrentTrain: epoch  0, batch    45 | loss: 9.8475733Losses:  7.865962028503418 0.9155647158622742
CurrentTrain: epoch  0, batch    46 | loss: 8.7815266Losses:  7.983587265014648 1.059513807296753
CurrentTrain: epoch  0, batch    47 | loss: 9.0431013Losses:  7.49874210357666 0.9912633895874023
CurrentTrain: epoch  0, batch    48 | loss: 8.4900055Losses:  7.504056453704834 1.0973849296569824
CurrentTrain: epoch  0, batch    49 | loss: 8.6014414Losses:  8.548319816589355 0.7760105133056641
CurrentTrain: epoch  0, batch    50 | loss: 9.3243303Losses:  6.592746734619141 0.8679510354995728
CurrentTrain: epoch  0, batch    51 | loss: 7.4606977Losses:  7.558855056762695 0.8023524284362793
CurrentTrain: epoch  0, batch    52 | loss: 8.3612080Losses:  8.179269790649414 0.8965187072753906
CurrentTrain: epoch  0, batch    53 | loss: 9.0757885Losses:  7.176584720611572 0.837193489074707
CurrentTrain: epoch  0, batch    54 | loss: 8.0137787Losses:  7.712542533874512 0.8856956362724304
CurrentTrain: epoch  0, batch    55 | loss: 8.5982380Losses:  8.873567581176758 0.9653983116149902
CurrentTrain: epoch  0, batch    56 | loss: 9.8389664Losses:  7.091635704040527 0.9934573173522949
CurrentTrain: epoch  0, batch    57 | loss: 8.0850925Losses:  6.954529762268066 0.980805516242981
CurrentTrain: epoch  0, batch    58 | loss: 7.9353352Losses:  8.539831161499023 1.019482135772705
CurrentTrain: epoch  0, batch    59 | loss: 9.5593128Losses:  8.156208038330078 1.1281774044036865
CurrentTrain: epoch  0, batch    60 | loss: 9.2843857Losses:  8.353577613830566 1.2078299522399902
CurrentTrain: epoch  0, batch    61 | loss: 9.5614071Losses:  7.3683671951293945 0.8689638376235962
CurrentTrain: epoch  0, batch    62 | loss: 8.2373314Losses:  7.754971504211426 0.7183246612548828
CurrentTrain: epoch  0, batch    63 | loss: 8.4732962Losses:  7.318761825561523 0.7796148061752319
CurrentTrain: epoch  0, batch    64 | loss: 8.0983763Losses:  8.513620376586914 0.88434898853302
CurrentTrain: epoch  0, batch    65 | loss: 9.3979692Losses:  7.834875106811523 0.942218542098999
CurrentTrain: epoch  0, batch    66 | loss: 8.7770939Losses:  6.7900285720825195 0.5560387372970581
CurrentTrain: epoch  0, batch    67 | loss: 7.3460674Losses:  7.678465366363525 0.7566993236541748
CurrentTrain: epoch  0, batch    68 | loss: 8.4351645Losses:  7.202561378479004 0.8368464708328247
CurrentTrain: epoch  0, batch    69 | loss: 8.0394077Losses:  7.450522422790527 0.7690818309783936
CurrentTrain: epoch  0, batch    70 | loss: 8.2196045Losses:  7.725331783294678 1.078747272491455
CurrentTrain: epoch  0, batch    71 | loss: 8.8040791Losses:  8.32839584350586 0.7741622924804688
CurrentTrain: epoch  0, batch    72 | loss: 9.1025581Losses:  6.580863952636719 1.0541729927062988
CurrentTrain: epoch  0, batch    73 | loss: 7.6350369Losses:  6.996212959289551 0.8695294857025146
CurrentTrain: epoch  0, batch    74 | loss: 7.8657427Losses:  7.254465103149414 0.9282127022743225
CurrentTrain: epoch  0, batch    75 | loss: 8.1826782Losses:  8.369335174560547 1.014153003692627
CurrentTrain: epoch  0, batch    76 | loss: 9.3834877Losses:  7.708248615264893 1.0375827550888062
CurrentTrain: epoch  0, batch    77 | loss: 8.7458315Losses:  8.4473295211792 0.9056492447853088
CurrentTrain: epoch  0, batch    78 | loss: 9.3529787Losses:  7.334753513336182 0.9582345485687256
CurrentTrain: epoch  0, batch    79 | loss: 8.2929878Losses:  8.774757385253906 0.9948276877403259
CurrentTrain: epoch  0, batch    80 | loss: 9.7695847Losses:  8.413820266723633 0.750242292881012
CurrentTrain: epoch  0, batch    81 | loss: 9.1640625Losses:  7.777416229248047 0.8950827121734619
CurrentTrain: epoch  0, batch    82 | loss: 8.6724987Losses:  6.84882116317749 0.8211132884025574
CurrentTrain: epoch  0, batch    83 | loss: 7.6699343Losses:  7.387315273284912 0.7568153142929077
CurrentTrain: epoch  0, batch    84 | loss: 8.1441307Losses:  6.655320644378662 0.7538024187088013
CurrentTrain: epoch  0, batch    85 | loss: 7.4091229Losses:  7.003628730773926 0.7397720217704773
CurrentTrain: epoch  0, batch    86 | loss: 7.7434006Losses:  6.727328300476074 0.4997343420982361
CurrentTrain: epoch  0, batch    87 | loss: 7.2270627Losses:  7.564364910125732 0.5914289951324463
CurrentTrain: epoch  0, batch    88 | loss: 8.1557941Losses:  6.547460556030273 0.7244654297828674
CurrentTrain: epoch  0, batch    89 | loss: 7.2719259Losses:  6.4710235595703125 0.8968092203140259
CurrentTrain: epoch  0, batch    90 | loss: 7.3678327Losses:  8.021763801574707 0.9844250082969666
CurrentTrain: epoch  0, batch    91 | loss: 9.0061884Losses:  7.675566673278809 0.8145239353179932
CurrentTrain: epoch  0, batch    92 | loss: 8.4900904Losses:  7.716962814331055 0.6301679611206055
CurrentTrain: epoch  0, batch    93 | loss: 8.3471308Losses:  6.395437717437744 0.6352944374084473
CurrentTrain: epoch  0, batch    94 | loss: 7.0307322Losses:  6.460862636566162 0.9237110614776611
CurrentTrain: epoch  0, batch    95 | loss: 7.3845739Losses:  7.123989105224609 0.9749031066894531
CurrentTrain: epoch  0, batch    96 | loss: 8.0988922Losses:  7.9810614585876465 1.2970569133758545
CurrentTrain: epoch  0, batch    97 | loss: 9.2781181Losses:  6.966597557067871 0.8134745359420776
CurrentTrain: epoch  0, batch    98 | loss: 7.7800722Losses:  7.149526119232178 0.8045476675033569
CurrentTrain: epoch  0, batch    99 | loss: 7.9540739Losses:  8.010754585266113 0.5920555591583252
CurrentTrain: epoch  0, batch   100 | loss: 8.6028099Losses:  6.931211471557617 1.0287630558013916
CurrentTrain: epoch  0, batch   101 | loss: 7.9599743Losses:  6.658938407897949 0.8161287307739258
CurrentTrain: epoch  0, batch   102 | loss: 7.4750671Losses:  7.027950286865234 0.8194528222084045
CurrentTrain: epoch  0, batch   103 | loss: 7.8474030Losses:  6.66494607925415 0.7262462377548218
CurrentTrain: epoch  0, batch   104 | loss: 7.3911924Losses:  7.164176940917969 0.31867456436157227
CurrentTrain: epoch  0, batch   105 | loss: 7.4828515Losses:  6.695366859436035 0.8447169661521912
CurrentTrain: epoch  0, batch   106 | loss: 7.5400839Losses:  6.928918838500977 0.8693814277648926
CurrentTrain: epoch  0, batch   107 | loss: 7.7983003Losses:  7.776301860809326 1.0449138879776
CurrentTrain: epoch  0, batch   108 | loss: 8.8212156Losses:  7.625174522399902 0.9366264343261719
CurrentTrain: epoch  0, batch   109 | loss: 8.5618010Losses:  6.634852886199951 0.6970189809799194
CurrentTrain: epoch  0, batch   110 | loss: 7.3318720Losses:  7.149068832397461 0.6648720502853394
CurrentTrain: epoch  0, batch   111 | loss: 7.8139410Losses:  5.774374008178711 0.5360895395278931
CurrentTrain: epoch  0, batch   112 | loss: 6.3104634Losses:  7.733563423156738 0.8349436521530151
CurrentTrain: epoch  0, batch   113 | loss: 8.5685072Losses:  6.82872200012207 0.5730981826782227
CurrentTrain: epoch  0, batch   114 | loss: 7.4018202Losses:  7.154743194580078 1.0559660196304321
CurrentTrain: epoch  0, batch   115 | loss: 8.2107096Losses:  7.809014320373535 0.6215397715568542
CurrentTrain: epoch  0, batch   116 | loss: 8.4305544Losses:  6.817860126495361 0.6656087636947632
CurrentTrain: epoch  0, batch   117 | loss: 7.4834690Losses:  6.908316612243652 0.5842012166976929
CurrentTrain: epoch  0, batch   118 | loss: 7.4925179Losses:  7.151391983032227 0.7019603848457336
CurrentTrain: epoch  0, batch   119 | loss: 7.8533525Losses:  6.924979209899902 0.5987894535064697
CurrentTrain: epoch  0, batch   120 | loss: 7.5237684Losses:  6.334913730621338 0.6389912366867065
CurrentTrain: epoch  0, batch   121 | loss: 6.9739051Losses:  6.744364261627197 0.5248806476593018
CurrentTrain: epoch  0, batch   122 | loss: 7.2692451Losses:  7.123499393463135 0.6282433271408081
CurrentTrain: epoch  0, batch   123 | loss: 7.7517428Losses:  7.94970178604126 0.6956374645233154
CurrentTrain: epoch  0, batch   124 | loss: 8.6453390Losses:  7.0347700119018555 0.5695300102233887
CurrentTrain: epoch  1, batch     0 | loss: 7.6043000Losses:  7.575605392456055 0.6774572134017944
CurrentTrain: epoch  1, batch     1 | loss: 8.2530622Losses:  6.59727668762207 0.32366764545440674
CurrentTrain: epoch  1, batch     2 | loss: 6.9209442Losses:  6.903404712677002 0.6643602848052979
CurrentTrain: epoch  1, batch     3 | loss: 7.5677652Losses:  7.459919452667236 0.6871156692504883
CurrentTrain: epoch  1, batch     4 | loss: 8.1470356Losses:  6.300421714782715 0.8515386581420898
CurrentTrain: epoch  1, batch     5 | loss: 7.1519604Losses:  6.318513870239258 0.565077543258667
CurrentTrain: epoch  1, batch     6 | loss: 6.8835917Losses:  6.47617244720459 0.6429976224899292
CurrentTrain: epoch  1, batch     7 | loss: 7.1191702Losses:  6.957911968231201 0.5903306603431702
CurrentTrain: epoch  1, batch     8 | loss: 7.5482426Losses:  7.35842227935791 0.550064742565155
CurrentTrain: epoch  1, batch     9 | loss: 7.9084868Losses:  6.0660295486450195 0.4002190828323364
CurrentTrain: epoch  1, batch    10 | loss: 6.4662485Losses:  6.469355583190918 0.5406072735786438
CurrentTrain: epoch  1, batch    11 | loss: 7.0099630Losses:  6.19483757019043 0.7239598035812378
CurrentTrain: epoch  1, batch    12 | loss: 6.9187975Losses:  6.023066520690918 0.5651850700378418
CurrentTrain: epoch  1, batch    13 | loss: 6.5882516Losses:  5.981287956237793 0.5861825942993164
CurrentTrain: epoch  1, batch    14 | loss: 6.5674706Losses:  6.109375476837158 0.3952740430831909
CurrentTrain: epoch  1, batch    15 | loss: 6.5046496Losses:  5.724865436553955 0.49918392300605774
CurrentTrain: epoch  1, batch    16 | loss: 6.2240496Losses:  6.291209697723389 0.33976396918296814
CurrentTrain: epoch  1, batch    17 | loss: 6.6309738Losses:  7.572207450866699 0.7749114036560059
CurrentTrain: epoch  1, batch    18 | loss: 8.3471184Losses:  6.247406959533691 0.6570529937744141
CurrentTrain: epoch  1, batch    19 | loss: 6.9044600Losses:  8.224334716796875 0.7228919267654419
CurrentTrain: epoch  1, batch    20 | loss: 8.9472265Losses:  5.458420276641846 0.4199090003967285
CurrentTrain: epoch  1, batch    21 | loss: 5.8783293Losses:  6.370920181274414 0.6902294158935547
CurrentTrain: epoch  1, batch    22 | loss: 7.0611496Losses:  5.9304094314575195 0.6941497921943665
CurrentTrain: epoch  1, batch    23 | loss: 6.6245594Losses:  6.418643951416016 0.7380218505859375
CurrentTrain: epoch  1, batch    24 | loss: 7.1566658Losses:  6.4575605392456055 0.42073988914489746
CurrentTrain: epoch  1, batch    25 | loss: 6.8783007Losses:  6.540413856506348 0.6549558639526367
CurrentTrain: epoch  1, batch    26 | loss: 7.1953697Losses:  6.410313606262207 0.6005960702896118
CurrentTrain: epoch  1, batch    27 | loss: 7.0109096Losses:  6.001842021942139 0.589998722076416
CurrentTrain: epoch  1, batch    28 | loss: 6.5918407Losses:  6.175307273864746 0.37010103464126587
CurrentTrain: epoch  1, batch    29 | loss: 6.5454082Losses:  7.128510475158691 0.7055460214614868
CurrentTrain: epoch  1, batch    30 | loss: 7.8340564Losses:  7.297625541687012 0.4224182367324829
CurrentTrain: epoch  1, batch    31 | loss: 7.7200437Losses:  6.566317558288574 0.2815488576889038
CurrentTrain: epoch  1, batch    32 | loss: 6.8478665Losses:  6.765196323394775 0.7044873237609863
CurrentTrain: epoch  1, batch    33 | loss: 7.4696836Losses:  5.7653679847717285 0.4212154448032379
CurrentTrain: epoch  1, batch    34 | loss: 6.1865835Losses:  5.527273178100586 0.48045408725738525
CurrentTrain: epoch  1, batch    35 | loss: 6.0077271Losses:  5.952709197998047 0.5340260863304138
CurrentTrain: epoch  1, batch    36 | loss: 6.4867353Losses:  6.131394863128662 0.47002696990966797
CurrentTrain: epoch  1, batch    37 | loss: 6.6014218Losses:  6.574984550476074 0.5410470962524414
CurrentTrain: epoch  1, batch    38 | loss: 7.1160316Losses:  6.029608726501465 0.5582972764968872
CurrentTrain: epoch  1, batch    39 | loss: 6.5879059Losses:  6.761453151702881 0.6064726114273071
CurrentTrain: epoch  1, batch    40 | loss: 7.3679256Losses:  6.641195297241211 0.5567197799682617
CurrentTrain: epoch  1, batch    41 | loss: 7.1979151Losses:  6.640249252319336 0.6977686882019043
CurrentTrain: epoch  1, batch    42 | loss: 7.3380179Losses:  6.110011577606201 0.5090324878692627
CurrentTrain: epoch  1, batch    43 | loss: 6.6190443Losses:  7.114833831787109 0.7453678250312805
CurrentTrain: epoch  1, batch    44 | loss: 7.8602018Losses:  6.271042823791504 0.6046916246414185
CurrentTrain: epoch  1, batch    45 | loss: 6.8757343Losses:  7.5768818855285645 0.5550799369812012
CurrentTrain: epoch  1, batch    46 | loss: 8.1319618Losses:  5.755949020385742 0.38740360736846924
CurrentTrain: epoch  1, batch    47 | loss: 6.1433525Losses:  5.890825271606445 0.5778244733810425
CurrentTrain: epoch  1, batch    48 | loss: 6.4686499Losses:  6.555192947387695 0.45834407210350037
CurrentTrain: epoch  1, batch    49 | loss: 7.0135369Losses:  5.514263153076172 0.3575197458267212
CurrentTrain: epoch  1, batch    50 | loss: 5.8717828Losses:  5.3876214027404785 0.46187275648117065
CurrentTrain: epoch  1, batch    51 | loss: 5.8494940Losses:  5.480141639709473 0.683564305305481
CurrentTrain: epoch  1, batch    52 | loss: 6.1637058Losses:  6.300169944763184 0.4322167932987213
CurrentTrain: epoch  1, batch    53 | loss: 6.7323866Losses:  5.575954437255859 0.43907415866851807
CurrentTrain: epoch  1, batch    54 | loss: 6.0150285Losses:  5.993889808654785 0.7967435121536255
CurrentTrain: epoch  1, batch    55 | loss: 6.7906332Losses:  6.040299415588379 0.5747054815292358
CurrentTrain: epoch  1, batch    56 | loss: 6.6150050Losses:  5.63667631149292 0.2867628037929535
CurrentTrain: epoch  1, batch    57 | loss: 5.9234390Losses:  5.773003101348877 0.3991641104221344
CurrentTrain: epoch  1, batch    58 | loss: 6.1721673Losses:  5.390361309051514 0.504368007183075
CurrentTrain: epoch  1, batch    59 | loss: 5.8947291Losses:  5.968594074249268 0.35554131865501404
CurrentTrain: epoch  1, batch    60 | loss: 6.3241353Losses:  5.760541915893555 0.32090479135513306
CurrentTrain: epoch  1, batch    61 | loss: 6.0814466Losses:  5.5184831619262695 0.3155643343925476
CurrentTrain: epoch  1, batch    62 | loss: 5.8340473Losses:  5.543144226074219 0.5188596844673157
CurrentTrain: epoch  1, batch    63 | loss: 6.0620041Losses:  6.278573989868164 0.5505647659301758
CurrentTrain: epoch  1, batch    64 | loss: 6.8291388Losses:  4.85154390335083 0.37164896726608276
CurrentTrain: epoch  1, batch    65 | loss: 5.2231927Losses:  5.294151306152344 0.44302263855934143
CurrentTrain: epoch  1, batch    66 | loss: 5.7371740Losses:  6.0483222007751465 0.2782191038131714
CurrentTrain: epoch  1, batch    67 | loss: 6.3265414Losses:  6.1476593017578125 0.6999719142913818
CurrentTrain: epoch  1, batch    68 | loss: 6.8476315Losses:  7.379045486450195 0.7186358571052551
CurrentTrain: epoch  1, batch    69 | loss: 8.0976810Losses:  5.9300689697265625 0.5416355133056641
CurrentTrain: epoch  1, batch    70 | loss: 6.4717045Losses:  5.058475494384766 0.33367353677749634
CurrentTrain: epoch  1, batch    71 | loss: 5.3921490Losses:  6.220864295959473 0.38311055302619934
CurrentTrain: epoch  1, batch    72 | loss: 6.6039748Losses:  5.990655899047852 0.4675346910953522
CurrentTrain: epoch  1, batch    73 | loss: 6.4581904Losses:  6.440841197967529 0.7361502051353455
CurrentTrain: epoch  1, batch    74 | loss: 7.1769915Losses:  5.965121269226074 0.3039710819721222
CurrentTrain: epoch  1, batch    75 | loss: 6.2690926Losses:  5.700220108032227 0.7384254932403564
CurrentTrain: epoch  1, batch    76 | loss: 6.4386454Losses:  7.326115608215332 0.7949495315551758
CurrentTrain: epoch  1, batch    77 | loss: 8.1210651Losses:  5.592237949371338 0.4077097773551941
CurrentTrain: epoch  1, batch    78 | loss: 5.9999475Losses:  5.164796829223633 0.3299869894981384
CurrentTrain: epoch  1, batch    79 | loss: 5.4947839Losses:  5.73853874206543 0.5096278190612793
CurrentTrain: epoch  1, batch    80 | loss: 6.2481666Losses:  6.450659275054932 0.3818473219871521
CurrentTrain: epoch  1, batch    81 | loss: 6.8325067Losses:  6.346351146697998 0.45384085178375244
CurrentTrain: epoch  1, batch    82 | loss: 6.8001919Losses:  6.804174423217773 0.5057447552680969
CurrentTrain: epoch  1, batch    83 | loss: 7.3099194Losses:  5.588593482971191 0.4891270697116852
CurrentTrain: epoch  1, batch    84 | loss: 6.0777206Losses:  6.272721290588379 0.577599048614502
CurrentTrain: epoch  1, batch    85 | loss: 6.8503203Losses:  6.664129257202148 0.5438299179077148
CurrentTrain: epoch  1, batch    86 | loss: 7.2079592Losses:  6.480672836303711 0.6374210119247437
CurrentTrain: epoch  1, batch    87 | loss: 7.1180940Losses:  7.5357160568237305 0.5777996778488159
CurrentTrain: epoch  1, batch    88 | loss: 8.1135159Losses:  6.0035600662231445 0.46162286400794983
CurrentTrain: epoch  1, batch    89 | loss: 6.4651828Losses:  5.411865234375 0.4729672968387604
CurrentTrain: epoch  1, batch    90 | loss: 5.8848324Losses:  5.3266072273254395 0.38401859998703003
CurrentTrain: epoch  1, batch    91 | loss: 5.7106256Losses:  5.135317325592041 0.4848583936691284
CurrentTrain: epoch  1, batch    92 | loss: 5.6201758Losses:  5.699875354766846 0.6172069311141968
CurrentTrain: epoch  1, batch    93 | loss: 6.3170824Losses:  5.305854797363281 0.5582714080810547
CurrentTrain: epoch  1, batch    94 | loss: 5.8641262Losses:  6.862564563751221 0.5848011374473572
CurrentTrain: epoch  1, batch    95 | loss: 7.4473658Losses:  5.677520275115967 0.616960883140564
CurrentTrain: epoch  1, batch    96 | loss: 6.2944813Losses:  6.180123805999756 0.4819672703742981
CurrentTrain: epoch  1, batch    97 | loss: 6.6620913Losses:  5.466670036315918 0.4918886423110962
CurrentTrain: epoch  1, batch    98 | loss: 5.9585586Losses:  5.0783843994140625 0.4767645597457886
CurrentTrain: epoch  1, batch    99 | loss: 5.5551491Losses:  5.17193078994751 0.2699415683746338
CurrentTrain: epoch  1, batch   100 | loss: 5.4418726Losses:  5.6861066818237305 0.40940341353416443
CurrentTrain: epoch  1, batch   101 | loss: 6.0955100Losses:  6.807353973388672 0.6077471375465393
CurrentTrain: epoch  1, batch   102 | loss: 7.4151011Losses:  6.849361419677734 0.4051028788089752
CurrentTrain: epoch  1, batch   103 | loss: 7.2544641Losses:  5.091760635375977 0.40690064430236816
CurrentTrain: epoch  1, batch   104 | loss: 5.4986610Losses:  5.607213973999023 0.5300264358520508
CurrentTrain: epoch  1, batch   105 | loss: 6.1372404Losses:  5.4877448081970215 0.3602297306060791
CurrentTrain: epoch  1, batch   106 | loss: 5.8479748Losses:  5.292679786682129 0.30719321966171265
CurrentTrain: epoch  1, batch   107 | loss: 5.5998731Losses:  5.374070167541504 0.6387817859649658
CurrentTrain: epoch  1, batch   108 | loss: 6.0128517Losses:  6.106287956237793 0.35393258929252625
CurrentTrain: epoch  1, batch   109 | loss: 6.4602203Losses:  4.997552871704102 0.21895183622837067
CurrentTrain: epoch  1, batch   110 | loss: 5.2165046Losses:  4.630659103393555 0.20900583267211914
CurrentTrain: epoch  1, batch   111 | loss: 4.8396649Losses:  5.308465003967285 0.456468790769577
CurrentTrain: epoch  1, batch   112 | loss: 5.7649336Losses:  4.783507823944092 0.2494897097349167
CurrentTrain: epoch  1, batch   113 | loss: 5.0329976Losses:  5.883703231811523 0.44931355118751526
CurrentTrain: epoch  1, batch   114 | loss: 6.3330169Losses:  5.293748378753662 0.37435901165008545
CurrentTrain: epoch  1, batch   115 | loss: 5.6681075Losses:  5.577001571655273 0.39911895990371704
CurrentTrain: epoch  1, batch   116 | loss: 5.9761205Losses:  4.681012153625488 0.3115253448486328
CurrentTrain: epoch  1, batch   117 | loss: 4.9925375Losses:  6.577314376831055 0.5502744317054749
CurrentTrain: epoch  1, batch   118 | loss: 7.1275887Losses:  5.462845802307129 0.37877437472343445
CurrentTrain: epoch  1, batch   119 | loss: 5.8416200Losses:  6.336719989776611 0.5387292504310608
CurrentTrain: epoch  1, batch   120 | loss: 6.8754492Losses:  5.523022174835205 0.4388057291507721
CurrentTrain: epoch  1, batch   121 | loss: 5.9618278Losses:  4.801836967468262 0.4328864514827728
CurrentTrain: epoch  1, batch   122 | loss: 5.2347236Losses:  5.761618137359619 0.3979242444038391
CurrentTrain: epoch  1, batch   123 | loss: 6.1595426Losses:  4.351369857788086 0.23705099523067474
CurrentTrain: epoch  1, batch   124 | loss: 4.5884209Losses:  4.443083763122559 0.18938428163528442
CurrentTrain: epoch  2, batch     0 | loss: 4.6324682Losses:  5.965590476989746 0.4764404892921448
CurrentTrain: epoch  2, batch     1 | loss: 6.4420309Losses:  5.798839569091797 0.3810814619064331
CurrentTrain: epoch  2, batch     2 | loss: 6.1799212Losses:  4.858200550079346 0.287681519985199
CurrentTrain: epoch  2, batch     3 | loss: 5.1458821Losses:  4.868117332458496 0.38239553570747375
CurrentTrain: epoch  2, batch     4 | loss: 5.2505131Losses:  6.227890491485596 0.4688393473625183
CurrentTrain: epoch  2, batch     5 | loss: 6.6967297Losses:  4.9051432609558105 0.32777008414268494
CurrentTrain: epoch  2, batch     6 | loss: 5.2329135Losses:  4.659821510314941 0.39463841915130615
CurrentTrain: epoch  2, batch     7 | loss: 5.0544600Losses:  5.373148441314697 0.3089372515678406
CurrentTrain: epoch  2, batch     8 | loss: 5.6820855Losses:  4.853096961975098 0.27564412355422974
CurrentTrain: epoch  2, batch     9 | loss: 5.1287413Losses:  5.011127471923828 0.3086319863796234
CurrentTrain: epoch  2, batch    10 | loss: 5.3197594Losses:  4.870092868804932 0.4601936340332031
CurrentTrain: epoch  2, batch    11 | loss: 5.3302865Losses:  5.06928014755249 0.3258419930934906
CurrentTrain: epoch  2, batch    12 | loss: 5.3951221Losses:  5.183017730712891 0.30249208211898804
CurrentTrain: epoch  2, batch    13 | loss: 5.4855099Losses:  4.932635307312012 0.38077130913734436
CurrentTrain: epoch  2, batch    14 | loss: 5.3134065Losses:  6.126284599304199 0.3991418480873108
CurrentTrain: epoch  2, batch    15 | loss: 6.5254264Losses:  4.7399396896362305 0.22107994556427002
CurrentTrain: epoch  2, batch    16 | loss: 4.9610195Losses:  7.532867908477783 0.5255656242370605
CurrentTrain: epoch  2, batch    17 | loss: 8.0584335Losses:  4.769008159637451 0.3094586431980133
CurrentTrain: epoch  2, batch    18 | loss: 5.0784669Losses:  5.397512435913086 0.26328831911087036
CurrentTrain: epoch  2, batch    19 | loss: 5.6608009Losses:  4.851574897766113 0.3443293273448944
CurrentTrain: epoch  2, batch    20 | loss: 5.1959043Losses:  4.764192581176758 0.3694610595703125
CurrentTrain: epoch  2, batch    21 | loss: 5.1336536Losses:  4.698919296264648 0.30517855286598206
CurrentTrain: epoch  2, batch    22 | loss: 5.0040979Losses:  5.070830345153809 0.263170450925827
CurrentTrain: epoch  2, batch    23 | loss: 5.3340006Losses:  5.216231346130371 0.3639042377471924
CurrentTrain: epoch  2, batch    24 | loss: 5.5801353Losses:  6.261393070220947 0.3794167935848236
CurrentTrain: epoch  2, batch    25 | loss: 6.6408100Losses:  5.885326862335205 0.37334588170051575
CurrentTrain: epoch  2, batch    26 | loss: 6.2586727Losses:  4.9202728271484375 0.2436252236366272
CurrentTrain: epoch  2, batch    27 | loss: 5.1638980Losses:  5.068942070007324 0.408508837223053
CurrentTrain: epoch  2, batch    28 | loss: 5.4774508Losses:  5.217824459075928 0.3019331693649292
CurrentTrain: epoch  2, batch    29 | loss: 5.5197577Losses:  4.887307167053223 0.25198137760162354
CurrentTrain: epoch  2, batch    30 | loss: 5.1392884Losses:  4.951586723327637 0.32142892479896545
CurrentTrain: epoch  2, batch    31 | loss: 5.2730155Losses:  4.778263568878174 0.2423361986875534
CurrentTrain: epoch  2, batch    32 | loss: 5.0205998Losses:  5.067105770111084 0.31996089220046997
CurrentTrain: epoch  2, batch    33 | loss: 5.3870668Losses:  4.834170818328857 0.290364146232605
CurrentTrain: epoch  2, batch    34 | loss: 5.1245351Losses:  4.278192520141602 0.26142221689224243
CurrentTrain: epoch  2, batch    35 | loss: 4.5396147Losses:  4.7749176025390625 0.20077727735042572
CurrentTrain: epoch  2, batch    36 | loss: 4.9756947Losses:  6.712922096252441 0.5115331411361694
CurrentTrain: epoch  2, batch    37 | loss: 7.2244554Losses:  6.286510467529297 0.413252592086792
CurrentTrain: epoch  2, batch    38 | loss: 6.6997633Losses:  4.604772567749023 0.13360068202018738
CurrentTrain: epoch  2, batch    39 | loss: 4.7383733Losses:  5.40981388092041 0.25388777256011963
CurrentTrain: epoch  2, batch    40 | loss: 5.6637015Losses:  4.67957878112793 0.4399107098579407
CurrentTrain: epoch  2, batch    41 | loss: 5.1194897Losses:  5.759906768798828 0.20340172946453094
CurrentTrain: epoch  2, batch    42 | loss: 5.9633083Losses:  4.773967742919922 0.22903671860694885
CurrentTrain: epoch  2, batch    43 | loss: 5.0030046Losses:  5.121432304382324 0.3899855613708496
CurrentTrain: epoch  2, batch    44 | loss: 5.5114179Losses:  4.607782363891602 0.2376590222120285
CurrentTrain: epoch  2, batch    45 | loss: 4.8454413Losses:  5.2900495529174805 0.24816098809242249
CurrentTrain: epoch  2, batch    46 | loss: 5.5382104Losses:  5.855118751525879 0.40029075741767883
CurrentTrain: epoch  2, batch    47 | loss: 6.2554097Losses:  6.1299662590026855 0.3347380459308624
CurrentTrain: epoch  2, batch    48 | loss: 6.4647045Losses:  5.790526866912842 0.5065850615501404
CurrentTrain: epoch  2, batch    49 | loss: 6.2971120Losses:  4.802700996398926 0.234405979514122
CurrentTrain: epoch  2, batch    50 | loss: 5.0371070Losses:  5.261252403259277 0.2702719569206238
CurrentTrain: epoch  2, batch    51 | loss: 5.5315242Losses:  4.7801690101623535 0.2709936201572418
CurrentTrain: epoch  2, batch    52 | loss: 5.0511627Losses:  4.923068523406982 0.2534233033657074
CurrentTrain: epoch  2, batch    53 | loss: 5.1764917Losses:  4.987165927886963 0.260879784822464
CurrentTrain: epoch  2, batch    54 | loss: 5.2480459Losses:  5.384922981262207 0.290797621011734
CurrentTrain: epoch  2, batch    55 | loss: 5.6757207Losses:  4.65419864654541 0.3511645495891571
CurrentTrain: epoch  2, batch    56 | loss: 5.0053630Losses:  5.064510345458984 0.19219475984573364
CurrentTrain: epoch  2, batch    57 | loss: 5.2567053Losses:  5.835664749145508 0.3191942572593689
CurrentTrain: epoch  2, batch    58 | loss: 6.1548591Losses:  5.383974075317383 0.3610684275627136
CurrentTrain: epoch  2, batch    59 | loss: 5.7450423Losses:  5.389266490936279 0.4026024043560028
CurrentTrain: epoch  2, batch    60 | loss: 5.7918687Losses:  5.128867149353027 0.3398134708404541
CurrentTrain: epoch  2, batch    61 | loss: 5.4686804Losses:  4.922136306762695 0.18678762018680573
CurrentTrain: epoch  2, batch    62 | loss: 5.1089239Losses:  5.09584379196167 0.3128693699836731
CurrentTrain: epoch  2, batch    63 | loss: 5.4087133Losses:  5.805483818054199 0.2802388668060303
CurrentTrain: epoch  2, batch    64 | loss: 6.0857229Losses:  4.9077324867248535 0.2275223582983017
CurrentTrain: epoch  2, batch    65 | loss: 5.1352549Losses:  4.737890243530273 0.3233920633792877
CurrentTrain: epoch  2, batch    66 | loss: 5.0612822Losses:  4.560128688812256 0.3817884624004364
CurrentTrain: epoch  2, batch    67 | loss: 4.9419169Losses:  5.419586181640625 0.3494095206260681
CurrentTrain: epoch  2, batch    68 | loss: 5.7689958Losses:  4.897493362426758 0.3739663362503052
CurrentTrain: epoch  2, batch    69 | loss: 5.2714596Losses:  4.915745735168457 0.20394301414489746
CurrentTrain: epoch  2, batch    70 | loss: 5.1196890Losses:  5.651058673858643 0.3722231388092041
CurrentTrain: epoch  2, batch    71 | loss: 6.0232821Losses:  4.6685895919799805 0.3354213833808899
CurrentTrain: epoch  2, batch    72 | loss: 5.0040112Losses:  4.580593585968018 0.2464378923177719
CurrentTrain: epoch  2, batch    73 | loss: 4.8270316Losses:  4.775596618652344 0.29696184396743774
CurrentTrain: epoch  2, batch    74 | loss: 5.0725584Losses:  5.0051751136779785 0.30535030364990234
CurrentTrain: epoch  2, batch    75 | loss: 5.3105254Losses:  5.304903507232666 0.23887744545936584
CurrentTrain: epoch  2, batch    76 | loss: 5.5437808Losses:  4.697736740112305 0.1436484158039093
CurrentTrain: epoch  2, batch    77 | loss: 4.8413854Losses:  4.401727676391602 0.18119850754737854
CurrentTrain: epoch  2, batch    78 | loss: 4.5829263Losses:  4.761568069458008 0.33076655864715576
CurrentTrain: epoch  2, batch    79 | loss: 5.0923347Losses:  4.92783260345459 0.4320834279060364
CurrentTrain: epoch  2, batch    80 | loss: 5.3599162Losses:  4.531968116760254 0.23089100420475006
CurrentTrain: epoch  2, batch    81 | loss: 4.7628593Losses:  4.612128257751465 0.3628641664981842
CurrentTrain: epoch  2, batch    82 | loss: 4.9749923Losses:  5.3669867515563965 0.3277446925640106
CurrentTrain: epoch  2, batch    83 | loss: 5.6947312Losses:  4.872234344482422 0.25931546092033386
CurrentTrain: epoch  2, batch    84 | loss: 5.1315498Losses:  4.972352027893066 0.31435155868530273
CurrentTrain: epoch  2, batch    85 | loss: 5.2867036Losses:  5.01956844329834 0.3375214636325836
CurrentTrain: epoch  2, batch    86 | loss: 5.3570900Losses:  5.127172470092773 0.2250438630580902
CurrentTrain: epoch  2, batch    87 | loss: 5.3522162Losses:  5.284572601318359 0.48228543996810913
CurrentTrain: epoch  2, batch    88 | loss: 5.7668581Losses:  4.617989540100098 0.2710331678390503
CurrentTrain: epoch  2, batch    89 | loss: 4.8890228Losses:  4.999321937561035 0.27479785680770874
CurrentTrain: epoch  2, batch    90 | loss: 5.2741199Losses:  5.195425987243652 0.26589682698249817
CurrentTrain: epoch  2, batch    91 | loss: 5.4613228Losses:  4.568857192993164 0.3451365530490875
CurrentTrain: epoch  2, batch    92 | loss: 4.9139938Losses:  4.588901042938232 0.211236834526062
CurrentTrain: epoch  2, batch    93 | loss: 4.8001380Losses:  5.901559829711914 0.42365404963493347
CurrentTrain: epoch  2, batch    94 | loss: 6.3252139Losses:  4.749675750732422 0.3105376660823822
CurrentTrain: epoch  2, batch    95 | loss: 5.0602136Losses:  4.634006500244141 0.20322096347808838
CurrentTrain: epoch  2, batch    96 | loss: 4.8372273Losses:  4.5415143966674805 0.3050294518470764
CurrentTrain: epoch  2, batch    97 | loss: 4.8465438Losses:  4.700567245483398 0.4188138544559479
CurrentTrain: epoch  2, batch    98 | loss: 5.1193810Losses:  5.153380870819092 0.32108959555625916
CurrentTrain: epoch  2, batch    99 | loss: 5.4744706Losses:  4.693022727966309 0.24827289581298828
CurrentTrain: epoch  2, batch   100 | loss: 4.9412956Losses:  4.423222541809082 0.1407717615365982
CurrentTrain: epoch  2, batch   101 | loss: 4.5639944Losses:  4.712002754211426 0.22884412109851837
CurrentTrain: epoch  2, batch   102 | loss: 4.9408469Losses:  4.413557052612305 0.21369874477386475
CurrentTrain: epoch  2, batch   103 | loss: 4.6272559Losses:  4.867823600769043 0.2732868790626526
CurrentTrain: epoch  2, batch   104 | loss: 5.1411104Losses:  5.111926078796387 0.16473010182380676
CurrentTrain: epoch  2, batch   105 | loss: 5.2766562Losses:  4.591163158416748 0.1327759176492691
CurrentTrain: epoch  2, batch   106 | loss: 4.7239389Losses:  4.765731334686279 0.30970704555511475
CurrentTrain: epoch  2, batch   107 | loss: 5.0754385Losses:  4.6549296379089355 0.23887939751148224
CurrentTrain: epoch  2, batch   108 | loss: 4.8938088Losses:  4.683559417724609 0.23710614442825317
CurrentTrain: epoch  2, batch   109 | loss: 4.9206657Losses:  4.669059753417969 0.33793267607688904
CurrentTrain: epoch  2, batch   110 | loss: 5.0069923Losses:  4.982019424438477 0.3585006892681122
CurrentTrain: epoch  2, batch   111 | loss: 5.3405199Losses:  5.277165412902832 0.4094036817550659
CurrentTrain: epoch  2, batch   112 | loss: 5.6865692Losses:  4.369614601135254 0.2606009840965271
CurrentTrain: epoch  2, batch   113 | loss: 4.6302156Losses:  4.909549713134766 0.2712131142616272
CurrentTrain: epoch  2, batch   114 | loss: 5.1807628Losses:  4.669130325317383 0.2972853183746338
CurrentTrain: epoch  2, batch   115 | loss: 4.9664154Losses:  4.636418342590332 0.2064499408006668
CurrentTrain: epoch  2, batch   116 | loss: 4.8428683Losses:  4.4137983322143555 0.3086319863796234
CurrentTrain: epoch  2, batch   117 | loss: 4.7224302Losses:  4.578766822814941 0.12675967812538147
CurrentTrain: epoch  2, batch   118 | loss: 4.7055264Losses:  4.6156721115112305 0.2578558921813965
CurrentTrain: epoch  2, batch   119 | loss: 4.8735280Losses:  4.570257186889648 0.23381246626377106
CurrentTrain: epoch  2, batch   120 | loss: 4.8040695Losses:  4.756583213806152 0.3107717037200928
CurrentTrain: epoch  2, batch   121 | loss: 5.0673552Losses:  4.622832298278809 0.301984041929245
CurrentTrain: epoch  2, batch   122 | loss: 4.9248161Losses:  4.475378036499023 0.1956411600112915
CurrentTrain: epoch  2, batch   123 | loss: 4.6710191Losses:  4.594715118408203 0.16084173321723938
CurrentTrain: epoch  2, batch   124 | loss: 4.7555571Losses:  5.161508560180664 0.07937748730182648
CurrentTrain: epoch  3, batch     0 | loss: 5.2408862Losses:  4.652851104736328 0.3582878112792969
CurrentTrain: epoch  3, batch     1 | loss: 5.0111389Losses:  5.669532775878906 0.37755149602890015
CurrentTrain: epoch  3, batch     2 | loss: 6.0470843Losses:  4.563761234283447 0.20680665969848633
CurrentTrain: epoch  3, batch     3 | loss: 4.7705679Losses:  4.580478668212891 0.2283836305141449
CurrentTrain: epoch  3, batch     4 | loss: 4.8088622Losses:  4.667871952056885 0.3019748330116272
CurrentTrain: epoch  3, batch     5 | loss: 4.9698467Losses:  4.68374490737915 0.2810465693473816
CurrentTrain: epoch  3, batch     6 | loss: 4.9647913Losses:  4.802375793457031 0.28720009326934814
CurrentTrain: epoch  3, batch     7 | loss: 5.0895758Losses:  4.587882041931152 0.2049877792596817
CurrentTrain: epoch  3, batch     8 | loss: 4.7928700Losses:  4.510410308837891 0.272749662399292
CurrentTrain: epoch  3, batch     9 | loss: 4.7831602Losses:  4.634535789489746 0.20764821767807007
CurrentTrain: epoch  3, batch    10 | loss: 4.8421841Losses:  4.450146675109863 0.15788371860980988
CurrentTrain: epoch  3, batch    11 | loss: 4.6080303Losses:  4.481398582458496 0.2900198698043823
CurrentTrain: epoch  3, batch    12 | loss: 4.7714186Losses:  4.605748653411865 0.24159102141857147
CurrentTrain: epoch  3, batch    13 | loss: 4.8473396Losses:  4.327967643737793 0.1782357096672058
CurrentTrain: epoch  3, batch    14 | loss: 4.5062032Losses:  4.513802528381348 0.16267339885234833
CurrentTrain: epoch  3, batch    15 | loss: 4.6764760Losses:  4.328036308288574 0.30904996395111084
CurrentTrain: epoch  3, batch    16 | loss: 4.6370864Losses:  4.997325897216797 0.15223950147628784
CurrentTrain: epoch  3, batch    17 | loss: 5.1495652Losses:  4.50694465637207 0.26539328694343567
CurrentTrain: epoch  3, batch    18 | loss: 4.7723379Losses:  4.604640007019043 0.1906769871711731
CurrentTrain: epoch  3, batch    19 | loss: 4.7953172Losses:  4.408990859985352 0.21179652214050293
CurrentTrain: epoch  3, batch    20 | loss: 4.6207876Losses:  4.772823333740234 0.22873181104660034
CurrentTrain: epoch  3, batch    21 | loss: 5.0015550Losses:  4.5124616622924805 0.25484609603881836
CurrentTrain: epoch  3, batch    22 | loss: 4.7673078Losses:  4.516186714172363 0.23289616405963898
CurrentTrain: epoch  3, batch    23 | loss: 4.7490830Losses:  4.390466213226318 0.1918119490146637
CurrentTrain: epoch  3, batch    24 | loss: 4.5822783Losses:  4.190141201019287 0.10922350734472275
CurrentTrain: epoch  3, batch    25 | loss: 4.2993646Losses:  4.955776214599609 0.24464353919029236
CurrentTrain: epoch  3, batch    26 | loss: 5.2004199Losses:  4.36765193939209 0.26658761501312256
CurrentTrain: epoch  3, batch    27 | loss: 4.6342397Losses:  4.343341827392578 0.20646438002586365
CurrentTrain: epoch  3, batch    28 | loss: 4.5498061Losses:  4.534242153167725 0.10231398046016693
CurrentTrain: epoch  3, batch    29 | loss: 4.6365561Losses:  4.396052360534668 0.27318477630615234
CurrentTrain: epoch  3, batch    30 | loss: 4.6692371Losses:  4.362880706787109 0.27927422523498535
CurrentTrain: epoch  3, batch    31 | loss: 4.6421547Losses:  4.379291534423828 0.24925638735294342
CurrentTrain: epoch  3, batch    32 | loss: 4.6285481Losses:  4.4348907470703125 0.15610750019550323
CurrentTrain: epoch  3, batch    33 | loss: 4.5909982Losses:  4.939727783203125 0.1862645000219345
CurrentTrain: epoch  3, batch    34 | loss: 5.1259923Losses:  5.498595237731934 0.21100331842899323
CurrentTrain: epoch  3, batch    35 | loss: 5.7095985Losses:  4.5798845291137695 0.2375953197479248
CurrentTrain: epoch  3, batch    36 | loss: 4.8174801Losses:  4.498785018920898 0.16210052371025085
CurrentTrain: epoch  3, batch    37 | loss: 4.6608853Losses:  4.437458038330078 0.28791192173957825
CurrentTrain: epoch  3, batch    38 | loss: 4.7253699Losses:  4.529440879821777 0.24421435594558716
CurrentTrain: epoch  3, batch    39 | loss: 4.7736554Losses:  4.493885040283203 0.16646285355091095
CurrentTrain: epoch  3, batch    40 | loss: 4.6603479Losses:  4.504586219787598 0.2616249918937683
CurrentTrain: epoch  3, batch    41 | loss: 4.7662110Losses:  4.404680252075195 0.2389710545539856
CurrentTrain: epoch  3, batch    42 | loss: 4.6436515Losses:  4.5558061599731445 0.23827369511127472
CurrentTrain: epoch  3, batch    43 | loss: 4.7940798Losses:  5.283384323120117 0.308083176612854
CurrentTrain: epoch  3, batch    44 | loss: 5.5914674Losses:  4.3094377517700195 0.17927885055541992
CurrentTrain: epoch  3, batch    45 | loss: 4.4887166Losses:  4.562141418457031 0.26908013224601746
CurrentTrain: epoch  3, batch    46 | loss: 4.8312216Losses:  4.325477600097656 0.23347754776477814
CurrentTrain: epoch  3, batch    47 | loss: 4.5589552Losses:  4.484386920928955 0.058453865349292755
CurrentTrain: epoch  3, batch    48 | loss: 4.5428410Losses:  4.413310527801514 0.2723570466041565
CurrentTrain: epoch  3, batch    49 | loss: 4.6856675Losses:  4.351069450378418 0.2949109375476837
CurrentTrain: epoch  3, batch    50 | loss: 4.6459804Losses:  4.308581829071045 0.15321332216262817
CurrentTrain: epoch  3, batch    51 | loss: 4.4617953Losses:  4.555168628692627 0.1857999563217163
CurrentTrain: epoch  3, batch    52 | loss: 4.7409687Losses:  4.582508087158203 0.2924962639808655
CurrentTrain: epoch  3, batch    53 | loss: 4.8750043Losses:  4.633222579956055 0.20093534886837006
CurrentTrain: epoch  3, batch    54 | loss: 4.8341579Losses:  4.308383941650391 0.14465422928333282
CurrentTrain: epoch  3, batch    55 | loss: 4.4530382Losses:  4.522207260131836 0.17319218814373016
CurrentTrain: epoch  3, batch    56 | loss: 4.6953993Losses:  4.48344612121582 0.3449060618877411
CurrentTrain: epoch  3, batch    57 | loss: 4.8283520Losses:  4.599792957305908 0.20765095949172974
CurrentTrain: epoch  3, batch    58 | loss: 4.8074441Losses:  6.246835231781006 0.5342357754707336
CurrentTrain: epoch  3, batch    59 | loss: 6.7810712Losses:  4.786242485046387 0.1947249323129654
CurrentTrain: epoch  3, batch    60 | loss: 4.9809675Losses:  4.698408126831055 0.245517760515213
CurrentTrain: epoch  3, batch    61 | loss: 4.9439259Losses:  4.29362678527832 0.25483667850494385
CurrentTrain: epoch  3, batch    62 | loss: 4.5484633Losses:  4.22648811340332 0.16534994542598724
CurrentTrain: epoch  3, batch    63 | loss: 4.3918381Losses:  4.325070858001709 0.12768559157848358
CurrentTrain: epoch  3, batch    64 | loss: 4.4527564Losses:  4.499696731567383 0.19639845192432404
CurrentTrain: epoch  3, batch    65 | loss: 4.6960950Losses:  4.556026935577393 0.1906830072402954
CurrentTrain: epoch  3, batch    66 | loss: 4.7467098Losses:  4.299675941467285 0.18000569939613342
CurrentTrain: epoch  3, batch    67 | loss: 4.4796815Losses:  4.268047332763672 0.21654313802719116
CurrentTrain: epoch  3, batch    68 | loss: 4.4845905Losses:  4.6831865310668945 0.24359312653541565
CurrentTrain: epoch  3, batch    69 | loss: 4.9267797Losses:  4.3169169425964355 0.14629647135734558
CurrentTrain: epoch  3, batch    70 | loss: 4.4632134Losses:  4.3846235275268555 0.30210813879966736
CurrentTrain: epoch  3, batch    71 | loss: 4.6867318Losses:  4.41801643371582 0.3934016823768616
CurrentTrain: epoch  3, batch    72 | loss: 4.8114181Losses:  4.517744541168213 0.1787784993648529
CurrentTrain: epoch  3, batch    73 | loss: 4.6965232Losses:  4.495976448059082 0.19256001710891724
CurrentTrain: epoch  3, batch    74 | loss: 4.6885366Losses:  4.251154899597168 0.1707087904214859
CurrentTrain: epoch  3, batch    75 | loss: 4.4218636Losses:  4.215152740478516 0.1651068925857544
CurrentTrain: epoch  3, batch    76 | loss: 4.3802595Losses:  4.460395812988281 0.1637338548898697
CurrentTrain: epoch  3, batch    77 | loss: 4.6241298Losses:  4.321328639984131 0.12140755355358124
CurrentTrain: epoch  3, batch    78 | loss: 4.4427361Losses:  4.465383052825928 0.10784303396940231
CurrentTrain: epoch  3, batch    79 | loss: 4.5732260Losses:  4.2736101150512695 0.23809278011322021
CurrentTrain: epoch  3, batch    80 | loss: 4.5117030Losses:  4.351081848144531 0.18755000829696655
CurrentTrain: epoch  3, batch    81 | loss: 4.5386319Losses:  4.182471752166748 0.2088438868522644
CurrentTrain: epoch  3, batch    82 | loss: 4.3913155Losses:  4.2607574462890625 0.1976279318332672
CurrentTrain: epoch  3, batch    83 | loss: 4.4583855Losses:  4.438201904296875 0.1603892743587494
CurrentTrain: epoch  3, batch    84 | loss: 4.5985913Losses:  4.40927791595459 0.07314291596412659
CurrentTrain: epoch  3, batch    85 | loss: 4.4824209Losses:  4.3143134117126465 0.19112177193164825
CurrentTrain: epoch  3, batch    86 | loss: 4.5054350Losses:  4.396419048309326 0.18062448501586914
CurrentTrain: epoch  3, batch    87 | loss: 4.5770435Losses:  4.998939037322998 0.1405208706855774
CurrentTrain: epoch  3, batch    88 | loss: 5.1394601Losses:  4.531108379364014 0.307734876871109
CurrentTrain: epoch  3, batch    89 | loss: 4.8388433Losses:  4.5547637939453125 0.17496246099472046
CurrentTrain: epoch  3, batch    90 | loss: 4.7297263Losses:  4.454529285430908 0.13963276147842407
CurrentTrain: epoch  3, batch    91 | loss: 4.5941620Losses:  4.271256446838379 0.22308951616287231
CurrentTrain: epoch  3, batch    92 | loss: 4.4943461Losses:  4.307175636291504 0.15899235010147095
CurrentTrain: epoch  3, batch    93 | loss: 4.4661679Losses:  4.217678546905518 0.18991702795028687
CurrentTrain: epoch  3, batch    94 | loss: 4.4075956Losses:  4.462536811828613 0.15166795253753662
CurrentTrain: epoch  3, batch    95 | loss: 4.6142049Losses:  4.651555061340332 0.1359233558177948
CurrentTrain: epoch  3, batch    96 | loss: 4.7874784Losses:  4.339920997619629 0.2471628338098526
CurrentTrain: epoch  3, batch    97 | loss: 4.5870838Losses:  4.230385780334473 0.23564788699150085
CurrentTrain: epoch  3, batch    98 | loss: 4.4660335Losses:  4.435494422912598 0.1816568374633789
CurrentTrain: epoch  3, batch    99 | loss: 4.6171513Losses:  4.386421203613281 0.19900798797607422
CurrentTrain: epoch  3, batch   100 | loss: 4.5854292Losses:  4.389087677001953 0.20205995440483093
CurrentTrain: epoch  3, batch   101 | loss: 4.5911474Losses:  4.447099208831787 0.16486813127994537
CurrentTrain: epoch  3, batch   102 | loss: 4.6119676Losses:  4.3188018798828125 0.20170286297798157
CurrentTrain: epoch  3, batch   103 | loss: 4.5205050Losses:  4.492815017700195 0.23816508054733276
CurrentTrain: epoch  3, batch   104 | loss: 4.7309799Losses:  4.346855640411377 0.2168004810810089
CurrentTrain: epoch  3, batch   105 | loss: 4.5636563Losses:  4.389073371887207 0.27450495958328247
CurrentTrain: epoch  3, batch   106 | loss: 4.6635785Losses:  4.275036334991455 0.14425113797187805
CurrentTrain: epoch  3, batch   107 | loss: 4.4192877Losses:  4.144123077392578 0.1749364584684372
CurrentTrain: epoch  3, batch   108 | loss: 4.3190594Losses:  4.488441467285156 0.24777841567993164
CurrentTrain: epoch  3, batch   109 | loss: 4.7362199Losses:  4.246037483215332 0.1394946277141571
CurrentTrain: epoch  3, batch   110 | loss: 4.3855319Losses:  4.377960205078125 0.25101879239082336
CurrentTrain: epoch  3, batch   111 | loss: 4.6289792Losses:  4.301903247833252 0.26573872566223145
CurrentTrain: epoch  3, batch   112 | loss: 4.5676422Losses:  4.124114990234375 0.1346486508846283
CurrentTrain: epoch  3, batch   113 | loss: 4.2587638Losses:  4.561934471130371 0.2038443684577942
CurrentTrain: epoch  3, batch   114 | loss: 4.7657790Losses:  4.349984169006348 0.27455031871795654
CurrentTrain: epoch  3, batch   115 | loss: 4.6245346Losses:  4.200775146484375 0.15488380193710327
CurrentTrain: epoch  3, batch   116 | loss: 4.3556590Losses:  4.37853479385376 0.10311128199100494
CurrentTrain: epoch  3, batch   117 | loss: 4.4816461Losses:  4.286686897277832 0.16659337282180786
CurrentTrain: epoch  3, batch   118 | loss: 4.4532804Losses:  4.336730480194092 0.14177921414375305
CurrentTrain: epoch  3, batch   119 | loss: 4.4785099Losses:  4.359302043914795 0.22968825697898865
CurrentTrain: epoch  3, batch   120 | loss: 4.5889902Losses:  4.3711748123168945 0.2653731107711792
CurrentTrain: epoch  3, batch   121 | loss: 4.6365480Losses:  4.299469470977783 0.1924373209476471
CurrentTrain: epoch  3, batch   122 | loss: 4.4919066Losses:  4.270480155944824 0.1898571252822876
CurrentTrain: epoch  3, batch   123 | loss: 4.4603372Losses:  4.245553970336914 0.08298749476671219
CurrentTrain: epoch  3, batch   124 | loss: 4.3285413Losses:  4.250328063964844 0.14657552540302277
CurrentTrain: epoch  4, batch     0 | loss: 4.3969035Losses:  3.939445972442627 0.06255703419446945
CurrentTrain: epoch  4, batch     1 | loss: 4.0020032Losses:  4.2645673751831055 0.16803264617919922
CurrentTrain: epoch  4, batch     2 | loss: 4.4326000Losses:  4.328064441680908 0.18074940145015717
CurrentTrain: epoch  4, batch     3 | loss: 4.5088139Losses:  4.354480743408203 0.2414497584104538
CurrentTrain: epoch  4, batch     4 | loss: 4.5959306Losses:  4.293949604034424 0.08778595924377441
CurrentTrain: epoch  4, batch     5 | loss: 4.3817358Losses:  4.235941410064697 0.06098458543419838
CurrentTrain: epoch  4, batch     6 | loss: 4.2969260Losses:  4.37050724029541 0.17085425555706024
CurrentTrain: epoch  4, batch     7 | loss: 4.5413613Losses:  4.390072345733643 0.05473870784044266
CurrentTrain: epoch  4, batch     8 | loss: 4.4448109Losses:  4.3149333000183105 0.21243037283420563
CurrentTrain: epoch  4, batch     9 | loss: 4.5273638Losses:  4.109487056732178 0.1605689972639084
CurrentTrain: epoch  4, batch    10 | loss: 4.2700562Losses:  4.399147987365723 0.17668332159519196
CurrentTrain: epoch  4, batch    11 | loss: 4.5758314Losses:  4.2863030433654785 0.08910205960273743
CurrentTrain: epoch  4, batch    12 | loss: 4.3754053Losses:  4.249253273010254 0.22342753410339355
CurrentTrain: epoch  4, batch    13 | loss: 4.4726810Losses:  4.336259841918945 0.2093343585729599
CurrentTrain: epoch  4, batch    14 | loss: 4.5455942Losses:  4.186816692352295 0.13133056461811066
CurrentTrain: epoch  4, batch    15 | loss: 4.3181472Losses:  4.168174743652344 0.19903478026390076
CurrentTrain: epoch  4, batch    16 | loss: 4.3672094Losses:  4.269621849060059 0.10865464806556702
CurrentTrain: epoch  4, batch    17 | loss: 4.3782763Losses:  4.214572906494141 0.19602331519126892
CurrentTrain: epoch  4, batch    18 | loss: 4.4105964Losses:  4.36300802230835 0.11123399436473846
CurrentTrain: epoch  4, batch    19 | loss: 4.4742422Losses:  4.099828243255615 0.08487401902675629
CurrentTrain: epoch  4, batch    20 | loss: 4.1847024Losses:  4.252326965332031 0.1637386530637741
CurrentTrain: epoch  4, batch    21 | loss: 4.4160657Losses:  4.25078010559082 0.15909899771213531
CurrentTrain: epoch  4, batch    22 | loss: 4.4098792Losses:  4.330717086791992 0.17750993371009827
CurrentTrain: epoch  4, batch    23 | loss: 4.5082269Losses:  4.264313697814941 0.14794093370437622
CurrentTrain: epoch  4, batch    24 | loss: 4.4122548Losses:  4.215446949005127 0.14219115674495697
CurrentTrain: epoch  4, batch    25 | loss: 4.3576379Losses:  4.298068523406982 0.20286428928375244
CurrentTrain: epoch  4, batch    26 | loss: 4.5009327Losses:  4.208785057067871 0.1919384002685547
CurrentTrain: epoch  4, batch    27 | loss: 4.4007235Losses:  4.205190658569336 0.06656310707330704
CurrentTrain: epoch  4, batch    28 | loss: 4.2717538Losses:  4.180322170257568 0.15413498878479004
CurrentTrain: epoch  4, batch    29 | loss: 4.3344574Losses:  4.1909990310668945 0.10922187566757202
CurrentTrain: epoch  4, batch    30 | loss: 4.3002210Losses:  4.233638763427734 0.11560322344303131
CurrentTrain: epoch  4, batch    31 | loss: 4.3492422Losses:  4.331463813781738 0.2072107195854187
CurrentTrain: epoch  4, batch    32 | loss: 4.5386744Losses:  4.362964630126953 0.17845028638839722
CurrentTrain: epoch  4, batch    33 | loss: 4.5414147Losses:  4.148348808288574 0.13158786296844482
CurrentTrain: epoch  4, batch    34 | loss: 4.2799368Losses:  4.230875015258789 0.05029216781258583
CurrentTrain: epoch  4, batch    35 | loss: 4.2811670Losses:  4.304468154907227 0.19850295782089233
CurrentTrain: epoch  4, batch    36 | loss: 4.5029712Losses:  4.2823004722595215 0.18038782477378845
CurrentTrain: epoch  4, batch    37 | loss: 4.4626884Losses:  4.240371227264404 0.11918947100639343
CurrentTrain: epoch  4, batch    38 | loss: 4.3595605Losses:  4.320509433746338 0.2589516043663025
CurrentTrain: epoch  4, batch    39 | loss: 4.5794611Losses:  4.893401145935059 0.2170678675174713
CurrentTrain: epoch  4, batch    40 | loss: 5.1104689Losses:  4.3090434074401855 0.15437394380569458
CurrentTrain: epoch  4, batch    41 | loss: 4.4634175Losses:  4.300554275512695 0.09585262089967728
CurrentTrain: epoch  4, batch    42 | loss: 4.3964071Losses:  4.361169338226318 0.09937293827533722
CurrentTrain: epoch  4, batch    43 | loss: 4.4605422Losses:  4.177946090698242 0.19131635129451752
CurrentTrain: epoch  4, batch    44 | loss: 4.3692622Losses:  4.143896102905273 0.11984536796808243
CurrentTrain: epoch  4, batch    45 | loss: 4.2637415Losses:  4.040660858154297 0.05231603980064392
CurrentTrain: epoch  4, batch    46 | loss: 4.0929770Losses:  4.259885787963867 0.06038665398955345
CurrentTrain: epoch  4, batch    47 | loss: 4.3202724Losses:  4.490548133850098 0.1643538475036621
CurrentTrain: epoch  4, batch    48 | loss: 4.6549020Losses:  4.234346389770508 0.11333203315734863
CurrentTrain: epoch  4, batch    49 | loss: 4.3476782Losses:  4.2645063400268555 0.14731504023075104
CurrentTrain: epoch  4, batch    50 | loss: 4.4118214Losses:  4.371284484863281 0.20454896986484528
CurrentTrain: epoch  4, batch    51 | loss: 4.5758333Losses:  4.2850141525268555 0.14122126996517181
CurrentTrain: epoch  4, batch    52 | loss: 4.4262352Losses:  4.1143341064453125 0.12256152927875519
CurrentTrain: epoch  4, batch    53 | loss: 4.2368956Losses:  4.216721534729004 0.0859520435333252
CurrentTrain: epoch  4, batch    54 | loss: 4.3026733Losses:  4.160061359405518 0.1176113411784172
CurrentTrain: epoch  4, batch    55 | loss: 4.2776728Losses:  4.326536178588867 0.1406693458557129
CurrentTrain: epoch  4, batch    56 | loss: 4.4672055Losses:  4.128240585327148 0.22310203313827515
CurrentTrain: epoch  4, batch    57 | loss: 4.3513427Losses:  4.139684677124023 0.10351458191871643
CurrentTrain: epoch  4, batch    58 | loss: 4.2431993Losses:  4.160688400268555 0.17413455247879028
CurrentTrain: epoch  4, batch    59 | loss: 4.3348231Losses:  4.120266914367676 0.08659157156944275
CurrentTrain: epoch  4, batch    60 | loss: 4.2068586Losses:  4.118532657623291 0.08279624581336975
CurrentTrain: epoch  4, batch    61 | loss: 4.2013288Losses:  4.057924270629883 0.10583587735891342
CurrentTrain: epoch  4, batch    62 | loss: 4.1637602Losses:  4.3222455978393555 0.19060516357421875
CurrentTrain: epoch  4, batch    63 | loss: 4.5128508Losses:  4.143060684204102 0.21227900683879852
CurrentTrain: epoch  4, batch    64 | loss: 4.3553395Losses:  4.16298770904541 0.2396741360425949
CurrentTrain: epoch  4, batch    65 | loss: 4.4026618Losses:  4.264543056488037 0.17268763482570648
CurrentTrain: epoch  4, batch    66 | loss: 4.4372306Losses:  4.222105026245117 0.16925100982189178
CurrentTrain: epoch  4, batch    67 | loss: 4.3913560Losses:  4.136834144592285 0.08357042074203491
CurrentTrain: epoch  4, batch    68 | loss: 4.2204046Losses:  4.222395896911621 0.07839078456163406
CurrentTrain: epoch  4, batch    69 | loss: 4.3007865Losses:  4.158419132232666 0.0972210019826889
CurrentTrain: epoch  4, batch    70 | loss: 4.2556400Losses:  4.257948398590088 0.23010121285915375
CurrentTrain: epoch  4, batch    71 | loss: 4.4880495Losses:  4.331521987915039 0.15034078061580658
CurrentTrain: epoch  4, batch    72 | loss: 4.4818625Losses:  4.165709495544434 0.15126386284828186
CurrentTrain: epoch  4, batch    73 | loss: 4.3169732Losses:  4.236876487731934 0.1893993765115738
CurrentTrain: epoch  4, batch    74 | loss: 4.4262757Losses:  4.229389190673828 0.1780666559934616
CurrentTrain: epoch  4, batch    75 | loss: 4.4074559Losses:  4.2204484939575195 0.07409349083900452
CurrentTrain: epoch  4, batch    76 | loss: 4.2945418Losses:  4.137536525726318 0.17674283683300018
CurrentTrain: epoch  4, batch    77 | loss: 4.3142796Losses:  4.217225551605225 0.1804300993680954
CurrentTrain: epoch  4, batch    78 | loss: 4.3976555Losses:  4.110953330993652 0.21703338623046875
CurrentTrain: epoch  4, batch    79 | loss: 4.3279867Losses:  4.184626579284668 0.07220418751239777
CurrentTrain: epoch  4, batch    80 | loss: 4.2568307Losses:  4.138432502746582 0.10654526948928833
CurrentTrain: epoch  4, batch    81 | loss: 4.2449780Losses:  4.129976749420166 0.1662490963935852
CurrentTrain: epoch  4, batch    82 | loss: 4.2962260Losses:  4.156670570373535 0.18313506245613098
CurrentTrain: epoch  4, batch    83 | loss: 4.3398056Losses:  4.321167469024658 0.17132626473903656
CurrentTrain: epoch  4, batch    84 | loss: 4.4924936Losses:  4.165116310119629 0.1672249287366867
CurrentTrain: epoch  4, batch    85 | loss: 4.3323412Losses:  4.201794147491455 0.121042400598526
CurrentTrain: epoch  4, batch    86 | loss: 4.3228364Losses:  4.198395252227783 0.09413207322359085
CurrentTrain: epoch  4, batch    87 | loss: 4.2925272Losses:  4.100351810455322 0.11372390389442444
CurrentTrain: epoch  4, batch    88 | loss: 4.2140756Losses:  4.267904281616211 0.060449790209531784
CurrentTrain: epoch  4, batch    89 | loss: 4.3283539Losses:  4.181807518005371 0.19472020864486694
CurrentTrain: epoch  4, batch    90 | loss: 4.3765278Losses:  4.509855270385742 0.13013581931591034
CurrentTrain: epoch  4, batch    91 | loss: 4.6399913Losses:  4.233570098876953 0.1924525946378708
CurrentTrain: epoch  4, batch    92 | loss: 4.4260225Losses:  4.122247695922852 0.15730299055576324
CurrentTrain: epoch  4, batch    93 | loss: 4.2795506Losses:  4.200446128845215 0.11501871794462204
CurrentTrain: epoch  4, batch    94 | loss: 4.3154650Losses:  4.1749491691589355 0.17484945058822632
CurrentTrain: epoch  4, batch    95 | loss: 4.3497987Losses:  4.072275638580322 0.14478477835655212
CurrentTrain: epoch  4, batch    96 | loss: 4.2170606Losses:  4.168202877044678 0.13922840356826782
CurrentTrain: epoch  4, batch    97 | loss: 4.3074312Losses:  4.501392364501953 0.17520534992218018
CurrentTrain: epoch  4, batch    98 | loss: 4.6765976Losses:  4.171341896057129 0.14324729144573212
CurrentTrain: epoch  4, batch    99 | loss: 4.3145890Losses:  4.180878639221191 0.10080884397029877
CurrentTrain: epoch  4, batch   100 | loss: 4.2816873Losses:  4.085309982299805 0.21144773066043854
CurrentTrain: epoch  4, batch   101 | loss: 4.2967577Losses:  4.165935516357422 0.09044050425291061
CurrentTrain: epoch  4, batch   102 | loss: 4.2563758Losses:  4.160218715667725 0.08977235853672028
CurrentTrain: epoch  4, batch   103 | loss: 4.2499909Losses:  4.087674140930176 0.20329371094703674
CurrentTrain: epoch  4, batch   104 | loss: 4.2909679Losses:  4.056612014770508 0.13257429003715515
CurrentTrain: epoch  4, batch   105 | loss: 4.1891861Losses:  4.076748847961426 0.1971651017665863
CurrentTrain: epoch  4, batch   106 | loss: 4.2739139Losses:  4.180037498474121 0.12723395228385925
CurrentTrain: epoch  4, batch   107 | loss: 4.3072715Losses:  4.172670841217041 0.22805452346801758
CurrentTrain: epoch  4, batch   108 | loss: 4.4007254Losses:  4.289380073547363 0.10628662258386612
CurrentTrain: epoch  4, batch   109 | loss: 4.3956666Losses:  4.141796588897705 0.20128902792930603
CurrentTrain: epoch  4, batch   110 | loss: 4.3430858Losses:  4.20197868347168 0.1472957730293274
CurrentTrain: epoch  4, batch   111 | loss: 4.3492746Losses:  4.10136604309082 0.15935871005058289
CurrentTrain: epoch  4, batch   112 | loss: 4.2607245Losses:  4.088131904602051 0.05838598310947418
CurrentTrain: epoch  4, batch   113 | loss: 4.1465178Losses:  4.134329795837402 0.09903678297996521
CurrentTrain: epoch  4, batch   114 | loss: 4.2333665Losses:  4.186120510101318 0.15028339624404907
CurrentTrain: epoch  4, batch   115 | loss: 4.3364038Losses:  4.177664756774902 0.1431584507226944
CurrentTrain: epoch  4, batch   116 | loss: 4.3208232Losses:  4.121762275695801 0.09434810280799866
CurrentTrain: epoch  4, batch   117 | loss: 4.2161102Losses:  4.145988464355469 0.14167112112045288
CurrentTrain: epoch  4, batch   118 | loss: 4.2876596Losses:  4.085328102111816 0.10915271937847137
CurrentTrain: epoch  4, batch   119 | loss: 4.1944809Losses:  4.115930557250977 0.1458178013563156
CurrentTrain: epoch  4, batch   120 | loss: 4.2617483Losses:  4.126216888427734 0.11502108722925186
CurrentTrain: epoch  4, batch   121 | loss: 4.2412381Losses:  4.161941051483154 0.16383588314056396
CurrentTrain: epoch  4, batch   122 | loss: 4.3257771Losses:  4.154790878295898 0.07136906683444977
CurrentTrain: epoch  4, batch   123 | loss: 4.2261600Losses:  4.425757884979248 0.15190252661705017
CurrentTrain: epoch  4, batch   124 | loss: 4.5776606Losses:  4.148839950561523 0.08529400825500488
CurrentTrain: epoch  5, batch     0 | loss: 4.2341337Losses:  4.056183815002441 0.1641906052827835
CurrentTrain: epoch  5, batch     1 | loss: 4.2203746Losses:  4.111264228820801 0.18307852745056152
CurrentTrain: epoch  5, batch     2 | loss: 4.2943430Losses:  4.082283020019531 0.13862362504005432
CurrentTrain: epoch  5, batch     3 | loss: 4.2209067Losses:  4.162547588348389 0.11509396135807037
CurrentTrain: epoch  5, batch     4 | loss: 4.2776418Losses:  4.109063148498535 0.09985038638114929
CurrentTrain: epoch  5, batch     5 | loss: 4.2089133Losses:  4.096108436584473 0.10355336219072342
CurrentTrain: epoch  5, batch     6 | loss: 4.1996617Losses:  4.1349992752075195 0.14855767786502838
CurrentTrain: epoch  5, batch     7 | loss: 4.2835569Losses:  4.109563827514648 0.09148295968770981
CurrentTrain: epoch  5, batch     8 | loss: 4.2010469Losses:  4.156044960021973 0.14153915643692017
CurrentTrain: epoch  5, batch     9 | loss: 4.2975841Losses:  4.1050872802734375 0.18689215183258057
CurrentTrain: epoch  5, batch    10 | loss: 4.2919793Losses:  4.147194862365723 0.09511847794055939
CurrentTrain: epoch  5, batch    11 | loss: 4.2423134Losses:  4.137049674987793 0.09396021068096161
CurrentTrain: epoch  5, batch    12 | loss: 4.2310100Losses:  4.144923686981201 0.11365144699811935
CurrentTrain: epoch  5, batch    13 | loss: 4.2585750Losses:  4.063459396362305 0.15484362840652466
CurrentTrain: epoch  5, batch    14 | loss: 4.2183032Losses:  4.071289539337158 0.09297596663236618
CurrentTrain: epoch  5, batch    15 | loss: 4.1642656Losses:  4.147735595703125 0.154513880610466
CurrentTrain: epoch  5, batch    16 | loss: 4.3022494Losses:  4.220428943634033 0.054477136582136154
CurrentTrain: epoch  5, batch    17 | loss: 4.2749062Losses:  4.087419033050537 0.11439161747694016
CurrentTrain: epoch  5, batch    18 | loss: 4.2018108Losses:  4.047865390777588 0.12802430987358093
CurrentTrain: epoch  5, batch    19 | loss: 4.1758895Losses:  4.095376968383789 0.18893910944461823
CurrentTrain: epoch  5, batch    20 | loss: 4.2843161Losses:  4.282403945922852 0.16522465646266937
CurrentTrain: epoch  5, batch    21 | loss: 4.4476285Losses:  4.070817947387695 0.11438664048910141
CurrentTrain: epoch  5, batch    22 | loss: 4.1852045Losses:  4.160407066345215 0.14169396460056305
CurrentTrain: epoch  5, batch    23 | loss: 4.3021011Losses:  4.077033519744873 0.11358034610748291
CurrentTrain: epoch  5, batch    24 | loss: 4.1906137Losses:  4.08094596862793 0.20722243189811707
CurrentTrain: epoch  5, batch    25 | loss: 4.2881684Losses:  4.074192047119141 0.07625802606344223
CurrentTrain: epoch  5, batch    26 | loss: 4.1504502Losses:  4.154264450073242 0.14922407269477844
CurrentTrain: epoch  5, batch    27 | loss: 4.3034887Losses:  4.098748207092285 0.18556183576583862
CurrentTrain: epoch  5, batch    28 | loss: 4.2843099Losses:  4.148892879486084 0.12094482779502869
CurrentTrain: epoch  5, batch    29 | loss: 4.2698379Losses:  4.096508979797363 0.0969809740781784
CurrentTrain: epoch  5, batch    30 | loss: 4.1934900Losses:  4.136432647705078 0.12887202203273773
CurrentTrain: epoch  5, batch    31 | loss: 4.2653046Losses:  4.0795488357543945 0.12705287337303162
CurrentTrain: epoch  5, batch    32 | loss: 4.2066016Losses:  4.201883316040039 0.15342767536640167
CurrentTrain: epoch  5, batch    33 | loss: 4.3553109Losses:  4.078858375549316 0.14474937319755554
CurrentTrain: epoch  5, batch    34 | loss: 4.2236075Losses:  4.098629474639893 0.12012003362178802
CurrentTrain: epoch  5, batch    35 | loss: 4.2187495Losses:  4.076274871826172 0.1014556884765625
CurrentTrain: epoch  5, batch    36 | loss: 4.1777306Losses:  4.123115539550781 0.12601923942565918
CurrentTrain: epoch  5, batch    37 | loss: 4.2491350Losses:  4.08349609375 0.11882089078426361
CurrentTrain: epoch  5, batch    38 | loss: 4.2023168Losses:  4.050410270690918 0.08417011052370071
CurrentTrain: epoch  5, batch    39 | loss: 4.1345806Losses:  4.167841911315918 0.11088240146636963
CurrentTrain: epoch  5, batch    40 | loss: 4.2787242Losses:  4.0144572257995605 0.12868349254131317
CurrentTrain: epoch  5, batch    41 | loss: 4.1431408Losses:  4.573546886444092 0.0955657809972763
CurrentTrain: epoch  5, batch    42 | loss: 4.6691127Losses:  4.117406368255615 0.1800193190574646
CurrentTrain: epoch  5, batch    43 | loss: 4.2974257Losses:  4.152709007263184 0.15316657721996307
CurrentTrain: epoch  5, batch    44 | loss: 4.3058758Losses:  4.038723945617676 0.08307203650474548
CurrentTrain: epoch  5, batch    45 | loss: 4.1217961Losses:  4.146056175231934 0.08758847415447235
CurrentTrain: epoch  5, batch    46 | loss: 4.2336445Losses:  4.106350898742676 0.15384839475154877
CurrentTrain: epoch  5, batch    47 | loss: 4.2601991Losses:  4.04935884475708 0.08062323182821274
CurrentTrain: epoch  5, batch    48 | loss: 4.1299820Losses:  4.009919166564941 0.12144707143306732
CurrentTrain: epoch  5, batch    49 | loss: 4.1313663Losses:  4.131770610809326 0.106498122215271
CurrentTrain: epoch  5, batch    50 | loss: 4.2382689Losses:  4.1519269943237305 0.1756708323955536
CurrentTrain: epoch  5, batch    51 | loss: 4.3275976Losses:  4.10690450668335 0.16230958700180054
CurrentTrain: epoch  5, batch    52 | loss: 4.2692142Losses:  4.126272678375244 0.15438684821128845
CurrentTrain: epoch  5, batch    53 | loss: 4.2806597Losses:  4.150399208068848 0.18001505732536316
CurrentTrain: epoch  5, batch    54 | loss: 4.3304143Losses:  4.069938659667969 0.03170090913772583
CurrentTrain: epoch  5, batch    55 | loss: 4.1016397Losses:  4.165809154510498 0.07228466868400574
CurrentTrain: epoch  5, batch    56 | loss: 4.2380939Losses:  4.178324222564697 0.15584850311279297
CurrentTrain: epoch  5, batch    57 | loss: 4.3341727Losses:  4.0520172119140625 0.14530599117279053
CurrentTrain: epoch  5, batch    58 | loss: 4.1973233Losses:  4.115416526794434 0.10511726140975952
CurrentTrain: epoch  5, batch    59 | loss: 4.2205338Losses:  4.06655216217041 0.0949321985244751
CurrentTrain: epoch  5, batch    60 | loss: 4.1614842Losses:  4.108512878417969 0.11841133236885071
CurrentTrain: epoch  5, batch    61 | loss: 4.2269244Losses:  4.171112537384033 0.06906246393918991
CurrentTrain: epoch  5, batch    62 | loss: 4.2401748Losses:  4.063271522521973 0.1527857780456543
CurrentTrain: epoch  5, batch    63 | loss: 4.2160573Losses:  4.133939743041992 0.10872400552034378
CurrentTrain: epoch  5, batch    64 | loss: 4.2426639Losses:  4.062547206878662 0.11592761427164078
CurrentTrain: epoch  5, batch    65 | loss: 4.1784749Losses:  4.027560710906982 0.20903411507606506
CurrentTrain: epoch  5, batch    66 | loss: 4.2365947Losses:  4.022392272949219 0.10801079869270325
CurrentTrain: epoch  5, batch    67 | loss: 4.1304030Losses:  4.041223526000977 0.16171053051948547
CurrentTrain: epoch  5, batch    68 | loss: 4.2029343Losses:  4.158043384552002 0.15848441421985626
CurrentTrain: epoch  5, batch    69 | loss: 4.3165278Losses:  4.055727481842041 0.08595091104507446
CurrentTrain: epoch  5, batch    70 | loss: 4.1416783Losses:  4.528752326965332 0.19492703676223755
CurrentTrain: epoch  5, batch    71 | loss: 4.7236795Losses:  4.150589942932129 0.1387704312801361
CurrentTrain: epoch  5, batch    72 | loss: 4.2893605Losses:  4.107300758361816 0.09430183470249176
CurrentTrain: epoch  5, batch    73 | loss: 4.2016025Losses:  4.083250999450684 0.21396014094352722
CurrentTrain: epoch  5, batch    74 | loss: 4.2972112Losses:  4.127545356750488 0.08089128136634827
CurrentTrain: epoch  5, batch    75 | loss: 4.2084365Losses:  4.134636402130127 0.1516212522983551
CurrentTrain: epoch  5, batch    76 | loss: 4.2862577Losses:  4.0934576988220215 0.08104990422725677
CurrentTrain: epoch  5, batch    77 | loss: 4.1745076Losses:  4.158614158630371 0.10026232898235321
CurrentTrain: epoch  5, batch    78 | loss: 4.2588763Losses:  4.0646514892578125 0.16172844171524048
CurrentTrain: epoch  5, batch    79 | loss: 4.2263799Losses:  4.089259624481201 0.11152342706918716
CurrentTrain: epoch  5, batch    80 | loss: 4.2007833Losses:  4.039407730102539 0.16394084692001343
CurrentTrain: epoch  5, batch    81 | loss: 4.2033486Losses:  4.175628662109375 0.13152703642845154
CurrentTrain: epoch  5, batch    82 | loss: 4.3071556Losses:  4.047990322113037 0.04952147230505943
CurrentTrain: epoch  5, batch    83 | loss: 4.0975118Losses:  4.184048652648926 0.09680309891700745
CurrentTrain: epoch  5, batch    84 | loss: 4.2808518Losses:  4.129544258117676 0.1915282905101776
CurrentTrain: epoch  5, batch    85 | loss: 4.3210726Losses:  4.047757148742676 0.1361103653907776
CurrentTrain: epoch  5, batch    86 | loss: 4.1838675Losses:  4.314912796020508 0.10711832344532013
CurrentTrain: epoch  5, batch    87 | loss: 4.4220309Losses:  4.134988784790039 0.1291612833738327
CurrentTrain: epoch  5, batch    88 | loss: 4.2641501Losses:  4.075523376464844 0.18950891494750977
CurrentTrain: epoch  5, batch    89 | loss: 4.2650323Losses:  4.014801025390625 0.22400489449501038
CurrentTrain: epoch  5, batch    90 | loss: 4.2388058Losses:  4.1021833419799805 0.13703641295433044
CurrentTrain: epoch  5, batch    91 | loss: 4.2392197Losses:  4.154412269592285 0.18352195620536804
CurrentTrain: epoch  5, batch    92 | loss: 4.3379340Losses:  4.186354160308838 0.15430547297000885
CurrentTrain: epoch  5, batch    93 | loss: 4.3406596Losses:  4.1118927001953125 0.18644818663597107
CurrentTrain: epoch  5, batch    94 | loss: 4.2983408Losses:  4.26360559463501 0.2048182338476181
CurrentTrain: epoch  5, batch    95 | loss: 4.4684238Losses:  4.075936317443848 0.11968567967414856
CurrentTrain: epoch  5, batch    96 | loss: 4.1956220Losses:  4.056014060974121 0.11719562113285065
CurrentTrain: epoch  5, batch    97 | loss: 4.1732097Losses:  4.0738019943237305 0.17884397506713867
CurrentTrain: epoch  5, batch    98 | loss: 4.2526460Losses:  4.032745361328125 0.14159369468688965
CurrentTrain: epoch  5, batch    99 | loss: 4.1743393Losses:  4.049386978149414 0.08945414423942566
CurrentTrain: epoch  5, batch   100 | loss: 4.1388412Losses:  4.149789810180664 0.11331629753112793
CurrentTrain: epoch  5, batch   101 | loss: 4.2631063Losses:  4.02548885345459 0.1802693009376526
CurrentTrain: epoch  5, batch   102 | loss: 4.2057581Losses:  4.182974815368652 0.05402277410030365
CurrentTrain: epoch  5, batch   103 | loss: 4.2369976Losses:  4.019218444824219 0.20772556960582733
CurrentTrain: epoch  5, batch   104 | loss: 4.2269440Losses:  4.042319297790527 0.08560165762901306
CurrentTrain: epoch  5, batch   105 | loss: 4.1279211Losses:  4.111081123352051 0.15485242009162903
CurrentTrain: epoch  5, batch   106 | loss: 4.2659335Losses:  4.09226131439209 0.10693375766277313
CurrentTrain: epoch  5, batch   107 | loss: 4.1991949Losses:  4.08857536315918 0.14213669300079346
CurrentTrain: epoch  5, batch   108 | loss: 4.2307119Losses:  4.126317024230957 0.1307319849729538
CurrentTrain: epoch  5, batch   109 | loss: 4.2570491Losses:  4.1352128982543945 0.13548918068408966
CurrentTrain: epoch  5, batch   110 | loss: 4.2707019Losses:  4.135643005371094 0.08334730565547943
CurrentTrain: epoch  5, batch   111 | loss: 4.2189903Losses:  4.043799877166748 0.1783965528011322
CurrentTrain: epoch  5, batch   112 | loss: 4.2221966Losses:  4.118383407592773 0.11104273051023483
CurrentTrain: epoch  5, batch   113 | loss: 4.2294259Losses:  4.127946376800537 0.1289374828338623
CurrentTrain: epoch  5, batch   114 | loss: 4.2568836Losses:  4.019155979156494 0.06265760958194733
CurrentTrain: epoch  5, batch   115 | loss: 4.0818138Losses:  4.116170883178711 0.08197394013404846
CurrentTrain: epoch  5, batch   116 | loss: 4.1981449Losses:  4.040102005004883 0.1040189117193222
CurrentTrain: epoch  5, batch   117 | loss: 4.1441207Losses:  4.107876777648926 0.18278518319129944
CurrentTrain: epoch  5, batch   118 | loss: 4.2906618Losses:  4.01142692565918 0.07782679796218872
CurrentTrain: epoch  5, batch   119 | loss: 4.0892539Losses:  4.059237480163574 0.09277308732271194
CurrentTrain: epoch  5, batch   120 | loss: 4.1520104Losses:  4.084878444671631 0.12971249222755432
CurrentTrain: epoch  5, batch   121 | loss: 4.2145910Losses:  4.0448198318481445 0.06808419525623322
CurrentTrain: epoch  5, batch   122 | loss: 4.1129041Losses:  4.069380760192871 0.0862232893705368
CurrentTrain: epoch  5, batch   123 | loss: 4.1556039Losses:  4.102754592895508 0.10432064533233643
CurrentTrain: epoch  5, batch   124 | loss: 4.2070751Losses:  4.126397132873535 0.09846610575914383
CurrentTrain: epoch  6, batch     0 | loss: 4.2248631Losses:  4.0377092361450195 0.054081473499536514
CurrentTrain: epoch  6, batch     1 | loss: 4.0917907Losses:  4.059508800506592 0.09652245789766312
CurrentTrain: epoch  6, batch     2 | loss: 4.1560311Losses:  4.06164026260376 0.10163900256156921
CurrentTrain: epoch  6, batch     3 | loss: 4.1632791Losses:  4.126413345336914 0.13635963201522827
CurrentTrain: epoch  6, batch     4 | loss: 4.2627730Losses:  3.9889330863952637 0.06985342502593994
CurrentTrain: epoch  6, batch     5 | loss: 4.0587864Losses:  4.022591590881348 0.14674369990825653
CurrentTrain: epoch  6, batch     6 | loss: 4.1693354Losses:  4.007841110229492 0.1508808732032776
CurrentTrain: epoch  6, batch     7 | loss: 4.1587219Losses:  4.095883369445801 0.15500135719776154
CurrentTrain: epoch  6, batch     8 | loss: 4.2508845Losses:  4.022851943969727 0.08110330253839493
CurrentTrain: epoch  6, batch     9 | loss: 4.1039553Losses:  4.089310646057129 0.09137603640556335
CurrentTrain: epoch  6, batch    10 | loss: 4.1806865Losses:  4.045949935913086 0.09747953712940216
CurrentTrain: epoch  6, batch    11 | loss: 4.1434293Losses:  4.138482570648193 0.11124258488416672
CurrentTrain: epoch  6, batch    12 | loss: 4.2497253Losses:  4.089789867401123 0.11139096319675446
CurrentTrain: epoch  6, batch    13 | loss: 4.2011809Losses:  4.0324602127075195 0.13764998316764832
CurrentTrain: epoch  6, batch    14 | loss: 4.1701102Losses:  4.051912784576416 0.07655832171440125
CurrentTrain: epoch  6, batch    15 | loss: 4.1284709Losses:  4.028158664703369 0.18093708157539368
CurrentTrain: epoch  6, batch    16 | loss: 4.2090960Losses:  4.086573600769043 0.1232704371213913
CurrentTrain: epoch  6, batch    17 | loss: 4.2098441Losses:  4.078037261962891 0.1622636616230011
CurrentTrain: epoch  6, batch    18 | loss: 4.2403011Losses:  3.9916539192199707 0.03297280892729759
CurrentTrain: epoch  6, batch    19 | loss: 4.0246267Losses:  4.038901329040527 0.12787391245365143
CurrentTrain: epoch  6, batch    20 | loss: 4.1667752Losses:  4.087981700897217 0.10354217886924744
CurrentTrain: epoch  6, batch    21 | loss: 4.1915240Losses:  4.054176330566406 0.13519813120365143
CurrentTrain: epoch  6, batch    22 | loss: 4.1893744Losses:  4.115833282470703 0.05367877334356308
CurrentTrain: epoch  6, batch    23 | loss: 4.1695123Losses:  4.028792858123779 0.11962173879146576
CurrentTrain: epoch  6, batch    24 | loss: 4.1484146Losses:  4.036957740783691 0.10900585353374481
CurrentTrain: epoch  6, batch    25 | loss: 4.1459637Losses:  4.008666515350342 0.10127316415309906
CurrentTrain: epoch  6, batch    26 | loss: 4.1099396Losses:  4.063943386077881 0.10446369647979736
CurrentTrain: epoch  6, batch    27 | loss: 4.1684070Losses:  4.060050964355469 0.15727519989013672
CurrentTrain: epoch  6, batch    28 | loss: 4.2173262Losses:  3.997699737548828 0.10244383662939072
CurrentTrain: epoch  6, batch    29 | loss: 4.1001434Losses:  4.013284683227539 0.15670230984687805
CurrentTrain: epoch  6, batch    30 | loss: 4.1699872Losses:  4.0422563552856445 0.08575055003166199
CurrentTrain: epoch  6, batch    31 | loss: 4.1280069Losses:  4.098597526550293 0.10567236691713333
CurrentTrain: epoch  6, batch    32 | loss: 4.2042699Losses:  4.076325416564941 0.12875379621982574
CurrentTrain: epoch  6, batch    33 | loss: 4.2050791Losses:  4.053895950317383 0.14006973803043365
CurrentTrain: epoch  6, batch    34 | loss: 4.1939659Losses:  4.001274108886719 0.05094437301158905
CurrentTrain: epoch  6, batch    35 | loss: 4.0522184Losses:  4.021011829376221 0.07229892164468765
CurrentTrain: epoch  6, batch    36 | loss: 4.0933108Losses:  4.095573425292969 0.0388113409280777
CurrentTrain: epoch  6, batch    37 | loss: 4.1343846Losses:  4.064575672149658 0.15390831232070923
CurrentTrain: epoch  6, batch    38 | loss: 4.2184839Losses:  4.031403541564941 0.14835233986377716
CurrentTrain: epoch  6, batch    39 | loss: 4.1797557Losses:  4.039481163024902 0.15578821301460266
CurrentTrain: epoch  6, batch    40 | loss: 4.1952696Losses:  4.076503753662109 0.11710740625858307
CurrentTrain: epoch  6, batch    41 | loss: 4.1936111Losses:  4.073735237121582 0.06761810928583145
CurrentTrain: epoch  6, batch    42 | loss: 4.1413531Losses:  4.017670631408691 0.09823521971702576
CurrentTrain: epoch  6, batch    43 | loss: 4.1159058Losses:  4.029448986053467 0.034452274441719055
CurrentTrain: epoch  6, batch    44 | loss: 4.0639014Losses:  4.044547080993652 0.14614519476890564
CurrentTrain: epoch  6, batch    45 | loss: 4.1906924Losses:  4.101017951965332 0.09635623544454575
CurrentTrain: epoch  6, batch    46 | loss: 4.1973743Losses:  4.158372402191162 0.09055963158607483
CurrentTrain: epoch  6, batch    47 | loss: 4.2489319Losses:  4.078065872192383 0.14154833555221558
CurrentTrain: epoch  6, batch    48 | loss: 4.2196140Losses:  3.9904065132141113 0.10252615809440613
CurrentTrain: epoch  6, batch    49 | loss: 4.0929327Losses:  4.0404815673828125 0.10050734877586365
CurrentTrain: epoch  6, batch    50 | loss: 4.1409888Losses:  4.038949012756348 0.1200394257903099
CurrentTrain: epoch  6, batch    51 | loss: 4.1589885Losses:  3.9845640659332275 0.10035128891468048
CurrentTrain: epoch  6, batch    52 | loss: 4.0849152Losses:  4.097835540771484 0.1366550326347351
CurrentTrain: epoch  6, batch    53 | loss: 4.2344904Losses:  4.009649276733398 0.08941319584846497
CurrentTrain: epoch  6, batch    54 | loss: 4.0990624Losses:  4.008048057556152 0.1340002715587616
CurrentTrain: epoch  6, batch    55 | loss: 4.1420484Losses:  4.0178608894348145 0.09398706257343292
CurrentTrain: epoch  6, batch    56 | loss: 4.1118479Losses:  4.060389518737793 0.11649719625711441
CurrentTrain: epoch  6, batch    57 | loss: 4.1768866Losses:  4.025627136230469 0.1695781946182251
CurrentTrain: epoch  6, batch    58 | loss: 4.1952052Losses:  4.013827323913574 0.12811173498630524
CurrentTrain: epoch  6, batch    59 | loss: 4.1419392Losses:  4.049372673034668 0.0918569564819336
CurrentTrain: epoch  6, batch    60 | loss: 4.1412296Losses:  4.08754301071167 0.13489893078804016
CurrentTrain: epoch  6, batch    61 | loss: 4.2224422Losses:  4.032901287078857 0.06928427517414093
CurrentTrain: epoch  6, batch    62 | loss: 4.1021857Losses:  4.131415367126465 0.025922656059265137
CurrentTrain: epoch  6, batch    63 | loss: 4.1573381Losses:  4.071764945983887 0.0754428431391716
CurrentTrain: epoch  6, batch    64 | loss: 4.1472077Losses:  4.048548698425293 0.10850407183170319
CurrentTrain: epoch  6, batch    65 | loss: 4.1570530Losses:  4.019927024841309 0.12976202368736267
CurrentTrain: epoch  6, batch    66 | loss: 4.1496892Losses:  4.035942554473877 0.11877906322479248
CurrentTrain: epoch  6, batch    67 | loss: 4.1547217Losses:  4.010336399078369 0.10357533395290375
CurrentTrain: epoch  6, batch    68 | loss: 4.1139116Losses:  4.00515079498291 0.2206871211528778
CurrentTrain: epoch  6, batch    69 | loss: 4.2258377Losses:  4.008085250854492 0.09024924039840698
CurrentTrain: epoch  6, batch    70 | loss: 4.0983343Losses:  3.9692368507385254 0.09464102983474731
CurrentTrain: epoch  6, batch    71 | loss: 4.0638781Losses:  4.043586254119873 0.04557688534259796
CurrentTrain: epoch  6, batch    72 | loss: 4.0891633Losses:  4.121013164520264 0.09613028913736343
CurrentTrain: epoch  6, batch    73 | loss: 4.2171435Losses:  4.021950721740723 0.13617870211601257
CurrentTrain: epoch  6, batch    74 | loss: 4.1581292Losses:  4.036983489990234 0.10414653271436691
CurrentTrain: epoch  6, batch    75 | loss: 4.1411300Losses:  4.050786972045898 0.16868069767951965
CurrentTrain: epoch  6, batch    76 | loss: 4.2194676Losses:  4.037937164306641 0.15205399692058563
CurrentTrain: epoch  6, batch    77 | loss: 4.1899910Losses:  4.040294647216797 0.06193574517965317
CurrentTrain: epoch  6, batch    78 | loss: 4.1022305Losses:  4.0001654624938965 0.11468695849180222
CurrentTrain: epoch  6, batch    79 | loss: 4.1148524Losses:  4.1139936447143555 0.07010152190923691
CurrentTrain: epoch  6, batch    80 | loss: 4.1840954Losses:  4.041765213012695 0.12203533947467804
CurrentTrain: epoch  6, batch    81 | loss: 4.1638007Losses:  4.006897926330566 0.0943586602807045
CurrentTrain: epoch  6, batch    82 | loss: 4.1012564Losses:  4.035539627075195 0.12382502853870392
CurrentTrain: epoch  6, batch    83 | loss: 4.1593647Losses:  3.974807024002075 0.12198926508426666
CurrentTrain: epoch  6, batch    84 | loss: 4.0967965Losses:  3.9705357551574707 0.11411111801862717
CurrentTrain: epoch  6, batch    85 | loss: 4.0846467Losses:  4.01397705078125 0.1460985243320465
CurrentTrain: epoch  6, batch    86 | loss: 4.1600757Losses:  3.9845402240753174 0.022020112723112106
CurrentTrain: epoch  6, batch    87 | loss: 4.0065603Losses:  4.0439863204956055 0.12404797971248627
CurrentTrain: epoch  6, batch    88 | loss: 4.1680341Losses:  4.035507678985596 0.09802882373332977
CurrentTrain: epoch  6, batch    89 | loss: 4.1335363Losses:  4.142638206481934 0.0793340802192688
CurrentTrain: epoch  6, batch    90 | loss: 4.2219725Losses:  4.021932601928711 0.1020093634724617
CurrentTrain: epoch  6, batch    91 | loss: 4.1239419Losses:  4.003323554992676 0.11022839695215225
CurrentTrain: epoch  6, batch    92 | loss: 4.1135521Losses:  3.950960636138916 0.08903452754020691
CurrentTrain: epoch  6, batch    93 | loss: 4.0399952Losses:  4.108386039733887 0.08048743009567261
CurrentTrain: epoch  6, batch    94 | loss: 4.1888733Losses:  3.8717355728149414 0.09470506012439728
CurrentTrain: epoch  6, batch    95 | loss: 3.9664407Losses:  3.9817137718200684 0.09612869471311569
CurrentTrain: epoch  6, batch    96 | loss: 4.0778422Losses:  4.038112640380859 0.09770263731479645
CurrentTrain: epoch  6, batch    97 | loss: 4.1358151Losses:  4.029967784881592 0.10080128908157349
CurrentTrain: epoch  6, batch    98 | loss: 4.1307693Losses:  4.034653663635254 0.06422606110572815
CurrentTrain: epoch  6, batch    99 | loss: 4.0988798Losses:  4.034064292907715 0.10187442600727081
CurrentTrain: epoch  6, batch   100 | loss: 4.1359386Losses:  4.072912216186523 0.10598288476467133
CurrentTrain: epoch  6, batch   101 | loss: 4.1788950Losses:  3.976907730102539 0.08807811141014099
CurrentTrain: epoch  6, batch   102 | loss: 4.0649858Losses:  4.031024932861328 0.10087963193655014
CurrentTrain: epoch  6, batch   103 | loss: 4.1319046Losses:  3.971874713897705 0.11795730143785477
CurrentTrain: epoch  6, batch   104 | loss: 4.0898318Losses:  4.0824971199035645 0.11382710933685303
CurrentTrain: epoch  6, batch   105 | loss: 4.1963243Losses:  4.056577205657959 0.06622717529535294
CurrentTrain: epoch  6, batch   106 | loss: 4.1228042Losses:  4.007259368896484 0.04957404360175133
CurrentTrain: epoch  6, batch   107 | loss: 4.0568333Losses:  4.033966541290283 0.09274177998304367
CurrentTrain: epoch  6, batch   108 | loss: 4.1267085Losses:  4.004543781280518 0.05160166695713997
CurrentTrain: epoch  6, batch   109 | loss: 4.0561457Losses:  3.9878058433532715 0.09068500995635986
CurrentTrain: epoch  6, batch   110 | loss: 4.0784907Losses:  3.97232723236084 0.11059071123600006
CurrentTrain: epoch  6, batch   111 | loss: 4.0829182Losses:  4.016921043395996 0.07920514047145844
CurrentTrain: epoch  6, batch   112 | loss: 4.0961261Losses:  4.039473533630371 0.08436036109924316
CurrentTrain: epoch  6, batch   113 | loss: 4.1238337Losses:  3.9825925827026367 0.09786053001880646
CurrentTrain: epoch  6, batch   114 | loss: 4.0804529Losses:  3.9926061630249023 0.08512791246175766
CurrentTrain: epoch  6, batch   115 | loss: 4.0777340Losses:  3.951678514480591 0.11016890406608582
CurrentTrain: epoch  6, batch   116 | loss: 4.0618472Losses:  4.014614105224609 0.14621977508068085
CurrentTrain: epoch  6, batch   117 | loss: 4.1608338Losses:  4.0067458152771 0.1355866938829422
CurrentTrain: epoch  6, batch   118 | loss: 4.1423326Losses:  4.012266635894775 0.13798601925373077
CurrentTrain: epoch  6, batch   119 | loss: 4.1502528Losses:  4.024768829345703 0.11993027478456497
CurrentTrain: epoch  6, batch   120 | loss: 4.1446991Losses:  4.028475761413574 0.03301730751991272
CurrentTrain: epoch  6, batch   121 | loss: 4.0614929Losses:  4.0045881271362305 0.06265538930892944
CurrentTrain: epoch  6, batch   122 | loss: 4.0672436Losses:  3.9930925369262695 0.12392610311508179
CurrentTrain: epoch  6, batch   123 | loss: 4.1170187Losses:  4.011112213134766 0.18150517344474792
CurrentTrain: epoch  6, batch   124 | loss: 4.1926174Losses:  3.964165449142456 0.08629610389471054
CurrentTrain: epoch  7, batch     0 | loss: 4.0504618Losses:  4.009756088256836 0.08612470328807831
CurrentTrain: epoch  7, batch     1 | loss: 4.0958810Losses:  4.082720756530762 0.12263573706150055
CurrentTrain: epoch  7, batch     2 | loss: 4.2053566Losses:  4.077364921569824 0.07103970646858215
CurrentTrain: epoch  7, batch     3 | loss: 4.1484046Losses:  4.0263776779174805 0.1091945618391037
CurrentTrain: epoch  7, batch     4 | loss: 4.1355724Losses:  3.996809959411621 0.04407871514558792
CurrentTrain: epoch  7, batch     5 | loss: 4.0408888Losses:  4.041958808898926 0.13623327016830444
CurrentTrain: epoch  7, batch     6 | loss: 4.1781921Losses:  4.030500411987305 0.05080560967326164
CurrentTrain: epoch  7, batch     7 | loss: 4.0813060Losses:  4.012650489807129 0.07127820700407028
CurrentTrain: epoch  7, batch     8 | loss: 4.0839286Losses:  3.964613914489746 0.03155126795172691
CurrentTrain: epoch  7, batch     9 | loss: 3.9961653Losses:  4.026464939117432 0.10214941203594208
CurrentTrain: epoch  7, batch    10 | loss: 4.1286144Losses:  4.096053123474121 0.05731840059161186
CurrentTrain: epoch  7, batch    11 | loss: 4.1533713Losses:  4.069051742553711 0.07670588046312332
CurrentTrain: epoch  7, batch    12 | loss: 4.1457577Losses:  4.016936302185059 0.1277923583984375
CurrentTrain: epoch  7, batch    13 | loss: 4.1447287Losses:  4.034898281097412 0.08393474668264389
CurrentTrain: epoch  7, batch    14 | loss: 4.1188331Losses:  4.008739471435547 0.1267080008983612
CurrentTrain: epoch  7, batch    15 | loss: 4.1354475Losses:  4.024356842041016 0.08696982264518738
CurrentTrain: epoch  7, batch    16 | loss: 4.1113267Losses:  4.077140808105469 0.05438998341560364
CurrentTrain: epoch  7, batch    17 | loss: 4.1315308Losses:  3.9617772102355957 0.13903944194316864
CurrentTrain: epoch  7, batch    18 | loss: 4.1008167Losses:  4.021279811859131 0.07680484652519226
CurrentTrain: epoch  7, batch    19 | loss: 4.0980844Losses:  4.037629127502441 0.11136108636856079
CurrentTrain: epoch  7, batch    20 | loss: 4.1489902Losses:  3.9843554496765137 0.1243411973118782
CurrentTrain: epoch  7, batch    21 | loss: 4.1086965Losses:  4.004612922668457 0.12302427738904953
CurrentTrain: epoch  7, batch    22 | loss: 4.1276374Losses:  3.9793665409088135 0.060539551079273224
CurrentTrain: epoch  7, batch    23 | loss: 4.0399060Losses:  4.054194450378418 0.026206128299236298
CurrentTrain: epoch  7, batch    24 | loss: 4.0804005Losses:  3.9759533405303955 0.14367958903312683
CurrentTrain: epoch  7, batch    25 | loss: 4.1196327Losses:  4.028232574462891 0.10217151045799255
CurrentTrain: epoch  7, batch    26 | loss: 4.1304040Losses:  3.9593067169189453 0.08119384944438934
CurrentTrain: epoch  7, batch    27 | loss: 4.0405006Losses:  4.038674354553223 0.09023325145244598
CurrentTrain: epoch  7, batch    28 | loss: 4.1289077Losses:  4.015112400054932 0.10668504238128662
CurrentTrain: epoch  7, batch    29 | loss: 4.1217976Losses:  4.017614364624023 0.167036235332489
CurrentTrain: epoch  7, batch    30 | loss: 4.1846504Losses:  4.02488899230957 0.1148502379655838
CurrentTrain: epoch  7, batch    31 | loss: 4.1397390Losses:  3.9906766414642334 0.07621236890554428
CurrentTrain: epoch  7, batch    32 | loss: 4.0668888Losses:  4.014653205871582 0.15417401492595673
CurrentTrain: epoch  7, batch    33 | loss: 4.1688271Losses:  4.025594711303711 0.13687005639076233
CurrentTrain: epoch  7, batch    34 | loss: 4.1624646Losses:  4.001025199890137 0.1252581924200058
CurrentTrain: epoch  7, batch    35 | loss: 4.1262832Losses:  4.040432929992676 0.07877901196479797
CurrentTrain: epoch  7, batch    36 | loss: 4.1192122Losses:  4.015002727508545 0.06910993903875351
CurrentTrain: epoch  7, batch    37 | loss: 4.0841126Losses:  4.001802921295166 0.1298573613166809
CurrentTrain: epoch  7, batch    38 | loss: 4.1316605Losses:  4.009730815887451 0.13197940587997437
CurrentTrain: epoch  7, batch    39 | loss: 4.1417103Losses:  4.040255546569824 0.08701276779174805
CurrentTrain: epoch  7, batch    40 | loss: 4.1272683Losses:  4.0281524658203125 0.08653193712234497
CurrentTrain: epoch  7, batch    41 | loss: 4.1146846Losses:  4.057563781738281 0.09844373166561127
CurrentTrain: epoch  7, batch    42 | loss: 4.1560073Losses:  3.968364715576172 0.0853857472538948
CurrentTrain: epoch  7, batch    43 | loss: 4.0537505Losses:  3.9647023677825928 0.0649951919913292
CurrentTrain: epoch  7, batch    44 | loss: 4.0296974Losses:  4.019612789154053 0.09118983894586563
CurrentTrain: epoch  7, batch    45 | loss: 4.1108027Losses:  4.058088779449463 0.06431527435779572
CurrentTrain: epoch  7, batch    46 | loss: 4.1224041Losses:  4.011091232299805 0.06258056312799454
CurrentTrain: epoch  7, batch    47 | loss: 4.0736718Losses:  3.982736349105835 0.09834828972816467
CurrentTrain: epoch  7, batch    48 | loss: 4.0810847Losses:  4.012113571166992 0.14526206254959106
CurrentTrain: epoch  7, batch    49 | loss: 4.1573758Losses:  4.034648895263672 0.13961341977119446
CurrentTrain: epoch  7, batch    50 | loss: 4.1742625Losses:  4.037878036499023 0.0308525413274765
CurrentTrain: epoch  7, batch    51 | loss: 4.0687304Losses:  4.022963523864746 0.04767361283302307
CurrentTrain: epoch  7, batch    52 | loss: 4.0706372Losses:  3.887118339538574 0.0582345649600029
CurrentTrain: epoch  7, batch    53 | loss: 3.9453528Losses:  4.038410186767578 0.09974561631679535
CurrentTrain: epoch  7, batch    54 | loss: 4.1381559Losses:  4.019809722900391 0.13740003108978271
CurrentTrain: epoch  7, batch    55 | loss: 4.1572099Losses:  3.9860830307006836 0.10101938992738724
CurrentTrain: epoch  7, batch    56 | loss: 4.0871024Losses:  4.015199661254883 0.04627564549446106
CurrentTrain: epoch  7, batch    57 | loss: 4.0614753Losses:  4.0371599197387695 0.0693034678697586
CurrentTrain: epoch  7, batch    58 | loss: 4.1064634Losses:  3.955918312072754 0.06840305775403976
CurrentTrain: epoch  7, batch    59 | loss: 4.0243216Losses:  3.976127862930298 0.10521207004785538
CurrentTrain: epoch  7, batch    60 | loss: 4.0813398Losses:  3.9966275691986084 0.04932384938001633
CurrentTrain: epoch  7, batch    61 | loss: 4.0459514Losses:  4.011881351470947 0.08784498274326324
CurrentTrain: epoch  7, batch    62 | loss: 4.0997262Losses:  4.071163654327393 0.09180667251348495
CurrentTrain: epoch  7, batch    63 | loss: 4.1629705Losses:  3.980463981628418 0.0758349820971489
CurrentTrain: epoch  7, batch    64 | loss: 4.0562987Losses:  3.9844257831573486 0.03689607232809067
CurrentTrain: epoch  7, batch    65 | loss: 4.0213218Losses:  3.955216407775879 0.10482170432806015
CurrentTrain: epoch  7, batch    66 | loss: 4.0600381Losses:  3.9921391010284424 0.0722137838602066
CurrentTrain: epoch  7, batch    67 | loss: 4.0643530Losses:  4.082503318786621 0.0593910813331604
CurrentTrain: epoch  7, batch    68 | loss: 4.1418943Losses:  4.0558977127075195 0.04280935972929001
CurrentTrain: epoch  7, batch    69 | loss: 4.0987072Losses:  4.031260967254639 0.04430137202143669
CurrentTrain: epoch  7, batch    70 | loss: 4.0755625Losses:  4.013579845428467 0.10998237133026123
CurrentTrain: epoch  7, batch    71 | loss: 4.1235623Losses:  3.980001449584961 0.06836186349391937
CurrentTrain: epoch  7, batch    72 | loss: 4.0483632Losses:  4.050473690032959 0.06952765583992004
CurrentTrain: epoch  7, batch    73 | loss: 4.1200013Losses:  3.9883651733398438 0.09180392324924469
CurrentTrain: epoch  7, batch    74 | loss: 4.0801692Losses:  3.9927656650543213 0.09888588637113571
CurrentTrain: epoch  7, batch    75 | loss: 4.0916514Losses:  3.997243642807007 0.1338527500629425
CurrentTrain: epoch  7, batch    76 | loss: 4.1310964Losses:  4.0337934494018555 0.07624431699514389
CurrentTrain: epoch  7, batch    77 | loss: 4.1100378Losses:  4.028050422668457 0.06153673306107521
CurrentTrain: epoch  7, batch    78 | loss: 4.0895872Losses:  3.928865432739258 0.0849519670009613
CurrentTrain: epoch  7, batch    79 | loss: 4.0138173Losses:  4.026902198791504 0.06317019462585449
CurrentTrain: epoch  7, batch    80 | loss: 4.0900726Losses:  4.070267677307129 0.1345304548740387
CurrentTrain: epoch  7, batch    81 | loss: 4.2047982Losses:  3.9851555824279785 0.10267192125320435
CurrentTrain: epoch  7, batch    82 | loss: 4.0878277Losses:  3.919818639755249 0.1058427169919014
CurrentTrain: epoch  7, batch    83 | loss: 4.0256615Losses:  3.9584662914276123 0.1345025897026062
CurrentTrain: epoch  7, batch    84 | loss: 4.0929689Losses:  4.050057411193848 0.06262272596359253
CurrentTrain: epoch  7, batch    85 | loss: 4.1126800Losses:  4.052531719207764 0.0956227034330368
CurrentTrain: epoch  7, batch    86 | loss: 4.1481543Losses:  4.048731803894043 0.10255652666091919
CurrentTrain: epoch  7, batch    87 | loss: 4.1512885Losses:  4.007809638977051 0.09118466824293137
CurrentTrain: epoch  7, batch    88 | loss: 4.0989943Losses:  3.961090087890625 0.055469539016485214
CurrentTrain: epoch  7, batch    89 | loss: 4.0165596Losses:  3.967315673828125 0.0635577142238617
CurrentTrain: epoch  7, batch    90 | loss: 4.0308733Losses:  4.00654935836792 0.07928524911403656
CurrentTrain: epoch  7, batch    91 | loss: 4.0858345Losses:  3.9988691806793213 0.08678331971168518
CurrentTrain: epoch  7, batch    92 | loss: 4.0856524Losses:  3.97329044342041 0.04934142529964447
CurrentTrain: epoch  7, batch    93 | loss: 4.0226316Losses:  4.011063575744629 0.1280474215745926
CurrentTrain: epoch  7, batch    94 | loss: 4.1391110Losses:  3.9271492958068848 0.04567082226276398
CurrentTrain: epoch  7, batch    95 | loss: 3.9728200Losses:  4.028785228729248 0.13950438797473907
CurrentTrain: epoch  7, batch    96 | loss: 4.1682897Losses:  4.008375644683838 0.15361329913139343
CurrentTrain: epoch  7, batch    97 | loss: 4.1619887Losses:  4.024360656738281 0.11225681006908417
CurrentTrain: epoch  7, batch    98 | loss: 4.1366177Losses:  4.0479350090026855 0.1864657700061798
CurrentTrain: epoch  7, batch    99 | loss: 4.2344007Losses:  3.9667184352874756 0.1130760982632637
CurrentTrain: epoch  7, batch   100 | loss: 4.0797944Losses:  4.0426459312438965 0.045839279890060425
CurrentTrain: epoch  7, batch   101 | loss: 4.0884852Losses:  4.000171661376953 0.11898179352283478
CurrentTrain: epoch  7, batch   102 | loss: 4.1191535Losses:  3.986206531524658 0.08131824433803558
CurrentTrain: epoch  7, batch   103 | loss: 4.0675249Losses:  3.9908995628356934 0.11358200013637543
CurrentTrain: epoch  7, batch   104 | loss: 4.1044817Losses:  3.926973581314087 0.07784148305654526
CurrentTrain: epoch  7, batch   105 | loss: 4.0048151Losses:  3.9048657417297363 0.0681658387184143
CurrentTrain: epoch  7, batch   106 | loss: 3.9730315Losses:  4.034434795379639 0.10777553915977478
CurrentTrain: epoch  7, batch   107 | loss: 4.1422105Losses:  3.9459164142608643 0.052644163370132446
CurrentTrain: epoch  7, batch   108 | loss: 3.9985607Losses:  4.01947021484375 0.06897246092557907
CurrentTrain: epoch  7, batch   109 | loss: 4.0884428Losses:  4.087977409362793 0.067548468708992
CurrentTrain: epoch  7, batch   110 | loss: 4.1555257Losses:  4.031826496124268 0.07632900774478912
CurrentTrain: epoch  7, batch   111 | loss: 4.1081557Losses:  4.024993419647217 0.03635723143815994
CurrentTrain: epoch  7, batch   112 | loss: 4.0613508Losses:  3.9903814792633057 0.07460324466228485
CurrentTrain: epoch  7, batch   113 | loss: 4.0649848Losses:  3.978424310684204 0.11465159058570862
CurrentTrain: epoch  7, batch   114 | loss: 4.0930758Losses:  3.9853339195251465 0.048207737505435944
CurrentTrain: epoch  7, batch   115 | loss: 4.0335417Losses:  3.980661630630493 0.13486653566360474
CurrentTrain: epoch  7, batch   116 | loss: 4.1155281Losses:  4.021177768707275 0.07106843590736389
CurrentTrain: epoch  7, batch   117 | loss: 4.0922461Losses:  3.9999969005584717 0.09920230507850647
CurrentTrain: epoch  7, batch   118 | loss: 4.0991993Losses:  4.051417827606201 0.08156747370958328
CurrentTrain: epoch  7, batch   119 | loss: 4.1329851Losses:  3.982990026473999 0.08948227763175964
CurrentTrain: epoch  7, batch   120 | loss: 4.0724721Losses:  4.026765823364258 0.12144016474485397
CurrentTrain: epoch  7, batch   121 | loss: 4.1482058Losses:  3.978455066680908 0.08050166070461273
CurrentTrain: epoch  7, batch   122 | loss: 4.0589566Losses:  3.9735054969787598 0.05351021885871887
CurrentTrain: epoch  7, batch   123 | loss: 4.0270157Losses:  4.046976089477539 0.07348444312810898
CurrentTrain: epoch  7, batch   124 | loss: 4.1204605Losses:  3.982952117919922 0.09261875599622726
CurrentTrain: epoch  8, batch     0 | loss: 4.0755711Losses:  3.9743402004241943 0.1461573839187622
CurrentTrain: epoch  8, batch     1 | loss: 4.1204977Losses:  4.050500869750977 0.11166678369045258
CurrentTrain: epoch  8, batch     2 | loss: 4.1621675Losses:  4.008159637451172 0.0667555034160614
CurrentTrain: epoch  8, batch     3 | loss: 4.0749149Losses:  3.9868574142456055 0.08967314660549164
CurrentTrain: epoch  8, batch     4 | loss: 4.0765305Losses:  4.001745700836182 0.10028177499771118
CurrentTrain: epoch  8, batch     5 | loss: 4.1020274Losses:  3.9840598106384277 0.05210946500301361
CurrentTrain: epoch  8, batch     6 | loss: 4.0361691Losses:  3.933340549468994 0.03238799422979355
CurrentTrain: epoch  8, batch     7 | loss: 3.9657285Losses:  3.955841541290283 0.09690876305103302
CurrentTrain: epoch  8, batch     8 | loss: 4.0527501Losses:  3.9174306392669678 0.06529690325260162
CurrentTrain: epoch  8, batch     9 | loss: 3.9827275Losses:  4.0362043380737305 0.07086388766765594
CurrentTrain: epoch  8, batch    10 | loss: 4.1070681Losses:  3.98787784576416 0.12204523384571075
CurrentTrain: epoch  8, batch    11 | loss: 4.1099229Losses:  3.984335422515869 0.08991268277168274
CurrentTrain: epoch  8, batch    12 | loss: 4.0742483Losses:  3.946902275085449 0.08281870931386948
CurrentTrain: epoch  8, batch    13 | loss: 4.0297208Losses:  3.984470844268799 0.09215886890888214
CurrentTrain: epoch  8, batch    14 | loss: 4.0766296Losses:  4.030797958374023 0.08115603029727936
CurrentTrain: epoch  8, batch    15 | loss: 4.1119542Losses:  4.000133991241455 0.08719775080680847
CurrentTrain: epoch  8, batch    16 | loss: 4.0873318Losses:  3.978278160095215 0.06373476982116699
CurrentTrain: epoch  8, batch    17 | loss: 4.0420132Losses:  4.001992225646973 0.08635212481021881
CurrentTrain: epoch  8, batch    18 | loss: 4.0883446Losses:  3.9663052558898926 0.07542794942855835
CurrentTrain: epoch  8, batch    19 | loss: 4.0417333Losses:  4.022395610809326 0.04908064752817154
CurrentTrain: epoch  8, batch    20 | loss: 4.0714765Losses:  3.9939353466033936 0.07108921557664871
CurrentTrain: epoch  8, batch    21 | loss: 4.0650244Losses:  4.01188325881958 0.0920817106962204
CurrentTrain: epoch  8, batch    22 | loss: 4.1039648Losses:  3.9743640422821045 0.04756023362278938
CurrentTrain: epoch  8, batch    23 | loss: 4.0219245Losses:  3.9578232765197754 0.06924401223659515
CurrentTrain: epoch  8, batch    24 | loss: 4.0270672Losses:  3.9594030380249023 0.1375342607498169
CurrentTrain: epoch  8, batch    25 | loss: 4.0969372Losses:  3.9831013679504395 0.07205353677272797
CurrentTrain: epoch  8, batch    26 | loss: 4.0551548Losses:  4.028905391693115 0.09532056003808975
CurrentTrain: epoch  8, batch    27 | loss: 4.1242261Losses:  3.9780452251434326 0.13006910681724548
CurrentTrain: epoch  8, batch    28 | loss: 4.1081142Losses:  4.030814170837402 0.10149093717336655
CurrentTrain: epoch  8, batch    29 | loss: 4.1323051Losses:  4.025658130645752 0.0673549473285675
CurrentTrain: epoch  8, batch    30 | loss: 4.0930133Losses:  3.9290547370910645 0.12061512470245361
CurrentTrain: epoch  8, batch    31 | loss: 4.0496697Losses:  3.968946695327759 0.05802645906805992
CurrentTrain: epoch  8, batch    32 | loss: 4.0269732Losses:  3.968407154083252 0.055247269570827484
CurrentTrain: epoch  8, batch    33 | loss: 4.0236545Losses:  4.031455039978027 0.09598995745182037
CurrentTrain: epoch  8, batch    34 | loss: 4.1274452Losses:  4.001204490661621 0.07934349775314331
CurrentTrain: epoch  8, batch    35 | loss: 4.0805478Losses:  3.962691307067871 0.08836446702480316
CurrentTrain: epoch  8, batch    36 | loss: 4.0510559Losses:  3.9677209854125977 0.09134474396705627
CurrentTrain: epoch  8, batch    37 | loss: 4.0590658Losses:  3.975473165512085 0.042760610580444336
CurrentTrain: epoch  8, batch    38 | loss: 4.0182338Losses:  3.98716402053833 0.09652847051620483
CurrentTrain: epoch  8, batch    39 | loss: 4.0836926Losses:  4.045044898986816 0.05740685015916824
CurrentTrain: epoch  8, batch    40 | loss: 4.1024518Losses:  3.894028663635254 0.058655962347984314
CurrentTrain: epoch  8, batch    41 | loss: 3.9526846Losses:  3.9811418056488037 0.0838162750005722
CurrentTrain: epoch  8, batch    42 | loss: 4.0649581Losses:  4.022820472717285 0.09146018326282501
CurrentTrain: epoch  8, batch    43 | loss: 4.1142807Losses:  3.947030544281006 0.06755335628986359
CurrentTrain: epoch  8, batch    44 | loss: 4.0145841Losses:  3.974778413772583 0.12044423073530197
CurrentTrain: epoch  8, batch    45 | loss: 4.0952225Losses:  4.0164384841918945 0.08234767615795135
CurrentTrain: epoch  8, batch    46 | loss: 4.0987864Losses:  3.9899215698242188 0.07170400023460388
CurrentTrain: epoch  8, batch    47 | loss: 4.0616255Losses:  3.9658403396606445 0.10335037857294083
CurrentTrain: epoch  8, batch    48 | loss: 4.0691905Losses:  3.9450173377990723 0.1525399535894394
CurrentTrain: epoch  8, batch    49 | loss: 4.0975571Losses:  4.008057117462158 0.07121998816728592
CurrentTrain: epoch  8, batch    50 | loss: 4.0792770Losses:  4.004762649536133 0.08954079449176788
CurrentTrain: epoch  8, batch    51 | loss: 4.0943036Losses:  3.9824604988098145 0.048641543835401535
CurrentTrain: epoch  8, batch    52 | loss: 4.0311022Losses:  3.9941792488098145 0.1072349101305008
CurrentTrain: epoch  8, batch    53 | loss: 4.1014142Losses:  4.098012924194336 0.08825182914733887
CurrentTrain: epoch  8, batch    54 | loss: 4.1862650Losses:  3.963521957397461 0.11654426157474518
CurrentTrain: epoch  8, batch    55 | loss: 4.0800662Losses:  4.029639720916748 0.10150456428527832
CurrentTrain: epoch  8, batch    56 | loss: 4.1311445Losses:  3.946075916290283 0.134966641664505
CurrentTrain: epoch  8, batch    57 | loss: 4.0810428Losses:  4.013617515563965 0.08360700309276581
CurrentTrain: epoch  8, batch    58 | loss: 4.0972247Losses:  3.99544358253479 0.08789309114217758
CurrentTrain: epoch  8, batch    59 | loss: 4.0833368Losses:  3.9787747859954834 0.06876082718372345
CurrentTrain: epoch  8, batch    60 | loss: 4.0475354Losses:  3.9622116088867188 0.07208983600139618
CurrentTrain: epoch  8, batch    61 | loss: 4.0343013Losses:  4.002330780029297 0.0377388522028923
CurrentTrain: epoch  8, batch    62 | loss: 4.0400696Losses:  4.012584209442139 0.09394374489784241
CurrentTrain: epoch  8, batch    63 | loss: 4.1065278Losses:  3.9252049922943115 0.061167873442173004
CurrentTrain: epoch  8, batch    64 | loss: 3.9863729Losses:  4.022286415100098 0.09256027638912201
CurrentTrain: epoch  8, batch    65 | loss: 4.1148467Losses:  3.9850759506225586 0.0898018628358841
CurrentTrain: epoch  8, batch    66 | loss: 4.0748777Losses:  3.969675302505493 0.06048164516687393
CurrentTrain: epoch  8, batch    67 | loss: 4.0301571Losses:  4.032029628753662 0.09309959411621094
CurrentTrain: epoch  8, batch    68 | loss: 4.1251292Losses:  3.9880781173706055 0.0692763552069664
CurrentTrain: epoch  8, batch    69 | loss: 4.0573545Losses:  3.9809560775756836 0.06772297620773315
CurrentTrain: epoch  8, batch    70 | loss: 4.0486789Losses:  3.9821934700012207 0.12497188150882721
CurrentTrain: epoch  8, batch    71 | loss: 4.1071653Losses:  3.9764106273651123 0.062182918190956116
CurrentTrain: epoch  8, batch    72 | loss: 4.0385938Losses:  3.9640321731567383 0.05232226476073265
CurrentTrain: epoch  8, batch    73 | loss: 4.0163546Losses:  4.033224105834961 0.11069923639297485
CurrentTrain: epoch  8, batch    74 | loss: 4.1439233Losses:  3.996065139770508 0.11684578657150269
CurrentTrain: epoch  8, batch    75 | loss: 4.1129107Losses:  3.941417932510376 0.12263456732034683
CurrentTrain: epoch  8, batch    76 | loss: 4.0640526Losses:  3.930914878845215 0.10131870210170746
CurrentTrain: epoch  8, batch    77 | loss: 4.0322337Losses:  4.011811256408691 0.1415325403213501
CurrentTrain: epoch  8, batch    78 | loss: 4.1533437Losses:  4.027136325836182 0.07017628848552704
CurrentTrain: epoch  8, batch    79 | loss: 4.0973125Losses:  4.010403633117676 0.09247718006372452
CurrentTrain: epoch  8, batch    80 | loss: 4.1028810Losses:  3.9873785972595215 0.08153310418128967
CurrentTrain: epoch  8, batch    81 | loss: 4.0689116Losses:  4.027207851409912 0.09616512060165405
CurrentTrain: epoch  8, batch    82 | loss: 4.1233730Losses:  3.9658987522125244 0.063337042927742
CurrentTrain: epoch  8, batch    83 | loss: 4.0292358Losses:  3.976816177368164 0.09277231991291046
CurrentTrain: epoch  8, batch    84 | loss: 4.0695887Losses:  3.9771811962127686 0.0657055452466011
CurrentTrain: epoch  8, batch    85 | loss: 4.0428867Losses:  3.97598934173584 0.0741422176361084
CurrentTrain: epoch  8, batch    86 | loss: 4.0501318Losses:  3.9833524227142334 0.10377657413482666
CurrentTrain: epoch  8, batch    87 | loss: 4.0871291Losses:  3.980304718017578 0.07418058812618256
CurrentTrain: epoch  8, batch    88 | loss: 4.0544853Losses:  4.003008842468262 0.11190447956323624
CurrentTrain: epoch  8, batch    89 | loss: 4.1149135Losses:  4.043454647064209 0.0719359740614891
CurrentTrain: epoch  8, batch    90 | loss: 4.1153908Losses:  4.05056619644165 0.022846480831503868
CurrentTrain: epoch  8, batch    91 | loss: 4.0734129Losses:  3.957542657852173 0.05383903905749321
CurrentTrain: epoch  8, batch    92 | loss: 4.0113816Losses:  3.9225850105285645 0.08125549554824829
CurrentTrain: epoch  8, batch    93 | loss: 4.0038404Losses:  3.93329119682312 0.09006767719984055
CurrentTrain: epoch  8, batch    94 | loss: 4.0233588Losses:  3.9639463424682617 0.11708931624889374
CurrentTrain: epoch  8, batch    95 | loss: 4.0810356Losses:  3.936490058898926 0.0738137811422348
CurrentTrain: epoch  8, batch    96 | loss: 4.0103040Losses:  3.934453248977661 0.042306698858737946
CurrentTrain: epoch  8, batch    97 | loss: 3.9767599Losses:  3.9650936126708984 0.08374225348234177
CurrentTrain: epoch  8, batch    98 | loss: 4.0488358Losses:  3.9848599433898926 0.09711188822984695
CurrentTrain: epoch  8, batch    99 | loss: 4.0819716Losses:  3.955622434616089 0.0865093469619751
CurrentTrain: epoch  8, batch   100 | loss: 4.0421319Losses:  3.959193468093872 0.08567006886005402
CurrentTrain: epoch  8, batch   101 | loss: 4.0448637Losses:  3.97354793548584 0.0847361609339714
CurrentTrain: epoch  8, batch   102 | loss: 4.0582843Losses:  3.891510009765625 0.03793330490589142
CurrentTrain: epoch  8, batch   103 | loss: 3.9294434Losses:  3.995278835296631 0.15759865939617157
CurrentTrain: epoch  8, batch   104 | loss: 4.1528773Losses:  3.930208206176758 0.08401735126972198
CurrentTrain: epoch  8, batch   105 | loss: 4.0142255Losses:  3.981351852416992 0.09592609107494354
CurrentTrain: epoch  8, batch   106 | loss: 4.0772781Losses:  4.032003402709961 0.0719529315829277
CurrentTrain: epoch  8, batch   107 | loss: 4.1039562Losses:  3.9719536304473877 0.11539269983768463
CurrentTrain: epoch  8, batch   108 | loss: 4.0873466Losses:  3.9672141075134277 0.06400106102228165
CurrentTrain: epoch  8, batch   109 | loss: 4.0312152Losses:  3.966543674468994 0.09160216152667999
CurrentTrain: epoch  8, batch   110 | loss: 4.0581460Losses:  4.028151512145996 0.05454293638467789
CurrentTrain: epoch  8, batch   111 | loss: 4.0826945Losses:  3.998502492904663 0.09610114991664886
CurrentTrain: epoch  8, batch   112 | loss: 4.0946035Losses:  4.046388626098633 0.05060798302292824
CurrentTrain: epoch  8, batch   113 | loss: 4.0969968Losses:  4.020803451538086 0.04966123402118683
CurrentTrain: epoch  8, batch   114 | loss: 4.0704646Losses:  3.9778995513916016 0.11560583114624023
CurrentTrain: epoch  8, batch   115 | loss: 4.0935054Losses:  3.996410846710205 0.07598432898521423
CurrentTrain: epoch  8, batch   116 | loss: 4.0723953Losses:  3.9783637523651123 0.06423293799161911
CurrentTrain: epoch  8, batch   117 | loss: 4.0425968Losses:  3.9628026485443115 0.06726522743701935
CurrentTrain: epoch  8, batch   118 | loss: 4.0300679Losses:  3.9657740592956543 0.09105789661407471
CurrentTrain: epoch  8, batch   119 | loss: 4.0568318Losses:  4.005037307739258 0.08271346986293793
CurrentTrain: epoch  8, batch   120 | loss: 4.0877509Losses:  4.0228071212768555 0.13801711797714233
CurrentTrain: epoch  8, batch   121 | loss: 4.1608243Losses:  3.93918514251709 0.08845694363117218
CurrentTrain: epoch  8, batch   122 | loss: 4.0276423Losses:  3.999300479888916 0.05628479644656181
CurrentTrain: epoch  8, batch   123 | loss: 4.0555854Losses:  3.984935998916626 0.043664488941431046
CurrentTrain: epoch  8, batch   124 | loss: 4.0286007Losses:  3.9849071502685547 0.0466347299516201
CurrentTrain: epoch  9, batch     0 | loss: 4.0315418Losses:  3.9565012454986572 0.06784846633672714
CurrentTrain: epoch  9, batch     1 | loss: 4.0243497Losses:  3.995595693588257 0.1069377213716507
CurrentTrain: epoch  9, batch     2 | loss: 4.1025333Losses:  3.984945058822632 0.0463232696056366
CurrentTrain: epoch  9, batch     3 | loss: 4.0312681Losses:  4.010516166687012 0.08010337501764297
CurrentTrain: epoch  9, batch     4 | loss: 4.0906196Losses:  3.9654488563537598 0.07272224128246307
CurrentTrain: epoch  9, batch     5 | loss: 4.0381713Losses:  3.9657773971557617 0.08842489123344421
CurrentTrain: epoch  9, batch     6 | loss: 4.0542021Losses:  3.9868087768554688 0.07377438247203827
CurrentTrain: epoch  9, batch     7 | loss: 4.0605831Losses:  3.9357690811157227 0.07647234201431274
CurrentTrain: epoch  9, batch     8 | loss: 4.0122414Losses:  4.002106189727783 0.07188788056373596
CurrentTrain: epoch  9, batch     9 | loss: 4.0739942Losses:  4.024308204650879 0.049856796860694885
CurrentTrain: epoch  9, batch    10 | loss: 4.0741649Losses:  4.00421142578125 0.08590483665466309
CurrentTrain: epoch  9, batch    11 | loss: 4.0901165Losses:  3.9889206886291504 0.06699185818433762
CurrentTrain: epoch  9, batch    12 | loss: 4.0559125Losses:  3.9671261310577393 0.09105432033538818
CurrentTrain: epoch  9, batch    13 | loss: 4.0581803Losses:  3.9900574684143066 0.07342354953289032
CurrentTrain: epoch  9, batch    14 | loss: 4.0634809Losses:  3.986750364303589 0.07108930498361588
CurrentTrain: epoch  9, batch    15 | loss: 4.0578399Losses:  3.8824410438537598 0.048166826367378235
CurrentTrain: epoch  9, batch    16 | loss: 3.9306078Losses:  3.978752374649048 0.07715917378664017
CurrentTrain: epoch  9, batch    17 | loss: 4.0559115Losses:  3.9797823429107666 0.10761771351099014
CurrentTrain: epoch  9, batch    18 | loss: 4.0874000Losses:  3.9708900451660156 0.13370025157928467
CurrentTrain: epoch  9, batch    19 | loss: 4.1045904Losses:  3.9425511360168457 0.07104197144508362
CurrentTrain: epoch  9, batch    20 | loss: 4.0135932Losses:  3.966486930847168 0.09872899204492569
CurrentTrain: epoch  9, batch    21 | loss: 4.0652161Losses:  3.9736721515655518 0.06964904069900513
CurrentTrain: epoch  9, batch    22 | loss: 4.0433211Losses:  4.023221015930176 0.11195353418588638
CurrentTrain: epoch  9, batch    23 | loss: 4.1351748Losses:  3.910999298095703 0.06663164496421814
CurrentTrain: epoch  9, batch    24 | loss: 3.9776309Losses:  3.9540462493896484 0.10765059292316437
CurrentTrain: epoch  9, batch    25 | loss: 4.0616970Losses:  3.9619884490966797 0.05308467522263527
CurrentTrain: epoch  9, batch    26 | loss: 4.0150733Losses:  3.9867231845855713 0.08295933902263641
CurrentTrain: epoch  9, batch    27 | loss: 4.0696826Losses:  3.953510284423828 0.10405170172452927
CurrentTrain: epoch  9, batch    28 | loss: 4.0575619Losses:  3.9406051635742188 0.09798357635736465
CurrentTrain: epoch  9, batch    29 | loss: 4.0385885Losses:  3.9932916164398193 0.0880064070224762
CurrentTrain: epoch  9, batch    30 | loss: 4.0812979Losses:  3.973456382751465 0.05498766154050827
CurrentTrain: epoch  9, batch    31 | loss: 4.0284438Losses:  3.950479030609131 0.1175859272480011
CurrentTrain: epoch  9, batch    32 | loss: 4.0680652Losses:  3.9869730472564697 0.06488042324781418
CurrentTrain: epoch  9, batch    33 | loss: 4.0518537Losses:  4.0304412841796875 0.06597983837127686
CurrentTrain: epoch  9, batch    34 | loss: 4.0964212Losses:  3.9741814136505127 0.08443007618188858
CurrentTrain: epoch  9, batch    35 | loss: 4.0586114Losses:  3.965959072113037 0.07101312279701233
CurrentTrain: epoch  9, batch    36 | loss: 4.0369720Losses:  4.017400741577148 0.038360122591257095
CurrentTrain: epoch  9, batch    37 | loss: 4.0557609Losses:  3.959287405014038 0.051194772124290466
CurrentTrain: epoch  9, batch    38 | loss: 4.0104823Losses:  3.8822264671325684 0.04200514405965805
CurrentTrain: epoch  9, batch    39 | loss: 3.9242315Losses:  4.046545028686523 0.04186735302209854
CurrentTrain: epoch  9, batch    40 | loss: 4.0884123Losses:  3.966912269592285 0.09278072416782379
CurrentTrain: epoch  9, batch    41 | loss: 4.0596929Losses:  3.9714856147766113 0.06558915972709656
CurrentTrain: epoch  9, batch    42 | loss: 4.0370746Losses:  3.908971071243286 0.09648725390434265
CurrentTrain: epoch  9, batch    43 | loss: 4.0054584Losses:  3.960677146911621 0.06019680202007294
CurrentTrain: epoch  9, batch    44 | loss: 4.0208740Losses:  4.050906181335449 0.08682946860790253
CurrentTrain: epoch  9, batch    45 | loss: 4.1377358Losses:  3.94095778465271 0.0791342556476593
CurrentTrain: epoch  9, batch    46 | loss: 4.0200920Losses:  3.9327712059020996 0.0629621148109436
CurrentTrain: epoch  9, batch    47 | loss: 3.9957333Losses:  3.9263830184936523 0.03166643902659416
CurrentTrain: epoch  9, batch    48 | loss: 3.9580495Losses:  3.978203296661377 0.0832681804895401
CurrentTrain: epoch  9, batch    49 | loss: 4.0614715Losses:  4.022960662841797 0.05873393639922142
CurrentTrain: epoch  9, batch    50 | loss: 4.0816946Losses:  3.9971892833709717 0.06843094527721405
CurrentTrain: epoch  9, batch    51 | loss: 4.0656204Losses:  3.9812278747558594 0.08792418986558914
CurrentTrain: epoch  9, batch    52 | loss: 4.0691519Losses:  3.984175682067871 0.09068364650011063
CurrentTrain: epoch  9, batch    53 | loss: 4.0748591Losses:  3.963793992996216 0.05342351645231247
CurrentTrain: epoch  9, batch    54 | loss: 4.0172176Losses:  4.0295915603637695 0.08596064150333405
CurrentTrain: epoch  9, batch    55 | loss: 4.1155524Losses:  3.9745984077453613 0.094906747341156
CurrentTrain: epoch  9, batch    56 | loss: 4.0695052Losses:  3.9642648696899414 0.08283449709415436
CurrentTrain: epoch  9, batch    57 | loss: 4.0470996Losses:  3.9893741607666016 0.10274465382099152
CurrentTrain: epoch  9, batch    58 | loss: 4.0921187Losses:  3.9503846168518066 0.07052650302648544
CurrentTrain: epoch  9, batch    59 | loss: 4.0209112Losses:  3.9231486320495605 0.0730418860912323
CurrentTrain: epoch  9, batch    60 | loss: 3.9961905Losses:  4.009796142578125 0.08026362210512161
CurrentTrain: epoch  9, batch    61 | loss: 4.0900598Losses:  3.9818758964538574 0.0946892499923706
CurrentTrain: epoch  9, batch    62 | loss: 4.0765653Losses:  3.9398157596588135 0.06457021832466125
CurrentTrain: epoch  9, batch    63 | loss: 4.0043859Losses:  3.9335741996765137 0.08576713502407074
CurrentTrain: epoch  9, batch    64 | loss: 4.0193415Losses:  3.980827808380127 0.06437590718269348
CurrentTrain: epoch  9, batch    65 | loss: 4.0452037Losses:  3.9577624797821045 0.036575138568878174
CurrentTrain: epoch  9, batch    66 | loss: 3.9943376Losses:  3.9747061729431152 0.058577150106430054
CurrentTrain: epoch  9, batch    67 | loss: 4.0332832Losses:  3.9812819957733154 0.061005495488643646
CurrentTrain: epoch  9, batch    68 | loss: 4.0422873Losses:  3.9366936683654785 0.07314915210008621
CurrentTrain: epoch  9, batch    69 | loss: 4.0098429Losses:  4.003233909606934 0.1125229001045227
CurrentTrain: epoch  9, batch    70 | loss: 4.1157570Losses:  3.948833703994751 0.09386104345321655
CurrentTrain: epoch  9, batch    71 | loss: 4.0426946Losses:  3.9611458778381348 0.057974208146333694
CurrentTrain: epoch  9, batch    72 | loss: 4.0191202Losses:  3.962520122528076 0.048735618591308594
CurrentTrain: epoch  9, batch    73 | loss: 4.0112557Losses:  3.9715449810028076 0.07091665267944336
CurrentTrain: epoch  9, batch    74 | loss: 4.0424614Losses:  3.9645326137542725 0.03862292319536209
CurrentTrain: epoch  9, batch    75 | loss: 4.0031557Losses:  3.922114133834839 0.044614896178245544
CurrentTrain: epoch  9, batch    76 | loss: 3.9667289Losses:  3.9887681007385254 0.07259204238653183
CurrentTrain: epoch  9, batch    77 | loss: 4.0613604Losses:  4.007521152496338 0.0634651780128479
CurrentTrain: epoch  9, batch    78 | loss: 4.0709863Losses:  3.9975852966308594 0.08698266744613647
CurrentTrain: epoch  9, batch    79 | loss: 4.0845680Losses:  4.004833221435547 0.08360424637794495
CurrentTrain: epoch  9, batch    80 | loss: 4.0884376Losses:  3.9914183616638184 0.05387567728757858
CurrentTrain: epoch  9, batch    81 | loss: 4.0452938Losses:  4.021024703979492 0.053538933396339417
CurrentTrain: epoch  9, batch    82 | loss: 4.0745635Losses:  4.017966270446777 0.04032890498638153
CurrentTrain: epoch  9, batch    83 | loss: 4.0582952Losses:  3.9763827323913574 0.05356740951538086
CurrentTrain: epoch  9, batch    84 | loss: 4.0299501Losses:  3.9371790885925293 0.04399367421865463
CurrentTrain: epoch  9, batch    85 | loss: 3.9811728Losses:  3.953502655029297 0.05530516803264618
CurrentTrain: epoch  9, batch    86 | loss: 4.0088077Losses:  3.9591124057769775 0.06040186062455177
CurrentTrain: epoch  9, batch    87 | loss: 4.0195141Losses:  3.967155694961548 0.04441944882273674
CurrentTrain: epoch  9, batch    88 | loss: 4.0115752Losses:  4.074591636657715 0.0476190522313118
CurrentTrain: epoch  9, batch    89 | loss: 4.1222105Losses:  3.937255859375 0.12224815040826797
CurrentTrain: epoch  9, batch    90 | loss: 4.0595040Losses:  3.970278739929199 0.06542672216892242
CurrentTrain: epoch  9, batch    91 | loss: 4.0357056Losses:  3.980346441268921 0.05261752009391785
CurrentTrain: epoch  9, batch    92 | loss: 4.0329638Losses:  3.998227596282959 0.0560549721121788
CurrentTrain: epoch  9, batch    93 | loss: 4.0542827Losses:  3.9727296829223633 0.07746067643165588
CurrentTrain: epoch  9, batch    94 | loss: 4.0501904Losses:  3.9867658615112305 0.07268673926591873
CurrentTrain: epoch  9, batch    95 | loss: 4.0594525Losses:  3.979417324066162 0.07518354058265686
CurrentTrain: epoch  9, batch    96 | loss: 4.0546007Losses:  3.9684805870056152 0.061418935656547546
CurrentTrain: epoch  9, batch    97 | loss: 4.0298996Losses:  3.992584228515625 0.07711102068424225
CurrentTrain: epoch  9, batch    98 | loss: 4.0696955Losses:  3.987179756164551 0.11064840853214264
CurrentTrain: epoch  9, batch    99 | loss: 4.0978284Losses:  3.93963623046875 0.07482007145881653
CurrentTrain: epoch  9, batch   100 | loss: 4.0144563Losses:  3.9622907638549805 0.11113904416561127
CurrentTrain: epoch  9, batch   101 | loss: 4.0734296Losses:  3.9497766494750977 0.07601148635149002
CurrentTrain: epoch  9, batch   102 | loss: 4.0257883Losses:  3.9984982013702393 0.09872037172317505
CurrentTrain: epoch  9, batch   103 | loss: 4.0972185Losses:  3.9686975479125977 0.058851003646850586
CurrentTrain: epoch  9, batch   104 | loss: 4.0275488Losses:  3.9640402793884277 0.05195149779319763
CurrentTrain: epoch  9, batch   105 | loss: 4.0159917Losses:  4.011355400085449 0.07682685554027557
CurrentTrain: epoch  9, batch   106 | loss: 4.0881824Losses:  3.9403128623962402 0.07125131040811539
CurrentTrain: epoch  9, batch   107 | loss: 4.0115643Losses:  3.972377300262451 0.08415745198726654
CurrentTrain: epoch  9, batch   108 | loss: 4.0565348Losses:  4.0064826011657715 0.07459549605846405
CurrentTrain: epoch  9, batch   109 | loss: 4.0810781Losses:  3.975198984146118 0.08079423010349274
CurrentTrain: epoch  9, batch   110 | loss: 4.0559931Losses:  4.0067033767700195 0.05270367115736008
CurrentTrain: epoch  9, batch   111 | loss: 4.0594072Losses:  3.9580061435699463 0.10108895599842072
CurrentTrain: epoch  9, batch   112 | loss: 4.0590949Losses:  3.9758172035217285 0.09928207844495773
CurrentTrain: epoch  9, batch   113 | loss: 4.0750995Losses:  3.967953681945801 0.08616357296705246
CurrentTrain: epoch  9, batch   114 | loss: 4.0541172Losses:  3.9130048751831055 0.10992904752492905
CurrentTrain: epoch  9, batch   115 | loss: 4.0229340Losses:  3.9835119247436523 0.08208592236042023
CurrentTrain: epoch  9, batch   116 | loss: 4.0655980Losses:  3.9745688438415527 0.06743590533733368
CurrentTrain: epoch  9, batch   117 | loss: 4.0420046Losses:  3.947317600250244 0.07423089444637299
CurrentTrain: epoch  9, batch   118 | loss: 4.0215483Losses:  3.939211845397949 0.09908545017242432
CurrentTrain: epoch  9, batch   119 | loss: 4.0382972Losses:  3.915799617767334 0.05026952922344208
CurrentTrain: epoch  9, batch   120 | loss: 3.9660692Losses:  3.9684860706329346 0.09001148492097855
CurrentTrain: epoch  9, batch   121 | loss: 4.0584974Losses:  3.9616870880126953 0.10523500293493271
CurrentTrain: epoch  9, batch   122 | loss: 4.0669222Losses:  3.972743511199951 0.04218272119760513
CurrentTrain: epoch  9, batch   123 | loss: 4.0149264Losses:  3.956515073776245 0.04358667880296707
CurrentTrain: epoch  9, batch   124 | loss: 4.0001016
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  6.6548662185668945 1.399888277053833
CurrentTrain: epoch  0, batch     0 | loss: 8.0547543Losses:  8.331888198852539 0.8752075433731079
CurrentTrain: epoch  0, batch     1 | loss: 9.2070961Losses:  6.501536846160889 0.8944510817527771
CurrentTrain: epoch  0, batch     2 | loss: 7.3959880Losses:  6.439985275268555 0.9945797920227051
CurrentTrain: epoch  0, batch     3 | loss: 7.4345651Losses:  6.814428329467773 0.7495301961898804
CurrentTrain: epoch  0, batch     4 | loss: 7.5639586Losses:  6.531407356262207 1.2212624549865723
CurrentTrain: epoch  0, batch     5 | loss: 7.7526698Losses:  8.768489837646484 1.1920930376163597e-07
CurrentTrain: epoch  0, batch     6 | loss: 8.7684898Losses:  6.640927314758301 1.07291579246521
CurrentTrain: epoch  1, batch     0 | loss: 7.7138433Losses:  5.786340713500977 1.0428626537322998
CurrentTrain: epoch  1, batch     1 | loss: 6.8292036Losses:  6.1484551429748535 0.9130501747131348
CurrentTrain: epoch  1, batch     2 | loss: 7.0615053Losses:  7.118059158325195 0.9815255403518677
CurrentTrain: epoch  1, batch     3 | loss: 8.0995846Losses:  5.4199442863464355 1.0304332971572876
CurrentTrain: epoch  1, batch     4 | loss: 6.4503775Losses:  5.308206081390381 0.9017542600631714
CurrentTrain: epoch  1, batch     5 | loss: 6.2099605Losses:  3.3489465713500977 0.38990074396133423
CurrentTrain: epoch  1, batch     6 | loss: 3.7388473Losses:  5.897392749786377 0.9692710638046265
CurrentTrain: epoch  2, batch     0 | loss: 6.8666639Losses:  4.996654510498047 0.9141538143157959
CurrentTrain: epoch  2, batch     1 | loss: 5.9108086Losses:  4.202156066894531 0.7089483141899109
CurrentTrain: epoch  2, batch     2 | loss: 4.9111042Losses:  4.613886833190918 0.7669374942779541
CurrentTrain: epoch  2, batch     3 | loss: 5.3808241Losses:  4.824856281280518 0.7738298177719116
CurrentTrain: epoch  2, batch     4 | loss: 5.5986862Losses:  3.1509342193603516 0.6670079231262207
CurrentTrain: epoch  2, batch     5 | loss: 3.8179421Losses:  5.166143417358398 0.11483979225158691
CurrentTrain: epoch  2, batch     6 | loss: 5.2809830Losses:  3.4684696197509766 0.9131383895874023
CurrentTrain: epoch  3, batch     0 | loss: 4.3816080Losses:  5.36672830581665 0.9437967538833618
CurrentTrain: epoch  3, batch     1 | loss: 6.3105249Losses:  3.6985058784484863 0.8229418992996216
CurrentTrain: epoch  3, batch     2 | loss: 4.5214477Losses:  3.1374454498291016 0.8219627141952515
CurrentTrain: epoch  3, batch     3 | loss: 3.9594083Losses:  3.322391986846924 0.9304131269454956
CurrentTrain: epoch  3, batch     4 | loss: 4.2528052Losses:  3.9362478256225586 0.9571278691291809
CurrentTrain: epoch  3, batch     5 | loss: 4.8933759Losses:  2.445760726928711 0.3125408887863159
CurrentTrain: epoch  3, batch     6 | loss: 2.7583017Losses:  2.8886075019836426 0.8092982769012451
CurrentTrain: epoch  4, batch     0 | loss: 3.6979058Losses:  3.5093812942504883 0.5190471410751343
CurrentTrain: epoch  4, batch     1 | loss: 4.0284286Losses:  2.8786888122558594 0.5737693905830383
CurrentTrain: epoch  4, batch     2 | loss: 3.4524581Losses:  3.0184121131896973 0.5385206937789917
CurrentTrain: epoch  4, batch     3 | loss: 3.5569329Losses:  4.226101875305176 0.5800904631614685
CurrentTrain: epoch  4, batch     4 | loss: 4.8061924Losses:  3.5672802925109863 0.605053186416626
CurrentTrain: epoch  4, batch     5 | loss: 4.1723337Losses:  3.6081323623657227 0.08809757232666016
CurrentTrain: epoch  4, batch     6 | loss: 3.6962299Losses:  3.064328670501709 0.538131594657898
CurrentTrain: epoch  5, batch     0 | loss: 3.6024604Losses:  2.8555750846862793 0.8000226020812988
CurrentTrain: epoch  5, batch     1 | loss: 3.6555977Losses:  2.6464037895202637 0.483895480632782
CurrentTrain: epoch  5, batch     2 | loss: 3.1302993Losses:  2.6421456336975098 0.9753953814506531
CurrentTrain: epoch  5, batch     3 | loss: 3.6175411Losses:  2.6945438385009766 0.7493957877159119
CurrentTrain: epoch  5, batch     4 | loss: 3.4439397Losses:  2.614593029022217 0.7897400856018066
CurrentTrain: epoch  5, batch     5 | loss: 3.4043331Losses:  5.193079948425293 0.07919981330633163
CurrentTrain: epoch  5, batch     6 | loss: 5.2722797Losses:  2.6808676719665527 0.7186654806137085
CurrentTrain: epoch  6, batch     0 | loss: 3.3995333Losses:  2.9131219387054443 0.5347698926925659
CurrentTrain: epoch  6, batch     1 | loss: 3.4478917Losses:  2.162914991378784 0.44785237312316895
CurrentTrain: epoch  6, batch     2 | loss: 2.6107674Losses:  2.830833911895752 0.5277471542358398
CurrentTrain: epoch  6, batch     3 | loss: 3.3585811Losses:  2.2299702167510986 0.538186252117157
CurrentTrain: epoch  6, batch     4 | loss: 2.7681565Losses:  2.6684908866882324 0.4612874984741211
CurrentTrain: epoch  6, batch     5 | loss: 3.1297784Losses:  2.4622974395751953 8.94069742685133e-08
CurrentTrain: epoch  6, batch     6 | loss: 2.4622974Losses:  2.4486207962036133 0.6383689641952515
CurrentTrain: epoch  7, batch     0 | loss: 3.0869899Losses:  2.466616630554199 0.5739644765853882
CurrentTrain: epoch  7, batch     1 | loss: 3.0405812Losses:  2.295245885848999 0.7582987546920776
CurrentTrain: epoch  7, batch     2 | loss: 3.0535445Losses:  2.1243574619293213 0.5290495753288269
CurrentTrain: epoch  7, batch     3 | loss: 2.6534071Losses:  1.9843071699142456 0.4800415635108948
CurrentTrain: epoch  7, batch     4 | loss: 2.4643488Losses:  2.1677045822143555 0.6131189465522766
CurrentTrain: epoch  7, batch     5 | loss: 2.7808235Losses:  2.025808811187744 0.0
CurrentTrain: epoch  7, batch     6 | loss: 2.0258088Losses:  2.053238868713379 0.6359107494354248
CurrentTrain: epoch  8, batch     0 | loss: 2.6891496Losses:  2.269404411315918 0.7259795665740967
CurrentTrain: epoch  8, batch     1 | loss: 2.9953840Losses:  2.154284715652466 0.34578222036361694
CurrentTrain: epoch  8, batch     2 | loss: 2.5000670Losses:  2.1393723487854004 0.5879637002944946
CurrentTrain: epoch  8, batch     3 | loss: 2.7273359Losses:  2.152590036392212 0.6614828109741211
CurrentTrain: epoch  8, batch     4 | loss: 2.8140728Losses:  2.220639705657959 0.43423986434936523
CurrentTrain: epoch  8, batch     5 | loss: 2.6548796Losses:  1.7955925464630127 0.051536787301301956
CurrentTrain: epoch  8, batch     6 | loss: 1.8471293Losses:  1.9460418224334717 0.592942476272583
CurrentTrain: epoch  9, batch     0 | loss: 2.5389843Losses:  1.9262681007385254 0.569293200969696
CurrentTrain: epoch  9, batch     1 | loss: 2.4955614Losses:  2.2206151485443115 0.5180956125259399
CurrentTrain: epoch  9, batch     2 | loss: 2.7387109Losses:  1.9427152872085571 0.393585741519928
CurrentTrain: epoch  9, batch     3 | loss: 2.3363011Losses:  1.8976049423217773 0.649872362613678
CurrentTrain: epoch  9, batch     4 | loss: 2.5474772Losses:  1.8454389572143555 0.42294639348983765
CurrentTrain: epoch  9, batch     5 | loss: 2.2683854Losses:  1.7598060369491577 0.05248459801077843
CurrentTrain: epoch  9, batch     6 | loss: 1.8122907
Losses:  2.800403594970703 0.8414897918701172
MemoryTrain:  epoch  0, batch     0 | loss: 3.6418934Losses:  2.6638660430908203 0.5974911451339722
MemoryTrain:  epoch  0, batch     1 | loss: 3.2613573Losses:  1.0301886796951294 0.08744461834430695
MemoryTrain:  epoch  0, batch     2 | loss: 1.1176333Losses:  2.196821689605713 0.19127364456653595
MemoryTrain:  epoch  1, batch     0 | loss: 2.3880954Losses:  2.6087865829467773 0.600700318813324
MemoryTrain:  epoch  1, batch     1 | loss: 3.2094870Losses:  2.888735771179199 0.6028218269348145
MemoryTrain:  epoch  1, batch     2 | loss: 3.4915576Losses:  2.2773663997650146 0.544914722442627
MemoryTrain:  epoch  2, batch     0 | loss: 2.8222811Losses:  1.1826647520065308 0.4569545388221741
MemoryTrain:  epoch  2, batch     1 | loss: 1.6396194Losses:  0.11025891453027725 0.24648118019104004
MemoryTrain:  epoch  2, batch     2 | loss: 0.3567401Losses:  0.07259946316480637 0.3691790699958801
MemoryTrain:  epoch  3, batch     0 | loss: 0.4417785Losses:  1.7655069828033447 0.7311334609985352
MemoryTrain:  epoch  3, batch     1 | loss: 2.4966404Losses:  2.007641553878784 0.3106187582015991
MemoryTrain:  epoch  3, batch     2 | loss: 2.3182602Losses:  1.426371693611145 0.7293753623962402
MemoryTrain:  epoch  4, batch     0 | loss: 2.1557469Losses:  1.1546258926391602 0.21543613076210022
MemoryTrain:  epoch  4, batch     1 | loss: 1.3700620Losses:  0.012580715119838715 0.18331602215766907
MemoryTrain:  epoch  4, batch     2 | loss: 0.1958967Losses:  0.87705397605896 0.4852072596549988
MemoryTrain:  epoch  5, batch     0 | loss: 1.3622613Losses:  1.2060387134552002 0.6221107244491577
MemoryTrain:  epoch  5, batch     1 | loss: 1.8281494Losses:  0.052015528082847595 0.10569213330745697
MemoryTrain:  epoch  5, batch     2 | loss: 0.1577077Losses:  0.8016083240509033 0.43090853095054626
MemoryTrain:  epoch  6, batch     0 | loss: 1.2325169Losses:  0.8488050103187561 0.5022121667861938
MemoryTrain:  epoch  6, batch     1 | loss: 1.3510172Losses:  0.09278403222560883 0.22974568605422974
MemoryTrain:  epoch  6, batch     2 | loss: 0.3225297Losses:  0.7221404314041138 0.3233122229576111
MemoryTrain:  epoch  7, batch     0 | loss: 1.0454526Losses:  0.07023879140615463 0.6523282527923584
MemoryTrain:  epoch  7, batch     1 | loss: 0.7225670Losses:  1.1635854244232178 0.12574970722198486
MemoryTrain:  epoch  7, batch     2 | loss: 1.2893351Losses:  0.569765031337738 0.2245303988456726
MemoryTrain:  epoch  8, batch     0 | loss: 0.7942954Losses:  0.42149755358695984 0.5692965984344482
MemoryTrain:  epoch  8, batch     1 | loss: 0.9907942Losses:  0.027458392083644867 0.28722310066223145
MemoryTrain:  epoch  8, batch     2 | loss: 0.3146815Losses:  0.8298636674880981 0.311337947845459
MemoryTrain:  epoch  9, batch     0 | loss: 1.1412016Losses:  0.03889251500368118 0.6648614406585693
MemoryTrain:  epoch  9, batch     1 | loss: 0.7037539Losses:  0.704052746295929 0.08436503261327744
MemoryTrain:  epoch  9, batch     2 | loss: 0.7884178
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 68.89%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 65.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.53%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.45%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.46%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.66%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 93.84%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.67%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 93.42%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.34%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 92.95%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.72%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 92.27%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 92.07%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.79%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.59%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 91.32%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 91.28%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.95%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 90.77%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.59%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 90.14%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 89.88%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 89.31%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 88.70%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 88.36%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 88.02%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.69%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.18%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.87%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 86.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 86.01%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 85.78%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 85.25%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.98%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 84.70%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 84.38%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.94%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.74%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 82.22%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.76%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 81.42%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 81.14%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 81.98%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 82.41%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.50%   
cur_acc:  ['0.9454', '0.7093']
his_acc:  ['0.9454', '0.8250']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  6.992040157318115 1.3473464250564575
CurrentTrain: epoch  0, batch     0 | loss: 8.3393869Losses:  6.365205764770508 1.2532857656478882
CurrentTrain: epoch  0, batch     1 | loss: 7.6184916Losses:  5.713953018188477 1.3179080486297607
CurrentTrain: epoch  0, batch     2 | loss: 7.0318613Losses:  6.100920677185059 1.245805263519287
CurrentTrain: epoch  0, batch     3 | loss: 7.3467259Losses:  6.855920791625977 1.213148832321167
CurrentTrain: epoch  0, batch     4 | loss: 8.0690699Losses:  6.544201850891113 1.5929057598114014
CurrentTrain: epoch  0, batch     5 | loss: 8.1371078Losses:  7.929380416870117 0.3840634822845459
CurrentTrain: epoch  0, batch     6 | loss: 8.3134441Losses:  5.005288124084473 1.4903264045715332
CurrentTrain: epoch  1, batch     0 | loss: 6.4956145Losses:  6.068142414093018 1.2797565460205078
CurrentTrain: epoch  1, batch     1 | loss: 7.3478990Losses:  7.261281967163086 1.5886157751083374
CurrentTrain: epoch  1, batch     2 | loss: 8.8498974Losses:  6.34494686126709 1.4775900840759277
CurrentTrain: epoch  1, batch     3 | loss: 7.8225369Losses:  4.183690071105957 0.9537347555160522
CurrentTrain: epoch  1, batch     4 | loss: 5.1374249Losses:  4.829208850860596 1.2668113708496094
CurrentTrain: epoch  1, batch     5 | loss: 6.0960202Losses:  5.5995707511901855 0.2785145938396454
CurrentTrain: epoch  1, batch     6 | loss: 5.8780851Losses:  3.5588009357452393 0.8132138252258301
CurrentTrain: epoch  2, batch     0 | loss: 4.3720150Losses:  3.7529234886169434 0.922207772731781
CurrentTrain: epoch  2, batch     1 | loss: 4.6751313Losses:  4.415106773376465 1.1784815788269043
CurrentTrain: epoch  2, batch     2 | loss: 5.5935884Losses:  4.948092460632324 1.1808068752288818
CurrentTrain: epoch  2, batch     3 | loss: 6.1288996Losses:  4.797889232635498 1.428153157234192
CurrentTrain: epoch  2, batch     4 | loss: 6.2260423Losses:  6.885990142822266 1.222456932067871
CurrentTrain: epoch  2, batch     5 | loss: 8.1084471Losses:  2.4470877647399902 0.16989824175834656
CurrentTrain: epoch  2, batch     6 | loss: 2.6169860Losses:  4.453863143920898 1.0663199424743652
CurrentTrain: epoch  3, batch     0 | loss: 5.5201831Losses:  4.133248805999756 0.997391939163208
CurrentTrain: epoch  3, batch     1 | loss: 5.1306410Losses:  3.5881505012512207 1.047773838043213
CurrentTrain: epoch  3, batch     2 | loss: 4.6359243Losses:  4.140041828155518 1.0918989181518555
CurrentTrain: epoch  3, batch     3 | loss: 5.2319407Losses:  4.891005516052246 1.2852892875671387
CurrentTrain: epoch  3, batch     4 | loss: 6.1762948Losses:  4.619162559509277 1.2759885787963867
CurrentTrain: epoch  3, batch     5 | loss: 5.8951511Losses:  4.755810737609863 0.23671741783618927
CurrentTrain: epoch  3, batch     6 | loss: 4.9925280Losses:  4.637500762939453 0.8829158544540405
CurrentTrain: epoch  4, batch     0 | loss: 5.5204167Losses:  4.68281888961792 1.1428675651550293
CurrentTrain: epoch  4, batch     1 | loss: 5.8256865Losses:  3.686980724334717 1.0375374555587769
CurrentTrain: epoch  4, batch     2 | loss: 4.7245183Losses:  4.474609375 1.108208417892456
CurrentTrain: epoch  4, batch     3 | loss: 5.5828180Losses:  3.1146206855773926 0.8336759209632874
CurrentTrain: epoch  4, batch     4 | loss: 3.9482965Losses:  3.171461820602417 0.8756001591682434
CurrentTrain: epoch  4, batch     5 | loss: 4.0470619Losses:  3.273749589920044 0.33402514457702637
CurrentTrain: epoch  4, batch     6 | loss: 3.6077747Losses:  4.294986724853516 1.2112932205200195
CurrentTrain: epoch  5, batch     0 | loss: 5.5062799Losses:  4.181728363037109 0.9227020740509033
CurrentTrain: epoch  5, batch     1 | loss: 5.1044302Losses:  3.243739604949951 0.6656800508499146
CurrentTrain: epoch  5, batch     2 | loss: 3.9094195Losses:  3.3697659969329834 0.7497345805168152
CurrentTrain: epoch  5, batch     3 | loss: 4.1195006Losses:  2.915764808654785 0.9856542348861694
CurrentTrain: epoch  5, batch     4 | loss: 3.9014192Losses:  3.4316275119781494 0.6643234491348267
CurrentTrain: epoch  5, batch     5 | loss: 4.0959511Losses:  6.684034824371338 0.584753692150116
CurrentTrain: epoch  5, batch     6 | loss: 7.2687883Losses:  3.761685371398926 1.1539790630340576
CurrentTrain: epoch  6, batch     0 | loss: 4.9156647Losses:  4.027859687805176 0.9745115041732788
CurrentTrain: epoch  6, batch     1 | loss: 5.0023713Losses:  3.410896062850952 0.957556426525116
CurrentTrain: epoch  6, batch     2 | loss: 4.3684525Losses:  3.7740068435668945 0.7531416416168213
CurrentTrain: epoch  6, batch     3 | loss: 4.5271482Losses:  2.936464786529541 0.5524885654449463
CurrentTrain: epoch  6, batch     4 | loss: 3.4889534Losses:  2.9497103691101074 0.7376354932785034
CurrentTrain: epoch  6, batch     5 | loss: 3.6873460Losses:  2.268014907836914 0.1908055543899536
CurrentTrain: epoch  6, batch     6 | loss: 2.4588203Losses:  2.7774860858917236 0.6360650658607483
CurrentTrain: epoch  7, batch     0 | loss: 3.4135511Losses:  3.0887646675109863 0.8897191286087036
CurrentTrain: epoch  7, batch     1 | loss: 3.9784837Losses:  3.4017224311828613 0.6751518845558167
CurrentTrain: epoch  7, batch     2 | loss: 4.0768743Losses:  2.9324374198913574 0.8648364543914795
CurrentTrain: epoch  7, batch     3 | loss: 3.7972739Losses:  3.258181095123291 0.8220917582511902
CurrentTrain: epoch  7, batch     4 | loss: 4.0802727Losses:  3.042928695678711 0.7199506759643555
CurrentTrain: epoch  7, batch     5 | loss: 3.7628794Losses:  4.115933418273926 0.5031761527061462
CurrentTrain: epoch  7, batch     6 | loss: 4.6191096Losses:  2.7728419303894043 0.9064271450042725
CurrentTrain: epoch  8, batch     0 | loss: 3.6792691Losses:  3.599503517150879 0.7335882186889648
CurrentTrain: epoch  8, batch     1 | loss: 4.3330917Losses:  2.730620861053467 0.7035800218582153
CurrentTrain: epoch  8, batch     2 | loss: 3.4342008Losses:  3.054746389389038 0.7908639907836914
CurrentTrain: epoch  8, batch     3 | loss: 3.8456104Losses:  2.2479257583618164 0.5133156776428223
CurrentTrain: epoch  8, batch     4 | loss: 2.7612414Losses:  3.5522682666778564 0.6563260555267334
CurrentTrain: epoch  8, batch     5 | loss: 4.2085943Losses:  2.082024097442627 0.059965185821056366
CurrentTrain: epoch  8, batch     6 | loss: 2.1419892Losses:  2.5336108207702637 0.7182440757751465
CurrentTrain: epoch  9, batch     0 | loss: 3.2518549Losses:  2.8154239654541016 0.6374098062515259
CurrentTrain: epoch  9, batch     1 | loss: 3.4528337Losses:  1.9755858182907104 0.5425898432731628
CurrentTrain: epoch  9, batch     2 | loss: 2.5181756Losses:  2.702195167541504 0.5746533870697021
CurrentTrain: epoch  9, batch     3 | loss: 3.2768486Losses:  2.5523157119750977 0.3545209765434265
CurrentTrain: epoch  9, batch     4 | loss: 2.9068367Losses:  3.188267707824707 0.4738283157348633
CurrentTrain: epoch  9, batch     5 | loss: 3.6620960Losses:  3.595304489135742 0.3943142294883728
CurrentTrain: epoch  9, batch     6 | loss: 3.9896188
Losses:  1.5089055299758911 0.5287104845046997
MemoryTrain:  epoch  0, batch     0 | loss: 2.0376160Losses:  1.247153878211975 0.6506022214889526
MemoryTrain:  epoch  0, batch     1 | loss: 1.8977561Losses:  0.8291531205177307 0.9385303258895874
MemoryTrain:  epoch  0, batch     2 | loss: 1.7676835Losses:  0.36005282402038574 0.2930822968482971
MemoryTrain:  epoch  0, batch     3 | loss: 0.6531351Losses:  1.3398582935333252 0.4846992492675781
MemoryTrain:  epoch  1, batch     0 | loss: 1.8245575Losses:  0.5135568380355835 0.827684760093689
MemoryTrain:  epoch  1, batch     1 | loss: 1.3412416Losses:  1.1524114608764648 0.46580344438552856
MemoryTrain:  epoch  1, batch     2 | loss: 1.6182148Losses:  1.4843952655792236 0.6103168725967407
MemoryTrain:  epoch  1, batch     3 | loss: 2.0947123Losses:  0.2808021306991577 0.3698652386665344
MemoryTrain:  epoch  2, batch     0 | loss: 0.6506674Losses:  0.48609358072280884 0.6132954359054565
MemoryTrain:  epoch  2, batch     1 | loss: 1.0993891Losses:  0.5357654094696045 0.5047414898872375
MemoryTrain:  epoch  2, batch     2 | loss: 1.0405068Losses:  0.4978533387184143 0.4528718888759613
MemoryTrain:  epoch  2, batch     3 | loss: 0.9507252Losses:  0.39404138922691345 0.4910641312599182
MemoryTrain:  epoch  3, batch     0 | loss: 0.8851055Losses:  0.12146322429180145 0.7499470710754395
MemoryTrain:  epoch  3, batch     1 | loss: 0.8714103Losses:  0.13774821162223816 0.6930617094039917
MemoryTrain:  epoch  3, batch     2 | loss: 0.8308100Losses:  0.050695281475782394 0.3260568380355835
MemoryTrain:  epoch  3, batch     3 | loss: 0.3767521Losses:  0.07823820412158966 0.5101839303970337
MemoryTrain:  epoch  4, batch     0 | loss: 0.5884221Losses:  0.14531227946281433 0.688389003276825
MemoryTrain:  epoch  4, batch     1 | loss: 0.8337013Losses:  0.42848312854766846 0.5953447222709656
MemoryTrain:  epoch  4, batch     2 | loss: 1.0238278Losses:  0.0455256812274456 0.24472609162330627
MemoryTrain:  epoch  4, batch     3 | loss: 0.2902518Losses:  0.06956816464662552 0.5675911903381348
MemoryTrain:  epoch  5, batch     0 | loss: 0.6371593Losses:  0.2871010899543762 0.5019449591636658
MemoryTrain:  epoch  5, batch     1 | loss: 0.7890460Losses:  0.07686346024274826 0.3181194067001343
MemoryTrain:  epoch  5, batch     2 | loss: 0.3949829Losses:  0.08847765624523163 0.3956279158592224
MemoryTrain:  epoch  5, batch     3 | loss: 0.4841056Losses:  0.04560393840074539 0.4534032940864563
MemoryTrain:  epoch  6, batch     0 | loss: 0.4990072Losses:  0.08051697164773941 0.4456890821456909
MemoryTrain:  epoch  6, batch     1 | loss: 0.5262061Losses:  0.08942417800426483 0.5142595171928406
MemoryTrain:  epoch  6, batch     2 | loss: 0.6036837Losses:  0.039469316601753235 0.48001766204833984
MemoryTrain:  epoch  6, batch     3 | loss: 0.5194870Losses:  0.03250360116362572 0.4499566853046417
MemoryTrain:  epoch  7, batch     0 | loss: 0.4824603Losses:  0.058989837765693665 0.35044434666633606
MemoryTrain:  epoch  7, batch     1 | loss: 0.4094342Losses:  0.039092183113098145 0.595660388469696
MemoryTrain:  epoch  7, batch     2 | loss: 0.6347526Losses:  0.026653876528143883 0.2975223958492279
MemoryTrain:  epoch  7, batch     3 | loss: 0.3241763Losses:  0.04829685762524605 0.41189032793045044
MemoryTrain:  epoch  8, batch     0 | loss: 0.4601872Losses:  0.039847481995821 0.7154421210289001
MemoryTrain:  epoch  8, batch     1 | loss: 0.7552896Losses:  0.03448251634836197 0.3810356557369232
MemoryTrain:  epoch  8, batch     2 | loss: 0.4155182Losses:  0.025014247745275497 0.26083940267562866
MemoryTrain:  epoch  8, batch     3 | loss: 0.2858537Losses:  0.02308657020330429 0.45683151483535767
MemoryTrain:  epoch  9, batch     0 | loss: 0.4799181Losses:  0.05635170638561249 0.6492738723754883
MemoryTrain:  epoch  9, batch     1 | loss: 0.7056256Losses:  0.03848689794540405 0.29153135418891907
MemoryTrain:  epoch  9, batch     2 | loss: 0.3300183Losses:  0.040743641555309296 0.21786287426948547
MemoryTrain:  epoch  9, batch     3 | loss: 0.2586065
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 74.16%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.31%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 93.35%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 93.26%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.57%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.66%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.67%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.59%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.51%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 93.35%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 93.20%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 92.81%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 92.68%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 92.32%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 92.26%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 91.91%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.79%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 91.38%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 91.15%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 90.90%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.93%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.76%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 90.39%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 90.03%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 89.74%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 89.26%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 88.98%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 88.65%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 88.38%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 87.75%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 87.01%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 86.66%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 86.37%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 86.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.63%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 85.01%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 84.46%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 83.86%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 83.39%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 82.98%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 82.69%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.84%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 83.63%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 83.87%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 83.71%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 83.54%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 83.41%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 83.40%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.77%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 84.01%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 83.50%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 83.08%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 82.67%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 82.39%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 82.12%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 81.77%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 82.04%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 81.54%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 81.09%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 80.76%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 80.36%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 79.93%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.99%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.12%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 81.03%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 81.03%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 81.03%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 81.03%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.14%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 81.07%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 80.72%   
cur_acc:  ['0.9454', '0.7093', '0.7431']
his_acc:  ['0.9454', '0.8250', '0.8072']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  5.48387336730957 1.4156029224395752
CurrentTrain: epoch  0, batch     0 | loss: 6.8994761Losses:  5.339095115661621 1.1979267597198486
CurrentTrain: epoch  0, batch     1 | loss: 6.5370216Losses:  4.643572807312012 1.3152248859405518
CurrentTrain: epoch  0, batch     2 | loss: 5.9587975Losses:  3.519742965698242 0.961441695690155
CurrentTrain: epoch  0, batch     3 | loss: 4.4811845Losses:  4.544623374938965 1.5472737550735474
CurrentTrain: epoch  0, batch     4 | loss: 6.0918970Losses:  5.13627290725708 1.3054397106170654
CurrentTrain: epoch  0, batch     5 | loss: 6.4417124Losses:  3.492648124694824 0.6178916096687317
CurrentTrain: epoch  0, batch     6 | loss: 4.1105399Losses:  3.873392343521118 1.393107295036316
CurrentTrain: epoch  1, batch     0 | loss: 5.2664995Losses:  4.280921936035156 0.9219263792037964
CurrentTrain: epoch  1, batch     1 | loss: 5.2028484Losses:  3.2240991592407227 1.1139086484909058
CurrentTrain: epoch  1, batch     2 | loss: 4.3380079Losses:  3.4929757118225098 1.1342257261276245
CurrentTrain: epoch  1, batch     3 | loss: 4.6272016Losses:  3.587448835372925 1.5238616466522217
CurrentTrain: epoch  1, batch     4 | loss: 5.1113105Losses:  3.05265474319458 1.298620343208313
CurrentTrain: epoch  1, batch     5 | loss: 4.3512750Losses:  2.8841023445129395 0.21541665494441986
CurrentTrain: epoch  1, batch     6 | loss: 3.0995190Losses:  4.562816143035889 1.3276095390319824
CurrentTrain: epoch  2, batch     0 | loss: 5.8904257Losses:  4.160199165344238 1.1488182544708252
CurrentTrain: epoch  2, batch     1 | loss: 5.3090172Losses:  3.414992094039917 1.1019785404205322
CurrentTrain: epoch  2, batch     2 | loss: 4.5169706Losses:  2.3720932006835938 0.9760428071022034
CurrentTrain: epoch  2, batch     3 | loss: 3.3481359Losses:  2.53351092338562 0.9169248938560486
CurrentTrain: epoch  2, batch     4 | loss: 3.4504359Losses:  2.1887786388397217 0.7317295074462891
CurrentTrain: epoch  2, batch     5 | loss: 2.9205081Losses:  1.9759598970413208 0.1821785569190979
CurrentTrain: epoch  2, batch     6 | loss: 2.1581385Losses:  2.566396713256836 0.943027675151825
CurrentTrain: epoch  3, batch     0 | loss: 3.5094244Losses:  3.1374690532684326 0.8108425140380859
CurrentTrain: epoch  3, batch     1 | loss: 3.9483116Losses:  2.857050895690918 0.8529765605926514
CurrentTrain: epoch  3, batch     2 | loss: 3.7100275Losses:  3.3109774589538574 0.8685392737388611
CurrentTrain: epoch  3, batch     3 | loss: 4.1795168Losses:  2.6777358055114746 0.8369453549385071
CurrentTrain: epoch  3, batch     4 | loss: 3.5146811Losses:  3.3147263526916504 0.9533393383026123
CurrentTrain: epoch  3, batch     5 | loss: 4.2680655Losses:  2.020399332046509 0.10482141375541687
CurrentTrain: epoch  3, batch     6 | loss: 2.1252208Losses:  2.2536332607269287 0.6620422601699829
CurrentTrain: epoch  4, batch     0 | loss: 2.9156756Losses:  3.2210025787353516 0.5911149382591248
CurrentTrain: epoch  4, batch     1 | loss: 3.8121176Losses:  2.497596263885498 0.9760980606079102
CurrentTrain: epoch  4, batch     2 | loss: 3.4736943Losses:  2.6572036743164062 0.8563934564590454
CurrentTrain: epoch  4, batch     3 | loss: 3.5135970Losses:  2.6882083415985107 0.6694839000701904
CurrentTrain: epoch  4, batch     4 | loss: 3.3576922Losses:  3.244779109954834 0.8727713823318481
CurrentTrain: epoch  4, batch     5 | loss: 4.1175504Losses:  2.1485376358032227 0.14473596215248108
CurrentTrain: epoch  4, batch     6 | loss: 2.2932737Losses:  2.9639816284179688 0.4566522538661957
CurrentTrain: epoch  5, batch     0 | loss: 3.4206338Losses:  2.2929396629333496 0.8534548282623291
CurrentTrain: epoch  5, batch     1 | loss: 3.1463945Losses:  3.291900157928467 0.8181670904159546
CurrentTrain: epoch  5, batch     2 | loss: 4.1100674Losses:  1.9714806079864502 0.5727512836456299
CurrentTrain: epoch  5, batch     3 | loss: 2.5442319Losses:  2.3195748329162598 0.8421899080276489
CurrentTrain: epoch  5, batch     4 | loss: 3.1617646Losses:  1.886328935623169 0.7801039218902588
CurrentTrain: epoch  5, batch     5 | loss: 2.6664329Losses:  2.0053205490112305 5.960465188081798e-08
CurrentTrain: epoch  5, batch     6 | loss: 2.0053205Losses:  2.040134906768799 0.4034076929092407
CurrentTrain: epoch  6, batch     0 | loss: 2.4435425Losses:  1.9547855854034424 0.6879702806472778
CurrentTrain: epoch  6, batch     1 | loss: 2.6427560Losses:  2.335420608520508 0.6166083812713623
CurrentTrain: epoch  6, batch     2 | loss: 2.9520290Losses:  2.633711338043213 0.6478124856948853
CurrentTrain: epoch  6, batch     3 | loss: 3.2815237Losses:  2.4759998321533203 0.8336864709854126
CurrentTrain: epoch  6, batch     4 | loss: 3.3096862Losses:  2.261064052581787 0.7734086513519287
CurrentTrain: epoch  6, batch     5 | loss: 3.0344727Losses:  2.5362653732299805 0.16677577793598175
CurrentTrain: epoch  6, batch     6 | loss: 2.7030411Losses:  2.189657688140869 0.49018165469169617
CurrentTrain: epoch  7, batch     0 | loss: 2.6798394Losses:  2.0493838787078857 0.3983055353164673
CurrentTrain: epoch  7, batch     1 | loss: 2.4476895Losses:  2.041337490081787 0.7024168968200684
CurrentTrain: epoch  7, batch     2 | loss: 2.7437544Losses:  1.9256209135055542 0.7496029734611511
CurrentTrain: epoch  7, batch     3 | loss: 2.6752238Losses:  2.1271615028381348 0.6521152257919312
CurrentTrain: epoch  7, batch     4 | loss: 2.7792768Losses:  2.5879640579223633 0.777994692325592
CurrentTrain: epoch  7, batch     5 | loss: 3.3659587Losses:  2.5830063819885254 0.2917402982711792
CurrentTrain: epoch  7, batch     6 | loss: 2.8747468Losses:  2.7039151191711426 0.7298732399940491
CurrentTrain: epoch  8, batch     0 | loss: 3.4337883Losses:  2.0185563564300537 0.7834618091583252
CurrentTrain: epoch  8, batch     1 | loss: 2.8020182Losses:  1.9686450958251953 0.5432754158973694
CurrentTrain: epoch  8, batch     2 | loss: 2.5119205Losses:  2.0450191497802734 0.3727940320968628
CurrentTrain: epoch  8, batch     3 | loss: 2.4178133Losses:  2.008542776107788 0.5163571238517761
CurrentTrain: epoch  8, batch     4 | loss: 2.5249000Losses:  1.799983263015747 0.3894389867782593
CurrentTrain: epoch  8, batch     5 | loss: 2.1894221Losses:  1.8042800426483154 0.14716736972332
CurrentTrain: epoch  8, batch     6 | loss: 1.9514474Losses:  1.74542236328125 0.7465723752975464
CurrentTrain: epoch  9, batch     0 | loss: 2.4919949Losses:  2.127656936645508 0.4362430274486542
CurrentTrain: epoch  9, batch     1 | loss: 2.5639000Losses:  2.6991896629333496 0.5797622203826904
CurrentTrain: epoch  9, batch     2 | loss: 3.2789519Losses:  1.78652822971344 0.44211214780807495
CurrentTrain: epoch  9, batch     3 | loss: 2.2286403Losses:  1.724069595336914 0.3422950208187103
CurrentTrain: epoch  9, batch     4 | loss: 2.0663645Losses:  1.8044285774230957 0.6431469917297363
CurrentTrain: epoch  9, batch     5 | loss: 2.4475756Losses:  1.7999495267868042 0.2020205855369568
CurrentTrain: epoch  9, batch     6 | loss: 2.0019701
Losses:  0.33447468280792236 0.6332352161407471
MemoryTrain:  epoch  0, batch     0 | loss: 0.9677099Losses:  0.7605230212211609 0.6008745431900024
MemoryTrain:  epoch  0, batch     1 | loss: 1.3613975Losses:  0.8648577332496643 0.4428884983062744
MemoryTrain:  epoch  0, batch     2 | loss: 1.3077462Losses:  1.8433897495269775 0.44549471139907837
MemoryTrain:  epoch  0, batch     3 | loss: 2.2888844Losses:  1.0893114805221558 0.46911293268203735
MemoryTrain:  epoch  0, batch     4 | loss: 1.5584245Losses:  1.2385923862457275 0.419019877910614
MemoryTrain:  epoch  1, batch     0 | loss: 1.6576123Losses:  0.33058464527130127 0.49040454626083374
MemoryTrain:  epoch  1, batch     1 | loss: 0.8209892Losses:  0.33372658491134644 0.5660434365272522
MemoryTrain:  epoch  1, batch     2 | loss: 0.8997700Losses:  2.0793068408966064 0.2917659282684326
MemoryTrain:  epoch  1, batch     3 | loss: 2.3710728Losses:  1.600716233253479 0.6859291791915894
MemoryTrain:  epoch  1, batch     4 | loss: 2.2866454Losses:  0.2694832384586334 0.5964933633804321
MemoryTrain:  epoch  2, batch     0 | loss: 0.8659766Losses:  0.8184894919395447 0.4286602735519409
MemoryTrain:  epoch  2, batch     1 | loss: 1.2471497Losses:  0.4526488184928894 0.49355995655059814
MemoryTrain:  epoch  2, batch     2 | loss: 0.9462088Losses:  0.66208815574646 0.4552551507949829
MemoryTrain:  epoch  2, batch     3 | loss: 1.1173433Losses:  0.2609177231788635 0.47380706667900085
MemoryTrain:  epoch  2, batch     4 | loss: 0.7347248Losses:  0.3749213218688965 0.554923415184021
MemoryTrain:  epoch  3, batch     0 | loss: 0.9298447Losses:  0.19518031179904938 0.5422567129135132
MemoryTrain:  epoch  3, batch     1 | loss: 0.7374370Losses:  0.1035725325345993 0.37460458278656006
MemoryTrain:  epoch  3, batch     2 | loss: 0.4781771Losses:  0.251221626996994 0.31306642293930054
MemoryTrain:  epoch  3, batch     3 | loss: 0.5642880Losses:  0.6229439377784729 0.578818142414093
MemoryTrain:  epoch  3, batch     4 | loss: 1.2017621Losses:  0.19108857214450836 0.5644323825836182
MemoryTrain:  epoch  4, batch     0 | loss: 0.7555209Losses:  0.0415196418762207 0.5504010915756226
MemoryTrain:  epoch  4, batch     1 | loss: 0.5919207Losses:  0.28315335512161255 0.47997990250587463
MemoryTrain:  epoch  4, batch     2 | loss: 0.7631333Losses:  0.029590357095003128 0.3316018581390381
MemoryTrain:  epoch  4, batch     3 | loss: 0.3611922Losses:  0.3062182068824768 0.4905576705932617
MemoryTrain:  epoch  4, batch     4 | loss: 0.7967759Losses:  0.35050031542778015 0.42906874418258667
MemoryTrain:  epoch  5, batch     0 | loss: 0.7795690Losses:  0.08931715041399002 0.34262269735336304
MemoryTrain:  epoch  5, batch     1 | loss: 0.4319398Losses:  0.06882746517658234 0.6600024700164795
MemoryTrain:  epoch  5, batch     2 | loss: 0.7288299Losses:  0.06835782527923584 0.5199878811836243
MemoryTrain:  epoch  5, batch     3 | loss: 0.5883457Losses:  0.04542749375104904 0.41751426458358765
MemoryTrain:  epoch  5, batch     4 | loss: 0.4629418Losses:  0.04046721011400223 0.3188825249671936
MemoryTrain:  epoch  6, batch     0 | loss: 0.3593497Losses:  0.04474570229649544 0.41822725534439087
MemoryTrain:  epoch  6, batch     1 | loss: 0.4629730Losses:  0.11114700138568878 0.70621258020401
MemoryTrain:  epoch  6, batch     2 | loss: 0.8173596Losses:  0.0467943400144577 0.34548240900039673
MemoryTrain:  epoch  6, batch     3 | loss: 0.3922768Losses:  0.04370896890759468 0.5814076066017151
MemoryTrain:  epoch  6, batch     4 | loss: 0.6251166Losses:  0.023726370185613632 0.4283544421195984
MemoryTrain:  epoch  7, batch     0 | loss: 0.4520808Losses:  0.046384233981370926 0.3137141764163971
MemoryTrain:  epoch  7, batch     1 | loss: 0.3600984Losses:  0.04837385565042496 0.44577816128730774
MemoryTrain:  epoch  7, batch     2 | loss: 0.4941520Losses:  0.03911678120493889 0.38410377502441406
MemoryTrain:  epoch  7, batch     3 | loss: 0.4232205Losses:  0.059388987720012665 0.4893597364425659
MemoryTrain:  epoch  7, batch     4 | loss: 0.5487487Losses:  0.030793458223342896 0.2647666931152344
MemoryTrain:  epoch  8, batch     0 | loss: 0.2955602Losses:  0.15226785838603973 0.3351379632949829
MemoryTrain:  epoch  8, batch     1 | loss: 0.4874058Losses:  0.03847508504986763 0.47902822494506836
MemoryTrain:  epoch  8, batch     2 | loss: 0.5175033Losses:  0.04569551721215248 0.500342845916748
MemoryTrain:  epoch  8, batch     3 | loss: 0.5460384Losses:  0.03528643772006035 0.46936291456222534
MemoryTrain:  epoch  8, batch     4 | loss: 0.5046493Losses:  0.044589459896087646 0.6307562589645386
MemoryTrain:  epoch  9, batch     0 | loss: 0.6753457Losses:  0.025118056684732437 0.30042052268981934
MemoryTrain:  epoch  9, batch     1 | loss: 0.3255386Losses:  0.08276481926441193 0.33979618549346924
MemoryTrain:  epoch  9, batch     2 | loss: 0.4225610Losses:  0.043784160166978836 0.377140611410141
MemoryTrain:  epoch  9, batch     3 | loss: 0.4209248Losses:  0.09203633666038513 0.3250519037246704
MemoryTrain:  epoch  9, batch     4 | loss: 0.4170882
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 62.19%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 55.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.80%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.44%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.19%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.37%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.67%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.61%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.53%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.40%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.14%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.87%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 88.40%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 88.10%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 87.72%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 87.00%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 86.79%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 86.25%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 86.13%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 85.35%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.84%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 84.61%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 84.31%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 83.86%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 83.78%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 83.38%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 82.98%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 82.72%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.13%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.56%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.99%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.38%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.32%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 79.49%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.66%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 79.89%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 79.68%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 79.54%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 79.38%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 80.20%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.16%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 79.72%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 79.33%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 78.95%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 78.70%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.45%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 78.56%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 78.08%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.61%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 77.39%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 76.94%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 76.52%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.51%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 76.54%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 76.85%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 77.06%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 77.42%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.51%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 77.46%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 77.49%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 77.57%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 77.62%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.66%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 77.61%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 77.46%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 77.28%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 77.52%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 77.59%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 77.58%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.67%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.77%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 77.60%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 77.33%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 77.19%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 76.96%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 76.70%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 76.50%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 76.55%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 76.54%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 76.40%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 76.21%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 76.02%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 75.78%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 75.60%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 75.33%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 75.06%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 75.90%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  224 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.52%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 77.37%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.88%   
cur_acc:  ['0.9454', '0.7093', '0.7431', '0.7817']
his_acc:  ['0.9454', '0.8250', '0.8072', '0.7788']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  5.362175941467285 1.0480326414108276
CurrentTrain: epoch  0, batch     0 | loss: 6.4102087Losses:  6.011165142059326 1.0421192646026611
CurrentTrain: epoch  0, batch     1 | loss: 7.0532846Losses:  5.347582817077637 1.0132834911346436
CurrentTrain: epoch  0, batch     2 | loss: 6.3608665Losses:  6.66048526763916 1.413534164428711
CurrentTrain: epoch  0, batch     3 | loss: 8.0740194Losses:  5.601975917816162 0.9954932928085327
CurrentTrain: epoch  0, batch     4 | loss: 6.5974693Losses:  5.741163730621338 0.8147186040878296
CurrentTrain: epoch  0, batch     5 | loss: 6.5558825Losses:  4.640157222747803 8.94069742685133e-08
CurrentTrain: epoch  0, batch     6 | loss: 4.6401572Losses:  4.665989398956299 0.9426895976066589
CurrentTrain: epoch  1, batch     0 | loss: 5.6086788Losses:  5.479540824890137 1.3879635334014893
CurrentTrain: epoch  1, batch     1 | loss: 6.8675041Losses:  3.8091063499450684 0.9862093925476074
CurrentTrain: epoch  1, batch     2 | loss: 4.7953157Losses:  4.907312393188477 1.357445240020752
CurrentTrain: epoch  1, batch     3 | loss: 6.2647576Losses:  4.140853404998779 1.0473361015319824
CurrentTrain: epoch  1, batch     4 | loss: 5.1881895Losses:  4.589805603027344 1.3572040796279907
CurrentTrain: epoch  1, batch     5 | loss: 5.9470096Losses:  4.4045023918151855 0.39635369181632996
CurrentTrain: epoch  1, batch     6 | loss: 4.8008561Losses:  4.861706733703613 1.2580907344818115
CurrentTrain: epoch  2, batch     0 | loss: 6.1197977Losses:  3.1286747455596924 1.1783270835876465
CurrentTrain: epoch  2, batch     1 | loss: 4.3070021Losses:  3.912607192993164 1.3417162895202637
CurrentTrain: epoch  2, batch     2 | loss: 5.2543235Losses:  5.395101547241211 1.0408415794372559
CurrentTrain: epoch  2, batch     3 | loss: 6.4359431Losses:  3.2054872512817383 0.9950038194656372
CurrentTrain: epoch  2, batch     4 | loss: 4.2004910Losses:  3.770108222961426 1.109468698501587
CurrentTrain: epoch  2, batch     5 | loss: 4.8795767Losses:  3.191316843032837 0.06681235134601593
CurrentTrain: epoch  2, batch     6 | loss: 3.2581291Losses:  4.682735443115234 1.1112853288650513
CurrentTrain: epoch  3, batch     0 | loss: 5.7940207Losses:  3.2950093746185303 0.7743600606918335
CurrentTrain: epoch  3, batch     1 | loss: 4.0693693Losses:  3.150911331176758 0.5830485820770264
CurrentTrain: epoch  3, batch     2 | loss: 3.7339599Losses:  4.1151533126831055 0.7013719081878662
CurrentTrain: epoch  3, batch     3 | loss: 4.8165255Losses:  3.339016914367676 0.8345628380775452
CurrentTrain: epoch  3, batch     4 | loss: 4.1735797Losses:  2.8471932411193848 0.8865725994110107
CurrentTrain: epoch  3, batch     5 | loss: 3.7337658Losses:  3.5635838508605957 0.4890134334564209
CurrentTrain: epoch  3, batch     6 | loss: 4.0525970Losses:  3.3399581909179688 0.9117709994316101
CurrentTrain: epoch  4, batch     0 | loss: 4.2517290Losses:  2.7570204734802246 0.592997670173645
CurrentTrain: epoch  4, batch     1 | loss: 3.3500180Losses:  3.1485953330993652 0.875306248664856
CurrentTrain: epoch  4, batch     2 | loss: 4.0239015Losses:  4.182023048400879 0.7722221612930298
CurrentTrain: epoch  4, batch     3 | loss: 4.9542451Losses:  3.218567132949829 0.5532609224319458
CurrentTrain: epoch  4, batch     4 | loss: 3.7718282Losses:  2.9884204864501953 0.8683507442474365
CurrentTrain: epoch  4, batch     5 | loss: 3.8567712Losses:  1.9490549564361572 0.20165526866912842
CurrentTrain: epoch  4, batch     6 | loss: 2.1507101Losses:  2.753587245941162 1.0324323177337646
CurrentTrain: epoch  5, batch     0 | loss: 3.7860196Losses:  2.834120988845825 0.9284424185752869
CurrentTrain: epoch  5, batch     1 | loss: 3.7625635Losses:  2.671124219894409 0.5382604598999023
CurrentTrain: epoch  5, batch     2 | loss: 3.2093847Losses:  2.6025776863098145 0.77603679895401
CurrentTrain: epoch  5, batch     3 | loss: 3.3786144Losses:  2.8719844818115234 0.5629724264144897
CurrentTrain: epoch  5, batch     4 | loss: 3.4349570Losses:  2.592651844024658 0.8099697232246399
CurrentTrain: epoch  5, batch     5 | loss: 3.4026215Losses:  4.098838806152344 0.3191607892513275
CurrentTrain: epoch  5, batch     6 | loss: 4.4179997Losses:  2.9908838272094727 0.5276418924331665
CurrentTrain: epoch  6, batch     0 | loss: 3.5185256Losses:  2.353222131729126 0.7481091022491455
CurrentTrain: epoch  6, batch     1 | loss: 3.1013312Losses:  2.3284964561462402 0.7630419731140137
CurrentTrain: epoch  6, batch     2 | loss: 3.0915384Losses:  2.162994861602783 0.5421277284622192
CurrentTrain: epoch  6, batch     3 | loss: 2.7051225Losses:  2.4236831665039062 0.8765809535980225
CurrentTrain: epoch  6, batch     4 | loss: 3.3002641Losses:  2.4909749031066895 0.7189345359802246
CurrentTrain: epoch  6, batch     5 | loss: 3.2099094Losses:  1.7512023448944092 0.18007026612758636
CurrentTrain: epoch  6, batch     6 | loss: 1.9312726Losses:  2.5971992015838623 0.6407495141029358
CurrentTrain: epoch  7, batch     0 | loss: 3.2379487Losses:  2.1873011589050293 0.5544173717498779
CurrentTrain: epoch  7, batch     1 | loss: 2.7417185Losses:  2.455209255218506 0.5897458791732788
CurrentTrain: epoch  7, batch     2 | loss: 3.0449553Losses:  2.2231836318969727 0.5681403875350952
CurrentTrain: epoch  7, batch     3 | loss: 2.7913241Losses:  2.15985107421875 0.6898719668388367
CurrentTrain: epoch  7, batch     4 | loss: 2.8497231Losses:  2.2044520378112793 0.49958544969558716
CurrentTrain: epoch  7, batch     5 | loss: 2.7040374Losses:  2.379164218902588 0.3268657922744751
CurrentTrain: epoch  7, batch     6 | loss: 2.7060299Losses:  2.3557937145233154 0.8157405853271484
CurrentTrain: epoch  8, batch     0 | loss: 3.1715343Losses:  1.9012172222137451 0.7162990570068359
CurrentTrain: epoch  8, batch     1 | loss: 2.6175163Losses:  1.9141645431518555 0.43571537733078003
CurrentTrain: epoch  8, batch     2 | loss: 2.3498800Losses:  2.6638782024383545 0.6168211698532104
CurrentTrain: epoch  8, batch     3 | loss: 3.2806993Losses:  1.911564826965332 0.4081336259841919
CurrentTrain: epoch  8, batch     4 | loss: 2.3196983Losses:  2.2720680236816406 0.5215747952461243
CurrentTrain: epoch  8, batch     5 | loss: 2.7936428Losses:  1.7405797243118286 0.06183470040559769
CurrentTrain: epoch  8, batch     6 | loss: 1.8024144Losses:  2.025355339050293 0.7371628880500793
CurrentTrain: epoch  9, batch     0 | loss: 2.7625182Losses:  1.9968254566192627 0.6436264514923096
CurrentTrain: epoch  9, batch     1 | loss: 2.6404519Losses:  1.9058549404144287 0.3863619565963745
CurrentTrain: epoch  9, batch     2 | loss: 2.2922168Losses:  2.0101709365844727 0.6069260835647583
CurrentTrain: epoch  9, batch     3 | loss: 2.6170969Losses:  2.080359697341919 0.5123446583747864
CurrentTrain: epoch  9, batch     4 | loss: 2.5927043Losses:  2.5478386878967285 0.7266747951507568
CurrentTrain: epoch  9, batch     5 | loss: 3.2745135Losses:  1.9143738746643066 0.15439358353614807
CurrentTrain: epoch  9, batch     6 | loss: 2.0687675
Losses:  1.317617654800415 0.42817139625549316
MemoryTrain:  epoch  0, batch     0 | loss: 1.7457891Losses:  0.2617431879043579 0.39064720273017883
MemoryTrain:  epoch  0, batch     1 | loss: 0.6523904Losses:  0.5972927212715149 0.7387861013412476
MemoryTrain:  epoch  0, batch     2 | loss: 1.3360789Losses:  0.5900166034698486 0.42978939414024353
MemoryTrain:  epoch  0, batch     3 | loss: 1.0198060Losses:  0.9341734647750854 0.36831945180892944
MemoryTrain:  epoch  0, batch     4 | loss: 1.3024929Losses:  0.19238507747650146 0.5253534317016602
MemoryTrain:  epoch  0, batch     5 | loss: 0.7177385Losses:  1.3365428447723389 0.05412854999303818
MemoryTrain:  epoch  0, batch     6 | loss: 1.3906714Losses:  0.41488179564476013 0.707018256187439
MemoryTrain:  epoch  1, batch     0 | loss: 1.1219001Losses:  1.8398537635803223 0.3586215078830719
MemoryTrain:  epoch  1, batch     1 | loss: 2.1984754Losses:  0.6261060237884521 0.42939919233322144
MemoryTrain:  epoch  1, batch     2 | loss: 1.0555053Losses:  1.280274510383606 0.6633306741714478
MemoryTrain:  epoch  1, batch     3 | loss: 1.9436052Losses:  0.7357879877090454 0.4234285056591034
MemoryTrain:  epoch  1, batch     4 | loss: 1.1592165Losses:  0.3151577115058899 0.5113983750343323
MemoryTrain:  epoch  1, batch     5 | loss: 0.8265561Losses:  0.10452240705490112 0.029408905655145645
MemoryTrain:  epoch  1, batch     6 | loss: 0.1339313Losses:  0.531926155090332 0.47058922052383423
MemoryTrain:  epoch  2, batch     0 | loss: 1.0025153Losses:  0.8773372173309326 0.33197924494743347
MemoryTrain:  epoch  2, batch     1 | loss: 1.2093165Losses:  0.23745018243789673 0.5629463195800781
MemoryTrain:  epoch  2, batch     2 | loss: 0.8003965Losses:  0.24469327926635742 0.39026227593421936
MemoryTrain:  epoch  2, batch     3 | loss: 0.6349555Losses:  0.2809658646583557 0.4052501916885376
MemoryTrain:  epoch  2, batch     4 | loss: 0.6862161Losses:  1.1694467067718506 0.49242880940437317
MemoryTrain:  epoch  2, batch     5 | loss: 1.6618755Losses:  0.0475044846534729 0.4473547041416168
MemoryTrain:  epoch  2, batch     6 | loss: 0.4948592Losses:  0.21254342794418335 0.3322441577911377
MemoryTrain:  epoch  3, batch     0 | loss: 0.5447876Losses:  0.09481963515281677 0.3532602787017822
MemoryTrain:  epoch  3, batch     1 | loss: 0.4480799Losses:  0.11200755834579468 0.7508028149604797
MemoryTrain:  epoch  3, batch     2 | loss: 0.8628104Losses:  0.4509960114955902 0.5264809131622314
MemoryTrain:  epoch  3, batch     3 | loss: 0.9774770Losses:  0.189451664686203 0.35437625646591187
MemoryTrain:  epoch  3, batch     4 | loss: 0.5438279Losses:  0.4000450372695923 0.6853148937225342
MemoryTrain:  epoch  3, batch     5 | loss: 1.0853599Losses:  0.03445351496338844 0.1687583029270172
MemoryTrain:  epoch  3, batch     6 | loss: 0.2032118Losses:  0.302273690700531 0.6682759523391724
MemoryTrain:  epoch  4, batch     0 | loss: 0.9705496Losses:  0.1228828877210617 0.3484622538089752
MemoryTrain:  epoch  4, batch     1 | loss: 0.4713451Losses:  0.2661727964878082 0.646622896194458
MemoryTrain:  epoch  4, batch     2 | loss: 0.9127957Losses:  0.07652592658996582 0.5228476524353027
MemoryTrain:  epoch  4, batch     3 | loss: 0.5993736Losses:  0.1282920390367508 0.5323715806007385
MemoryTrain:  epoch  4, batch     4 | loss: 0.6606636Losses:  0.04477454721927643 0.3580002784729004
MemoryTrain:  epoch  4, batch     5 | loss: 0.4027748Losses:  0.022909440100193024 0.04576616734266281
MemoryTrain:  epoch  4, batch     6 | loss: 0.0686756Losses:  0.11508969217538834 0.3865237832069397
MemoryTrain:  epoch  5, batch     0 | loss: 0.5016135Losses:  0.04826609045267105 0.3733271360397339
MemoryTrain:  epoch  5, batch     1 | loss: 0.4215932Losses:  0.12439355999231339 0.4906928837299347
MemoryTrain:  epoch  5, batch     2 | loss: 0.6150864Losses:  0.06474388390779495 0.3666647672653198
MemoryTrain:  epoch  5, batch     3 | loss: 0.4314086Losses:  0.10386325418949127 0.4614093601703644
MemoryTrain:  epoch  5, batch     4 | loss: 0.5652726Losses:  0.09565318375825882 0.7321599721908569
MemoryTrain:  epoch  5, batch     5 | loss: 0.8278131Losses:  0.0378909632563591 0.01952364854514599
MemoryTrain:  epoch  5, batch     6 | loss: 0.0574146Losses:  0.06709636747837067 0.4652063846588135
MemoryTrain:  epoch  6, batch     0 | loss: 0.5323027Losses:  0.047271840274333954 0.4165384769439697
MemoryTrain:  epoch  6, batch     1 | loss: 0.4638103Losses:  0.04074033349752426 0.4523638188838959
MemoryTrain:  epoch  6, batch     2 | loss: 0.4931042Losses:  0.05306798219680786 0.5601890087127686
MemoryTrain:  epoch  6, batch     3 | loss: 0.6132570Losses:  0.09848995506763458 0.5498324632644653
MemoryTrain:  epoch  6, batch     4 | loss: 0.6483224Losses:  0.04573535546660423 0.44344496726989746
MemoryTrain:  epoch  6, batch     5 | loss: 0.4891803Losses:  0.018279986456036568 0.02831529825925827
MemoryTrain:  epoch  6, batch     6 | loss: 0.0465953Losses:  0.043197181075811386 0.40792423486709595
MemoryTrain:  epoch  7, batch     0 | loss: 0.4511214Losses:  0.14891186356544495 0.5286240577697754
MemoryTrain:  epoch  7, batch     1 | loss: 0.6775359Losses:  0.05017602816224098 0.33481261134147644
MemoryTrain:  epoch  7, batch     2 | loss: 0.3849886Losses:  0.045135341584682465 0.3513122797012329
MemoryTrain:  epoch  7, batch     3 | loss: 0.3964476Losses:  0.06164330244064331 0.5378515720367432
MemoryTrain:  epoch  7, batch     4 | loss: 0.5994949Losses:  0.03037019446492195 0.39423999190330505
MemoryTrain:  epoch  7, batch     5 | loss: 0.4246102Losses:  0.02764250338077545 0.008615551516413689
MemoryTrain:  epoch  7, batch     6 | loss: 0.0362581Losses:  0.06065986678004265 0.5778415203094482
MemoryTrain:  epoch  8, batch     0 | loss: 0.6385014Losses:  0.02169129252433777 0.4495212733745575
MemoryTrain:  epoch  8, batch     1 | loss: 0.4712126Losses:  0.024164222180843353 0.29336780309677124
MemoryTrain:  epoch  8, batch     2 | loss: 0.3175320Losses:  0.040950387716293335 0.3679441809654236
MemoryTrain:  epoch  8, batch     3 | loss: 0.4088946Losses:  0.06729307025671005 0.29485759139060974
MemoryTrain:  epoch  8, batch     4 | loss: 0.3621507Losses:  0.09310334920883179 0.4189983606338501
MemoryTrain:  epoch  8, batch     5 | loss: 0.5121017Losses:  0.04668135195970535 0.12460903078317642
MemoryTrain:  epoch  8, batch     6 | loss: 0.1712904Losses:  0.03378327190876007 0.27365514636039734
MemoryTrain:  epoch  9, batch     0 | loss: 0.3074384Losses:  0.06295608729124069 0.6052533388137817
MemoryTrain:  epoch  9, batch     1 | loss: 0.6682094Losses:  0.031534843146800995 0.5015279650688171
MemoryTrain:  epoch  9, batch     2 | loss: 0.5330628Losses:  0.027832742780447006 0.3802044689655304
MemoryTrain:  epoch  9, batch     3 | loss: 0.4080372Losses:  0.033265478909015656 0.3820592164993286
MemoryTrain:  epoch  9, batch     4 | loss: 0.4153247Losses:  0.03463788330554962 0.37038329243659973
MemoryTrain:  epoch  9, batch     5 | loss: 0.4050212Losses:  0.013342378661036491 0.005131291691213846
MemoryTrain:  epoch  9, batch     6 | loss: 0.0184737
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 64.76%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.31%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 88.86%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.49%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 87.93%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 87.71%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.40%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.40%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 88.32%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 87.99%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.66%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 87.19%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 87.04%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 86.51%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 86.07%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 86.01%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 85.74%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 85.20%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 84.90%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 84.58%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 84.48%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.17%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 83.80%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 83.31%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 82.89%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 82.49%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 82.28%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 81.95%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 81.56%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 81.13%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 80.82%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 80.34%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 80.05%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 79.76%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 79.54%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 79.09%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.47%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 77.87%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.27%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 76.75%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 76.40%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.16%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 77.48%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.83%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 77.61%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 77.44%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.51%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.08%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.22%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 77.74%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.37%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 76.99%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 76.72%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.49%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.22%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 76.66%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 76.15%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.69%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.49%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.56%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.56%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 74.92%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 76.05%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 76.17%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 76.13%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.93%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 76.43%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 76.51%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 76.31%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 76.05%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 75.63%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 75.35%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 75.16%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 75.15%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 75.12%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.97%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 74.73%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 74.52%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 74.23%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 73.97%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 74.83%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 74.67%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.24%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 76.77%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 76.71%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 76.57%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 76.50%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.38%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 76.15%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 76.11%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 76.08%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 76.01%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 76.17%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 76.10%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 76.03%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 75.78%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 75.55%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 75.50%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 75.33%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 75.20%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 75.24%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 75.36%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 75.38%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 75.38%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 75.44%   [EVAL] batch:  298 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 76.34%   
cur_acc:  ['0.9454', '0.7093', '0.7431', '0.7817', '0.7431']
his_acc:  ['0.9454', '0.8250', '0.8072', '0.7788', '0.7634']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  5.280759334564209 1.0247337818145752
CurrentTrain: epoch  0, batch     0 | loss: 6.3054934Losses:  6.210265636444092 0.9343286752700806
CurrentTrain: epoch  0, batch     1 | loss: 7.1445942Losses:  5.6881561279296875 0.8375069499015808
CurrentTrain: epoch  0, batch     2 | loss: 6.5256629Losses:  6.843685626983643 1.09380304813385
CurrentTrain: epoch  0, batch     3 | loss: 7.9374886Losses:  7.24100923538208 1.1506389379501343
CurrentTrain: epoch  0, batch     4 | loss: 8.3916483Losses:  6.6011762619018555 1.0768709182739258
CurrentTrain: epoch  0, batch     5 | loss: 7.6780472Losses:  5.976306438446045 2.9802322387695312e-08
CurrentTrain: epoch  0, batch     6 | loss: 5.9763064Losses:  5.547053813934326 1.151275634765625
CurrentTrain: epoch  1, batch     0 | loss: 6.6983294Losses:  5.864147186279297 1.1272536516189575
CurrentTrain: epoch  1, batch     1 | loss: 6.9914007Losses:  4.391676425933838 1.1449649333953857
CurrentTrain: epoch  1, batch     2 | loss: 5.5366411Losses:  4.346254348754883 0.8466651439666748
CurrentTrain: epoch  1, batch     3 | loss: 5.1929197Losses:  4.0099616050720215 0.8671842813491821
CurrentTrain: epoch  1, batch     4 | loss: 4.8771458Losses:  4.071954727172852 1.0697226524353027
CurrentTrain: epoch  1, batch     5 | loss: 5.1416774Losses:  4.395113468170166 0.35379332304000854
CurrentTrain: epoch  1, batch     6 | loss: 4.7489066Losses:  4.4789958000183105 1.0790349245071411
CurrentTrain: epoch  2, batch     0 | loss: 5.5580306Losses:  3.297593355178833 0.8130277395248413
CurrentTrain: epoch  2, batch     1 | loss: 4.1106210Losses:  4.015087127685547 1.0248714685440063
CurrentTrain: epoch  2, batch     2 | loss: 5.0399585Losses:  4.470937252044678 0.5636111497879028
CurrentTrain: epoch  2, batch     3 | loss: 5.0345483Losses:  4.046663284301758 0.7629455924034119
CurrentTrain: epoch  2, batch     4 | loss: 4.8096089Losses:  3.321436882019043 1.0113989114761353
CurrentTrain: epoch  2, batch     5 | loss: 4.3328357Losses:  4.2474365234375 0.2370009869337082
CurrentTrain: epoch  2, batch     6 | loss: 4.4844375Losses:  2.626124858856201 0.7876335382461548
CurrentTrain: epoch  3, batch     0 | loss: 3.4137583Losses:  3.5940070152282715 1.0761462450027466
CurrentTrain: epoch  3, batch     1 | loss: 4.6701531Losses:  3.220845937728882 0.6612477898597717
CurrentTrain: epoch  3, batch     2 | loss: 3.8820937Losses:  4.073129653930664 1.1869562864303589
CurrentTrain: epoch  3, batch     3 | loss: 5.2600861Losses:  3.7407150268554688 0.938481330871582
CurrentTrain: epoch  3, batch     4 | loss: 4.6791964Losses:  3.435518264770508 0.9496592283248901
CurrentTrain: epoch  3, batch     5 | loss: 4.3851776Losses:  3.472564220428467 0.23061618208885193
CurrentTrain: epoch  3, batch     6 | loss: 3.7031803Losses:  2.901005744934082 0.9055461883544922
CurrentTrain: epoch  4, batch     0 | loss: 3.8065519Losses:  2.334477186203003 0.7209534049034119
CurrentTrain: epoch  4, batch     1 | loss: 3.0554307Losses:  3.360917806625366 0.7767220139503479
CurrentTrain: epoch  4, batch     2 | loss: 4.1376400Losses:  3.926453113555908 0.7190456986427307
CurrentTrain: epoch  4, batch     3 | loss: 4.6454988Losses:  3.337644338607788 0.734486997127533
CurrentTrain: epoch  4, batch     4 | loss: 4.0721312Losses:  2.914867877960205 0.5946950912475586
CurrentTrain: epoch  4, batch     5 | loss: 3.5095630Losses:  2.113280773162842 0.19407901167869568
CurrentTrain: epoch  4, batch     6 | loss: 2.3073597Losses:  2.1773900985717773 0.7217386960983276
CurrentTrain: epoch  5, batch     0 | loss: 2.8991289Losses:  3.038571357727051 0.6534950733184814
CurrentTrain: epoch  5, batch     1 | loss: 3.6920664Losses:  3.0007688999176025 0.5224748253822327
CurrentTrain: epoch  5, batch     2 | loss: 3.5232437Losses:  3.045123815536499 0.5193139314651489
CurrentTrain: epoch  5, batch     3 | loss: 3.5644379Losses:  2.7993569374084473 0.9802706241607666
CurrentTrain: epoch  5, batch     4 | loss: 3.7796276Losses:  2.2374086380004883 0.4684182107448578
CurrentTrain: epoch  5, batch     5 | loss: 2.7058268Losses:  2.35099458694458 0.1512850672006607
CurrentTrain: epoch  5, batch     6 | loss: 2.5022798Losses:  2.0644640922546387 0.6523700952529907
CurrentTrain: epoch  6, batch     0 | loss: 2.7168341Losses:  2.138139247894287 0.7333571910858154
CurrentTrain: epoch  6, batch     1 | loss: 2.8714964Losses:  2.7494730949401855 0.7844821214675903
CurrentTrain: epoch  6, batch     2 | loss: 3.5339551Losses:  2.263460159301758 0.8538642525672913
CurrentTrain: epoch  6, batch     3 | loss: 3.1173244Losses:  2.9323320388793945 0.9314019083976746
CurrentTrain: epoch  6, batch     4 | loss: 3.8637340Losses:  2.0549824237823486 0.6610907316207886
CurrentTrain: epoch  6, batch     5 | loss: 2.7160730Losses:  2.503783702850342 0.067034050822258
CurrentTrain: epoch  6, batch     6 | loss: 2.5708177Losses:  2.2134103775024414 0.7346574068069458
CurrentTrain: epoch  7, batch     0 | loss: 2.9480677Losses:  2.27779221534729 0.6136716604232788
CurrentTrain: epoch  7, batch     1 | loss: 2.8914638Losses:  2.3415117263793945 1.001957654953003
CurrentTrain: epoch  7, batch     2 | loss: 3.3434694Losses:  2.074125289916992 0.7041000127792358
CurrentTrain: epoch  7, batch     3 | loss: 2.7782254Losses:  2.1140530109405518 0.6332391500473022
CurrentTrain: epoch  7, batch     4 | loss: 2.7472920Losses:  1.9523659944534302 0.30822062492370605
CurrentTrain: epoch  7, batch     5 | loss: 2.2605867Losses:  2.5522677898406982 0.1451968550682068
CurrentTrain: epoch  7, batch     6 | loss: 2.6974647Losses:  2.189577579498291 0.5886764526367188
CurrentTrain: epoch  8, batch     0 | loss: 2.7782540Losses:  1.8882712125778198 0.37319183349609375
CurrentTrain: epoch  8, batch     1 | loss: 2.2614632Losses:  1.9079402685165405 0.47128164768218994
CurrentTrain: epoch  8, batch     2 | loss: 2.3792219Losses:  1.944784164428711 0.5714919567108154
CurrentTrain: epoch  8, batch     3 | loss: 2.5162761Losses:  2.369999647140503 0.7962594032287598
CurrentTrain: epoch  8, batch     4 | loss: 3.1662591Losses:  2.4221436977386475 0.8524354100227356
CurrentTrain: epoch  8, batch     5 | loss: 3.2745790Losses:  1.7415802478790283 0.02734060399234295
CurrentTrain: epoch  8, batch     6 | loss: 1.7689209Losses:  1.8315116167068481 0.40166419744491577
CurrentTrain: epoch  9, batch     0 | loss: 2.2331758Losses:  2.0458812713623047 0.5492432713508606
CurrentTrain: epoch  9, batch     1 | loss: 2.5951245Losses:  2.1283609867095947 0.5107264518737793
CurrentTrain: epoch  9, batch     2 | loss: 2.6390874Losses:  1.8456987142562866 0.5474815368652344
CurrentTrain: epoch  9, batch     3 | loss: 2.3931804Losses:  2.1894171237945557 0.6869463920593262
CurrentTrain: epoch  9, batch     4 | loss: 2.8763635Losses:  1.9041872024536133 0.4879103899002075
CurrentTrain: epoch  9, batch     5 | loss: 2.3920975Losses:  2.623293399810791 0.295987069606781
CurrentTrain: epoch  9, batch     6 | loss: 2.9192805
Losses:  0.831264078617096 0.5148417353630066
MemoryTrain:  epoch  0, batch     0 | loss: 1.3461058Losses:  0.5416757464408875 0.35355621576309204
MemoryTrain:  epoch  0, batch     1 | loss: 0.8952320Losses:  1.5799925327301025 0.6679720878601074
MemoryTrain:  epoch  0, batch     2 | loss: 2.2479646Losses:  1.1875321865081787 0.5526244640350342
MemoryTrain:  epoch  0, batch     3 | loss: 1.7401567Losses:  1.4133996963500977 0.5693145394325256
MemoryTrain:  epoch  0, batch     4 | loss: 1.9827142Losses:  0.3018466532230377 0.32362908124923706
MemoryTrain:  epoch  0, batch     5 | loss: 0.6254758Losses:  0.8717156648635864 0.4979207515716553
MemoryTrain:  epoch  0, batch     6 | loss: 1.3696364Losses:  0.05604304000735283 0.25297537446022034
MemoryTrain:  epoch  0, batch     7 | loss: 0.3090184Losses:  1.835163950920105 0.581855833530426
MemoryTrain:  epoch  1, batch     0 | loss: 2.4170198Losses:  1.1234225034713745 0.2982133626937866
MemoryTrain:  epoch  1, batch     1 | loss: 1.4216359Losses:  1.126875638961792 0.5098381042480469
MemoryTrain:  epoch  1, batch     2 | loss: 1.6367137Losses:  0.5071612596511841 0.39922958612442017
MemoryTrain:  epoch  1, batch     3 | loss: 0.9063908Losses:  0.8684094548225403 0.5711441040039062
MemoryTrain:  epoch  1, batch     4 | loss: 1.4395535Losses:  1.6010974645614624 0.4999334514141083
MemoryTrain:  epoch  1, batch     5 | loss: 2.1010308Losses:  0.7113072872161865 0.5460177063941956
MemoryTrain:  epoch  1, batch     6 | loss: 1.2573249Losses:  0.16822180151939392 0.16696159541606903
MemoryTrain:  epoch  1, batch     7 | loss: 0.3351834Losses:  0.803458571434021 0.5633844137191772
MemoryTrain:  epoch  2, batch     0 | loss: 1.3668430Losses:  0.5172248482704163 0.4473450183868408
MemoryTrain:  epoch  2, batch     1 | loss: 0.9645699Losses:  0.05721630901098251 0.36153459548950195
MemoryTrain:  epoch  2, batch     2 | loss: 0.4187509Losses:  0.1544201523065567 0.6894341707229614
MemoryTrain:  epoch  2, batch     3 | loss: 0.8438543Losses:  0.2733643651008606 0.31264302134513855
MemoryTrain:  epoch  2, batch     4 | loss: 0.5860074Losses:  0.857398509979248 0.40435630083084106
MemoryTrain:  epoch  2, batch     5 | loss: 1.2617548Losses:  0.40451282262802124 0.30670326948165894
MemoryTrain:  epoch  2, batch     6 | loss: 0.7112161Losses:  0.372734397649765 0.3311567008495331
MemoryTrain:  epoch  2, batch     7 | loss: 0.7038911Losses:  0.0867888480424881 0.4242297112941742
MemoryTrain:  epoch  3, batch     0 | loss: 0.5110186Losses:  0.0810488760471344 0.4744008779525757
MemoryTrain:  epoch  3, batch     1 | loss: 0.5554497Losses:  0.24671146273612976 0.2740476727485657
MemoryTrain:  epoch  3, batch     2 | loss: 0.5207591Losses:  0.4731731116771698 0.6026756167411804
MemoryTrain:  epoch  3, batch     3 | loss: 1.0758487Losses:  0.25201845169067383 0.2584724724292755
MemoryTrain:  epoch  3, batch     4 | loss: 0.5104909Losses:  0.49876412749290466 0.42587265372276306
MemoryTrain:  epoch  3, batch     5 | loss: 0.9246368Losses:  0.1233237087726593 0.6388987898826599
MemoryTrain:  epoch  3, batch     6 | loss: 0.7622225Losses:  1.5668299198150635 0.2823934555053711
MemoryTrain:  epoch  3, batch     7 | loss: 1.8492234Losses:  0.5335896015167236 0.48221856355667114
MemoryTrain:  epoch  4, batch     0 | loss: 1.0158081Losses:  0.399085134267807 0.8333678245544434
MemoryTrain:  epoch  4, batch     1 | loss: 1.2324530Losses:  0.044371724128723145 0.33965957164764404
MemoryTrain:  epoch  4, batch     2 | loss: 0.3840313Losses:  0.15050841867923737 0.32242292165756226
MemoryTrain:  epoch  4, batch     3 | loss: 0.4729313Losses:  0.05284096300601959 0.34709152579307556
MemoryTrain:  epoch  4, batch     4 | loss: 0.3999325Losses:  0.12397506833076477 0.40332284569740295
MemoryTrain:  epoch  4, batch     5 | loss: 0.5272979Losses:  0.09010572731494904 0.3903053402900696
MemoryTrain:  epoch  4, batch     6 | loss: 0.4804111Losses:  0.2701049745082855 0.27711939811706543
MemoryTrain:  epoch  4, batch     7 | loss: 0.5472244Losses:  0.06725513935089111 0.3906729817390442
MemoryTrain:  epoch  5, batch     0 | loss: 0.4579281Losses:  0.04238332435488701 0.2795862555503845
MemoryTrain:  epoch  5, batch     1 | loss: 0.3219696Losses:  0.11153309047222137 0.46483898162841797
MemoryTrain:  epoch  5, batch     2 | loss: 0.5763721Losses:  0.3536318242549896 0.5716007947921753
MemoryTrain:  epoch  5, batch     3 | loss: 0.9252326Losses:  0.416698157787323 0.563170850276947
MemoryTrain:  epoch  5, batch     4 | loss: 0.9798690Losses:  0.04906793683767319 0.45618385076522827
MemoryTrain:  epoch  5, batch     5 | loss: 0.5052518Losses:  0.041294701397418976 0.3646318316459656
MemoryTrain:  epoch  5, batch     6 | loss: 0.4059265Losses:  0.20399409532546997 0.34805822372436523
MemoryTrain:  epoch  5, batch     7 | loss: 0.5520523Losses:  0.03253982216119766 0.4333343505859375
MemoryTrain:  epoch  6, batch     0 | loss: 0.4658742Losses:  0.12351012229919434 0.3187560439109802
MemoryTrain:  epoch  6, batch     1 | loss: 0.4422662Losses:  0.05074688047170639 0.5930643081665039
MemoryTrain:  epoch  6, batch     2 | loss: 0.6438112Losses:  0.03732519969344139 0.3010338246822357
MemoryTrain:  epoch  6, batch     3 | loss: 0.3383590Losses:  0.05212804675102234 0.6091299057006836
MemoryTrain:  epoch  6, batch     4 | loss: 0.6612580Losses:  0.04520823061466217 0.40688782930374146
MemoryTrain:  epoch  6, batch     5 | loss: 0.4520960Losses:  0.06198175251483917 0.40512341260910034
MemoryTrain:  epoch  6, batch     6 | loss: 0.4671052Losses:  0.035706084221601486 0.12484337389469147
MemoryTrain:  epoch  6, batch     7 | loss: 0.1605495Losses:  0.04226351156830788 0.7161197662353516
MemoryTrain:  epoch  7, batch     0 | loss: 0.7583833Losses:  0.03270667791366577 0.2458956241607666
MemoryTrain:  epoch  7, batch     1 | loss: 0.2786023Losses:  0.04250636324286461 0.4586982727050781
MemoryTrain:  epoch  7, batch     2 | loss: 0.5012046Losses:  0.039946042001247406 0.512433648109436
MemoryTrain:  epoch  7, batch     3 | loss: 0.5523797Losses:  0.028956007212400436 0.38912492990493774
MemoryTrain:  epoch  7, batch     4 | loss: 0.4180809Losses:  0.0404796227812767 0.37527576088905334
MemoryTrain:  epoch  7, batch     5 | loss: 0.4157554Losses:  0.028652220964431763 0.344434916973114
MemoryTrain:  epoch  7, batch     6 | loss: 0.3730871Losses:  0.1112508550286293 0.25918853282928467
MemoryTrain:  epoch  7, batch     7 | loss: 0.3704394Losses:  0.0197127778083086 0.39206624031066895
MemoryTrain:  epoch  8, batch     0 | loss: 0.4117790Losses:  0.024340704083442688 0.4681526720523834
MemoryTrain:  epoch  8, batch     1 | loss: 0.4924934Losses:  0.11667011678218842 0.3533822298049927
MemoryTrain:  epoch  8, batch     2 | loss: 0.4700524Losses:  0.04739511013031006 0.39884674549102783
MemoryTrain:  epoch  8, batch     3 | loss: 0.4462419Losses:  0.030496081337332726 0.4821087121963501
MemoryTrain:  epoch  8, batch     4 | loss: 0.5126048Losses:  0.05612361058592796 0.5465644598007202
MemoryTrain:  epoch  8, batch     5 | loss: 0.6026881Losses:  0.12887316942214966 0.41367846727371216
MemoryTrain:  epoch  8, batch     6 | loss: 0.5425516Losses:  0.08729392290115356 0.14275509119033813
MemoryTrain:  epoch  8, batch     7 | loss: 0.2300490Losses:  0.02826743945479393 0.3297445774078369
MemoryTrain:  epoch  9, batch     0 | loss: 0.3580120Losses:  0.06941704452037811 0.3496417999267578
MemoryTrain:  epoch  9, batch     1 | loss: 0.4190589Losses:  0.038789305835962296 0.4654221534729004
MemoryTrain:  epoch  9, batch     2 | loss: 0.5042115Losses:  0.030359020456671715 0.5585105419158936
MemoryTrain:  epoch  9, batch     3 | loss: 0.5888696Losses:  0.02763422578573227 0.4062732458114624
MemoryTrain:  epoch  9, batch     4 | loss: 0.4339075Losses:  0.043553560972213745 0.46772223711013794
MemoryTrain:  epoch  9, batch     5 | loss: 0.5112758Losses:  0.041414204984903336 0.3491228222846985
MemoryTrain:  epoch  9, batch     6 | loss: 0.3905370Losses:  0.04840461537241936 0.2367498278617859
MemoryTrain:  epoch  9, batch     7 | loss: 0.2851544
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 78.25%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 77.89%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 77.46%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.69%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.21%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 88.41%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 87.83%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 86.96%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 86.23%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 85.86%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.58%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 85.22%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 85.06%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 84.66%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 84.24%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 83.79%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.29%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 84.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.62%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 84.17%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 84.02%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 83.06%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.49%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 82.04%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 81.60%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 81.32%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 80.98%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 80.65%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.12%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 79.87%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 79.49%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 79.32%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 78.95%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 78.79%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 77.97%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 77.70%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 77.25%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 76.98%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 76.73%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 76.53%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 76.11%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.52%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.94%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.38%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.87%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.49%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.29%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 74.12%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 73.92%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 73.69%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 73.65%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 73.28%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 73.14%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.95%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 73.52%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.21%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 72.87%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.67%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.47%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.81%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.37%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 71.94%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.75%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 71.36%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 71.46%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 71.60%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 71.20%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 70.93%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 70.55%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 70.21%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 70.20%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 71.13%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 70.99%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 70.73%   [EVAL] batch:  196 | acc: 18.75%,  total acc: 70.46%   [EVAL] batch:  197 | acc: 12.50%,  total acc: 70.17%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 69.91%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 69.69%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 69.77%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 69.87%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 69.72%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 69.23%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 69.80%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 69.67%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.29%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.49%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 72.18%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 72.14%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 72.03%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 72.42%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 72.27%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 72.21%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 72.13%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 72.01%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 71.91%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.52%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 71.34%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 71.13%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 70.88%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 70.98%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 70.95%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 70.92%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 71.90%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 71.87%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 72.13%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 72.00%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 71.92%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.87%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 72.42%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  341 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 72.67%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 72.93%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 72.83%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 72.69%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 72.54%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 72.42%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 72.85%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 72.84%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:  366 | acc: 87.50%,  total acc: 72.85%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 72.75%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 72.83%   
cur_acc:  ['0.9454', '0.7093', '0.7431', '0.7817', '0.7431', '0.7669']
his_acc:  ['0.9454', '0.8250', '0.8072', '0.7788', '0.7634', '0.7283']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  5.913169860839844 0.9504424333572388
CurrentTrain: epoch  0, batch     0 | loss: 6.8636122Losses:  5.719813346862793 1.3890072107315063
CurrentTrain: epoch  0, batch     1 | loss: 7.1088204Losses:  7.291937351226807 0.8526093363761902
CurrentTrain: epoch  0, batch     2 | loss: 8.1445465Losses:  6.600644111633301 0.7974499464035034
CurrentTrain: epoch  0, batch     3 | loss: 7.3980942Losses:  6.497984886169434 1.4741325378417969
CurrentTrain: epoch  0, batch     4 | loss: 7.9721174Losses:  6.228110313415527 1.2173457145690918
CurrentTrain: epoch  0, batch     5 | loss: 7.4454560Losses:  7.539633274078369 0.3756811022758484
CurrentTrain: epoch  0, batch     6 | loss: 7.9153142Losses:  5.199825763702393 1.2835625410079956
CurrentTrain: epoch  1, batch     0 | loss: 6.4833884Losses:  5.4821953773498535 1.3724870681762695
CurrentTrain: epoch  1, batch     1 | loss: 6.8546824Losses:  6.049921035766602 1.2573792934417725
CurrentTrain: epoch  1, batch     2 | loss: 7.3073006Losses:  5.066601753234863 1.0657190084457397
CurrentTrain: epoch  1, batch     3 | loss: 6.1323209Losses:  4.560791969299316 1.1683284044265747
CurrentTrain: epoch  1, batch     4 | loss: 5.7291203Losses:  4.8686442375183105 1.1938343048095703
CurrentTrain: epoch  1, batch     5 | loss: 6.0624785Losses:  6.69681453704834 0.41003644466400146
CurrentTrain: epoch  1, batch     6 | loss: 7.1068511Losses:  3.8700497150421143 1.1865410804748535
CurrentTrain: epoch  2, batch     0 | loss: 5.0565910Losses:  5.01798152923584 1.3502379655838013
CurrentTrain: epoch  2, batch     1 | loss: 6.3682194Losses:  4.227235794067383 0.9947315454483032
CurrentTrain: epoch  2, batch     2 | loss: 5.2219672Losses:  4.683854103088379 1.1583664417266846
CurrentTrain: epoch  2, batch     3 | loss: 5.8422203Losses:  4.8392744064331055 1.2606799602508545
CurrentTrain: epoch  2, batch     4 | loss: 6.0999546Losses:  4.6904377937316895 0.9859451055526733
CurrentTrain: epoch  2, batch     5 | loss: 5.6763830Losses:  5.447956085205078 0.33998292684555054
CurrentTrain: epoch  2, batch     6 | loss: 5.7879391Losses:  4.050805568695068 0.9466597437858582
CurrentTrain: epoch  3, batch     0 | loss: 4.9974651Losses:  5.444207191467285 1.340437650680542
CurrentTrain: epoch  3, batch     1 | loss: 6.7846451Losses:  4.03255558013916 1.2764363288879395
CurrentTrain: epoch  3, batch     2 | loss: 5.3089919Losses:  4.182207107543945 0.9044961929321289
CurrentTrain: epoch  3, batch     3 | loss: 5.0867033Losses:  3.9292638301849365 0.8952977061271667
CurrentTrain: epoch  3, batch     4 | loss: 4.8245616Losses:  3.4526314735412598 1.0605604648590088
CurrentTrain: epoch  3, batch     5 | loss: 4.5131922Losses:  2.743795871734619 0.1340750753879547
CurrentTrain: epoch  3, batch     6 | loss: 2.8778710Losses:  3.4713516235351562 0.9990383982658386
CurrentTrain: epoch  4, batch     0 | loss: 4.4703898Losses:  4.76005744934082 1.1242146492004395
CurrentTrain: epoch  4, batch     1 | loss: 5.8842721Losses:  2.7716217041015625 0.8222697377204895
CurrentTrain: epoch  4, batch     2 | loss: 3.5938914Losses:  3.3984436988830566 0.8359724879264832
CurrentTrain: epoch  4, batch     3 | loss: 4.2344160Losses:  3.3640918731689453 0.9554259181022644
CurrentTrain: epoch  4, batch     4 | loss: 4.3195176Losses:  3.4625515937805176 1.1530438661575317
CurrentTrain: epoch  4, batch     5 | loss: 4.6155953Losses:  5.801061630249023 0.47961390018463135
CurrentTrain: epoch  4, batch     6 | loss: 6.2806754Losses:  3.260530948638916 0.7979096174240112
CurrentTrain: epoch  5, batch     0 | loss: 4.0584407Losses:  4.205765247344971 1.142142415046692
CurrentTrain: epoch  5, batch     1 | loss: 5.3479075Losses:  2.846947193145752 0.8681305646896362
CurrentTrain: epoch  5, batch     2 | loss: 3.7150779Losses:  3.5095157623291016 0.9057122468948364
CurrentTrain: epoch  5, batch     3 | loss: 4.4152279Losses:  2.816495895385742 0.8431705236434937
CurrentTrain: epoch  5, batch     4 | loss: 3.6596665Losses:  3.248690366744995 1.1998008489608765
CurrentTrain: epoch  5, batch     5 | loss: 4.4484911Losses:  2.2755544185638428 0.20555752515792847
CurrentTrain: epoch  5, batch     6 | loss: 2.4811120Losses:  3.0514090061187744 1.0045937299728394
CurrentTrain: epoch  6, batch     0 | loss: 4.0560026Losses:  3.280205488204956 0.8846269249916077
CurrentTrain: epoch  6, batch     1 | loss: 4.1648326Losses:  3.056565761566162 0.6567342281341553
CurrentTrain: epoch  6, batch     2 | loss: 3.7133000Losses:  2.9277336597442627 0.9719246029853821
CurrentTrain: epoch  6, batch     3 | loss: 3.8996582Losses:  2.335237503051758 0.5797237753868103
CurrentTrain: epoch  6, batch     4 | loss: 2.9149613Losses:  2.660581588745117 0.6607204675674438
CurrentTrain: epoch  6, batch     5 | loss: 3.3213019Losses:  3.5342841148376465 0.16838032007217407
CurrentTrain: epoch  6, batch     6 | loss: 3.7026644Losses:  2.2949624061584473 0.6619993448257446
CurrentTrain: epoch  7, batch     0 | loss: 2.9569616Losses:  2.6484084129333496 0.8465812802314758
CurrentTrain: epoch  7, batch     1 | loss: 3.4949896Losses:  2.545793056488037 0.699126124382019
CurrentTrain: epoch  7, batch     2 | loss: 3.2449193Losses:  3.2059824466705322 0.7656072378158569
CurrentTrain: epoch  7, batch     3 | loss: 3.9715896Losses:  2.8805301189422607 0.9149044752120972
CurrentTrain: epoch  7, batch     4 | loss: 3.7954345Losses:  2.514338970184326 0.8046438694000244
CurrentTrain: epoch  7, batch     5 | loss: 3.3189828Losses:  2.4381320476531982 0.3884173035621643
CurrentTrain: epoch  7, batch     6 | loss: 2.8265493Losses:  2.4493300914764404 0.9555027484893799
CurrentTrain: epoch  8, batch     0 | loss: 3.4048328Losses:  2.9564414024353027 1.0365912914276123
CurrentTrain: epoch  8, batch     1 | loss: 3.9930327Losses:  2.325437545776367 0.6082159280776978
CurrentTrain: epoch  8, batch     2 | loss: 2.9336534Losses:  2.2541210651397705 0.5612828731536865
CurrentTrain: epoch  8, batch     3 | loss: 2.8154039Losses:  2.434288740158081 0.9424458742141724
CurrentTrain: epoch  8, batch     4 | loss: 3.3767347Losses:  2.2971322536468506 0.9706059694290161
CurrentTrain: epoch  8, batch     5 | loss: 3.2677383Losses:  2.340684413909912 0.18684148788452148
CurrentTrain: epoch  8, batch     6 | loss: 2.5275259Losses:  2.234332323074341 0.704218864440918
CurrentTrain: epoch  9, batch     0 | loss: 2.9385512Losses:  2.2144384384155273 0.7312018871307373
CurrentTrain: epoch  9, batch     1 | loss: 2.9456403Losses:  2.733384132385254 0.922864556312561
CurrentTrain: epoch  9, batch     2 | loss: 3.6562486Losses:  1.9269077777862549 0.6920535564422607
CurrentTrain: epoch  9, batch     3 | loss: 2.6189613Losses:  2.338794708251953 0.7667532563209534
CurrentTrain: epoch  9, batch     4 | loss: 3.1055479Losses:  2.112426996231079 0.689195990562439
CurrentTrain: epoch  9, batch     5 | loss: 2.8016229Losses:  2.5267834663391113 0.44018295407295227
CurrentTrain: epoch  9, batch     6 | loss: 2.9669664
Losses:  1.4173132181167603 0.6072200536727905
MemoryTrain:  epoch  0, batch     0 | loss: 2.0245333Losses:  0.5820015668869019 0.42780041694641113
MemoryTrain:  epoch  0, batch     1 | loss: 1.0098020Losses:  0.4500125050544739 0.9808294773101807
MemoryTrain:  epoch  0, batch     2 | loss: 1.4308419Losses:  0.11106075346469879 0.4605991840362549
MemoryTrain:  epoch  0, batch     3 | loss: 0.5716599Losses:  0.8154586553573608 0.47243595123291016
MemoryTrain:  epoch  0, batch     4 | loss: 1.2878946Losses:  1.358485460281372 0.7551781535148621
MemoryTrain:  epoch  0, batch     5 | loss: 2.1136637Losses:  0.2365044206380844 0.5086100101470947
MemoryTrain:  epoch  0, batch     6 | loss: 0.7451144Losses:  0.6299022436141968 0.5584207773208618
MemoryTrain:  epoch  0, batch     7 | loss: 1.1883230Losses:  0.05430297181010246 0.22858142852783203
MemoryTrain:  epoch  0, batch     8 | loss: 0.2828844Losses:  0.5458659529685974 0.36624738574028015
MemoryTrain:  epoch  1, batch     0 | loss: 0.9121133Losses:  0.13980893790721893 0.3379502296447754
MemoryTrain:  epoch  1, batch     1 | loss: 0.4777592Losses:  0.9747286438941956 0.3450448513031006
MemoryTrain:  epoch  1, batch     2 | loss: 1.3197734Losses:  0.4445347785949707 0.5561167001724243
MemoryTrain:  epoch  1, batch     3 | loss: 1.0006515Losses:  0.7213217616081238 0.4882013201713562
MemoryTrain:  epoch  1, batch     4 | loss: 1.2095231Losses:  0.3044220209121704 0.4702141582965851
MemoryTrain:  epoch  1, batch     5 | loss: 0.7746361Losses:  1.0238341093063354 0.642020583152771
MemoryTrain:  epoch  1, batch     6 | loss: 1.6658547Losses:  0.7425603866577148 0.6138723492622375
MemoryTrain:  epoch  1, batch     7 | loss: 1.3564327Losses:  0.25415948033332825 0.5580057501792908
MemoryTrain:  epoch  1, batch     8 | loss: 0.8121653Losses:  0.8525058627128601 0.42470288276672363
MemoryTrain:  epoch  2, batch     0 | loss: 1.2772088Losses:  0.04476054385304451 0.45041853189468384
MemoryTrain:  epoch  2, batch     1 | loss: 0.4951791Losses:  0.16905003786087036 0.511532187461853
MemoryTrain:  epoch  2, batch     2 | loss: 0.6805822Losses:  0.3482825756072998 0.2857514023780823
MemoryTrain:  epoch  2, batch     3 | loss: 0.6340340Losses:  0.12803351879119873 0.48742014169692993
MemoryTrain:  epoch  2, batch     4 | loss: 0.6154537Losses:  0.22658506035804749 0.6023527979850769
MemoryTrain:  epoch  2, batch     5 | loss: 0.8289379Losses:  0.771253764629364 0.5274847745895386
MemoryTrain:  epoch  2, batch     6 | loss: 1.2987385Losses:  0.46548551321029663 0.8238784670829773
MemoryTrain:  epoch  2, batch     7 | loss: 1.2893640Losses:  0.5677477121353149 0.415409654378891
MemoryTrain:  epoch  2, batch     8 | loss: 0.9831574Losses:  0.17222686111927032 0.5726149082183838
MemoryTrain:  epoch  3, batch     0 | loss: 0.7448418Losses:  0.1207064837217331 0.4595566987991333
MemoryTrain:  epoch  3, batch     1 | loss: 0.5802632Losses:  0.30122703313827515 0.4770093262195587
MemoryTrain:  epoch  3, batch     2 | loss: 0.7782364Losses:  0.07449778914451599 0.3029805123806
MemoryTrain:  epoch  3, batch     3 | loss: 0.3774783Losses:  0.17440293729305267 0.388683557510376
MemoryTrain:  epoch  3, batch     4 | loss: 0.5630865Losses:  0.05271859094500542 0.43369901180267334
MemoryTrain:  epoch  3, batch     5 | loss: 0.4864176Losses:  0.19296613335609436 0.588550865650177
MemoryTrain:  epoch  3, batch     6 | loss: 0.7815170Losses:  0.591243326663971 0.4690079689025879
MemoryTrain:  epoch  3, batch     7 | loss: 1.0602512Losses:  0.36504918336868286 0.40236735343933105
MemoryTrain:  epoch  3, batch     8 | loss: 0.7674165Losses:  0.04459838569164276 0.40311139822006226
MemoryTrain:  epoch  4, batch     0 | loss: 0.4477098Losses:  0.06712361425161362 0.3308165371417999
MemoryTrain:  epoch  4, batch     1 | loss: 0.3979402Losses:  0.29006996750831604 0.4610027074813843
MemoryTrain:  epoch  4, batch     2 | loss: 0.7510726Losses:  0.04132245481014252 0.407453328371048
MemoryTrain:  epoch  4, batch     3 | loss: 0.4487758Losses:  0.10209155082702637 0.3624902069568634
MemoryTrain:  epoch  4, batch     4 | loss: 0.4645818Losses:  0.05368456244468689 0.39433762431144714
MemoryTrain:  epoch  4, batch     5 | loss: 0.4480222Losses:  0.7101221680641174 0.7586265206336975
MemoryTrain:  epoch  4, batch     6 | loss: 1.4687487Losses:  0.17837926745414734 0.6395496129989624
MemoryTrain:  epoch  4, batch     7 | loss: 0.8179289Losses:  0.09225130081176758 0.5139893889427185
MemoryTrain:  epoch  4, batch     8 | loss: 0.6062407Losses:  0.0537123940885067 0.32534217834472656
MemoryTrain:  epoch  5, batch     0 | loss: 0.3790546Losses:  0.31071937084198 0.606931209564209
MemoryTrain:  epoch  5, batch     1 | loss: 0.9176506Losses:  0.040296610444784164 0.4842669367790222
MemoryTrain:  epoch  5, batch     2 | loss: 0.5245636Losses:  0.05900285765528679 0.4881249964237213
MemoryTrain:  epoch  5, batch     3 | loss: 0.5471278Losses:  0.05856005847454071 0.3445803225040436
MemoryTrain:  epoch  5, batch     4 | loss: 0.4031404Losses:  0.21694312989711761 0.5466394424438477
MemoryTrain:  epoch  5, batch     5 | loss: 0.7635826Losses:  0.0697459727525711 0.5057917833328247
MemoryTrain:  epoch  5, batch     6 | loss: 0.5755377Losses:  0.8261043429374695 0.44061142206192017
MemoryTrain:  epoch  5, batch     7 | loss: 1.2667158Losses:  0.03297858685255051 0.23632457852363586
MemoryTrain:  epoch  5, batch     8 | loss: 0.2693032Losses:  0.1133190393447876 0.5271081924438477
MemoryTrain:  epoch  6, batch     0 | loss: 0.6404272Losses:  0.04964367300271988 0.3661920428276062
MemoryTrain:  epoch  6, batch     1 | loss: 0.4158357Losses:  0.0489119291305542 0.3785550594329834
MemoryTrain:  epoch  6, batch     2 | loss: 0.4274670Losses:  0.09081797301769257 0.31441530585289
MemoryTrain:  epoch  6, batch     3 | loss: 0.4052333Losses:  0.09885705262422562 0.37616583704948425
MemoryTrain:  epoch  6, batch     4 | loss: 0.4750229Losses:  0.10382630676031113 0.5815732479095459
MemoryTrain:  epoch  6, batch     5 | loss: 0.6853995Losses:  0.0743682011961937 0.5259792804718018
MemoryTrain:  epoch  6, batch     6 | loss: 0.6003475Losses:  0.06740975379943848 0.49065399169921875
MemoryTrain:  epoch  6, batch     7 | loss: 0.5580637Losses:  0.4048856496810913 0.32819241285324097
MemoryTrain:  epoch  6, batch     8 | loss: 0.7330781Losses:  0.1589275449514389 0.3680437207221985
MemoryTrain:  epoch  7, batch     0 | loss: 0.5269713Losses:  0.08212007582187653 0.43047279119491577
MemoryTrain:  epoch  7, batch     1 | loss: 0.5125929Losses:  0.06853672862052917 0.4920198917388916
MemoryTrain:  epoch  7, batch     2 | loss: 0.5605567Losses:  0.058326371014118195 0.5298813581466675
MemoryTrain:  epoch  7, batch     3 | loss: 0.5882077Losses:  0.09440374374389648 0.6009210348129272
MemoryTrain:  epoch  7, batch     4 | loss: 0.6953248Losses:  0.06999927759170532 0.35994571447372437
MemoryTrain:  epoch  7, batch     5 | loss: 0.4299450Losses:  0.0558868870139122 0.26207005977630615
MemoryTrain:  epoch  7, batch     6 | loss: 0.3179570Losses:  0.09508299827575684 0.5081669092178345
MemoryTrain:  epoch  7, batch     7 | loss: 0.6032499Losses:  0.06772273033857346 0.2210359424352646
MemoryTrain:  epoch  7, batch     8 | loss: 0.2887587Losses:  0.11169151216745377 0.42277371883392334
MemoryTrain:  epoch  8, batch     0 | loss: 0.5344653Losses:  0.05584588646888733 0.3364628553390503
MemoryTrain:  epoch  8, batch     1 | loss: 0.3923087Losses:  0.03511959686875343 0.5098173022270203
MemoryTrain:  epoch  8, batch     2 | loss: 0.5449369Losses:  0.1024530902504921 0.6319841742515564
MemoryTrain:  epoch  8, batch     3 | loss: 0.7344373Losses:  0.05351296067237854 0.5378042459487915
MemoryTrain:  epoch  8, batch     4 | loss: 0.5913172Losses:  0.04153308644890785 0.44512736797332764
MemoryTrain:  epoch  8, batch     5 | loss: 0.4866605Losses:  0.16593854129314423 0.3647964596748352
MemoryTrain:  epoch  8, batch     6 | loss: 0.5307350Losses:  0.044851139187812805 0.44812971353530884
MemoryTrain:  epoch  8, batch     7 | loss: 0.4929808Losses:  0.09010398387908936 0.3385816514492035
MemoryTrain:  epoch  8, batch     8 | loss: 0.4286856Losses:  0.06669335067272186 0.5884877443313599
MemoryTrain:  epoch  9, batch     0 | loss: 0.6551811Losses:  0.04798278957605362 0.3985312581062317
MemoryTrain:  epoch  9, batch     1 | loss: 0.4465140Losses:  0.1323484182357788 0.3071494698524475
MemoryTrain:  epoch  9, batch     2 | loss: 0.4394979Losses:  0.06349831819534302 0.42919665575027466
MemoryTrain:  epoch  9, batch     3 | loss: 0.4926950Losses:  0.04537346586585045 0.692736029624939
MemoryTrain:  epoch  9, batch     4 | loss: 0.7381095Losses:  0.03694246709346771 0.33477339148521423
MemoryTrain:  epoch  9, batch     5 | loss: 0.3717158Losses:  0.03019838221371174 0.39569661021232605
MemoryTrain:  epoch  9, batch     6 | loss: 0.4258950Losses:  0.13504377007484436 0.35460758209228516
MemoryTrain:  epoch  9, batch     7 | loss: 0.4896514Losses:  0.07173095643520355 0.2594113349914551
MemoryTrain:  epoch  9, batch     8 | loss: 0.3311423
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 57.03%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 51.62%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 48.71%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 47.71%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 46.37%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 48.11%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 51.22%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 52.20%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 53.29%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 54.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 56.40%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 57.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 57.64%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 56.39%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 55.59%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 54.95%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 53.83%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 52.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 53.55%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 54.33%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 55.07%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 55.79%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 56.36%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 56.92%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 57.13%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 56.90%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 56.78%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 56.77%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 56.66%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 56.96%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 56.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.70%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.04%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 86.59%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 86.07%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 85.34%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 84.85%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 84.43%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 83.40%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 83.27%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 83.15%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 83.80%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 83.36%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 83.23%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 83.05%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 82.08%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 81.62%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 80.96%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 79.89%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.21%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 78.96%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 78.71%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 78.33%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 77.76%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 77.19%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 76.91%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 76.02%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 75.25%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 74.28%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 74.05%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 73.36%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.80%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 72.19%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 71.59%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.06%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 70.46%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.36%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 71.17%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.29%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.92%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 70.99%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 70.60%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.18%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 69.97%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 69.56%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 69.07%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 68.99%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 68.14%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 66.72%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.58%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 66.34%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 66.07%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 67.18%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 66.96%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 66.81%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 66.64%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 66.09%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 65.87%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 65.67%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 65.51%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 65.28%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 66.27%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 66.14%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.97%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 68.87%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 69.16%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.51%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.16%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.92%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 68.10%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 68.95%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 68.85%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 68.73%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 68.55%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 68.44%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.79%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 68.81%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 68.79%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 68.79%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  341 | acc: 87.50%,  total acc: 69.55%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 69.78%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 69.54%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.40%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 69.26%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.66%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 69.57%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 69.50%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 69.55%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 69.50%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 69.46%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 69.39%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 69.60%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 69.62%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 69.40%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 69.27%   [EVAL] batch:  396 | acc: 12.50%,  total acc: 69.13%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 68.99%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 68.83%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 68.67%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 68.52%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.38%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.22%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 68.08%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 67.96%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.81%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  419 | acc: 12.50%,  total acc: 68.30%   [EVAL] batch:  420 | acc: 0.00%,  total acc: 68.14%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 68.02%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:  423 | acc: 0.00%,  total acc: 67.76%   [EVAL] batch:  424 | acc: 0.00%,  total acc: 67.60%   [EVAL] batch:  425 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  431 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 67.88%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 67.82%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 67.78%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 67.71%   
cur_acc:  ['0.9454', '0.7093', '0.7431', '0.7817', '0.7431', '0.7669', '0.5655']
his_acc:  ['0.9454', '0.8250', '0.8072', '0.7788', '0.7634', '0.7283', '0.6771']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  5.682424545288086 1.0465353727340698
CurrentTrain: epoch  0, batch     0 | loss: 6.7289600Losses:  6.1869354248046875 1.2698503732681274
CurrentTrain: epoch  0, batch     1 | loss: 7.4567857Losses:  6.256709098815918 1.2418886423110962
CurrentTrain: epoch  0, batch     2 | loss: 7.4985976Losses:  5.939968585968018 1.385613203048706
CurrentTrain: epoch  0, batch     3 | loss: 7.3255816Losses:  6.024395942687988 0.9409657716751099
CurrentTrain: epoch  0, batch     4 | loss: 6.9653616Losses:  5.277796745300293 0.7382251024246216
CurrentTrain: epoch  0, batch     5 | loss: 6.0160217Losses:  5.882102966308594 0.251051127910614
CurrentTrain: epoch  0, batch     6 | loss: 6.1331539Losses:  4.1177449226379395 0.9276475310325623
CurrentTrain: epoch  1, batch     0 | loss: 5.0453925Losses:  5.412968635559082 0.7786571383476257
CurrentTrain: epoch  1, batch     1 | loss: 6.1916256Losses:  5.99452018737793 1.117152452468872
CurrentTrain: epoch  1, batch     2 | loss: 7.1116724Losses:  4.520766735076904 1.2465400695800781
CurrentTrain: epoch  1, batch     3 | loss: 5.7673068Losses:  4.236268043518066 0.8631925582885742
CurrentTrain: epoch  1, batch     4 | loss: 5.0994606Losses:  6.263716697692871 1.027265191078186
CurrentTrain: epoch  1, batch     5 | loss: 7.2909818Losses:  2.3038198947906494 8.94069742685133e-08
CurrentTrain: epoch  1, batch     6 | loss: 2.3038199Losses:  4.292605400085449 1.023900032043457
CurrentTrain: epoch  2, batch     0 | loss: 5.3165054Losses:  4.255765914916992 0.5591253638267517
CurrentTrain: epoch  2, batch     1 | loss: 4.8148913Losses:  4.626384735107422 0.8162965178489685
CurrentTrain: epoch  2, batch     2 | loss: 5.4426813Losses:  5.143266677856445 0.8931280374526978
CurrentTrain: epoch  2, batch     3 | loss: 6.0363946Losses:  5.552700996398926 0.803194522857666
CurrentTrain: epoch  2, batch     4 | loss: 6.3558955Losses:  3.6238210201263428 0.726456344127655
CurrentTrain: epoch  2, batch     5 | loss: 4.3502774Losses:  3.35153865814209 0.2666059136390686
CurrentTrain: epoch  2, batch     6 | loss: 3.6181445Losses:  4.521866321563721 1.1207542419433594
CurrentTrain: epoch  3, batch     0 | loss: 5.6426206Losses:  4.262595176696777 0.9763190746307373
CurrentTrain: epoch  3, batch     1 | loss: 5.2389145Losses:  5.108486652374268 0.6818583011627197
CurrentTrain: epoch  3, batch     2 | loss: 5.7903452Losses:  3.723602056503296 0.7564101815223694
CurrentTrain: epoch  3, batch     3 | loss: 4.4800124Losses:  3.2420315742492676 0.5667972564697266
CurrentTrain: epoch  3, batch     4 | loss: 3.8088288Losses:  4.3078765869140625 0.997398853302002
CurrentTrain: epoch  3, batch     5 | loss: 5.3052754Losses:  4.391916275024414 0.5656786561012268
CurrentTrain: epoch  3, batch     6 | loss: 4.9575949Losses:  3.3335764408111572 0.541736364364624
CurrentTrain: epoch  4, batch     0 | loss: 3.8753128Losses:  3.786229133605957 0.8514552116394043
CurrentTrain: epoch  4, batch     1 | loss: 4.6376843Losses:  3.651146411895752 0.7768042087554932
CurrentTrain: epoch  4, batch     2 | loss: 4.4279509Losses:  2.74202823638916 0.7281460165977478
CurrentTrain: epoch  4, batch     3 | loss: 3.4701743Losses:  5.5744428634643555 0.9585667848587036
CurrentTrain: epoch  4, batch     4 | loss: 6.5330095Losses:  2.9074134826660156 0.786126971244812
CurrentTrain: epoch  4, batch     5 | loss: 3.6935406Losses:  4.683229446411133 0.2644268274307251
CurrentTrain: epoch  4, batch     6 | loss: 4.9476562Losses:  3.2263598442077637 0.962242603302002
CurrentTrain: epoch  5, batch     0 | loss: 4.1886024Losses:  3.9203381538391113 0.6783867478370667
CurrentTrain: epoch  5, batch     1 | loss: 4.5987248Losses:  4.569039344787598 0.609377384185791
CurrentTrain: epoch  5, batch     2 | loss: 5.1784167Losses:  3.5323057174682617 0.863092303276062
CurrentTrain: epoch  5, batch     3 | loss: 4.3953981Losses:  2.5357890129089355 0.5606496334075928
CurrentTrain: epoch  5, batch     4 | loss: 3.0964386Losses:  3.1593494415283203 0.7927003502845764
CurrentTrain: epoch  5, batch     5 | loss: 3.9520497Losses:  1.9990015029907227 0.13964691758155823
CurrentTrain: epoch  5, batch     6 | loss: 2.1386485Losses:  3.01400089263916 1.0064667463302612
CurrentTrain: epoch  6, batch     0 | loss: 4.0204678Losses:  3.562392234802246 0.7605783939361572
CurrentTrain: epoch  6, batch     1 | loss: 4.3229704Losses:  3.8420212268829346 0.5485800504684448
CurrentTrain: epoch  6, batch     2 | loss: 4.3906012Losses:  2.991654396057129 0.846364438533783
CurrentTrain: epoch  6, batch     3 | loss: 3.8380189Losses:  2.540712833404541 0.4101596474647522
CurrentTrain: epoch  6, batch     4 | loss: 2.9508724Losses:  2.8822011947631836 0.5040410161018372
CurrentTrain: epoch  6, batch     5 | loss: 3.3862422Losses:  2.277316093444824 0.27701517939567566
CurrentTrain: epoch  6, batch     6 | loss: 2.5543313Losses:  2.6998584270477295 0.5438293814659119
CurrentTrain: epoch  7, batch     0 | loss: 3.2436879Losses:  3.364534616470337 0.8084330558776855
CurrentTrain: epoch  7, batch     1 | loss: 4.1729679Losses:  2.4159045219421387 0.5079646110534668
CurrentTrain: epoch  7, batch     2 | loss: 2.9238691Losses:  2.8691930770874023 0.5744808912277222
CurrentTrain: epoch  7, batch     3 | loss: 3.4436741Losses:  2.513052463531494 0.6862413883209229
CurrentTrain: epoch  7, batch     4 | loss: 3.1992939Losses:  2.3456594944000244 0.6640366911888123
CurrentTrain: epoch  7, batch     5 | loss: 3.0096962Losses:  2.166348934173584 0.1527959704399109
CurrentTrain: epoch  7, batch     6 | loss: 2.3191450Losses:  2.309943199157715 0.6099796295166016
CurrentTrain: epoch  8, batch     0 | loss: 2.9199228Losses:  3.041795253753662 0.4029248058795929
CurrentTrain: epoch  8, batch     1 | loss: 3.4447200Losses:  2.2208337783813477 0.5267360210418701
CurrentTrain: epoch  8, batch     2 | loss: 2.7475698Losses:  2.210636854171753 0.6111856698989868
CurrentTrain: epoch  8, batch     3 | loss: 2.8218226Losses:  2.1601779460906982 0.6062372922897339
CurrentTrain: epoch  8, batch     4 | loss: 2.7664151Losses:  2.245077610015869 0.5877621173858643
CurrentTrain: epoch  8, batch     5 | loss: 2.8328397Losses:  2.7556838989257812 0.09244678914546967
CurrentTrain: epoch  8, batch     6 | loss: 2.8481307Losses:  2.105807065963745 0.5220764875411987
CurrentTrain: epoch  9, batch     0 | loss: 2.6278834Losses:  2.0156631469726562 0.36455684900283813
CurrentTrain: epoch  9, batch     1 | loss: 2.3802199Losses:  2.0957891941070557 0.6486141085624695
CurrentTrain: epoch  9, batch     2 | loss: 2.7444034Losses:  2.2772376537323 0.681675374507904
CurrentTrain: epoch  9, batch     3 | loss: 2.9589131Losses:  2.245452404022217 0.6175588369369507
CurrentTrain: epoch  9, batch     4 | loss: 2.8630114Losses:  2.0843586921691895 0.540778398513794
CurrentTrain: epoch  9, batch     5 | loss: 2.6251371Losses:  2.469667434692383 0.16174165904521942
CurrentTrain: epoch  9, batch     6 | loss: 2.6314092
Losses:  0.10465071350336075 0.30093544721603394
MemoryTrain:  epoch  0, batch     0 | loss: 0.4055862Losses:  0.46133488416671753 0.49729102849960327
MemoryTrain:  epoch  0, batch     1 | loss: 0.9586259Losses:  0.7728738784790039 0.5362934470176697
MemoryTrain:  epoch  0, batch     2 | loss: 1.3091674Losses:  0.508298397064209 0.6607497334480286
MemoryTrain:  epoch  0, batch     3 | loss: 1.1690481Losses:  0.16903147101402283 0.5651286244392395
MemoryTrain:  epoch  0, batch     4 | loss: 0.7341601Losses:  0.3131996989250183 0.8518223762512207
MemoryTrain:  epoch  0, batch     5 | loss: 1.1650221Losses:  0.35510024428367615 0.462969034910202
MemoryTrain:  epoch  0, batch     6 | loss: 0.8180693Losses:  0.16563166677951813 0.5201267004013062
MemoryTrain:  epoch  0, batch     7 | loss: 0.6857584Losses:  0.5555581450462341 0.372443825006485
MemoryTrain:  epoch  0, batch     8 | loss: 0.9280020Losses:  0.22823986411094666 0.5246092677116394
MemoryTrain:  epoch  0, batch     9 | loss: 0.7528491Losses:  1.0874624252319336 0.3936872184276581
MemoryTrain:  epoch  1, batch     0 | loss: 1.4811497Losses:  0.6421670317649841 0.6079609990119934
MemoryTrain:  epoch  1, batch     1 | loss: 1.2501280Losses:  0.1942180097103119 0.391695499420166
MemoryTrain:  epoch  1, batch     2 | loss: 0.5859135Losses:  0.16301125288009644 0.38221967220306396
MemoryTrain:  epoch  1, batch     3 | loss: 0.5452309Losses:  0.1523323357105255 0.5131833553314209
MemoryTrain:  epoch  1, batch     4 | loss: 0.6655157Losses:  0.6007969379425049 0.4407227039337158
MemoryTrain:  epoch  1, batch     5 | loss: 1.0415196Losses:  0.5921186208724976 0.3939412534236908
MemoryTrain:  epoch  1, batch     6 | loss: 0.9860599Losses:  0.3401309847831726 0.7830040454864502
MemoryTrain:  epoch  1, batch     7 | loss: 1.1231351Losses:  0.21528178453445435 0.5975631475448608
MemoryTrain:  epoch  1, batch     8 | loss: 0.8128449Losses:  0.31457772850990295 0.43253111839294434
MemoryTrain:  epoch  1, batch     9 | loss: 0.7471088Losses:  0.3035750985145569 0.42147645354270935
MemoryTrain:  epoch  2, batch     0 | loss: 0.7250515Losses:  0.1174442321062088 0.3214205801486969
MemoryTrain:  epoch  2, batch     1 | loss: 0.4388648Losses:  0.4281907081604004 0.5401841402053833
MemoryTrain:  epoch  2, batch     2 | loss: 0.9683748Losses:  0.27071714401245117 0.6412234306335449
MemoryTrain:  epoch  2, batch     3 | loss: 0.9119406Losses:  0.10544956475496292 0.3975266218185425
MemoryTrain:  epoch  2, batch     4 | loss: 0.5029762Losses:  0.14232414960861206 0.5416505336761475
MemoryTrain:  epoch  2, batch     5 | loss: 0.6839747Losses:  0.080232173204422 0.5597497820854187
MemoryTrain:  epoch  2, batch     6 | loss: 0.6399820Losses:  0.10158224403858185 0.32583004236221313
MemoryTrain:  epoch  2, batch     7 | loss: 0.4274123Losses:  0.17167870700359344 0.36185377836227417
MemoryTrain:  epoch  2, batch     8 | loss: 0.5335325Losses:  0.06777506321668625 0.317182719707489
MemoryTrain:  epoch  2, batch     9 | loss: 0.3849578Losses:  0.1326047033071518 0.452467143535614
MemoryTrain:  epoch  3, batch     0 | loss: 0.5850719Losses:  0.07538409531116486 0.5513819456100464
MemoryTrain:  epoch  3, batch     1 | loss: 0.6267660Losses:  0.13067041337490082 0.4545370936393738
MemoryTrain:  epoch  3, batch     2 | loss: 0.5852075Losses:  0.20137739181518555 0.5283474922180176
MemoryTrain:  epoch  3, batch     3 | loss: 0.7297249Losses:  0.113345667719841 0.29461997747421265
MemoryTrain:  epoch  3, batch     4 | loss: 0.4079657Losses:  0.057735592126846313 0.4485511779785156
MemoryTrain:  epoch  3, batch     5 | loss: 0.5062867Losses:  0.07812412828207016 0.3972738981246948
MemoryTrain:  epoch  3, batch     6 | loss: 0.4753980Losses:  1.3863719701766968 0.47279366850852966
MemoryTrain:  epoch  3, batch     7 | loss: 1.8591657Losses:  0.10499202460050583 0.23556211590766907
MemoryTrain:  epoch  3, batch     8 | loss: 0.3405541Losses:  0.21149782836437225 0.5857359170913696
MemoryTrain:  epoch  3, batch     9 | loss: 0.7972338Losses:  0.09228374063968658 0.34770962595939636
MemoryTrain:  epoch  4, batch     0 | loss: 0.4399934Losses:  0.13318343460559845 0.4126465916633606
MemoryTrain:  epoch  4, batch     1 | loss: 0.5458300Losses:  0.05688928812742233 0.318254679441452
MemoryTrain:  epoch  4, batch     2 | loss: 0.3751440Losses:  0.07554354518651962 0.44007548689842224
MemoryTrain:  epoch  4, batch     3 | loss: 0.5156190Losses:  0.6408613920211792 0.5106149911880493
MemoryTrain:  epoch  4, batch     4 | loss: 1.1514764Losses:  0.0992516353726387 0.3241267204284668
MemoryTrain:  epoch  4, batch     5 | loss: 0.4233783Losses:  0.09385000914335251 0.4863892197608948
MemoryTrain:  epoch  4, batch     6 | loss: 0.5802392Losses:  0.07006197422742844 0.3984498083591461
MemoryTrain:  epoch  4, batch     7 | loss: 0.4685118Losses:  0.07760979980230331 0.5396357178688049
MemoryTrain:  epoch  4, batch     8 | loss: 0.6172455Losses:  0.13606978952884674 0.6291877627372742
MemoryTrain:  epoch  4, batch     9 | loss: 0.7652575Losses:  0.02794291265308857 0.217551127076149
MemoryTrain:  epoch  5, batch     0 | loss: 0.2454940Losses:  0.055868521332740784 0.3821443021297455
MemoryTrain:  epoch  5, batch     1 | loss: 0.4380128Losses:  0.09526383876800537 0.43524616956710815
MemoryTrain:  epoch  5, batch     2 | loss: 0.5305100Losses:  0.04382847994565964 0.49146974086761475
MemoryTrain:  epoch  5, batch     3 | loss: 0.5352982Losses:  0.1365443766117096 0.5563659071922302
MemoryTrain:  epoch  5, batch     4 | loss: 0.6929103Losses:  0.05350753664970398 0.30092552304267883
MemoryTrain:  epoch  5, batch     5 | loss: 0.3544331Losses:  0.05463353544473648 0.3396979570388794
MemoryTrain:  epoch  5, batch     6 | loss: 0.3943315Losses:  0.271109402179718 0.7164449691772461
MemoryTrain:  epoch  5, batch     7 | loss: 0.9875544Losses:  0.0806635320186615 0.39994966983795166
MemoryTrain:  epoch  5, batch     8 | loss: 0.4806132Losses:  0.1464429497718811 0.4156818985939026
MemoryTrain:  epoch  5, batch     9 | loss: 0.5621248Losses:  0.0809103474020958 0.44407764077186584
MemoryTrain:  epoch  6, batch     0 | loss: 0.5249880Losses:  0.08889174461364746 0.3961348533630371
MemoryTrain:  epoch  6, batch     1 | loss: 0.4850266Losses:  0.03830400109291077 0.34139275550842285
MemoryTrain:  epoch  6, batch     2 | loss: 0.3796968Losses:  0.05061747878789902 0.23536181449890137
MemoryTrain:  epoch  6, batch     3 | loss: 0.2859793Losses:  0.06886311620473862 0.48069432377815247
MemoryTrain:  epoch  6, batch     4 | loss: 0.5495574Losses:  0.18328207731246948 0.37991389632225037
MemoryTrain:  epoch  6, batch     5 | loss: 0.5631959Losses:  0.09681156277656555 0.65111243724823
MemoryTrain:  epoch  6, batch     6 | loss: 0.7479240Losses:  0.08363566547632217 0.5149129033088684
MemoryTrain:  epoch  6, batch     7 | loss: 0.5985486Losses:  0.021360550075769424 0.35003378987312317
MemoryTrain:  epoch  6, batch     8 | loss: 0.3713943Losses:  0.08547947555780411 0.4367213845252991
MemoryTrain:  epoch  6, batch     9 | loss: 0.5222009Losses:  0.10923793911933899 0.5120050311088562
MemoryTrain:  epoch  7, batch     0 | loss: 0.6212430Losses:  0.04859317094087601 0.489358127117157
MemoryTrain:  epoch  7, batch     1 | loss: 0.5379513Losses:  0.04830155894160271 0.38078516721725464
MemoryTrain:  epoch  7, batch     2 | loss: 0.4290867Losses:  0.06843958795070648 0.4563165009021759
MemoryTrain:  epoch  7, batch     3 | loss: 0.5247561Losses:  0.02943350374698639 0.26083534955978394
MemoryTrain:  epoch  7, batch     4 | loss: 0.2902688Losses:  0.047056153416633606 0.3900243639945984
MemoryTrain:  epoch  7, batch     5 | loss: 0.4370805Losses:  0.046210017055273056 0.5035115480422974
MemoryTrain:  epoch  7, batch     6 | loss: 0.5497215Losses:  0.07644741237163544 0.4777927100658417
MemoryTrain:  epoch  7, batch     7 | loss: 0.5542401Losses:  0.03929051384329796 0.30920809507369995
MemoryTrain:  epoch  7, batch     8 | loss: 0.3484986Losses:  0.050769366323947906 0.37511032819747925
MemoryTrain:  epoch  7, batch     9 | loss: 0.4258797Losses:  0.040435753762722015 0.5130281448364258
MemoryTrain:  epoch  8, batch     0 | loss: 0.5534639Losses:  0.03751077502965927 0.3516136407852173
MemoryTrain:  epoch  8, batch     1 | loss: 0.3891244Losses:  0.02916320040822029 0.47004276514053345
MemoryTrain:  epoch  8, batch     2 | loss: 0.4992060Losses:  0.3235616385936737 0.349864661693573
MemoryTrain:  epoch  8, batch     3 | loss: 0.6734263Losses:  0.09654327481985092 0.33346086740493774
MemoryTrain:  epoch  8, batch     4 | loss: 0.4300041Losses:  0.04771074652671814 0.3846004009246826
MemoryTrain:  epoch  8, batch     5 | loss: 0.4323111Losses:  0.05933286249637604 0.5182821154594421
MemoryTrain:  epoch  8, batch     6 | loss: 0.5776150Losses:  0.029613293707370758 0.33277660608291626
MemoryTrain:  epoch  8, batch     7 | loss: 0.3623899Losses:  0.0867559164762497 0.3924177885055542
MemoryTrain:  epoch  8, batch     8 | loss: 0.4791737Losses:  0.03391977772116661 0.3696480989456177
MemoryTrain:  epoch  8, batch     9 | loss: 0.4035679Losses:  0.1251777708530426 0.436107337474823
MemoryTrain:  epoch  9, batch     0 | loss: 0.5612851Losses:  0.03548556566238403 0.3148750066757202
MemoryTrain:  epoch  9, batch     1 | loss: 0.3503606Losses:  0.04127015173435211 0.39265504479408264
MemoryTrain:  epoch  9, batch     2 | loss: 0.4339252Losses:  0.11957176774740219 0.40417784452438354
MemoryTrain:  epoch  9, batch     3 | loss: 0.5237496Losses:  0.10760895907878876 0.6258310079574585
MemoryTrain:  epoch  9, batch     4 | loss: 0.7334400Losses:  0.06563232094049454 0.39501944184303284
MemoryTrain:  epoch  9, batch     5 | loss: 0.4606518Losses:  0.038096293807029724 0.3267222046852112
MemoryTrain:  epoch  9, batch     6 | loss: 0.3648185Losses:  0.0848124772310257 0.26233601570129395
MemoryTrain:  epoch  9, batch     7 | loss: 0.3471485Losses:  0.030549349263310432 0.43094414472579956
MemoryTrain:  epoch  9, batch     8 | loss: 0.4614935Losses:  0.049964144825935364 0.34618717432022095
MemoryTrain:  epoch  9, batch     9 | loss: 0.3961513
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 71.73%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 73.78%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 72.53%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 68.54%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 67.50%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 65.42%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 64.38%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.46%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.70%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 85.73%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.45%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 84.87%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 84.16%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 83.47%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.09%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.86%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 82.74%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 82.71%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 82.69%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.48%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.47%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 83.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 82.39%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 82.37%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 82.28%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 82.34%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 82.09%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 81.78%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 81.47%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 81.03%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 80.32%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.78%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 79.44%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 78.87%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 78.43%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 77.70%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 76.85%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 76.77%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 76.11%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 75.86%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 75.36%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 74.88%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 74.65%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 74.18%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.61%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 72.99%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.39%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.85%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 71.37%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.18%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 72.45%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 72.16%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 71.98%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 71.70%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.26%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 71.17%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.16%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 70.86%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 70.12%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 69.81%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.95%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 69.57%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.12%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.71%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.27%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.83%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  157 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 67.87%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 67.73%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 67.47%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 67.22%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.16%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 66.99%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 66.61%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.47%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 66.24%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 65.47%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 65.20%   [EVAL] batch:  178 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 64.79%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 64.64%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 65.78%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 65.47%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 65.20%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 64.98%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 64.75%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 64.47%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 64.07%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 63.80%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 65.11%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 65.03%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 67.51%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 67.48%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 67.49%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 67.42%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 67.38%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 67.29%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 67.21%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 67.91%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 67.74%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 67.48%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 67.40%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.12%   [EVAL] batch:  284 | acc: 6.25%,  total acc: 66.91%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 66.72%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 66.49%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 67.52%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 67.40%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 67.11%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 66.97%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 67.04%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 67.21%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 67.16%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.01%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 66.92%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 68.18%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 67.80%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 67.71%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 68.13%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.87%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 67.69%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 67.59%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 67.51%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 67.47%   [EVAL] batch:  380 | acc: 43.75%,  total acc: 67.40%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  393 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:  394 | acc: 6.25%,  total acc: 67.50%   [EVAL] batch:  395 | acc: 0.00%,  total acc: 67.33%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 67.16%   [EVAL] batch:  397 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  398 | acc: 0.00%,  total acc: 66.82%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 66.67%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 66.04%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 65.75%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:  420 | acc: 25.00%,  total acc: 66.27%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 66.17%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  423 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  424 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 66.09%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 65.96%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  438 | acc: 68.75%,  total acc: 65.96%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  440 | acc: 81.25%,  total acc: 66.01%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  446 | acc: 50.00%,  total acc: 66.04%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  448 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 66.26%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 66.20%   [EVAL] batch:  458 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  459 | acc: 37.50%,  total acc: 66.10%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 66.02%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  465 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  476 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 66.73%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 66.60%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  486 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  487 | acc: 50.00%,  total acc: 66.47%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:  491 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  492 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  494 | acc: 0.00%,  total acc: 66.44%   [EVAL] batch:  495 | acc: 6.25%,  total acc: 66.32%   [EVAL] batch:  496 | acc: 6.25%,  total acc: 66.20%   [EVAL] batch:  497 | acc: 0.00%,  total acc: 66.06%   [EVAL] batch:  498 | acc: 6.25%,  total acc: 65.94%   [EVAL] batch:  499 | acc: 0.00%,  total acc: 65.81%   
cur_acc:  ['0.9454', '0.7093', '0.7431', '0.7817', '0.7431', '0.7669', '0.5655', '0.6438']
his_acc:  ['0.9454', '0.8250', '0.8072', '0.7788', '0.7634', '0.7283', '0.6771', '0.6581']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.445810317993164 1.8504819869995117
CurrentTrain: epoch  0, batch     0 | loss: 12.2962923Losses:  10.06286334991455 1.538740873336792
CurrentTrain: epoch  0, batch     1 | loss: 11.6016045Losses:  9.793401718139648 1.4425890445709229
CurrentTrain: epoch  0, batch     2 | loss: 11.2359905Losses:  10.44334602355957 1.7119903564453125
CurrentTrain: epoch  0, batch     3 | loss: 12.1553364Losses:  9.286259651184082 1.6131922006607056
CurrentTrain: epoch  0, batch     4 | loss: 10.8994522Losses:  9.458785057067871 1.187690258026123
CurrentTrain: epoch  0, batch     5 | loss: 10.6464748Losses:  10.703094482421875 1.8163596391677856
CurrentTrain: epoch  0, batch     6 | loss: 12.5194540Losses:  8.992332458496094 1.2927097082138062
CurrentTrain: epoch  0, batch     7 | loss: 10.2850418Losses:  10.020500183105469 1.5173633098602295
CurrentTrain: epoch  0, batch     8 | loss: 11.5378637Losses:  9.215662956237793 1.396897554397583
CurrentTrain: epoch  0, batch     9 | loss: 10.6125603Losses:  9.128525733947754 1.2606309652328491
CurrentTrain: epoch  0, batch    10 | loss: 10.3891563Losses:  8.497024536132812 1.6129133701324463
CurrentTrain: epoch  0, batch    11 | loss: 10.1099377Losses:  8.674918174743652 1.2863197326660156
CurrentTrain: epoch  0, batch    12 | loss: 9.9612379Losses:  10.036510467529297 1.3288142681121826
CurrentTrain: epoch  0, batch    13 | loss: 11.3653250Losses:  8.459482192993164 0.9309982657432556
CurrentTrain: epoch  0, batch    14 | loss: 9.3904800Losses:  8.987972259521484 1.217246413230896
CurrentTrain: epoch  0, batch    15 | loss: 10.2052183Losses:  9.043728828430176 1.2622506618499756
CurrentTrain: epoch  0, batch    16 | loss: 10.3059797Losses:  9.0014009475708 1.6280603408813477
CurrentTrain: epoch  0, batch    17 | loss: 10.6294613Losses:  8.170807838439941 1.4130241870880127
CurrentTrain: epoch  0, batch    18 | loss: 9.5838318Losses:  9.1421537399292 1.3779186010360718
CurrentTrain: epoch  0, batch    19 | loss: 10.5200720Losses:  9.312559127807617 1.2977304458618164
CurrentTrain: epoch  0, batch    20 | loss: 10.6102896Losses:  8.744073867797852 1.1702731847763062
CurrentTrain: epoch  0, batch    21 | loss: 9.9143467Losses:  7.735922813415527 0.9056274890899658
CurrentTrain: epoch  0, batch    22 | loss: 8.6415501Losses:  8.624547004699707 1.1906819343566895
CurrentTrain: epoch  0, batch    23 | loss: 9.8152294Losses:  8.524168968200684 1.487174391746521
CurrentTrain: epoch  0, batch    24 | loss: 10.0113430Losses:  8.291542053222656 1.1572295427322388
CurrentTrain: epoch  0, batch    25 | loss: 9.4487715Losses:  8.472439765930176 1.2071876525878906
CurrentTrain: epoch  0, batch    26 | loss: 9.6796274Losses:  8.779385566711426 1.359476089477539
CurrentTrain: epoch  0, batch    27 | loss: 10.1388617Losses:  8.53345775604248 1.0244301557540894
CurrentTrain: epoch  0, batch    28 | loss: 9.5578880Losses:  8.964310646057129 1.0407204627990723
CurrentTrain: epoch  0, batch    29 | loss: 10.0050316Losses:  6.956307411193848 0.9436116218566895
CurrentTrain: epoch  0, batch    30 | loss: 7.8999190Losses:  8.963306427001953 1.174853801727295
CurrentTrain: epoch  0, batch    31 | loss: 10.1381607Losses:  9.188953399658203 1.077413558959961
CurrentTrain: epoch  0, batch    32 | loss: 10.2663670Losses:  7.783958435058594 1.312269926071167
CurrentTrain: epoch  0, batch    33 | loss: 9.0962286Losses:  8.012358665466309 0.8575711250305176
CurrentTrain: epoch  0, batch    34 | loss: 8.8699303Losses:  8.51461124420166 1.1845358610153198
CurrentTrain: epoch  0, batch    35 | loss: 9.6991472Losses:  8.541863441467285 0.7641786336898804
CurrentTrain: epoch  0, batch    36 | loss: 9.3060417Losses:  8.851264953613281 1.457733154296875
CurrentTrain: epoch  0, batch    37 | loss: 10.3089981Losses:  7.1041340827941895 0.8294113874435425
CurrentTrain: epoch  0, batch    38 | loss: 7.9335456Losses:  8.165040969848633 1.2730319499969482
CurrentTrain: epoch  0, batch    39 | loss: 9.4380732Losses:  7.0570068359375 0.998443067073822
CurrentTrain: epoch  0, batch    40 | loss: 8.0554495Losses:  7.914987564086914 0.8323476314544678
CurrentTrain: epoch  0, batch    41 | loss: 8.7473354Losses:  7.7441511154174805 0.7291158437728882
CurrentTrain: epoch  0, batch    42 | loss: 8.4732666Losses:  8.862190246582031 1.048111915588379
CurrentTrain: epoch  0, batch    43 | loss: 9.9103022Losses:  7.727876663208008 0.901618480682373
CurrentTrain: epoch  0, batch    44 | loss: 8.6294956Losses:  7.825634956359863 0.8509495854377747
CurrentTrain: epoch  0, batch    45 | loss: 8.6765842Losses:  7.6514692306518555 0.8268941044807434
CurrentTrain: epoch  0, batch    46 | loss: 8.4783630Losses:  7.917575836181641 0.7361281514167786
CurrentTrain: epoch  0, batch    47 | loss: 8.6537037Losses:  7.510694980621338 0.8257076144218445
CurrentTrain: epoch  0, batch    48 | loss: 8.3364029Losses:  8.936580657958984 0.9150820970535278
CurrentTrain: epoch  0, batch    49 | loss: 9.8516626Losses:  7.880068302154541 1.10725736618042
CurrentTrain: epoch  0, batch    50 | loss: 8.9873257Losses:  7.396854877471924 0.971825361251831
CurrentTrain: epoch  0, batch    51 | loss: 8.3686800Losses:  8.134461402893066 1.131126880645752
CurrentTrain: epoch  0, batch    52 | loss: 9.2655888Losses:  8.259445190429688 0.9294612407684326
CurrentTrain: epoch  0, batch    53 | loss: 9.1889067Losses:  8.372417449951172 1.1315109729766846
CurrentTrain: epoch  0, batch    54 | loss: 9.5039282Losses:  8.542678833007812 1.1144819259643555
CurrentTrain: epoch  0, batch    55 | loss: 9.6571608Losses:  8.385726928710938 1.0302259922027588
CurrentTrain: epoch  0, batch    56 | loss: 9.4159527Losses:  7.636746406555176 0.9216304421424866
CurrentTrain: epoch  0, batch    57 | loss: 8.5583773Losses:  6.685009002685547 0.7516051530838013
CurrentTrain: epoch  0, batch    58 | loss: 7.4366140Losses:  9.121129989624023 0.9237753748893738
CurrentTrain: epoch  0, batch    59 | loss: 10.0449057Losses:  8.37209415435791 0.8448606729507446
CurrentTrain: epoch  0, batch    60 | loss: 9.2169552Losses:  6.958714485168457 0.9119101762771606
CurrentTrain: epoch  0, batch    61 | loss: 7.8706245Losses:  7.170103073120117 1.055781364440918
CurrentTrain: epoch  0, batch    62 | loss: 8.2258844Losses:  8.194681167602539 1.3908940553665161
CurrentTrain: epoch  0, batch    63 | loss: 9.5855751Losses:  8.50117015838623 1.0809564590454102
CurrentTrain: epoch  0, batch    64 | loss: 9.5821266Losses:  7.471591949462891 1.2280222177505493
CurrentTrain: epoch  0, batch    65 | loss: 8.6996145Losses:  8.226155281066895 1.2790412902832031
CurrentTrain: epoch  0, batch    66 | loss: 9.5051966Losses:  7.064477443695068 1.1184484958648682
CurrentTrain: epoch  0, batch    67 | loss: 8.1829262Losses:  8.452770233154297 1.097421407699585
CurrentTrain: epoch  0, batch    68 | loss: 9.5501919Losses:  6.697046279907227 0.6090283989906311
CurrentTrain: epoch  0, batch    69 | loss: 7.3060746Losses:  6.886598587036133 1.1569328308105469
CurrentTrain: epoch  0, batch    70 | loss: 8.0435314Losses:  7.11053466796875 0.7767206430435181
CurrentTrain: epoch  0, batch    71 | loss: 7.8872552Losses:  8.26490592956543 0.9157896041870117
CurrentTrain: epoch  0, batch    72 | loss: 9.1806955Losses:  7.884662628173828 1.0757694244384766
CurrentTrain: epoch  0, batch    73 | loss: 8.9604321Losses:  7.215368270874023 0.9470227360725403
CurrentTrain: epoch  0, batch    74 | loss: 8.1623907Losses:  7.113371849060059 0.8426661491394043
CurrentTrain: epoch  0, batch    75 | loss: 7.9560380Losses:  8.382123947143555 0.8645534515380859
CurrentTrain: epoch  0, batch    76 | loss: 9.2466774Losses:  7.510453224182129 0.8782953023910522
CurrentTrain: epoch  0, batch    77 | loss: 8.3887482Losses:  7.064085960388184 0.8951137065887451
CurrentTrain: epoch  0, batch    78 | loss: 7.9591999Losses:  8.728679656982422 0.8515098094940186
CurrentTrain: epoch  0, batch    79 | loss: 9.5801897Losses:  6.550587177276611 0.7825702428817749
CurrentTrain: epoch  0, batch    80 | loss: 7.3331575Losses:  8.983015060424805 1.127211332321167
CurrentTrain: epoch  0, batch    81 | loss: 10.1102266Losses:  7.09295129776001 0.860304057598114
CurrentTrain: epoch  0, batch    82 | loss: 7.9532552Losses:  7.632019996643066 0.9400108456611633
CurrentTrain: epoch  0, batch    83 | loss: 8.5720310Losses:  7.195932388305664 0.58353590965271
CurrentTrain: epoch  0, batch    84 | loss: 7.7794685Losses:  7.525453567504883 0.9179213047027588
CurrentTrain: epoch  0, batch    85 | loss: 8.4433746Losses:  7.471541881561279 0.7960721254348755
CurrentTrain: epoch  0, batch    86 | loss: 8.2676144Losses:  7.283790588378906 0.5750737190246582
CurrentTrain: epoch  0, batch    87 | loss: 7.8588643Losses:  6.8015289306640625 0.8453489542007446
CurrentTrain: epoch  0, batch    88 | loss: 7.6468778Losses:  7.715519905090332 0.8070080280303955
CurrentTrain: epoch  0, batch    89 | loss: 8.5225277Losses:  7.5127668380737305 0.8512155413627625
CurrentTrain: epoch  0, batch    90 | loss: 8.3639822Losses:  7.733536720275879 0.9136434197425842
CurrentTrain: epoch  0, batch    91 | loss: 8.6471806Losses:  7.893306732177734 0.9677197933197021
CurrentTrain: epoch  0, batch    92 | loss: 8.8610268Losses:  6.840693473815918 0.6243430972099304
CurrentTrain: epoch  0, batch    93 | loss: 7.4650364Losses:  7.1304612159729 0.7675197124481201
CurrentTrain: epoch  0, batch    94 | loss: 7.8979807Losses:  6.294463157653809 0.8268489241600037
CurrentTrain: epoch  0, batch    95 | loss: 7.1213121Losses:  8.149202346801758 0.8102172017097473
CurrentTrain: epoch  0, batch    96 | loss: 8.9594193Losses:  7.082819938659668 0.893947958946228
CurrentTrain: epoch  0, batch    97 | loss: 7.9767680Losses:  6.470521926879883 0.7976655960083008
CurrentTrain: epoch  0, batch    98 | loss: 7.2681875Losses:  8.351268768310547 0.9114123582839966
CurrentTrain: epoch  0, batch    99 | loss: 9.2626810Losses:  7.558684825897217 0.7128856778144836
CurrentTrain: epoch  0, batch   100 | loss: 8.2715702Losses:  8.766658782958984 0.5484911203384399
CurrentTrain: epoch  0, batch   101 | loss: 9.3151503Losses:  6.8761420249938965 0.6871680617332458
CurrentTrain: epoch  0, batch   102 | loss: 7.5633101Losses:  5.977285385131836 0.7136543989181519
CurrentTrain: epoch  0, batch   103 | loss: 6.6909399Losses:  7.313206672668457 0.8139123320579529
CurrentTrain: epoch  0, batch   104 | loss: 8.1271191Losses:  7.021820545196533 0.8573230504989624
CurrentTrain: epoch  0, batch   105 | loss: 7.8791437Losses:  7.700196743011475 0.47515225410461426
CurrentTrain: epoch  0, batch   106 | loss: 8.1753492Losses:  6.793882369995117 0.5288094282150269
CurrentTrain: epoch  0, batch   107 | loss: 7.3226919Losses:  9.349056243896484 0.6115949749946594
CurrentTrain: epoch  0, batch   108 | loss: 9.9606514Losses:  7.443596839904785 0.8187211751937866
CurrentTrain: epoch  0, batch   109 | loss: 8.2623177Losses:  9.517095565795898 1.1183462142944336
CurrentTrain: epoch  0, batch   110 | loss: 10.6354418Losses:  7.202998161315918 0.8066980838775635
CurrentTrain: epoch  0, batch   111 | loss: 8.0096960Losses:  6.296545505523682 0.6849652528762817
CurrentTrain: epoch  0, batch   112 | loss: 6.9815106Losses:  6.073276519775391 0.7720949649810791
CurrentTrain: epoch  0, batch   113 | loss: 6.8453712Losses:  6.619978427886963 0.545150637626648
CurrentTrain: epoch  0, batch   114 | loss: 7.1651292Losses:  5.695285797119141 0.3406382203102112
CurrentTrain: epoch  0, batch   115 | loss: 6.0359240Losses:  6.918744087219238 1.0254433155059814
CurrentTrain: epoch  0, batch   116 | loss: 7.9441872Losses:  7.273963928222656 0.7772979736328125
CurrentTrain: epoch  0, batch   117 | loss: 8.0512619Losses:  7.493773460388184 0.6567221879959106
CurrentTrain: epoch  0, batch   118 | loss: 8.1504955Losses:  7.09291934967041 0.832446813583374
CurrentTrain: epoch  0, batch   119 | loss: 7.9253664Losses:  7.220166206359863 0.79676353931427
CurrentTrain: epoch  0, batch   120 | loss: 8.0169296Losses:  6.673365592956543 0.8209874033927917
CurrentTrain: epoch  0, batch   121 | loss: 7.4943528Losses:  7.381585121154785 0.6135891079902649
CurrentTrain: epoch  0, batch   122 | loss: 7.9951744Losses:  6.267278671264648 0.8213523030281067
CurrentTrain: epoch  0, batch   123 | loss: 7.0886312Losses:  6.872162818908691 0.8699049949645996
CurrentTrain: epoch  0, batch   124 | loss: 7.7420678Losses:  7.660895347595215 0.751375675201416
CurrentTrain: epoch  1, batch     0 | loss: 8.4122715Losses:  8.246936798095703 0.4458957612514496
CurrentTrain: epoch  1, batch     1 | loss: 8.6928329Losses:  6.020340919494629 0.6826751232147217
CurrentTrain: epoch  1, batch     2 | loss: 6.7030163Losses:  5.87131929397583 0.5576205849647522
CurrentTrain: epoch  1, batch     3 | loss: 6.4289398Losses:  6.867588996887207 0.7807292938232422
CurrentTrain: epoch  1, batch     4 | loss: 7.6483183Losses:  7.0794854164123535 0.6436060667037964
CurrentTrain: epoch  1, batch     5 | loss: 7.7230916Losses:  7.522507667541504 0.7133079767227173
CurrentTrain: epoch  1, batch     6 | loss: 8.2358160Losses:  5.986710071563721 0.6505647897720337
CurrentTrain: epoch  1, batch     7 | loss: 6.6372747Losses:  6.185274124145508 0.7348723411560059
CurrentTrain: epoch  1, batch     8 | loss: 6.9201465Losses:  5.772382736206055 0.23197557032108307
CurrentTrain: epoch  1, batch     9 | loss: 6.0043583Losses:  6.082544803619385 0.531326413154602
CurrentTrain: epoch  1, batch    10 | loss: 6.6138711Losses:  6.20207405090332 0.4108794033527374
CurrentTrain: epoch  1, batch    11 | loss: 6.6129537Losses:  5.963085174560547 0.5396287441253662
CurrentTrain: epoch  1, batch    12 | loss: 6.5027142Losses:  8.334339141845703 0.5052248239517212
CurrentTrain: epoch  1, batch    13 | loss: 8.8395643Losses:  6.5172882080078125 0.7166506052017212
CurrentTrain: epoch  1, batch    14 | loss: 7.2339387Losses:  7.057426452636719 0.7954910397529602
CurrentTrain: epoch  1, batch    15 | loss: 7.8529177Losses:  6.671722888946533 0.5481869578361511
CurrentTrain: epoch  1, batch    16 | loss: 7.2199097Losses:  6.286011695861816 0.8358704447746277
CurrentTrain: epoch  1, batch    17 | loss: 7.1218820Losses:  6.292623043060303 0.6148467063903809
CurrentTrain: epoch  1, batch    18 | loss: 6.9074697Losses:  6.211911678314209 0.6100990772247314
CurrentTrain: epoch  1, batch    19 | loss: 6.8220110Losses:  6.825270175933838 0.7641138434410095
CurrentTrain: epoch  1, batch    20 | loss: 7.5893841Losses:  6.285584449768066 0.5447952151298523
CurrentTrain: epoch  1, batch    21 | loss: 6.8303795Losses:  7.909816741943359 0.5229588150978088
CurrentTrain: epoch  1, batch    22 | loss: 8.4327755Losses:  6.6316680908203125 0.8316037654876709
CurrentTrain: epoch  1, batch    23 | loss: 7.4632721Losses:  6.437498569488525 0.5528318881988525
CurrentTrain: epoch  1, batch    24 | loss: 6.9903307Losses:  6.556337356567383 0.799419641494751
CurrentTrain: epoch  1, batch    25 | loss: 7.3557568Losses:  5.83070182800293 0.5804076194763184
CurrentTrain: epoch  1, batch    26 | loss: 6.4111094Losses:  7.238349914550781 0.7089263200759888
CurrentTrain: epoch  1, batch    27 | loss: 7.9472761Losses:  7.0311994552612305 1.0256867408752441
CurrentTrain: epoch  1, batch    28 | loss: 8.0568867Losses:  6.269148349761963 0.7008470892906189
CurrentTrain: epoch  1, batch    29 | loss: 6.9699955Losses:  7.37803316116333 0.713809609413147
CurrentTrain: epoch  1, batch    30 | loss: 8.0918427Losses:  6.267132759094238 0.611187219619751
CurrentTrain: epoch  1, batch    31 | loss: 6.8783197Losses:  5.69223165512085 0.6490153074264526
CurrentTrain: epoch  1, batch    32 | loss: 6.3412471Losses:  6.631191253662109 0.5086394548416138
CurrentTrain: epoch  1, batch    33 | loss: 7.1398306Losses:  8.187060356140137 0.8374009132385254
CurrentTrain: epoch  1, batch    34 | loss: 9.0244617Losses:  7.195956230163574 0.5065711736679077
CurrentTrain: epoch  1, batch    35 | loss: 7.7025275Losses:  6.1965107917785645 0.6549932360649109
CurrentTrain: epoch  1, batch    36 | loss: 6.8515038Losses:  5.866761207580566 0.5545091032981873
CurrentTrain: epoch  1, batch    37 | loss: 6.4212704Losses:  6.321906089782715 0.6937215924263
CurrentTrain: epoch  1, batch    38 | loss: 7.0156279Losses:  6.169824600219727 0.6990619897842407
CurrentTrain: epoch  1, batch    39 | loss: 6.8688865Losses:  6.942150115966797 0.6718650460243225
CurrentTrain: epoch  1, batch    40 | loss: 7.6140151Losses:  6.003200531005859 0.6249995827674866
CurrentTrain: epoch  1, batch    41 | loss: 6.6282001Losses:  5.866855144500732 0.5348407030105591
CurrentTrain: epoch  1, batch    42 | loss: 6.4016957Losses:  7.014392852783203 0.5905324816703796
CurrentTrain: epoch  1, batch    43 | loss: 7.6049252Losses:  6.8249616622924805 0.7445613741874695
CurrentTrain: epoch  1, batch    44 | loss: 7.5695229Losses:  6.442951679229736 0.6142367124557495
CurrentTrain: epoch  1, batch    45 | loss: 7.0571885Losses:  6.153163909912109 0.6068610548973083
CurrentTrain: epoch  1, batch    46 | loss: 6.7600250Losses:  6.787353515625 0.5567101240158081
CurrentTrain: epoch  1, batch    47 | loss: 7.3440638Losses:  6.008523941040039 0.5749619007110596
CurrentTrain: epoch  1, batch    48 | loss: 6.5834856Losses:  7.0225324630737305 0.6889132261276245
CurrentTrain: epoch  1, batch    49 | loss: 7.7114458Losses:  6.149531841278076 0.6843498945236206
CurrentTrain: epoch  1, batch    50 | loss: 6.8338819Losses:  5.549017429351807 0.45396777987480164
CurrentTrain: epoch  1, batch    51 | loss: 6.0029850Losses:  8.183073997497559 0.759371280670166
CurrentTrain: epoch  1, batch    52 | loss: 8.9424458Losses:  6.713427543640137 0.5125994682312012
CurrentTrain: epoch  1, batch    53 | loss: 7.2260270Losses:  6.111485481262207 0.515102744102478
CurrentTrain: epoch  1, batch    54 | loss: 6.6265883Losses:  5.610960960388184 0.3101162314414978
CurrentTrain: epoch  1, batch    55 | loss: 5.9210773Losses:  5.698318958282471 0.20447245240211487
CurrentTrain: epoch  1, batch    56 | loss: 5.9027915Losses:  5.738521575927734 0.514707624912262
CurrentTrain: epoch  1, batch    57 | loss: 6.2532291Losses:  5.834720134735107 0.5102990865707397
CurrentTrain: epoch  1, batch    58 | loss: 6.3450193Losses:  6.659446716308594 0.5648711919784546
CurrentTrain: epoch  1, batch    59 | loss: 7.2243180Losses:  5.265878677368164 0.5114635229110718
CurrentTrain: epoch  1, batch    60 | loss: 5.7773423Losses:  7.755781173706055 0.7059336304664612
CurrentTrain: epoch  1, batch    61 | loss: 8.4617147Losses:  6.128912925720215 0.5232300162315369
CurrentTrain: epoch  1, batch    62 | loss: 6.6521430Losses:  6.506986618041992 0.32040661573410034
CurrentTrain: epoch  1, batch    63 | loss: 6.8273931Losses:  6.044484615325928 0.274591326713562
CurrentTrain: epoch  1, batch    64 | loss: 6.3190761Losses:  6.115009784698486 0.46011462807655334
CurrentTrain: epoch  1, batch    65 | loss: 6.5751243Losses:  6.29624080657959 0.48645317554473877
CurrentTrain: epoch  1, batch    66 | loss: 6.7826939Losses:  6.582127571105957 0.7100256681442261
CurrentTrain: epoch  1, batch    67 | loss: 7.2921534Losses:  5.831793785095215 0.5660293102264404
CurrentTrain: epoch  1, batch    68 | loss: 6.3978233Losses:  7.205248832702637 0.32238075137138367
CurrentTrain: epoch  1, batch    69 | loss: 7.5276294Losses:  6.4381585121154785 0.46309059858322144
CurrentTrain: epoch  1, batch    70 | loss: 6.9012489Losses:  7.437134265899658 0.6148964762687683
CurrentTrain: epoch  1, batch    71 | loss: 8.0520306Losses:  5.9296159744262695 0.5491926670074463
CurrentTrain: epoch  1, batch    72 | loss: 6.4788084Losses:  5.929010391235352 0.5361769199371338
CurrentTrain: epoch  1, batch    73 | loss: 6.4651871Losses:  6.343662261962891 0.5726638436317444
CurrentTrain: epoch  1, batch    74 | loss: 6.9163260Losses:  6.177888870239258 0.5222362279891968
CurrentTrain: epoch  1, batch    75 | loss: 6.7001252Losses:  6.353079795837402 0.425908625125885
CurrentTrain: epoch  1, batch    76 | loss: 6.7789884Losses:  5.572629928588867 0.6102767586708069
CurrentTrain: epoch  1, batch    77 | loss: 6.1829066Losses:  5.464953422546387 0.31386271119117737
CurrentTrain: epoch  1, batch    78 | loss: 5.7788162Losses:  5.200412750244141 0.42061978578567505
CurrentTrain: epoch  1, batch    79 | loss: 5.6210327Losses:  6.024679183959961 0.5540856122970581
CurrentTrain: epoch  1, batch    80 | loss: 6.5787649Losses:  6.795804500579834 0.43735209107398987
CurrentTrain: epoch  1, batch    81 | loss: 7.2331567Losses:  5.887513160705566 0.4894811511039734
CurrentTrain: epoch  1, batch    82 | loss: 6.3769941Losses:  6.875937461853027 0.43414556980133057
CurrentTrain: epoch  1, batch    83 | loss: 7.3100829Losses:  5.65279483795166 0.4700758457183838
CurrentTrain: epoch  1, batch    84 | loss: 6.1228704Losses:  6.118622779846191 0.7147729396820068
CurrentTrain: epoch  1, batch    85 | loss: 6.8333960Losses:  6.092070579528809 0.5603743195533752
CurrentTrain: epoch  1, batch    86 | loss: 6.6524448Losses:  6.022949695587158 0.31599849462509155
CurrentTrain: epoch  1, batch    87 | loss: 6.3389482Losses:  6.5091753005981445 0.5060234069824219
CurrentTrain: epoch  1, batch    88 | loss: 7.0151987Losses:  6.927764892578125 0.565696120262146
CurrentTrain: epoch  1, batch    89 | loss: 7.4934611Losses:  5.612459659576416 0.4177594780921936
CurrentTrain: epoch  1, batch    90 | loss: 6.0302191Losses:  5.725475311279297 0.48947402834892273
CurrentTrain: epoch  1, batch    91 | loss: 6.2149491Losses:  5.568446159362793 0.5013974905014038
CurrentTrain: epoch  1, batch    92 | loss: 6.0698438Losses:  6.314306735992432 0.30985966324806213
CurrentTrain: epoch  1, batch    93 | loss: 6.6241665Losses:  6.098132133483887 0.4183924198150635
CurrentTrain: epoch  1, batch    94 | loss: 6.5165243Losses:  5.520439147949219 0.3231876492500305
CurrentTrain: epoch  1, batch    95 | loss: 5.8436270Losses:  6.80316686630249 0.33286377787590027
CurrentTrain: epoch  1, batch    96 | loss: 7.1360307Losses:  5.713345527648926 0.3727365732192993
CurrentTrain: epoch  1, batch    97 | loss: 6.0860820Losses:  6.013826370239258 0.32793349027633667
CurrentTrain: epoch  1, batch    98 | loss: 6.3417597Losses:  5.0489726066589355 0.37534987926483154
CurrentTrain: epoch  1, batch    99 | loss: 5.4243226Losses:  5.542729377746582 0.29629871249198914
CurrentTrain: epoch  1, batch   100 | loss: 5.8390279Losses:  5.3574981689453125 0.29515188932418823
CurrentTrain: epoch  1, batch   101 | loss: 5.6526499Losses:  5.078151226043701 0.22080105543136597
CurrentTrain: epoch  1, batch   102 | loss: 5.2989521Losses:  4.997650146484375 0.24790729582309723
CurrentTrain: epoch  1, batch   103 | loss: 5.2455573Losses:  6.594106674194336 0.4093548059463501
CurrentTrain: epoch  1, batch   104 | loss: 7.0034614Losses:  5.419496536254883 0.40155649185180664
CurrentTrain: epoch  1, batch   105 | loss: 5.8210530Losses:  5.23448371887207 0.5289055109024048
CurrentTrain: epoch  1, batch   106 | loss: 5.7633891Losses:  4.969118595123291 0.49820297956466675
CurrentTrain: epoch  1, batch   107 | loss: 5.4673214Losses:  5.743241310119629 0.3338210880756378
CurrentTrain: epoch  1, batch   108 | loss: 6.0770626Losses:  6.053048133850098 0.5249690413475037
CurrentTrain: epoch  1, batch   109 | loss: 6.5780172Losses:  5.574410438537598 0.3583892285823822
CurrentTrain: epoch  1, batch   110 | loss: 5.9327998Losses:  5.006656169891357 0.1147436872124672
CurrentTrain: epoch  1, batch   111 | loss: 5.1213999Losses:  5.714041709899902 0.4086931347846985
CurrentTrain: epoch  1, batch   112 | loss: 6.1227350Losses:  6.371982574462891 0.4137285649776459
CurrentTrain: epoch  1, batch   113 | loss: 6.7857113Losses:  5.703545570373535 0.4369843602180481
CurrentTrain: epoch  1, batch   114 | loss: 6.1405301Losses:  5.435142993927002 0.285249263048172
CurrentTrain: epoch  1, batch   115 | loss: 5.7203922Losses:  5.257757186889648 0.3655197024345398
CurrentTrain: epoch  1, batch   116 | loss: 5.6232767Losses:  5.268764019012451 0.4465186297893524
CurrentTrain: epoch  1, batch   117 | loss: 5.7152824Losses:  5.18602991104126 0.509347677230835
CurrentTrain: epoch  1, batch   118 | loss: 5.6953773Losses:  5.3579206466674805 0.4600179195404053
CurrentTrain: epoch  1, batch   119 | loss: 5.8179388Losses:  5.286627769470215 0.3780863285064697
CurrentTrain: epoch  1, batch   120 | loss: 5.6647139Losses:  5.325569152832031 0.20867370069026947
CurrentTrain: epoch  1, batch   121 | loss: 5.5342426Losses:  5.418376445770264 0.40180498361587524
CurrentTrain: epoch  1, batch   122 | loss: 5.8201814Losses:  6.893509387969971 0.3153596818447113
CurrentTrain: epoch  1, batch   123 | loss: 7.2088690Losses:  5.375720024108887 0.33826497197151184
CurrentTrain: epoch  1, batch   124 | loss: 5.7139850Losses:  5.876612663269043 0.36275655031204224
CurrentTrain: epoch  2, batch     0 | loss: 6.2393694Losses:  5.22020959854126 0.2675451636314392
CurrentTrain: epoch  2, batch     1 | loss: 5.4877548Losses:  4.644745826721191 0.30192244052886963
CurrentTrain: epoch  2, batch     2 | loss: 4.9466681Losses:  5.216429233551025 0.31255465745925903
CurrentTrain: epoch  2, batch     3 | loss: 5.5289841Losses:  5.287356376647949 0.3446679711341858
CurrentTrain: epoch  2, batch     4 | loss: 5.6320243Losses:  4.866278648376465 0.2963033616542816
CurrentTrain: epoch  2, batch     5 | loss: 5.1625819Losses:  5.095466136932373 0.2736906111240387
CurrentTrain: epoch  2, batch     6 | loss: 5.3691568Losses:  5.3089518547058105 0.40171587467193604
CurrentTrain: epoch  2, batch     7 | loss: 5.7106676Losses:  4.795555591583252 0.43148669600486755
CurrentTrain: epoch  2, batch     8 | loss: 5.2270422Losses:  5.329601764678955 0.4610641300678253
CurrentTrain: epoch  2, batch     9 | loss: 5.7906661Losses:  5.570244312286377 0.15300922095775604
CurrentTrain: epoch  2, batch    10 | loss: 5.7232537Losses:  5.637845516204834 0.470030814409256
CurrentTrain: epoch  2, batch    11 | loss: 6.1078763Losses:  5.011175155639648 0.35453706979751587
CurrentTrain: epoch  2, batch    12 | loss: 5.3657122Losses:  4.866731643676758 0.3615506887435913
CurrentTrain: epoch  2, batch    13 | loss: 5.2282825Losses:  5.592776298522949 0.34239572286605835
CurrentTrain: epoch  2, batch    14 | loss: 5.9351721Losses:  5.935915470123291 0.47557297348976135
CurrentTrain: epoch  2, batch    15 | loss: 6.4114885Losses:  4.920970916748047 0.34257572889328003
CurrentTrain: epoch  2, batch    16 | loss: 5.2635465Losses:  5.3078203201293945 0.4657142460346222
CurrentTrain: epoch  2, batch    17 | loss: 5.7735348Losses:  4.520070552825928 0.2283930778503418
CurrentTrain: epoch  2, batch    18 | loss: 4.7484636Losses:  5.084246635437012 0.42032214999198914
CurrentTrain: epoch  2, batch    19 | loss: 5.5045686Losses:  5.281308174133301 0.4000592827796936
CurrentTrain: epoch  2, batch    20 | loss: 5.6813674Losses:  5.140106201171875 0.2753308117389679
CurrentTrain: epoch  2, batch    21 | loss: 5.4154372Losses:  4.894527912139893 0.3097694516181946
CurrentTrain: epoch  2, batch    22 | loss: 5.2042975Losses:  5.71661376953125 0.32452890276908875
CurrentTrain: epoch  2, batch    23 | loss: 6.0411425Losses:  4.811478614807129 0.35245415568351746
CurrentTrain: epoch  2, batch    24 | loss: 5.1639328Losses:  5.232776641845703 0.14753936231136322
CurrentTrain: epoch  2, batch    25 | loss: 5.3803158Losses:  5.066253662109375 0.22516080737113953
CurrentTrain: epoch  2, batch    26 | loss: 5.2914143Losses:  5.271998405456543 0.412727952003479
CurrentTrain: epoch  2, batch    27 | loss: 5.6847262Losses:  5.314229965209961 0.22159698605537415
CurrentTrain: epoch  2, batch    28 | loss: 5.5358272Losses:  5.750913619995117 0.3293803632259369
CurrentTrain: epoch  2, batch    29 | loss: 6.0802941Losses:  5.076667785644531 0.2989659309387207
CurrentTrain: epoch  2, batch    30 | loss: 5.3756337Losses:  4.811593055725098 0.35002467036247253
CurrentTrain: epoch  2, batch    31 | loss: 5.1616178Losses:  5.228691577911377 0.3462185263633728
CurrentTrain: epoch  2, batch    32 | loss: 5.5749102Losses:  4.695257663726807 0.17717885971069336
CurrentTrain: epoch  2, batch    33 | loss: 4.8724365Losses:  6.5623698234558105 0.4149177372455597
CurrentTrain: epoch  2, batch    34 | loss: 6.9772878Losses:  5.183087348937988 0.2698324918746948
CurrentTrain: epoch  2, batch    35 | loss: 5.4529200Losses:  5.0046892166137695 0.4652681350708008
CurrentTrain: epoch  2, batch    36 | loss: 5.4699574Losses:  4.764060974121094 0.4527697265148163
CurrentTrain: epoch  2, batch    37 | loss: 5.2168307Losses:  4.94797945022583 0.33965250849723816
CurrentTrain: epoch  2, batch    38 | loss: 5.2876320Losses:  5.041591167449951 0.21903055906295776
CurrentTrain: epoch  2, batch    39 | loss: 5.2606215Losses:  6.302331924438477 0.18815484642982483
CurrentTrain: epoch  2, batch    40 | loss: 6.4904866Losses:  5.129000663757324 0.18769505620002747
CurrentTrain: epoch  2, batch    41 | loss: 5.3166957Losses:  5.107325553894043 0.3474983870983124
CurrentTrain: epoch  2, batch    42 | loss: 5.4548240Losses:  5.1652727127075195 0.29323697090148926
CurrentTrain: epoch  2, batch    43 | loss: 5.4585094Losses:  4.951778888702393 0.3236449956893921
CurrentTrain: epoch  2, batch    44 | loss: 5.2754240Losses:  5.103403091430664 0.3750670850276947
CurrentTrain: epoch  2, batch    45 | loss: 5.4784703Losses:  4.831544876098633 0.24153004586696625
CurrentTrain: epoch  2, batch    46 | loss: 5.0730748Losses:  5.438122749328613 0.32168713212013245
CurrentTrain: epoch  2, batch    47 | loss: 5.7598100Losses:  4.853378772735596 0.439898818731308
CurrentTrain: epoch  2, batch    48 | loss: 5.2932777Losses:  4.4254374504089355 0.33256661891937256
CurrentTrain: epoch  2, batch    49 | loss: 4.7580042Losses:  5.038969993591309 0.26292723417282104
CurrentTrain: epoch  2, batch    50 | loss: 5.3018970Losses:  4.860400676727295 0.20239773392677307
CurrentTrain: epoch  2, batch    51 | loss: 5.0627985Losses:  4.806100368499756 0.26889216899871826
CurrentTrain: epoch  2, batch    52 | loss: 5.0749927Losses:  4.590030670166016 0.272155225276947
CurrentTrain: epoch  2, batch    53 | loss: 4.8621860Losses:  5.287435531616211 0.3316918909549713
CurrentTrain: epoch  2, batch    54 | loss: 5.6191273Losses:  4.845273971557617 0.42137008905410767
CurrentTrain: epoch  2, batch    55 | loss: 5.2666440Losses:  4.8399457931518555 0.21475686132907867
CurrentTrain: epoch  2, batch    56 | loss: 5.0547028Losses:  4.393528938293457 0.13917995989322662
CurrentTrain: epoch  2, batch    57 | loss: 4.5327091Losses:  4.73927116394043 0.33505886793136597
CurrentTrain: epoch  2, batch    58 | loss: 5.0743299Losses:  5.525848388671875 0.49065691232681274
CurrentTrain: epoch  2, batch    59 | loss: 6.0165052Losses:  4.790710926055908 0.18504422903060913
CurrentTrain: epoch  2, batch    60 | loss: 4.9757552Losses:  5.050926208496094 0.30835050344467163
CurrentTrain: epoch  2, batch    61 | loss: 5.3592768Losses:  4.929332733154297 0.2850843369960785
CurrentTrain: epoch  2, batch    62 | loss: 5.2144170Losses:  5.67235803604126 0.5271302461624146
CurrentTrain: epoch  2, batch    63 | loss: 6.1994882Losses:  5.099461555480957 0.2911366820335388
CurrentTrain: epoch  2, batch    64 | loss: 5.3905983Losses:  4.988706588745117 0.3230888843536377
CurrentTrain: epoch  2, batch    65 | loss: 5.3117952Losses:  5.335312366485596 0.3976097106933594
CurrentTrain: epoch  2, batch    66 | loss: 5.7329221Losses:  4.68714714050293 0.24090515077114105
CurrentTrain: epoch  2, batch    67 | loss: 4.9280524Losses:  4.791091442108154 0.31649941205978394
CurrentTrain: epoch  2, batch    68 | loss: 5.1075907Losses:  6.319104194641113 0.2636865973472595
CurrentTrain: epoch  2, batch    69 | loss: 6.5827909Losses:  4.866625785827637 0.3450731635093689
CurrentTrain: epoch  2, batch    70 | loss: 5.2116990Losses:  5.308952808380127 0.3136896789073944
CurrentTrain: epoch  2, batch    71 | loss: 5.6226425Losses:  5.498856067657471 0.4516143500804901
CurrentTrain: epoch  2, batch    72 | loss: 5.9504704Losses:  4.832192897796631 0.43758147954940796
CurrentTrain: epoch  2, batch    73 | loss: 5.2697744Losses:  4.636669635772705 0.2743273079395294
CurrentTrain: epoch  2, batch    74 | loss: 4.9109969Losses:  4.933568000793457 0.33843815326690674
CurrentTrain: epoch  2, batch    75 | loss: 5.2720060Losses:  4.759988307952881 0.36033129692077637
CurrentTrain: epoch  2, batch    76 | loss: 5.1203194Losses:  4.72576379776001 0.3646531105041504
CurrentTrain: epoch  2, batch    77 | loss: 5.0904169Losses:  4.9762139320373535 0.3585263192653656
CurrentTrain: epoch  2, batch    78 | loss: 5.3347402Losses:  4.744837760925293 0.2699831426143646
CurrentTrain: epoch  2, batch    79 | loss: 5.0148211Losses:  6.231091022491455 0.35666194558143616
CurrentTrain: epoch  2, batch    80 | loss: 6.5877528Losses:  6.1734418869018555 0.4158109128475189
CurrentTrain: epoch  2, batch    81 | loss: 6.5892529Losses:  4.981490612030029 0.15168949961662292
CurrentTrain: epoch  2, batch    82 | loss: 5.1331801Losses:  4.656182289123535 0.21538162231445312
CurrentTrain: epoch  2, batch    83 | loss: 4.8715639Losses:  4.9914374351501465 0.326659619808197
CurrentTrain: epoch  2, batch    84 | loss: 5.3180971Losses:  4.784067153930664 0.2607566714286804
CurrentTrain: epoch  2, batch    85 | loss: 5.0448236Losses:  4.639878273010254 0.2528689205646515
CurrentTrain: epoch  2, batch    86 | loss: 4.8927474Losses:  5.653810501098633 0.26296162605285645
CurrentTrain: epoch  2, batch    87 | loss: 5.9167719Losses:  4.680069923400879 0.2714972496032715
CurrentTrain: epoch  2, batch    88 | loss: 4.9515672Losses:  4.592101097106934 0.29862070083618164
CurrentTrain: epoch  2, batch    89 | loss: 4.8907218Losses:  4.978235244750977 0.2035476565361023
CurrentTrain: epoch  2, batch    90 | loss: 5.1817827Losses:  4.575892925262451 0.10176078230142593
CurrentTrain: epoch  2, batch    91 | loss: 4.6776538Losses:  5.935220718383789 0.24080468714237213
CurrentTrain: epoch  2, batch    92 | loss: 6.1760254Losses:  4.5919294357299805 0.26966047286987305
CurrentTrain: epoch  2, batch    93 | loss: 4.8615899Losses:  4.75577449798584 0.20927424728870392
CurrentTrain: epoch  2, batch    94 | loss: 4.9650488Losses:  4.640293121337891 0.16810482740402222
CurrentTrain: epoch  2, batch    95 | loss: 4.8083978Losses:  5.034521102905273 0.21761447191238403
CurrentTrain: epoch  2, batch    96 | loss: 5.2521358Losses:  4.893269062042236 0.18305940926074982
CurrentTrain: epoch  2, batch    97 | loss: 5.0763283Losses:  5.022774696350098 0.24229131639003754
CurrentTrain: epoch  2, batch    98 | loss: 5.2650661Losses:  5.14990234375 0.183991938829422
CurrentTrain: epoch  2, batch    99 | loss: 5.3338943Losses:  5.148778915405273 0.19690679013729095
CurrentTrain: epoch  2, batch   100 | loss: 5.3456855Losses:  4.79773473739624 0.2308986634016037
CurrentTrain: epoch  2, batch   101 | loss: 5.0286336Losses:  4.942991256713867 0.21793682873249054
CurrentTrain: epoch  2, batch   102 | loss: 5.1609282Losses:  4.610306262969971 0.19575640559196472
CurrentTrain: epoch  2, batch   103 | loss: 4.8060627Losses:  5.27690315246582 0.23401106894016266
CurrentTrain: epoch  2, batch   104 | loss: 5.5109143Losses:  4.825572967529297 0.3194506764411926
CurrentTrain: epoch  2, batch   105 | loss: 5.1450238Losses:  4.881014823913574 0.19315792620182037
CurrentTrain: epoch  2, batch   106 | loss: 5.0741730Losses:  4.944756507873535 0.19223815202713013
CurrentTrain: epoch  2, batch   107 | loss: 5.1369948Losses:  4.888340950012207 0.31531721353530884
CurrentTrain: epoch  2, batch   108 | loss: 5.2036581Losses:  4.8765459060668945 0.1481006145477295
CurrentTrain: epoch  2, batch   109 | loss: 5.0246468Losses:  4.9133100509643555 0.24509310722351074
CurrentTrain: epoch  2, batch   110 | loss: 5.1584034Losses:  5.2945404052734375 0.35653406381607056
CurrentTrain: epoch  2, batch   111 | loss: 5.6510744Losses:  4.517670154571533 0.20970281958580017
CurrentTrain: epoch  2, batch   112 | loss: 4.7273731Losses:  4.51494026184082 0.23342342674732208
CurrentTrain: epoch  2, batch   113 | loss: 4.7483635Losses:  4.438672065734863 0.1437041163444519
CurrentTrain: epoch  2, batch   114 | loss: 4.5823760Losses:  5.703596115112305 0.4766261577606201
CurrentTrain: epoch  2, batch   115 | loss: 6.1802225Losses:  4.435827732086182 0.2332129031419754
CurrentTrain: epoch  2, batch   116 | loss: 4.6690407Losses:  4.713862895965576 0.2105226218700409
CurrentTrain: epoch  2, batch   117 | loss: 4.9243855Losses:  4.685525417327881 0.25435882806777954
CurrentTrain: epoch  2, batch   118 | loss: 4.9398842Losses:  5.274827003479004 0.3838016986846924
CurrentTrain: epoch  2, batch   119 | loss: 5.6586285Losses:  4.413700103759766 0.14432290196418762
CurrentTrain: epoch  2, batch   120 | loss: 4.5580230Losses:  4.675296783447266 0.29715538024902344
CurrentTrain: epoch  2, batch   121 | loss: 4.9724522Losses:  4.330458164215088 0.15495525300502777
CurrentTrain: epoch  2, batch   122 | loss: 4.4854136Losses:  4.645003318786621 0.33705800771713257
CurrentTrain: epoch  2, batch   123 | loss: 4.9820614Losses:  4.9034929275512695 0.20911994576454163
CurrentTrain: epoch  2, batch   124 | loss: 5.1126127Losses:  4.568722724914551 0.2662801444530487
CurrentTrain: epoch  3, batch     0 | loss: 4.8350029Losses:  4.670285224914551 0.249533012509346
CurrentTrain: epoch  3, batch     1 | loss: 4.9198184Losses:  4.475665092468262 0.12997087836265564
CurrentTrain: epoch  3, batch     2 | loss: 4.6056361Losses:  4.523192405700684 0.18886664509773254
CurrentTrain: epoch  3, batch     3 | loss: 4.7120590Losses:  4.83608341217041 0.2866073548793793
CurrentTrain: epoch  3, batch     4 | loss: 5.1226907Losses:  4.244601249694824 0.19945168495178223
CurrentTrain: epoch  3, batch     5 | loss: 4.4440527Losses:  4.421494483947754 0.2752542495727539
CurrentTrain: epoch  3, batch     6 | loss: 4.6967487Losses:  4.325547218322754 0.22252637147903442
CurrentTrain: epoch  3, batch     7 | loss: 4.5480738Losses:  4.54660177230835 0.20493832230567932
CurrentTrain: epoch  3, batch     8 | loss: 4.7515402Losses:  4.4759979248046875 0.23032984137535095
CurrentTrain: epoch  3, batch     9 | loss: 4.7063279Losses:  4.809188365936279 0.25535741448402405
CurrentTrain: epoch  3, batch    10 | loss: 5.0645456Losses:  4.5547685623168945 0.2636338174343109
CurrentTrain: epoch  3, batch    11 | loss: 4.8184023Losses:  4.428613185882568 0.329407662153244
CurrentTrain: epoch  3, batch    12 | loss: 4.7580209Losses:  4.817706108093262 0.20648065209388733
CurrentTrain: epoch  3, batch    13 | loss: 5.0241866Losses:  4.718737602233887 0.2121700793504715
CurrentTrain: epoch  3, batch    14 | loss: 4.9309077Losses:  4.771191596984863 0.19383837282657623
CurrentTrain: epoch  3, batch    15 | loss: 4.9650302Losses:  4.392571449279785 0.17153862118721008
CurrentTrain: epoch  3, batch    16 | loss: 4.5641103Losses:  4.986133575439453 0.3410722613334656
CurrentTrain: epoch  3, batch    17 | loss: 5.3272057Losses:  4.712133407592773 0.2633945941925049
CurrentTrain: epoch  3, batch    18 | loss: 4.9755278Losses:  4.509881019592285 0.17815764248371124
CurrentTrain: epoch  3, batch    19 | loss: 4.6880388Losses:  5.779200077056885 0.388782262802124
CurrentTrain: epoch  3, batch    20 | loss: 6.1679821Losses:  4.435305595397949 0.2885686755180359
CurrentTrain: epoch  3, batch    21 | loss: 4.7238741Losses:  4.754796504974365 0.2737855315208435
CurrentTrain: epoch  3, batch    22 | loss: 5.0285821Losses:  4.491307258605957 0.25379520654678345
CurrentTrain: epoch  3, batch    23 | loss: 4.7451024Losses:  4.398124694824219 0.19761639833450317
CurrentTrain: epoch  3, batch    24 | loss: 4.5957413Losses:  4.752735137939453 0.20727133750915527
CurrentTrain: epoch  3, batch    25 | loss: 4.9600067Losses:  4.648306846618652 0.23790806531906128
CurrentTrain: epoch  3, batch    26 | loss: 4.8862147Losses:  4.313379287719727 0.2602999806404114
CurrentTrain: epoch  3, batch    27 | loss: 4.5736794Losses:  5.5149126052856445 0.19939523935317993
CurrentTrain: epoch  3, batch    28 | loss: 5.7143078Losses:  4.611922740936279 0.2492569386959076
CurrentTrain: epoch  3, batch    29 | loss: 4.8611798Losses:  4.376827239990234 0.17401501536369324
CurrentTrain: epoch  3, batch    30 | loss: 4.5508423Losses:  4.660710334777832 0.28111666440963745
CurrentTrain: epoch  3, batch    31 | loss: 4.9418268Losses:  5.080223083496094 0.20990370213985443
CurrentTrain: epoch  3, batch    32 | loss: 5.2901268Losses:  4.4930267333984375 0.25813400745391846
CurrentTrain: epoch  3, batch    33 | loss: 4.7511606Losses:  4.5096659660339355 0.15379145741462708
CurrentTrain: epoch  3, batch    34 | loss: 4.6634574Losses:  4.472140312194824 0.11614088714122772
CurrentTrain: epoch  3, batch    35 | loss: 4.5882812Losses:  4.474649429321289 0.11642362177371979
CurrentTrain: epoch  3, batch    36 | loss: 4.5910730Losses:  4.8591461181640625 0.32584068179130554
CurrentTrain: epoch  3, batch    37 | loss: 5.1849866Losses:  4.442020893096924 0.26555824279785156
CurrentTrain: epoch  3, batch    38 | loss: 4.7075791Losses:  4.495996475219727 0.1319953203201294
CurrentTrain: epoch  3, batch    39 | loss: 4.6279917Losses:  4.550704002380371 0.23062220215797424
CurrentTrain: epoch  3, batch    40 | loss: 4.7813263Losses:  4.31402587890625 0.22795894742012024
CurrentTrain: epoch  3, batch    41 | loss: 4.5419850Losses:  4.306456565856934 0.24070116877555847
CurrentTrain: epoch  3, batch    42 | loss: 4.5471578Losses:  4.621000289916992 0.13452742993831635
CurrentTrain: epoch  3, batch    43 | loss: 4.7555275Losses:  4.429525375366211 0.3069193363189697
CurrentTrain: epoch  3, batch    44 | loss: 4.7364445Losses:  4.391898155212402 0.2525556683540344
CurrentTrain: epoch  3, batch    45 | loss: 4.6444540Losses:  4.632980823516846 0.14705583453178406
CurrentTrain: epoch  3, batch    46 | loss: 4.7800364Losses:  4.460562705993652 0.18868876993656158
CurrentTrain: epoch  3, batch    47 | loss: 4.6492515Losses:  4.517434120178223 0.16021007299423218
CurrentTrain: epoch  3, batch    48 | loss: 4.6776443Losses:  4.3716325759887695 0.19091354310512543
CurrentTrain: epoch  3, batch    49 | loss: 4.5625463Losses:  4.785653114318848 0.22593766450881958
CurrentTrain: epoch  3, batch    50 | loss: 5.0115910Losses:  4.346142768859863 0.1546562761068344
CurrentTrain: epoch  3, batch    51 | loss: 4.5007992Losses:  4.35214900970459 0.2605965733528137
CurrentTrain: epoch  3, batch    52 | loss: 4.6127458Losses:  4.573796272277832 0.2460496425628662
CurrentTrain: epoch  3, batch    53 | loss: 4.8198462Losses:  4.692113876342773 0.25812071561813354
CurrentTrain: epoch  3, batch    54 | loss: 4.9502344Losses:  4.280971527099609 0.2622048258781433
CurrentTrain: epoch  3, batch    55 | loss: 4.5431762Losses:  4.62039852142334 0.14435067772865295
CurrentTrain: epoch  3, batch    56 | loss: 4.7647491Losses:  5.283590316772461 0.22121411561965942
CurrentTrain: epoch  3, batch    57 | loss: 5.5048046Losses:  4.399885654449463 0.22302797436714172
CurrentTrain: epoch  3, batch    58 | loss: 4.6229138Losses:  4.454656600952148 0.2006547749042511
CurrentTrain: epoch  3, batch    59 | loss: 4.6553116Losses:  4.27156925201416 0.24118100106716156
CurrentTrain: epoch  3, batch    60 | loss: 4.5127501Losses:  4.408236503601074 0.15441890060901642
CurrentTrain: epoch  3, batch    61 | loss: 4.5626554Losses:  4.419919013977051 0.1511097550392151
CurrentTrain: epoch  3, batch    62 | loss: 4.5710287Losses:  6.12083625793457 0.12604621052742004
CurrentTrain: epoch  3, batch    63 | loss: 6.2468824Losses:  4.305892467498779 0.25920164585113525
CurrentTrain: epoch  3, batch    64 | loss: 4.5650940Losses:  4.5162200927734375 0.20214654505252838
CurrentTrain: epoch  3, batch    65 | loss: 4.7183666Losses:  4.403090000152588 0.1532820612192154
CurrentTrain: epoch  3, batch    66 | loss: 4.5563722Losses:  4.423394203186035 0.1470569670200348
CurrentTrain: epoch  3, batch    67 | loss: 4.5704513Losses:  4.451087951660156 0.2840687036514282
CurrentTrain: epoch  3, batch    68 | loss: 4.7351565Losses:  4.457991123199463 0.29807165265083313
CurrentTrain: epoch  3, batch    69 | loss: 4.7560630Losses:  4.343140602111816 0.11180751025676727
CurrentTrain: epoch  3, batch    70 | loss: 4.4549479Losses:  4.331645965576172 0.28243136405944824
CurrentTrain: epoch  3, batch    71 | loss: 4.6140776Losses:  4.361870288848877 0.22728128731250763
CurrentTrain: epoch  3, batch    72 | loss: 4.5891514Losses:  4.291129112243652 0.15021787583827972
CurrentTrain: epoch  3, batch    73 | loss: 4.4413471Losses:  4.323518753051758 0.20480969548225403
CurrentTrain: epoch  3, batch    74 | loss: 4.5283284Losses:  4.272598743438721 0.13315698504447937
CurrentTrain: epoch  3, batch    75 | loss: 4.4057555Losses:  4.3563232421875 0.17382988333702087
CurrentTrain: epoch  3, batch    76 | loss: 4.5301533Losses:  4.352428436279297 0.11660788953304291
CurrentTrain: epoch  3, batch    77 | loss: 4.4690361Losses:  4.411226272583008 0.15887582302093506
CurrentTrain: epoch  3, batch    78 | loss: 4.5701022Losses:  4.183539390563965 0.04044697433710098
CurrentTrain: epoch  3, batch    79 | loss: 4.2239861Losses:  4.327885627746582 0.26654937863349915
CurrentTrain: epoch  3, batch    80 | loss: 4.5944352Losses:  4.397209644317627 0.33593738079071045
CurrentTrain: epoch  3, batch    81 | loss: 4.7331471Losses:  4.521803855895996 0.15777334570884705
CurrentTrain: epoch  3, batch    82 | loss: 4.6795774Losses:  4.47385311126709 0.23939375579357147
CurrentTrain: epoch  3, batch    83 | loss: 4.7132468Losses:  4.469908714294434 0.1566825658082962
CurrentTrain: epoch  3, batch    84 | loss: 4.6265912Losses:  4.752128601074219 0.22004598379135132
CurrentTrain: epoch  3, batch    85 | loss: 4.9721746Losses:  4.391079902648926 0.17695288360118866
CurrentTrain: epoch  3, batch    86 | loss: 4.5680327Losses:  4.282662391662598 0.15879322588443756
CurrentTrain: epoch  3, batch    87 | loss: 4.4414558Losses:  4.327309608459473 0.15507325530052185
CurrentTrain: epoch  3, batch    88 | loss: 4.4823828Losses:  4.293935298919678 0.18264871835708618
CurrentTrain: epoch  3, batch    89 | loss: 4.4765840Losses:  4.348034858703613 0.22070720791816711
CurrentTrain: epoch  3, batch    90 | loss: 4.5687423Losses:  4.922039031982422 0.2414178103208542
CurrentTrain: epoch  3, batch    91 | loss: 5.1634569Losses:  4.550692558288574 0.340237557888031
CurrentTrain: epoch  3, batch    92 | loss: 4.8909302Losses:  4.268580436706543 0.11232760548591614
CurrentTrain: epoch  3, batch    93 | loss: 4.3809080Losses:  4.659414768218994 0.2682458162307739
CurrentTrain: epoch  3, batch    94 | loss: 4.9276605Losses:  4.249780654907227 0.1417299211025238
CurrentTrain: epoch  3, batch    95 | loss: 4.3915105Losses:  4.615544319152832 0.2641907334327698
CurrentTrain: epoch  3, batch    96 | loss: 4.8797350Losses:  4.323940277099609 0.18318699300289154
CurrentTrain: epoch  3, batch    97 | loss: 4.5071273Losses:  4.2132978439331055 0.19476282596588135
CurrentTrain: epoch  3, batch    98 | loss: 4.4080606Losses:  4.156055927276611 0.11414801329374313
CurrentTrain: epoch  3, batch    99 | loss: 4.2702041Losses:  4.8196563720703125 0.18242383003234863
CurrentTrain: epoch  3, batch   100 | loss: 5.0020800Losses:  4.471050262451172 0.18454967439174652
CurrentTrain: epoch  3, batch   101 | loss: 4.6556001Losses:  4.245963096618652 0.22324904799461365
CurrentTrain: epoch  3, batch   102 | loss: 4.4692121Losses:  4.2963409423828125 0.17853005230426788
CurrentTrain: epoch  3, batch   103 | loss: 4.4748712Losses:  4.373739242553711 0.1362578421831131
CurrentTrain: epoch  3, batch   104 | loss: 4.5099969Losses:  4.22111177444458 0.1443132758140564
CurrentTrain: epoch  3, batch   105 | loss: 4.3654251Losses:  4.202887058258057 0.17787882685661316
CurrentTrain: epoch  3, batch   106 | loss: 4.3807659Losses:  4.415283679962158 0.07970738410949707
CurrentTrain: epoch  3, batch   107 | loss: 4.4949913Losses:  4.327216148376465 0.2107948213815689
CurrentTrain: epoch  3, batch   108 | loss: 4.5380111Losses:  4.316864013671875 0.1883169710636139
CurrentTrain: epoch  3, batch   109 | loss: 4.5051808Losses:  4.300126075744629 0.18101245164871216
CurrentTrain: epoch  3, batch   110 | loss: 4.4811387Losses:  4.332324028015137 0.17062219977378845
CurrentTrain: epoch  3, batch   111 | loss: 4.5029464Losses:  4.403903961181641 0.16960349678993225
CurrentTrain: epoch  3, batch   112 | loss: 4.5735073Losses:  4.304705619812012 0.14772248268127441
CurrentTrain: epoch  3, batch   113 | loss: 4.4524279Losses:  4.150969505310059 0.2387293577194214
CurrentTrain: epoch  3, batch   114 | loss: 4.3896990Losses:  4.20970344543457 0.08441545069217682
CurrentTrain: epoch  3, batch   115 | loss: 4.2941189Losses:  4.2175798416137695 0.1960703581571579
CurrentTrain: epoch  3, batch   116 | loss: 4.4136500Losses:  4.29909610748291 0.21925249695777893
CurrentTrain: epoch  3, batch   117 | loss: 4.5183487Losses:  4.299505233764648 0.14987637102603912
CurrentTrain: epoch  3, batch   118 | loss: 4.4493818Losses:  4.2159833908081055 0.3057442307472229
CurrentTrain: epoch  3, batch   119 | loss: 4.5217276Losses:  4.4353437423706055 0.23358482122421265
CurrentTrain: epoch  3, batch   120 | loss: 4.6689286Losses:  4.130334854125977 0.2189277857542038
CurrentTrain: epoch  3, batch   121 | loss: 4.3492627Losses:  4.237054824829102 0.2127167284488678
CurrentTrain: epoch  3, batch   122 | loss: 4.4497714Losses:  4.45901346206665 0.21899066865444183
CurrentTrain: epoch  3, batch   123 | loss: 4.6780043Losses:  4.300127983093262 0.19429489970207214
CurrentTrain: epoch  3, batch   124 | loss: 4.4944229Losses:  4.2734055519104 0.19732585549354553
CurrentTrain: epoch  4, batch     0 | loss: 4.4707313Losses:  4.3137006759643555 0.21239449083805084
CurrentTrain: epoch  4, batch     1 | loss: 4.5260954Losses:  4.174771308898926 0.15709030628204346
CurrentTrain: epoch  4, batch     2 | loss: 4.3318615Losses:  4.240653991699219 0.28102442622184753
CurrentTrain: epoch  4, batch     3 | loss: 4.5216784Losses:  4.190852642059326 0.0960039496421814
CurrentTrain: epoch  4, batch     4 | loss: 4.2868567Losses:  5.006503105163574 0.26014336943626404
CurrentTrain: epoch  4, batch     5 | loss: 5.2666464Losses:  4.122108459472656 0.17282254993915558
CurrentTrain: epoch  4, batch     6 | loss: 4.2949309Losses:  4.316204071044922 0.2301943600177765
CurrentTrain: epoch  4, batch     7 | loss: 4.5463986Losses:  4.167086601257324 0.18324828147888184
CurrentTrain: epoch  4, batch     8 | loss: 4.3503351Losses:  4.398330211639404 0.2274136245250702
CurrentTrain: epoch  4, batch     9 | loss: 4.6257439Losses:  4.2943902015686035 0.11135318130254745
CurrentTrain: epoch  4, batch    10 | loss: 4.4057436Losses:  4.283777713775635 0.143419086933136
CurrentTrain: epoch  4, batch    11 | loss: 4.4271970Losses:  4.256253719329834 0.2266392707824707
CurrentTrain: epoch  4, batch    12 | loss: 4.4828930Losses:  4.52754545211792 0.09091083705425262
CurrentTrain: epoch  4, batch    13 | loss: 4.6184564Losses:  4.132026672363281 0.17857477068901062
CurrentTrain: epoch  4, batch    14 | loss: 4.3106012Losses:  4.390244960784912 0.19370673596858978
CurrentTrain: epoch  4, batch    15 | loss: 4.5839515Losses:  4.260445594787598 0.27951356768608093
CurrentTrain: epoch  4, batch    16 | loss: 4.5399590Losses:  4.2343645095825195 0.22688491642475128
CurrentTrain: epoch  4, batch    17 | loss: 4.4612494Losses:  4.274437427520752 0.25195643305778503
CurrentTrain: epoch  4, batch    18 | loss: 4.5263939Losses:  4.355521202087402 0.12829269468784332
CurrentTrain: epoch  4, batch    19 | loss: 4.4838138Losses:  4.232653617858887 0.16445329785346985
CurrentTrain: epoch  4, batch    20 | loss: 4.3971071Losses:  4.230234622955322 0.20051659643650055
CurrentTrain: epoch  4, batch    21 | loss: 4.4307513Losses:  4.275117874145508 0.11539508402347565
CurrentTrain: epoch  4, batch    22 | loss: 4.3905129Losses:  4.135662078857422 0.12309816479682922
CurrentTrain: epoch  4, batch    23 | loss: 4.2587605Losses:  4.160332679748535 0.10060818493366241
CurrentTrain: epoch  4, batch    24 | loss: 4.2609410Losses:  4.183166027069092 0.17962829768657684
CurrentTrain: epoch  4, batch    25 | loss: 4.3627944Losses:  4.220221519470215 0.18897080421447754
CurrentTrain: epoch  4, batch    26 | loss: 4.4091921Losses:  4.231321811676025 0.30006223917007446
CurrentTrain: epoch  4, batch    27 | loss: 4.5313840Losses:  4.281820297241211 0.10553602874279022
CurrentTrain: epoch  4, batch    28 | loss: 4.3873563Losses:  4.2772932052612305 0.0924564078450203
CurrentTrain: epoch  4, batch    29 | loss: 4.3697495Losses:  4.230052947998047 0.1755117028951645
CurrentTrain: epoch  4, batch    30 | loss: 4.4055648Losses:  4.192203044891357 0.11953781545162201
CurrentTrain: epoch  4, batch    31 | loss: 4.3117409Losses:  4.182037353515625 0.205061674118042
CurrentTrain: epoch  4, batch    32 | loss: 4.3870993Losses:  4.142204284667969 0.153769850730896
CurrentTrain: epoch  4, batch    33 | loss: 4.2959743Losses:  4.232907295227051 0.20125293731689453
CurrentTrain: epoch  4, batch    34 | loss: 4.4341602Losses:  4.24404764175415 0.11510802060365677
CurrentTrain: epoch  4, batch    35 | loss: 4.3591557Losses:  4.101133346557617 0.26265543699264526
CurrentTrain: epoch  4, batch    36 | loss: 4.3637886Losses:  4.15635871887207 0.1996302306652069
CurrentTrain: epoch  4, batch    37 | loss: 4.3559890Losses:  4.171395301818848 0.14806193113327026
CurrentTrain: epoch  4, batch    38 | loss: 4.3194571Losses:  4.198329925537109 0.2614091634750366
CurrentTrain: epoch  4, batch    39 | loss: 4.4597392Losses:  4.1853203773498535 0.15901094675064087
CurrentTrain: epoch  4, batch    40 | loss: 4.3443313Losses:  4.385048866271973 0.13102436065673828
CurrentTrain: epoch  4, batch    41 | loss: 4.5160732Losses:  4.158227920532227 0.16166849434375763
CurrentTrain: epoch  4, batch    42 | loss: 4.3198962Losses:  4.108755111694336 0.12344539165496826
CurrentTrain: epoch  4, batch    43 | loss: 4.2322006Losses:  4.185103416442871 0.19192230701446533
CurrentTrain: epoch  4, batch    44 | loss: 4.3770256Losses:  4.28731632232666 0.14330196380615234
CurrentTrain: epoch  4, batch    45 | loss: 4.4306183Losses:  4.1816840171813965 0.17317652702331543
CurrentTrain: epoch  4, batch    46 | loss: 4.3548603Losses:  4.184139728546143 0.2048145830631256
CurrentTrain: epoch  4, batch    47 | loss: 4.3889542Losses:  4.200603485107422 0.10573321580886841
CurrentTrain: epoch  4, batch    48 | loss: 4.3063369Losses:  4.1207380294799805 0.1878732144832611
CurrentTrain: epoch  4, batch    49 | loss: 4.3086114Losses:  4.116494655609131 0.2595829367637634
CurrentTrain: epoch  4, batch    50 | loss: 4.3760777Losses:  4.244386672973633 0.21623820066452026
CurrentTrain: epoch  4, batch    51 | loss: 4.4606247Losses:  4.155239105224609 0.1151554137468338
CurrentTrain: epoch  4, batch    52 | loss: 4.2703943Losses:  4.12843656539917 0.19279742240905762
CurrentTrain: epoch  4, batch    53 | loss: 4.3212337Losses:  4.073049545288086 0.20169420540332794
CurrentTrain: epoch  4, batch    54 | loss: 4.2747436Losses:  4.367002487182617 0.11498445272445679
CurrentTrain: epoch  4, batch    55 | loss: 4.4819870Losses:  4.1686577796936035 0.23501771688461304
CurrentTrain: epoch  4, batch    56 | loss: 4.4036756Losses:  4.185467720031738 0.16857345402240753
CurrentTrain: epoch  4, batch    57 | loss: 4.3540411Losses:  4.23415470123291 0.09358610212802887
CurrentTrain: epoch  4, batch    58 | loss: 4.3277407Losses:  4.395794868469238 0.1566084921360016
CurrentTrain: epoch  4, batch    59 | loss: 4.5524035Losses:  4.2683916091918945 0.18820782005786896
CurrentTrain: epoch  4, batch    60 | loss: 4.4565992Losses:  4.236507415771484 0.12460258603096008
CurrentTrain: epoch  4, batch    61 | loss: 4.3611102Losses:  4.322791576385498 0.1084778755903244
CurrentTrain: epoch  4, batch    62 | loss: 4.4312696Losses:  4.339319705963135 0.08844441175460815
CurrentTrain: epoch  4, batch    63 | loss: 4.4277639Losses:  4.202715873718262 0.1029202789068222
CurrentTrain: epoch  4, batch    64 | loss: 4.3056359Losses:  4.222412109375 0.15958422422409058
CurrentTrain: epoch  4, batch    65 | loss: 4.3819962Losses:  4.194680690765381 0.07440827786922455
CurrentTrain: epoch  4, batch    66 | loss: 4.2690887Losses:  4.297348976135254 0.0888393223285675
CurrentTrain: epoch  4, batch    67 | loss: 4.3861885Losses:  4.1753034591674805 0.0815044641494751
CurrentTrain: epoch  4, batch    68 | loss: 4.2568078Losses:  4.258289813995361 0.06069052219390869
CurrentTrain: epoch  4, batch    69 | loss: 4.3189802Losses:  4.208937644958496 0.17974968254566193
CurrentTrain: epoch  4, batch    70 | loss: 4.3886871Losses:  4.176202774047852 0.21249507367610931
CurrentTrain: epoch  4, batch    71 | loss: 4.3886976Losses:  4.173253059387207 0.14293138682842255
CurrentTrain: epoch  4, batch    72 | loss: 4.3161845Losses:  4.170813083648682 0.12673898041248322
CurrentTrain: epoch  4, batch    73 | loss: 4.2975521Losses:  4.265578269958496 0.19733935594558716
CurrentTrain: epoch  4, batch    74 | loss: 4.4629178Losses:  4.279446601867676 0.15271976590156555
CurrentTrain: epoch  4, batch    75 | loss: 4.4321666Losses:  4.189359664916992 0.1663658618927002
CurrentTrain: epoch  4, batch    76 | loss: 4.3557253Losses:  4.200497627258301 0.08449344336986542
CurrentTrain: epoch  4, batch    77 | loss: 4.2849913Losses:  4.241944789886475 0.1569843739271164
CurrentTrain: epoch  4, batch    78 | loss: 4.3989291Losses:  4.290167808532715 0.1032664030790329
CurrentTrain: epoch  4, batch    79 | loss: 4.3934340Losses:  4.145533561706543 0.16870494186878204
CurrentTrain: epoch  4, batch    80 | loss: 4.3142385Losses:  4.178258895874023 0.17672693729400635
CurrentTrain: epoch  4, batch    81 | loss: 4.3549857Losses:  4.172324180603027 0.10992762446403503
CurrentTrain: epoch  4, batch    82 | loss: 4.2822518Losses:  4.1948747634887695 0.10724227875471115
CurrentTrain: epoch  4, batch    83 | loss: 4.3021169Losses:  4.170657157897949 0.10906680673360825
CurrentTrain: epoch  4, batch    84 | loss: 4.2797241Losses:  4.1447553634643555 0.18108592927455902
CurrentTrain: epoch  4, batch    85 | loss: 4.3258414Losses:  4.159844398498535 0.16953733563423157
CurrentTrain: epoch  4, batch    86 | loss: 4.3293819Losses:  4.268767356872559 0.0796390026807785
CurrentTrain: epoch  4, batch    87 | loss: 4.3484063Losses:  4.193179130554199 0.10171160846948624
CurrentTrain: epoch  4, batch    88 | loss: 4.2948909Losses:  4.115760803222656 0.16035759449005127
CurrentTrain: epoch  4, batch    89 | loss: 4.2761183Losses:  4.2539849281311035 0.19774357974529266
CurrentTrain: epoch  4, batch    90 | loss: 4.4517283Losses:  4.108475685119629 0.15685871243476868
CurrentTrain: epoch  4, batch    91 | loss: 4.2653346Losses:  4.152775764465332 0.09852022677659988
CurrentTrain: epoch  4, batch    92 | loss: 4.2512960Losses:  4.164453983306885 0.18375536799430847
CurrentTrain: epoch  4, batch    93 | loss: 4.3482094Losses:  4.251856803894043 0.11180903762578964
CurrentTrain: epoch  4, batch    94 | loss: 4.3636661Losses:  4.0517168045043945 0.0735577791929245
CurrentTrain: epoch  4, batch    95 | loss: 4.1252747Losses:  4.159970760345459 0.1249408945441246
CurrentTrain: epoch  4, batch    96 | loss: 4.2849116Losses:  4.183890342712402 0.16599921882152557
CurrentTrain: epoch  4, batch    97 | loss: 4.3498898Losses:  4.197443008422852 0.13371193408966064
CurrentTrain: epoch  4, batch    98 | loss: 4.3311548Losses:  4.183834075927734 0.16955861449241638
CurrentTrain: epoch  4, batch    99 | loss: 4.3533926Losses:  4.073052406311035 0.11499430239200592
CurrentTrain: epoch  4, batch   100 | loss: 4.1880469Losses:  4.133478164672852 0.17375539243221283
CurrentTrain: epoch  4, batch   101 | loss: 4.3072333Losses:  4.166223526000977 0.13184809684753418
CurrentTrain: epoch  4, batch   102 | loss: 4.2980719Losses:  4.186216354370117 0.1448807418346405
CurrentTrain: epoch  4, batch   103 | loss: 4.3310971Losses:  4.076777935028076 0.14382494986057281
CurrentTrain: epoch  4, batch   104 | loss: 4.2206030Losses:  4.1100969314575195 0.1846403330564499
CurrentTrain: epoch  4, batch   105 | loss: 4.2947373Losses:  4.110711097717285 0.07340026646852493
CurrentTrain: epoch  4, batch   106 | loss: 4.1841116Losses:  4.168143272399902 0.22296032309532166
CurrentTrain: epoch  4, batch   107 | loss: 4.3911037Losses:  4.052958011627197 0.21553701162338257
CurrentTrain: epoch  4, batch   108 | loss: 4.2684951Losses:  4.427008628845215 0.08323987573385239
CurrentTrain: epoch  4, batch   109 | loss: 4.5102487Losses:  4.114975929260254 0.20025195181369781
CurrentTrain: epoch  4, batch   110 | loss: 4.3152280Losses:  4.130181312561035 0.13605454564094543
CurrentTrain: epoch  4, batch   111 | loss: 4.2662358Losses:  4.171682357788086 0.17117366194725037
CurrentTrain: epoch  4, batch   112 | loss: 4.3428559Losses:  4.006197452545166 0.09865802526473999
CurrentTrain: epoch  4, batch   113 | loss: 4.1048555Losses:  4.089698791503906 0.07857194542884827
CurrentTrain: epoch  4, batch   114 | loss: 4.1682706Losses:  4.181067943572998 0.25644034147262573
CurrentTrain: epoch  4, batch   115 | loss: 4.4375081Losses:  4.187526702880859 0.12483800947666168
CurrentTrain: epoch  4, batch   116 | loss: 4.3123646Losses:  4.351081371307373 0.2433800995349884
CurrentTrain: epoch  4, batch   117 | loss: 4.5944614Losses:  3.9937779903411865 0.1442297399044037
CurrentTrain: epoch  4, batch   118 | loss: 4.1380076Losses:  4.150707721710205 0.15342223644256592
CurrentTrain: epoch  4, batch   119 | loss: 4.3041301Losses:  4.0668625831604 0.20388992130756378
CurrentTrain: epoch  4, batch   120 | loss: 4.2707524Losses:  4.195427894592285 0.08371759951114655
CurrentTrain: epoch  4, batch   121 | loss: 4.2791457Losses:  4.046202659606934 0.15326662361621857
CurrentTrain: epoch  4, batch   122 | loss: 4.1994691Losses:  4.138627529144287 0.14405614137649536
CurrentTrain: epoch  4, batch   123 | loss: 4.2826838Losses:  4.1311469078063965 0.07657794654369354
CurrentTrain: epoch  4, batch   124 | loss: 4.2077250Losses:  4.103623867034912 0.17391830682754517
CurrentTrain: epoch  5, batch     0 | loss: 4.2775421Losses:  4.132162094116211 0.15065142512321472
CurrentTrain: epoch  5, batch     1 | loss: 4.2828135Losses:  4.116973876953125 0.11616857349872589
CurrentTrain: epoch  5, batch     2 | loss: 4.2331424Losses:  4.073910713195801 0.12518709897994995
CurrentTrain: epoch  5, batch     3 | loss: 4.1990976Losses:  4.114870071411133 0.13553345203399658
CurrentTrain: epoch  5, batch     4 | loss: 4.2504034Losses:  4.14324951171875 0.06706640124320984
CurrentTrain: epoch  5, batch     5 | loss: 4.2103157Losses:  4.156675338745117 0.10660921037197113
CurrentTrain: epoch  5, batch     6 | loss: 4.2632847Losses:  4.1301774978637695 0.09892524778842926
CurrentTrain: epoch  5, batch     7 | loss: 4.2291026Losses:  4.059133529663086 0.12426268309354782
CurrentTrain: epoch  5, batch     8 | loss: 4.1833963Losses:  4.084603309631348 0.2057904452085495
CurrentTrain: epoch  5, batch     9 | loss: 4.2903938Losses:  3.9308362007141113 0.09865882992744446
CurrentTrain: epoch  5, batch    10 | loss: 4.0294952Losses:  4.030978202819824 0.12305442988872528
CurrentTrain: epoch  5, batch    11 | loss: 4.1540327Losses:  4.060847282409668 0.16738533973693848
CurrentTrain: epoch  5, batch    12 | loss: 4.2282324Losses:  4.061339378356934 0.09646140784025192
CurrentTrain: epoch  5, batch    13 | loss: 4.1578007Losses:  4.139581680297852 0.18636704981327057
CurrentTrain: epoch  5, batch    14 | loss: 4.3259487Losses:  4.123528003692627 0.07677756249904633
CurrentTrain: epoch  5, batch    15 | loss: 4.2003055Losses:  4.240553855895996 0.16974306106567383
CurrentTrain: epoch  5, batch    16 | loss: 4.4102969Losses:  4.1401262283325195 0.1300230622291565
CurrentTrain: epoch  5, batch    17 | loss: 4.2701492Losses:  4.1232523918151855 0.08549031615257263
CurrentTrain: epoch  5, batch    18 | loss: 4.2087426Losses:  4.078892230987549 0.15649500489234924
CurrentTrain: epoch  5, batch    19 | loss: 4.2353873Losses:  4.089268684387207 0.1489320695400238
CurrentTrain: epoch  5, batch    20 | loss: 4.2382007Losses:  4.012251853942871 0.14960967004299164
CurrentTrain: epoch  5, batch    21 | loss: 4.1618614Losses:  4.150589466094971 0.11024599522352219
CurrentTrain: epoch  5, batch    22 | loss: 4.2608356Losses:  4.094518661499023 0.11285872012376785
CurrentTrain: epoch  5, batch    23 | loss: 4.2073774Losses:  4.0772199630737305 0.13518278300762177
CurrentTrain: epoch  5, batch    24 | loss: 4.2124028Losses:  4.0894365310668945 0.09434869885444641
CurrentTrain: epoch  5, batch    25 | loss: 4.1837854Losses:  4.124504089355469 0.1494094878435135
CurrentTrain: epoch  5, batch    26 | loss: 4.2739134Losses:  4.070683479309082 0.13601461052894592
CurrentTrain: epoch  5, batch    27 | loss: 4.2066979Losses:  4.073883056640625 0.12832698225975037
CurrentTrain: epoch  5, batch    28 | loss: 4.2022099Losses:  4.183218479156494 0.05379939079284668
CurrentTrain: epoch  5, batch    29 | loss: 4.2370176Losses:  4.098409652709961 0.17350809276103973
CurrentTrain: epoch  5, batch    30 | loss: 4.2719178Losses:  4.109258651733398 0.10717111080884933
CurrentTrain: epoch  5, batch    31 | loss: 4.2164297Losses:  4.0720415115356445 0.14482145011425018
CurrentTrain: epoch  5, batch    32 | loss: 4.2168632Losses:  4.0820488929748535 0.15298235416412354
CurrentTrain: epoch  5, batch    33 | loss: 4.2350311Losses:  4.128620624542236 0.14178466796875
CurrentTrain: epoch  5, batch    34 | loss: 4.2704053Losses:  4.0161285400390625 0.1496226191520691
CurrentTrain: epoch  5, batch    35 | loss: 4.1657510Losses:  4.07401180267334 0.09409921616315842
CurrentTrain: epoch  5, batch    36 | loss: 4.1681108Losses:  4.119226455688477 0.13279816508293152
CurrentTrain: epoch  5, batch    37 | loss: 4.2520247Losses:  4.1387104988098145 0.12490010261535645
CurrentTrain: epoch  5, batch    38 | loss: 4.2636108Losses:  4.07975959777832 0.13344237208366394
CurrentTrain: epoch  5, batch    39 | loss: 4.2132020Losses:  4.155246734619141 0.1582014113664627
CurrentTrain: epoch  5, batch    40 | loss: 4.3134480Losses:  4.165587902069092 0.14568987488746643
CurrentTrain: epoch  5, batch    41 | loss: 4.3112779Losses:  4.036472320556641 0.054645586758852005
CurrentTrain: epoch  5, batch    42 | loss: 4.0911179Losses:  4.160304546356201 0.14611779153347015
CurrentTrain: epoch  5, batch    43 | loss: 4.3064222Losses:  4.117731094360352 0.12566931545734406
CurrentTrain: epoch  5, batch    44 | loss: 4.2434006Losses:  4.142677307128906 0.20766878128051758
CurrentTrain: epoch  5, batch    45 | loss: 4.3503461Losses:  4.108329772949219 0.1246630847454071
CurrentTrain: epoch  5, batch    46 | loss: 4.2329926Losses:  4.0730509757995605 0.12473864108324051
CurrentTrain: epoch  5, batch    47 | loss: 4.1977897Losses:  4.055936813354492 0.10025100409984589
CurrentTrain: epoch  5, batch    48 | loss: 4.1561880Losses:  4.092384338378906 0.12906759977340698
CurrentTrain: epoch  5, batch    49 | loss: 4.2214518Losses:  4.103217124938965 0.1768605262041092
CurrentTrain: epoch  5, batch    50 | loss: 4.2800775Losses:  4.098824501037598 0.11390427500009537
CurrentTrain: epoch  5, batch    51 | loss: 4.2127290Losses:  4.097988128662109 0.09788006544113159
CurrentTrain: epoch  5, batch    52 | loss: 4.1958680Losses:  4.151401042938232 0.11891279369592667
CurrentTrain: epoch  5, batch    53 | loss: 4.2703137Losses:  4.136785984039307 0.14311860501766205
CurrentTrain: epoch  5, batch    54 | loss: 4.2799044Losses:  4.1597900390625 0.10937073081731796
CurrentTrain: epoch  5, batch    55 | loss: 4.2691607Losses:  4.092719078063965 0.1249111145734787
CurrentTrain: epoch  5, batch    56 | loss: 4.2176304Losses:  4.139254570007324 0.13189373910427094
CurrentTrain: epoch  5, batch    57 | loss: 4.2711482Losses:  4.180606842041016 0.08804395794868469
CurrentTrain: epoch  5, batch    58 | loss: 4.2686510Losses:  4.064116477966309 0.13643184304237366
CurrentTrain: epoch  5, batch    59 | loss: 4.2005482Losses:  4.001565933227539 0.044579699635505676
CurrentTrain: epoch  5, batch    60 | loss: 4.0461454Losses:  4.087972164154053 0.16728943586349487
CurrentTrain: epoch  5, batch    61 | loss: 4.2552614Losses:  4.077742576599121 0.12716105580329895
CurrentTrain: epoch  5, batch    62 | loss: 4.2049036Losses:  4.087554454803467 0.09654735028743744
CurrentTrain: epoch  5, batch    63 | loss: 4.1841016Losses:  4.077824592590332 0.1430029571056366
CurrentTrain: epoch  5, batch    64 | loss: 4.2208276Losses:  4.020063400268555 0.15332598984241486
CurrentTrain: epoch  5, batch    65 | loss: 4.1733894Losses:  4.077683925628662 0.13696779310703278
CurrentTrain: epoch  5, batch    66 | loss: 4.2146516Losses:  4.081536293029785 0.09158369898796082
CurrentTrain: epoch  5, batch    67 | loss: 4.1731200Losses:  4.098883152008057 0.17452853918075562
CurrentTrain: epoch  5, batch    68 | loss: 4.2734118Losses:  4.136521339416504 0.06816485524177551
CurrentTrain: epoch  5, batch    69 | loss: 4.2046862Losses:  4.110604286193848 0.1427939087152481
CurrentTrain: epoch  5, batch    70 | loss: 4.2533984Losses:  4.0647292137146 0.10022228956222534
CurrentTrain: epoch  5, batch    71 | loss: 4.1649513Losses:  4.083690643310547 0.08936278522014618
CurrentTrain: epoch  5, batch    72 | loss: 4.1730533Losses:  3.9715261459350586 0.1056964248418808
CurrentTrain: epoch  5, batch    73 | loss: 4.0772223Losses:  4.069567680358887 0.17725050449371338
CurrentTrain: epoch  5, batch    74 | loss: 4.2468181Losses:  4.102327346801758 0.1039774939417839
CurrentTrain: epoch  5, batch    75 | loss: 4.2063050Losses:  4.088386535644531 0.14397993683815002
CurrentTrain: epoch  5, batch    76 | loss: 4.2323666Losses:  4.072077751159668 0.09461343288421631
CurrentTrain: epoch  5, batch    77 | loss: 4.1666913Losses:  4.030058860778809 0.12523841857910156
CurrentTrain: epoch  5, batch    78 | loss: 4.1552973Losses:  4.140713214874268 0.11610561609268188
CurrentTrain: epoch  5, batch    79 | loss: 4.2568188Losses:  4.0787129402160645 0.1298830807209015
CurrentTrain: epoch  5, batch    80 | loss: 4.2085962Losses:  4.095165252685547 0.1351184993982315
CurrentTrain: epoch  5, batch    81 | loss: 4.2302837Losses:  4.095559597015381 0.1404011845588684
CurrentTrain: epoch  5, batch    82 | loss: 4.2359610Losses:  4.073264122009277 0.11180371046066284
CurrentTrain: epoch  5, batch    83 | loss: 4.1850677Losses:  4.107664585113525 0.11868239939212799
CurrentTrain: epoch  5, batch    84 | loss: 4.2263470Losses:  3.98331880569458 0.13464313745498657
CurrentTrain: epoch  5, batch    85 | loss: 4.1179619Losses:  4.116995334625244 0.09233950823545456
CurrentTrain: epoch  5, batch    86 | loss: 4.2093349Losses:  4.078885078430176 0.15770530700683594
CurrentTrain: epoch  5, batch    87 | loss: 4.2365904Losses:  4.076880931854248 0.0654841810464859
CurrentTrain: epoch  5, batch    88 | loss: 4.1423650Losses:  4.006045341491699 0.10245445370674133
CurrentTrain: epoch  5, batch    89 | loss: 4.1085000Losses:  4.202411651611328 0.07481275498867035
CurrentTrain: epoch  5, batch    90 | loss: 4.2772245Losses:  4.088483810424805 0.08366299420595169
CurrentTrain: epoch  5, batch    91 | loss: 4.1721468Losses:  4.0221147537231445 0.16497254371643066
CurrentTrain: epoch  5, batch    92 | loss: 4.1870871Losses:  4.037112712860107 0.07239877432584763
CurrentTrain: epoch  5, batch    93 | loss: 4.1095114Losses:  4.1719794273376465 0.08406679332256317
CurrentTrain: epoch  5, batch    94 | loss: 4.2560463Losses:  4.419947147369385 0.16206446290016174
CurrentTrain: epoch  5, batch    95 | loss: 4.5820117Losses:  4.0952043533325195 0.1730051040649414
CurrentTrain: epoch  5, batch    96 | loss: 4.2682095Losses:  4.0395660400390625 0.08833745121955872
CurrentTrain: epoch  5, batch    97 | loss: 4.1279035Losses:  4.045480728149414 0.15161854028701782
CurrentTrain: epoch  5, batch    98 | loss: 4.1970992Losses:  4.077603340148926 0.1905825436115265
CurrentTrain: epoch  5, batch    99 | loss: 4.2681861Losses:  4.0636186599731445 0.11159096658229828
CurrentTrain: epoch  5, batch   100 | loss: 4.1752095Losses:  4.095697402954102 0.14433783292770386
CurrentTrain: epoch  5, batch   101 | loss: 4.2400351Losses:  4.04464054107666 0.09985177218914032
CurrentTrain: epoch  5, batch   102 | loss: 4.1444921Losses:  3.9750282764434814 0.18164114654064178
CurrentTrain: epoch  5, batch   103 | loss: 4.1566696Losses:  4.185081481933594 0.16326387226581573
CurrentTrain: epoch  5, batch   104 | loss: 4.3483453Losses:  4.046060085296631 0.10708928108215332
CurrentTrain: epoch  5, batch   105 | loss: 4.1531496Losses:  4.0572710037231445 0.0917634516954422
CurrentTrain: epoch  5, batch   106 | loss: 4.1490345Losses:  3.9826231002807617 0.1710887849330902
CurrentTrain: epoch  5, batch   107 | loss: 4.1537118Losses:  4.057135105133057 0.11991602182388306
CurrentTrain: epoch  5, batch   108 | loss: 4.1770511Losses:  4.074469566345215 0.08660687506198883
CurrentTrain: epoch  5, batch   109 | loss: 4.1610765Losses:  4.078473091125488 0.17565935850143433
CurrentTrain: epoch  5, batch   110 | loss: 4.2541323Losses:  4.049452304840088 0.14926204085350037
CurrentTrain: epoch  5, batch   111 | loss: 4.1987143Losses:  4.534064292907715 0.2524067759513855
CurrentTrain: epoch  5, batch   112 | loss: 4.7864709Losses:  4.034877777099609 0.07621493935585022
CurrentTrain: epoch  5, batch   113 | loss: 4.1110926Losses:  4.096909523010254 0.12699413299560547
CurrentTrain: epoch  5, batch   114 | loss: 4.2239037Losses:  4.086915493011475 0.10347522050142288
CurrentTrain: epoch  5, batch   115 | loss: 4.1903906Losses:  4.082779884338379 0.03913063928484917
CurrentTrain: epoch  5, batch   116 | loss: 4.1219106Losses:  4.190252304077148 0.12398257851600647
CurrentTrain: epoch  5, batch   117 | loss: 4.3142347Losses:  4.045095443725586 0.13262741267681122
CurrentTrain: epoch  5, batch   118 | loss: 4.1777229Losses:  3.9999442100524902 0.10497777163982391
CurrentTrain: epoch  5, batch   119 | loss: 4.1049218Losses:  4.038128852844238 0.06583167612552643
CurrentTrain: epoch  5, batch   120 | loss: 4.1039605Losses:  5.453893184661865 0.3101445436477661
CurrentTrain: epoch  5, batch   121 | loss: 5.7640376Losses:  4.313597679138184 0.17337697744369507
CurrentTrain: epoch  5, batch   122 | loss: 4.4869747Losses:  4.703542709350586 0.14036321640014648
CurrentTrain: epoch  5, batch   123 | loss: 4.8439059Losses:  4.0246405601501465 0.11775895953178406
CurrentTrain: epoch  5, batch   124 | loss: 4.1423993Losses:  3.9878158569335938 0.12271933257579803
CurrentTrain: epoch  6, batch     0 | loss: 4.1105351Losses:  4.080910682678223 0.11672599613666534
CurrentTrain: epoch  6, batch     1 | loss: 4.1976366Losses:  4.119622707366943 0.08163144439458847
CurrentTrain: epoch  6, batch     2 | loss: 4.2012544Losses:  4.112883567810059 0.051825810223817825
CurrentTrain: epoch  6, batch     3 | loss: 4.1647096Losses:  4.104211807250977 0.14719456434249878
CurrentTrain: epoch  6, batch     4 | loss: 4.2514062Losses:  4.022139549255371 0.07188288867473602
CurrentTrain: epoch  6, batch     5 | loss: 4.0940223Losses:  4.0629425048828125 0.15269902348518372
CurrentTrain: epoch  6, batch     6 | loss: 4.2156415Losses:  4.200562477111816 0.10589705407619476
CurrentTrain: epoch  6, batch     7 | loss: 4.3064594Losses:  4.008421897888184 0.13171781599521637
CurrentTrain: epoch  6, batch     8 | loss: 4.1401396Losses:  4.11945915222168 0.20344620943069458
CurrentTrain: epoch  6, batch     9 | loss: 4.3229055Losses:  5.6929779052734375 0.14601194858551025
CurrentTrain: epoch  6, batch    10 | loss: 5.8389897Losses:  4.359454154968262 0.2319962978363037
CurrentTrain: epoch  6, batch    11 | loss: 4.5914507Losses:  4.142866134643555 0.1627463847398758
CurrentTrain: epoch  6, batch    12 | loss: 4.3056126Losses:  4.5391082763671875 0.19319364428520203
CurrentTrain: epoch  6, batch    13 | loss: 4.7323017Losses:  4.0715508460998535 0.15182754397392273
CurrentTrain: epoch  6, batch    14 | loss: 4.2233782Losses:  4.025391578674316 0.05256237834692001
CurrentTrain: epoch  6, batch    15 | loss: 4.0779538Losses:  4.040991306304932 0.14010921120643616
CurrentTrain: epoch  6, batch    16 | loss: 4.1811004Losses:  4.1705732345581055 0.08597715198993683
CurrentTrain: epoch  6, batch    17 | loss: 4.2565503Losses:  4.124916076660156 0.11085738986730576
CurrentTrain: epoch  6, batch    18 | loss: 4.2357736Losses:  4.152152061462402 0.10150107741355896
CurrentTrain: epoch  6, batch    19 | loss: 4.2536530Losses:  4.096902370452881 0.1264713555574417
CurrentTrain: epoch  6, batch    20 | loss: 4.2233739Losses:  4.113725662231445 0.12931334972381592
CurrentTrain: epoch  6, batch    21 | loss: 4.2430391Losses:  4.184786796569824 0.0784183144569397
CurrentTrain: epoch  6, batch    22 | loss: 4.2632051Losses:  4.118950843811035 0.1266731321811676
CurrentTrain: epoch  6, batch    23 | loss: 4.2456241Losses:  4.099643230438232 0.07055702805519104
CurrentTrain: epoch  6, batch    24 | loss: 4.1702003Losses:  4.01281213760376 0.06393197178840637
CurrentTrain: epoch  6, batch    25 | loss: 4.0767441Losses:  4.1210856437683105 0.10922546684741974
CurrentTrain: epoch  6, batch    26 | loss: 4.2303109Losses:  4.1067094802856445 0.17617946863174438
CurrentTrain: epoch  6, batch    27 | loss: 4.2828889Losses:  3.9737839698791504 0.16539639234542847
CurrentTrain: epoch  6, batch    28 | loss: 4.1391802Losses:  4.100005149841309 0.12472604960203171
CurrentTrain: epoch  6, batch    29 | loss: 4.2247310Losses:  4.0223188400268555 0.0843687653541565
CurrentTrain: epoch  6, batch    30 | loss: 4.1066875Losses:  4.1358795166015625 0.1079891100525856
CurrentTrain: epoch  6, batch    31 | loss: 4.2438688Losses:  4.038068771362305 0.14154605567455292
CurrentTrain: epoch  6, batch    32 | loss: 4.1796150Losses:  5.116840362548828 0.3267093002796173
CurrentTrain: epoch  6, batch    33 | loss: 5.4435496Losses:  4.11262845993042 0.1608666032552719
CurrentTrain: epoch  6, batch    34 | loss: 4.2734952Losses:  4.150655746459961 0.06696933507919312
CurrentTrain: epoch  6, batch    35 | loss: 4.2176251Losses:  4.0826568603515625 0.11317472904920578
CurrentTrain: epoch  6, batch    36 | loss: 4.1958318Losses:  4.244437217712402 0.1623799204826355
CurrentTrain: epoch  6, batch    37 | loss: 4.4068170Losses:  4.126742362976074 0.12577179074287415
CurrentTrain: epoch  6, batch    38 | loss: 4.2525144Losses:  4.084042549133301 0.11329860985279083
CurrentTrain: epoch  6, batch    39 | loss: 4.1973410Losses:  4.068606376647949 0.03719443082809448
CurrentTrain: epoch  6, batch    40 | loss: 4.1058006Losses:  4.123403549194336 0.08583036810159683
CurrentTrain: epoch  6, batch    41 | loss: 4.2092338Losses:  4.499543190002441 0.13096404075622559
CurrentTrain: epoch  6, batch    42 | loss: 4.6305075Losses:  4.06688117980957 0.14808523654937744
CurrentTrain: epoch  6, batch    43 | loss: 4.2149663Losses:  4.047434329986572 0.0984429270029068
CurrentTrain: epoch  6, batch    44 | loss: 4.1458774Losses:  4.126335144042969 0.1308254599571228
CurrentTrain: epoch  6, batch    45 | loss: 4.2571607Losses:  4.114710330963135 0.09057335555553436
CurrentTrain: epoch  6, batch    46 | loss: 4.2052836Losses:  4.062174320220947 0.09274717420339584
CurrentTrain: epoch  6, batch    47 | loss: 4.1549215Losses:  4.107410907745361 0.10981172323226929
CurrentTrain: epoch  6, batch    48 | loss: 4.2172227Losses:  4.152260780334473 0.10568736493587494
CurrentTrain: epoch  6, batch    49 | loss: 4.2579479Losses:  4.136968612670898 0.10188703238964081
CurrentTrain: epoch  6, batch    50 | loss: 4.2388558Losses:  4.10190486907959 0.14934852719306946
CurrentTrain: epoch  6, batch    51 | loss: 4.2512536Losses:  4.1343092918396 0.06558595597743988
CurrentTrain: epoch  6, batch    52 | loss: 4.1998954Losses:  4.13955020904541 0.06321187317371368
CurrentTrain: epoch  6, batch    53 | loss: 4.2027621Losses:  4.212896347045898 0.07996772229671478
CurrentTrain: epoch  6, batch    54 | loss: 4.2928638Losses:  3.9993128776550293 0.08882258832454681
CurrentTrain: epoch  6, batch    55 | loss: 4.0881352Losses:  4.059221267700195 0.09140272438526154
CurrentTrain: epoch  6, batch    56 | loss: 4.1506238Losses:  4.223278999328613 0.15383696556091309
CurrentTrain: epoch  6, batch    57 | loss: 4.3771162Losses:  4.083044052124023 0.11915688216686249
CurrentTrain: epoch  6, batch    58 | loss: 4.2022009Losses:  4.087126731872559 0.08899599313735962
CurrentTrain: epoch  6, batch    59 | loss: 4.1761227Losses:  4.017499923706055 0.04869009181857109
CurrentTrain: epoch  6, batch    60 | loss: 4.0661902Losses:  4.126400947570801 0.17273986339569092
CurrentTrain: epoch  6, batch    61 | loss: 4.2991409Losses:  4.042840003967285 0.13088808953762054
CurrentTrain: epoch  6, batch    62 | loss: 4.1737280Losses:  4.045494079589844 0.15366441011428833
CurrentTrain: epoch  6, batch    63 | loss: 4.1991587Losses:  4.085192680358887 0.16875627636909485
CurrentTrain: epoch  6, batch    64 | loss: 4.2539492Losses:  4.20722770690918 0.13589605689048767
CurrentTrain: epoch  6, batch    65 | loss: 4.3431239Losses:  4.050847053527832 0.11062493920326233
CurrentTrain: epoch  6, batch    66 | loss: 4.1614718Losses:  5.186861038208008 0.24337239563465118
CurrentTrain: epoch  6, batch    67 | loss: 5.4302335Losses:  4.057915210723877 0.1656213402748108
CurrentTrain: epoch  6, batch    68 | loss: 4.2235365Losses:  4.159226417541504 0.07611753046512604
CurrentTrain: epoch  6, batch    69 | loss: 4.2353439Losses:  4.145772933959961 0.13407720625400543
CurrentTrain: epoch  6, batch    70 | loss: 4.2798500Losses:  4.093484878540039 0.1461608111858368
CurrentTrain: epoch  6, batch    71 | loss: 4.2396455Losses:  4.0348358154296875 0.05044626072049141
CurrentTrain: epoch  6, batch    72 | loss: 4.0852818Losses:  4.882972717285156 0.17789816856384277
CurrentTrain: epoch  6, batch    73 | loss: 5.0608711Losses:  4.042472839355469 0.05274296924471855
CurrentTrain: epoch  6, batch    74 | loss: 4.0952158Losses:  4.007097244262695 0.06630842387676239
CurrentTrain: epoch  6, batch    75 | loss: 4.0734057Losses:  4.161731719970703 0.07712709158658981
CurrentTrain: epoch  6, batch    76 | loss: 4.2388587Losses:  4.060294151306152 0.04884522780776024
CurrentTrain: epoch  6, batch    77 | loss: 4.1091394Losses:  4.18243408203125 0.07994425296783447
CurrentTrain: epoch  6, batch    78 | loss: 4.2623782Losses:  4.049489974975586 0.1755927950143814
CurrentTrain: epoch  6, batch    79 | loss: 4.2250829Losses:  4.037604808807373 0.1698811650276184
CurrentTrain: epoch  6, batch    80 | loss: 4.2074862Losses:  4.087038040161133 0.09232663363218307
CurrentTrain: epoch  6, batch    81 | loss: 4.1793647Losses:  4.084928512573242 0.14640560746192932
CurrentTrain: epoch  6, batch    82 | loss: 4.2313342Losses:  4.068676948547363 0.07201748341321945
CurrentTrain: epoch  6, batch    83 | loss: 4.1406946Losses:  6.142591953277588 0.3012833297252655
CurrentTrain: epoch  6, batch    84 | loss: 6.4438753Losses:  4.95743989944458 0.06342769414186478
CurrentTrain: epoch  6, batch    85 | loss: 5.0208678Losses:  4.873546600341797 0.09339529275894165
CurrentTrain: epoch  6, batch    86 | loss: 4.9669418Losses:  4.291390895843506 0.10313504934310913
CurrentTrain: epoch  6, batch    87 | loss: 4.3945260Losses:  4.115968704223633 0.08540443331003189
CurrentTrain: epoch  6, batch    88 | loss: 4.2013731Losses:  4.03924560546875 0.09719580411911011
CurrentTrain: epoch  6, batch    89 | loss: 4.1364412Losses:  4.041225433349609 0.03982949256896973
CurrentTrain: epoch  6, batch    90 | loss: 4.0810547Losses:  4.133073806762695 0.09592706710100174
CurrentTrain: epoch  6, batch    91 | loss: 4.2290010Losses:  4.935471057891846 0.07639805972576141
CurrentTrain: epoch  6, batch    92 | loss: 5.0118690Losses:  4.070159912109375 0.14119723439216614
CurrentTrain: epoch  6, batch    93 | loss: 4.2113571Losses:  4.131936073303223 0.07548132538795471
CurrentTrain: epoch  6, batch    94 | loss: 4.2074175Losses:  4.249050140380859 0.07217809557914734
CurrentTrain: epoch  6, batch    95 | loss: 4.3212280Losses:  4.074805736541748 0.09658780694007874
CurrentTrain: epoch  6, batch    96 | loss: 4.1713934Losses:  4.455432891845703 0.09929264336824417
CurrentTrain: epoch  6, batch    97 | loss: 4.5547256Losses:  4.1013617515563965 0.10849997401237488
CurrentTrain: epoch  6, batch    98 | loss: 4.2098618Losses:  4.32435417175293 0.08551429212093353
CurrentTrain: epoch  6, batch    99 | loss: 4.4098682Losses:  4.133998394012451 0.1137646734714508
CurrentTrain: epoch  6, batch   100 | loss: 4.2477632Losses:  4.507249355316162 0.12800365686416626
CurrentTrain: epoch  6, batch   101 | loss: 4.6352530Losses:  4.704845428466797 0.11498836427927017
CurrentTrain: epoch  6, batch   102 | loss: 4.8198338Losses:  4.428577423095703 0.0818442702293396
CurrentTrain: epoch  6, batch   103 | loss: 4.5104218Losses:  4.147152900695801 0.1313180774450302
CurrentTrain: epoch  6, batch   104 | loss: 4.2784710Losses:  4.347410202026367 0.08528831601142883
CurrentTrain: epoch  6, batch   105 | loss: 4.4326987Losses:  4.066007137298584 0.12348747253417969
CurrentTrain: epoch  6, batch   106 | loss: 4.1894946Losses:  4.094888210296631 0.15773290395736694
CurrentTrain: epoch  6, batch   107 | loss: 4.2526212Losses:  4.106672286987305 0.12503384053707123
CurrentTrain: epoch  6, batch   108 | loss: 4.2317061Losses:  4.039671897888184 0.08349192887544632
CurrentTrain: epoch  6, batch   109 | loss: 4.1231637Losses:  4.256394386291504 0.08571811765432358
CurrentTrain: epoch  6, batch   110 | loss: 4.3421125Losses:  4.307353973388672 0.1023544892668724
CurrentTrain: epoch  6, batch   111 | loss: 4.4097085Losses:  4.234869003295898 0.15879398584365845
CurrentTrain: epoch  6, batch   112 | loss: 4.3936629Losses:  4.363348007202148 0.09784995019435883
CurrentTrain: epoch  6, batch   113 | loss: 4.4611979Losses:  4.1235761642456055 0.1094120666384697
CurrentTrain: epoch  6, batch   114 | loss: 4.2329884Losses:  4.038484573364258 0.10993539541959763
CurrentTrain: epoch  6, batch   115 | loss: 4.1484199Losses:  4.155009746551514 0.12477326393127441
CurrentTrain: epoch  6, batch   116 | loss: 4.2797832Losses:  4.467455863952637 0.09538868069648743
CurrentTrain: epoch  6, batch   117 | loss: 4.5628448Losses:  4.098843574523926 0.096255823969841
CurrentTrain: epoch  6, batch   118 | loss: 4.1950994Losses:  4.107970237731934 0.1249774843454361
CurrentTrain: epoch  6, batch   119 | loss: 4.2329478Losses:  4.117114543914795 0.07932817935943604
CurrentTrain: epoch  6, batch   120 | loss: 4.1964426Losses:  4.203134536743164 0.11508657038211823
CurrentTrain: epoch  6, batch   121 | loss: 4.3182211Losses:  3.9765303134918213 0.13890573382377625
CurrentTrain: epoch  6, batch   122 | loss: 4.1154361Losses:  4.169814109802246 0.08933864533901215
CurrentTrain: epoch  6, batch   123 | loss: 4.2591529Losses:  4.281979560852051 0.15504147112369537
CurrentTrain: epoch  6, batch   124 | loss: 4.4370213Losses:  4.08849573135376 0.05904596298933029
CurrentTrain: epoch  7, batch     0 | loss: 4.1475415Losses:  4.215907573699951 0.12326965481042862
CurrentTrain: epoch  7, batch     1 | loss: 4.3391771Losses:  4.133656978607178 0.10131235420703888
CurrentTrain: epoch  7, batch     2 | loss: 4.2349691Losses:  4.089699745178223 0.1301266998052597
CurrentTrain: epoch  7, batch     3 | loss: 4.2198262Losses:  4.079079627990723 0.09924902766942978
CurrentTrain: epoch  7, batch     4 | loss: 4.1783285Losses:  4.166259765625 0.16372600197792053
CurrentTrain: epoch  7, batch     5 | loss: 4.3299856Losses:  4.185268402099609 0.12069141864776611
CurrentTrain: epoch  7, batch     6 | loss: 4.3059597Losses:  4.338228225708008 0.09285986423492432
CurrentTrain: epoch  7, batch     7 | loss: 4.4310880Losses:  4.0688300132751465 0.17866165935993195
CurrentTrain: epoch  7, batch     8 | loss: 4.2474918Losses:  4.086226463317871 0.07817661017179489
CurrentTrain: epoch  7, batch     9 | loss: 4.1644030Losses:  4.166388511657715 0.07895883917808533
CurrentTrain: epoch  7, batch    10 | loss: 4.2453475Losses:  4.323116302490234 0.09118449687957764
CurrentTrain: epoch  7, batch    11 | loss: 4.4143009Losses:  4.150430202484131 0.11902730166912079
CurrentTrain: epoch  7, batch    12 | loss: 4.2694573Losses:  4.142988204956055 0.07222767919301987
CurrentTrain: epoch  7, batch    13 | loss: 4.2152157Losses:  4.183711051940918 0.10348760336637497
CurrentTrain: epoch  7, batch    14 | loss: 4.2871985Losses:  4.081847190856934 0.07499045133590698
CurrentTrain: epoch  7, batch    15 | loss: 4.1568375Losses:  4.172153949737549 0.06550759822130203
CurrentTrain: epoch  7, batch    16 | loss: 4.2376614Losses:  3.9762473106384277 0.04497426003217697
CurrentTrain: epoch  7, batch    17 | loss: 4.0212216Losses:  4.243122577667236 0.17671625316143036
CurrentTrain: epoch  7, batch    18 | loss: 4.4198389Losses:  4.098031044006348 0.08485988527536392
CurrentTrain: epoch  7, batch    19 | loss: 4.1828909Losses:  4.136048793792725 0.13244648277759552
CurrentTrain: epoch  7, batch    20 | loss: 4.2684951Losses:  4.139240264892578 0.14571328461170197
CurrentTrain: epoch  7, batch    21 | loss: 4.2849536Losses:  4.155886650085449 0.08533263206481934
CurrentTrain: epoch  7, batch    22 | loss: 4.2412195Losses:  4.1685333251953125 0.03705570846796036
CurrentTrain: epoch  7, batch    23 | loss: 4.2055888Losses:  4.04122257232666 0.10259739309549332
CurrentTrain: epoch  7, batch    24 | loss: 4.1438198Losses:  4.149840354919434 0.11889401078224182
CurrentTrain: epoch  7, batch    25 | loss: 4.2687345Losses:  4.19425106048584 0.18045559525489807
CurrentTrain: epoch  7, batch    26 | loss: 4.3747067Losses:  4.180562973022461 0.10798031091690063
CurrentTrain: epoch  7, batch    27 | loss: 4.2885432Losses:  4.193487167358398 0.11650097370147705
CurrentTrain: epoch  7, batch    28 | loss: 4.3099880Losses:  4.104100227355957 0.13858023285865784
CurrentTrain: epoch  7, batch    29 | loss: 4.2426805Losses:  4.1917829513549805 0.11422320455312729
CurrentTrain: epoch  7, batch    30 | loss: 4.3060060Losses:  4.103703022003174 0.09931190311908722
CurrentTrain: epoch  7, batch    31 | loss: 4.2030149Losses:  4.044629096984863 0.051549363881349564
CurrentTrain: epoch  7, batch    32 | loss: 4.0961785Losses:  4.077042579650879 0.07153823971748352
CurrentTrain: epoch  7, batch    33 | loss: 4.1485810Losses:  4.375559329986572 0.06851593405008316
CurrentTrain: epoch  7, batch    34 | loss: 4.4440751Losses:  4.2626142501831055 0.0897384062409401
CurrentTrain: epoch  7, batch    35 | loss: 4.3523526Losses:  4.073505401611328 0.061797190457582474
CurrentTrain: epoch  7, batch    36 | loss: 4.1353025Losses:  4.060710430145264 0.08337382227182388
CurrentTrain: epoch  7, batch    37 | loss: 4.1440845Losses:  4.002117156982422 0.06533843278884888
CurrentTrain: epoch  7, batch    38 | loss: 4.0674558Losses:  4.283334732055664 0.1505158245563507
CurrentTrain: epoch  7, batch    39 | loss: 4.4338508Losses:  4.106427192687988 0.07862132042646408
CurrentTrain: epoch  7, batch    40 | loss: 4.1850486Losses:  4.092246055603027 0.1124766394495964
CurrentTrain: epoch  7, batch    41 | loss: 4.2047229Losses:  4.203068733215332 0.09679767489433289
CurrentTrain: epoch  7, batch    42 | loss: 4.2998662Losses:  4.212639808654785 0.07284633815288544
CurrentTrain: epoch  7, batch    43 | loss: 4.2854862Losses:  4.191117763519287 0.13544084131717682
CurrentTrain: epoch  7, batch    44 | loss: 4.3265586Losses:  4.082606315612793 0.10707685351371765
CurrentTrain: epoch  7, batch    45 | loss: 4.1896830Losses:  4.096972465515137 0.08245760947465897
CurrentTrain: epoch  7, batch    46 | loss: 4.1794300Losses:  4.097750663757324 0.13164329528808594
CurrentTrain: epoch  7, batch    47 | loss: 4.2293940Losses:  4.09674072265625 0.13832128047943115
CurrentTrain: epoch  7, batch    48 | loss: 4.2350621Losses:  4.036227226257324 0.07459641993045807
CurrentTrain: epoch  7, batch    49 | loss: 4.1108236Losses:  4.074715614318848 0.06908757984638214
CurrentTrain: epoch  7, batch    50 | loss: 4.1438031Losses:  4.20218563079834 0.15954503417015076
CurrentTrain: epoch  7, batch    51 | loss: 4.3617306Losses:  4.160107135772705 0.12286579608917236
CurrentTrain: epoch  7, batch    52 | loss: 4.2829728Losses:  4.047243595123291 0.0630100816488266
CurrentTrain: epoch  7, batch    53 | loss: 4.1102538Losses:  4.1601691246032715 0.06556620448827744
CurrentTrain: epoch  7, batch    54 | loss: 4.2257352Losses:  4.018050193786621 0.07874718308448792
CurrentTrain: epoch  7, batch    55 | loss: 4.0967975Losses:  4.041927814483643 0.06872627139091492
CurrentTrain: epoch  7, batch    56 | loss: 4.1106539Losses:  4.094715118408203 0.09121174365282059
CurrentTrain: epoch  7, batch    57 | loss: 4.1859269Losses:  4.1805009841918945 0.13550841808319092
CurrentTrain: epoch  7, batch    58 | loss: 4.3160095Losses:  4.251662731170654 0.13784652948379517
CurrentTrain: epoch  7, batch    59 | loss: 4.3895092Losses:  4.0538740158081055 0.15175971388816833
CurrentTrain: epoch  7, batch    60 | loss: 4.2056336Losses:  4.104475975036621 0.11777225136756897
CurrentTrain: epoch  7, batch    61 | loss: 4.2222481Losses:  4.235363006591797 0.11739002913236618
CurrentTrain: epoch  7, batch    62 | loss: 4.3527532Losses:  4.055346488952637 0.07800477743148804
CurrentTrain: epoch  7, batch    63 | loss: 4.1333513Losses:  3.975370407104492 0.062396340072155
CurrentTrain: epoch  7, batch    64 | loss: 4.0377669Losses:  3.995285749435425 0.110526904463768
CurrentTrain: epoch  7, batch    65 | loss: 4.1058125Losses:  4.154483318328857 0.07660737633705139
CurrentTrain: epoch  7, batch    66 | loss: 4.2310905Losses:  4.124096870422363 0.07565073668956757
CurrentTrain: epoch  7, batch    67 | loss: 4.1997476Losses:  4.0384016036987305 0.0574665442109108
CurrentTrain: epoch  7, batch    68 | loss: 4.0958681Losses:  4.076013565063477 0.1290949583053589
CurrentTrain: epoch  7, batch    69 | loss: 4.2051086Losses:  4.100130081176758 0.04864847660064697
CurrentTrain: epoch  7, batch    70 | loss: 4.1487784Losses:  4.153357028961182 0.11118926852941513
CurrentTrain: epoch  7, batch    71 | loss: 4.2645464Losses:  4.205245018005371 0.07977382838726044
CurrentTrain: epoch  7, batch    72 | loss: 4.2850189Losses:  4.027093887329102 0.07308462262153625
CurrentTrain: epoch  7, batch    73 | loss: 4.1001787Losses:  4.08371639251709 0.115737684071064
CurrentTrain: epoch  7, batch    74 | loss: 4.1994543Losses:  4.033903121948242 0.09751090407371521
CurrentTrain: epoch  7, batch    75 | loss: 4.1314139Losses:  4.077840805053711 0.0683843195438385
CurrentTrain: epoch  7, batch    76 | loss: 4.1462250Losses:  4.067139148712158 0.05750842019915581
CurrentTrain: epoch  7, batch    77 | loss: 4.1246476Losses:  4.153321266174316 0.08610781282186508
CurrentTrain: epoch  7, batch    78 | loss: 4.2394290Losses:  4.086698532104492 0.09004072844982147
CurrentTrain: epoch  7, batch    79 | loss: 4.1767392Losses:  3.98164701461792 0.08353076875209808
CurrentTrain: epoch  7, batch    80 | loss: 4.0651779Losses:  4.0628252029418945 0.1350381225347519
CurrentTrain: epoch  7, batch    81 | loss: 4.1978631Losses:  4.035069465637207 0.0823625698685646
CurrentTrain: epoch  7, batch    82 | loss: 4.1174321Losses:  4.017183303833008 0.08309438079595566
CurrentTrain: epoch  7, batch    83 | loss: 4.1002779Losses:  4.065522193908691 0.11412770301103592
CurrentTrain: epoch  7, batch    84 | loss: 4.1796498Losses:  4.101526260375977 0.07515013217926025
CurrentTrain: epoch  7, batch    85 | loss: 4.1766763Losses:  4.036349296569824 0.09702505171298981
CurrentTrain: epoch  7, batch    86 | loss: 4.1333742Losses:  3.965846061706543 0.06079309061169624
CurrentTrain: epoch  7, batch    87 | loss: 4.0266390Losses:  4.035439491271973 0.08697211742401123
CurrentTrain: epoch  7, batch    88 | loss: 4.1224117Losses:  4.007754802703857 0.10630840063095093
CurrentTrain: epoch  7, batch    89 | loss: 4.1140633Losses:  4.04199743270874 0.15964153409004211
CurrentTrain: epoch  7, batch    90 | loss: 4.2016392Losses:  4.027653694152832 0.0780499279499054
CurrentTrain: epoch  7, batch    91 | loss: 4.1057038Losses:  4.048836708068848 0.03135811537504196
CurrentTrain: epoch  7, batch    92 | loss: 4.0801950Losses:  4.068430423736572 0.1368042230606079
CurrentTrain: epoch  7, batch    93 | loss: 4.2052345Losses:  4.064548015594482 0.06662286818027496
CurrentTrain: epoch  7, batch    94 | loss: 4.1311707Losses:  4.007689476013184 0.07975374162197113
CurrentTrain: epoch  7, batch    95 | loss: 4.0874434Losses:  4.044713020324707 0.11896060407161713
CurrentTrain: epoch  7, batch    96 | loss: 4.1636734Losses:  4.121867656707764 0.11762645840644836
CurrentTrain: epoch  7, batch    97 | loss: 4.2394943Losses:  4.0321807861328125 0.09845159947872162
CurrentTrain: epoch  7, batch    98 | loss: 4.1306324Losses:  4.037927627563477 0.02398211508989334
CurrentTrain: epoch  7, batch    99 | loss: 4.0619097Losses:  3.9546141624450684 0.07208280265331268
CurrentTrain: epoch  7, batch   100 | loss: 4.0266972Losses:  4.085567474365234 0.09300846606492996
CurrentTrain: epoch  7, batch   101 | loss: 4.1785760Losses:  4.011955261230469 0.09983561187982559
CurrentTrain: epoch  7, batch   102 | loss: 4.1117907Losses:  4.038407802581787 0.12128926068544388
CurrentTrain: epoch  7, batch   103 | loss: 4.1596971Losses:  4.066035270690918 0.07788258790969849
CurrentTrain: epoch  7, batch   104 | loss: 4.1439180Losses:  4.029007434844971 0.12222298234701157
CurrentTrain: epoch  7, batch   105 | loss: 4.1512303Losses:  4.035586833953857 0.14537444710731506
CurrentTrain: epoch  7, batch   106 | loss: 4.1809611Losses:  3.9952239990234375 0.09179876744747162
CurrentTrain: epoch  7, batch   107 | loss: 4.0870228Losses:  4.046116352081299 0.0602264404296875
CurrentTrain: epoch  7, batch   108 | loss: 4.1063428Losses:  4.08629035949707 0.16518935561180115
CurrentTrain: epoch  7, batch   109 | loss: 4.2514796Losses:  3.972569465637207 0.11739987879991531
CurrentTrain: epoch  7, batch   110 | loss: 4.0899692Losses:  4.017759799957275 0.07470212131738663
CurrentTrain: epoch  7, batch   111 | loss: 4.0924621Losses:  4.026607990264893 0.12902924418449402
CurrentTrain: epoch  7, batch   112 | loss: 4.1556373Losses:  3.9867758750915527 0.045853324234485626
CurrentTrain: epoch  7, batch   113 | loss: 4.0326290Losses:  4.023563861846924 0.09240284562110901
CurrentTrain: epoch  7, batch   114 | loss: 4.1159668Losses:  4.020135879516602 0.11805896461009979
CurrentTrain: epoch  7, batch   115 | loss: 4.1381950Losses:  4.032495975494385 0.045533791184425354
CurrentTrain: epoch  7, batch   116 | loss: 4.0780296Losses:  4.0693769454956055 0.10637238621711731
CurrentTrain: epoch  7, batch   117 | loss: 4.1757493Losses:  4.015238285064697 0.11885052919387817
CurrentTrain: epoch  7, batch   118 | loss: 4.1340890Losses:  4.073143005371094 0.04330308735370636
CurrentTrain: epoch  7, batch   119 | loss: 4.1164460Losses:  4.026663780212402 0.15293331444263458
CurrentTrain: epoch  7, batch   120 | loss: 4.1795969Losses:  4.004390716552734 0.1259467899799347
CurrentTrain: epoch  7, batch   121 | loss: 4.1303377Losses:  3.997093677520752 0.1431729644536972
CurrentTrain: epoch  7, batch   122 | loss: 4.1402664Losses:  4.046334743499756 0.11859499663114548
CurrentTrain: epoch  7, batch   123 | loss: 4.1649299Losses:  4.034838676452637 0.09318454563617706
CurrentTrain: epoch  7, batch   124 | loss: 4.1280231Losses:  4.059724807739258 0.07981635630130768
CurrentTrain: epoch  8, batch     0 | loss: 4.1395411Losses:  4.065139293670654 0.05298950523138046
CurrentTrain: epoch  8, batch     1 | loss: 4.1181288Losses:  4.06093168258667 0.16425921022891998
CurrentTrain: epoch  8, batch     2 | loss: 4.2251911Losses:  4.036501884460449 0.06342148780822754
CurrentTrain: epoch  8, batch     3 | loss: 4.0999231Losses:  4.038188457489014 0.1687978208065033
CurrentTrain: epoch  8, batch     4 | loss: 4.2069864Losses:  3.964726448059082 0.12584275007247925
CurrentTrain: epoch  8, batch     5 | loss: 4.0905690Losses:  3.956047296524048 0.08552367985248566
CurrentTrain: epoch  8, batch     6 | loss: 4.0415711Losses:  3.9465363025665283 0.08094704151153564
CurrentTrain: epoch  8, batch     7 | loss: 4.0274835Losses:  3.9478046894073486 0.0607014074921608
CurrentTrain: epoch  8, batch     8 | loss: 4.0085063Losses:  4.018466949462891 0.05748314410448074
CurrentTrain: epoch  8, batch     9 | loss: 4.0759501Losses:  4.06580114364624 0.1299515664577484
CurrentTrain: epoch  8, batch    10 | loss: 4.1957526Losses:  3.905489921569824 0.11953102052211761
CurrentTrain: epoch  8, batch    11 | loss: 4.0250211Losses:  4.015686511993408 0.0724247470498085
CurrentTrain: epoch  8, batch    12 | loss: 4.0881114Losses:  3.978807210922241 0.1256098449230194
CurrentTrain: epoch  8, batch    13 | loss: 4.1044168Losses:  3.9818310737609863 0.07217010855674744
CurrentTrain: epoch  8, batch    14 | loss: 4.0540013Losses:  3.994502067565918 0.15161779522895813
CurrentTrain: epoch  8, batch    15 | loss: 4.1461201Losses:  4.030524730682373 0.06167849153280258
CurrentTrain: epoch  8, batch    16 | loss: 4.0922031Losses:  3.9991252422332764 0.06239304691553116
CurrentTrain: epoch  8, batch    17 | loss: 4.0615182Losses:  4.084223747253418 0.06618577241897583
CurrentTrain: epoch  8, batch    18 | loss: 4.1504097Losses:  3.996842622756958 0.055767226964235306
CurrentTrain: epoch  8, batch    19 | loss: 4.0526099Losses:  3.9957566261291504 0.06583785265684128
CurrentTrain: epoch  8, batch    20 | loss: 4.0615945Losses:  3.976296901702881 0.1032547652721405
CurrentTrain: epoch  8, batch    21 | loss: 4.0795517Losses:  4.020082473754883 0.06242557242512703
CurrentTrain: epoch  8, batch    22 | loss: 4.0825081Losses:  3.9820504188537598 0.1448482871055603
CurrentTrain: epoch  8, batch    23 | loss: 4.1268988Losses:  3.9882378578186035 0.1052633672952652
CurrentTrain: epoch  8, batch    24 | loss: 4.0935011Losses:  4.012321472167969 0.05708247423171997
CurrentTrain: epoch  8, batch    25 | loss: 4.0694041Losses:  4.013211250305176 0.11379648000001907
CurrentTrain: epoch  8, batch    26 | loss: 4.1270080Losses:  4.005854606628418 0.08367519080638885
CurrentTrain: epoch  8, batch    27 | loss: 4.0895300Losses:  4.019787788391113 0.07842407375574112
CurrentTrain: epoch  8, batch    28 | loss: 4.0982118Losses:  4.013764381408691 0.05737315118312836
CurrentTrain: epoch  8, batch    29 | loss: 4.0711374Losses:  3.989678382873535 0.09679944813251495
CurrentTrain: epoch  8, batch    30 | loss: 4.0864778Losses:  3.9989752769470215 0.04814793914556503
CurrentTrain: epoch  8, batch    31 | loss: 4.0471234Losses:  3.9928436279296875 0.10047093778848648
CurrentTrain: epoch  8, batch    32 | loss: 4.0933146Losses:  4.009426593780518 0.058611638844013214
CurrentTrain: epoch  8, batch    33 | loss: 4.0680385Losses:  4.02072811126709 0.10232667624950409
CurrentTrain: epoch  8, batch    34 | loss: 4.1230550Losses:  3.970853090286255 0.09039989113807678
CurrentTrain: epoch  8, batch    35 | loss: 4.0612531Losses:  3.973940849304199 0.1452757567167282
CurrentTrain: epoch  8, batch    36 | loss: 4.1192164Losses:  4.089925765991211 0.07149747014045715
CurrentTrain: epoch  8, batch    37 | loss: 4.1614232Losses:  4.000378608703613 0.09965842962265015
CurrentTrain: epoch  8, batch    38 | loss: 4.1000371Losses:  3.9879024028778076 0.13044464588165283
CurrentTrain: epoch  8, batch    39 | loss: 4.1183472Losses:  3.967533588409424 0.07291434705257416
CurrentTrain: epoch  8, batch    40 | loss: 4.0404477Losses:  4.020304203033447 0.052674226462841034
CurrentTrain: epoch  8, batch    41 | loss: 4.0729785Losses:  4.051476001739502 0.0893356204032898
CurrentTrain: epoch  8, batch    42 | loss: 4.1408114Losses:  3.9855737686157227 0.10591253638267517
CurrentTrain: epoch  8, batch    43 | loss: 4.0914865Losses:  4.006710529327393 0.06754085421562195
CurrentTrain: epoch  8, batch    44 | loss: 4.0742512Losses:  4.020536422729492 0.1165086179971695
CurrentTrain: epoch  8, batch    45 | loss: 4.1370449Losses:  3.941234588623047 0.03383168205618858
CurrentTrain: epoch  8, batch    46 | loss: 3.9750662Losses:  4.018346309661865 0.08755272626876831
CurrentTrain: epoch  8, batch    47 | loss: 4.1058989Losses:  4.053426742553711 0.07674261927604675
CurrentTrain: epoch  8, batch    48 | loss: 4.1301694Losses:  3.946573495864868 0.12028634548187256
CurrentTrain: epoch  8, batch    49 | loss: 4.0668597Losses:  4.047147274017334 0.14015768468379974
CurrentTrain: epoch  8, batch    50 | loss: 4.1873050Losses:  3.976579427719116 0.11327984929084778
CurrentTrain: epoch  8, batch    51 | loss: 4.0898595Losses:  4.065679550170898 0.059586748480796814
CurrentTrain: epoch  8, batch    52 | loss: 4.1252661Losses:  4.009528636932373 0.11584928631782532
CurrentTrain: epoch  8, batch    53 | loss: 4.1253781Losses:  3.9579663276672363 0.07954161614179611
CurrentTrain: epoch  8, batch    54 | loss: 4.0375080Losses:  4.0460076332092285 0.11542290449142456
CurrentTrain: epoch  8, batch    55 | loss: 4.1614304Losses:  3.9448647499084473 0.10581986606121063
CurrentTrain: epoch  8, batch    56 | loss: 4.0506845Losses:  4.007446765899658 0.08615545928478241
CurrentTrain: epoch  8, batch    57 | loss: 4.0936022Losses:  4.071507453918457 0.07799045741558075
CurrentTrain: epoch  8, batch    58 | loss: 4.1494980Losses:  3.991940975189209 0.061143659055233
CurrentTrain: epoch  8, batch    59 | loss: 4.0530849Losses:  3.9634251594543457 0.04743114486336708
CurrentTrain: epoch  8, batch    60 | loss: 4.0108562Losses:  4.016493797302246 0.1059669479727745
CurrentTrain: epoch  8, batch    61 | loss: 4.1224608Losses:  4.07855749130249 0.07090479880571365
CurrentTrain: epoch  8, batch    62 | loss: 4.1494622Losses:  3.977285385131836 0.11472290754318237
CurrentTrain: epoch  8, batch    63 | loss: 4.0920081Losses:  3.967006206512451 0.0393022820353508
CurrentTrain: epoch  8, batch    64 | loss: 4.0063086Losses:  3.986138343811035 0.1547738015651703
CurrentTrain: epoch  8, batch    65 | loss: 4.1409121Losses:  3.9877190589904785 0.10445517301559448
CurrentTrain: epoch  8, batch    66 | loss: 4.0921741Losses:  4.024728775024414 0.08549623936414719
CurrentTrain: epoch  8, batch    67 | loss: 4.1102252Losses:  4.071579933166504 0.08737501502037048
CurrentTrain: epoch  8, batch    68 | loss: 4.1589551Losses:  3.987703561782837 0.06481537222862244
CurrentTrain: epoch  8, batch    69 | loss: 4.0525188Losses:  3.9579834938049316 0.11205202341079712
CurrentTrain: epoch  8, batch    70 | loss: 4.0700355Losses:  4.044301986694336 0.0728553831577301
CurrentTrain: epoch  8, batch    71 | loss: 4.1171575Losses:  4.059122562408447 0.06667010486125946
CurrentTrain: epoch  8, batch    72 | loss: 4.1257925Losses:  3.992448329925537 0.14629465341567993
CurrentTrain: epoch  8, batch    73 | loss: 4.1387429Losses:  3.985316514968872 0.11470295488834381
CurrentTrain: epoch  8, batch    74 | loss: 4.1000195Losses:  4.001831531524658 0.09657023847103119
CurrentTrain: epoch  8, batch    75 | loss: 4.0984015Losses:  3.912494659423828 0.081101194024086
CurrentTrain: epoch  8, batch    76 | loss: 3.9935958Losses:  3.9872589111328125 0.08033330738544464
CurrentTrain: epoch  8, batch    77 | loss: 4.0675921Losses:  3.9756903648376465 0.15468670427799225
CurrentTrain: epoch  8, batch    78 | loss: 4.1303773Losses:  3.9068918228149414 0.05261143296957016
CurrentTrain: epoch  8, batch    79 | loss: 3.9595032Losses:  4.003253936767578 0.0611269548535347
CurrentTrain: epoch  8, batch    80 | loss: 4.0643811Losses:  3.9860002994537354 0.06460753828287125
CurrentTrain: epoch  8, batch    81 | loss: 4.0506077Losses:  3.963315486907959 0.07434193044900894
CurrentTrain: epoch  8, batch    82 | loss: 4.0376573Losses:  3.9767098426818848 0.09304400533437729
CurrentTrain: epoch  8, batch    83 | loss: 4.0697536Losses:  4.029959201812744 0.07681065052747726
CurrentTrain: epoch  8, batch    84 | loss: 4.1067700Losses:  4.041903495788574 0.06258442997932434
CurrentTrain: epoch  8, batch    85 | loss: 4.1044879Losses:  4.0143723487854 0.08605940639972687
CurrentTrain: epoch  8, batch    86 | loss: 4.1004319Losses:  4.0253400802612305 0.07128894329071045
CurrentTrain: epoch  8, batch    87 | loss: 4.0966291Losses:  4.030936241149902 0.12550920248031616
CurrentTrain: epoch  8, batch    88 | loss: 4.1564455Losses:  3.9543910026550293 0.10690052807331085
CurrentTrain: epoch  8, batch    89 | loss: 4.0612917Losses:  3.9691035747528076 0.09839397668838501
CurrentTrain: epoch  8, batch    90 | loss: 4.0674977Losses:  3.9956979751586914 0.07676654309034348
CurrentTrain: epoch  8, batch    91 | loss: 4.0724645Losses:  4.014650344848633 0.08793526887893677
CurrentTrain: epoch  8, batch    92 | loss: 4.1025858Losses:  4.000509738922119 0.07197660952806473
CurrentTrain: epoch  8, batch    93 | loss: 4.0724864Losses:  3.978566884994507 0.04471093416213989
CurrentTrain: epoch  8, batch    94 | loss: 4.0232778Losses:  4.040119647979736 0.08935245871543884
CurrentTrain: epoch  8, batch    95 | loss: 4.1294723Losses:  4.014402389526367 0.037613146007061005
CurrentTrain: epoch  8, batch    96 | loss: 4.0520153Losses:  4.029884338378906 0.14064179360866547
CurrentTrain: epoch  8, batch    97 | loss: 4.1705260Losses:  4.063182353973389 0.08256170153617859
CurrentTrain: epoch  8, batch    98 | loss: 4.1457438Losses:  4.017826080322266 0.10123289376497269
CurrentTrain: epoch  8, batch    99 | loss: 4.1190591Losses:  3.977691650390625 0.07381189614534378
CurrentTrain: epoch  8, batch   100 | loss: 4.0515037Losses:  3.9990012645721436 0.06200370192527771
CurrentTrain: epoch  8, batch   101 | loss: 4.0610051Losses:  4.013580322265625 0.07585430145263672
CurrentTrain: epoch  8, batch   102 | loss: 4.0894346Losses:  3.9704580307006836 0.10636578500270844
CurrentTrain: epoch  8, batch   103 | loss: 4.0768237Losses:  3.991692066192627 0.08290819823741913
CurrentTrain: epoch  8, batch   104 | loss: 4.0746002Losses:  3.9684605598449707 0.0911712571978569
CurrentTrain: epoch  8, batch   105 | loss: 4.0596318Losses:  4.007689476013184 0.1383265256881714
CurrentTrain: epoch  8, batch   106 | loss: 4.1460161Losses:  3.991525650024414 0.09838917851448059
CurrentTrain: epoch  8, batch   107 | loss: 4.0899148Losses:  4.019680500030518 0.07385692000389099
CurrentTrain: epoch  8, batch   108 | loss: 4.0935373Losses:  3.987619400024414 0.11965644359588623
CurrentTrain: epoch  8, batch   109 | loss: 4.1072760Losses:  4.03601598739624 0.12811076641082764
CurrentTrain: epoch  8, batch   110 | loss: 4.1641269Losses:  4.005141735076904 0.06846695393323898
CurrentTrain: epoch  8, batch   111 | loss: 4.0736089Losses:  3.984494209289551 0.08052057027816772
CurrentTrain: epoch  8, batch   112 | loss: 4.0650148Losses:  4.001239776611328 0.07775989919900894
CurrentTrain: epoch  8, batch   113 | loss: 4.0789995Losses:  3.9397506713867188 0.0690319612622261
CurrentTrain: epoch  8, batch   114 | loss: 4.0087829Losses:  3.967900276184082 0.07350336015224457
CurrentTrain: epoch  8, batch   115 | loss: 4.0414038Losses:  3.89520263671875 0.05024193972349167
CurrentTrain: epoch  8, batch   116 | loss: 3.9454446Losses:  4.027377128601074 0.08140462636947632
CurrentTrain: epoch  8, batch   117 | loss: 4.1087818Losses:  3.957078456878662 0.07940088212490082
CurrentTrain: epoch  8, batch   118 | loss: 4.0364795Losses:  4.002813339233398 0.09806554019451141
CurrentTrain: epoch  8, batch   119 | loss: 4.1008787Losses:  4.000012397766113 0.03789978474378586
CurrentTrain: epoch  8, batch   120 | loss: 4.0379124Losses:  4.027135848999023 0.10838113725185394
CurrentTrain: epoch  8, batch   121 | loss: 4.1355171Losses:  3.965200901031494 0.07725799083709717
CurrentTrain: epoch  8, batch   122 | loss: 4.0424590Losses:  3.9736905097961426 0.12026563286781311
CurrentTrain: epoch  8, batch   123 | loss: 4.0939560Losses:  4.005390167236328 0.09986604750156403
CurrentTrain: epoch  8, batch   124 | loss: 4.1052561Losses:  4.006743431091309 0.11211864650249481
CurrentTrain: epoch  9, batch     0 | loss: 4.1188622Losses:  4.005182266235352 0.06302867829799652
CurrentTrain: epoch  9, batch     1 | loss: 4.0682111Losses:  4.015676498413086 0.10691501200199127
CurrentTrain: epoch  9, batch     2 | loss: 4.1225915Losses:  3.93521785736084 0.0669952780008316
CurrentTrain: epoch  9, batch     3 | loss: 4.0022130Losses:  3.993168830871582 0.12355279922485352
CurrentTrain: epoch  9, batch     4 | loss: 4.1167216Losses:  3.985626459121704 0.09729380905628204
CurrentTrain: epoch  9, batch     5 | loss: 4.0829201Losses:  3.973855972290039 0.11787094920873642
CurrentTrain: epoch  9, batch     6 | loss: 4.0917268Losses:  4.000502586364746 0.0823250561952591
CurrentTrain: epoch  9, batch     7 | loss: 4.0828276Losses:  3.979867935180664 0.04189566522836685
CurrentTrain: epoch  9, batch     8 | loss: 4.0217638Losses:  3.9949536323547363 0.08990399539470673
CurrentTrain: epoch  9, batch     9 | loss: 4.0848575Losses:  4.034004211425781 0.11222492158412933
CurrentTrain: epoch  9, batch    10 | loss: 4.1462293Losses:  3.997922420501709 0.07733502238988876
CurrentTrain: epoch  9, batch    11 | loss: 4.0752573Losses:  3.979368209838867 0.09682700037956238
CurrentTrain: epoch  9, batch    12 | loss: 4.0761952Losses:  4.041847229003906 0.0784417986869812
CurrentTrain: epoch  9, batch    13 | loss: 4.1202888Losses:  3.9653878211975098 0.08219315111637115
CurrentTrain: epoch  9, batch    14 | loss: 4.0475812Losses:  3.99257755279541 0.06128402799367905
CurrentTrain: epoch  9, batch    15 | loss: 4.0538616Losses:  3.959649085998535 0.07224615663290024
CurrentTrain: epoch  9, batch    16 | loss: 4.0318952Losses:  4.006470680236816 0.07817015051841736
CurrentTrain: epoch  9, batch    17 | loss: 4.0846410Losses:  3.965147018432617 0.07539783418178558
CurrentTrain: epoch  9, batch    18 | loss: 4.0405450Losses:  3.9859070777893066 0.1380327045917511
CurrentTrain: epoch  9, batch    19 | loss: 4.1239400Losses:  3.9217019081115723 0.07928203791379929
CurrentTrain: epoch  9, batch    20 | loss: 4.0009837Losses:  3.9928202629089355 0.08068487048149109
CurrentTrain: epoch  9, batch    21 | loss: 4.0735049Losses:  3.9649741649627686 0.0631190687417984
CurrentTrain: epoch  9, batch    22 | loss: 4.0280933Losses:  3.9984827041625977 0.09407050907611847
CurrentTrain: epoch  9, batch    23 | loss: 4.0925531Losses:  3.9710311889648438 0.08172812312841415
CurrentTrain: epoch  9, batch    24 | loss: 4.0527592Losses:  3.963979959487915 0.09945252537727356
CurrentTrain: epoch  9, batch    25 | loss: 4.0634327Losses:  3.961453914642334 0.04947516322135925
CurrentTrain: epoch  9, batch    26 | loss: 4.0109291Losses:  4.006031036376953 0.06198381632566452
CurrentTrain: epoch  9, batch    27 | loss: 4.0680146Losses:  3.9258594512939453 0.11651694774627686
CurrentTrain: epoch  9, batch    28 | loss: 4.0423765Losses:  3.997947931289673 0.10242891311645508
CurrentTrain: epoch  9, batch    29 | loss: 4.1003771Losses:  3.9868884086608887 0.07296255230903625
CurrentTrain: epoch  9, batch    30 | loss: 4.0598512Losses:  3.9797682762145996 0.09707223623991013
CurrentTrain: epoch  9, batch    31 | loss: 4.0768404Losses:  3.9679882526397705 0.14679042994976044
CurrentTrain: epoch  9, batch    32 | loss: 4.1147785Losses:  3.970937728881836 0.14430716633796692
CurrentTrain: epoch  9, batch    33 | loss: 4.1152449Losses:  3.9877805709838867 0.08489450812339783
CurrentTrain: epoch  9, batch    34 | loss: 4.0726752Losses:  3.9657905101776123 0.04956838861107826
CurrentTrain: epoch  9, batch    35 | loss: 4.0153589Losses:  3.9579343795776367 0.13370853662490845
CurrentTrain: epoch  9, batch    36 | loss: 4.0916429Losses:  3.971890687942505 0.0537400022149086
CurrentTrain: epoch  9, batch    37 | loss: 4.0256305Losses:  4.031616687774658 0.12945647537708282
CurrentTrain: epoch  9, batch    38 | loss: 4.1610732Losses:  4.010646343231201 0.06500415503978729
CurrentTrain: epoch  9, batch    39 | loss: 4.0756507Losses:  3.9602603912353516 0.11661386489868164
CurrentTrain: epoch  9, batch    40 | loss: 4.0768743Losses:  4.014200210571289 0.11742427200078964
CurrentTrain: epoch  9, batch    41 | loss: 4.1316247Losses:  3.9628682136535645 0.05761834233999252
CurrentTrain: epoch  9, batch    42 | loss: 4.0204864Losses:  3.998117446899414 0.06061836704611778
CurrentTrain: epoch  9, batch    43 | loss: 4.0587358Losses:  3.9598517417907715 0.10841703414916992
CurrentTrain: epoch  9, batch    44 | loss: 4.0682688Losses:  3.9375391006469727 0.09321677684783936
CurrentTrain: epoch  9, batch    45 | loss: 4.0307560Losses:  4.047225475311279 0.10761788487434387
CurrentTrain: epoch  9, batch    46 | loss: 4.1548433Losses:  4.0189080238342285 0.1022735983133316
CurrentTrain: epoch  9, batch    47 | loss: 4.1211815Losses:  3.9677228927612305 0.07741306722164154
CurrentTrain: epoch  9, batch    48 | loss: 4.0451360Losses:  4.01371431350708 0.08703167736530304
CurrentTrain: epoch  9, batch    49 | loss: 4.1007462Losses:  4.032833099365234 0.08298544585704803
CurrentTrain: epoch  9, batch    50 | loss: 4.1158185Losses:  3.950143814086914 0.06446269899606705
CurrentTrain: epoch  9, batch    51 | loss: 4.0146065Losses:  3.994990348815918 0.11072943359613419
CurrentTrain: epoch  9, batch    52 | loss: 4.1057196Losses:  3.9577932357788086 0.07562212646007538
CurrentTrain: epoch  9, batch    53 | loss: 4.0334153Losses:  4.055479049682617 0.09651884436607361
CurrentTrain: epoch  9, batch    54 | loss: 4.1519980Losses:  3.940025806427002 0.08104177564382553
CurrentTrain: epoch  9, batch    55 | loss: 4.0210676Losses:  3.9511117935180664 0.12599147856235504
CurrentTrain: epoch  9, batch    56 | loss: 4.0771031Losses:  4.032017707824707 0.05980074405670166
CurrentTrain: epoch  9, batch    57 | loss: 4.0918183Losses:  3.980940580368042 0.08965872973203659
CurrentTrain: epoch  9, batch    58 | loss: 4.0705991Losses:  4.000205993652344 0.06799279153347015
CurrentTrain: epoch  9, batch    59 | loss: 4.0681987Losses:  3.97357177734375 0.07267935574054718
CurrentTrain: epoch  9, batch    60 | loss: 4.0462513Losses:  4.019658088684082 0.09904419630765915
CurrentTrain: epoch  9, batch    61 | loss: 4.1187024Losses:  3.9940247535705566 0.07376322150230408
CurrentTrain: epoch  9, batch    62 | loss: 4.0677881Losses:  3.9746999740600586 0.07504721730947495
CurrentTrain: epoch  9, batch    63 | loss: 4.0497470Losses:  3.9470396041870117 0.060099050402641296
CurrentTrain: epoch  9, batch    64 | loss: 4.0071387Losses:  3.9658987522125244 0.08341211825609207
CurrentTrain: epoch  9, batch    65 | loss: 4.0493107Losses:  3.949565887451172 0.06763505935668945
CurrentTrain: epoch  9, batch    66 | loss: 4.0172009Losses:  4.00738000869751 0.08811497688293457
CurrentTrain: epoch  9, batch    67 | loss: 4.0954952Losses:  3.979621410369873 0.0668100118637085
CurrentTrain: epoch  9, batch    68 | loss: 4.0464315Losses:  3.9522604942321777 0.07821597158908844
CurrentTrain: epoch  9, batch    69 | loss: 4.0304766Losses:  3.90679931640625 0.04078500717878342
CurrentTrain: epoch  9, batch    70 | loss: 3.9475844Losses:  3.9563896656036377 0.12662675976753235
CurrentTrain: epoch  9, batch    71 | loss: 4.0830164Losses:  4.000243186950684 0.09588880091905594
CurrentTrain: epoch  9, batch    72 | loss: 4.0961318Losses:  4.048644065856934 0.04158271476626396
CurrentTrain: epoch  9, batch    73 | loss: 4.0902267Losses:  3.963984966278076 0.09169961512088776
CurrentTrain: epoch  9, batch    74 | loss: 4.0556846Losses:  3.9753611087799072 0.054929330945014954
CurrentTrain: epoch  9, batch    75 | loss: 4.0302906Losses:  3.959315776824951 0.09840583801269531
CurrentTrain: epoch  9, batch    76 | loss: 4.0577216Losses:  3.9090375900268555 0.043512649834156036
CurrentTrain: epoch  9, batch    77 | loss: 3.9525502Losses:  3.974290370941162 0.11014942079782486
CurrentTrain: epoch  9, batch    78 | loss: 4.0844398Losses:  3.9877476692199707 0.08510453999042511
CurrentTrain: epoch  9, batch    79 | loss: 4.0728521Losses:  3.9360060691833496 0.13104690611362457
CurrentTrain: epoch  9, batch    80 | loss: 4.0670528Losses:  4.014057636260986 0.08172717690467834
CurrentTrain: epoch  9, batch    81 | loss: 4.0957847Losses:  3.979200839996338 0.07624772936105728
CurrentTrain: epoch  9, batch    82 | loss: 4.0554485Losses:  4.016304969787598 0.06001877039670944
CurrentTrain: epoch  9, batch    83 | loss: 4.0763235Losses:  3.9282636642456055 0.022817041724920273
CurrentTrain: epoch  9, batch    84 | loss: 3.9510808Losses:  4.01500940322876 0.03809991478919983
CurrentTrain: epoch  9, batch    85 | loss: 4.0531092Losses:  3.99153208732605 0.08446501940488815
CurrentTrain: epoch  9, batch    86 | loss: 4.0759969Losses:  3.965395450592041 0.09269582480192184
CurrentTrain: epoch  9, batch    87 | loss: 4.0580912Losses:  4.005581378936768 0.037206679582595825
CurrentTrain: epoch  9, batch    88 | loss: 4.0427880Losses:  3.9863715171813965 0.11832013726234436
CurrentTrain: epoch  9, batch    89 | loss: 4.1046915Losses:  3.997298240661621 0.07585541903972626
CurrentTrain: epoch  9, batch    90 | loss: 4.0731535Losses:  3.9607677459716797 0.08715012669563293
CurrentTrain: epoch  9, batch    91 | loss: 4.0479178Losses:  3.970684766769409 0.1001429408788681
CurrentTrain: epoch  9, batch    92 | loss: 4.0708275Losses:  4.000093936920166 0.08055727183818817
CurrentTrain: epoch  9, batch    93 | loss: 4.0806513Losses:  3.9626197814941406 0.0545022189617157
CurrentTrain: epoch  9, batch    94 | loss: 4.0171218Losses:  4.035519599914551 0.050752222537994385
CurrentTrain: epoch  9, batch    95 | loss: 4.0862718Losses:  3.9554200172424316 0.04735972359776497
CurrentTrain: epoch  9, batch    96 | loss: 4.0027800Losses:  4.010904312133789 0.09111794829368591
CurrentTrain: epoch  9, batch    97 | loss: 4.1020222Losses:  4.001292705535889 0.07331354916095734
CurrentTrain: epoch  9, batch    98 | loss: 4.0746064Losses:  3.992203712463379 0.05434785410761833
CurrentTrain: epoch  9, batch    99 | loss: 4.0465517Losses:  3.958019256591797 0.056961413472890854
CurrentTrain: epoch  9, batch   100 | loss: 4.0149808Losses:  4.083581447601318 0.13610978424549103
CurrentTrain: epoch  9, batch   101 | loss: 4.2196913Losses:  4.058320045471191 0.04312809184193611
CurrentTrain: epoch  9, batch   102 | loss: 4.1014481Losses:  3.969374656677246 0.06208384782075882
CurrentTrain: epoch  9, batch   103 | loss: 4.0314584Losses:  3.9817936420440674 0.04737140238285065
CurrentTrain: epoch  9, batch   104 | loss: 4.0291653Losses:  4.014580249786377 0.11717034876346588
CurrentTrain: epoch  9, batch   105 | loss: 4.1317506Losses:  3.9468166828155518 0.058010347187519073
CurrentTrain: epoch  9, batch   106 | loss: 4.0048270Losses:  4.061039924621582 0.040591202676296234
CurrentTrain: epoch  9, batch   107 | loss: 4.1016312Losses:  3.9704856872558594 0.039323076605796814
CurrentTrain: epoch  9, batch   108 | loss: 4.0098085Losses:  3.950113534927368 0.05976913869380951
CurrentTrain: epoch  9, batch   109 | loss: 4.0098825Losses:  3.8781087398529053 0.06412726640701294
CurrentTrain: epoch  9, batch   110 | loss: 3.9422359Losses:  3.9546947479248047 0.07698765397071838
CurrentTrain: epoch  9, batch   111 | loss: 4.0316825Losses:  4.00440788269043 0.10731383413076401
CurrentTrain: epoch  9, batch   112 | loss: 4.1117215Losses:  3.908787965774536 0.055570729076862335
CurrentTrain: epoch  9, batch   113 | loss: 3.9643588Losses:  3.9496583938598633 0.0756787583231926
CurrentTrain: epoch  9, batch   114 | loss: 4.0253372Losses:  3.9656662940979004 0.04974734038114548
CurrentTrain: epoch  9, batch   115 | loss: 4.0154138Losses:  3.972219944000244 0.10862904787063599
CurrentTrain: epoch  9, batch   116 | loss: 4.0808492Losses:  4.00011682510376 0.05709611997008324
CurrentTrain: epoch  9, batch   117 | loss: 4.0572128Losses:  4.021139144897461 0.0888851135969162
CurrentTrain: epoch  9, batch   118 | loss: 4.1100245Losses:  3.9563775062561035 0.0539851076900959
CurrentTrain: epoch  9, batch   119 | loss: 4.0103626Losses:  3.9727561473846436 0.039516910910606384
CurrentTrain: epoch  9, batch   120 | loss: 4.0122728Losses:  4.001297950744629 0.10322321951389313
CurrentTrain: epoch  9, batch   121 | loss: 4.1045213Losses:  3.9861903190612793 0.059840284287929535
CurrentTrain: epoch  9, batch   122 | loss: 4.0460305Losses:  3.962589740753174 0.11154349148273468
CurrentTrain: epoch  9, batch   123 | loss: 4.0741334Losses:  4.037261486053467 0.0628206729888916
CurrentTrain: epoch  9, batch   124 | loss: 4.1000824
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
Clustering into  9  clusters
Clusters:  [1 5 4 0 1 1 7 1 6 8 2 0 1 0 0 3 1 1 1 1]
Losses:  8.680777549743652 1.643450140953064
CurrentTrain: epoch  0, batch     0 | loss: 10.3242273Losses:  6.99257755279541 1.4042747020721436
CurrentTrain: epoch  0, batch     1 | loss: 8.3968525Losses:  7.971914768218994 1.6526668071746826
CurrentTrain: epoch  0, batch     2 | loss: 9.6245813Losses:  8.684600830078125 1.4999297857284546
CurrentTrain: epoch  0, batch     3 | loss: 10.1845303Losses:  8.335014343261719 2.0255560874938965
CurrentTrain: epoch  0, batch     4 | loss: 10.3605709Losses:  7.210102081298828 1.9198365211486816
CurrentTrain: epoch  0, batch     5 | loss: 9.1299381Losses:  6.289758682250977 0.39425164461135864
CurrentTrain: epoch  0, batch     6 | loss: 6.6840105Losses:  6.642950057983398 1.5300681591033936
CurrentTrain: epoch  1, batch     0 | loss: 8.1730185Losses:  7.6886444091796875 1.576257348060608
CurrentTrain: epoch  1, batch     1 | loss: 9.2649021Losses:  6.599668025970459 1.386399745941162
CurrentTrain: epoch  1, batch     2 | loss: 7.9860678Losses:  6.126001834869385 1.404197096824646
CurrentTrain: epoch  1, batch     3 | loss: 7.5301991Losses:  5.919936180114746 1.7371790409088135
CurrentTrain: epoch  1, batch     4 | loss: 7.6571150Losses:  6.265625953674316 1.8724015951156616
CurrentTrain: epoch  1, batch     5 | loss: 8.1380272Losses:  5.367226600646973 0.6800656318664551
CurrentTrain: epoch  1, batch     6 | loss: 6.0472922Losses:  6.457322120666504 0.982858419418335
CurrentTrain: epoch  2, batch     0 | loss: 7.4401808Losses:  5.6708784103393555 1.4362419843673706
CurrentTrain: epoch  2, batch     1 | loss: 7.1071205Losses:  4.3795166015625 1.2820602655410767
CurrentTrain: epoch  2, batch     2 | loss: 5.6615767Losses:  5.0719780921936035 1.4684414863586426
CurrentTrain: epoch  2, batch     3 | loss: 6.5404196Losses:  4.233793258666992 1.3422328233718872
CurrentTrain: epoch  2, batch     4 | loss: 5.5760260Losses:  5.105813026428223 1.423983097076416
CurrentTrain: epoch  2, batch     5 | loss: 6.5297961Losses:  7.509912014007568 0.8679022192955017
CurrentTrain: epoch  2, batch     6 | loss: 8.3778143Losses:  5.503759384155273 1.6377391815185547
CurrentTrain: epoch  3, batch     0 | loss: 7.1414986Losses:  4.171663761138916 1.2938939332962036
CurrentTrain: epoch  3, batch     1 | loss: 5.4655576Losses:  4.790611267089844 1.6469407081604004
CurrentTrain: epoch  3, batch     2 | loss: 6.4375520Losses:  5.068119525909424 1.233625888824463
CurrentTrain: epoch  3, batch     3 | loss: 6.3017454Losses:  4.3747100830078125 1.4486675262451172
CurrentTrain: epoch  3, batch     4 | loss: 5.8233776Losses:  3.6017043590545654 0.8585553765296936
CurrentTrain: epoch  3, batch     5 | loss: 4.4602599Losses:  5.007711410522461 0.4583088755607605
CurrentTrain: epoch  3, batch     6 | loss: 5.4660201Losses:  4.691768646240234 1.0171558856964111
CurrentTrain: epoch  4, batch     0 | loss: 5.7089243Losses:  3.7307772636413574 1.4166169166564941
CurrentTrain: epoch  4, batch     1 | loss: 5.1473942Losses:  4.006355285644531 0.686995267868042
CurrentTrain: epoch  4, batch     2 | loss: 4.6933508Losses:  5.091385841369629 0.8842617273330688
CurrentTrain: epoch  4, batch     3 | loss: 5.9756474Losses:  4.1742706298828125 1.2775863409042358
CurrentTrain: epoch  4, batch     4 | loss: 5.4518571Losses:  3.3170742988586426 1.1872029304504395
CurrentTrain: epoch  4, batch     5 | loss: 4.5042772Losses:  5.563165187835693 0.47517645359039307
CurrentTrain: epoch  4, batch     6 | loss: 6.0383415Losses:  4.151650905609131 0.986911416053772
CurrentTrain: epoch  5, batch     0 | loss: 5.1385622Losses:  5.163369178771973 1.062962293624878
CurrentTrain: epoch  5, batch     1 | loss: 6.2263317Losses:  3.725594997406006 1.3773481845855713
CurrentTrain: epoch  5, batch     2 | loss: 5.1029434Losses:  4.234714984893799 1.2997478246688843
CurrentTrain: epoch  5, batch     3 | loss: 5.5344629Losses:  3.727872371673584 1.5124974250793457
CurrentTrain: epoch  5, batch     4 | loss: 5.2403698Losses:  2.878563404083252 1.1082438230514526
CurrentTrain: epoch  5, batch     5 | loss: 3.9868073Losses:  2.366565465927124 0.3418283462524414
CurrentTrain: epoch  5, batch     6 | loss: 2.7083938Losses:  3.7826762199401855 1.1938056945800781
CurrentTrain: epoch  6, batch     0 | loss: 4.9764819Losses:  3.1944565773010254 1.1295721530914307
CurrentTrain: epoch  6, batch     1 | loss: 4.3240290Losses:  4.0040178298950195 1.0922930240631104
CurrentTrain: epoch  6, batch     2 | loss: 5.0963106Losses:  2.9580821990966797 1.1113040447235107
CurrentTrain: epoch  6, batch     3 | loss: 4.0693865Losses:  3.1738109588623047 1.1335022449493408
CurrentTrain: epoch  6, batch     4 | loss: 4.3073130Losses:  3.5524938106536865 1.3679180145263672
CurrentTrain: epoch  6, batch     5 | loss: 4.9204121Losses:  3.958463430404663 0.3772667646408081
CurrentTrain: epoch  6, batch     6 | loss: 4.3357301Losses:  3.100027561187744 0.7744100093841553
CurrentTrain: epoch  7, batch     0 | loss: 3.8744376Losses:  3.1336255073547363 1.1026501655578613
CurrentTrain: epoch  7, batch     1 | loss: 4.2362757Losses:  2.2363386154174805 1.121418833732605
CurrentTrain: epoch  7, batch     2 | loss: 3.3577576Losses:  3.104059934616089 1.1900811195373535
CurrentTrain: epoch  7, batch     3 | loss: 4.2941408Losses:  2.5566043853759766 1.220510721206665
CurrentTrain: epoch  7, batch     4 | loss: 3.7771151Losses:  3.518146514892578 1.1625263690948486
CurrentTrain: epoch  7, batch     5 | loss: 4.6806726Losses:  8.01205062866211 8.94069742685133e-08
CurrentTrain: epoch  7, batch     6 | loss: 8.0120506Losses:  3.2584028244018555 1.2310835123062134
CurrentTrain: epoch  8, batch     0 | loss: 4.4894862Losses:  2.8820600509643555 0.9862039089202881
CurrentTrain: epoch  8, batch     1 | loss: 3.8682640Losses:  2.8199639320373535 1.1154705286026
CurrentTrain: epoch  8, batch     2 | loss: 3.9354343Losses:  3.49818754196167 0.9367750287055969
CurrentTrain: epoch  8, batch     3 | loss: 4.4349627Losses:  3.1765661239624023 1.4066736698150635
CurrentTrain: epoch  8, batch     4 | loss: 4.5832396Losses:  2.071707010269165 1.0227633714675903
CurrentTrain: epoch  8, batch     5 | loss: 3.0944705Losses:  3.802704334259033 0.2975965142250061
CurrentTrain: epoch  8, batch     6 | loss: 4.1003008Losses:  2.3026249408721924 0.9107838869094849
CurrentTrain: epoch  9, batch     0 | loss: 3.2134089Losses:  3.0537962913513184 0.87860107421875
CurrentTrain: epoch  9, batch     1 | loss: 3.9323974Losses:  4.238824844360352 0.9379584789276123
CurrentTrain: epoch  9, batch     2 | loss: 5.1767836Losses:  2.723174810409546 0.7366426587104797
CurrentTrain: epoch  9, batch     3 | loss: 3.4598174Losses:  2.027787685394287 1.0467292070388794
CurrentTrain: epoch  9, batch     4 | loss: 3.0745168Losses:  2.9455511569976807 0.7568044662475586
CurrentTrain: epoch  9, batch     5 | loss: 3.7023556Losses:  2.5837197303771973 0.5404706597328186
CurrentTrain: epoch  9, batch     6 | loss: 3.1241903
Losses:  0.6646947860717773 0.5511928796768188
MemoryTrain:  epoch  0, batch     0 | loss: 1.2158877Losses:  0.8209114074707031 0.5836923122406006
MemoryTrain:  epoch  0, batch     1 | loss: 1.4046037Losses:  0.6582472920417786 0.5468063950538635
MemoryTrain:  epoch  0, batch     2 | loss: 1.2050537Losses:  1.9227983951568604 1.013451337814331
MemoryTrain:  epoch  1, batch     0 | loss: 2.9362497Losses:  0.4140995740890503 0.6802979707717896
MemoryTrain:  epoch  1, batch     1 | loss: 1.0943975Losses:  0.08058235794305801 0.1543881595134735
MemoryTrain:  epoch  1, batch     2 | loss: 0.2349705Losses:  0.39395010471343994 0.5931868553161621
MemoryTrain:  epoch  2, batch     0 | loss: 0.9871370Losses:  0.6479958295822144 0.42805007100105286
MemoryTrain:  epoch  2, batch     1 | loss: 1.0760459Losses:  0.2391476035118103 0.5755472183227539
MemoryTrain:  epoch  2, batch     2 | loss: 0.8146948Losses:  0.08357605338096619 0.6326064467430115
MemoryTrain:  epoch  3, batch     0 | loss: 0.7161825Losses:  0.15583795309066772 0.7027527093887329
MemoryTrain:  epoch  3, batch     1 | loss: 0.8585907Losses:  0.14239419996738434 0.21021319925785065
MemoryTrain:  epoch  3, batch     2 | loss: 0.3526074Losses:  0.14290358126163483 0.5915318727493286
MemoryTrain:  epoch  4, batch     0 | loss: 0.7344354Losses:  0.041216932237148285 0.6780992150306702
MemoryTrain:  epoch  4, batch     1 | loss: 0.7193161Losses:  0.065180204808712 0.20580914616584778
MemoryTrain:  epoch  4, batch     2 | loss: 0.2709894Losses:  0.08028353005647659 0.7402435541152954
MemoryTrain:  epoch  5, batch     0 | loss: 0.8205271Losses:  0.054967001080513 0.4316398799419403
MemoryTrain:  epoch  5, batch     1 | loss: 0.4866069Losses:  0.04207282513380051 0.39284056425094604
MemoryTrain:  epoch  5, batch     2 | loss: 0.4349134Losses:  0.028995495289564133 0.5903919339179993
MemoryTrain:  epoch  6, batch     0 | loss: 0.6193874Losses:  0.06539621949195862 0.49230608344078064
MemoryTrain:  epoch  6, batch     1 | loss: 0.5577023Losses:  0.04122763127088547 0.2876867949962616
MemoryTrain:  epoch  6, batch     2 | loss: 0.3289144Losses:  0.01733817160129547 0.6344594955444336
MemoryTrain:  epoch  7, batch     0 | loss: 0.6517977Losses:  0.03138500079512596 0.36178478598594666
MemoryTrain:  epoch  7, batch     1 | loss: 0.3931698Losses:  0.17648930847644806 0.557318389415741
MemoryTrain:  epoch  7, batch     2 | loss: 0.7338077Losses:  0.030407298356294632 0.3622930645942688
MemoryTrain:  epoch  8, batch     0 | loss: 0.3927004Losses:  0.01899528317153454 0.5827798843383789
MemoryTrain:  epoch  8, batch     1 | loss: 0.6017752Losses:  0.03093292936682701 0.3212997019290924
MemoryTrain:  epoch  8, batch     2 | loss: 0.3522326Losses:  0.04486161470413208 0.537797212600708
MemoryTrain:  epoch  9, batch     0 | loss: 0.5826588Losses:  0.007543756626546383 0.2830450236797333
MemoryTrain:  epoch  9, batch     1 | loss: 0.2905888Losses:  0.009288186207413673 0.5105651617050171
MemoryTrain:  epoch  9, batch     2 | loss: 0.5198534
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 54.37%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.62%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 52.93%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 53.98%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 54.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 55.89%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 56.60%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 57.26%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 58.06%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 58.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 59.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 60.67%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 61.92%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 62.08%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 61.82%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 61.70%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 61.59%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 61.35%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 61.52%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 62.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 63.93%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 63.49%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.40%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 94.36%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 93.85%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 92.68%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 91.44%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 89.55%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 88.42%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 87.86%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.26%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.45%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 88.03%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 87.65%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 86.91%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 86.56%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 85.92%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 85.37%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.41%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.47%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 82.55%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 81.79%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 80.98%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 80.25%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 80.69%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 81.02%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 80.23%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 80.07%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 79.91%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.70%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 79.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 80.20%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 79.70%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 79.37%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 79.20%   
cur_acc:  ['0.9474', '0.6349']
his_acc:  ['0.9474', '0.7920']
Clustering into  14  clusters
Clusters:  [ 2 11  7  0  2  2 12  2 10  5  9  0  2  0  0  4  2  2  2  2  2  2  2  8
  2  3  2  1 13  6]
Losses:  5.44977331161499 0.9853174090385437
CurrentTrain: epoch  0, batch     0 | loss: 6.4350905Losses:  6.3167314529418945 1.3050391674041748
CurrentTrain: epoch  0, batch     1 | loss: 7.6217709Losses:  6.533836364746094 0.7162569761276245
CurrentTrain: epoch  0, batch     2 | loss: 7.2500935Losses:  5.748399257659912 0.777915358543396
CurrentTrain: epoch  0, batch     3 | loss: 6.5263147Losses:  5.533242225646973 1.1537861824035645
CurrentTrain: epoch  0, batch     4 | loss: 6.6870284Losses:  5.179140090942383 1.2286815643310547
CurrentTrain: epoch  0, batch     5 | loss: 6.4078217Losses:  4.72818660736084 0.335677832365036
CurrentTrain: epoch  0, batch     6 | loss: 5.0638642Losses:  4.575357437133789 1.1737825870513916
CurrentTrain: epoch  1, batch     0 | loss: 5.7491398Losses:  4.915282249450684 1.054337978363037
CurrentTrain: epoch  1, batch     1 | loss: 5.9696202Losses:  4.5744781494140625 0.679957389831543
CurrentTrain: epoch  1, batch     2 | loss: 5.2544355Losses:  4.793083667755127 0.8358120918273926
CurrentTrain: epoch  1, batch     3 | loss: 5.6288958Losses:  5.062704086303711 1.0009934902191162
CurrentTrain: epoch  1, batch     4 | loss: 6.0636978Losses:  3.028921127319336 0.7366266250610352
CurrentTrain: epoch  1, batch     5 | loss: 3.7655478Losses:  4.002808570861816 0.23999115824699402
CurrentTrain: epoch  1, batch     6 | loss: 4.2427998Losses:  3.9433717727661133 0.7217754125595093
CurrentTrain: epoch  2, batch     0 | loss: 4.6651473Losses:  3.451150417327881 0.8979973196983337
CurrentTrain: epoch  2, batch     1 | loss: 4.3491478Losses:  2.939591407775879 1.0177578926086426
CurrentTrain: epoch  2, batch     2 | loss: 3.9573493Losses:  3.1797382831573486 0.5674110651016235
CurrentTrain: epoch  2, batch     3 | loss: 3.7471495Losses:  3.618828773498535 0.8630603551864624
CurrentTrain: epoch  2, batch     4 | loss: 4.4818892Losses:  4.797195911407471 0.8983916640281677
CurrentTrain: epoch  2, batch     5 | loss: 5.6955876Losses:  4.163050651550293 0.2683790922164917
CurrentTrain: epoch  2, batch     6 | loss: 4.4314299Losses:  3.7619690895080566 0.6617343425750732
CurrentTrain: epoch  3, batch     0 | loss: 4.4237032Losses:  3.5088067054748535 0.5034265518188477
CurrentTrain: epoch  3, batch     1 | loss: 4.0122333Losses:  3.0800368785858154 0.8726735711097717
CurrentTrain: epoch  3, batch     2 | loss: 3.9527104Losses:  2.7930116653442383 0.5093219876289368
CurrentTrain: epoch  3, batch     3 | loss: 3.3023336Losses:  3.217221260070801 0.5797291994094849
CurrentTrain: epoch  3, batch     4 | loss: 3.7969503Losses:  2.460003614425659 0.6243472695350647
CurrentTrain: epoch  3, batch     5 | loss: 3.0843508Losses:  4.41121768951416 0.03232220560312271
CurrentTrain: epoch  3, batch     6 | loss: 4.4435401Losses:  2.6459097862243652 0.5501858592033386
CurrentTrain: epoch  4, batch     0 | loss: 3.1960957Losses:  2.3828296661376953 0.5975618958473206
CurrentTrain: epoch  4, batch     1 | loss: 2.9803915Losses:  3.361945629119873 0.58613121509552
CurrentTrain: epoch  4, batch     2 | loss: 3.9480767Losses:  2.2480485439300537 0.4417116641998291
CurrentTrain: epoch  4, batch     3 | loss: 2.6897602Losses:  2.9820914268493652 0.8573161363601685
CurrentTrain: epoch  4, batch     4 | loss: 3.8394074Losses:  2.883175849914551 0.6364163160324097
CurrentTrain: epoch  4, batch     5 | loss: 3.5195923Losses:  2.270850658416748 0.39719298481941223
CurrentTrain: epoch  4, batch     6 | loss: 2.6680436Losses:  2.274285078048706 0.6900376081466675
CurrentTrain: epoch  5, batch     0 | loss: 2.9643226Losses:  2.6205945014953613 0.46067968010902405
CurrentTrain: epoch  5, batch     1 | loss: 3.0812743Losses:  2.626277446746826 0.5370951890945435
CurrentTrain: epoch  5, batch     2 | loss: 3.1633725Losses:  2.1686999797821045 0.40509068965911865
CurrentTrain: epoch  5, batch     3 | loss: 2.5737906Losses:  2.268789529800415 0.5786012411117554
CurrentTrain: epoch  5, batch     4 | loss: 2.8473907Losses:  2.5704917907714844 0.6407501697540283
CurrentTrain: epoch  5, batch     5 | loss: 3.2112420Losses:  1.9053646326065063 0.06683261692523956
CurrentTrain: epoch  5, batch     6 | loss: 1.9721973Losses:  2.081678867340088 0.5635212063789368
CurrentTrain: epoch  6, batch     0 | loss: 2.6452000Losses:  2.8162546157836914 0.6273148059844971
CurrentTrain: epoch  6, batch     1 | loss: 3.4435694Losses:  2.164660692214966 0.3968793749809265
CurrentTrain: epoch  6, batch     2 | loss: 2.5615401Losses:  2.0357611179351807 0.3925076127052307
CurrentTrain: epoch  6, batch     3 | loss: 2.4282687Losses:  1.8803529739379883 0.2808351516723633
CurrentTrain: epoch  6, batch     4 | loss: 2.1611881Losses:  2.0306527614593506 0.5910778045654297
CurrentTrain: epoch  6, batch     5 | loss: 2.6217306Losses:  2.2620909214019775 0.029912885278463364
CurrentTrain: epoch  6, batch     6 | loss: 2.2920039Losses:  2.023890972137451 0.4857839345932007
CurrentTrain: epoch  7, batch     0 | loss: 2.5096750Losses:  2.019334316253662 0.5129333138465881
CurrentTrain: epoch  7, batch     1 | loss: 2.5322676Losses:  1.9766461849212646 0.6354800462722778
CurrentTrain: epoch  7, batch     2 | loss: 2.6121264Losses:  1.8956141471862793 0.3942829668521881
CurrentTrain: epoch  7, batch     3 | loss: 2.2898972Losses:  2.0427534580230713 0.5579161643981934
CurrentTrain: epoch  7, batch     4 | loss: 2.6006696Losses:  2.2666807174682617 0.3790375590324402
CurrentTrain: epoch  7, batch     5 | loss: 2.6457183Losses:  2.247981548309326 0.04967920109629631
CurrentTrain: epoch  7, batch     6 | loss: 2.2976608Losses:  1.9237034320831299 0.4229341745376587
CurrentTrain: epoch  8, batch     0 | loss: 2.3466377Losses:  1.826008677482605 0.3867408335208893
CurrentTrain: epoch  8, batch     1 | loss: 2.2127495Losses:  1.9191008806228638 0.6075925827026367
CurrentTrain: epoch  8, batch     2 | loss: 2.5266933Losses:  1.893052339553833 0.5842615365982056
CurrentTrain: epoch  8, batch     3 | loss: 2.4773140Losses:  1.9022271633148193 0.5122659206390381
CurrentTrain: epoch  8, batch     4 | loss: 2.4144931Losses:  1.7966837882995605 0.5337061882019043
CurrentTrain: epoch  8, batch     5 | loss: 2.3303900Losses:  2.4245572090148926 0.0
CurrentTrain: epoch  8, batch     6 | loss: 2.4245572Losses:  1.8179582357406616 0.4756695032119751
CurrentTrain: epoch  9, batch     0 | loss: 2.2936277Losses:  1.9667061567306519 0.3712933361530304
CurrentTrain: epoch  9, batch     1 | loss: 2.3379996Losses:  1.8654569387435913 0.4051647186279297
CurrentTrain: epoch  9, batch     2 | loss: 2.2706218Losses:  1.8420066833496094 0.38039717078208923
CurrentTrain: epoch  9, batch     3 | loss: 2.2224038Losses:  1.7963104248046875 0.47418248653411865
CurrentTrain: epoch  9, batch     4 | loss: 2.2704930Losses:  1.902933120727539 0.36321738362312317
CurrentTrain: epoch  9, batch     5 | loss: 2.2661505Losses:  1.699463129043579 0.03403650224208832
CurrentTrain: epoch  9, batch     6 | loss: 1.7334996
Losses:  2.230238437652588 0.6202867031097412
MemoryTrain:  epoch  0, batch     0 | loss: 2.8505251Losses:  2.469066858291626 0.7764530181884766
MemoryTrain:  epoch  0, batch     1 | loss: 3.2455199Losses:  0.7525662779808044 0.49465811252593994
MemoryTrain:  epoch  0, batch     2 | loss: 1.2472243Losses:  1.66232430934906 0.3360542058944702
MemoryTrain:  epoch  0, batch     3 | loss: 1.9983785Losses:  0.8851568698883057 0.6182860136032104
MemoryTrain:  epoch  1, batch     0 | loss: 1.5034429Losses:  3.6684577465057373 0.5297579765319824
MemoryTrain:  epoch  1, batch     1 | loss: 4.1982155Losses:  0.5487171411514282 0.5758219957351685
MemoryTrain:  epoch  1, batch     2 | loss: 1.1245391Losses:  1.2897865772247314 0.43086379766464233
MemoryTrain:  epoch  1, batch     3 | loss: 1.7206504Losses:  0.7628535628318787 0.7620365023612976
MemoryTrain:  epoch  2, batch     0 | loss: 1.5248901Losses:  0.33571574091911316 0.560521125793457
MemoryTrain:  epoch  2, batch     1 | loss: 0.8962369Losses:  2.4592456817626953 0.550575315952301
MemoryTrain:  epoch  2, batch     2 | loss: 3.0098209Losses:  1.6759958267211914 0.2396298497915268
MemoryTrain:  epoch  2, batch     3 | loss: 1.9156257Losses:  0.9312151074409485 0.4898073077201843
MemoryTrain:  epoch  3, batch     0 | loss: 1.4210224Losses:  0.6214902997016907 0.6624178886413574
MemoryTrain:  epoch  3, batch     1 | loss: 1.2839081Losses:  1.2337543964385986 0.7108583450317383
MemoryTrain:  epoch  3, batch     2 | loss: 1.9446127Losses:  0.2988274097442627 0.14814028143882751
MemoryTrain:  epoch  3, batch     3 | loss: 0.4469677Losses:  1.2224006652832031 0.34674620628356934
MemoryTrain:  epoch  4, batch     0 | loss: 1.5691469Losses:  0.9487482905387878 0.6380892992019653
MemoryTrain:  epoch  4, batch     1 | loss: 1.5868375Losses:  0.2021295726299286 0.8748956918716431
MemoryTrain:  epoch  4, batch     2 | loss: 1.0770253Losses:  0.087013378739357 0.18004189431667328
MemoryTrain:  epoch  4, batch     3 | loss: 0.2670553Losses:  0.08457273989915848 0.6779909729957581
MemoryTrain:  epoch  5, batch     0 | loss: 0.7625637Losses:  0.07912422716617584 0.6346731185913086
MemoryTrain:  epoch  5, batch     1 | loss: 0.7137973Losses:  1.0772309303283691 0.6521748304367065
MemoryTrain:  epoch  5, batch     2 | loss: 1.7294058Losses:  0.9998077750205994 0.30175596475601196
MemoryTrain:  epoch  5, batch     3 | loss: 1.3015637Losses:  0.6508665084838867 0.5016272068023682
MemoryTrain:  epoch  6, batch     0 | loss: 1.1524937Losses:  0.6078165769577026 0.49355548620224
MemoryTrain:  epoch  6, batch     1 | loss: 1.1013720Losses:  0.06276451051235199 0.5595412254333496
MemoryTrain:  epoch  6, batch     2 | loss: 0.6223058Losses:  0.4353174567222595 0.3985083997249603
MemoryTrain:  epoch  6, batch     3 | loss: 0.8338258Losses:  0.5005671977996826 0.5247712731361389
MemoryTrain:  epoch  7, batch     0 | loss: 1.0253384Losses:  0.04708793759346008 0.8359270691871643
MemoryTrain:  epoch  7, batch     1 | loss: 0.8830150Losses:  0.06967729330062866 0.45565974712371826
MemoryTrain:  epoch  7, batch     2 | loss: 0.5253370Losses:  0.5893880724906921 0.37819668650627136
MemoryTrain:  epoch  7, batch     3 | loss: 0.9675847Losses:  0.04666902497410774 0.7725003361701965
MemoryTrain:  epoch  8, batch     0 | loss: 0.8191693Losses:  0.3412136733531952 0.5498664379119873
MemoryTrain:  epoch  8, batch     1 | loss: 0.8910801Losses:  0.2398889660835266 0.2990746796131134
MemoryTrain:  epoch  8, batch     2 | loss: 0.5389637Losses:  0.0375896655023098 0.298833966255188
MemoryTrain:  epoch  8, batch     3 | loss: 0.3364236Losses:  0.29030928015708923 0.6119017601013184
MemoryTrain:  epoch  9, batch     0 | loss: 0.9022111Losses:  0.03196225315332413 0.5624805688858032
MemoryTrain:  epoch  9, batch     1 | loss: 0.5944428Losses:  0.05369490385055542 0.5432558059692383
MemoryTrain:  epoch  9, batch     2 | loss: 0.5969507Losses:  0.16119998693466187 0.24058416485786438
MemoryTrain:  epoch  9, batch     3 | loss: 0.4017842
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.02%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 95.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 73.64%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 90.99%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 92.98%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 92.52%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.44%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.77%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 89.04%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 87.69%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 86.47%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 84.42%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 84.76%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.08%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 84.60%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 83.96%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 83.11%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 82.35%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 81.61%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 80.75%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 79.97%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 79.07%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 78.19%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.34%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 76.56%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 75.81%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 75.13%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.46%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 75.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 76.02%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 76.07%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 76.06%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 75.88%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.37%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 76.04%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.83%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 75.51%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 75.40%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.07%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 77.31%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 77.37%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 77.42%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.45%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 77.54%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 77.61%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 77.64%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 77.58%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 77.58%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 77.44%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 77.38%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 77.17%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 77.02%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 76.76%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 76.55%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 76.20%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 76.08%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 76.00%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 75.91%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 75.60%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 75.52%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 75.22%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 74.93%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 74.31%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 74.07%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 73.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 75.37%   
cur_acc:  ['0.9474', '0.6349', '0.7530']
his_acc:  ['0.9474', '0.7920', '0.7537']
Clustering into  19  clusters
Clusters:  [ 1 11 15  0  1  1 16  1 12 17 10  0  1  0  0  9  1  1  1  1  1  1  1  4
  1  7  1 18 13  5  8  1 14  1  6  2  3  1  1  1]
Losses:  5.636775016784668 1.3286714553833008
CurrentTrain: epoch  0, batch     0 | loss: 6.9654465Losses:  6.046133995056152 1.398158311843872
CurrentTrain: epoch  0, batch     1 | loss: 7.4442921Losses:  5.392569541931152 1.268444538116455
CurrentTrain: epoch  0, batch     2 | loss: 6.6610141Losses:  8.48873233795166 0.9750578999519348
CurrentTrain: epoch  0, batch     3 | loss: 9.4637899Losses:  5.881830215454102 1.1004881858825684
CurrentTrain: epoch  0, batch     4 | loss: 6.9823184Losses:  7.802305221557617 0.9058260321617126
CurrentTrain: epoch  0, batch     5 | loss: 8.7081308Losses:  6.149137496948242 0.36238861083984375
CurrentTrain: epoch  0, batch     6 | loss: 6.5115261Losses:  6.106148719787598 1.1730051040649414
CurrentTrain: epoch  1, batch     0 | loss: 7.2791538Losses:  6.902702331542969 1.4374910593032837
CurrentTrain: epoch  1, batch     1 | loss: 8.3401937Losses:  4.4266510009765625 1.1192314624786377
CurrentTrain: epoch  1, batch     2 | loss: 5.5458822Losses:  5.417988300323486 1.020944595336914
CurrentTrain: epoch  1, batch     3 | loss: 6.4389329Losses:  5.050969123840332 0.8961188197135925
CurrentTrain: epoch  1, batch     4 | loss: 5.9470878Losses:  5.0521345138549805 1.2999199628829956
CurrentTrain: epoch  1, batch     5 | loss: 6.3520546Losses:  8.213717460632324 0.5840566158294678
CurrentTrain: epoch  1, batch     6 | loss: 8.7977743Losses:  4.510882377624512 1.02956223487854
CurrentTrain: epoch  2, batch     0 | loss: 5.5404444Losses:  4.314043045043945 1.101037621498108
CurrentTrain: epoch  2, batch     1 | loss: 5.4150805Losses:  4.856934547424316 1.1628950834274292
CurrentTrain: epoch  2, batch     2 | loss: 6.0198298Losses:  5.845272064208984 1.1552066802978516
CurrentTrain: epoch  2, batch     3 | loss: 7.0004787Losses:  3.9914121627807617 1.1203726530075073
CurrentTrain: epoch  2, batch     4 | loss: 5.1117849Losses:  5.812892913818359 0.7711104154586792
CurrentTrain: epoch  2, batch     5 | loss: 6.5840034Losses:  2.388115406036377 8.94069742685133e-08
CurrentTrain: epoch  2, batch     6 | loss: 2.3881154Losses:  2.9705166816711426 0.7262254953384399
CurrentTrain: epoch  3, batch     0 | loss: 3.6967421Losses:  2.7959556579589844 0.6249085664749146
CurrentTrain: epoch  3, batch     1 | loss: 3.4208641Losses:  6.484851360321045 0.9328784942626953
CurrentTrain: epoch  3, batch     2 | loss: 7.4177299Losses:  4.138591766357422 1.123337745666504
CurrentTrain: epoch  3, batch     3 | loss: 5.2619295Losses:  5.504541873931885 0.9383261799812317
CurrentTrain: epoch  3, batch     4 | loss: 6.4428682Losses:  5.126490592956543 1.1812819242477417
CurrentTrain: epoch  3, batch     5 | loss: 6.3077726Losses:  2.1522345542907715 0.21198788285255432
CurrentTrain: epoch  3, batch     6 | loss: 2.3642225Losses:  4.325690269470215 1.0713820457458496
CurrentTrain: epoch  4, batch     0 | loss: 5.3970723Losses:  4.173541069030762 0.9819381237030029
CurrentTrain: epoch  4, batch     1 | loss: 5.1554794Losses:  5.040393829345703 0.9329971075057983
CurrentTrain: epoch  4, batch     2 | loss: 5.9733911Losses:  2.6788501739501953 0.622657299041748
CurrentTrain: epoch  4, batch     3 | loss: 3.3015075Losses:  4.6757049560546875 0.9783678650856018
CurrentTrain: epoch  4, batch     4 | loss: 5.6540728Losses:  4.12464714050293 0.8569343686103821
CurrentTrain: epoch  4, batch     5 | loss: 4.9815817Losses:  3.954712390899658 0.13188797235488892
CurrentTrain: epoch  4, batch     6 | loss: 4.0866003Losses:  5.833922863006592 1.0685784816741943
CurrentTrain: epoch  5, batch     0 | loss: 6.9025011Losses:  3.6317129135131836 1.0134788751602173
CurrentTrain: epoch  5, batch     1 | loss: 4.6451917Losses:  3.5895333290100098 0.8636887073516846
CurrentTrain: epoch  5, batch     2 | loss: 4.4532223Losses:  3.760234832763672 0.9176455736160278
CurrentTrain: epoch  5, batch     3 | loss: 4.6778803Losses:  3.522667407989502 0.7726186513900757
CurrentTrain: epoch  5, batch     4 | loss: 4.2952862Losses:  4.514562606811523 0.9626680612564087
CurrentTrain: epoch  5, batch     5 | loss: 5.4772305Losses:  1.7040908336639404 0.0
CurrentTrain: epoch  5, batch     6 | loss: 1.7040908Losses:  4.30637788772583 1.1305886507034302
CurrentTrain: epoch  6, batch     0 | loss: 5.4369664Losses:  3.41211199760437 0.8374893665313721
CurrentTrain: epoch  6, batch     1 | loss: 4.2496014Losses:  3.7116823196411133 0.8482455611228943
CurrentTrain: epoch  6, batch     2 | loss: 4.5599279Losses:  3.095951557159424 0.7098374366760254
CurrentTrain: epoch  6, batch     3 | loss: 3.8057890Losses:  3.5284390449523926 0.9403041005134583
CurrentTrain: epoch  6, batch     4 | loss: 4.4687433Losses:  3.265878915786743 0.5981872081756592
CurrentTrain: epoch  6, batch     5 | loss: 3.8640661Losses:  3.456653118133545 0.10929464548826218
CurrentTrain: epoch  6, batch     6 | loss: 3.5659478Losses:  3.2995383739471436 0.6842684745788574
CurrentTrain: epoch  7, batch     0 | loss: 3.9838068Losses:  3.834789752960205 0.9088454246520996
CurrentTrain: epoch  7, batch     1 | loss: 4.7436352Losses:  2.8157825469970703 0.7272634506225586
CurrentTrain: epoch  7, batch     2 | loss: 3.5430460Losses:  3.807645320892334 0.9322042465209961
CurrentTrain: epoch  7, batch     3 | loss: 4.7398496Losses:  3.095937490463257 0.8966028690338135
CurrentTrain: epoch  7, batch     4 | loss: 3.9925404Losses:  2.8827719688415527 0.7609477043151855
CurrentTrain: epoch  7, batch     5 | loss: 3.6437197Losses:  1.9378414154052734 0.15450292825698853
CurrentTrain: epoch  7, batch     6 | loss: 2.0923443Losses:  2.8408758640289307 0.8471955060958862
CurrentTrain: epoch  8, batch     0 | loss: 3.6880713Losses:  2.8484983444213867 0.7242623567581177
CurrentTrain: epoch  8, batch     1 | loss: 3.5727606Losses:  2.5612082481384277 0.8158190846443176
CurrentTrain: epoch  8, batch     2 | loss: 3.3770273Losses:  3.012847423553467 0.7276980876922607
CurrentTrain: epoch  8, batch     3 | loss: 3.7405455Losses:  3.3055026531219482 0.5565943717956543
CurrentTrain: epoch  8, batch     4 | loss: 3.8620970Losses:  3.6534852981567383 0.5955368280410767
CurrentTrain: epoch  8, batch     5 | loss: 4.2490220Losses:  2.0066401958465576 0.18340572714805603
CurrentTrain: epoch  8, batch     6 | loss: 2.1900458Losses:  2.6312079429626465 0.740484893321991
CurrentTrain: epoch  9, batch     0 | loss: 3.3716929Losses:  3.2894954681396484 0.6782282590866089
CurrentTrain: epoch  9, batch     1 | loss: 3.9677238Losses:  2.7369070053100586 0.7401608824729919
CurrentTrain: epoch  9, batch     2 | loss: 3.4770679Losses:  3.032409191131592 0.7734851241111755
CurrentTrain: epoch  9, batch     3 | loss: 3.8058944Losses:  2.550316095352173 0.8303534388542175
CurrentTrain: epoch  9, batch     4 | loss: 3.3806696Losses:  2.232347249984741 0.8032494187355042
CurrentTrain: epoch  9, batch     5 | loss: 3.0355966Losses:  3.426546573638916 0.2526283860206604
CurrentTrain: epoch  9, batch     6 | loss: 3.6791749
Losses:  0.7529759407043457 0.6183602809906006
MemoryTrain:  epoch  0, batch     0 | loss: 1.3713362Losses:  1.4030194282531738 0.7324710488319397
MemoryTrain:  epoch  0, batch     1 | loss: 2.1354904Losses:  0.669865071773529 0.49582237005233765
MemoryTrain:  epoch  0, batch     2 | loss: 1.1656874Losses:  0.6980447769165039 0.8157812356948853
MemoryTrain:  epoch  0, batch     3 | loss: 1.5138260Losses:  0.4685767590999603 0.5127934217453003
MemoryTrain:  epoch  0, batch     4 | loss: 0.9813702Losses:  1.0307704210281372 0.8625349998474121
MemoryTrain:  epoch  1, batch     0 | loss: 1.8933054Losses:  0.9948235750198364 0.4932842254638672
MemoryTrain:  epoch  1, batch     1 | loss: 1.4881078Losses:  0.700739860534668 0.43538200855255127
MemoryTrain:  epoch  1, batch     2 | loss: 1.1361219Losses:  0.7367092370986938 0.6566644906997681
MemoryTrain:  epoch  1, batch     3 | loss: 1.3933737Losses:  0.3946904242038727 0.5750417709350586
MemoryTrain:  epoch  1, batch     4 | loss: 0.9697322Losses:  0.5708084106445312 0.729920506477356
MemoryTrain:  epoch  2, batch     0 | loss: 1.3007289Losses:  0.24768158793449402 0.6898250579833984
MemoryTrain:  epoch  2, batch     1 | loss: 0.9375067Losses:  0.30929556488990784 0.7071934938430786
MemoryTrain:  epoch  2, batch     2 | loss: 1.0164890Losses:  0.30861368775367737 0.34382766485214233
MemoryTrain:  epoch  2, batch     3 | loss: 0.6524414Losses:  0.6725616455078125 0.4717317223548889
MemoryTrain:  epoch  2, batch     4 | loss: 1.1442933Losses:  0.21838343143463135 0.6887472867965698
MemoryTrain:  epoch  3, batch     0 | loss: 0.9071307Losses:  0.17031025886535645 0.6849300861358643
MemoryTrain:  epoch  3, batch     1 | loss: 0.8552403Losses:  0.3837817311286926 0.4034118056297302
MemoryTrain:  epoch  3, batch     2 | loss: 0.7871935Losses:  0.3868750333786011 0.5673391222953796
MemoryTrain:  epoch  3, batch     3 | loss: 0.9542142Losses:  0.2335556149482727 0.5805002450942993
MemoryTrain:  epoch  3, batch     4 | loss: 0.8140559Losses:  0.13297225534915924 0.8188347816467285
MemoryTrain:  epoch  4, batch     0 | loss: 0.9518070Losses:  0.0583685040473938 0.4113536477088928
MemoryTrain:  epoch  4, batch     1 | loss: 0.4697222Losses:  0.12822221219539642 0.7537652850151062
MemoryTrain:  epoch  4, batch     2 | loss: 0.8819875Losses:  0.12520378828048706 0.44964879751205444
MemoryTrain:  epoch  4, batch     3 | loss: 0.5748526Losses:  0.12283172458410263 0.6524674296379089
MemoryTrain:  epoch  4, batch     4 | loss: 0.7752991Losses:  0.09871954470872879 0.5373616218566895
MemoryTrain:  epoch  5, batch     0 | loss: 0.6360812Losses:  0.06805873662233353 0.3328518867492676
MemoryTrain:  epoch  5, batch     1 | loss: 0.4009106Losses:  0.05730701982975006 0.7173337340354919
MemoryTrain:  epoch  5, batch     2 | loss: 0.7746407Losses:  0.07855549454689026 0.7113468647003174
MemoryTrain:  epoch  5, batch     3 | loss: 0.7899023Losses:  0.12989719212055206 0.8191943168640137
MemoryTrain:  epoch  5, batch     4 | loss: 0.9490915Losses:  0.13933977484703064 0.5956114530563354
MemoryTrain:  epoch  6, batch     0 | loss: 0.7349513Losses:  0.07602493464946747 0.5431500673294067
MemoryTrain:  epoch  6, batch     1 | loss: 0.6191750Losses:  0.0778394341468811 0.5293529033660889
MemoryTrain:  epoch  6, batch     2 | loss: 0.6071923Losses:  0.043238185346126556 0.46101313829421997
MemoryTrain:  epoch  6, batch     3 | loss: 0.5042513Losses:  0.059575267136096954 0.7004510164260864
MemoryTrain:  epoch  6, batch     4 | loss: 0.7600263Losses:  0.08241061866283417 0.6034970283508301
MemoryTrain:  epoch  7, batch     0 | loss: 0.6859077Losses:  0.06325095146894455 0.5652638673782349
MemoryTrain:  epoch  7, batch     1 | loss: 0.6285148Losses:  0.16878479719161987 0.40841740369796753
MemoryTrain:  epoch  7, batch     2 | loss: 0.5772022Losses:  0.025929659605026245 0.5428258180618286
MemoryTrain:  epoch  7, batch     3 | loss: 0.5687555Losses:  0.11194920539855957 0.5091422200202942
MemoryTrain:  epoch  7, batch     4 | loss: 0.6210914Losses:  0.045149024575948715 0.4701174199581146
MemoryTrain:  epoch  8, batch     0 | loss: 0.5152664Losses:  0.04596024751663208 0.3843649625778198
MemoryTrain:  epoch  8, batch     1 | loss: 0.4303252Losses:  0.07315488159656525 0.9149129390716553
MemoryTrain:  epoch  8, batch     2 | loss: 0.9880678Losses:  0.04122205451130867 0.43175747990608215
MemoryTrain:  epoch  8, batch     3 | loss: 0.4729795Losses:  0.06318937242031097 0.4553346633911133
MemoryTrain:  epoch  8, batch     4 | loss: 0.5185241Losses:  0.32194408774375916 0.6773582696914673
MemoryTrain:  epoch  9, batch     0 | loss: 0.9993024Losses:  0.06695793569087982 0.6051242351531982
MemoryTrain:  epoch  9, batch     1 | loss: 0.6720822Losses:  0.19292016327381134 0.40940526127815247
MemoryTrain:  epoch  9, batch     2 | loss: 0.6023254Losses:  0.02729850448668003 0.540450930595398
MemoryTrain:  epoch  9, batch     3 | loss: 0.5677494Losses:  0.04878200963139534 0.38327324390411377
MemoryTrain:  epoch  9, batch     4 | loss: 0.4320553
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 84.84%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 83.61%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 81.65%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 81.12%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 80.79%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 80.70%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 79.74%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 77.25%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 76.31%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 75.40%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 90.81%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.98%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.25%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 92.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 91.21%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.94%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.68%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 89.88%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 88.48%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 87.31%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 84.89%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 83.73%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 83.15%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.53%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 83.69%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.94%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 83.61%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 82.98%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 82.14%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 81.32%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 80.60%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 79.74%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 79.05%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 78.16%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 77.29%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 76.44%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 75.68%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 74.93%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 74.34%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 76.05%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 75.69%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 75.06%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 74.94%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 74.56%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 74.45%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 74.51%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 74.62%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 74.68%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 74.33%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 73.96%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 73.82%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 73.58%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 73.35%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 73.41%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 73.14%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 74.53%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.50%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 74.30%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 74.02%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 74.03%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 73.79%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 73.68%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 73.49%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 73.42%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 73.43%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 73.28%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 73.35%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.16%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.68%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.17%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 71.93%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 71.70%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 71.50%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 72.71%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.91%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 73.11%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 73.12%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 73.13%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 73.06%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 73.17%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 73.31%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  210 | acc: 81.25%,  total acc: 73.76%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 73.85%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 75.25%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 75.11%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 74.87%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 74.66%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 74.90%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 74.85%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 74.62%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 74.39%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 74.19%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 74.07%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 73.90%   
cur_acc:  ['0.9474', '0.6349', '0.7530', '0.7540']
his_acc:  ['0.9474', '0.7920', '0.7537', '0.7390']
Clustering into  24  clusters
Clusters:  [ 2  1 15  0  2  2 19  2 14 21 22  0  2  0  0 20  2  2  2  2  2  2  2  9
  2 16  2 23 13 12 18  2 11  2  7 17  8  2  2  2  1  6  2  2  2  3  2 10
  5  4]
Losses:  5.111542701721191 1.148449182510376
CurrentTrain: epoch  0, batch     0 | loss: 6.2599916Losses:  4.5001654624938965 0.9513635635375977
CurrentTrain: epoch  0, batch     1 | loss: 5.4515290Losses:  6.165943145751953 1.2466524839401245
CurrentTrain: epoch  0, batch     2 | loss: 7.4125957Losses:  7.597692489624023 1.1940981149673462
CurrentTrain: epoch  0, batch     3 | loss: 8.7917910Losses:  5.613681793212891 1.0643342733383179
CurrentTrain: epoch  0, batch     4 | loss: 6.6780162Losses:  6.100814342498779 1.1150479316711426
CurrentTrain: epoch  0, batch     5 | loss: 7.2158623Losses:  4.960143566131592 0.20958620309829712
CurrentTrain: epoch  0, batch     6 | loss: 5.1697297Losses:  3.804488182067871 0.5068220496177673
CurrentTrain: epoch  1, batch     0 | loss: 4.3113103Losses:  6.555602073669434 0.8423884510993958
CurrentTrain: epoch  1, batch     1 | loss: 7.3979907Losses:  4.581517219543457 0.9011344313621521
CurrentTrain: epoch  1, batch     2 | loss: 5.4826517Losses:  4.601107597351074 1.1388094425201416
CurrentTrain: epoch  1, batch     3 | loss: 5.7399168Losses:  3.928731679916382 1.1158781051635742
CurrentTrain: epoch  1, batch     4 | loss: 5.0446100Losses:  4.563741683959961 0.8126014471054077
CurrentTrain: epoch  1, batch     5 | loss: 5.3763433Losses:  4.069126605987549 0.2698144316673279
CurrentTrain: epoch  1, batch     6 | loss: 4.3389411Losses:  5.596774578094482 0.9128689765930176
CurrentTrain: epoch  2, batch     0 | loss: 6.5096436Losses:  4.0118865966796875 0.7288254499435425
CurrentTrain: epoch  2, batch     1 | loss: 4.7407122Losses:  3.238337993621826 0.9500714540481567
CurrentTrain: epoch  2, batch     2 | loss: 4.1884093Losses:  4.128583908081055 1.041054368019104
CurrentTrain: epoch  2, batch     3 | loss: 5.1696382Losses:  2.8109607696533203 0.9931442141532898
CurrentTrain: epoch  2, batch     4 | loss: 3.8041050Losses:  3.454263687133789 0.7813396453857422
CurrentTrain: epoch  2, batch     5 | loss: 4.2356033Losses:  5.789128303527832 0.3211256265640259
CurrentTrain: epoch  2, batch     6 | loss: 6.1102538Losses:  4.056556701660156 1.2057006359100342
CurrentTrain: epoch  3, batch     0 | loss: 5.2622576Losses:  2.8034253120422363 0.7835077047348022
CurrentTrain: epoch  3, batch     1 | loss: 3.5869331Losses:  3.543477773666382 0.7474926114082336
CurrentTrain: epoch  3, batch     2 | loss: 4.2909703Losses:  3.2871103286743164 0.9480615258216858
CurrentTrain: epoch  3, batch     3 | loss: 4.2351718Losses:  2.623399019241333 0.4667838215827942
CurrentTrain: epoch  3, batch     4 | loss: 3.0901828Losses:  3.7442240715026855 1.0323083400726318
CurrentTrain: epoch  3, batch     5 | loss: 4.7765322Losses:  6.0765862464904785 0.33667778968811035
CurrentTrain: epoch  3, batch     6 | loss: 6.4132643Losses:  2.2868659496307373 0.5531454086303711
CurrentTrain: epoch  4, batch     0 | loss: 2.8400114Losses:  3.56601619720459 0.774421751499176
CurrentTrain: epoch  4, batch     1 | loss: 4.3404379Losses:  3.8911428451538086 0.7777979373931885
CurrentTrain: epoch  4, batch     2 | loss: 4.6689405Losses:  2.9440369606018066 0.735173761844635
CurrentTrain: epoch  4, batch     3 | loss: 3.6792107Losses:  2.3830442428588867 0.8421955704689026
CurrentTrain: epoch  4, batch     4 | loss: 3.2252398Losses:  2.98911190032959 0.8140029907226562
CurrentTrain: epoch  4, batch     5 | loss: 3.8031149Losses:  4.656432151794434 0.42308083176612854
CurrentTrain: epoch  4, batch     6 | loss: 5.0795131Losses:  3.165663719177246 0.9435609579086304
CurrentTrain: epoch  5, batch     0 | loss: 4.1092248Losses:  3.0670158863067627 0.8144135475158691
CurrentTrain: epoch  5, batch     1 | loss: 3.8814294Losses:  2.8635644912719727 0.6983816623687744
CurrentTrain: epoch  5, batch     2 | loss: 3.5619462Losses:  2.485048532485962 0.8129496574401855
CurrentTrain: epoch  5, batch     3 | loss: 3.2979982Losses:  2.499485492706299 0.8649486303329468
CurrentTrain: epoch  5, batch     4 | loss: 3.3644342Losses:  3.2133893966674805 0.6452666521072388
CurrentTrain: epoch  5, batch     5 | loss: 3.8586559Losses:  2.38883900642395 0.14135652780532837
CurrentTrain: epoch  5, batch     6 | loss: 2.5301955Losses:  2.664313316345215 0.7515612244606018
CurrentTrain: epoch  6, batch     0 | loss: 3.4158745Losses:  3.1015939712524414 0.6956228017807007
CurrentTrain: epoch  6, batch     1 | loss: 3.7972169Losses:  2.7857608795166016 0.8376867771148682
CurrentTrain: epoch  6, batch     2 | loss: 3.6234477Losses:  2.6545217037200928 0.8823077082633972
CurrentTrain: epoch  6, batch     3 | loss: 3.5368295Losses:  1.9808878898620605 0.4777345359325409
CurrentTrain: epoch  6, batch     4 | loss: 2.4586225Losses:  2.5828826427459717 0.7502537965774536
CurrentTrain: epoch  6, batch     5 | loss: 3.3331366Losses:  1.78653085231781 0.17034034430980682
CurrentTrain: epoch  6, batch     6 | loss: 1.9568712Losses:  2.572996139526367 0.7438743114471436
CurrentTrain: epoch  7, batch     0 | loss: 3.3168705Losses:  2.122401237487793 0.4942026734352112
CurrentTrain: epoch  7, batch     1 | loss: 2.6166039Losses:  2.141680955886841 0.7530759572982788
CurrentTrain: epoch  7, batch     2 | loss: 2.8947568Losses:  2.375584602355957 0.5578274130821228
CurrentTrain: epoch  7, batch     3 | loss: 2.9334121Losses:  2.2788290977478027 0.8192972540855408
CurrentTrain: epoch  7, batch     4 | loss: 3.0981264Losses:  2.7112929821014404 0.8552533388137817
CurrentTrain: epoch  7, batch     5 | loss: 3.5665464Losses:  3.0697476863861084 0.4862896800041199
CurrentTrain: epoch  7, batch     6 | loss: 3.5560374Losses:  2.355370044708252 0.7206709384918213
CurrentTrain: epoch  8, batch     0 | loss: 3.0760410Losses:  1.9108045101165771 0.5919537544250488
CurrentTrain: epoch  8, batch     1 | loss: 2.5027583Losses:  2.0551705360412598 0.6921981573104858
CurrentTrain: epoch  8, batch     2 | loss: 2.7473688Losses:  2.3254992961883545 0.43890631198883057
CurrentTrain: epoch  8, batch     3 | loss: 2.7644057Losses:  2.404224395751953 0.556308388710022
CurrentTrain: epoch  8, batch     4 | loss: 2.9605327Losses:  2.5664916038513184 0.5194021463394165
CurrentTrain: epoch  8, batch     5 | loss: 3.0858936Losses:  2.424891948699951 0.12129310518503189
CurrentTrain: epoch  8, batch     6 | loss: 2.5461850Losses:  2.163438320159912 0.39388447999954224
CurrentTrain: epoch  9, batch     0 | loss: 2.5573227Losses:  2.30911922454834 0.8346876502037048
CurrentTrain: epoch  9, batch     1 | loss: 3.1438069Losses:  1.881409764289856 0.5052316188812256
CurrentTrain: epoch  9, batch     2 | loss: 2.3866415Losses:  2.1950221061706543 0.5331117510795593
CurrentTrain: epoch  9, batch     3 | loss: 2.7281339Losses:  2.1063239574432373 0.49734780192375183
CurrentTrain: epoch  9, batch     4 | loss: 2.6036718Losses:  2.4658565521240234 0.6646794676780701
CurrentTrain: epoch  9, batch     5 | loss: 3.1305361Losses:  2.3145627975463867 0.07919054478406906
CurrentTrain: epoch  9, batch     6 | loss: 2.3937533
Losses:  0.9209355711936951 0.8411533832550049
MemoryTrain:  epoch  0, batch     0 | loss: 1.7620890Losses:  0.7464866042137146 0.6119649410247803
MemoryTrain:  epoch  0, batch     1 | loss: 1.3584516Losses:  1.31056547164917 0.6623594760894775
MemoryTrain:  epoch  0, batch     2 | loss: 1.9729249Losses:  1.0839537382125854 0.613438606262207
MemoryTrain:  epoch  0, batch     3 | loss: 1.6973923Losses:  0.6609653234481812 0.46692368388175964
MemoryTrain:  epoch  0, batch     4 | loss: 1.1278890Losses:  0.11844394356012344 0.3683425784111023
MemoryTrain:  epoch  0, batch     5 | loss: 0.4867865Losses:  0.03932860493659973 0.1670892834663391
MemoryTrain:  epoch  0, batch     6 | loss: 0.2064179Losses:  0.945581316947937 0.4716370105743408
MemoryTrain:  epoch  1, batch     0 | loss: 1.4172183Losses:  0.9484753012657166 0.7109285593032837
MemoryTrain:  epoch  1, batch     1 | loss: 1.6594038Losses:  0.6053360104560852 0.44764161109924316
MemoryTrain:  epoch  1, batch     2 | loss: 1.0529776Losses:  0.7620108127593994 0.3766169548034668
MemoryTrain:  epoch  1, batch     3 | loss: 1.1386278Losses:  0.4990035593509674 0.6544456481933594
MemoryTrain:  epoch  1, batch     4 | loss: 1.1534492Losses:  1.1723434925079346 0.45467376708984375
MemoryTrain:  epoch  1, batch     5 | loss: 1.6270173Losses:  0.8575432300567627 0.04373908042907715
MemoryTrain:  epoch  1, batch     6 | loss: 0.9012823Losses:  0.1739155352115631 0.5629856586456299
MemoryTrain:  epoch  2, batch     0 | loss: 0.7369012Losses:  0.07155359536409378 0.37658965587615967
MemoryTrain:  epoch  2, batch     1 | loss: 0.4481432Losses:  0.3482358455657959 0.5118464231491089
MemoryTrain:  epoch  2, batch     2 | loss: 0.8600823Losses:  0.4253842830657959 0.5210815072059631
MemoryTrain:  epoch  2, batch     3 | loss: 0.9464658Losses:  0.654518723487854 0.7267482280731201
MemoryTrain:  epoch  2, batch     4 | loss: 1.3812670Losses:  0.20765893161296844 0.7454906702041626
MemoryTrain:  epoch  2, batch     5 | loss: 0.9531496Losses:  0.016768956556916237 0.016224121674895287
MemoryTrain:  epoch  2, batch     6 | loss: 0.0329931Losses:  0.10733524709939957 0.48681163787841797
MemoryTrain:  epoch  3, batch     0 | loss: 0.5941469Losses:  0.1881745457649231 0.7885333299636841
MemoryTrain:  epoch  3, batch     1 | loss: 0.9767079Losses:  1.1013070344924927 0.5978080034255981
MemoryTrain:  epoch  3, batch     2 | loss: 1.6991150Losses:  0.34029483795166016 0.6530923843383789
MemoryTrain:  epoch  3, batch     3 | loss: 0.9933872Losses:  0.07418376952409744 0.4541561007499695
MemoryTrain:  epoch  3, batch     4 | loss: 0.5283399Losses:  0.2101462483406067 0.35694581270217896
MemoryTrain:  epoch  3, batch     5 | loss: 0.5670921Losses:  0.16348643600940704 0.09463174641132355
MemoryTrain:  epoch  3, batch     6 | loss: 0.2581182Losses:  0.1152026429772377 0.4852222502231598
MemoryTrain:  epoch  4, batch     0 | loss: 0.6004249Losses:  0.17739981412887573 0.5662418603897095
MemoryTrain:  epoch  4, batch     1 | loss: 0.7436417Losses:  0.09311208128929138 0.5108383893966675
MemoryTrain:  epoch  4, batch     2 | loss: 0.6039505Losses:  0.12375671416521072 0.525298535823822
MemoryTrain:  epoch  4, batch     3 | loss: 0.6490552Losses:  0.09479618072509766 0.6996960043907166
MemoryTrain:  epoch  4, batch     4 | loss: 0.7944922Losses:  0.11916877329349518 0.5491806864738464
MemoryTrain:  epoch  4, batch     5 | loss: 0.6683494Losses:  0.07496722787618637 0.00814556423574686
MemoryTrain:  epoch  4, batch     6 | loss: 0.0831128Losses:  0.17321792244911194 0.6801329851150513
MemoryTrain:  epoch  5, batch     0 | loss: 0.8533509Losses:  0.055685147643089294 0.5609833002090454
MemoryTrain:  epoch  5, batch     1 | loss: 0.6166685Losses:  0.04553763568401337 0.5561914443969727
MemoryTrain:  epoch  5, batch     2 | loss: 0.6017291Losses:  0.05023680999875069 0.4101272225379944
MemoryTrain:  epoch  5, batch     3 | loss: 0.4603640Losses:  0.039729636162519455 0.3834347724914551
MemoryTrain:  epoch  5, batch     4 | loss: 0.4231644Losses:  0.10581512004137039 0.5520921945571899
MemoryTrain:  epoch  5, batch     5 | loss: 0.6579073Losses:  0.03383293375372887 0.018365144729614258
MemoryTrain:  epoch  5, batch     6 | loss: 0.0521981Losses:  0.04003637284040451 0.5213009119033813
MemoryTrain:  epoch  6, batch     0 | loss: 0.5613373Losses:  0.03438882529735565 0.40060338377952576
MemoryTrain:  epoch  6, batch     1 | loss: 0.4349922Losses:  0.04381338506937027 0.4575364291667938
MemoryTrain:  epoch  6, batch     2 | loss: 0.5013498Losses:  0.07781195640563965 0.518765926361084
MemoryTrain:  epoch  6, batch     3 | loss: 0.5965779Losses:  0.03685405105352402 0.5889913439750671
MemoryTrain:  epoch  6, batch     4 | loss: 0.6258454Losses:  0.08517713844776154 0.4894380569458008
MemoryTrain:  epoch  6, batch     5 | loss: 0.5746152Losses:  0.13282619416713715 0.17069774866104126
MemoryTrain:  epoch  6, batch     6 | loss: 0.3035240Losses:  0.046739477664232254 0.6459015607833862
MemoryTrain:  epoch  7, batch     0 | loss: 0.6926410Losses:  0.05456879734992981 0.4600324034690857
MemoryTrain:  epoch  7, batch     1 | loss: 0.5146012Losses:  0.061656929552555084 0.5101621150970459
MemoryTrain:  epoch  7, batch     2 | loss: 0.5718191Losses:  0.035896189510822296 0.29355162382125854
MemoryTrain:  epoch  7, batch     3 | loss: 0.3294478Losses:  0.03883429989218712 0.39710354804992676
MemoryTrain:  epoch  7, batch     4 | loss: 0.4359379Losses:  0.06967026740312576 0.49508363008499146
MemoryTrain:  epoch  7, batch     5 | loss: 0.5647539Losses:  0.05165240913629532 0.10840168595314026
MemoryTrain:  epoch  7, batch     6 | loss: 0.1600541Losses:  0.042678095400333405 0.3509534001350403
MemoryTrain:  epoch  8, batch     0 | loss: 0.3936315Losses:  0.043866101652383804 0.5052855610847473
MemoryTrain:  epoch  8, batch     1 | loss: 0.5491517Losses:  0.03778574988245964 0.5546002388000488
MemoryTrain:  epoch  8, batch     2 | loss: 0.5923860Losses:  0.05617627501487732 0.46113860607147217
MemoryTrain:  epoch  8, batch     3 | loss: 0.5173149Losses:  0.029724884778261185 0.5065798163414001
MemoryTrain:  epoch  8, batch     4 | loss: 0.5363047Losses:  0.05873630568385124 0.52001953125
MemoryTrain:  epoch  8, batch     5 | loss: 0.5787559Losses:  0.020882559940218925 0.01788477785885334
MemoryTrain:  epoch  8, batch     6 | loss: 0.0387673Losses:  0.06645247340202332 0.42872217297554016
MemoryTrain:  epoch  9, batch     0 | loss: 0.4951746Losses:  0.0360703282058239 0.395263135433197
MemoryTrain:  epoch  9, batch     1 | loss: 0.4313335Losses:  0.04595273733139038 0.4679725170135498
MemoryTrain:  epoch  9, batch     2 | loss: 0.5139253Losses:  0.04036959260702133 0.4368136525154114
MemoryTrain:  epoch  9, batch     3 | loss: 0.4771833Losses:  0.04104657098650932 0.49054619669914246
MemoryTrain:  epoch  9, batch     4 | loss: 0.5315928Losses:  0.03620140999555588 0.4992569088935852
MemoryTrain:  epoch  9, batch     5 | loss: 0.5354583Losses:  0.022643890231847763 0.04358293116092682
MemoryTrain:  epoch  9, batch     6 | loss: 0.0662268
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.02%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.48%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.69%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 87.30%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 86.06%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 83.58%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 81.88%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 81.95%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 82.65%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.63%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 82.91%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.05%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 82.23%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 81.55%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 80.81%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 80.09%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 79.31%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 77.67%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 76.81%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.96%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 75.20%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 74.88%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 74.88%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 74.20%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 73.52%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 72.86%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 72.21%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 71.74%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 71.33%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 71.11%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 70.90%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 70.48%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 70.18%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 71.73%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 71.55%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 71.24%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 71.10%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 70.77%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 70.51%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 70.30%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 70.21%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 70.13%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 69.92%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 69.82%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 69.74%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 69.16%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 67.85%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 69.48%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 69.76%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 69.86%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 69.88%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 71.75%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 71.65%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 71.61%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 71.79%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 71.65%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 71.56%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 71.45%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 71.41%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 71.34%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 71.38%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 71.26%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 71.28%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.16%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 71.30%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 71.72%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 71.59%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 71.30%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.27%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 71.17%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 71.12%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 70.84%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 70.77%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 70.86%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 71.01%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 72.08%   
cur_acc:  ['0.9474', '0.6349', '0.7530', '0.7540', '0.7500']
his_acc:  ['0.9474', '0.7920', '0.7537', '0.7390', '0.7208']
Clustering into  29  clusters
Clusters:  [ 0  1 21  0  0  0 25  0 15 27 16  0  0  2  2  7  0  0  0  0  0  0  0 28
  0 19  0 23 17 22 24  0 14  0 18 20  9  0  0  0  1  8  0  0  0  3  0 13
 26 11 10  4  2  0  0 12  0  6  0  5]
Losses:  6.704677104949951 1.0997179746627808
CurrentTrain: epoch  0, batch     0 | loss: 7.8043952Losses:  5.162194728851318 0.8132842779159546
CurrentTrain: epoch  0, batch     1 | loss: 5.9754791Losses:  6.42861795425415 1.2019606828689575
CurrentTrain: epoch  0, batch     2 | loss: 7.6305785Losses:  6.738931655883789 1.0795584917068481
CurrentTrain: epoch  0, batch     3 | loss: 7.8184900Losses:  7.270812034606934 1.7329330444335938
CurrentTrain: epoch  0, batch     4 | loss: 9.0037451Losses:  6.259133338928223 1.1794770956039429
CurrentTrain: epoch  0, batch     5 | loss: 7.4386106Losses:  8.205263137817383 0.3986862897872925
CurrentTrain: epoch  0, batch     6 | loss: 8.6039495Losses:  4.5458221435546875 0.7910395264625549
CurrentTrain: epoch  1, batch     0 | loss: 5.3368616Losses:  5.388246059417725 1.1875089406967163
CurrentTrain: epoch  1, batch     1 | loss: 6.5757551Losses:  5.577746391296387 1.3360896110534668
CurrentTrain: epoch  1, batch     2 | loss: 6.9138360Losses:  5.016473770141602 0.7599548697471619
CurrentTrain: epoch  1, batch     3 | loss: 5.7764287Losses:  5.964478492736816 1.1440049409866333
CurrentTrain: epoch  1, batch     4 | loss: 7.1084833Losses:  5.690206527709961 1.323235273361206
CurrentTrain: epoch  1, batch     5 | loss: 7.0134420Losses:  9.21843147277832 0.7366342544555664
CurrentTrain: epoch  1, batch     6 | loss: 9.9550657Losses:  5.4589314460754395 0.791837751865387
CurrentTrain: epoch  2, batch     0 | loss: 6.2507691Losses:  3.88067626953125 1.0804733037948608
CurrentTrain: epoch  2, batch     1 | loss: 4.9611497Losses:  4.104776382446289 1.2961845397949219
CurrentTrain: epoch  2, batch     2 | loss: 5.4009609Losses:  5.882932186126709 1.072275161743164
CurrentTrain: epoch  2, batch     3 | loss: 6.9552073Losses:  3.9498376846313477 0.9123774766921997
CurrentTrain: epoch  2, batch     4 | loss: 4.8622150Losses:  4.865381240844727 1.051336646080017
CurrentTrain: epoch  2, batch     5 | loss: 5.9167180Losses:  2.832216262817383 0.30561020970344543
CurrentTrain: epoch  2, batch     6 | loss: 3.1378264Losses:  3.3361833095550537 0.9280195236206055
CurrentTrain: epoch  3, batch     0 | loss: 4.2642031Losses:  4.161486625671387 1.2058837413787842
CurrentTrain: epoch  3, batch     1 | loss: 5.3673706Losses:  5.509316444396973 1.0251543521881104
CurrentTrain: epoch  3, batch     2 | loss: 6.5344706Losses:  5.394933700561523 1.0719616413116455
CurrentTrain: epoch  3, batch     3 | loss: 6.4668951Losses:  3.37410044670105 0.9286425113677979
CurrentTrain: epoch  3, batch     4 | loss: 4.3027430Losses:  2.3127458095550537 0.4580603837966919
CurrentTrain: epoch  3, batch     5 | loss: 2.7708063Losses:  6.847780227661133 0.6173624396324158
CurrentTrain: epoch  3, batch     6 | loss: 7.4651427Losses:  3.457775831222534 0.7412207126617432
CurrentTrain: epoch  4, batch     0 | loss: 4.1989965Losses:  5.330633640289307 1.1993694305419922
CurrentTrain: epoch  4, batch     1 | loss: 6.5300031Losses:  4.199913024902344 1.0639979839324951
CurrentTrain: epoch  4, batch     2 | loss: 5.2639112Losses:  3.936145305633545 0.9741144180297852
CurrentTrain: epoch  4, batch     3 | loss: 4.9102597Losses:  2.3395934104919434 0.5039674639701843
CurrentTrain: epoch  4, batch     4 | loss: 2.8435609Losses:  4.198270797729492 0.9808027744293213
CurrentTrain: epoch  4, batch     5 | loss: 5.1790733Losses:  2.343358039855957 0.37095290422439575
CurrentTrain: epoch  4, batch     6 | loss: 2.7143109Losses:  3.997891426086426 0.5682352781295776
CurrentTrain: epoch  5, batch     0 | loss: 4.5661268Losses:  4.010729789733887 0.7652389407157898
CurrentTrain: epoch  5, batch     1 | loss: 4.7759686Losses:  3.868293523788452 0.8579800128936768
CurrentTrain: epoch  5, batch     2 | loss: 4.7262735Losses:  3.898502826690674 0.9775995016098022
CurrentTrain: epoch  5, batch     3 | loss: 4.8761024Losses:  3.195695400238037 0.7117305397987366
CurrentTrain: epoch  5, batch     4 | loss: 3.9074259Losses:  2.209321975708008 0.4665345549583435
CurrentTrain: epoch  5, batch     5 | loss: 2.6758566Losses:  3.9324300289154053 0.34884727001190186
CurrentTrain: epoch  5, batch     6 | loss: 4.2812772Losses:  3.4656598567962646 1.029979944229126
CurrentTrain: epoch  6, batch     0 | loss: 4.4956398Losses:  2.663243055343628 0.758013904094696
CurrentTrain: epoch  6, batch     1 | loss: 3.4212570Losses:  3.2608718872070312 0.7819996476173401
CurrentTrain: epoch  6, batch     2 | loss: 4.0428715Losses:  3.203115463256836 0.8308492302894592
CurrentTrain: epoch  6, batch     3 | loss: 4.0339646Losses:  3.2630138397216797 0.5071244239807129
CurrentTrain: epoch  6, batch     4 | loss: 3.7701383Losses:  3.688368082046509 0.6179736852645874
CurrentTrain: epoch  6, batch     5 | loss: 4.3063416Losses:  5.01046085357666 0.13267113268375397
CurrentTrain: epoch  6, batch     6 | loss: 5.1431322Losses:  3.4433670043945312 1.0358726978302002
CurrentTrain: epoch  7, batch     0 | loss: 4.4792395Losses:  3.2397475242614746 0.7343080043792725
CurrentTrain: epoch  7, batch     1 | loss: 3.9740555Losses:  2.861600637435913 0.43508273363113403
CurrentTrain: epoch  7, batch     2 | loss: 3.2966833Losses:  2.9900383949279785 0.7925289869308472
CurrentTrain: epoch  7, batch     3 | loss: 3.7825675Losses:  3.043494939804077 0.654838502407074
CurrentTrain: epoch  7, batch     4 | loss: 3.6983335Losses:  3.369717597961426 0.9416095018386841
CurrentTrain: epoch  7, batch     5 | loss: 4.3113270Losses:  1.8859808444976807 0.0910944864153862
CurrentTrain: epoch  7, batch     6 | loss: 1.9770753Losses:  2.7117745876312256 0.6789226531982422
CurrentTrain: epoch  8, batch     0 | loss: 3.3906972Losses:  3.0827481746673584 0.6677576899528503
CurrentTrain: epoch  8, batch     1 | loss: 3.7505059Losses:  2.9225058555603027 0.5691239833831787
CurrentTrain: epoch  8, batch     2 | loss: 3.4916298Losses:  3.0602989196777344 0.5900428295135498
CurrentTrain: epoch  8, batch     3 | loss: 3.6503417Losses:  2.8485636711120605 0.8446652293205261
CurrentTrain: epoch  8, batch     4 | loss: 3.6932290Losses:  3.082064390182495 0.7561469078063965
CurrentTrain: epoch  8, batch     5 | loss: 3.8382113Losses:  1.7535998821258545 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.7535999Losses:  2.1429591178894043 0.520068883895874
CurrentTrain: epoch  9, batch     0 | loss: 2.6630280Losses:  3.1499829292297363 0.45740342140197754
CurrentTrain: epoch  9, batch     1 | loss: 3.6073864Losses:  3.1495277881622314 0.8028576374053955
CurrentTrain: epoch  9, batch     2 | loss: 3.9523854Losses:  3.1754000186920166 0.7601814270019531
CurrentTrain: epoch  9, batch     3 | loss: 3.9355814Losses:  2.550792694091797 0.6121323108673096
CurrentTrain: epoch  9, batch     4 | loss: 3.1629250Losses:  3.2594752311706543 0.738988995552063
CurrentTrain: epoch  9, batch     5 | loss: 3.9984641Losses:  1.8894932270050049 0.1108071506023407
CurrentTrain: epoch  9, batch     6 | loss: 2.0003004
Losses:  1.1816704273223877 0.5030083656311035
MemoryTrain:  epoch  0, batch     0 | loss: 1.6846788Losses:  0.46352046728134155 0.3907380700111389
MemoryTrain:  epoch  0, batch     1 | loss: 0.8542585Losses:  0.9972819089889526 0.574901819229126
MemoryTrain:  epoch  0, batch     2 | loss: 1.5721837Losses:  0.18408796191215515 0.6989941596984863
MemoryTrain:  epoch  0, batch     3 | loss: 0.8830822Losses:  0.14602932333946228 0.37939441204071045
MemoryTrain:  epoch  0, batch     4 | loss: 0.5254238Losses:  0.03960733860731125 0.6724194288253784
MemoryTrain:  epoch  0, batch     5 | loss: 0.7120268Losses:  1.2306091785430908 0.6791709661483765
MemoryTrain:  epoch  0, batch     6 | loss: 1.9097801Losses:  1.322399377822876 0.49402952194213867
MemoryTrain:  epoch  0, batch     7 | loss: 1.8164289Losses:  0.398429274559021 0.6136511564254761
MemoryTrain:  epoch  1, batch     0 | loss: 1.0120804Losses:  1.148505687713623 0.650408923625946
MemoryTrain:  epoch  1, batch     1 | loss: 1.7989147Losses:  0.0584757998585701 0.4768166244029999
MemoryTrain:  epoch  1, batch     2 | loss: 0.5352924Losses:  0.5989838242530823 0.45204877853393555
MemoryTrain:  epoch  1, batch     3 | loss: 1.0510325Losses:  0.6158187389373779 0.375271737575531
MemoryTrain:  epoch  1, batch     4 | loss: 0.9910905Losses:  0.9061378836631775 0.8723678588867188
MemoryTrain:  epoch  1, batch     5 | loss: 1.7785058Losses:  0.4966105818748474 0.3662829101085663
MemoryTrain:  epoch  1, batch     6 | loss: 0.8628935Losses:  0.1941101849079132 0.2177410125732422
MemoryTrain:  epoch  1, batch     7 | loss: 0.4118512Losses:  0.18308588862419128 0.48078495264053345
MemoryTrain:  epoch  2, batch     0 | loss: 0.6638708Losses:  0.5980316400527954 0.5189168453216553
MemoryTrain:  epoch  2, batch     1 | loss: 1.1169485Losses:  0.6480672359466553 0.5008947849273682
MemoryTrain:  epoch  2, batch     2 | loss: 1.1489620Losses:  0.10704254359006882 0.42834991216659546
MemoryTrain:  epoch  2, batch     3 | loss: 0.5353925Losses:  0.14184796810150146 0.40280312299728394
MemoryTrain:  epoch  2, batch     4 | loss: 0.5446511Losses:  0.35570013523101807 0.6585425734519958
MemoryTrain:  epoch  2, batch     5 | loss: 1.0142426Losses:  0.0752096176147461 0.32476985454559326
MemoryTrain:  epoch  2, batch     6 | loss: 0.3999795Losses:  0.08500225841999054 0.47374188899993896
MemoryTrain:  epoch  2, batch     7 | loss: 0.5587441Losses:  0.1562202423810959 0.3331872820854187
MemoryTrain:  epoch  3, batch     0 | loss: 0.4894075Losses:  0.14592060446739197 0.5211904048919678
MemoryTrain:  epoch  3, batch     1 | loss: 0.6671110Losses:  0.07172887027263641 0.6120587587356567
MemoryTrain:  epoch  3, batch     2 | loss: 0.6837876Losses:  0.1931336224079132 0.5231192111968994
MemoryTrain:  epoch  3, batch     3 | loss: 0.7162528Losses:  0.27215924859046936 0.87711101770401
MemoryTrain:  epoch  3, batch     4 | loss: 1.1492703Losses:  0.05993940308690071 0.42091548442840576
MemoryTrain:  epoch  3, batch     5 | loss: 0.4808549Losses:  0.5226818919181824 0.4583391547203064
MemoryTrain:  epoch  3, batch     6 | loss: 0.9810210Losses:  0.039843130856752396 0.21664679050445557
MemoryTrain:  epoch  3, batch     7 | loss: 0.2564899Losses:  0.043597280979156494 0.3613947927951813
MemoryTrain:  epoch  4, batch     0 | loss: 0.4049921Losses:  0.1005219966173172 0.4531777799129486
MemoryTrain:  epoch  4, batch     1 | loss: 0.5536998Losses:  0.2584144175052643 0.4217489957809448
MemoryTrain:  epoch  4, batch     2 | loss: 0.6801634Losses:  0.15818887948989868 0.5313025712966919
MemoryTrain:  epoch  4, batch     3 | loss: 0.6894915Losses:  0.16606377065181732 0.6428025960922241
MemoryTrain:  epoch  4, batch     4 | loss: 0.8088664Losses:  0.13221606612205505 0.5058172941207886
MemoryTrain:  epoch  4, batch     5 | loss: 0.6380334Losses:  0.16768866777420044 0.5106359720230103
MemoryTrain:  epoch  4, batch     6 | loss: 0.6783246Losses:  0.08452432602643967 0.18318790197372437
MemoryTrain:  epoch  4, batch     7 | loss: 0.2677122Losses:  0.059395432472229004 0.521738588809967
MemoryTrain:  epoch  5, batch     0 | loss: 0.5811340Losses:  0.1914898157119751 0.548952579498291
MemoryTrain:  epoch  5, batch     1 | loss: 0.7404424Losses:  0.06318177282810211 0.30850139260292053
MemoryTrain:  epoch  5, batch     2 | loss: 0.3716832Losses:  0.07511574029922485 0.576233983039856
MemoryTrain:  epoch  5, batch     3 | loss: 0.6513497Losses:  0.043441273272037506 0.4444189667701721
MemoryTrain:  epoch  5, batch     4 | loss: 0.4878602Losses:  0.06234478950500488 0.45102623105049133
MemoryTrain:  epoch  5, batch     5 | loss: 0.5133710Losses:  0.04730154573917389 0.42230361700057983
MemoryTrain:  epoch  5, batch     6 | loss: 0.4696051Losses:  0.3528251647949219 0.35577139258384705
MemoryTrain:  epoch  5, batch     7 | loss: 0.7085966Losses:  0.0958920568227768 0.5577641129493713
MemoryTrain:  epoch  6, batch     0 | loss: 0.6536562Losses:  0.12425853312015533 0.3850100636482239
MemoryTrain:  epoch  6, batch     1 | loss: 0.5092686Losses:  0.06283555924892426 0.5178331136703491
MemoryTrain:  epoch  6, batch     2 | loss: 0.5806687Losses:  0.11892367154359818 0.42906618118286133
MemoryTrain:  epoch  6, batch     3 | loss: 0.5479898Losses:  0.04588817059993744 0.4457433819770813
MemoryTrain:  epoch  6, batch     4 | loss: 0.4916316Losses:  0.030865907669067383 0.369337797164917
MemoryTrain:  epoch  6, batch     5 | loss: 0.4002037Losses:  0.06712228059768677 0.4556081295013428
MemoryTrain:  epoch  6, batch     6 | loss: 0.5227304Losses:  0.026843052357435226 0.23966410756111145
MemoryTrain:  epoch  6, batch     7 | loss: 0.2665071Losses:  0.05834002047777176 0.2962985038757324
MemoryTrain:  epoch  7, batch     0 | loss: 0.3546385Losses:  0.04199230298399925 0.45000210404396057
MemoryTrain:  epoch  7, batch     1 | loss: 0.4919944Losses:  0.06004119664430618 0.4621378779411316
MemoryTrain:  epoch  7, batch     2 | loss: 0.5221791Losses:  0.051260486245155334 0.37257349491119385
MemoryTrain:  epoch  7, batch     3 | loss: 0.4238340Losses:  0.05139349400997162 0.549615204334259
MemoryTrain:  epoch  7, batch     4 | loss: 0.6010087Losses:  0.10203081369400024 0.6374894380569458
MemoryTrain:  epoch  7, batch     5 | loss: 0.7395203Losses:  0.0531289204955101 0.3597721457481384
MemoryTrain:  epoch  7, batch     6 | loss: 0.4129011Losses:  0.08662248402833939 0.14090131223201752
MemoryTrain:  epoch  7, batch     7 | loss: 0.2275238Losses:  0.09525148570537567 0.4281344413757324
MemoryTrain:  epoch  8, batch     0 | loss: 0.5233859Losses:  0.03335414454340935 0.5017796754837036
MemoryTrain:  epoch  8, batch     1 | loss: 0.5351338Losses:  0.03649540990591049 0.2663182020187378
MemoryTrain:  epoch  8, batch     2 | loss: 0.3028136Losses:  0.03944671154022217 0.41754692792892456
MemoryTrain:  epoch  8, batch     3 | loss: 0.4569936Losses:  0.044378165155649185 0.4441224932670593
MemoryTrain:  epoch  8, batch     4 | loss: 0.4885007Losses:  0.0467321015894413 0.5126996040344238
MemoryTrain:  epoch  8, batch     5 | loss: 0.5594317Losses:  0.09359225630760193 0.541576623916626
MemoryTrain:  epoch  8, batch     6 | loss: 0.6351689Losses:  0.06717254221439362 0.10467886924743652
MemoryTrain:  epoch  8, batch     7 | loss: 0.1718514Losses:  0.0436839759349823 0.3473045825958252
MemoryTrain:  epoch  9, batch     0 | loss: 0.3909886Losses:  0.03504052758216858 0.6037611961364746
MemoryTrain:  epoch  9, batch     1 | loss: 0.6388017Losses:  0.04720696806907654 0.4507054090499878
MemoryTrain:  epoch  9, batch     2 | loss: 0.4979124Losses:  0.04098663479089737 0.3532203137874603
MemoryTrain:  epoch  9, batch     3 | loss: 0.3942069Losses:  0.05908103659749031 0.48681214451789856
MemoryTrain:  epoch  9, batch     4 | loss: 0.5458932Losses:  0.03238853067159653 0.36650797724723816
MemoryTrain:  epoch  9, batch     5 | loss: 0.3988965Losses:  0.10058684647083282 0.5055188536643982
MemoryTrain:  epoch  9, batch     6 | loss: 0.6061057Losses:  0.06872107088565826 0.4281538426876068
MemoryTrain:  epoch  9, batch     7 | loss: 0.4968749
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 55.62%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 54.03%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 54.49%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.68%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 56.80%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 57.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 60.74%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.09%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 61.16%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.79%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 63.22%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 62.85%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 65.18%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.50%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.79%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.45%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.23%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 87.20%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 83.33%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 82.09%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 80.88%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 80.25%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 80.37%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 80.76%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 80.46%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 80.02%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 79.34%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 78.77%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 77.28%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 75.72%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 74.23%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 73.40%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 72.60%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 71.88%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 71.17%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 70.61%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 72.78%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 72.11%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 71.50%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 70.85%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 70.21%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 69.59%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 69.08%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 68.96%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 68.07%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 67.73%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 67.37%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 69.19%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 68.79%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 68.59%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 68.47%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 68.08%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 68.08%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 67.62%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 67.33%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 67.09%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 67.90%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 67.81%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 67.94%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 67.54%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 69.39%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 69.23%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 69.17%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 69.37%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 69.41%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 69.28%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  248 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 69.13%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 69.01%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 68.94%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 69.50%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 69.29%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.19%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 69.15%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 68.77%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 68.62%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 68.62%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 70.24%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 70.49%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 70.28%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 70.12%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 69.91%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 69.70%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 69.49%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 69.88%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 69.67%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 69.50%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 69.34%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 68.84%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 69.26%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 69.24%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 69.31%   [EVAL] batch:  359 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  362 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 69.28%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 69.57%   
cur_acc:  ['0.9474', '0.6349', '0.7530', '0.7540', '0.7500', '0.6518']
his_acc:  ['0.9474', '0.7920', '0.7537', '0.7390', '0.7208', '0.6957']
Clustering into  34  clusters
Clusters:  [ 1  2 24  1  1  1 30  1 22  0 31  1  1  3  3 19  1  1  1  1  1  1  1 32
  1  0  1 27 17 26 25  1 33  1 20 23 21  1  1  1  2 15  1  1  1  9  1 16
 29 28 14 10  3  1  1 18  1  4  1 12  8 13  1  1  6 11  7  1  1  5]
Losses:  4.467807769775391 1.0776540040969849
CurrentTrain: epoch  0, batch     0 | loss: 5.5454617Losses:  5.259651184082031 1.129451036453247
CurrentTrain: epoch  0, batch     1 | loss: 6.3891020Losses:  4.366988182067871 1.4579997062683105
CurrentTrain: epoch  0, batch     2 | loss: 5.8249879Losses:  5.2133402824401855 1.268519401550293
CurrentTrain: epoch  0, batch     3 | loss: 6.4818597Losses:  4.753267288208008 1.3419421911239624
CurrentTrain: epoch  0, batch     4 | loss: 6.0952096Losses:  4.227330684661865 0.5908244848251343
CurrentTrain: epoch  0, batch     5 | loss: 4.8181553Losses:  7.474782943725586 0.4215134382247925
CurrentTrain: epoch  0, batch     6 | loss: 7.8962965Losses:  4.603306770324707 0.8661336898803711
CurrentTrain: epoch  1, batch     0 | loss: 5.4694405Losses:  3.615217447280884 0.9511577486991882
CurrentTrain: epoch  1, batch     1 | loss: 4.5663753Losses:  3.7841310501098633 0.6289029121398926
CurrentTrain: epoch  1, batch     2 | loss: 4.4130340Losses:  3.6167478561401367 1.1558170318603516
CurrentTrain: epoch  1, batch     3 | loss: 4.7725649Losses:  3.6651575565338135 1.346895456314087
CurrentTrain: epoch  1, batch     4 | loss: 5.0120530Losses:  2.9423460960388184 1.2521830797195435
CurrentTrain: epoch  1, batch     5 | loss: 4.1945291Losses:  3.079322576522827 0.24457406997680664
CurrentTrain: epoch  1, batch     6 | loss: 3.3238966Losses:  3.3092074394226074 1.3018351793289185
CurrentTrain: epoch  2, batch     0 | loss: 4.6110425Losses:  3.217583179473877 0.8947524428367615
CurrentTrain: epoch  2, batch     1 | loss: 4.1123357Losses:  3.2547504901885986 0.769483208656311
CurrentTrain: epoch  2, batch     2 | loss: 4.0242338Losses:  3.077869415283203 0.8925846815109253
CurrentTrain: epoch  2, batch     3 | loss: 3.9704542Losses:  2.5858798027038574 1.0648961067199707
CurrentTrain: epoch  2, batch     4 | loss: 3.6507759Losses:  2.760493755340576 1.101996898651123
CurrentTrain: epoch  2, batch     5 | loss: 3.8624907Losses:  1.7590619325637817 5.960465188081798e-08
CurrentTrain: epoch  2, batch     6 | loss: 1.7590621Losses:  2.3374123573303223 0.726651668548584
CurrentTrain: epoch  3, batch     0 | loss: 3.0640640Losses:  2.807349681854248 0.8746761083602905
CurrentTrain: epoch  3, batch     1 | loss: 3.6820259Losses:  2.6485257148742676 0.9297555685043335
CurrentTrain: epoch  3, batch     2 | loss: 3.5782814Losses:  2.617983341217041 0.866304874420166
CurrentTrain: epoch  3, batch     3 | loss: 3.4842882Losses:  2.6834187507629395 0.6669230461120605
CurrentTrain: epoch  3, batch     4 | loss: 3.3503418Losses:  2.860084295272827 0.9024931192398071
CurrentTrain: epoch  3, batch     5 | loss: 3.7625775Losses:  2.965395927429199 0.20960398018360138
CurrentTrain: epoch  3, batch     6 | loss: 3.1750000Losses:  2.3379476070404053 0.5533534288406372
CurrentTrain: epoch  4, batch     0 | loss: 2.8913012Losses:  2.9586431980133057 0.6123812198638916
CurrentTrain: epoch  4, batch     1 | loss: 3.5710244Losses:  2.808137893676758 1.0546234846115112
CurrentTrain: epoch  4, batch     2 | loss: 3.8627615Losses:  2.5286946296691895 0.7592059373855591
CurrentTrain: epoch  4, batch     3 | loss: 3.2879004Losses:  2.451772689819336 0.3144212067127228
CurrentTrain: epoch  4, batch     4 | loss: 2.7661939Losses:  1.8407175540924072 0.5241460800170898
CurrentTrain: epoch  4, batch     5 | loss: 2.3648636Losses:  1.8785611391067505 0.2073165774345398
CurrentTrain: epoch  4, batch     6 | loss: 2.0858777Losses:  2.394627094268799 0.6731973886489868
CurrentTrain: epoch  5, batch     0 | loss: 3.0678244Losses:  3.1899914741516113 0.6082409620285034
CurrentTrain: epoch  5, batch     1 | loss: 3.7982326Losses:  2.304455518722534 0.4396989941596985
CurrentTrain: epoch  5, batch     2 | loss: 2.7441545Losses:  1.9292125701904297 0.3747411370277405
CurrentTrain: epoch  5, batch     3 | loss: 2.3039536Losses:  2.1576175689697266 0.503042459487915
CurrentTrain: epoch  5, batch     4 | loss: 2.6606600Losses:  2.127032995223999 0.5515433549880981
CurrentTrain: epoch  5, batch     5 | loss: 2.6785765Losses:  2.1062750816345215 0.21504172682762146
CurrentTrain: epoch  5, batch     6 | loss: 2.3213167Losses:  2.196902275085449 0.5561076402664185
CurrentTrain: epoch  6, batch     0 | loss: 2.7530098Losses:  2.244678497314453 0.6034637689590454
CurrentTrain: epoch  6, batch     1 | loss: 2.8481421Losses:  2.7675962448120117 0.9474912285804749
CurrentTrain: epoch  6, batch     2 | loss: 3.7150874Losses:  2.042955160140991 0.42470234632492065
CurrentTrain: epoch  6, batch     3 | loss: 2.4676576Losses:  2.1349539756774902 0.7403517961502075
CurrentTrain: epoch  6, batch     4 | loss: 2.8753057Losses:  1.8678979873657227 0.4616342782974243
CurrentTrain: epoch  6, batch     5 | loss: 2.3295321Losses:  1.7160714864730835 0.11063643544912338
CurrentTrain: epoch  6, batch     6 | loss: 1.8267080Losses:  2.0582289695739746 0.6848034858703613
CurrentTrain: epoch  7, batch     0 | loss: 2.7430325Losses:  1.8658232688903809 0.7116851806640625
CurrentTrain: epoch  7, batch     1 | loss: 2.5775084Losses:  1.8510493040084839 0.4273795485496521
CurrentTrain: epoch  7, batch     2 | loss: 2.2784288Losses:  2.131425380706787 0.671481192111969
CurrentTrain: epoch  7, batch     3 | loss: 2.8029065Losses:  2.290214776992798 0.23806756734848022
CurrentTrain: epoch  7, batch     4 | loss: 2.5282824Losses:  2.4341928958892822 0.8341430425643921
CurrentTrain: epoch  7, batch     5 | loss: 3.2683358Losses:  1.931119680404663 0.07362394779920578
CurrentTrain: epoch  7, batch     6 | loss: 2.0047436Losses:  2.1360604763031006 0.6734349131584167
CurrentTrain: epoch  8, batch     0 | loss: 2.8094954Losses:  2.0519943237304688 0.3613261282444
CurrentTrain: epoch  8, batch     1 | loss: 2.4133205Losses:  1.7588953971862793 0.33100396394729614
CurrentTrain: epoch  8, batch     2 | loss: 2.0898993Losses:  2.841071128845215 0.596032977104187
CurrentTrain: epoch  8, batch     3 | loss: 3.4371042Losses:  2.0847039222717285 0.45100143551826477
CurrentTrain: epoch  8, batch     4 | loss: 2.5357053Losses:  1.8470951318740845 0.6301800608634949
CurrentTrain: epoch  8, batch     5 | loss: 2.4772751Losses:  1.9730274677276611 0.0991375744342804
CurrentTrain: epoch  8, batch     6 | loss: 2.0721650Losses:  2.656665802001953 0.4992312788963318
CurrentTrain: epoch  9, batch     0 | loss: 3.1558971Losses:  2.0752570629119873 0.31402695178985596
CurrentTrain: epoch  9, batch     1 | loss: 2.3892841Losses:  1.8390429019927979 0.36720144748687744
CurrentTrain: epoch  9, batch     2 | loss: 2.2062445Losses:  1.9779446125030518 0.5727114081382751
CurrentTrain: epoch  9, batch     3 | loss: 2.5506561Losses:  1.998066782951355 0.7206511497497559
CurrentTrain: epoch  9, batch     4 | loss: 2.7187181Losses:  1.7981756925582886 0.45041215419769287
CurrentTrain: epoch  9, batch     5 | loss: 2.2485878Losses:  1.829077959060669 0.15683382749557495
CurrentTrain: epoch  9, batch     6 | loss: 1.9859118
Losses:  0.7589440941810608 0.48148053884506226
MemoryTrain:  epoch  0, batch     0 | loss: 1.2404246Losses:  0.8433247208595276 0.39835357666015625
MemoryTrain:  epoch  0, batch     1 | loss: 1.2416782Losses:  0.4912709593772888 0.5538592338562012
MemoryTrain:  epoch  0, batch     2 | loss: 1.0451303Losses:  0.4281953275203705 0.5465701818466187
MemoryTrain:  epoch  0, batch     3 | loss: 0.9747655Losses:  0.09549446403980255 0.4579263925552368
MemoryTrain:  epoch  0, batch     4 | loss: 0.5534208Losses:  0.6954692602157593 0.5388914346694946
MemoryTrain:  epoch  0, batch     5 | loss: 1.2343607Losses:  0.5353160500526428 0.5065293312072754
MemoryTrain:  epoch  0, batch     6 | loss: 1.0418453Losses:  0.7081092596054077 0.5655950307846069
MemoryTrain:  epoch  0, batch     7 | loss: 1.2737043Losses:  0.39698004722595215 0.4184330403804779
MemoryTrain:  epoch  0, batch     8 | loss: 0.8154131Losses:  0.9151175022125244 0.41580072045326233
MemoryTrain:  epoch  1, batch     0 | loss: 1.3309182Losses:  0.9290885925292969 0.5769650340080261
MemoryTrain:  epoch  1, batch     1 | loss: 1.5060537Losses:  1.0156629085540771 0.5113614201545715
MemoryTrain:  epoch  1, batch     2 | loss: 1.5270243Losses:  0.44403818249702454 0.37894389033317566
MemoryTrain:  epoch  1, batch     3 | loss: 0.8229821Losses:  0.0949292778968811 0.4529515206813812
MemoryTrain:  epoch  1, batch     4 | loss: 0.5478808Losses:  0.7064797878265381 0.4322035312652588
MemoryTrain:  epoch  1, batch     5 | loss: 1.1386833Losses:  0.30458492040634155 0.41950714588165283
MemoryTrain:  epoch  1, batch     6 | loss: 0.7240921Losses:  0.12692773342132568 0.5236725807189941
MemoryTrain:  epoch  1, batch     7 | loss: 0.6506003Losses:  0.7258338928222656 0.5904556512832642
MemoryTrain:  epoch  1, batch     8 | loss: 1.3162895Losses:  0.08922473341226578 0.404396116733551
MemoryTrain:  epoch  2, batch     0 | loss: 0.4936208Losses:  0.3276982307434082 0.48650822043418884
MemoryTrain:  epoch  2, batch     1 | loss: 0.8142065Losses:  0.21411177515983582 0.4726816713809967
MemoryTrain:  epoch  2, batch     2 | loss: 0.6867934Losses:  0.18249942362308502 0.43772509694099426
MemoryTrain:  epoch  2, batch     3 | loss: 0.6202245Losses:  0.24605253338813782 0.4818717837333679
MemoryTrain:  epoch  2, batch     4 | loss: 0.7279243Losses:  0.14478614926338196 0.5906524658203125
MemoryTrain:  epoch  2, batch     5 | loss: 0.7354386Losses:  0.310727059841156 0.5108087658882141
MemoryTrain:  epoch  2, batch     6 | loss: 0.8215358Losses:  0.20760202407836914 0.37876570224761963
MemoryTrain:  epoch  2, batch     7 | loss: 0.5863677Losses:  0.09094204753637314 0.34466108679771423
MemoryTrain:  epoch  2, batch     8 | loss: 0.4356031Losses:  0.18239805102348328 0.362373948097229
MemoryTrain:  epoch  3, batch     0 | loss: 0.5447720Losses:  0.12771576642990112 0.37838149070739746
MemoryTrain:  epoch  3, batch     1 | loss: 0.5060973Losses:  0.07675821334123611 0.46662214398384094
MemoryTrain:  epoch  3, batch     2 | loss: 0.5433804Losses:  0.1017138808965683 0.3097819685935974
MemoryTrain:  epoch  3, batch     3 | loss: 0.4114959Losses:  0.19882497191429138 0.4518898129463196
MemoryTrain:  epoch  3, batch     4 | loss: 0.6507148Losses:  0.06628626585006714 0.5591038465499878
MemoryTrain:  epoch  3, batch     5 | loss: 0.6253901Losses:  0.0719614177942276 0.23954226076602936
MemoryTrain:  epoch  3, batch     6 | loss: 0.3115037Losses:  0.1407286524772644 0.5873121023178101
MemoryTrain:  epoch  3, batch     7 | loss: 0.7280408Losses:  0.09180926531553268 0.4462275505065918
MemoryTrain:  epoch  3, batch     8 | loss: 0.5380368Losses:  0.12329486012458801 0.33628877997398376
MemoryTrain:  epoch  4, batch     0 | loss: 0.4595836Losses:  0.06315264105796814 0.3966835141181946
MemoryTrain:  epoch  4, batch     1 | loss: 0.4598362Losses:  0.10863710939884186 0.42579352855682373
MemoryTrain:  epoch  4, batch     2 | loss: 0.5344306Losses:  0.12764428555965424 0.37700557708740234
MemoryTrain:  epoch  4, batch     3 | loss: 0.5046499Losses:  0.06062926724553108 0.5234206318855286
MemoryTrain:  epoch  4, batch     4 | loss: 0.5840499Losses:  0.08335564285516739 0.6055723428726196
MemoryTrain:  epoch  4, batch     5 | loss: 0.6889280Losses:  0.10350348800420761 0.5332297086715698
MemoryTrain:  epoch  4, batch     6 | loss: 0.6367332Losses:  0.04557373374700546 0.44677311182022095
MemoryTrain:  epoch  4, batch     7 | loss: 0.4923469Losses:  0.077214315533638 0.2194107621908188
MemoryTrain:  epoch  4, batch     8 | loss: 0.2966251Losses:  0.09752285480499268 0.45458200573921204
MemoryTrain:  epoch  5, batch     0 | loss: 0.5521048Losses:  0.05894144997000694 0.6235297918319702
MemoryTrain:  epoch  5, batch     1 | loss: 0.6824712Losses:  0.05744355916976929 0.24305489659309387
MemoryTrain:  epoch  5, batch     2 | loss: 0.3004985Losses:  0.07890525460243225 0.31704065203666687
MemoryTrain:  epoch  5, batch     3 | loss: 0.3959459Losses:  0.07336339354515076 0.5400214791297913
MemoryTrain:  epoch  5, batch     4 | loss: 0.6133848Losses:  0.030138494446873665 0.31227633357048035
MemoryTrain:  epoch  5, batch     5 | loss: 0.3424148Losses:  0.03297314792871475 0.35732758045196533
MemoryTrain:  epoch  5, batch     6 | loss: 0.3903007Losses:  0.08438459038734436 0.34594160318374634
MemoryTrain:  epoch  5, batch     7 | loss: 0.4303262Losses:  0.08072315156459808 0.257047563791275
MemoryTrain:  epoch  5, batch     8 | loss: 0.3377707Losses:  0.2688820958137512 0.5937367081642151
MemoryTrain:  epoch  6, batch     0 | loss: 0.8626188Losses:  0.08899916708469391 0.27293652296066284
MemoryTrain:  epoch  6, batch     1 | loss: 0.3619357Losses:  0.05529865249991417 0.6037904620170593
MemoryTrain:  epoch  6, batch     2 | loss: 0.6590891Losses:  0.08850270509719849 0.37178415060043335
MemoryTrain:  epoch  6, batch     3 | loss: 0.4602869Losses:  0.03384775668382645 0.2416529357433319
MemoryTrain:  epoch  6, batch     4 | loss: 0.2755007Losses:  0.08849256485700607 0.45393186807632446
MemoryTrain:  epoch  6, batch     5 | loss: 0.5424244Losses:  0.044452033936977386 0.4530757963657379
MemoryTrain:  epoch  6, batch     6 | loss: 0.4975278Losses:  0.07436944544315338 0.3308882713317871
MemoryTrain:  epoch  6, batch     7 | loss: 0.4052577Losses:  0.16648167371749878 0.26373744010925293
MemoryTrain:  epoch  6, batch     8 | loss: 0.4302191Losses:  0.18817394971847534 0.45103102922439575
MemoryTrain:  epoch  7, batch     0 | loss: 0.6392050Losses:  0.06020241230726242 0.40312081575393677
MemoryTrain:  epoch  7, batch     1 | loss: 0.4633232Losses:  0.04150541126728058 0.512706458568573
MemoryTrain:  epoch  7, batch     2 | loss: 0.5542119Losses:  0.03814153000712395 0.4541779160499573
MemoryTrain:  epoch  7, batch     3 | loss: 0.4923194Losses:  0.06275476515293121 0.3323986530303955
MemoryTrain:  epoch  7, batch     4 | loss: 0.3951534Losses:  0.051031336188316345 0.45180827379226685
MemoryTrain:  epoch  7, batch     5 | loss: 0.5028396Losses:  0.032103393226861954 0.3676118850708008
MemoryTrain:  epoch  7, batch     6 | loss: 0.3997153Losses:  0.06303726136684418 0.5801128149032593
MemoryTrain:  epoch  7, batch     7 | loss: 0.6431501Losses:  0.04040250554680824 0.335815966129303
MemoryTrain:  epoch  7, batch     8 | loss: 0.3762185Losses:  0.06313995271921158 0.35731035470962524
MemoryTrain:  epoch  8, batch     0 | loss: 0.4204503Losses:  0.06530829519033432 0.4331299662590027
MemoryTrain:  epoch  8, batch     1 | loss: 0.4984383Losses:  0.05111908167600632 0.41542068123817444
MemoryTrain:  epoch  8, batch     2 | loss: 0.4665398Losses:  0.09461107105016708 0.47548216581344604
MemoryTrain:  epoch  8, batch     3 | loss: 0.5700932Losses:  0.18011316657066345 0.2428593635559082
MemoryTrain:  epoch  8, batch     4 | loss: 0.4229725Losses:  0.05175819247961044 0.4473358690738678
MemoryTrain:  epoch  8, batch     5 | loss: 0.4990941Losses:  0.061699166893959045 0.47143319249153137
MemoryTrain:  epoch  8, batch     6 | loss: 0.5331324Losses:  0.0294631514698267 0.3709790110588074
MemoryTrain:  epoch  8, batch     7 | loss: 0.4004422Losses:  0.025341778993606567 0.3514326810836792
MemoryTrain:  epoch  8, batch     8 | loss: 0.3767745Losses:  0.03640884906053543 0.30651533603668213
MemoryTrain:  epoch  9, batch     0 | loss: 0.3429242Losses:  0.046264924108982086 0.5794997215270996
MemoryTrain:  epoch  9, batch     1 | loss: 0.6257647Losses:  0.12968333065509796 0.34906357526779175
MemoryTrain:  epoch  9, batch     2 | loss: 0.4787469Losses:  0.040104106068611145 0.35656818747520447
MemoryTrain:  epoch  9, batch     3 | loss: 0.3966723Losses:  0.07141708582639694 0.351920485496521
MemoryTrain:  epoch  9, batch     4 | loss: 0.4233376Losses:  0.026426564902067184 0.371645987033844
MemoryTrain:  epoch  9, batch     5 | loss: 0.3980725Losses:  0.0500992089509964 0.3917493522167206
MemoryTrain:  epoch  9, batch     6 | loss: 0.4418486Losses:  0.04864697903394699 0.39349526166915894
MemoryTrain:  epoch  9, batch     7 | loss: 0.4421422Losses:  0.025717562064528465 0.3014824688434601
MemoryTrain:  epoch  9, batch     8 | loss: 0.3272000
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.32%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 84.33%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.53%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 81.06%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 79.83%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 78.64%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 77.48%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 77.76%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 77.93%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 77.81%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 77.70%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.06%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.58%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 75.15%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 74.42%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 73.64%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 72.12%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 71.32%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 70.54%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 69.77%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.09%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 70.62%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 69.97%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 69.32%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 68.69%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 68.07%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 67.20%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 67.21%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 67.68%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 67.55%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 67.41%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 67.26%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 67.17%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 66.73%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 66.60%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 66.28%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 66.20%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 67.73%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 67.74%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 67.70%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 67.54%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 67.21%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 66.85%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 66.54%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 66.14%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 65.46%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 65.27%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 65.11%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 66.84%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 66.81%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 66.59%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 66.57%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 68.48%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 68.38%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 68.41%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 68.67%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 68.52%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:  248 | acc: 56.25%,  total acc: 68.37%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.44%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.39%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 68.14%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.09%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 68.05%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 68.59%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 68.55%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 68.20%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 67.94%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 67.77%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.05%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 69.19%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 68.98%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 68.83%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 68.62%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 68.20%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 68.42%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 68.22%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 68.04%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 67.85%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 67.66%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 67.55%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 67.77%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 67.55%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 67.47%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  357 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  359 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 67.36%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  365 | acc: 31.25%,  total acc: 67.18%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 67.09%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 68.03%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 68.00%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  394 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 67.99%   [EVAL] batch:  397 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 68.00%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 67.92%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 68.41%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 68.42%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.73%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 69.86%   
cur_acc:  ['0.9474', '0.6349', '0.7530', '0.7540', '0.7500', '0.6518', '0.8433']
his_acc:  ['0.9474', '0.7920', '0.7537', '0.7390', '0.7208', '0.6957', '0.6986']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 35  0  0  1  1 19  0  0  0  0  0  0  0 25
  0 34  0 23 31 36 26  0 22  0 20 12 17  0  0  0  5 29  0  0  0  2  0 37
 27  9 32 28  1  0  0 18  0 30  0 16  8 10  0  0 13 15 14  0  0  7  2  3
  0  0  6  0  0  0  4 11]
Losses:  5.956942558288574 1.0241258144378662
CurrentTrain: epoch  0, batch     0 | loss: 6.9810686Losses:  6.303668022155762 1.1416122913360596
CurrentTrain: epoch  0, batch     1 | loss: 7.4452801Losses:  6.679864883422852 1.2385485172271729
CurrentTrain: epoch  0, batch     2 | loss: 7.9184132Losses:  7.5431365966796875 1.0629262924194336
CurrentTrain: epoch  0, batch     3 | loss: 8.6060629Losses:  6.730422019958496 1.1008610725402832
CurrentTrain: epoch  0, batch     4 | loss: 7.8312831Losses:  6.828543186187744 0.8171277046203613
CurrentTrain: epoch  0, batch     5 | loss: 7.6456709Losses:  3.3215878009796143 0.3770989179611206
CurrentTrain: epoch  0, batch     6 | loss: 3.6986866Losses:  4.894375324249268 1.2645890712738037
CurrentTrain: epoch  1, batch     0 | loss: 6.1589642Losses:  5.581664085388184 0.7504090070724487
CurrentTrain: epoch  1, batch     1 | loss: 6.3320732Losses:  5.941153526306152 1.2182481288909912
CurrentTrain: epoch  1, batch     2 | loss: 7.1594019Losses:  4.630911827087402 1.0826506614685059
CurrentTrain: epoch  1, batch     3 | loss: 5.7135625Losses:  4.618351936340332 0.8323953151702881
CurrentTrain: epoch  1, batch     4 | loss: 5.4507475Losses:  5.613371849060059 1.0177068710327148
CurrentTrain: epoch  1, batch     5 | loss: 6.6310787Losses:  3.940032720565796 0.23437346518039703
CurrentTrain: epoch  1, batch     6 | loss: 4.1744061Losses:  3.5999441146850586 1.2300529479980469
CurrentTrain: epoch  2, batch     0 | loss: 4.8299971Losses:  4.386471748352051 1.0838514566421509
CurrentTrain: epoch  2, batch     1 | loss: 5.4703231Losses:  4.433791160583496 0.7582307457923889
CurrentTrain: epoch  2, batch     2 | loss: 5.1920218Losses:  4.351041793823242 0.7305666208267212
CurrentTrain: epoch  2, batch     3 | loss: 5.0816083Losses:  4.5042724609375 1.0643632411956787
CurrentTrain: epoch  2, batch     4 | loss: 5.5686359Losses:  4.3202433586120605 1.1356885433197021
CurrentTrain: epoch  2, batch     5 | loss: 5.4559317Losses:  3.972154140472412 0.5173943638801575
CurrentTrain: epoch  2, batch     6 | loss: 4.4895487Losses:  3.286388397216797 0.8945430517196655
CurrentTrain: epoch  3, batch     0 | loss: 4.1809316Losses:  4.3629961013793945 0.9823739528656006
CurrentTrain: epoch  3, batch     1 | loss: 5.3453703Losses:  3.397132396697998 0.9645565748214722
CurrentTrain: epoch  3, batch     2 | loss: 4.3616891Losses:  4.595424652099609 0.7747287750244141
CurrentTrain: epoch  3, batch     3 | loss: 5.3701534Losses:  3.6003479957580566 0.8324291706085205
CurrentTrain: epoch  3, batch     4 | loss: 4.4327774Losses:  4.5701704025268555 0.9138753414154053
CurrentTrain: epoch  3, batch     5 | loss: 5.4840460Losses:  3.2697348594665527 0.5818365812301636
CurrentTrain: epoch  3, batch     6 | loss: 3.8515716Losses:  3.850861072540283 0.8628261089324951
CurrentTrain: epoch  4, batch     0 | loss: 4.7136869Losses:  3.649827241897583 0.9388654828071594
CurrentTrain: epoch  4, batch     1 | loss: 4.5886927Losses:  4.172415733337402 0.8180187940597534
CurrentTrain: epoch  4, batch     2 | loss: 4.9904346Losses:  3.315370559692383 0.7516418099403381
CurrentTrain: epoch  4, batch     3 | loss: 4.0670123Losses:  3.191236972808838 0.9290723204612732
CurrentTrain: epoch  4, batch     4 | loss: 4.1203094Losses:  2.876232624053955 0.7498392462730408
CurrentTrain: epoch  4, batch     5 | loss: 3.6260719Losses:  2.4219884872436523 0.37278127670288086
CurrentTrain: epoch  4, batch     6 | loss: 2.7947698Losses:  3.2982592582702637 0.7112007141113281
CurrentTrain: epoch  5, batch     0 | loss: 4.0094600Losses:  2.9290895462036133 0.6132400035858154
CurrentTrain: epoch  5, batch     1 | loss: 3.5423295Losses:  3.413031578063965 0.6845802664756775
CurrentTrain: epoch  5, batch     2 | loss: 4.0976119Losses:  3.154492139816284 0.8431227803230286
CurrentTrain: epoch  5, batch     3 | loss: 3.9976149Losses:  2.7725095748901367 0.5650255084037781
CurrentTrain: epoch  5, batch     4 | loss: 3.3375351Losses:  2.8561408519744873 0.7860157489776611
CurrentTrain: epoch  5, batch     5 | loss: 3.6421566Losses:  2.9581549167633057 0.32616564631462097
CurrentTrain: epoch  5, batch     6 | loss: 3.2843206Losses:  2.839815139770508 0.5136628150939941
CurrentTrain: epoch  6, batch     0 | loss: 3.3534780Losses:  2.187302589416504 0.6389121413230896
CurrentTrain: epoch  6, batch     1 | loss: 2.8262148Losses:  2.8856163024902344 0.6265928745269775
CurrentTrain: epoch  6, batch     2 | loss: 3.5122092Losses:  3.568721294403076 0.8960332870483398
CurrentTrain: epoch  6, batch     3 | loss: 4.4647546Losses:  1.9423396587371826 0.5358449816703796
CurrentTrain: epoch  6, batch     4 | loss: 2.4781847Losses:  2.7275407314300537 0.6982621550559998
CurrentTrain: epoch  6, batch     5 | loss: 3.4258029Losses:  4.435218811035156 0.6398441195487976
CurrentTrain: epoch  6, batch     6 | loss: 5.0750628Losses:  2.161618709564209 0.4019079804420471
CurrentTrain: epoch  7, batch     0 | loss: 2.5635266Losses:  2.837571620941162 0.6732377409934998
CurrentTrain: epoch  7, batch     1 | loss: 3.5108094Losses:  2.323794364929199 0.8574398159980774
CurrentTrain: epoch  7, batch     2 | loss: 3.1812341Losses:  2.3830983638763428 0.8795446157455444
CurrentTrain: epoch  7, batch     3 | loss: 3.2626429Losses:  2.4673900604248047 0.752766489982605
CurrentTrain: epoch  7, batch     4 | loss: 3.2201567Losses:  2.2859058380126953 0.6006625890731812
CurrentTrain: epoch  7, batch     5 | loss: 2.8865685Losses:  2.1973655223846436 0.05514370650053024
CurrentTrain: epoch  7, batch     6 | loss: 2.2525091Losses:  2.4093899726867676 0.6998590230941772
CurrentTrain: epoch  8, batch     0 | loss: 3.1092491Losses:  2.597379446029663 0.5967140793800354
CurrentTrain: epoch  8, batch     1 | loss: 3.1940935Losses:  2.0004701614379883 0.42499518394470215
CurrentTrain: epoch  8, batch     2 | loss: 2.4254653Losses:  2.300107717514038 0.8621394634246826
CurrentTrain: epoch  8, batch     3 | loss: 3.1622472Losses:  1.9459850788116455 0.36015695333480835
CurrentTrain: epoch  8, batch     4 | loss: 2.3061421Losses:  2.577072858810425 0.4613614082336426
CurrentTrain: epoch  8, batch     5 | loss: 3.0384343Losses:  1.8050044775009155 0.020193133503198624
CurrentTrain: epoch  8, batch     6 | loss: 1.8251976Losses:  2.2421164512634277 0.6172119975090027
CurrentTrain: epoch  9, batch     0 | loss: 2.8593285Losses:  2.2228052616119385 0.4863949120044708
CurrentTrain: epoch  9, batch     1 | loss: 2.7092001Losses:  2.2433600425720215 0.5111135840415955
CurrentTrain: epoch  9, batch     2 | loss: 2.7544737Losses:  2.300673007965088 0.5184586048126221
CurrentTrain: epoch  9, batch     3 | loss: 2.8191316Losses:  1.88441801071167 0.36809828877449036
CurrentTrain: epoch  9, batch     4 | loss: 2.2525163Losses:  2.509096145629883 0.7993314266204834
CurrentTrain: epoch  9, batch     5 | loss: 3.3084276Losses:  1.9500420093536377 0.11114846169948578
CurrentTrain: epoch  9, batch     6 | loss: 2.0611904
Losses:  0.6806350946426392 0.3946636915206909
MemoryTrain:  epoch  0, batch     0 | loss: 1.0752988Losses:  0.3259078860282898 0.481407105922699
MemoryTrain:  epoch  0, batch     1 | loss: 0.8073150Losses:  0.6897562742233276 0.4564284086227417
MemoryTrain:  epoch  0, batch     2 | loss: 1.1461847Losses:  0.44165948033332825 0.474381685256958
MemoryTrain:  epoch  0, batch     3 | loss: 0.9160411Losses:  0.552651047706604 0.38709673285484314
MemoryTrain:  epoch  0, batch     4 | loss: 0.9397478Losses:  0.21224021911621094 0.4914082884788513
MemoryTrain:  epoch  0, batch     5 | loss: 0.7036485Losses:  0.3076499402523041 0.45881426334381104
MemoryTrain:  epoch  0, batch     6 | loss: 0.7664642Losses:  0.25000840425491333 0.48803746700286865
MemoryTrain:  epoch  0, batch     7 | loss: 0.7380459Losses:  0.6940252184867859 0.6007664203643799
MemoryTrain:  epoch  0, batch     8 | loss: 1.2947917Losses:  0.94254469871521 0.32463809847831726
MemoryTrain:  epoch  0, batch     9 | loss: 1.2671828Losses:  0.5376958847045898 0.45794135332107544
MemoryTrain:  epoch  1, batch     0 | loss: 0.9956372Losses:  0.5212100744247437 0.38816356658935547
MemoryTrain:  epoch  1, batch     1 | loss: 0.9093736Losses:  0.11036768555641174 0.3649994730949402
MemoryTrain:  epoch  1, batch     2 | loss: 0.4753672Losses:  0.80311518907547 0.48373621702194214
MemoryTrain:  epoch  1, batch     3 | loss: 1.2868514Losses:  0.27488741278648376 0.2994050681591034
MemoryTrain:  epoch  1, batch     4 | loss: 0.5742925Losses:  1.123674750328064 0.5122804641723633
MemoryTrain:  epoch  1, batch     5 | loss: 1.6359552Losses:  0.9925730228424072 0.4214489161968231
MemoryTrain:  epoch  1, batch     6 | loss: 1.4140220Losses:  0.18385633826255798 0.38791194558143616
MemoryTrain:  epoch  1, batch     7 | loss: 0.5717683Losses:  0.8760402798652649 0.3798705041408539
MemoryTrain:  epoch  1, batch     8 | loss: 1.2559108Losses:  0.5740172863006592 0.6982754468917847
MemoryTrain:  epoch  1, batch     9 | loss: 1.2722927Losses:  0.7054272890090942 0.3686044216156006
MemoryTrain:  epoch  2, batch     0 | loss: 1.0740317Losses:  0.39618799090385437 0.5669738054275513
MemoryTrain:  epoch  2, batch     1 | loss: 0.9631618Losses:  0.2065289467573166 0.3998974561691284
MemoryTrain:  epoch  2, batch     2 | loss: 0.6064264Losses:  0.3560755252838135 0.4350697100162506
MemoryTrain:  epoch  2, batch     3 | loss: 0.7911452Losses:  0.46110787987709045 0.35444164276123047
MemoryTrain:  epoch  2, batch     4 | loss: 0.8155495Losses:  0.2605118751525879 0.4212269186973572
MemoryTrain:  epoch  2, batch     5 | loss: 0.6817388Losses:  0.08942224830389023 0.6326239705085754
MemoryTrain:  epoch  2, batch     6 | loss: 0.7220462Losses:  0.33851858973503113 0.43111860752105713
MemoryTrain:  epoch  2, batch     7 | loss: 0.7696372Losses:  0.32808661460876465 0.42348766326904297
MemoryTrain:  epoch  2, batch     8 | loss: 0.7515743Losses:  0.04501493275165558 0.3966584801673889
MemoryTrain:  epoch  2, batch     9 | loss: 0.4416734Losses:  0.7128738164901733 0.29367008805274963
MemoryTrain:  epoch  3, batch     0 | loss: 1.0065439Losses:  0.06121096760034561 0.29097744822502136
MemoryTrain:  epoch  3, batch     1 | loss: 0.3521884Losses:  0.2419784814119339 0.42715930938720703
MemoryTrain:  epoch  3, batch     2 | loss: 0.6691378Losses:  0.06654776632785797 0.4853109121322632
MemoryTrain:  epoch  3, batch     3 | loss: 0.5518587Losses:  0.08625472337007523 0.4525740146636963
MemoryTrain:  epoch  3, batch     4 | loss: 0.5388287Losses:  0.3366207778453827 0.48696017265319824
MemoryTrain:  epoch  3, batch     5 | loss: 0.8235810Losses:  0.18367405235767365 0.47066575288772583
MemoryTrain:  epoch  3, batch     6 | loss: 0.6543398Losses:  0.22591428458690643 0.33025920391082764
MemoryTrain:  epoch  3, batch     7 | loss: 0.5561735Losses:  0.2194162905216217 0.29212164878845215
MemoryTrain:  epoch  3, batch     8 | loss: 0.5115379Losses:  0.04301237314939499 0.4475855231285095
MemoryTrain:  epoch  3, batch     9 | loss: 0.4905979Losses:  0.11940187215805054 0.44737544655799866
MemoryTrain:  epoch  4, batch     0 | loss: 0.5667773Losses:  0.2338457554578781 0.4617370069026947
MemoryTrain:  epoch  4, batch     1 | loss: 0.6955827Losses:  0.245887890458107 0.38050389289855957
MemoryTrain:  epoch  4, batch     2 | loss: 0.6263918Losses:  0.08420778065919876 0.5601977109909058
MemoryTrain:  epoch  4, batch     3 | loss: 0.6444055Losses:  0.04104889929294586 0.4128381609916687
MemoryTrain:  epoch  4, batch     4 | loss: 0.4538870Losses:  0.2395489513874054 0.4504860043525696
MemoryTrain:  epoch  4, batch     5 | loss: 0.6900350Losses:  0.11364057660102844 0.4757048785686493
MemoryTrain:  epoch  4, batch     6 | loss: 0.5893455Losses:  0.04429002106189728 0.41288816928863525
MemoryTrain:  epoch  4, batch     7 | loss: 0.4571782Losses:  0.05897346884012222 0.3733302056789398
MemoryTrain:  epoch  4, batch     8 | loss: 0.4323037Losses:  0.14442843198776245 0.3081095516681671
MemoryTrain:  epoch  4, batch     9 | loss: 0.4525380Losses:  0.07980050146579742 0.3345770239830017
MemoryTrain:  epoch  5, batch     0 | loss: 0.4143775Losses:  0.0829787477850914 0.38497182726860046
MemoryTrain:  epoch  5, batch     1 | loss: 0.4679506Losses:  0.27961966395378113 0.4424586892127991
MemoryTrain:  epoch  5, batch     2 | loss: 0.7220783Losses:  0.057168006896972656 0.44942983984947205
MemoryTrain:  epoch  5, batch     3 | loss: 0.5065979Losses:  0.05299249291419983 0.44504842162132263
MemoryTrain:  epoch  5, batch     4 | loss: 0.4980409Losses:  0.09321194887161255 0.45316022634506226
MemoryTrain:  epoch  5, batch     5 | loss: 0.5463722Losses:  0.14284566044807434 0.346668541431427
MemoryTrain:  epoch  5, batch     6 | loss: 0.4895142Losses:  0.08012726902961731 0.39274531602859497
MemoryTrain:  epoch  5, batch     7 | loss: 0.4728726Losses:  0.02675245888531208 0.44928568601608276
MemoryTrain:  epoch  5, batch     8 | loss: 0.4760382Losses:  0.12495829910039902 0.392230749130249
MemoryTrain:  epoch  5, batch     9 | loss: 0.5171890Losses:  0.07320217788219452 0.38711151480674744
MemoryTrain:  epoch  6, batch     0 | loss: 0.4603137Losses:  0.03523855656385422 0.2732894718647003
MemoryTrain:  epoch  6, batch     1 | loss: 0.3085280Losses:  0.10020443797111511 0.48025405406951904
MemoryTrain:  epoch  6, batch     2 | loss: 0.5804585Losses:  0.17193984985351562 0.3871912360191345
MemoryTrain:  epoch  6, batch     3 | loss: 0.5591311Losses:  0.06374849379062653 0.2799813747406006
MemoryTrain:  epoch  6, batch     4 | loss: 0.3437299Losses:  0.05170827358961105 0.6597509384155273
MemoryTrain:  epoch  6, batch     5 | loss: 0.7114592Losses:  0.04545538127422333 0.43020710349082947
MemoryTrain:  epoch  6, batch     6 | loss: 0.4756625Losses:  0.14679667353630066 0.447574257850647
MemoryTrain:  epoch  6, batch     7 | loss: 0.5943710Losses:  0.06799086183309555 0.43631213903427124
MemoryTrain:  epoch  6, batch     8 | loss: 0.5043030Losses:  0.02959585003554821 0.33717578649520874
MemoryTrain:  epoch  6, batch     9 | loss: 0.3667716Losses:  0.06941905617713928 0.3283722698688507
MemoryTrain:  epoch  7, batch     0 | loss: 0.3977913Losses:  0.057793885469436646 0.727372407913208
MemoryTrain:  epoch  7, batch     1 | loss: 0.7851663Losses:  0.04976328834891319 0.5100871324539185
MemoryTrain:  epoch  7, batch     2 | loss: 0.5598504Losses:  0.038997337222099304 0.3568441867828369
MemoryTrain:  epoch  7, batch     3 | loss: 0.3958415Losses:  0.10047146677970886 0.344605028629303
MemoryTrain:  epoch  7, batch     4 | loss: 0.4450765Losses:  0.08638274669647217 0.4392756223678589
MemoryTrain:  epoch  7, batch     5 | loss: 0.5256584Losses:  0.06376076489686966 0.3838846981525421
MemoryTrain:  epoch  7, batch     6 | loss: 0.4476455Losses:  0.08090640604496002 0.39121976494789124
MemoryTrain:  epoch  7, batch     7 | loss: 0.4721262Losses:  0.03267835080623627 0.3026171028614044
MemoryTrain:  epoch  7, batch     8 | loss: 0.3352954Losses:  0.037854887545108795 0.3208131790161133
MemoryTrain:  epoch  7, batch     9 | loss: 0.3586681Losses:  0.07586149871349335 0.21012181043624878
MemoryTrain:  epoch  8, batch     0 | loss: 0.2859833Losses:  0.06570959091186523 0.5428531169891357
MemoryTrain:  epoch  8, batch     1 | loss: 0.6085627Losses:  0.050872303545475006 0.4271530210971832
MemoryTrain:  epoch  8, batch     2 | loss: 0.4780253Losses:  0.02418217808008194 0.249044269323349
MemoryTrain:  epoch  8, batch     3 | loss: 0.2732264Losses:  0.041091181337833405 0.5696497559547424
MemoryTrain:  epoch  8, batch     4 | loss: 0.6107410Losses:  0.06037328392267227 0.4023786187171936
MemoryTrain:  epoch  8, batch     5 | loss: 0.4627519Losses:  0.029476948082447052 0.3204268515110016
MemoryTrain:  epoch  8, batch     6 | loss: 0.3499038Losses:  0.07126198709011078 0.5216835737228394
MemoryTrain:  epoch  8, batch     7 | loss: 0.5929456Losses:  0.026782691478729248 0.27424299716949463
MemoryTrain:  epoch  8, batch     8 | loss: 0.3010257Losses:  0.05413774400949478 0.46237993240356445
MemoryTrain:  epoch  8, batch     9 | loss: 0.5165177Losses:  0.05641108751296997 0.46690458059310913
MemoryTrain:  epoch  9, batch     0 | loss: 0.5233157Losses:  0.0665004700422287 0.4787543714046478
MemoryTrain:  epoch  9, batch     1 | loss: 0.5452548Losses:  0.042091794312000275 0.4359146058559418
MemoryTrain:  epoch  9, batch     2 | loss: 0.4780064Losses:  0.04005761072039604 0.4412617087364197
MemoryTrain:  epoch  9, batch     3 | loss: 0.4813193Losses:  0.01696532778441906 0.28576433658599854
MemoryTrain:  epoch  9, batch     4 | loss: 0.3027297Losses:  0.049420326948165894 0.41198912262916565
MemoryTrain:  epoch  9, batch     5 | loss: 0.4614094Losses:  0.027388636022806168 0.2574513554573059
MemoryTrain:  epoch  9, batch     6 | loss: 0.2848400Losses:  0.041053928434848785 0.406150758266449
MemoryTrain:  epoch  9, batch     7 | loss: 0.4472047Losses:  0.03610768914222717 0.25131314992904663
MemoryTrain:  epoch  9, batch     8 | loss: 0.2874208Losses:  0.03609669953584671 0.4839761555194855
MemoryTrain:  epoch  9, batch     9 | loss: 0.5200729
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 75.24%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 74.55%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.23%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 73.12%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.15%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 85.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 84.21%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 82.08%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.75%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 79.49%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 77.18%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 76.03%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 74.91%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 74.28%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 75.08%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 75.08%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 74.92%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 74.24%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 72.43%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 71.19%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 70.60%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 69.80%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 69.03%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 68.27%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 66.87%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 68.69%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 68.06%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 67.49%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 66.88%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 65.49%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 66.26%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 66.01%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 65.70%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 65.11%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 64.89%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 64.53%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 64.74%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 65.56%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 65.60%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 65.46%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 65.24%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 65.02%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 64.80%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 64.52%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 64.30%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 64.19%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 64.00%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 63.74%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 63.48%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 63.25%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 63.04%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 65.03%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 64.63%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 64.56%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 64.52%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.57%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 64.59%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 64.73%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.78%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 65.04%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 64.77%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 66.63%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 66.73%   [EVAL] batch:  245 | acc: 0.00%,  total acc: 66.46%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 65.76%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 65.58%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.93%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 65.79%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 65.70%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 66.21%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 66.01%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 65.81%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 65.42%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.23%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.00%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 64.95%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 65.23%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.06%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 64.97%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 65.92%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 65.98%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 65.83%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 65.63%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 65.24%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 65.25%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 65.72%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 65.33%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 64.81%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 65.12%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 65.02%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 64.85%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 64.82%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 64.72%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 64.63%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 64.47%   [EVAL] batch:  359 | acc: 25.00%,  total acc: 64.36%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 64.25%   [EVAL] batch:  361 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 64.05%   [EVAL] batch:  363 | acc: 18.75%,  total acc: 63.93%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 63.84%   [EVAL] batch:  365 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 63.76%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 64.59%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 64.75%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 64.72%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 64.70%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 64.67%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  396 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 64.62%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 66.70%   [EVAL] batch:  440 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  441 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  443 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  450 | acc: 12.50%,  total acc: 66.78%   [EVAL] batch:  451 | acc: 12.50%,  total acc: 66.66%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 66.54%   [EVAL] batch:  453 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  454 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:  455 | acc: 18.75%,  total acc: 66.21%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  466 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  468 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  475 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:  476 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  479 | acc: 56.25%,  total acc: 67.30%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  485 | acc: 87.50%,  total acc: 67.57%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  488 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  489 | acc: 68.75%,  total acc: 67.72%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 67.72%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 67.67%   [EVAL] batch:  494 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 67.67%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 67.67%   
cur_acc:  ['0.9474', '0.6349', '0.7530', '0.7540', '0.7500', '0.6518', '0.8433', '0.7312']
his_acc:  ['0.9474', '0.7920', '0.7537', '0.7390', '0.7208', '0.6957', '0.6986', '0.6767']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.122406959533691 1.4018974304199219
CurrentTrain: epoch  0, batch     0 | loss: 10.5243044Losses:  10.919297218322754 1.9167296886444092
CurrentTrain: epoch  0, batch     1 | loss: 12.8360271Losses:  9.459390640258789 1.8173635005950928
CurrentTrain: epoch  0, batch     2 | loss: 11.2767544Losses:  9.566590309143066 1.682509183883667
CurrentTrain: epoch  0, batch     3 | loss: 11.2490997Losses:  9.80642318725586 1.717570185661316
CurrentTrain: epoch  0, batch     4 | loss: 11.5239935Losses:  9.253011703491211 1.1833851337432861
CurrentTrain: epoch  0, batch     5 | loss: 10.4363966Losses:  11.180103302001953 1.4232451915740967
CurrentTrain: epoch  0, batch     6 | loss: 12.6033487Losses:  9.886088371276855 1.2676031589508057
CurrentTrain: epoch  0, batch     7 | loss: 11.1536913Losses:  8.76937484741211 1.4103612899780273
CurrentTrain: epoch  0, batch     8 | loss: 10.1797361Losses:  9.611242294311523 1.4345693588256836
CurrentTrain: epoch  0, batch     9 | loss: 11.0458117Losses:  8.32087516784668 1.0415949821472168
CurrentTrain: epoch  0, batch    10 | loss: 9.3624706Losses:  9.366846084594727 1.596855878829956
CurrentTrain: epoch  0, batch    11 | loss: 10.9637022Losses:  8.299787521362305 1.513437271118164
CurrentTrain: epoch  0, batch    12 | loss: 9.8132248Losses:  10.139568328857422 1.4532232284545898
CurrentTrain: epoch  0, batch    13 | loss: 11.5927916Losses:  8.030622482299805 1.0958704948425293
CurrentTrain: epoch  0, batch    14 | loss: 9.1264935Losses:  9.079214096069336 1.4760754108428955
CurrentTrain: epoch  0, batch    15 | loss: 10.5552893Losses:  7.874109268188477 0.7325652837753296
CurrentTrain: epoch  0, batch    16 | loss: 8.6066742Losses:  8.420731544494629 1.3361454010009766
CurrentTrain: epoch  0, batch    17 | loss: 9.7568769Losses:  9.169084548950195 0.7363736629486084
CurrentTrain: epoch  0, batch    18 | loss: 9.9054585Losses:  8.922523498535156 1.6360688209533691
CurrentTrain: epoch  0, batch    19 | loss: 10.5585918Losses:  8.772923469543457 0.9998352527618408
CurrentTrain: epoch  0, batch    20 | loss: 9.7727585Losses:  7.991171836853027 1.1299619674682617
CurrentTrain: epoch  0, batch    21 | loss: 9.1211338Losses:  8.450324058532715 1.4858006238937378
CurrentTrain: epoch  0, batch    22 | loss: 9.9361248Losses:  8.545108795166016 1.0895495414733887
CurrentTrain: epoch  0, batch    23 | loss: 9.6346588Losses:  7.981956958770752 1.4458203315734863
CurrentTrain: epoch  0, batch    24 | loss: 9.4277773Losses:  8.304219245910645 1.1356154680252075
CurrentTrain: epoch  0, batch    25 | loss: 9.4398346Losses:  8.348550796508789 1.0613231658935547
CurrentTrain: epoch  0, batch    26 | loss: 9.4098740Losses:  8.871277809143066 1.2736458778381348
CurrentTrain: epoch  0, batch    27 | loss: 10.1449242Losses:  10.135062217712402 0.6321583986282349
CurrentTrain: epoch  0, batch    28 | loss: 10.7672205Losses:  8.397235870361328 0.926972508430481
CurrentTrain: epoch  0, batch    29 | loss: 9.3242083Losses:  8.575155258178711 1.279833197593689
CurrentTrain: epoch  0, batch    30 | loss: 9.8549881Losses:  7.402975082397461 0.8065390586853027
CurrentTrain: epoch  0, batch    31 | loss: 8.2095146Losses:  7.903616905212402 0.970935583114624
CurrentTrain: epoch  0, batch    32 | loss: 8.8745527Losses:  8.199503898620605 1.255514144897461
CurrentTrain: epoch  0, batch    33 | loss: 9.4550180Losses:  8.930639266967773 1.262830138206482
CurrentTrain: epoch  0, batch    34 | loss: 10.1934690Losses:  7.82602596282959 1.2563601732254028
CurrentTrain: epoch  0, batch    35 | loss: 9.0823860Losses:  8.00964641571045 1.1993218660354614
CurrentTrain: epoch  0, batch    36 | loss: 9.2089682Losses:  8.3934326171875 1.1749939918518066
CurrentTrain: epoch  0, batch    37 | loss: 9.5684261Losses:  8.697813034057617 1.4666935205459595
CurrentTrain: epoch  0, batch    38 | loss: 10.1645069Losses:  8.600318908691406 0.8169814348220825
CurrentTrain: epoch  0, batch    39 | loss: 9.4173002Losses:  7.84950590133667 0.945007860660553
CurrentTrain: epoch  0, batch    40 | loss: 8.7945137Losses:  8.29885482788086 1.1935162544250488
CurrentTrain: epoch  0, batch    41 | loss: 9.4923706Losses:  8.02333927154541 0.8764548301696777
CurrentTrain: epoch  0, batch    42 | loss: 8.8997936Losses:  8.92698860168457 0.9740660190582275
CurrentTrain: epoch  0, batch    43 | loss: 9.9010544Losses:  8.44783878326416 1.103320598602295
CurrentTrain: epoch  0, batch    44 | loss: 9.5511589Losses:  8.139476776123047 1.0889979600906372
CurrentTrain: epoch  0, batch    45 | loss: 9.2284746Losses:  8.34329891204834 0.8440525531768799
CurrentTrain: epoch  0, batch    46 | loss: 9.1873512Losses:  8.34473991394043 1.236261248588562
CurrentTrain: epoch  0, batch    47 | loss: 9.5810013Losses:  7.639007568359375 0.9911078214645386
CurrentTrain: epoch  0, batch    48 | loss: 8.6301155Losses:  8.446660995483398 0.9703969955444336
CurrentTrain: epoch  0, batch    49 | loss: 9.4170580Losses:  9.014778137207031 0.9758540987968445
CurrentTrain: epoch  0, batch    50 | loss: 9.9906321Losses:  8.972722053527832 0.950350284576416
CurrentTrain: epoch  0, batch    51 | loss: 9.9230728Losses:  8.473365783691406 1.1150598526000977
CurrentTrain: epoch  0, batch    52 | loss: 9.5884256Losses:  7.504218101501465 1.1206414699554443
CurrentTrain: epoch  0, batch    53 | loss: 8.6248598Losses:  8.576641082763672 0.780351996421814
CurrentTrain: epoch  0, batch    54 | loss: 9.3569927Losses:  8.204145431518555 0.7346099615097046
CurrentTrain: epoch  0, batch    55 | loss: 8.9387550Losses:  8.5911226272583 0.9459414482116699
CurrentTrain: epoch  0, batch    56 | loss: 9.5370636Losses:  7.903048515319824 1.2529284954071045
CurrentTrain: epoch  0, batch    57 | loss: 9.1559772Losses:  7.498170852661133 1.2425178289413452
CurrentTrain: epoch  0, batch    58 | loss: 8.7406883Losses:  7.031320571899414 0.8315834403038025
CurrentTrain: epoch  0, batch    59 | loss: 7.8629041Losses:  8.719161987304688 0.9318201541900635
CurrentTrain: epoch  0, batch    60 | loss: 9.6509819Losses:  8.129671096801758 1.045931339263916
CurrentTrain: epoch  0, batch    61 | loss: 9.1756020Losses:  6.940930366516113 0.4151797890663147
CurrentTrain: epoch  0, batch    62 | loss: 7.3561101Losses:  7.57056999206543 0.8783748745918274
CurrentTrain: epoch  0, batch    63 | loss: 8.4489450Losses:  7.503049850463867 0.979203999042511
CurrentTrain: epoch  0, batch    64 | loss: 8.4822540Losses:  6.716729640960693 0.5906193256378174
CurrentTrain: epoch  0, batch    65 | loss: 7.3073492Losses:  7.373073577880859 0.927308976650238
CurrentTrain: epoch  0, batch    66 | loss: 8.3003826Losses:  6.913068771362305 0.4936855435371399
CurrentTrain: epoch  0, batch    67 | loss: 7.4067545Losses:  8.265989303588867 0.8504366278648376
CurrentTrain: epoch  0, batch    68 | loss: 9.1164255Losses:  7.206820011138916 0.9370375871658325
CurrentTrain: epoch  0, batch    69 | loss: 8.1438580Losses:  7.6087446212768555 0.8636513352394104
CurrentTrain: epoch  0, batch    70 | loss: 8.4723959Losses:  6.797825813293457 0.5684855580329895
CurrentTrain: epoch  0, batch    71 | loss: 7.3663116Losses:  7.420679092407227 0.7279281616210938
CurrentTrain: epoch  0, batch    72 | loss: 8.1486073Losses:  7.42790412902832 1.0319254398345947
CurrentTrain: epoch  0, batch    73 | loss: 8.4598293Losses:  6.811985015869141 0.6566176414489746
CurrentTrain: epoch  0, batch    74 | loss: 7.4686027Losses:  8.440674781799316 0.9926026463508606
CurrentTrain: epoch  0, batch    75 | loss: 9.4332771Losses:  7.412475109100342 0.8292717933654785
CurrentTrain: epoch  0, batch    76 | loss: 8.2417469Losses:  6.776525020599365 0.5875076055526733
CurrentTrain: epoch  0, batch    77 | loss: 7.3640327Losses:  9.165046691894531 1.101280927658081
CurrentTrain: epoch  0, batch    78 | loss: 10.2663279Losses:  6.986309051513672 0.6409282088279724
CurrentTrain: epoch  0, batch    79 | loss: 7.6272373Losses:  7.079896450042725 0.6524950861930847
CurrentTrain: epoch  0, batch    80 | loss: 7.7323914Losses:  8.064386367797852 0.5312944650650024
CurrentTrain: epoch  0, batch    81 | loss: 8.5956812Losses:  7.456057071685791 0.9324325323104858
CurrentTrain: epoch  0, batch    82 | loss: 8.3884897Losses:  6.99395751953125 0.7106707096099854
CurrentTrain: epoch  0, batch    83 | loss: 7.7046280Losses:  10.088228225708008 0.9716446995735168
CurrentTrain: epoch  0, batch    84 | loss: 11.0598726Losses:  6.563848972320557 0.900621235370636
CurrentTrain: epoch  0, batch    85 | loss: 7.4644704Losses:  7.345791816711426 0.8668267130851746
CurrentTrain: epoch  0, batch    86 | loss: 8.2126188Losses:  7.501793384552002 0.9401510953903198
CurrentTrain: epoch  0, batch    87 | loss: 8.4419441Losses:  8.741222381591797 0.8939652442932129
CurrentTrain: epoch  0, batch    88 | loss: 9.6351871Losses:  7.655117034912109 0.5746777057647705
CurrentTrain: epoch  0, batch    89 | loss: 8.2297945Losses:  7.415879726409912 0.8626021146774292
CurrentTrain: epoch  0, batch    90 | loss: 8.2784815Losses:  8.491780281066895 0.7984686493873596
CurrentTrain: epoch  0, batch    91 | loss: 9.2902489Losses:  6.792823791503906 0.6608447432518005
CurrentTrain: epoch  0, batch    92 | loss: 7.4536686Losses:  6.87662410736084 0.6659272909164429
CurrentTrain: epoch  0, batch    93 | loss: 7.5425515Losses:  8.636592864990234 0.8917971253395081
CurrentTrain: epoch  0, batch    94 | loss: 9.5283899Losses:  7.123558044433594 0.78126060962677
CurrentTrain: epoch  0, batch    95 | loss: 7.9048185Losses:  7.4371771812438965 0.5768426656723022
CurrentTrain: epoch  0, batch    96 | loss: 8.0140200Losses:  8.62723159790039 0.8960410356521606
CurrentTrain: epoch  0, batch    97 | loss: 9.5232725Losses:  6.430747032165527 0.8981162309646606
CurrentTrain: epoch  0, batch    98 | loss: 7.3288631Losses:  8.611370086669922 0.8853369951248169
CurrentTrain: epoch  0, batch    99 | loss: 9.4967070Losses:  6.404250144958496 0.5976999998092651
CurrentTrain: epoch  0, batch   100 | loss: 7.0019503Losses:  7.421938896179199 0.781394362449646
CurrentTrain: epoch  0, batch   101 | loss: 8.2033329Losses:  7.842292785644531 0.7516319751739502
CurrentTrain: epoch  0, batch   102 | loss: 8.5939245Losses:  6.930741310119629 0.8598107099533081
CurrentTrain: epoch  0, batch   103 | loss: 7.7905521Losses:  6.452939987182617 0.6973785161972046
CurrentTrain: epoch  0, batch   104 | loss: 7.1503186Losses:  7.401930809020996 0.7637053728103638
CurrentTrain: epoch  0, batch   105 | loss: 8.1656361Losses:  6.3480448722839355 0.6657066941261292
CurrentTrain: epoch  0, batch   106 | loss: 7.0137515Losses:  6.629644393920898 0.57301926612854
CurrentTrain: epoch  0, batch   107 | loss: 7.2026634Losses:  7.809591293334961 0.6387861371040344
CurrentTrain: epoch  0, batch   108 | loss: 8.4483776Losses:  6.752954959869385 0.7644618153572083
CurrentTrain: epoch  0, batch   109 | loss: 7.5174170Losses:  7.418151378631592 0.8281911611557007
CurrentTrain: epoch  0, batch   110 | loss: 8.2463427Losses:  6.938570976257324 0.611947774887085
CurrentTrain: epoch  0, batch   111 | loss: 7.5505190Losses:  7.631277084350586 1.0521160364151
CurrentTrain: epoch  0, batch   112 | loss: 8.6833935Losses:  6.235945701599121 0.7334321141242981
CurrentTrain: epoch  0, batch   113 | loss: 6.9693780Losses:  5.869903564453125 0.7919902801513672
CurrentTrain: epoch  0, batch   114 | loss: 6.6618938Losses:  6.977479934692383 0.7087894678115845
CurrentTrain: epoch  0, batch   115 | loss: 7.6862693Losses:  7.8936567306518555 0.965979814529419
CurrentTrain: epoch  0, batch   116 | loss: 8.8596363Losses:  6.327606201171875 0.7342026233673096
CurrentTrain: epoch  0, batch   117 | loss: 7.0618086Losses:  6.466766357421875 0.6748430728912354
CurrentTrain: epoch  0, batch   118 | loss: 7.1416092Losses:  6.766685962677002 0.8312383890151978
CurrentTrain: epoch  0, batch   119 | loss: 7.5979242Losses:  6.168926239013672 0.8787068128585815
CurrentTrain: epoch  0, batch   120 | loss: 7.0476332Losses:  5.878113269805908 0.5726379156112671
CurrentTrain: epoch  0, batch   121 | loss: 6.4507513Losses:  6.982584476470947 0.6339247226715088
CurrentTrain: epoch  0, batch   122 | loss: 7.6165094Losses:  6.499975204467773 0.6956061124801636
CurrentTrain: epoch  0, batch   123 | loss: 7.1955814Losses:  5.78269100189209 0.6130316257476807
CurrentTrain: epoch  0, batch   124 | loss: 6.3957224Losses:  7.862580299377441 0.7838408946990967
CurrentTrain: epoch  1, batch     0 | loss: 8.6464214Losses:  7.328550815582275 0.4800509810447693
CurrentTrain: epoch  1, batch     1 | loss: 7.8086019Losses:  6.5547075271606445 0.6262856125831604
CurrentTrain: epoch  1, batch     2 | loss: 7.1809931Losses:  6.654353618621826 0.6240463256835938
CurrentTrain: epoch  1, batch     3 | loss: 7.2783999Losses:  6.704982757568359 0.5752013325691223
CurrentTrain: epoch  1, batch     4 | loss: 7.2801843Losses:  5.899684429168701 0.813438355922699
CurrentTrain: epoch  1, batch     5 | loss: 6.7131228Losses:  7.628442764282227 0.7022799849510193
CurrentTrain: epoch  1, batch     6 | loss: 8.3307228Losses:  6.340425491333008 0.666133463382721
CurrentTrain: epoch  1, batch     7 | loss: 7.0065589Losses:  6.014483451843262 0.49784550070762634
CurrentTrain: epoch  1, batch     8 | loss: 6.5123291Losses:  5.300814628601074 0.5491135716438293
CurrentTrain: epoch  1, batch     9 | loss: 5.8499284Losses:  6.157317161560059 0.5355035066604614
CurrentTrain: epoch  1, batch    10 | loss: 6.6928205Losses:  5.841651916503906 0.8340939283370972
CurrentTrain: epoch  1, batch    11 | loss: 6.6757460Losses:  7.121413707733154 0.7183709740638733
CurrentTrain: epoch  1, batch    12 | loss: 7.8397846Losses:  6.068197250366211 0.6104283332824707
CurrentTrain: epoch  1, batch    13 | loss: 6.6786256Losses:  6.252159118652344 0.5387254953384399
CurrentTrain: epoch  1, batch    14 | loss: 6.7908845Losses:  6.885492324829102 0.6675379276275635
CurrentTrain: epoch  1, batch    15 | loss: 7.5530300Losses:  6.919437408447266 0.8565415143966675
CurrentTrain: epoch  1, batch    16 | loss: 7.7759790Losses:  6.456514358520508 0.4046958088874817
CurrentTrain: epoch  1, batch    17 | loss: 6.8612103Losses:  5.874913692474365 0.6481695175170898
CurrentTrain: epoch  1, batch    18 | loss: 6.5230832Losses:  6.569112300872803 0.7551935911178589
CurrentTrain: epoch  1, batch    19 | loss: 7.3243060Losses:  5.341814041137695 0.614372193813324
CurrentTrain: epoch  1, batch    20 | loss: 5.9561863Losses:  6.435477256774902 0.4779335856437683
CurrentTrain: epoch  1, batch    21 | loss: 6.9134107Losses:  6.873376369476318 0.7348452806472778
CurrentTrain: epoch  1, batch    22 | loss: 7.6082215Losses:  6.586827278137207 0.783363938331604
CurrentTrain: epoch  1, batch    23 | loss: 7.3701911Losses:  6.238125324249268 0.5433700084686279
CurrentTrain: epoch  1, batch    24 | loss: 6.7814951Losses:  7.362277507781982 0.5240575075149536
CurrentTrain: epoch  1, batch    25 | loss: 7.8863349Losses:  5.56762170791626 0.5560215711593628
CurrentTrain: epoch  1, batch    26 | loss: 6.1236434Losses:  5.812142372131348 0.6274933815002441
CurrentTrain: epoch  1, batch    27 | loss: 6.4396358Losses:  6.181615829467773 0.5740857124328613
CurrentTrain: epoch  1, batch    28 | loss: 6.7557015Losses:  6.394609451293945 0.45903444290161133
CurrentTrain: epoch  1, batch    29 | loss: 6.8536439Losses:  6.136846542358398 0.6437814235687256
CurrentTrain: epoch  1, batch    30 | loss: 6.7806282Losses:  5.538016319274902 0.45496004819869995
CurrentTrain: epoch  1, batch    31 | loss: 5.9929762Losses:  8.006366729736328 0.5418965220451355
CurrentTrain: epoch  1, batch    32 | loss: 8.5482635Losses:  7.119384765625 0.6212271451950073
CurrentTrain: epoch  1, batch    33 | loss: 7.7406120Losses:  6.290106773376465 0.5389057397842407
CurrentTrain: epoch  1, batch    34 | loss: 6.8290124Losses:  5.722363471984863 0.37824681401252747
CurrentTrain: epoch  1, batch    35 | loss: 6.1006103Losses:  7.5561203956604 0.5182156562805176
CurrentTrain: epoch  1, batch    36 | loss: 8.0743361Losses:  5.758991241455078 0.24127840995788574
CurrentTrain: epoch  1, batch    37 | loss: 6.0002699Losses:  7.09478759765625 0.4952559471130371
CurrentTrain: epoch  1, batch    38 | loss: 7.5900435Losses:  8.495071411132812 0.537253737449646
CurrentTrain: epoch  1, batch    39 | loss: 9.0323248Losses:  6.02403450012207 0.4376993179321289
CurrentTrain: epoch  1, batch    40 | loss: 6.4617338Losses:  6.54952335357666 0.2992649972438812
CurrentTrain: epoch  1, batch    41 | loss: 6.8487883Losses:  5.040684700012207 0.4130724370479584
CurrentTrain: epoch  1, batch    42 | loss: 5.4537573Losses:  5.696606636047363 0.508118748664856
CurrentTrain: epoch  1, batch    43 | loss: 6.2047253Losses:  6.3104448318481445 0.4163517355918884
CurrentTrain: epoch  1, batch    44 | loss: 6.7267966Losses:  5.020774841308594 0.4013952612876892
CurrentTrain: epoch  1, batch    45 | loss: 5.4221702Losses:  5.566977500915527 0.6151749491691589
CurrentTrain: epoch  1, batch    46 | loss: 6.1821523Losses:  5.356692314147949 0.3409043550491333
CurrentTrain: epoch  1, batch    47 | loss: 5.6975965Losses:  5.840364933013916 0.49000996351242065
CurrentTrain: epoch  1, batch    48 | loss: 6.3303747Losses:  6.066176414489746 0.7116334438323975
CurrentTrain: epoch  1, batch    49 | loss: 6.7778101Losses:  5.929295539855957 0.6073279976844788
CurrentTrain: epoch  1, batch    50 | loss: 6.5366235Losses:  5.847275733947754 0.3387518525123596
CurrentTrain: epoch  1, batch    51 | loss: 6.1860275Losses:  6.594864368438721 0.7336859703063965
CurrentTrain: epoch  1, batch    52 | loss: 7.3285503Losses:  7.245915412902832 0.5578261613845825
CurrentTrain: epoch  1, batch    53 | loss: 7.8037415Losses:  5.254376411437988 0.6733105778694153
CurrentTrain: epoch  1, batch    54 | loss: 5.9276872Losses:  6.562566757202148 0.42834174633026123
CurrentTrain: epoch  1, batch    55 | loss: 6.9909086Losses:  5.871637344360352 0.6160599589347839
CurrentTrain: epoch  1, batch    56 | loss: 6.4876971Losses:  5.933602333068848 0.44921034574508667
CurrentTrain: epoch  1, batch    57 | loss: 6.3828125Losses:  6.901769638061523 0.7624775767326355
CurrentTrain: epoch  1, batch    58 | loss: 7.6642470Losses:  6.995455265045166 0.8421851992607117
CurrentTrain: epoch  1, batch    59 | loss: 7.8376403Losses:  5.84058141708374 0.18159955739974976
CurrentTrain: epoch  1, batch    60 | loss: 6.0221810Losses:  5.357174873352051 0.4781769812107086
CurrentTrain: epoch  1, batch    61 | loss: 5.8353519Losses:  6.331055641174316 0.4169677495956421
CurrentTrain: epoch  1, batch    62 | loss: 6.7480235Losses:  7.056434631347656 0.630500316619873
CurrentTrain: epoch  1, batch    63 | loss: 7.6869349Losses:  5.946386337280273 0.4892508387565613
CurrentTrain: epoch  1, batch    64 | loss: 6.4356370Losses:  5.9519758224487305 0.6677310466766357
CurrentTrain: epoch  1, batch    65 | loss: 6.6197071Losses:  7.049327850341797 0.6551160216331482
CurrentTrain: epoch  1, batch    66 | loss: 7.7044439Losses:  6.8191680908203125 0.331406831741333
CurrentTrain: epoch  1, batch    67 | loss: 7.1505747Losses:  5.667072296142578 0.5265979766845703
CurrentTrain: epoch  1, batch    68 | loss: 6.1936703Losses:  6.352471351623535 0.4049561619758606
CurrentTrain: epoch  1, batch    69 | loss: 6.7574277Losses:  5.831915378570557 0.23658426105976105
CurrentTrain: epoch  1, batch    70 | loss: 6.0684996Losses:  5.915423393249512 0.3929225206375122
CurrentTrain: epoch  1, batch    71 | loss: 6.3083458Losses:  5.215267181396484 0.5067222118377686
CurrentTrain: epoch  1, batch    72 | loss: 5.7219896Losses:  5.864933013916016 0.5090066194534302
CurrentTrain: epoch  1, batch    73 | loss: 6.3739395Losses:  7.473577499389648 0.7213191986083984
CurrentTrain: epoch  1, batch    74 | loss: 8.1948967Losses:  5.896432876586914 0.4445117115974426
CurrentTrain: epoch  1, batch    75 | loss: 6.3409448Losses:  6.15839958190918 0.47470712661743164
CurrentTrain: epoch  1, batch    76 | loss: 6.6331067Losses:  5.676338195800781 0.5207080245018005
CurrentTrain: epoch  1, batch    77 | loss: 6.1970463Losses:  6.691135406494141 0.6421864628791809
CurrentTrain: epoch  1, batch    78 | loss: 7.3333220Losses:  6.473761558532715 0.5062713623046875
CurrentTrain: epoch  1, batch    79 | loss: 6.9800329Losses:  5.486495018005371 0.4554165005683899
CurrentTrain: epoch  1, batch    80 | loss: 5.9419117Losses:  5.907773971557617 0.4238085150718689
CurrentTrain: epoch  1, batch    81 | loss: 6.3315825Losses:  5.641075134277344 0.554822564125061
CurrentTrain: epoch  1, batch    82 | loss: 6.1958976Losses:  7.8311238288879395 0.4564126133918762
CurrentTrain: epoch  1, batch    83 | loss: 8.2875366Losses:  7.1833953857421875 0.71109938621521
CurrentTrain: epoch  1, batch    84 | loss: 7.8944950Losses:  6.292961597442627 0.40567535161972046
CurrentTrain: epoch  1, batch    85 | loss: 6.6986370Losses:  5.933942794799805 0.5292597413063049
CurrentTrain: epoch  1, batch    86 | loss: 6.4632025Losses:  6.706091403961182 0.4363301396369934
CurrentTrain: epoch  1, batch    87 | loss: 7.1424217Losses:  6.12845516204834 0.43151846528053284
CurrentTrain: epoch  1, batch    88 | loss: 6.5599737Losses:  5.16136360168457 0.27648621797561646
CurrentTrain: epoch  1, batch    89 | loss: 5.4378500Losses:  7.591062068939209 0.6509018540382385
CurrentTrain: epoch  1, batch    90 | loss: 8.2419643Losses:  6.318264961242676 0.5626616477966309
CurrentTrain: epoch  1, batch    91 | loss: 6.8809266Losses:  5.689445972442627 0.4429996907711029
CurrentTrain: epoch  1, batch    92 | loss: 6.1324458Losses:  4.7627177238464355 0.3058273196220398
CurrentTrain: epoch  1, batch    93 | loss: 5.0685449Losses:  5.521759986877441 0.2693496346473694
CurrentTrain: epoch  1, batch    94 | loss: 5.7911096Losses:  6.381315231323242 0.43736758828163147
CurrentTrain: epoch  1, batch    95 | loss: 6.8186827Losses:  6.762183666229248 0.5432902574539185
CurrentTrain: epoch  1, batch    96 | loss: 7.3054738Losses:  5.382354736328125 0.43886598944664
CurrentTrain: epoch  1, batch    97 | loss: 5.8212209Losses:  5.585407733917236 0.28213122487068176
CurrentTrain: epoch  1, batch    98 | loss: 5.8675389Losses:  5.173643112182617 0.3037565350532532
CurrentTrain: epoch  1, batch    99 | loss: 5.4773998Losses:  5.612029075622559 0.4772229790687561
CurrentTrain: epoch  1, batch   100 | loss: 6.0892520Losses:  4.896696090698242 0.38695621490478516
CurrentTrain: epoch  1, batch   101 | loss: 5.2836523Losses:  7.618795394897461 0.5952291488647461
CurrentTrain: epoch  1, batch   102 | loss: 8.2140245Losses:  5.9409894943237305 0.44304585456848145
CurrentTrain: epoch  1, batch   103 | loss: 6.3840351Losses:  4.9626688957214355 0.3663427233695984
CurrentTrain: epoch  1, batch   104 | loss: 5.3290114Losses:  5.69871711730957 0.5138198137283325
CurrentTrain: epoch  1, batch   105 | loss: 6.2125368Losses:  4.818086624145508 0.38171902298927307
CurrentTrain: epoch  1, batch   106 | loss: 5.1998057Losses:  5.312294960021973 0.4241058826446533
CurrentTrain: epoch  1, batch   107 | loss: 5.7364006Losses:  5.730320930480957 0.5018428564071655
CurrentTrain: epoch  1, batch   108 | loss: 6.2321639Losses:  5.32098913192749 0.49058687686920166
CurrentTrain: epoch  1, batch   109 | loss: 5.8115759Losses:  5.166696071624756 0.45729315280914307
CurrentTrain: epoch  1, batch   110 | loss: 5.6239891Losses:  5.841564178466797 0.5274035334587097
CurrentTrain: epoch  1, batch   111 | loss: 6.3689675Losses:  5.706661224365234 0.42661869525909424
CurrentTrain: epoch  1, batch   112 | loss: 6.1332798Losses:  5.197749614715576 0.27260270714759827
CurrentTrain: epoch  1, batch   113 | loss: 5.4703522Losses:  5.327610015869141 0.2506486177444458
CurrentTrain: epoch  1, batch   114 | loss: 5.5782585Losses:  4.996652603149414 0.4672688841819763
CurrentTrain: epoch  1, batch   115 | loss: 5.4639215Losses:  4.919780731201172 0.35634249448776245
CurrentTrain: epoch  1, batch   116 | loss: 5.2761230Losses:  6.4220781326293945 0.5278608798980713
CurrentTrain: epoch  1, batch   117 | loss: 6.9499388Losses:  6.329808235168457 0.6175934672355652
CurrentTrain: epoch  1, batch   118 | loss: 6.9474015Losses:  4.940439701080322 0.3677648603916168
CurrentTrain: epoch  1, batch   119 | loss: 5.3082047Losses:  4.894277572631836 0.3142295777797699
CurrentTrain: epoch  1, batch   120 | loss: 5.2085071Losses:  6.923789978027344 0.5262569785118103
CurrentTrain: epoch  1, batch   121 | loss: 7.4500470Losses:  5.716294288635254 0.3571937084197998
CurrentTrain: epoch  1, batch   122 | loss: 6.0734882Losses:  5.253152370452881 0.4524157643318176
CurrentTrain: epoch  1, batch   123 | loss: 5.7055683Losses:  4.858362197875977 0.38643649220466614
CurrentTrain: epoch  1, batch   124 | loss: 5.2447987Losses:  5.736450672149658 0.3287680149078369
CurrentTrain: epoch  2, batch     0 | loss: 6.0652189Losses:  5.010391712188721 0.4400860667228699
CurrentTrain: epoch  2, batch     1 | loss: 5.4504776Losses:  5.605072021484375 0.4781785309314728
CurrentTrain: epoch  2, batch     2 | loss: 6.0832505Losses:  5.235278606414795 0.2457510232925415
CurrentTrain: epoch  2, batch     3 | loss: 5.4810295Losses:  5.72666072845459 0.29641133546829224
CurrentTrain: epoch  2, batch     4 | loss: 6.0230722Losses:  5.90962553024292 0.46894943714141846
CurrentTrain: epoch  2, batch     5 | loss: 6.3785748Losses:  5.226097106933594 0.35996222496032715
CurrentTrain: epoch  2, batch     6 | loss: 5.5860596Losses:  5.0581889152526855 0.2042199671268463
CurrentTrain: epoch  2, batch     7 | loss: 5.2624087Losses:  4.354717254638672 0.22597602009773254
CurrentTrain: epoch  2, batch     8 | loss: 4.5806932Losses:  6.162858963012695 0.2948238253593445
CurrentTrain: epoch  2, batch     9 | loss: 6.4576826Losses:  5.101067543029785 0.38039109110832214
CurrentTrain: epoch  2, batch    10 | loss: 5.4814587Losses:  4.787715911865234 0.41956788301467896
CurrentTrain: epoch  2, batch    11 | loss: 5.2072840Losses:  4.890987873077393 0.2625555098056793
CurrentTrain: epoch  2, batch    12 | loss: 5.1535435Losses:  5.088622570037842 0.41047704219818115
CurrentTrain: epoch  2, batch    13 | loss: 5.4990997Losses:  5.624619483947754 0.42133447527885437
CurrentTrain: epoch  2, batch    14 | loss: 6.0459538Losses:  5.24170446395874 0.38708072900772095
CurrentTrain: epoch  2, batch    15 | loss: 5.6287851Losses:  5.523004531860352 0.36052846908569336
CurrentTrain: epoch  2, batch    16 | loss: 5.8835330Losses:  5.075077056884766 0.5179020166397095
CurrentTrain: epoch  2, batch    17 | loss: 5.5929790Losses:  5.577207088470459 0.2490214705467224
CurrentTrain: epoch  2, batch    18 | loss: 5.8262286Losses:  4.514775276184082 0.20935259759426117
CurrentTrain: epoch  2, batch    19 | loss: 4.7241278Losses:  5.072434425354004 0.26660391688346863
CurrentTrain: epoch  2, batch    20 | loss: 5.3390384Losses:  4.877904891967773 0.3854947090148926
CurrentTrain: epoch  2, batch    21 | loss: 5.2633996Losses:  4.937871932983398 0.22929644584655762
CurrentTrain: epoch  2, batch    22 | loss: 5.1671686Losses:  6.507010459899902 0.6182404160499573
CurrentTrain: epoch  2, batch    23 | loss: 7.1252508Losses:  5.019915580749512 0.2204374372959137
CurrentTrain: epoch  2, batch    24 | loss: 5.2403531Losses:  4.970294952392578 0.3480270802974701
CurrentTrain: epoch  2, batch    25 | loss: 5.3183222Losses:  4.820731163024902 0.2644142508506775
CurrentTrain: epoch  2, batch    26 | loss: 5.0851455Losses:  4.771055698394775 0.4370502531528473
CurrentTrain: epoch  2, batch    27 | loss: 5.2081060Losses:  5.058765888214111 0.22159895300865173
CurrentTrain: epoch  2, batch    28 | loss: 5.2803650Losses:  5.347211837768555 0.35902848839759827
CurrentTrain: epoch  2, batch    29 | loss: 5.7062402Losses:  5.03835916519165 0.3122606873512268
CurrentTrain: epoch  2, batch    30 | loss: 5.3506198Losses:  6.05745792388916 0.37633559107780457
CurrentTrain: epoch  2, batch    31 | loss: 6.4337935Losses:  5.125086784362793 0.2347712516784668
CurrentTrain: epoch  2, batch    32 | loss: 5.3598580Losses:  6.105762481689453 0.26604974269866943
CurrentTrain: epoch  2, batch    33 | loss: 6.3718123Losses:  6.702645301818848 0.6116825342178345
CurrentTrain: epoch  2, batch    34 | loss: 7.3143277Losses:  4.754119396209717 0.3213452696800232
CurrentTrain: epoch  2, batch    35 | loss: 5.0754647Losses:  5.177237510681152 0.38986772298812866
CurrentTrain: epoch  2, batch    36 | loss: 5.5671053Losses:  4.890329360961914 0.2652485966682434
CurrentTrain: epoch  2, batch    37 | loss: 5.1555781Losses:  4.643685340881348 0.3799234628677368
CurrentTrain: epoch  2, batch    38 | loss: 5.0236087Losses:  4.6248931884765625 0.2863440215587616
CurrentTrain: epoch  2, batch    39 | loss: 4.9112372Losses:  4.928920745849609 0.20863495767116547
CurrentTrain: epoch  2, batch    40 | loss: 5.1375556Losses:  5.989380836486816 0.2640165090560913
CurrentTrain: epoch  2, batch    41 | loss: 6.2533975Losses:  5.265326499938965 0.46347343921661377
CurrentTrain: epoch  2, batch    42 | loss: 5.7287998Losses:  5.503403663635254 0.31566476821899414
CurrentTrain: epoch  2, batch    43 | loss: 5.8190684Losses:  5.067770481109619 0.2350579798221588
CurrentTrain: epoch  2, batch    44 | loss: 5.3028283Losses:  5.014883995056152 0.2468193769454956
CurrentTrain: epoch  2, batch    45 | loss: 5.2617035Losses:  5.377593040466309 0.3113090395927429
CurrentTrain: epoch  2, batch    46 | loss: 5.6889019Losses:  5.308258533477783 0.3026602268218994
CurrentTrain: epoch  2, batch    47 | loss: 5.6109190Losses:  5.489473819732666 0.4580935537815094
CurrentTrain: epoch  2, batch    48 | loss: 5.9475675Losses:  4.713316917419434 0.2309078872203827
CurrentTrain: epoch  2, batch    49 | loss: 4.9442248Losses:  5.491470813751221 0.5344017148017883
CurrentTrain: epoch  2, batch    50 | loss: 6.0258727Losses:  4.777091979980469 0.3752933442592621
CurrentTrain: epoch  2, batch    51 | loss: 5.1523852Losses:  4.895907402038574 0.4306311011314392
CurrentTrain: epoch  2, batch    52 | loss: 5.3265386Losses:  5.221925735473633 0.3414181172847748
CurrentTrain: epoch  2, batch    53 | loss: 5.5633440Losses:  5.022194862365723 0.15522795915603638
CurrentTrain: epoch  2, batch    54 | loss: 5.1774230Losses:  4.8612165451049805 0.3260157108306885
CurrentTrain: epoch  2, batch    55 | loss: 5.1872320Losses:  5.116698265075684 0.2849588394165039
CurrentTrain: epoch  2, batch    56 | loss: 5.4016571Losses:  5.298664093017578 0.32914137840270996
CurrentTrain: epoch  2, batch    57 | loss: 5.6278057Losses:  5.214607238769531 0.28080734610557556
CurrentTrain: epoch  2, batch    58 | loss: 5.4954147Losses:  5.1999969482421875 0.42986956238746643
CurrentTrain: epoch  2, batch    59 | loss: 5.6298666Losses:  4.786129474639893 0.28959083557128906
CurrentTrain: epoch  2, batch    60 | loss: 5.0757203Losses:  4.82522439956665 0.31567126512527466
CurrentTrain: epoch  2, batch    61 | loss: 5.1408958Losses:  5.193815231323242 0.37407156825065613
CurrentTrain: epoch  2, batch    62 | loss: 5.5678868Losses:  4.456315994262695 0.0902281180024147
CurrentTrain: epoch  2, batch    63 | loss: 4.5465441Losses:  4.669693946838379 0.24296468496322632
CurrentTrain: epoch  2, batch    64 | loss: 4.9126587Losses:  4.482631683349609 0.24461513757705688
CurrentTrain: epoch  2, batch    65 | loss: 4.7272468Losses:  5.864543914794922 0.22023122012615204
CurrentTrain: epoch  2, batch    66 | loss: 6.0847750Losses:  4.913158893585205 0.3286932408809662
CurrentTrain: epoch  2, batch    67 | loss: 5.2418523Losses:  4.8303141593933105 0.3167766034603119
CurrentTrain: epoch  2, batch    68 | loss: 5.1470909Losses:  4.960968971252441 0.41374659538269043
CurrentTrain: epoch  2, batch    69 | loss: 5.3747158Losses:  4.9758992195129395 0.43022966384887695
CurrentTrain: epoch  2, batch    70 | loss: 5.4061289Losses:  5.104011535644531 0.4020463228225708
CurrentTrain: epoch  2, batch    71 | loss: 5.5060577Losses:  5.100641250610352 0.22114774584770203
CurrentTrain: epoch  2, batch    72 | loss: 5.3217888Losses:  5.017895221710205 0.3127177655696869
CurrentTrain: epoch  2, batch    73 | loss: 5.3306131Losses:  4.730526924133301 0.301493763923645
CurrentTrain: epoch  2, batch    74 | loss: 5.0320206Losses:  5.099957466125488 0.3621506690979004
CurrentTrain: epoch  2, batch    75 | loss: 5.4621081Losses:  4.946739196777344 0.3826051354408264
CurrentTrain: epoch  2, batch    76 | loss: 5.3293443Losses:  4.5693464279174805 0.1932150423526764
CurrentTrain: epoch  2, batch    77 | loss: 4.7625613Losses:  4.785408973693848 0.305439293384552
CurrentTrain: epoch  2, batch    78 | loss: 5.0908484Losses:  4.826467514038086 0.25939372181892395
CurrentTrain: epoch  2, batch    79 | loss: 5.0858612Losses:  4.409783363342285 0.368709921836853
CurrentTrain: epoch  2, batch    80 | loss: 4.7784934Losses:  4.7157487869262695 0.21605658531188965
CurrentTrain: epoch  2, batch    81 | loss: 4.9318056Losses:  4.670684814453125 0.41043227910995483
CurrentTrain: epoch  2, batch    82 | loss: 5.0811172Losses:  4.791670799255371 0.26291656494140625
CurrentTrain: epoch  2, batch    83 | loss: 5.0545874Losses:  4.832547187805176 0.2429383397102356
CurrentTrain: epoch  2, batch    84 | loss: 5.0754857Losses:  4.696706771850586 0.31740880012512207
CurrentTrain: epoch  2, batch    85 | loss: 5.0141153Losses:  4.547314643859863 0.3085353672504425
CurrentTrain: epoch  2, batch    86 | loss: 4.8558502Losses:  5.039555072784424 0.3196752071380615
CurrentTrain: epoch  2, batch    87 | loss: 5.3592300Losses:  5.750110149383545 0.44611656665802
CurrentTrain: epoch  2, batch    88 | loss: 6.1962266Losses:  5.403952121734619 0.3726218342781067
CurrentTrain: epoch  2, batch    89 | loss: 5.7765741Losses:  5.19001579284668 0.18260937929153442
CurrentTrain: epoch  2, batch    90 | loss: 5.3726254Losses:  4.755527496337891 0.43813082575798035
CurrentTrain: epoch  2, batch    91 | loss: 5.1936584Losses:  4.889817714691162 0.2880122661590576
CurrentTrain: epoch  2, batch    92 | loss: 5.1778297Losses:  4.747747421264648 0.26774370670318604
CurrentTrain: epoch  2, batch    93 | loss: 5.0154910Losses:  4.5234174728393555 0.23642602562904358
CurrentTrain: epoch  2, batch    94 | loss: 4.7598433Losses:  5.519397735595703 0.2599962055683136
CurrentTrain: epoch  2, batch    95 | loss: 5.7793941Losses:  5.09365701675415 0.3200700283050537
CurrentTrain: epoch  2, batch    96 | loss: 5.4137268Losses:  4.541110038757324 0.38595086336135864
CurrentTrain: epoch  2, batch    97 | loss: 4.9270611Losses:  5.891124725341797 0.26233822107315063
CurrentTrain: epoch  2, batch    98 | loss: 6.1534629Losses:  4.940013408660889 0.23239338397979736
CurrentTrain: epoch  2, batch    99 | loss: 5.1724067Losses:  4.698246002197266 0.23001700639724731
CurrentTrain: epoch  2, batch   100 | loss: 4.9282632Losses:  5.747131824493408 0.6240704655647278
CurrentTrain: epoch  2, batch   101 | loss: 6.3712025Losses:  4.554376602172852 0.13672608137130737
CurrentTrain: epoch  2, batch   102 | loss: 4.6911025Losses:  4.537450790405273 0.25790104269981384
CurrentTrain: epoch  2, batch   103 | loss: 4.7953520Losses:  4.454588413238525 0.15838360786437988
CurrentTrain: epoch  2, batch   104 | loss: 4.6129723Losses:  4.956906795501709 0.18085864186286926
CurrentTrain: epoch  2, batch   105 | loss: 5.1377654Losses:  4.508269309997559 0.2986189126968384
CurrentTrain: epoch  2, batch   106 | loss: 4.8068881Losses:  4.646516799926758 0.4574483036994934
CurrentTrain: epoch  2, batch   107 | loss: 5.1039653Losses:  4.753615856170654 0.3435036838054657
CurrentTrain: epoch  2, batch   108 | loss: 5.0971193Losses:  4.826119899749756 0.3277539610862732
CurrentTrain: epoch  2, batch   109 | loss: 5.1538739Losses:  4.430387020111084 0.3869468569755554
CurrentTrain: epoch  2, batch   110 | loss: 4.8173337Losses:  4.906976699829102 0.2664468586444855
CurrentTrain: epoch  2, batch   111 | loss: 5.1734238Losses:  4.903463363647461 0.302229642868042
CurrentTrain: epoch  2, batch   112 | loss: 5.2056932Losses:  4.555222034454346 0.20150849223136902
CurrentTrain: epoch  2, batch   113 | loss: 4.7567306Losses:  4.612516403198242 0.3679370582103729
CurrentTrain: epoch  2, batch   114 | loss: 4.9804535Losses:  4.8464555740356445 0.3148180842399597
CurrentTrain: epoch  2, batch   115 | loss: 5.1612735Losses:  4.67866849899292 0.28557461500167847
CurrentTrain: epoch  2, batch   116 | loss: 4.9642429Losses:  4.727509498596191 0.2667941451072693
CurrentTrain: epoch  2, batch   117 | loss: 4.9943037Losses:  5.519171714782715 0.18605449795722961
CurrentTrain: epoch  2, batch   118 | loss: 5.7052264Losses:  4.4457807540893555 0.26802265644073486
CurrentTrain: epoch  2, batch   119 | loss: 4.7138033Losses:  4.527153015136719 0.3472171425819397
CurrentTrain: epoch  2, batch   120 | loss: 4.8743701Losses:  4.884699821472168 0.2542928457260132
CurrentTrain: epoch  2, batch   121 | loss: 5.1389928Losses:  4.378868103027344 0.09891746938228607
CurrentTrain: epoch  2, batch   122 | loss: 4.4777856Losses:  5.012681007385254 0.29966506361961365
CurrentTrain: epoch  2, batch   123 | loss: 5.3123460Losses:  4.341549396514893 0.18279311060905457
CurrentTrain: epoch  2, batch   124 | loss: 4.5243425Losses:  4.533055305480957 0.2908404469490051
CurrentTrain: epoch  3, batch     0 | loss: 4.8238959Losses:  4.438065528869629 0.28807809948921204
CurrentTrain: epoch  3, batch     1 | loss: 4.7261438Losses:  4.665879249572754 0.19220732152462006
CurrentTrain: epoch  3, batch     2 | loss: 4.8580866Losses:  4.307998180389404 0.1339356005191803
CurrentTrain: epoch  3, batch     3 | loss: 4.4419336Losses:  4.551496505737305 0.2364393025636673
CurrentTrain: epoch  3, batch     4 | loss: 4.7879357Losses:  4.797606468200684 0.29412001371383667
CurrentTrain: epoch  3, batch     5 | loss: 5.0917263Losses:  4.684782028198242 0.2983206808567047
CurrentTrain: epoch  3, batch     6 | loss: 4.9831028Losses:  4.704132080078125 0.32623934745788574
CurrentTrain: epoch  3, batch     7 | loss: 5.0303717Losses:  4.412599563598633 0.22809621691703796
CurrentTrain: epoch  3, batch     8 | loss: 4.6406956Losses:  4.522189140319824 0.22493097186088562
CurrentTrain: epoch  3, batch     9 | loss: 4.7471199Losses:  4.65479850769043 0.14362579584121704
CurrentTrain: epoch  3, batch    10 | loss: 4.7984242Losses:  4.503026962280273 0.1594666987657547
CurrentTrain: epoch  3, batch    11 | loss: 4.6624937Losses:  4.662128448486328 0.2387906312942505
CurrentTrain: epoch  3, batch    12 | loss: 4.9009190Losses:  4.556858062744141 0.16296958923339844
CurrentTrain: epoch  3, batch    13 | loss: 4.7198277Losses:  4.522141933441162 0.26652851700782776
CurrentTrain: epoch  3, batch    14 | loss: 4.7886705Losses:  4.661046028137207 0.2700830101966858
CurrentTrain: epoch  3, batch    15 | loss: 4.9311290Losses:  4.521600723266602 0.1713555008172989
CurrentTrain: epoch  3, batch    16 | loss: 4.6929564Losses:  4.489465713500977 0.300503134727478
CurrentTrain: epoch  3, batch    17 | loss: 4.7899690Losses:  4.694748401641846 0.30932092666625977
CurrentTrain: epoch  3, batch    18 | loss: 5.0040693Losses:  4.322290420532227 0.09150228649377823
CurrentTrain: epoch  3, batch    19 | loss: 4.4137926Losses:  4.178182125091553 0.22364845871925354
CurrentTrain: epoch  3, batch    20 | loss: 4.4018307Losses:  4.502918243408203 0.23012763261795044
CurrentTrain: epoch  3, batch    21 | loss: 4.7330461Losses:  4.310991287231445 0.24148951470851898
CurrentTrain: epoch  3, batch    22 | loss: 4.5524807Losses:  4.454142093658447 0.31520259380340576
CurrentTrain: epoch  3, batch    23 | loss: 4.7693448Losses:  4.389589309692383 0.15164226293563843
CurrentTrain: epoch  3, batch    24 | loss: 4.5412316Losses:  4.902268409729004 0.2817089259624481
CurrentTrain: epoch  3, batch    25 | loss: 5.1839771Losses:  4.469841957092285 0.21846601366996765
CurrentTrain: epoch  3, batch    26 | loss: 4.6883078Losses:  4.356540679931641 0.32538771629333496
CurrentTrain: epoch  3, batch    27 | loss: 4.6819286Losses:  4.461721420288086 0.18034303188323975
CurrentTrain: epoch  3, batch    28 | loss: 4.6420646Losses:  4.922760963439941 0.31044068932533264
CurrentTrain: epoch  3, batch    29 | loss: 5.2332015Losses:  4.580748558044434 0.2863645553588867
CurrentTrain: epoch  3, batch    30 | loss: 4.8671131Losses:  4.48563289642334 0.15174254775047302
CurrentTrain: epoch  3, batch    31 | loss: 4.6373754Losses:  5.026315689086914 0.2060047835111618
CurrentTrain: epoch  3, batch    32 | loss: 5.2323203Losses:  4.816565990447998 0.19727298617362976
CurrentTrain: epoch  3, batch    33 | loss: 5.0138388Losses:  4.743618488311768 0.13021403551101685
CurrentTrain: epoch  3, batch    34 | loss: 4.8738327Losses:  4.997442245483398 0.3360196053981781
CurrentTrain: epoch  3, batch    35 | loss: 5.3334618Losses:  4.56122350692749 0.27629756927490234
CurrentTrain: epoch  3, batch    36 | loss: 4.8375211Losses:  4.2496747970581055 0.1685800850391388
CurrentTrain: epoch  3, batch    37 | loss: 4.4182549Losses:  4.612921714782715 0.21900475025177002
CurrentTrain: epoch  3, batch    38 | loss: 4.8319263Losses:  4.36305570602417 0.27342551946640015
CurrentTrain: epoch  3, batch    39 | loss: 4.6364813Losses:  4.40623140335083 0.232624351978302
CurrentTrain: epoch  3, batch    40 | loss: 4.6388559Losses:  4.6257123947143555 0.23358950018882751
CurrentTrain: epoch  3, batch    41 | loss: 4.8593020Losses:  4.378233432769775 0.1768028438091278
CurrentTrain: epoch  3, batch    42 | loss: 4.5550361Losses:  4.352774620056152 0.25346195697784424
CurrentTrain: epoch  3, batch    43 | loss: 4.6062365Losses:  4.606135845184326 0.2519776225090027
CurrentTrain: epoch  3, batch    44 | loss: 4.8581133Losses:  4.6849470138549805 0.24749113619327545
CurrentTrain: epoch  3, batch    45 | loss: 4.9324384Losses:  4.237948417663574 0.11020077764987946
CurrentTrain: epoch  3, batch    46 | loss: 4.3481493Losses:  4.456654071807861 0.2124369740486145
CurrentTrain: epoch  3, batch    47 | loss: 4.6690912Losses:  4.5133256912231445 0.13894538581371307
CurrentTrain: epoch  3, batch    48 | loss: 4.6522713Losses:  4.452336311340332 0.19533227384090424
CurrentTrain: epoch  3, batch    49 | loss: 4.6476684Losses:  4.4119110107421875 0.19414615631103516
CurrentTrain: epoch  3, batch    50 | loss: 4.6060572Losses:  4.535524368286133 0.2718488276004791
CurrentTrain: epoch  3, batch    51 | loss: 4.8073730Losses:  4.556435585021973 0.22092199325561523
CurrentTrain: epoch  3, batch    52 | loss: 4.7773576Losses:  4.749703884124756 0.17242135107517242
CurrentTrain: epoch  3, batch    53 | loss: 4.9221253Losses:  4.591521739959717 0.1315896213054657
CurrentTrain: epoch  3, batch    54 | loss: 4.7231112Losses:  5.436264514923096 0.26203274726867676
CurrentTrain: epoch  3, batch    55 | loss: 5.6982975Losses:  4.463315963745117 0.32632097601890564
CurrentTrain: epoch  3, batch    56 | loss: 4.7896371Losses:  4.3685760498046875 0.24967530369758606
CurrentTrain: epoch  3, batch    57 | loss: 4.6182513Losses:  4.328853607177734 0.1424640268087387
CurrentTrain: epoch  3, batch    58 | loss: 4.4713178Losses:  4.424129962921143 0.19394205510616302
CurrentTrain: epoch  3, batch    59 | loss: 4.6180720Losses:  4.180789947509766 0.13480763137340546
CurrentTrain: epoch  3, batch    60 | loss: 4.3155975Losses:  4.491274833679199 0.12832853198051453
CurrentTrain: epoch  3, batch    61 | loss: 4.6196032Losses:  4.28495979309082 0.15813781321048737
CurrentTrain: epoch  3, batch    62 | loss: 4.4430976Losses:  4.563335418701172 0.21286669373512268
CurrentTrain: epoch  3, batch    63 | loss: 4.7762022Losses:  4.281574249267578 0.297440767288208
CurrentTrain: epoch  3, batch    64 | loss: 4.5790148Losses:  4.746420860290527 0.1651824563741684
CurrentTrain: epoch  3, batch    65 | loss: 4.9116035Losses:  4.333548069000244 0.3418702483177185
CurrentTrain: epoch  3, batch    66 | loss: 4.6754184Losses:  4.428653717041016 0.24111169576644897
CurrentTrain: epoch  3, batch    67 | loss: 4.6697655Losses:  4.2304301261901855 0.2881634831428528
CurrentTrain: epoch  3, batch    68 | loss: 4.5185938Losses:  4.504889488220215 0.2165246307849884
CurrentTrain: epoch  3, batch    69 | loss: 4.7214141Losses:  5.005125045776367 0.28204846382141113
CurrentTrain: epoch  3, batch    70 | loss: 5.2871733Losses:  4.401440620422363 0.1392979770898819
CurrentTrain: epoch  3, batch    71 | loss: 4.5407386Losses:  4.843890190124512 0.3054700195789337
CurrentTrain: epoch  3, batch    72 | loss: 5.1493602Losses:  4.285510540008545 0.29025939106941223
CurrentTrain: epoch  3, batch    73 | loss: 4.5757699Losses:  4.193240165710449 0.17623761296272278
CurrentTrain: epoch  3, batch    74 | loss: 4.3694777Losses:  4.4311137199401855 0.28814342617988586
CurrentTrain: epoch  3, batch    75 | loss: 4.7192574Losses:  4.139653205871582 0.1302948296070099
CurrentTrain: epoch  3, batch    76 | loss: 4.2699480Losses:  6.448908805847168 0.49243655800819397
CurrentTrain: epoch  3, batch    77 | loss: 6.9413452Losses:  5.947112083435059 0.33164656162261963
CurrentTrain: epoch  3, batch    78 | loss: 6.2787585Losses:  5.0145792961120605 0.21775704622268677
CurrentTrain: epoch  3, batch    79 | loss: 5.2323365Losses:  4.6716203689575195 0.12056301534175873
CurrentTrain: epoch  3, batch    80 | loss: 4.7921834Losses:  4.350872039794922 0.17909842729568481
CurrentTrain: epoch  3, batch    81 | loss: 4.5299706Losses:  5.076874732971191 0.20071525871753693
CurrentTrain: epoch  3, batch    82 | loss: 5.2775898Losses:  4.330341339111328 0.11324086785316467
CurrentTrain: epoch  3, batch    83 | loss: 4.4435821Losses:  4.5575666427612305 0.1535274088382721
CurrentTrain: epoch  3, batch    84 | loss: 4.7110939Losses:  5.522006988525391 0.27329444885253906
CurrentTrain: epoch  3, batch    85 | loss: 5.7953014Losses:  4.370135307312012 0.19379082322120667
CurrentTrain: epoch  3, batch    86 | loss: 4.5639262Losses:  4.154763698577881 0.19535201787948608
CurrentTrain: epoch  3, batch    87 | loss: 4.3501158Losses:  4.2715606689453125 0.1772482991218567
CurrentTrain: epoch  3, batch    88 | loss: 4.4488091Losses:  4.790350914001465 0.2732100486755371
CurrentTrain: epoch  3, batch    89 | loss: 5.0635610Losses:  4.5192975997924805 0.15970154106616974
CurrentTrain: epoch  3, batch    90 | loss: 4.6789989Losses:  5.369922637939453 0.18641351163387299
CurrentTrain: epoch  3, batch    91 | loss: 5.5563359Losses:  4.9086480140686035 0.19994384050369263
CurrentTrain: epoch  3, batch    92 | loss: 5.1085920Losses:  4.2683916091918945 0.07381430268287659
CurrentTrain: epoch  3, batch    93 | loss: 4.3422060Losses:  4.323989391326904 0.30946531891822815
CurrentTrain: epoch  3, batch    94 | loss: 4.6334548Losses:  4.274359226226807 0.24472969770431519
CurrentTrain: epoch  3, batch    95 | loss: 4.5190887Losses:  4.329163551330566 0.12591831386089325
CurrentTrain: epoch  3, batch    96 | loss: 4.4550819Losses:  4.273062705993652 0.17976409196853638
CurrentTrain: epoch  3, batch    97 | loss: 4.4528270Losses:  4.281275749206543 0.15516118705272675
CurrentTrain: epoch  3, batch    98 | loss: 4.4364371Losses:  4.499448776245117 0.18397827446460724
CurrentTrain: epoch  3, batch    99 | loss: 4.6834269Losses:  4.731510162353516 0.2725287079811096
CurrentTrain: epoch  3, batch   100 | loss: 5.0040388Losses:  4.321700572967529 0.1156395822763443
CurrentTrain: epoch  3, batch   101 | loss: 4.4373403Losses:  4.43006706237793 0.2334282100200653
CurrentTrain: epoch  3, batch   102 | loss: 4.6634951Losses:  4.8197021484375 0.1786767840385437
CurrentTrain: epoch  3, batch   103 | loss: 4.9983788Losses:  4.400278568267822 0.1434694528579712
CurrentTrain: epoch  3, batch   104 | loss: 4.5437479Losses:  4.220580577850342 0.11550907790660858
CurrentTrain: epoch  3, batch   105 | loss: 4.3360896Losses:  4.2909722328186035 0.11971677839756012
CurrentTrain: epoch  3, batch   106 | loss: 4.4106889Losses:  4.404667854309082 0.19440633058547974
CurrentTrain: epoch  3, batch   107 | loss: 4.5990744Losses:  4.437150001525879 0.26662418246269226
CurrentTrain: epoch  3, batch   108 | loss: 4.7037740Losses:  4.351045608520508 0.20371446013450623
CurrentTrain: epoch  3, batch   109 | loss: 4.5547600Losses:  4.709126949310303 0.22689461708068848
CurrentTrain: epoch  3, batch   110 | loss: 4.9360218Losses:  4.341303825378418 0.16263270378112793
CurrentTrain: epoch  3, batch   111 | loss: 4.5039368Losses:  4.318757057189941 0.1517917513847351
CurrentTrain: epoch  3, batch   112 | loss: 4.4705486Losses:  4.4540300369262695 0.11782658100128174
CurrentTrain: epoch  3, batch   113 | loss: 4.5718565Losses:  4.322672367095947 0.09908035397529602
CurrentTrain: epoch  3, batch   114 | loss: 4.4217529Losses:  4.435430526733398 0.2621557414531708
CurrentTrain: epoch  3, batch   115 | loss: 4.6975861Losses:  4.275854587554932 0.1771557629108429
CurrentTrain: epoch  3, batch   116 | loss: 4.4530106Losses:  5.8891496658325195 0.43996167182922363
CurrentTrain: epoch  3, batch   117 | loss: 6.3291111Losses:  4.240399360656738 0.16679221391677856
CurrentTrain: epoch  3, batch   118 | loss: 4.4071918Losses:  4.324288368225098 0.22983768582344055
CurrentTrain: epoch  3, batch   119 | loss: 4.5541263Losses:  4.605355262756348 0.24513694643974304
CurrentTrain: epoch  3, batch   120 | loss: 4.8504920Losses:  4.411538600921631 0.2739829421043396
CurrentTrain: epoch  3, batch   121 | loss: 4.6855216Losses:  4.266826629638672 0.2764647305011749
CurrentTrain: epoch  3, batch   122 | loss: 4.5432916Losses:  4.2271928787231445 0.2678077518939972
CurrentTrain: epoch  3, batch   123 | loss: 4.4950008Losses:  4.231736183166504 0.11575189232826233
CurrentTrain: epoch  3, batch   124 | loss: 4.3474879Losses:  4.234421253204346 0.20034056901931763
CurrentTrain: epoch  4, batch     0 | loss: 4.4347620Losses:  4.416426658630371 0.15234020352363586
CurrentTrain: epoch  4, batch     1 | loss: 4.5687671Losses:  4.9571943283081055 0.3012082576751709
CurrentTrain: epoch  4, batch     2 | loss: 5.2584028Losses:  4.552059650421143 0.0920569971203804
CurrentTrain: epoch  4, batch     3 | loss: 4.6441169Losses:  4.840232849121094 0.20406603813171387
CurrentTrain: epoch  4, batch     4 | loss: 5.0442991Losses:  5.62880802154541 0.3794432282447815
CurrentTrain: epoch  4, batch     5 | loss: 6.0082512Losses:  4.586503028869629 0.15056608617305756
CurrentTrain: epoch  4, batch     6 | loss: 4.7370691Losses:  4.340889930725098 0.16531811654567719
CurrentTrain: epoch  4, batch     7 | loss: 4.5062079Losses:  4.448698997497559 0.1333862543106079
CurrentTrain: epoch  4, batch     8 | loss: 4.5820851Losses:  4.562126159667969 0.15879884362220764
CurrentTrain: epoch  4, batch     9 | loss: 4.7209249Losses:  4.414562702178955 0.2673060894012451
CurrentTrain: epoch  4, batch    10 | loss: 4.6818686Losses:  4.7505879402160645 0.169834703207016
CurrentTrain: epoch  4, batch    11 | loss: 4.9204226Losses:  4.332869529724121 0.13965114951133728
CurrentTrain: epoch  4, batch    12 | loss: 4.4725208Losses:  4.233033180236816 0.13302090764045715
CurrentTrain: epoch  4, batch    13 | loss: 4.3660541Losses:  4.506536483764648 0.1599198579788208
CurrentTrain: epoch  4, batch    14 | loss: 4.6664562Losses:  4.296504497528076 0.1393001675605774
CurrentTrain: epoch  4, batch    15 | loss: 4.4358048Losses:  4.213617324829102 0.0917922705411911
CurrentTrain: epoch  4, batch    16 | loss: 4.3054094Losses:  4.209815502166748 0.21509113907814026
CurrentTrain: epoch  4, batch    17 | loss: 4.4249067Losses:  4.337730407714844 0.2066957652568817
CurrentTrain: epoch  4, batch    18 | loss: 4.5444260Losses:  4.25053596496582 0.22044339776039124
CurrentTrain: epoch  4, batch    19 | loss: 4.4709792Losses:  4.413966655731201 0.21125489473342896
CurrentTrain: epoch  4, batch    20 | loss: 4.6252217Losses:  4.515267848968506 0.27184194326400757
CurrentTrain: epoch  4, batch    21 | loss: 4.7871099Losses:  4.25029182434082 0.12587997317314148
CurrentTrain: epoch  4, batch    22 | loss: 4.3761716Losses:  4.26580810546875 0.1038406565785408
CurrentTrain: epoch  4, batch    23 | loss: 4.3696489Losses:  4.497731685638428 0.1258677840232849
CurrentTrain: epoch  4, batch    24 | loss: 4.6235995Losses:  4.418187141418457 0.16016407310962677
CurrentTrain: epoch  4, batch    25 | loss: 4.5783510Losses:  4.931888580322266 0.10008145868778229
CurrentTrain: epoch  4, batch    26 | loss: 5.0319700Losses:  4.342137813568115 0.21412993967533112
CurrentTrain: epoch  4, batch    27 | loss: 4.5562677Losses:  4.424739837646484 0.16610780358314514
CurrentTrain: epoch  4, batch    28 | loss: 4.5908475Losses:  4.234619140625 0.1498546153306961
CurrentTrain: epoch  4, batch    29 | loss: 4.3844738Losses:  4.495906829833984 0.10989430546760559
CurrentTrain: epoch  4, batch    30 | loss: 4.6058011Losses:  4.342363357543945 0.28586965799331665
CurrentTrain: epoch  4, batch    31 | loss: 4.6282330Losses:  4.166490077972412 0.13532382249832153
CurrentTrain: epoch  4, batch    32 | loss: 4.3018141Losses:  4.557548522949219 0.11931803077459335
CurrentTrain: epoch  4, batch    33 | loss: 4.6768665Losses:  4.532468795776367 0.3660701513290405
CurrentTrain: epoch  4, batch    34 | loss: 4.8985391Losses:  4.496239185333252 0.15097366273403168
CurrentTrain: epoch  4, batch    35 | loss: 4.6472130Losses:  4.365689277648926 0.2289889007806778
CurrentTrain: epoch  4, batch    36 | loss: 4.5946784Losses:  4.383431434631348 0.09417599439620972
CurrentTrain: epoch  4, batch    37 | loss: 4.4776073Losses:  4.281573295593262 0.22158364951610565
CurrentTrain: epoch  4, batch    38 | loss: 4.5031571Losses:  4.403987407684326 0.15574918687343597
CurrentTrain: epoch  4, batch    39 | loss: 4.5597367Losses:  4.302450180053711 0.2952510714530945
CurrentTrain: epoch  4, batch    40 | loss: 4.5977011Losses:  4.505436420440674 0.23174680769443512
CurrentTrain: epoch  4, batch    41 | loss: 4.7371831Losses:  4.556934356689453 0.164851576089859
CurrentTrain: epoch  4, batch    42 | loss: 4.7217860Losses:  4.341955184936523 0.18174004554748535
CurrentTrain: epoch  4, batch    43 | loss: 4.5236950Losses:  4.2219767570495605 0.12394674122333527
CurrentTrain: epoch  4, batch    44 | loss: 4.3459234Losses:  4.440855026245117 0.099294513463974
CurrentTrain: epoch  4, batch    45 | loss: 4.5401497Losses:  4.367407321929932 0.23153391480445862
CurrentTrain: epoch  4, batch    46 | loss: 4.5989413Losses:  4.2762603759765625 0.2381834089756012
CurrentTrain: epoch  4, batch    47 | loss: 4.5144439Losses:  4.191117286682129 0.15370020270347595
CurrentTrain: epoch  4, batch    48 | loss: 4.3448176Losses:  4.131157875061035 0.08639727532863617
CurrentTrain: epoch  4, batch    49 | loss: 4.2175550Losses:  4.604912281036377 0.17440259456634521
CurrentTrain: epoch  4, batch    50 | loss: 4.7793150Losses:  4.357783794403076 0.21214601397514343
CurrentTrain: epoch  4, batch    51 | loss: 4.5699296Losses:  4.354763031005859 0.19887731969356537
CurrentTrain: epoch  4, batch    52 | loss: 4.5536404Losses:  4.247640609741211 0.19902446866035461
CurrentTrain: epoch  4, batch    53 | loss: 4.4466653Losses:  4.520672798156738 0.11783348023891449
CurrentTrain: epoch  4, batch    54 | loss: 4.6385064Losses:  4.246636390686035 0.2290646731853485
CurrentTrain: epoch  4, batch    55 | loss: 4.4757009Losses:  4.268101692199707 0.2461632490158081
CurrentTrain: epoch  4, batch    56 | loss: 4.5142651Losses:  4.245989799499512 0.15576821565628052
CurrentTrain: epoch  4, batch    57 | loss: 4.4017582Losses:  5.033237934112549 0.2867669463157654
CurrentTrain: epoch  4, batch    58 | loss: 5.3200049Losses:  4.1314377784729 0.12887272238731384
CurrentTrain: epoch  4, batch    59 | loss: 4.2603106Losses:  4.51784086227417 0.1423141360282898
CurrentTrain: epoch  4, batch    60 | loss: 4.6601548Losses:  4.248517990112305 0.19666408002376556
CurrentTrain: epoch  4, batch    61 | loss: 4.4451818Losses:  4.308794975280762 0.14407791197299957
CurrentTrain: epoch  4, batch    62 | loss: 4.4528728Losses:  4.303580284118652 0.14656247198581696
CurrentTrain: epoch  4, batch    63 | loss: 4.4501429Losses:  4.270673751831055 0.23532138764858246
CurrentTrain: epoch  4, batch    64 | loss: 4.5059953Losses:  4.195387363433838 0.2522304654121399
CurrentTrain: epoch  4, batch    65 | loss: 4.4476180Losses:  4.393124580383301 0.1535649597644806
CurrentTrain: epoch  4, batch    66 | loss: 4.5466895Losses:  4.422987461090088 0.11975375562906265
CurrentTrain: epoch  4, batch    67 | loss: 4.5427413Losses:  4.317648410797119 0.10871811956167221
CurrentTrain: epoch  4, batch    68 | loss: 4.4263663Losses:  4.14483642578125 0.2491707056760788
CurrentTrain: epoch  4, batch    69 | loss: 4.3940072Losses:  4.197718620300293 0.16958162188529968
CurrentTrain: epoch  4, batch    70 | loss: 4.3673000Losses:  4.110960960388184 0.17162102460861206
CurrentTrain: epoch  4, batch    71 | loss: 4.2825818Losses:  4.423007965087891 0.11878055334091187
CurrentTrain: epoch  4, batch    72 | loss: 4.5417886Losses:  4.2990593910217285 0.1868647038936615
CurrentTrain: epoch  4, batch    73 | loss: 4.4859242Losses:  4.275279521942139 0.17977161705493927
CurrentTrain: epoch  4, batch    74 | loss: 4.4550509Losses:  4.355714321136475 0.24372124671936035
CurrentTrain: epoch  4, batch    75 | loss: 4.5994358Losses:  4.26204776763916 0.10475622862577438
CurrentTrain: epoch  4, batch    76 | loss: 4.3668041Losses:  4.186404705047607 0.11836791783571243
CurrentTrain: epoch  4, batch    77 | loss: 4.3047729Losses:  4.3438920974731445 0.16127309203147888
CurrentTrain: epoch  4, batch    78 | loss: 4.5051651Losses:  5.833585739135742 0.45005518198013306
CurrentTrain: epoch  4, batch    79 | loss: 6.2836409Losses:  4.275932788848877 0.17025144398212433
CurrentTrain: epoch  4, batch    80 | loss: 4.4461842Losses:  4.439079284667969 0.20540009438991547
CurrentTrain: epoch  4, batch    81 | loss: 4.6444793Losses:  4.584120750427246 0.25043803453445435
CurrentTrain: epoch  4, batch    82 | loss: 4.8345590Losses:  4.149089336395264 0.29641520977020264
CurrentTrain: epoch  4, batch    83 | loss: 4.4455047Losses:  4.445075511932373 0.1601465344429016
CurrentTrain: epoch  4, batch    84 | loss: 4.6052222Losses:  4.5860595703125 0.14577701687812805
CurrentTrain: epoch  4, batch    85 | loss: 4.7318368Losses:  4.5556254386901855 0.2300490438938141
CurrentTrain: epoch  4, batch    86 | loss: 4.7856746Losses:  4.293638706207275 0.2616165578365326
CurrentTrain: epoch  4, batch    87 | loss: 4.5552554Losses:  4.316931247711182 0.129771888256073
CurrentTrain: epoch  4, batch    88 | loss: 4.4467030Losses:  4.347629547119141 0.12246401607990265
CurrentTrain: epoch  4, batch    89 | loss: 4.4700937Losses:  4.226060390472412 0.2140636295080185
CurrentTrain: epoch  4, batch    90 | loss: 4.4401240Losses:  4.31535530090332 0.12118521332740784
CurrentTrain: epoch  4, batch    91 | loss: 4.4365406Losses:  4.2260308265686035 0.12683559954166412
CurrentTrain: epoch  4, batch    92 | loss: 4.3528666Losses:  4.375134468078613 0.2696429491043091
CurrentTrain: epoch  4, batch    93 | loss: 4.6447773Losses:  4.285614967346191 0.08991894125938416
CurrentTrain: epoch  4, batch    94 | loss: 4.3755341Losses:  4.444829940795898 0.2832918167114258
CurrentTrain: epoch  4, batch    95 | loss: 4.7281218Losses:  4.181129455566406 0.06926675140857697
CurrentTrain: epoch  4, batch    96 | loss: 4.2503963Losses:  4.18354606628418 0.2738777697086334
CurrentTrain: epoch  4, batch    97 | loss: 4.4574237Losses:  4.279332160949707 0.22125431895256042
CurrentTrain: epoch  4, batch    98 | loss: 4.5005865Losses:  4.209127902984619 0.11211100220680237
CurrentTrain: epoch  4, batch    99 | loss: 4.3212390Losses:  4.250848293304443 0.10449342429637909
CurrentTrain: epoch  4, batch   100 | loss: 4.3553419Losses:  4.455635070800781 0.2326197624206543
CurrentTrain: epoch  4, batch   101 | loss: 4.6882548Losses:  4.257785797119141 0.10796055942773819
CurrentTrain: epoch  4, batch   102 | loss: 4.3657465Losses:  3.968996524810791 0.12876582145690918
CurrentTrain: epoch  4, batch   103 | loss: 4.0977621Losses:  4.212686538696289 0.15271304547786713
CurrentTrain: epoch  4, batch   104 | loss: 4.3653994Losses:  4.246435165405273 0.12291645258665085
CurrentTrain: epoch  4, batch   105 | loss: 4.3693514Losses:  4.1835246086120605 0.10230705887079239
CurrentTrain: epoch  4, batch   106 | loss: 4.2858315Losses:  4.210577487945557 0.13078473508358002
CurrentTrain: epoch  4, batch   107 | loss: 4.3413620Losses:  4.1422271728515625 0.22812175750732422
CurrentTrain: epoch  4, batch   108 | loss: 4.3703489Losses:  4.276777267456055 0.14993394911289215
CurrentTrain: epoch  4, batch   109 | loss: 4.4267111Losses:  4.0561113357543945 0.10905693471431732
CurrentTrain: epoch  4, batch   110 | loss: 4.1651683Losses:  4.22217321395874 0.13923496007919312
CurrentTrain: epoch  4, batch   111 | loss: 4.3614082Losses:  4.103117942810059 0.09672369062900543
CurrentTrain: epoch  4, batch   112 | loss: 4.1998415Losses:  4.081516742706299 0.0711624026298523
CurrentTrain: epoch  4, batch   113 | loss: 4.1526790Losses:  4.357052326202393 0.16922855377197266
CurrentTrain: epoch  4, batch   114 | loss: 4.5262809Losses:  4.228487968444824 0.07645298540592194
CurrentTrain: epoch  4, batch   115 | loss: 4.3049412Losses:  4.219483852386475 0.17207027971744537
CurrentTrain: epoch  4, batch   116 | loss: 4.3915544Losses:  4.193960666656494 0.2282022088766098
CurrentTrain: epoch  4, batch   117 | loss: 4.4221630Losses:  4.125597953796387 0.18378204107284546
CurrentTrain: epoch  4, batch   118 | loss: 4.3093801Losses:  4.3738884925842285 0.1105840727686882
CurrentTrain: epoch  4, batch   119 | loss: 4.4844728Losses:  4.19886589050293 0.11514634639024734
CurrentTrain: epoch  4, batch   120 | loss: 4.3140121Losses:  4.148194313049316 0.14758074283599854
CurrentTrain: epoch  4, batch   121 | loss: 4.2957749Losses:  4.103222370147705 0.06372687965631485
CurrentTrain: epoch  4, batch   122 | loss: 4.1669493Losses:  4.346806049346924 0.11324038356542587
CurrentTrain: epoch  4, batch   123 | loss: 4.4600463Losses:  4.201595783233643 0.10174243897199631
CurrentTrain: epoch  4, batch   124 | loss: 4.3033381Losses:  4.230705261230469 0.17607063055038452
CurrentTrain: epoch  5, batch     0 | loss: 4.4067760Losses:  4.228699684143066 0.13047140836715698
CurrentTrain: epoch  5, batch     1 | loss: 4.3591709Losses:  4.05759859085083 0.07515402138233185
CurrentTrain: epoch  5, batch     2 | loss: 4.1327524Losses:  4.080982208251953 0.11161139607429504
CurrentTrain: epoch  5, batch     3 | loss: 4.1925936Losses:  4.339911460876465 0.13319462537765503
CurrentTrain: epoch  5, batch     4 | loss: 4.4731059Losses:  4.29918098449707 0.16710001230239868
CurrentTrain: epoch  5, batch     5 | loss: 4.4662809Losses:  4.163996696472168 0.13752204179763794
CurrentTrain: epoch  5, batch     6 | loss: 4.3015189Losses:  4.288410186767578 0.1844921112060547
CurrentTrain: epoch  5, batch     7 | loss: 4.4729023Losses:  4.12653923034668 0.13731037080287933
CurrentTrain: epoch  5, batch     8 | loss: 4.2638497Losses:  4.050914764404297 0.06769277155399323
CurrentTrain: epoch  5, batch     9 | loss: 4.1186075Losses:  4.221660614013672 0.12106522172689438
CurrentTrain: epoch  5, batch    10 | loss: 4.3427258Losses:  4.213013648986816 0.061389271169900894
CurrentTrain: epoch  5, batch    11 | loss: 4.2744031Losses:  4.080677032470703 0.1347055733203888
CurrentTrain: epoch  5, batch    12 | loss: 4.2153826Losses:  4.342108726501465 0.18187221884727478
CurrentTrain: epoch  5, batch    13 | loss: 4.5239811Losses:  4.359973907470703 0.14902527630329132
CurrentTrain: epoch  5, batch    14 | loss: 4.5089993Losses:  4.1684370040893555 0.23929594457149506
CurrentTrain: epoch  5, batch    15 | loss: 4.4077330Losses:  4.123929500579834 0.17074409127235413
CurrentTrain: epoch  5, batch    16 | loss: 4.2946734Losses:  4.154736042022705 0.15104258060455322
CurrentTrain: epoch  5, batch    17 | loss: 4.3057785Losses:  4.204930305480957 0.12653373181819916
CurrentTrain: epoch  5, batch    18 | loss: 4.3314638Losses:  4.230835914611816 0.1352412849664688
CurrentTrain: epoch  5, batch    19 | loss: 4.3660774Losses:  4.156487464904785 0.13294869661331177
CurrentTrain: epoch  5, batch    20 | loss: 4.2894363Losses:  4.204833030700684 0.08371473848819733
CurrentTrain: epoch  5, batch    21 | loss: 4.2885480Losses:  4.142636299133301 0.1826225221157074
CurrentTrain: epoch  5, batch    22 | loss: 4.3252587Losses:  4.240532398223877 0.11890360713005066
CurrentTrain: epoch  5, batch    23 | loss: 4.3594360Losses:  4.189223289489746 0.11757778376340866
CurrentTrain: epoch  5, batch    24 | loss: 4.3068008Losses:  4.175356864929199 0.12123416364192963
CurrentTrain: epoch  5, batch    25 | loss: 4.2965908Losses:  4.211377143859863 0.15025073289871216
CurrentTrain: epoch  5, batch    26 | loss: 4.3616281Losses:  4.268044948577881 0.10768889635801315
CurrentTrain: epoch  5, batch    27 | loss: 4.3757339Losses:  4.282548904418945 0.13780516386032104
CurrentTrain: epoch  5, batch    28 | loss: 4.4203539Losses:  4.154976844787598 0.05661284178495407
CurrentTrain: epoch  5, batch    29 | loss: 4.2115898Losses:  4.129795074462891 0.15436524152755737
CurrentTrain: epoch  5, batch    30 | loss: 4.2841601Losses:  4.184444904327393 0.15143983066082
CurrentTrain: epoch  5, batch    31 | loss: 4.3358846Losses:  4.109878063201904 0.13292182981967926
CurrentTrain: epoch  5, batch    32 | loss: 4.2427998Losses:  3.9858601093292236 0.120333731174469
CurrentTrain: epoch  5, batch    33 | loss: 4.1061940Losses:  4.201786041259766 0.11409303545951843
CurrentTrain: epoch  5, batch    34 | loss: 4.3158789Losses:  4.146940231323242 0.13904578983783722
CurrentTrain: epoch  5, batch    35 | loss: 4.2859859Losses:  4.0853681564331055 0.11239621788263321
CurrentTrain: epoch  5, batch    36 | loss: 4.1977644Losses:  4.150961875915527 0.07106642425060272
CurrentTrain: epoch  5, batch    37 | loss: 4.2220283Losses:  4.3344316482543945 0.16604548692703247
CurrentTrain: epoch  5, batch    38 | loss: 4.5004773Losses:  4.204443454742432 0.17924770712852478
CurrentTrain: epoch  5, batch    39 | loss: 4.3836913Losses:  4.15270471572876 0.14098027348518372
CurrentTrain: epoch  5, batch    40 | loss: 4.2936850Losses:  4.140347957611084 0.14991554617881775
CurrentTrain: epoch  5, batch    41 | loss: 4.2902637Losses:  4.033246040344238 0.14592590928077698
CurrentTrain: epoch  5, batch    42 | loss: 4.1791720Losses:  4.184101104736328 0.10520651936531067
CurrentTrain: epoch  5, batch    43 | loss: 4.2893076Losses:  4.112943172454834 0.13361099362373352
CurrentTrain: epoch  5, batch    44 | loss: 4.2465544Losses:  4.143986701965332 0.10628322511911392
CurrentTrain: epoch  5, batch    45 | loss: 4.2502699Losses:  4.158109188079834 0.10521243512630463
CurrentTrain: epoch  5, batch    46 | loss: 4.2633214Losses:  4.108381271362305 0.1322094202041626
CurrentTrain: epoch  5, batch    47 | loss: 4.2405906Losses:  4.200046062469482 0.21216034889221191
CurrentTrain: epoch  5, batch    48 | loss: 4.4122066Losses:  4.00771427154541 0.1131855919957161
CurrentTrain: epoch  5, batch    49 | loss: 4.1208997Losses:  4.104410171508789 0.09777431190013885
CurrentTrain: epoch  5, batch    50 | loss: 4.2021847Losses:  4.012687683105469 0.11981748044490814
CurrentTrain: epoch  5, batch    51 | loss: 4.1325049Losses:  4.091707229614258 0.12712198495864868
CurrentTrain: epoch  5, batch    52 | loss: 4.2188292Losses:  4.2049689292907715 0.1084834635257721
CurrentTrain: epoch  5, batch    53 | loss: 4.3134522Losses:  4.135623931884766 0.07449246197938919
CurrentTrain: epoch  5, batch    54 | loss: 4.2101164Losses:  4.131214141845703 0.12282835692167282
CurrentTrain: epoch  5, batch    55 | loss: 4.2540426Losses:  4.194659233093262 0.16105207800865173
CurrentTrain: epoch  5, batch    56 | loss: 4.3557115Losses:  4.171321868896484 0.06492730230093002
CurrentTrain: epoch  5, batch    57 | loss: 4.2362490Losses:  4.125994682312012 0.2181967943906784
CurrentTrain: epoch  5, batch    58 | loss: 4.3441916Losses:  4.164109230041504 0.08866818249225616
CurrentTrain: epoch  5, batch    59 | loss: 4.2527776Losses:  4.10554838180542 0.17575347423553467
CurrentTrain: epoch  5, batch    60 | loss: 4.2813020Losses:  4.19048547744751 0.1320285201072693
CurrentTrain: epoch  5, batch    61 | loss: 4.3225141Losses:  4.031920433044434 0.12836898863315582
CurrentTrain: epoch  5, batch    62 | loss: 4.1602893Losses:  4.134037017822266 0.0973079726099968
CurrentTrain: epoch  5, batch    63 | loss: 4.2313452Losses:  4.077579975128174 0.15467029809951782
CurrentTrain: epoch  5, batch    64 | loss: 4.2322502Losses:  4.115703582763672 0.13605406880378723
CurrentTrain: epoch  5, batch    65 | loss: 4.2517576Losses:  4.120303153991699 0.1065334603190422
CurrentTrain: epoch  5, batch    66 | loss: 4.2268367Losses:  4.144162654876709 0.15377864241600037
CurrentTrain: epoch  5, batch    67 | loss: 4.2979412Losses:  4.162652969360352 0.15720447897911072
CurrentTrain: epoch  5, batch    68 | loss: 4.3198576Losses:  4.103392124176025 0.10803885012865067
CurrentTrain: epoch  5, batch    69 | loss: 4.2114310Losses:  4.081273555755615 0.14391177892684937
CurrentTrain: epoch  5, batch    70 | loss: 4.2251854Losses:  4.164585113525391 0.14710891246795654
CurrentTrain: epoch  5, batch    71 | loss: 4.3116941Losses:  4.708233833312988 0.3001428246498108
CurrentTrain: epoch  5, batch    72 | loss: 5.0083766Losses:  4.141707897186279 0.0798080638051033
CurrentTrain: epoch  5, batch    73 | loss: 4.2215161Losses:  4.176997661590576 0.12179505825042725
CurrentTrain: epoch  5, batch    74 | loss: 4.2987928Losses:  4.143354415893555 0.09809011220932007
CurrentTrain: epoch  5, batch    75 | loss: 4.2414446Losses:  4.068451881408691 0.09966887533664703
CurrentTrain: epoch  5, batch    76 | loss: 4.1681209Losses:  4.0054931640625 0.19328543543815613
CurrentTrain: epoch  5, batch    77 | loss: 4.1987786Losses:  4.08325719833374 0.15034708380699158
CurrentTrain: epoch  5, batch    78 | loss: 4.2336044Losses:  4.2140374183654785 0.1807132512331009
CurrentTrain: epoch  5, batch    79 | loss: 4.3947506Losses:  4.2116804122924805 0.2617873549461365
CurrentTrain: epoch  5, batch    80 | loss: 4.4734678Losses:  4.064927101135254 0.08503162860870361
CurrentTrain: epoch  5, batch    81 | loss: 4.1499586Losses:  4.0308051109313965 0.09424031525850296
CurrentTrain: epoch  5, batch    82 | loss: 4.1250453Losses:  4.1726226806640625 0.1544264853000641
CurrentTrain: epoch  5, batch    83 | loss: 4.3270493Losses:  4.094907283782959 0.21179209649562836
CurrentTrain: epoch  5, batch    84 | loss: 4.3066993Losses:  4.0708770751953125 0.14606930315494537
CurrentTrain: epoch  5, batch    85 | loss: 4.2169466Losses:  4.125082492828369 0.1575315296649933
CurrentTrain: epoch  5, batch    86 | loss: 4.2826142Losses:  4.072814464569092 0.1454915702342987
CurrentTrain: epoch  5, batch    87 | loss: 4.2183061Losses:  4.056297779083252 0.15861159563064575
CurrentTrain: epoch  5, batch    88 | loss: 4.2149096Losses:  4.210239410400391 0.1151578277349472
CurrentTrain: epoch  5, batch    89 | loss: 4.3253970Losses:  4.052731990814209 0.11625127494335175
CurrentTrain: epoch  5, batch    90 | loss: 4.1689835Losses:  4.079075813293457 0.15028256177902222
CurrentTrain: epoch  5, batch    91 | loss: 4.2293582Losses:  4.093363285064697 0.10677897185087204
CurrentTrain: epoch  5, batch    92 | loss: 4.2001424Losses:  4.138038635253906 0.10209249705076218
CurrentTrain: epoch  5, batch    93 | loss: 4.2401309Losses:  4.045101165771484 0.09434714168310165
CurrentTrain: epoch  5, batch    94 | loss: 4.1394482Losses:  4.071462631225586 0.20246684551239014
CurrentTrain: epoch  5, batch    95 | loss: 4.2739296Losses:  4.051705837249756 0.07846978306770325
CurrentTrain: epoch  5, batch    96 | loss: 4.1301756Losses:  4.138181686401367 0.16265922784805298
CurrentTrain: epoch  5, batch    97 | loss: 4.3008409Losses:  4.090888977050781 0.15858237445354462
CurrentTrain: epoch  5, batch    98 | loss: 4.2494712Losses:  4.1463165283203125 0.0817166417837143
CurrentTrain: epoch  5, batch    99 | loss: 4.2280331Losses:  4.055543422698975 0.13078810274600983
CurrentTrain: epoch  5, batch   100 | loss: 4.1863317Losses:  4.107532501220703 0.10436044633388519
CurrentTrain: epoch  5, batch   101 | loss: 4.2118931Losses:  4.013838768005371 0.13868959248065948
CurrentTrain: epoch  5, batch   102 | loss: 4.1525283Losses:  4.10800838470459 0.11957046389579773
CurrentTrain: epoch  5, batch   103 | loss: 4.2275786Losses:  4.175270080566406 0.10787959396839142
CurrentTrain: epoch  5, batch   104 | loss: 4.2831497Losses:  4.059748649597168 0.12069950997829437
CurrentTrain: epoch  5, batch   105 | loss: 4.1804481Losses:  4.075599670410156 0.1356416493654251
CurrentTrain: epoch  5, batch   106 | loss: 4.2112412Losses:  4.036532402038574 0.14312566816806793
CurrentTrain: epoch  5, batch   107 | loss: 4.1796579Losses:  4.1178107261657715 0.05897527560591698
CurrentTrain: epoch  5, batch   108 | loss: 4.1767859Losses:  4.112751007080078 0.12218108773231506
CurrentTrain: epoch  5, batch   109 | loss: 4.2349319Losses:  4.140036582946777 0.11620545387268066
CurrentTrain: epoch  5, batch   110 | loss: 4.2562418Losses:  4.029957294464111 0.11071218550205231
CurrentTrain: epoch  5, batch   111 | loss: 4.1406693Losses:  4.0335845947265625 0.17420445382595062
CurrentTrain: epoch  5, batch   112 | loss: 4.2077889Losses:  4.062504768371582 0.0938449576497078
CurrentTrain: epoch  5, batch   113 | loss: 4.1563497Losses:  4.0228047370910645 0.13248692452907562
CurrentTrain: epoch  5, batch   114 | loss: 4.1552916Losses:  4.122176170349121 0.12893268465995789
CurrentTrain: epoch  5, batch   115 | loss: 4.2511086Losses:  4.04680061340332 0.07865668833255768
CurrentTrain: epoch  5, batch   116 | loss: 4.1254573Losses:  4.056739807128906 0.10317713022232056
CurrentTrain: epoch  5, batch   117 | loss: 4.1599169Losses:  4.053440570831299 0.06513871252536774
CurrentTrain: epoch  5, batch   118 | loss: 4.1185794Losses:  4.063077926635742 0.1648663878440857
CurrentTrain: epoch  5, batch   119 | loss: 4.2279444Losses:  4.069162368774414 0.12057651579380035
CurrentTrain: epoch  5, batch   120 | loss: 4.1897388Losses:  4.052877902984619 0.19040405750274658
CurrentTrain: epoch  5, batch   121 | loss: 4.2432818Losses:  4.073768615722656 0.20009762048721313
CurrentTrain: epoch  5, batch   122 | loss: 4.2738662Losses:  4.036620140075684 0.08509010076522827
CurrentTrain: epoch  5, batch   123 | loss: 4.1217103Losses:  4.04113245010376 0.1293957531452179
CurrentTrain: epoch  5, batch   124 | loss: 4.1705284Losses:  4.023230075836182 0.12700487673282623
CurrentTrain: epoch  6, batch     0 | loss: 4.1502352Losses:  4.114289283752441 0.09497340768575668
CurrentTrain: epoch  6, batch     1 | loss: 4.2092628Losses:  4.070865631103516 0.12633809447288513
CurrentTrain: epoch  6, batch     2 | loss: 4.1972036Losses:  4.029163360595703 0.15926463901996613
CurrentTrain: epoch  6, batch     3 | loss: 4.1884279Losses:  4.050415992736816 0.0799207016825676
CurrentTrain: epoch  6, batch     4 | loss: 4.1303368Losses:  4.069333076477051 0.11515088379383087
CurrentTrain: epoch  6, batch     5 | loss: 4.1844840Losses:  4.126451015472412 0.21133196353912354
CurrentTrain: epoch  6, batch     6 | loss: 4.3377829Losses:  4.093242645263672 0.15109790861606598
CurrentTrain: epoch  6, batch     7 | loss: 4.2443404Losses:  4.046126842498779 0.06872055679559708
CurrentTrain: epoch  6, batch     8 | loss: 4.1148472Losses:  4.058869361877441 0.1454114317893982
CurrentTrain: epoch  6, batch     9 | loss: 4.2042809Losses:  4.092468738555908 0.12856823205947876
CurrentTrain: epoch  6, batch    10 | loss: 4.2210369Losses:  4.108494758605957 0.061339594423770905
CurrentTrain: epoch  6, batch    11 | loss: 4.1698341Losses:  4.07562255859375 0.09656576812267303
CurrentTrain: epoch  6, batch    12 | loss: 4.1721883Losses:  4.03741455078125 0.08745847642421722
CurrentTrain: epoch  6, batch    13 | loss: 4.1248732Losses:  4.0724406242370605 0.14770284295082092
CurrentTrain: epoch  6, batch    14 | loss: 4.2201433Losses:  4.066156387329102 0.07723573595285416
CurrentTrain: epoch  6, batch    15 | loss: 4.1433921Losses:  4.046207427978516 0.1692861020565033
CurrentTrain: epoch  6, batch    16 | loss: 4.2154937Losses:  4.021046161651611 0.08240899443626404
CurrentTrain: epoch  6, batch    17 | loss: 4.1034551Losses:  4.109768867492676 0.1430935263633728
CurrentTrain: epoch  6, batch    18 | loss: 4.2528625Losses:  4.0499420166015625 0.05974266678094864
CurrentTrain: epoch  6, batch    19 | loss: 4.1096845Losses:  4.143468856811523 0.12320642173290253
CurrentTrain: epoch  6, batch    20 | loss: 4.2666755Losses:  4.0491533279418945 0.059198759496212006
CurrentTrain: epoch  6, batch    21 | loss: 4.1083522Losses:  4.020832061767578 0.072624571621418
CurrentTrain: epoch  6, batch    22 | loss: 4.0934567Losses:  4.055567741394043 0.07435859739780426
CurrentTrain: epoch  6, batch    23 | loss: 4.1299262Losses:  4.037891387939453 0.1143609881401062
CurrentTrain: epoch  6, batch    24 | loss: 4.1522522Losses:  4.062725067138672 0.07906250655651093
CurrentTrain: epoch  6, batch    25 | loss: 4.1417875Losses:  4.099282741546631 0.0961189717054367
CurrentTrain: epoch  6, batch    26 | loss: 4.1954017Losses:  3.9905219078063965 0.09930846840143204
CurrentTrain: epoch  6, batch    27 | loss: 4.0898304Losses:  4.028188705444336 0.11245489120483398
CurrentTrain: epoch  6, batch    28 | loss: 4.1406436Losses:  4.053346633911133 0.08465313166379929
CurrentTrain: epoch  6, batch    29 | loss: 4.1379995Losses:  4.05662202835083 0.08427879959344864
CurrentTrain: epoch  6, batch    30 | loss: 4.1409006Losses:  4.084826469421387 0.11798793822526932
CurrentTrain: epoch  6, batch    31 | loss: 4.2028146Losses:  4.043536186218262 0.13545113801956177
CurrentTrain: epoch  6, batch    32 | loss: 4.1789875Losses:  4.043850421905518 0.127081960439682
CurrentTrain: epoch  6, batch    33 | loss: 4.1709323Losses:  4.022992134094238 0.12366802990436554
CurrentTrain: epoch  6, batch    34 | loss: 4.1466603Losses:  3.979471206665039 0.10693290084600449
CurrentTrain: epoch  6, batch    35 | loss: 4.0864043Losses:  3.9879496097564697 0.08321113139390945
CurrentTrain: epoch  6, batch    36 | loss: 4.0711608Losses:  4.021995544433594 0.134456068277359
CurrentTrain: epoch  6, batch    37 | loss: 4.1564517Losses:  4.074136734008789 0.1510193943977356
CurrentTrain: epoch  6, batch    38 | loss: 4.2251563Losses:  4.001523017883301 0.10364703834056854
CurrentTrain: epoch  6, batch    39 | loss: 4.1051702Losses:  4.019512176513672 0.20114141702651978
CurrentTrain: epoch  6, batch    40 | loss: 4.2206535Losses:  4.03887939453125 0.1424841284751892
CurrentTrain: epoch  6, batch    41 | loss: 4.1813636Losses:  4.014060974121094 0.14166513085365295
CurrentTrain: epoch  6, batch    42 | loss: 4.1557260Losses:  4.046469211578369 0.18053841590881348
CurrentTrain: epoch  6, batch    43 | loss: 4.2270079Losses:  4.0152363777160645 0.18701407313346863
CurrentTrain: epoch  6, batch    44 | loss: 4.2022505Losses:  3.972137451171875 0.06146995723247528
CurrentTrain: epoch  6, batch    45 | loss: 4.0336075Losses:  4.016026973724365 0.06782913953065872
CurrentTrain: epoch  6, batch    46 | loss: 4.0838561Losses:  4.021708011627197 0.09179872274398804
CurrentTrain: epoch  6, batch    47 | loss: 4.1135068Losses:  3.9771647453308105 0.07622869312763214
CurrentTrain: epoch  6, batch    48 | loss: 4.0533934Losses:  4.040453910827637 0.17455482482910156
CurrentTrain: epoch  6, batch    49 | loss: 4.2150087Losses:  4.015293121337891 0.09931600838899612
CurrentTrain: epoch  6, batch    50 | loss: 4.1146092Losses:  4.080542087554932 0.14110974967479706
CurrentTrain: epoch  6, batch    51 | loss: 4.2216520Losses:  4.052823543548584 0.08319247514009476
CurrentTrain: epoch  6, batch    52 | loss: 4.1360159Losses:  4.094864845275879 0.10805876553058624
CurrentTrain: epoch  6, batch    53 | loss: 4.2029238Losses:  3.908780813217163 0.10336662083864212
CurrentTrain: epoch  6, batch    54 | loss: 4.0121474Losses:  3.941946506500244 0.10876016318798065
CurrentTrain: epoch  6, batch    55 | loss: 4.0507069Losses:  4.02330207824707 0.08780993521213531
CurrentTrain: epoch  6, batch    56 | loss: 4.1111121Losses:  4.082549571990967 0.07417909801006317
CurrentTrain: epoch  6, batch    57 | loss: 4.1567287Losses:  4.028532981872559 0.12349294126033783
CurrentTrain: epoch  6, batch    58 | loss: 4.1520257Losses:  4.153979301452637 0.07410586625337601
CurrentTrain: epoch  6, batch    59 | loss: 4.2280850Losses:  4.039938926696777 0.0582837238907814
CurrentTrain: epoch  6, batch    60 | loss: 4.0982227Losses:  4.078441619873047 0.09840923547744751
CurrentTrain: epoch  6, batch    61 | loss: 4.1768508Losses:  3.998260021209717 0.09860274195671082
CurrentTrain: epoch  6, batch    62 | loss: 4.0968628Losses:  4.034346580505371 0.12593580782413483
CurrentTrain: epoch  6, batch    63 | loss: 4.1602826Losses:  3.9780113697052 0.13428287208080292
CurrentTrain: epoch  6, batch    64 | loss: 4.1122942Losses:  3.9605541229248047 0.07898220419883728
CurrentTrain: epoch  6, batch    65 | loss: 4.0395365Losses:  4.018848419189453 0.14403799176216125
CurrentTrain: epoch  6, batch    66 | loss: 4.1628866Losses:  4.043539524078369 0.15390852093696594
CurrentTrain: epoch  6, batch    67 | loss: 4.1974483Losses:  4.064553737640381 0.13782355189323425
CurrentTrain: epoch  6, batch    68 | loss: 4.2023773Losses:  4.089842319488525 0.06878949701786041
CurrentTrain: epoch  6, batch    69 | loss: 4.1586318Losses:  3.934948444366455 0.08495716750621796
CurrentTrain: epoch  6, batch    70 | loss: 4.0199056Losses:  4.069403171539307 0.10450581461191177
CurrentTrain: epoch  6, batch    71 | loss: 4.1739092Losses:  4.072461128234863 0.12177686393260956
CurrentTrain: epoch  6, batch    72 | loss: 4.1942382Losses:  4.083580493927002 0.04751903563737869
CurrentTrain: epoch  6, batch    73 | loss: 4.1310997Losses:  4.047484397888184 0.12864325940608978
CurrentTrain: epoch  6, batch    74 | loss: 4.1761274Losses:  3.988802194595337 0.08110193908214569
CurrentTrain: epoch  6, batch    75 | loss: 4.0699043Losses:  4.014578342437744 0.08651597797870636
CurrentTrain: epoch  6, batch    76 | loss: 4.1010942Losses:  4.059755325317383 0.1220795214176178
CurrentTrain: epoch  6, batch    77 | loss: 4.1818347Losses:  4.028594493865967 0.09164901077747345
CurrentTrain: epoch  6, batch    78 | loss: 4.1202435Losses:  4.013652801513672 0.20787234604358673
CurrentTrain: epoch  6, batch    79 | loss: 4.2215252Losses:  4.038336753845215 0.1152036041021347
CurrentTrain: epoch  6, batch    80 | loss: 4.1535401Losses:  4.059813499450684 0.10961336642503738
CurrentTrain: epoch  6, batch    81 | loss: 4.1694269Losses:  4.044869422912598 0.09596991539001465
CurrentTrain: epoch  6, batch    82 | loss: 4.1408396Losses:  4.0417585372924805 0.06562910974025726
CurrentTrain: epoch  6, batch    83 | loss: 4.1073875Losses:  4.114116191864014 0.07494325935840607
CurrentTrain: epoch  6, batch    84 | loss: 4.1890593Losses:  4.0221076011657715 0.09192679822444916
CurrentTrain: epoch  6, batch    85 | loss: 4.1140342Losses:  4.031278610229492 0.14095541834831238
CurrentTrain: epoch  6, batch    86 | loss: 4.1722341Losses:  4.058960914611816 0.15186427533626556
CurrentTrain: epoch  6, batch    87 | loss: 4.2108250Losses:  4.038744926452637 0.08122986555099487
CurrentTrain: epoch  6, batch    88 | loss: 4.1199746Losses:  4.104423522949219 0.08304157853126526
CurrentTrain: epoch  6, batch    89 | loss: 4.1874652Losses:  4.0799360275268555 0.18059802055358887
CurrentTrain: epoch  6, batch    90 | loss: 4.2605343Losses:  4.038095951080322 0.09080319106578827
CurrentTrain: epoch  6, batch    91 | loss: 4.1288991Losses:  4.019041061401367 0.12635748088359833
CurrentTrain: epoch  6, batch    92 | loss: 4.1453986Losses:  3.9934799671173096 0.19873178005218506
CurrentTrain: epoch  6, batch    93 | loss: 4.1922116Losses:  4.035497665405273 0.11362235248088837
CurrentTrain: epoch  6, batch    94 | loss: 4.1491199Losses:  4.053804397583008 0.05874326080083847
CurrentTrain: epoch  6, batch    95 | loss: 4.1125479Losses:  4.043428421020508 0.10196258127689362
CurrentTrain: epoch  6, batch    96 | loss: 4.1453910Losses:  3.983555793762207 0.11952421814203262
CurrentTrain: epoch  6, batch    97 | loss: 4.1030798Losses:  4.032485008239746 0.10116533190011978
CurrentTrain: epoch  6, batch    98 | loss: 4.1336503Losses:  4.0271687507629395 0.17550024390220642
CurrentTrain: epoch  6, batch    99 | loss: 4.2026691Losses:  4.007913112640381 0.08815023303031921
CurrentTrain: epoch  6, batch   100 | loss: 4.0960631Losses:  3.956798791885376 0.09275338798761368
CurrentTrain: epoch  6, batch   101 | loss: 4.0495520Losses:  4.031221866607666 0.12721997499465942
CurrentTrain: epoch  6, batch   102 | loss: 4.1584420Losses:  3.9558305740356445 0.055841945111751556
CurrentTrain: epoch  6, batch   103 | loss: 4.0116725Losses:  4.0579833984375 0.156389057636261
CurrentTrain: epoch  6, batch   104 | loss: 4.2143726Losses:  4.075507640838623 0.07072146236896515
CurrentTrain: epoch  6, batch   105 | loss: 4.1462293Losses:  4.026068687438965 0.13982629776000977
CurrentTrain: epoch  6, batch   106 | loss: 4.1658950Losses:  4.121609687805176 0.08153633028268814
CurrentTrain: epoch  6, batch   107 | loss: 4.2031460Losses:  4.063743591308594 0.13945703208446503
CurrentTrain: epoch  6, batch   108 | loss: 4.2032008Losses:  4.005093097686768 0.1537940502166748
CurrentTrain: epoch  6, batch   109 | loss: 4.1588869Losses:  4.078500270843506 0.08158771693706512
CurrentTrain: epoch  6, batch   110 | loss: 4.1600881Losses:  3.9985756874084473 0.09797406941652298
CurrentTrain: epoch  6, batch   111 | loss: 4.0965500Losses:  4.077373504638672 0.06661958992481232
CurrentTrain: epoch  6, batch   112 | loss: 4.1439929Losses:  4.055259704589844 0.0978861153125763
CurrentTrain: epoch  6, batch   113 | loss: 4.1531458Losses:  4.066821575164795 0.11184516549110413
CurrentTrain: epoch  6, batch   114 | loss: 4.1786666Losses:  4.035613059997559 0.10079015791416168
CurrentTrain: epoch  6, batch   115 | loss: 4.1364031Losses:  4.022041320800781 0.12627652287483215
CurrentTrain: epoch  6, batch   116 | loss: 4.1483178Losses:  4.003212928771973 0.08297297358512878
CurrentTrain: epoch  6, batch   117 | loss: 4.0861859Losses:  3.989617347717285 0.09765574336051941
CurrentTrain: epoch  6, batch   118 | loss: 4.0872731Losses:  4.213281631469727 0.20337063074111938
CurrentTrain: epoch  6, batch   119 | loss: 4.4166522Losses:  4.06442928314209 0.07128302752971649
CurrentTrain: epoch  6, batch   120 | loss: 4.1357121Losses:  4.0436601638793945 0.09236383438110352
CurrentTrain: epoch  6, batch   121 | loss: 4.1360240Losses:  3.9782049655914307 0.12473618984222412
CurrentTrain: epoch  6, batch   122 | loss: 4.1029410Losses:  4.071501731872559 0.04063519090414047
CurrentTrain: epoch  6, batch   123 | loss: 4.1121368Losses:  4.004052639007568 0.10045851767063141
CurrentTrain: epoch  6, batch   124 | loss: 4.1045113Losses:  4.051607131958008 0.07380148768424988
CurrentTrain: epoch  7, batch     0 | loss: 4.1254086Losses:  4.0038909912109375 0.1341134011745453
CurrentTrain: epoch  7, batch     1 | loss: 4.1380043Losses:  4.008580207824707 0.1247318834066391
CurrentTrain: epoch  7, batch     2 | loss: 4.1333122Losses:  3.9708962440490723 0.05618870258331299
CurrentTrain: epoch  7, batch     3 | loss: 4.0270848Losses:  3.9953227043151855 0.08592407405376434
CurrentTrain: epoch  7, batch     4 | loss: 4.0812469Losses:  4.065627098083496 0.0891275703907013
CurrentTrain: epoch  7, batch     5 | loss: 4.1547546Losses:  4.07484245300293 0.12196139991283417
CurrentTrain: epoch  7, batch     6 | loss: 4.1968040Losses:  3.983515739440918 0.14119769632816315
CurrentTrain: epoch  7, batch     7 | loss: 4.1247134Losses:  3.9682791233062744 0.16057729721069336
CurrentTrain: epoch  7, batch     8 | loss: 4.1288567Losses:  3.9834847450256348 0.06976798176765442
CurrentTrain: epoch  7, batch     9 | loss: 4.0532527Losses:  3.9854648113250732 0.15440741181373596
CurrentTrain: epoch  7, batch    10 | loss: 4.1398721Losses:  3.969780921936035 0.15907111763954163
CurrentTrain: epoch  7, batch    11 | loss: 4.1288519Losses:  3.9923412799835205 0.1035737469792366
CurrentTrain: epoch  7, batch    12 | loss: 4.0959148Losses:  4.017197608947754 0.0515822097659111
CurrentTrain: epoch  7, batch    13 | loss: 4.0687799Losses:  4.008452415466309 0.09979742765426636
CurrentTrain: epoch  7, batch    14 | loss: 4.1082497Losses:  4.072591304779053 0.05509781092405319
CurrentTrain: epoch  7, batch    15 | loss: 4.1276889Losses:  4.02009916305542 0.06578849256038666
CurrentTrain: epoch  7, batch    16 | loss: 4.0858874Losses:  4.020282745361328 0.1528404951095581
CurrentTrain: epoch  7, batch    17 | loss: 4.1731234Losses:  4.013925075531006 0.08522287011146545
CurrentTrain: epoch  7, batch    18 | loss: 4.0991478Losses:  4.002306938171387 0.08756942301988602
CurrentTrain: epoch  7, batch    19 | loss: 4.0898762Losses:  4.020126819610596 0.17412802577018738
CurrentTrain: epoch  7, batch    20 | loss: 4.1942549Losses:  4.0539655685424805 0.07374653220176697
CurrentTrain: epoch  7, batch    21 | loss: 4.1277122Losses:  4.0017595291137695 0.07924710214138031
CurrentTrain: epoch  7, batch    22 | loss: 4.0810065Losses:  3.968297004699707 0.0770363062620163
CurrentTrain: epoch  7, batch    23 | loss: 4.0453334Losses:  4.002584934234619 0.10502137243747711
CurrentTrain: epoch  7, batch    24 | loss: 4.1076064Losses:  4.0085225105285645 0.09948694705963135
CurrentTrain: epoch  7, batch    25 | loss: 4.1080093Losses:  4.017260551452637 0.07032635807991028
CurrentTrain: epoch  7, batch    26 | loss: 4.0875869Losses:  3.96720290184021 0.06900203227996826
CurrentTrain: epoch  7, batch    27 | loss: 4.0362048Losses:  4.038549423217773 0.1851169615983963
CurrentTrain: epoch  7, batch    28 | loss: 4.2236662Losses:  4.008561134338379 0.05703166872262955
CurrentTrain: epoch  7, batch    29 | loss: 4.0655928Losses:  3.9872307777404785 0.11179951578378677
CurrentTrain: epoch  7, batch    30 | loss: 4.0990305Losses:  3.976470470428467 0.07199352234601974
CurrentTrain: epoch  7, batch    31 | loss: 4.0484638Losses:  3.986219882965088 0.09236054122447968
CurrentTrain: epoch  7, batch    32 | loss: 4.0785804Losses:  3.987729072570801 0.06310683488845825
CurrentTrain: epoch  7, batch    33 | loss: 4.0508361Losses:  3.9766616821289062 0.07063907384872437
CurrentTrain: epoch  7, batch    34 | loss: 4.0473008Losses:  4.007768154144287 0.1389484703540802
CurrentTrain: epoch  7, batch    35 | loss: 4.1467166Losses:  3.9355666637420654 0.07903525233268738
CurrentTrain: epoch  7, batch    36 | loss: 4.0146017Losses:  3.989203929901123 0.12236902862787247
CurrentTrain: epoch  7, batch    37 | loss: 4.1115727Losses:  3.9688568115234375 0.060049593448638916
CurrentTrain: epoch  7, batch    38 | loss: 4.0289063Losses:  4.035328388214111 0.07633864134550095
CurrentTrain: epoch  7, batch    39 | loss: 4.1116672Losses:  4.04614782333374 0.06137978285551071
CurrentTrain: epoch  7, batch    40 | loss: 4.1075277Losses:  3.990351676940918 0.0850377231836319
CurrentTrain: epoch  7, batch    41 | loss: 4.0753894Losses:  4.001800060272217 0.06952447444200516
CurrentTrain: epoch  7, batch    42 | loss: 4.0713243Losses:  4.036590576171875 0.05331994593143463
CurrentTrain: epoch  7, batch    43 | loss: 4.0899105Losses:  3.998199939727783 0.04691231995820999
CurrentTrain: epoch  7, batch    44 | loss: 4.0451121Losses:  3.993830680847168 0.10921956598758698
CurrentTrain: epoch  7, batch    45 | loss: 4.1030502Losses:  3.962954521179199 0.1478494107723236
CurrentTrain: epoch  7, batch    46 | loss: 4.1108041Losses:  4.025265693664551 0.11081983149051666
CurrentTrain: epoch  7, batch    47 | loss: 4.1360855Losses:  3.980132579803467 0.13784128427505493
CurrentTrain: epoch  7, batch    48 | loss: 4.1179738Losses:  3.9817352294921875 0.04917585477232933
CurrentTrain: epoch  7, batch    49 | loss: 4.0309110Losses:  3.9679393768310547 0.07494288682937622
CurrentTrain: epoch  7, batch    50 | loss: 4.0428824Losses:  3.992259979248047 0.04544847458600998
CurrentTrain: epoch  7, batch    51 | loss: 4.0377083Losses:  3.9925379753112793 0.07848658412694931
CurrentTrain: epoch  7, batch    52 | loss: 4.0710244Losses:  4.008230209350586 0.10816370695829391
CurrentTrain: epoch  7, batch    53 | loss: 4.1163940Losses:  4.006274700164795 0.06663233041763306
CurrentTrain: epoch  7, batch    54 | loss: 4.0729070Losses:  3.9810948371887207 0.1803055703639984
CurrentTrain: epoch  7, batch    55 | loss: 4.1614003Losses:  4.0619306564331055 0.06032556667923927
CurrentTrain: epoch  7, batch    56 | loss: 4.1222563Losses:  4.006453514099121 0.11785580217838287
CurrentTrain: epoch  7, batch    57 | loss: 4.1243095Losses:  4.040956497192383 0.09969212859869003
CurrentTrain: epoch  7, batch    58 | loss: 4.1406488Losses:  4.034108638763428 0.09486328810453415
CurrentTrain: epoch  7, batch    59 | loss: 4.1289721Losses:  4.041111946105957 0.14904235303401947
CurrentTrain: epoch  7, batch    60 | loss: 4.1901541Losses:  4.046222686767578 0.02770126797258854
CurrentTrain: epoch  7, batch    61 | loss: 4.0739241Losses:  3.9939236640930176 0.12727773189544678
CurrentTrain: epoch  7, batch    62 | loss: 4.1212015Losses:  4.042292594909668 0.07015769183635712
CurrentTrain: epoch  7, batch    63 | loss: 4.1124501Losses:  3.9237804412841797 0.04235832393169403
CurrentTrain: epoch  7, batch    64 | loss: 3.9661388Losses:  4.036129474639893 0.13417461514472961
CurrentTrain: epoch  7, batch    65 | loss: 4.1703043Losses:  3.9825358390808105 0.10320234298706055
CurrentTrain: epoch  7, batch    66 | loss: 4.0857382Losses:  4.022927284240723 0.12886714935302734
CurrentTrain: epoch  7, batch    67 | loss: 4.1517944Losses:  4.023479461669922 0.057390764355659485
CurrentTrain: epoch  7, batch    68 | loss: 4.0808702Losses:  3.978200912475586 0.07835935056209564
CurrentTrain: epoch  7, batch    69 | loss: 4.0565600Losses:  3.9734280109405518 0.07722998410463333
CurrentTrain: epoch  7, batch    70 | loss: 4.0506582Losses:  4.074249267578125 0.04806266725063324
CurrentTrain: epoch  7, batch    71 | loss: 4.1223121Losses:  3.9211201667785645 0.09120054543018341
CurrentTrain: epoch  7, batch    72 | loss: 4.0123205Losses:  3.9185376167297363 0.1519218385219574
CurrentTrain: epoch  7, batch    73 | loss: 4.0704594Losses:  4.0093793869018555 0.12281866371631622
CurrentTrain: epoch  7, batch    74 | loss: 4.1321979Losses:  3.9994921684265137 0.07786602526903152
CurrentTrain: epoch  7, batch    75 | loss: 4.0773582Losses:  3.9351649284362793 0.02989555150270462
CurrentTrain: epoch  7, batch    76 | loss: 3.9650605Losses:  4.013657569885254 0.07108631730079651
CurrentTrain: epoch  7, batch    77 | loss: 4.0847440Losses:  3.938892126083374 0.06044425442814827
CurrentTrain: epoch  7, batch    78 | loss: 3.9993365Losses:  3.9951257705688477 0.10025735199451447
CurrentTrain: epoch  7, batch    79 | loss: 4.0953832Losses:  3.986630439758301 0.0726594626903534
CurrentTrain: epoch  7, batch    80 | loss: 4.0592899Losses:  3.9555559158325195 0.0341605544090271
CurrentTrain: epoch  7, batch    81 | loss: 3.9897165Losses:  4.043566703796387 0.07623936235904694
CurrentTrain: epoch  7, batch    82 | loss: 4.1198063Losses:  3.988715648651123 0.08077260851860046
CurrentTrain: epoch  7, batch    83 | loss: 4.0694880Losses:  3.982820510864258 0.10277840495109558
CurrentTrain: epoch  7, batch    84 | loss: 4.0855989Losses:  4.082144737243652 0.10016609728336334
CurrentTrain: epoch  7, batch    85 | loss: 4.1823111Losses:  4.038382530212402 0.05997876822948456
CurrentTrain: epoch  7, batch    86 | loss: 4.0983615Losses:  3.997333526611328 0.0896812230348587
CurrentTrain: epoch  7, batch    87 | loss: 4.0870147Losses:  3.9861598014831543 0.12090784311294556
CurrentTrain: epoch  7, batch    88 | loss: 4.1070676Losses:  3.9783878326416016 0.06320740282535553
CurrentTrain: epoch  7, batch    89 | loss: 4.0415955Losses:  3.924506902694702 0.08451618254184723
CurrentTrain: epoch  7, batch    90 | loss: 4.0090232Losses:  4.027325630187988 0.10958793014287949
CurrentTrain: epoch  7, batch    91 | loss: 4.1369138Losses:  4.019477367401123 0.08175142854452133
CurrentTrain: epoch  7, batch    92 | loss: 4.1012287Losses:  3.9994311332702637 0.0589146763086319
CurrentTrain: epoch  7, batch    93 | loss: 4.0583458Losses:  3.98130464553833 0.05945904180407524
CurrentTrain: epoch  7, batch    94 | loss: 4.0407639Losses:  4.050751686096191 0.06600239872932434
CurrentTrain: epoch  7, batch    95 | loss: 4.1167541Losses:  3.9848155975341797 0.11367061734199524
CurrentTrain: epoch  7, batch    96 | loss: 4.0984864Losses:  4.061508655548096 0.07727426290512085
CurrentTrain: epoch  7, batch    97 | loss: 4.1387830Losses:  4.000426769256592 0.08822999149560928
CurrentTrain: epoch  7, batch    98 | loss: 4.0886569Losses:  4.031725883483887 0.08750110864639282
CurrentTrain: epoch  7, batch    99 | loss: 4.1192269Losses:  4.031787872314453 0.044053930789232254
CurrentTrain: epoch  7, batch   100 | loss: 4.0758419Losses:  4.009891510009766 0.11111652851104736
CurrentTrain: epoch  7, batch   101 | loss: 4.1210079Losses:  4.002437591552734 0.08303642272949219
CurrentTrain: epoch  7, batch   102 | loss: 4.0854740Losses:  4.048335075378418 0.09436337649822235
CurrentTrain: epoch  7, batch   103 | loss: 4.1426983Losses:  4.027407646179199 0.11339513957500458
CurrentTrain: epoch  7, batch   104 | loss: 4.1408029Losses:  4.043861389160156 0.1108635812997818
CurrentTrain: epoch  7, batch   105 | loss: 4.1547251Losses:  4.007103443145752 0.13806948065757751
CurrentTrain: epoch  7, batch   106 | loss: 4.1451731Losses:  3.9416468143463135 0.09888438135385513
CurrentTrain: epoch  7, batch   107 | loss: 4.0405312Losses:  3.9620840549468994 0.11490383744239807
CurrentTrain: epoch  7, batch   108 | loss: 4.0769877Losses:  3.975429058074951 0.0720595270395279
CurrentTrain: epoch  7, batch   109 | loss: 4.0474887Losses:  4.008293628692627 0.0754375010728836
CurrentTrain: epoch  7, batch   110 | loss: 4.0837312Losses:  4.005575180053711 0.09991054236888885
CurrentTrain: epoch  7, batch   111 | loss: 4.1054859Losses:  3.9975807666778564 0.11820034682750702
CurrentTrain: epoch  7, batch   112 | loss: 4.1157813Losses:  4.002635955810547 0.1234896332025528
CurrentTrain: epoch  7, batch   113 | loss: 4.1261258Losses:  4.040978908538818 0.14515523612499237
CurrentTrain: epoch  7, batch   114 | loss: 4.1861343Losses:  3.96573543548584 0.12137039750814438
CurrentTrain: epoch  7, batch   115 | loss: 4.0871058Losses:  3.973336696624756 0.06849060952663422
CurrentTrain: epoch  7, batch   116 | loss: 4.0418272Losses:  4.024079322814941 0.12434948980808258
CurrentTrain: epoch  7, batch   117 | loss: 4.1484289Losses:  3.9640746116638184 0.07751345634460449
CurrentTrain: epoch  7, batch   118 | loss: 4.0415878Losses:  3.949922561645508 0.09451645612716675
CurrentTrain: epoch  7, batch   119 | loss: 4.0444388Losses:  4.003135681152344 0.06113851070404053
CurrentTrain: epoch  7, batch   120 | loss: 4.0642743Losses:  4.009006500244141 0.1044907346367836
CurrentTrain: epoch  7, batch   121 | loss: 4.1134973Losses:  3.9964332580566406 0.09410113096237183
CurrentTrain: epoch  7, batch   122 | loss: 4.0905342Losses:  4.009308815002441 0.09829218685626984
CurrentTrain: epoch  7, batch   123 | loss: 4.1076012Losses:  3.9721927642822266 0.08472539484500885
CurrentTrain: epoch  7, batch   124 | loss: 4.0569181Losses:  3.999983787536621 0.06843151152133942
CurrentTrain: epoch  8, batch     0 | loss: 4.0684152Losses:  3.9725048542022705 0.06753799319267273
CurrentTrain: epoch  8, batch     1 | loss: 4.0400429Losses:  4.0187764167785645 0.07804101705551147
CurrentTrain: epoch  8, batch     2 | loss: 4.0968175Losses:  4.0044331550598145 0.07000312954187393
CurrentTrain: epoch  8, batch     3 | loss: 4.0744362Losses:  3.9569473266601562 0.1063569188117981
CurrentTrain: epoch  8, batch     4 | loss: 4.0633044Losses:  3.966686487197876 0.09297744184732437
CurrentTrain: epoch  8, batch     5 | loss: 4.0596638Losses:  3.9940195083618164 0.10495806485414505
CurrentTrain: epoch  8, batch     6 | loss: 4.0989776Losses:  3.9848105907440186 0.09214510768651962
CurrentTrain: epoch  8, batch     7 | loss: 4.0769558Losses:  4.039795398712158 0.05389763414859772
CurrentTrain: epoch  8, batch     8 | loss: 4.0936933Losses:  4.025914192199707 0.06805720925331116
CurrentTrain: epoch  8, batch     9 | loss: 4.0939713Losses:  4.0067644119262695 0.10572397708892822
CurrentTrain: epoch  8, batch    10 | loss: 4.1124883Losses:  3.8952741622924805 0.0272219181060791
CurrentTrain: epoch  8, batch    11 | loss: 3.9224961Losses:  3.9926981925964355 0.14214451611042023
CurrentTrain: epoch  8, batch    12 | loss: 4.1348429Losses:  3.992374897003174 0.04493219032883644
CurrentTrain: epoch  8, batch    13 | loss: 4.0373073Losses:  3.934157371520996 0.10265051573514938
CurrentTrain: epoch  8, batch    14 | loss: 4.0368080Losses:  3.990434169769287 0.1509842574596405
CurrentTrain: epoch  8, batch    15 | loss: 4.1414185Losses:  4.041927337646484 0.05929332971572876
CurrentTrain: epoch  8, batch    16 | loss: 4.1012206Losses:  3.9212708473205566 0.08965116739273071
CurrentTrain: epoch  8, batch    17 | loss: 4.0109220Losses:  3.9567489624023438 0.10715441405773163
CurrentTrain: epoch  8, batch    18 | loss: 4.0639033Losses:  3.9357786178588867 0.04279588907957077
CurrentTrain: epoch  8, batch    19 | loss: 3.9785745Losses:  4.032791614532471 0.06350822001695633
CurrentTrain: epoch  8, batch    20 | loss: 4.0962996Losses:  4.046185493469238 0.06567453593015671
CurrentTrain: epoch  8, batch    21 | loss: 4.1118598Losses:  3.94948410987854 0.07712548971176147
CurrentTrain: epoch  8, batch    22 | loss: 4.0266094Losses:  4.014056205749512 0.11003870517015457
CurrentTrain: epoch  8, batch    23 | loss: 4.1240950Losses:  3.9951486587524414 0.14160823822021484
CurrentTrain: epoch  8, batch    24 | loss: 4.1367569Losses:  4.037700653076172 0.08268065005540848
CurrentTrain: epoch  8, batch    25 | loss: 4.1203814Losses:  4.006010055541992 0.05813244730234146
CurrentTrain: epoch  8, batch    26 | loss: 4.0641427Losses:  3.984151601791382 0.09378424286842346
CurrentTrain: epoch  8, batch    27 | loss: 4.0779357Losses:  3.941636562347412 0.04056093469262123
CurrentTrain: epoch  8, batch    28 | loss: 3.9821975Losses:  3.9632725715637207 0.15979361534118652
CurrentTrain: epoch  8, batch    29 | loss: 4.1230659Losses:  4.020371437072754 0.1059207022190094
CurrentTrain: epoch  8, batch    30 | loss: 4.1262922Losses:  3.98819637298584 0.06892260909080505
CurrentTrain: epoch  8, batch    31 | loss: 4.0571189Losses:  3.991748571395874 0.12424400448799133
CurrentTrain: epoch  8, batch    32 | loss: 4.1159925Losses:  3.982530117034912 0.04338657855987549
CurrentTrain: epoch  8, batch    33 | loss: 4.0259166Losses:  4.002440452575684 0.07080014795064926
CurrentTrain: epoch  8, batch    34 | loss: 4.0732408Losses:  3.9782443046569824 0.10534119606018066
CurrentTrain: epoch  8, batch    35 | loss: 4.0835857Losses:  3.9375195503234863 0.11017006635665894
CurrentTrain: epoch  8, batch    36 | loss: 4.0476894Losses:  3.966209888458252 0.10130081325769424
CurrentTrain: epoch  8, batch    37 | loss: 4.0675106Losses:  3.9394278526306152 0.1120116114616394
CurrentTrain: epoch  8, batch    38 | loss: 4.0514393Losses:  3.9544734954833984 0.14068305492401123
CurrentTrain: epoch  8, batch    39 | loss: 4.0951567Losses:  3.9859700202941895 0.05674954876303673
CurrentTrain: epoch  8, batch    40 | loss: 4.0427194Losses:  3.980720043182373 0.057001225650310516
CurrentTrain: epoch  8, batch    41 | loss: 4.0377212Losses:  3.990032196044922 0.07523559033870697
CurrentTrain: epoch  8, batch    42 | loss: 4.0652676Losses:  4.048053741455078 0.050730083137750626
CurrentTrain: epoch  8, batch    43 | loss: 4.0987840Losses:  4.013269424438477 0.07881594449281693
CurrentTrain: epoch  8, batch    44 | loss: 4.0920854Losses:  3.967372417449951 0.14100906252861023
CurrentTrain: epoch  8, batch    45 | loss: 4.1083813Losses:  4.040666103363037 0.06545516848564148
CurrentTrain: epoch  8, batch    46 | loss: 4.1061211Losses:  3.9719038009643555 0.1422324776649475
CurrentTrain: epoch  8, batch    47 | loss: 4.1141362Losses:  3.973599433898926 0.12172605097293854
CurrentTrain: epoch  8, batch    48 | loss: 4.0953255Losses:  4.064546585083008 0.0650925263762474
CurrentTrain: epoch  8, batch    49 | loss: 4.1296391Losses:  4.007814407348633 0.08038260787725449
CurrentTrain: epoch  8, batch    50 | loss: 4.0881972Losses:  3.9688172340393066 0.09317471086978912
CurrentTrain: epoch  8, batch    51 | loss: 4.0619922Losses:  4.021742343902588 0.07040873169898987
CurrentTrain: epoch  8, batch    52 | loss: 4.0921512Losses:  3.9563026428222656 0.062431737780570984
CurrentTrain: epoch  8, batch    53 | loss: 4.0187345Losses:  3.986781120300293 0.08307351917028427
CurrentTrain: epoch  8, batch    54 | loss: 4.0698547Losses:  4.038937091827393 0.08178981393575668
CurrentTrain: epoch  8, batch    55 | loss: 4.1207271Losses:  4.046535491943359 0.0671958178281784
CurrentTrain: epoch  8, batch    56 | loss: 4.1137314Losses:  3.9952988624572754 0.11309722065925598
CurrentTrain: epoch  8, batch    57 | loss: 4.1083961Losses:  3.9508886337280273 0.04290996864438057
CurrentTrain: epoch  8, batch    58 | loss: 3.9937985Losses:  4.052412509918213 0.04445364326238632
CurrentTrain: epoch  8, batch    59 | loss: 4.0968661Losses:  3.946544647216797 0.10569671541452408
CurrentTrain: epoch  8, batch    60 | loss: 4.0522413Losses:  4.002859115600586 0.06583698838949203
CurrentTrain: epoch  8, batch    61 | loss: 4.0686960Losses:  4.011326313018799 0.10103317350149155
CurrentTrain: epoch  8, batch    62 | loss: 4.1123595Losses:  4.005302906036377 0.07671840488910675
CurrentTrain: epoch  8, batch    63 | loss: 4.0820212Losses:  3.9514288902282715 0.05293557792901993
CurrentTrain: epoch  8, batch    64 | loss: 4.0043645Losses:  4.000519752502441 0.09618708491325378
CurrentTrain: epoch  8, batch    65 | loss: 4.0967069Losses:  3.9846878051757812 0.06558539718389511
CurrentTrain: epoch  8, batch    66 | loss: 4.0502734Losses:  3.918961524963379 0.12629485130310059
CurrentTrain: epoch  8, batch    67 | loss: 4.0452566Losses:  3.975515365600586 0.11889857053756714
CurrentTrain: epoch  8, batch    68 | loss: 4.0944138Losses:  3.9564051628112793 0.0474172905087471
CurrentTrain: epoch  8, batch    69 | loss: 4.0038223Losses:  3.997973918914795 0.07536858320236206
CurrentTrain: epoch  8, batch    70 | loss: 4.0733423Losses:  4.007486343383789 0.049154795706272125
CurrentTrain: epoch  8, batch    71 | loss: 4.0566411Losses:  3.999863862991333 0.10562972724437714
CurrentTrain: epoch  8, batch    72 | loss: 4.1054935Losses:  4.006074905395508 0.08604734390974045
CurrentTrain: epoch  8, batch    73 | loss: 4.0921221Losses:  3.9854745864868164 0.11604107171297073
CurrentTrain: epoch  8, batch    74 | loss: 4.1015158Losses:  3.9858717918395996 0.06608586758375168
CurrentTrain: epoch  8, batch    75 | loss: 4.0519576Losses:  3.9547159671783447 0.023023899644613266
CurrentTrain: epoch  8, batch    76 | loss: 3.9777398Losses:  3.9472830295562744 0.10101215541362762
CurrentTrain: epoch  8, batch    77 | loss: 4.0482950Losses:  3.999662399291992 0.10974699258804321
CurrentTrain: epoch  8, batch    78 | loss: 4.1094093Losses:  3.9639036655426025 0.10975944250822067
CurrentTrain: epoch  8, batch    79 | loss: 4.0736632Losses:  3.970334529876709 0.05362824350595474
CurrentTrain: epoch  8, batch    80 | loss: 4.0239630Losses:  3.9972734451293945 0.07073260843753815
CurrentTrain: epoch  8, batch    81 | loss: 4.0680060Losses:  3.996657133102417 0.10887787491083145
CurrentTrain: epoch  8, batch    82 | loss: 4.1055350Losses:  3.9152028560638428 0.04826244339346886
CurrentTrain: epoch  8, batch    83 | loss: 3.9634652Losses:  3.9940154552459717 0.04826304689049721
CurrentTrain: epoch  8, batch    84 | loss: 4.0422783Losses:  3.986398935317993 0.10490119457244873
CurrentTrain: epoch  8, batch    85 | loss: 4.0913000Losses:  3.903337240219116 0.13649903237819672
CurrentTrain: epoch  8, batch    86 | loss: 4.0398364Losses:  4.019067287445068 0.08457661420106888
CurrentTrain: epoch  8, batch    87 | loss: 4.1036439Losses:  4.039941310882568 0.09895255416631699
CurrentTrain: epoch  8, batch    88 | loss: 4.1388941Losses:  4.050844192504883 0.14416998624801636
CurrentTrain: epoch  8, batch    89 | loss: 4.1950140Losses:  3.971006393432617 0.10344603657722473
CurrentTrain: epoch  8, batch    90 | loss: 4.0744524Losses:  3.95574688911438 0.08519210666418076
CurrentTrain: epoch  8, batch    91 | loss: 4.0409389Losses:  3.987639904022217 0.10835714638233185
CurrentTrain: epoch  8, batch    92 | loss: 4.0959969Losses:  3.964350700378418 0.06800884008407593
CurrentTrain: epoch  8, batch    93 | loss: 4.0323596Losses:  3.970099449157715 0.04367928206920624
CurrentTrain: epoch  8, batch    94 | loss: 4.0137787Losses:  4.011495590209961 0.06148184835910797
CurrentTrain: epoch  8, batch    95 | loss: 4.0729775Losses:  3.9564409255981445 0.0595729798078537
CurrentTrain: epoch  8, batch    96 | loss: 4.0160141Losses:  3.9846668243408203 0.07537383586168289
CurrentTrain: epoch  8, batch    97 | loss: 4.0600405Losses:  3.9690723419189453 0.0722743421792984
CurrentTrain: epoch  8, batch    98 | loss: 4.0413465Losses:  3.9041054248809814 0.11069117486476898
CurrentTrain: epoch  8, batch    99 | loss: 4.0147967Losses:  3.9694936275482178 0.08896301686763763
CurrentTrain: epoch  8, batch   100 | loss: 4.0584564Losses:  3.9809656143188477 0.059970561414957047
CurrentTrain: epoch  8, batch   101 | loss: 4.0409360Losses:  4.069697380065918 0.0668787807226181
CurrentTrain: epoch  8, batch   102 | loss: 4.1365762Losses:  3.9612624645233154 0.09186454862356186
CurrentTrain: epoch  8, batch   103 | loss: 4.0531268Losses:  4.0167436599731445 0.08535243570804596
CurrentTrain: epoch  8, batch   104 | loss: 4.1020961Losses:  3.9797916412353516 0.0858783945441246
CurrentTrain: epoch  8, batch   105 | loss: 4.0656700Losses:  4.011919975280762 0.09129424393177032
CurrentTrain: epoch  8, batch   106 | loss: 4.1032143Losses:  3.973660945892334 0.14393460750579834
CurrentTrain: epoch  8, batch   107 | loss: 4.1175957Losses:  3.985553503036499 0.06987299025058746
CurrentTrain: epoch  8, batch   108 | loss: 4.0554266Losses:  3.976234197616577 0.06284380704164505
CurrentTrain: epoch  8, batch   109 | loss: 4.0390782Losses:  3.933053970336914 0.034586966037750244
CurrentTrain: epoch  8, batch   110 | loss: 3.9676409Losses:  3.9971442222595215 0.06756232678890228
CurrentTrain: epoch  8, batch   111 | loss: 4.0647063Losses:  3.9197707176208496 0.09428070485591888
CurrentTrain: epoch  8, batch   112 | loss: 4.0140514Losses:  3.971228837966919 0.08613698184490204
CurrentTrain: epoch  8, batch   113 | loss: 4.0573659Losses:  4.044306755065918 0.06077442318201065
CurrentTrain: epoch  8, batch   114 | loss: 4.1050811Losses:  3.940408706665039 0.09478527307510376
CurrentTrain: epoch  8, batch   115 | loss: 4.0351939Losses:  3.95597505569458 0.07105080038309097
CurrentTrain: epoch  8, batch   116 | loss: 4.0270257Losses:  3.9044137001037598 0.06985162198543549
CurrentTrain: epoch  8, batch   117 | loss: 3.9742653Losses:  4.004671096801758 0.0744066834449768
CurrentTrain: epoch  8, batch   118 | loss: 4.0790777Losses:  3.9876255989074707 0.11940334737300873
CurrentTrain: epoch  8, batch   119 | loss: 4.1070290Losses:  4.003928184509277 0.06181800365447998
CurrentTrain: epoch  8, batch   120 | loss: 4.0657463Losses:  3.956144094467163 0.1326020359992981
CurrentTrain: epoch  8, batch   121 | loss: 4.0887461Losses:  3.9178218841552734 0.08475201576948166
CurrentTrain: epoch  8, batch   122 | loss: 4.0025740Losses:  3.9868457317352295 0.06168939173221588
CurrentTrain: epoch  8, batch   123 | loss: 4.0485353Losses:  3.965716600418091 0.07256709039211273
CurrentTrain: epoch  8, batch   124 | loss: 4.0382838Losses:  3.957364559173584 0.11853277683258057
CurrentTrain: epoch  9, batch     0 | loss: 4.0758972Losses:  3.952026844024658 0.12452743947505951
CurrentTrain: epoch  9, batch     1 | loss: 4.0765543Losses:  3.9697327613830566 0.06145687773823738
CurrentTrain: epoch  9, batch     2 | loss: 4.0311894Losses:  3.977203607559204 0.08081412315368652
CurrentTrain: epoch  9, batch     3 | loss: 4.0580177Losses:  3.9523260593414307 0.039918169379234314
CurrentTrain: epoch  9, batch     4 | loss: 3.9922442Losses:  3.944106101989746 0.04867659881711006
CurrentTrain: epoch  9, batch     5 | loss: 3.9927826Losses:  3.977393865585327 0.054756514728069305
CurrentTrain: epoch  9, batch     6 | loss: 4.0321503Losses:  3.9958767890930176 0.08674953132867813
CurrentTrain: epoch  9, batch     7 | loss: 4.0826263Losses:  4.0017924308776855 0.06342677772045135
CurrentTrain: epoch  9, batch     8 | loss: 4.0652194Losses:  4.0171918869018555 0.10934421420097351
CurrentTrain: epoch  9, batch     9 | loss: 4.1265359Losses:  4.0050272941589355 0.08546832203865051
CurrentTrain: epoch  9, batch    10 | loss: 4.0904956Losses:  3.9136600494384766 0.06547284126281738
CurrentTrain: epoch  9, batch    11 | loss: 3.9791329Losses:  3.983140468597412 0.05474696308374405
CurrentTrain: epoch  9, batch    12 | loss: 4.0378876Losses:  4.0050249099731445 0.0952402725815773
CurrentTrain: epoch  9, batch    13 | loss: 4.1002650Losses:  3.9612860679626465 0.09003794193267822
CurrentTrain: epoch  9, batch    14 | loss: 4.0513239Losses:  3.991821765899658 0.118071548640728
CurrentTrain: epoch  9, batch    15 | loss: 4.1098933Losses:  4.03753662109375 0.04094589874148369
CurrentTrain: epoch  9, batch    16 | loss: 4.0784826Losses:  3.964418649673462 0.057131603360176086
CurrentTrain: epoch  9, batch    17 | loss: 4.0215502Losses:  3.974905014038086 0.05838035047054291
CurrentTrain: epoch  9, batch    18 | loss: 4.0332851Losses:  3.9617443084716797 0.08201590925455093
CurrentTrain: epoch  9, batch    19 | loss: 4.0437603Losses:  3.979012966156006 0.08390122652053833
CurrentTrain: epoch  9, batch    20 | loss: 4.0629144Losses:  3.9767439365386963 0.06101951748132706
CurrentTrain: epoch  9, batch    21 | loss: 4.0377636Losses:  4.016766548156738 0.0614420548081398
CurrentTrain: epoch  9, batch    22 | loss: 4.0782084Losses:  3.995112895965576 0.0888349711894989
CurrentTrain: epoch  9, batch    23 | loss: 4.0839477Losses:  4.009110450744629 0.06490861624479294
CurrentTrain: epoch  9, batch    24 | loss: 4.0740190Losses:  3.9710702896118164 0.04726948216557503
CurrentTrain: epoch  9, batch    25 | loss: 4.0183396Losses:  3.968677043914795 0.0750960111618042
CurrentTrain: epoch  9, batch    26 | loss: 4.0437732Losses:  3.9740586280822754 0.0989018902182579
CurrentTrain: epoch  9, batch    27 | loss: 4.0729604Losses:  3.9594168663024902 0.08391482383012772
CurrentTrain: epoch  9, batch    28 | loss: 4.0433316Losses:  3.9838409423828125 0.06030444800853729
CurrentTrain: epoch  9, batch    29 | loss: 4.0441456Losses:  3.975486993789673 0.09509119391441345
CurrentTrain: epoch  9, batch    30 | loss: 4.0705781Losses:  3.975472927093506 0.09361188113689423
CurrentTrain: epoch  9, batch    31 | loss: 4.0690846Losses:  3.999096155166626 0.07284155488014221
CurrentTrain: epoch  9, batch    32 | loss: 4.0719376Losses:  3.9537248611450195 0.07628843188285828
CurrentTrain: epoch  9, batch    33 | loss: 4.0300131Losses:  3.942185640335083 0.08628193289041519
CurrentTrain: epoch  9, batch    34 | loss: 4.0284677Losses:  3.9803309440612793 0.06849728524684906
CurrentTrain: epoch  9, batch    35 | loss: 4.0488281Losses:  3.9254136085510254 0.07487475872039795
CurrentTrain: epoch  9, batch    36 | loss: 4.0002885Losses:  3.931013584136963 0.049281664192676544
CurrentTrain: epoch  9, batch    37 | loss: 3.9802952Losses:  3.989908218383789 0.1122799888253212
CurrentTrain: epoch  9, batch    38 | loss: 4.1021881Losses:  4.0062575340271 0.04927321523427963
CurrentTrain: epoch  9, batch    39 | loss: 4.0555305Losses:  3.8720715045928955 0.13690000772476196
CurrentTrain: epoch  9, batch    40 | loss: 4.0089717Losses:  3.9458861351013184 0.07771677523851395
CurrentTrain: epoch  9, batch    41 | loss: 4.0236030Losses:  3.9938488006591797 0.09533294290304184
CurrentTrain: epoch  9, batch    42 | loss: 4.0891819Losses:  3.998260736465454 0.0956563875079155
CurrentTrain: epoch  9, batch    43 | loss: 4.0939169Losses:  3.9542970657348633 0.10681729018688202
CurrentTrain: epoch  9, batch    44 | loss: 4.0611143Losses:  3.9957151412963867 0.10406801849603653
CurrentTrain: epoch  9, batch    45 | loss: 4.0997829Losses:  3.936776638031006 0.06764774024486542
CurrentTrain: epoch  9, batch    46 | loss: 4.0044246Losses:  3.996164321899414 0.049618199467659
CurrentTrain: epoch  9, batch    47 | loss: 4.0457826Losses:  3.95241641998291 0.1245269775390625
CurrentTrain: epoch  9, batch    48 | loss: 4.0769434Losses:  3.9055914878845215 0.07437177747488022
CurrentTrain: epoch  9, batch    49 | loss: 3.9799633Losses:  4.000694274902344 0.07760031521320343
CurrentTrain: epoch  9, batch    50 | loss: 4.0782948Losses:  4.003566265106201 0.06456349045038223
CurrentTrain: epoch  9, batch    51 | loss: 4.0681295Losses:  3.947343587875366 0.094655841588974
CurrentTrain: epoch  9, batch    52 | loss: 4.0419993Losses:  3.984793186187744 0.1041029617190361
CurrentTrain: epoch  9, batch    53 | loss: 4.0888963Losses:  3.987779378890991 0.1103711724281311
CurrentTrain: epoch  9, batch    54 | loss: 4.0981507Losses:  3.911630153656006 0.05120064690709114
CurrentTrain: epoch  9, batch    55 | loss: 3.9628308Losses:  3.9628443717956543 0.09070515632629395
CurrentTrain: epoch  9, batch    56 | loss: 4.0535498Losses:  3.9483227729797363 0.10397326201200485
CurrentTrain: epoch  9, batch    57 | loss: 4.0522962Losses:  4.023899555206299 0.07932444661855698
CurrentTrain: epoch  9, batch    58 | loss: 4.1032238Losses:  3.9485080242156982 0.08008599281311035
CurrentTrain: epoch  9, batch    59 | loss: 4.0285940Losses:  3.9236369132995605 0.039372269064188004
CurrentTrain: epoch  9, batch    60 | loss: 3.9630091Losses:  3.9655051231384277 0.07723835110664368
CurrentTrain: epoch  9, batch    61 | loss: 4.0427437Losses:  3.9750876426696777 0.046188924461603165
CurrentTrain: epoch  9, batch    62 | loss: 4.0212765Losses:  3.9719126224517822 0.062107425183057785
CurrentTrain: epoch  9, batch    63 | loss: 4.0340199Losses:  3.9789175987243652 0.026696225628256798
CurrentTrain: epoch  9, batch    64 | loss: 4.0056138Losses:  3.969940423965454 0.06845846772193909
CurrentTrain: epoch  9, batch    65 | loss: 4.0383987Losses:  3.986469268798828 0.022635893896222115
CurrentTrain: epoch  9, batch    66 | loss: 4.0091052Losses:  4.000729560852051 0.04128832370042801
CurrentTrain: epoch  9, batch    67 | loss: 4.0420179Losses:  3.9683682918548584 0.06737393140792847
CurrentTrain: epoch  9, batch    68 | loss: 4.0357423Losses:  3.973606824874878 0.09376546740531921
CurrentTrain: epoch  9, batch    69 | loss: 4.0673723Losses:  3.9494636058807373 0.14197295904159546
CurrentTrain: epoch  9, batch    70 | loss: 4.0914364Losses:  4.015758991241455 0.06391870975494385
CurrentTrain: epoch  9, batch    71 | loss: 4.0796776Losses:  3.989192485809326 0.05102599412202835
CurrentTrain: epoch  9, batch    72 | loss: 4.0402184Losses:  3.978069305419922 0.14297597110271454
CurrentTrain: epoch  9, batch    73 | loss: 4.1210451Losses:  3.9285387992858887 0.03127726912498474
CurrentTrain: epoch  9, batch    74 | loss: 3.9598160Losses:  3.9870729446411133 0.0787474662065506
CurrentTrain: epoch  9, batch    75 | loss: 4.0658202Losses:  4.013376235961914 0.09362020343542099
CurrentTrain: epoch  9, batch    76 | loss: 4.1069965Losses:  3.92293643951416 0.03380546718835831
CurrentTrain: epoch  9, batch    77 | loss: 3.9567418Losses:  3.98618483543396 0.08157186955213547
CurrentTrain: epoch  9, batch    78 | loss: 4.0677567Losses:  4.007803440093994 0.10036078840494156
CurrentTrain: epoch  9, batch    79 | loss: 4.1081643Losses:  3.934131622314453 0.06744745373725891
CurrentTrain: epoch  9, batch    80 | loss: 4.0015793Losses:  3.9683451652526855 0.09364607185125351
CurrentTrain: epoch  9, batch    81 | loss: 4.0619912Losses:  3.992871046066284 0.08914400637149811
CurrentTrain: epoch  9, batch    82 | loss: 4.0820150Losses:  3.978306770324707 0.06568864732980728
CurrentTrain: epoch  9, batch    83 | loss: 4.0439954Losses:  4.051159381866455 0.04749329015612602
CurrentTrain: epoch  9, batch    84 | loss: 4.0986528Losses:  3.9623324871063232 0.09309787303209305
CurrentTrain: epoch  9, batch    85 | loss: 4.0554304Losses:  3.9535319805145264 0.07741342484951019
CurrentTrain: epoch  9, batch    86 | loss: 4.0309453Losses:  3.9581754207611084 0.06712086498737335
CurrentTrain: epoch  9, batch    87 | loss: 4.0252962Losses:  3.989902973175049 0.039564210921525955
CurrentTrain: epoch  9, batch    88 | loss: 4.0294671Losses:  3.9325737953186035 0.09409822523593903
CurrentTrain: epoch  9, batch    89 | loss: 4.0266719Losses:  3.970961809158325 0.07456086575984955
CurrentTrain: epoch  9, batch    90 | loss: 4.0455227Losses:  3.9921631813049316 0.08249125629663467
CurrentTrain: epoch  9, batch    91 | loss: 4.0746546Losses:  3.8595950603485107 0.04905310645699501
CurrentTrain: epoch  9, batch    92 | loss: 3.9086483Losses:  3.9344992637634277 0.049078747630119324
CurrentTrain: epoch  9, batch    93 | loss: 3.9835780Losses:  3.9327595233917236 0.025567982345819473
CurrentTrain: epoch  9, batch    94 | loss: 3.9583275Losses:  3.981250286102295 0.05197632312774658
CurrentTrain: epoch  9, batch    95 | loss: 4.0332265Losses:  3.89510178565979 0.06992185115814209
CurrentTrain: epoch  9, batch    96 | loss: 3.9650235Losses:  3.9246625900268555 0.044479355216026306
CurrentTrain: epoch  9, batch    97 | loss: 3.9691420Losses:  3.914607524871826 0.07282242178916931
CurrentTrain: epoch  9, batch    98 | loss: 3.9874299Losses:  3.947113037109375 0.06840960681438446
CurrentTrain: epoch  9, batch    99 | loss: 4.0155225Losses:  3.928380012512207 0.10650256276130676
CurrentTrain: epoch  9, batch   100 | loss: 4.0348825Losses:  3.96543288230896 0.04207226634025574
CurrentTrain: epoch  9, batch   101 | loss: 4.0075049Losses:  3.9735207557678223 0.09387439489364624
CurrentTrain: epoch  9, batch   102 | loss: 4.0673952Losses:  3.976438045501709 0.13609735667705536
CurrentTrain: epoch  9, batch   103 | loss: 4.1125355Losses:  3.976494312286377 0.09482012689113617
CurrentTrain: epoch  9, batch   104 | loss: 4.0713143Losses:  4.031712532043457 0.06551028788089752
CurrentTrain: epoch  9, batch   105 | loss: 4.0972228Losses:  4.008234024047852 0.08048181235790253
CurrentTrain: epoch  9, batch   106 | loss: 4.0887160Losses:  3.919966220855713 0.055792756378650665
CurrentTrain: epoch  9, batch   107 | loss: 3.9757590Losses:  4.019122123718262 0.0829971432685852
CurrentTrain: epoch  9, batch   108 | loss: 4.1021194Losses:  3.9335989952087402 0.09139972180128098
CurrentTrain: epoch  9, batch   109 | loss: 4.0249987Losses:  3.9852688312530518 0.04042927175760269
CurrentTrain: epoch  9, batch   110 | loss: 4.0256982Losses:  3.983912944793701 0.09344550967216492
CurrentTrain: epoch  9, batch   111 | loss: 4.0773582Losses:  3.927671432495117 0.05264574661850929
CurrentTrain: epoch  9, batch   112 | loss: 3.9803171Losses:  3.960592746734619 0.08679494261741638
CurrentTrain: epoch  9, batch   113 | loss: 4.0473876Losses:  3.963833808898926 0.056539978832006454
CurrentTrain: epoch  9, batch   114 | loss: 4.0203738Losses:  3.9288854598999023 0.04014071822166443
CurrentTrain: epoch  9, batch   115 | loss: 3.9690261Losses:  4.012022018432617 0.0714917853474617
CurrentTrain: epoch  9, batch   116 | loss: 4.0835137Losses:  3.9161734580993652 0.0678337886929512
CurrentTrain: epoch  9, batch   117 | loss: 3.9840074Losses:  3.9567065238952637 0.04379009082913399
CurrentTrain: epoch  9, batch   118 | loss: 4.0004964Losses:  4.019698619842529 0.05140414834022522
CurrentTrain: epoch  9, batch   119 | loss: 4.0711026Losses:  3.9921956062316895 0.07169574499130249
CurrentTrain: epoch  9, batch   120 | loss: 4.0638914Losses:  3.9323978424072266 0.0809263065457344
CurrentTrain: epoch  9, batch   121 | loss: 4.0133243Losses:  3.978902816772461 0.08557049930095673
CurrentTrain: epoch  9, batch   122 | loss: 4.0644732Losses:  4.060581207275391 0.045919984579086304
CurrentTrain: epoch  9, batch   123 | loss: 4.1065011Losses:  3.9696879386901855 0.03360829874873161
CurrentTrain: epoch  9, batch   124 | loss: 4.0032964
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  7.169760704040527 1.7072561979293823
CurrentTrain: epoch  0, batch     0 | loss: 8.8770170Losses:  7.800952911376953 1.6183960437774658
CurrentTrain: epoch  0, batch     1 | loss: 9.4193487Losses:  8.694778442382812 1.6274374723434448
CurrentTrain: epoch  0, batch     2 | loss: 10.3222160Losses:  6.130274772644043 1.093326449394226
CurrentTrain: epoch  0, batch     3 | loss: 7.2236013Losses:  6.969827651977539 1.3979804515838623
CurrentTrain: epoch  0, batch     4 | loss: 8.3678083Losses:  7.427499771118164 1.4737460613250732
CurrentTrain: epoch  0, batch     5 | loss: 8.9012461Losses:  3.6105642318725586 0.22733786702156067
CurrentTrain: epoch  0, batch     6 | loss: 3.8379021Losses:  7.001037120819092 1.5132471323013306
CurrentTrain: epoch  1, batch     0 | loss: 8.5142841Losses:  5.939218044281006 1.2506191730499268
CurrentTrain: epoch  1, batch     1 | loss: 7.1898375Losses:  4.925539970397949 1.279439926147461
CurrentTrain: epoch  1, batch     2 | loss: 6.2049799Losses:  8.092145919799805 1.5560798645019531
CurrentTrain: epoch  1, batch     3 | loss: 9.6482258Losses:  6.137042999267578 1.468956470489502
CurrentTrain: epoch  1, batch     4 | loss: 7.6059995Losses:  5.6781158447265625 1.1641430854797363
CurrentTrain: epoch  1, batch     5 | loss: 6.8422589Losses:  4.572673797607422 0.3708186745643616
CurrentTrain: epoch  1, batch     6 | loss: 4.9434924Losses:  5.172184944152832 0.9556301832199097
CurrentTrain: epoch  2, batch     0 | loss: 6.1278152Losses:  5.294659614562988 1.5172932147979736
CurrentTrain: epoch  2, batch     1 | loss: 6.8119526Losses:  6.765655517578125 1.3567383289337158
CurrentTrain: epoch  2, batch     2 | loss: 8.1223936Losses:  4.42974853515625 0.9235798120498657
CurrentTrain: epoch  2, batch     3 | loss: 5.3533282Losses:  4.836285591125488 1.2919623851776123
CurrentTrain: epoch  2, batch     4 | loss: 6.1282482Losses:  4.565600395202637 1.2834343910217285
CurrentTrain: epoch  2, batch     5 | loss: 5.8490348Losses:  3.2861526012420654 0.23127864301204681
CurrentTrain: epoch  2, batch     6 | loss: 3.5174313Losses:  4.635735988616943 0.8952253460884094
CurrentTrain: epoch  3, batch     0 | loss: 5.5309615Losses:  5.473942279815674 1.0644886493682861
CurrentTrain: epoch  3, batch     1 | loss: 6.5384312Losses:  4.079296112060547 1.191511631011963
CurrentTrain: epoch  3, batch     2 | loss: 5.2708077Losses:  4.361198902130127 0.9480687975883484
CurrentTrain: epoch  3, batch     3 | loss: 5.3092675Losses:  4.453855991363525 1.474686622619629
CurrentTrain: epoch  3, batch     4 | loss: 5.9285426Losses:  4.967013835906982 1.3321144580841064
CurrentTrain: epoch  3, batch     5 | loss: 6.2991285Losses:  3.346043586730957 0.3444795608520508
CurrentTrain: epoch  3, batch     6 | loss: 3.6905231Losses:  3.971440076828003 1.1704996824264526
CurrentTrain: epoch  4, batch     0 | loss: 5.1419396Losses:  3.166787624359131 0.7845933437347412
CurrentTrain: epoch  4, batch     1 | loss: 3.9513810Losses:  5.895273208618164 0.7912726998329163
CurrentTrain: epoch  4, batch     2 | loss: 6.6865458Losses:  5.117669105529785 1.289402961730957
CurrentTrain: epoch  4, batch     3 | loss: 6.4070721Losses:  4.010987281799316 0.9456614255905151
CurrentTrain: epoch  4, batch     4 | loss: 4.9566488Losses:  2.981809377670288 0.9896596670150757
CurrentTrain: epoch  4, batch     5 | loss: 3.9714689Losses:  7.967281818389893 1.1920930376163597e-07
CurrentTrain: epoch  4, batch     6 | loss: 7.9672818Losses:  4.284578323364258 1.0809850692749023
CurrentTrain: epoch  5, batch     0 | loss: 5.3655634Losses:  3.870882987976074 0.8082653284072876
CurrentTrain: epoch  5, batch     1 | loss: 4.6791482Losses:  4.500498294830322 1.004225254058838
CurrentTrain: epoch  5, batch     2 | loss: 5.5047235Losses:  2.9701905250549316 0.8575990200042725
CurrentTrain: epoch  5, batch     3 | loss: 3.8277895Losses:  5.399336338043213 1.1912108659744263
CurrentTrain: epoch  5, batch     4 | loss: 6.5905471Losses:  4.345844268798828 1.163787603378296
CurrentTrain: epoch  5, batch     5 | loss: 5.5096321Losses:  1.84199857711792 0.09272170066833496
CurrentTrain: epoch  5, batch     6 | loss: 1.9347203Losses:  4.881326198577881 1.3066319227218628
CurrentTrain: epoch  6, batch     0 | loss: 6.1879582Losses:  3.3401832580566406 1.118380069732666
CurrentTrain: epoch  6, batch     1 | loss: 4.4585633Losses:  2.7882192134857178 0.7709102630615234
CurrentTrain: epoch  6, batch     2 | loss: 3.5591295Losses:  4.760210990905762 1.218541145324707
CurrentTrain: epoch  6, batch     3 | loss: 5.9787521Losses:  3.4443938732147217 0.9226324558258057
CurrentTrain: epoch  6, batch     4 | loss: 4.3670263Losses:  4.380237579345703 0.8552063703536987
CurrentTrain: epoch  6, batch     5 | loss: 5.2354441Losses:  2.806380271911621 1.4901162614933128e-07
CurrentTrain: epoch  6, batch     6 | loss: 2.8063805Losses:  4.755298137664795 0.7518705129623413
CurrentTrain: epoch  7, batch     0 | loss: 5.5071688Losses:  3.999907970428467 0.6911433339118958
CurrentTrain: epoch  7, batch     1 | loss: 4.6910515Losses:  2.9510695934295654 0.8869715929031372
CurrentTrain: epoch  7, batch     2 | loss: 3.8380413Losses:  4.317131042480469 1.0840566158294678
CurrentTrain: epoch  7, batch     3 | loss: 5.4011879Losses:  2.4510326385498047 0.6910425424575806
CurrentTrain: epoch  7, batch     4 | loss: 3.1420751Losses:  3.4072906970977783 1.0101702213287354
CurrentTrain: epoch  7, batch     5 | loss: 4.4174609Losses:  4.829612731933594 0.3477529287338257
CurrentTrain: epoch  7, batch     6 | loss: 5.1773658Losses:  3.516361951828003 1.1456927061080933
CurrentTrain: epoch  8, batch     0 | loss: 4.6620545Losses:  2.9557268619537354 0.8614093661308289
CurrentTrain: epoch  8, batch     1 | loss: 3.8171363Losses:  4.536834716796875 1.1356260776519775
CurrentTrain: epoch  8, batch     2 | loss: 5.6724606Losses:  2.406012535095215 0.7555995583534241
CurrentTrain: epoch  8, batch     3 | loss: 3.1616120Losses:  3.120661735534668 0.7166655659675598
CurrentTrain: epoch  8, batch     4 | loss: 3.8373272Losses:  4.290345191955566 0.7263236045837402
CurrentTrain: epoch  8, batch     5 | loss: 5.0166688Losses:  1.9422634840011597 0.17225100100040436
CurrentTrain: epoch  8, batch     6 | loss: 2.1145146Losses:  3.5039031505584717 0.8041732311248779
CurrentTrain: epoch  9, batch     0 | loss: 4.3080764Losses:  2.8968591690063477 0.8185915350914001
CurrentTrain: epoch  9, batch     1 | loss: 3.7154508Losses:  3.2335405349731445 0.9330955147743225
CurrentTrain: epoch  9, batch     2 | loss: 4.1666360Losses:  4.367706298828125 0.7698478698730469
CurrentTrain: epoch  9, batch     3 | loss: 5.1375542Losses:  2.86885929107666 0.5918741822242737
CurrentTrain: epoch  9, batch     4 | loss: 3.4607334Losses:  2.6056289672851562 0.6566487550735474
CurrentTrain: epoch  9, batch     5 | loss: 3.2622776Losses:  1.7224111557006836 0.12492268532514572
CurrentTrain: epoch  9, batch     6 | loss: 1.8473338
Losses:  2.0241692066192627 0.8306076526641846
MemoryTrain:  epoch  0, batch     0 | loss: 2.8547769Losses:  1.4132518768310547 0.6482120752334595
MemoryTrain:  epoch  0, batch     1 | loss: 2.0614638Losses:  0.7528243064880371 0.31942224502563477
MemoryTrain:  epoch  0, batch     2 | loss: 1.0722466Losses:  1.7257664203643799 0.6229678392410278
MemoryTrain:  epoch  1, batch     0 | loss: 2.3487344Losses:  1.9922341108322144 0.7313618063926697
MemoryTrain:  epoch  1, batch     1 | loss: 2.7235959Losses:  0.1375357210636139 0.3272743225097656
MemoryTrain:  epoch  1, batch     2 | loss: 0.4648100Losses:  0.7017539143562317 0.6400185823440552
MemoryTrain:  epoch  2, batch     0 | loss: 1.3417726Losses:  0.44368094205856323 0.5835123062133789
MemoryTrain:  epoch  2, batch     1 | loss: 1.0271933Losses:  0.4107036590576172 0.18944501876831055
MemoryTrain:  epoch  2, batch     2 | loss: 0.6001487Losses:  0.5716352462768555 0.5744080543518066
MemoryTrain:  epoch  3, batch     0 | loss: 1.1460433Losses:  0.3254339098930359 0.729927659034729
MemoryTrain:  epoch  3, batch     1 | loss: 1.0553615Losses:  0.02338280901312828 0.14360792934894562
MemoryTrain:  epoch  3, batch     2 | loss: 0.1669907Losses:  0.022255880758166313 0.4694538116455078
MemoryTrain:  epoch  4, batch     0 | loss: 0.4917097Losses:  0.9453181028366089 0.5643101930618286
MemoryTrain:  epoch  4, batch     1 | loss: 1.5096283Losses:  0.8289476037025452 0.1691325157880783
MemoryTrain:  epoch  4, batch     2 | loss: 0.9980801Losses:  0.02299853414297104 0.29550591111183167
MemoryTrain:  epoch  5, batch     0 | loss: 0.3185045Losses:  0.7846169471740723 0.804620087146759
MemoryTrain:  epoch  5, batch     1 | loss: 1.5892370Losses:  0.15446841716766357 0.13479676842689514
MemoryTrain:  epoch  5, batch     2 | loss: 0.2892652Losses:  0.023231904953718185 0.3281179666519165
MemoryTrain:  epoch  6, batch     0 | loss: 0.3513499Losses:  0.3679191470146179 0.7004122734069824
MemoryTrain:  epoch  6, batch     1 | loss: 1.0683315Losses:  0.023475447669625282 0.3848235607147217
MemoryTrain:  epoch  6, batch     2 | loss: 0.4082990Losses:  0.2008885145187378 0.638792872428894
MemoryTrain:  epoch  7, batch     0 | loss: 0.8396814Losses:  0.05553031340241432 0.5163941979408264
MemoryTrain:  epoch  7, batch     1 | loss: 0.5719245Losses:  0.06147802993655205 0.1829543262720108
MemoryTrain:  epoch  7, batch     2 | loss: 0.2444324Losses:  0.16686150431632996 0.5000418424606323
MemoryTrain:  epoch  8, batch     0 | loss: 0.6669034Losses:  0.02918795309960842 0.5088764429092407
MemoryTrain:  epoch  8, batch     1 | loss: 0.5380644Losses:  0.09737133979797363 0.1966833472251892
MemoryTrain:  epoch  8, batch     2 | loss: 0.2940547Losses:  0.03274211287498474 0.5079678893089294
MemoryTrain:  epoch  9, batch     0 | loss: 0.5407100Losses:  0.04087761044502258 0.6393799781799316
MemoryTrain:  epoch  9, batch     1 | loss: 0.6802576Losses:  0.009468065574765205 0.053493618965148926
MemoryTrain:  epoch  9, batch     2 | loss: 0.0629617
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.98%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 76.27%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 74.60%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.01%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 91.93%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.68%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 91.06%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 90.06%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 88.42%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 87.95%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 87.94%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 87.67%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 87.34%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.26%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 87.10%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 87.10%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 86.81%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.82%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 86.62%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 86.70%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 86.71%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.87%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 87.87%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 87.81%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 87.56%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 87.44%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 87.44%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.38%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 87.15%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 86.75%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 86.64%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 86.42%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 86.26%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 86.05%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 86.18%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.31%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.32%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.39%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 86.24%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 85.78%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 85.49%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 85.19%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 84.76%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 84.48%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 84.25%   
cur_acc:  ['0.9484', '0.7401']
his_acc:  ['0.9484', '0.8425']
Clustering into  14  clusters
Clusters:  [ 2  9 13  2  2  2  0  2 10  1  0  2  4  2 11  6  7  2  2  2  2  2  2  8
  2  1  2  3 12  5]
Losses:  7.2216796875 1.112616777420044
CurrentTrain: epoch  0, batch     0 | loss: 8.3342962Losses:  6.853826999664307 0.9918109178543091
CurrentTrain: epoch  0, batch     1 | loss: 7.8456378Losses:  5.81135368347168 1.1184598207473755
CurrentTrain: epoch  0, batch     2 | loss: 6.9298134Losses:  6.088822364807129 1.2293155193328857
CurrentTrain: epoch  0, batch     3 | loss: 7.3181381Losses:  5.434492111206055 1.145567536354065
CurrentTrain: epoch  0, batch     4 | loss: 6.5800595Losses:  6.730687141418457 0.6406346559524536
CurrentTrain: epoch  0, batch     5 | loss: 7.3713217Losses:  4.2996063232421875 0.11740265041589737
CurrentTrain: epoch  0, batch     6 | loss: 4.4170089Losses:  6.250998020172119 0.8850992918014526
CurrentTrain: epoch  1, batch     0 | loss: 7.1360974Losses:  5.025538444519043 0.705961287021637
CurrentTrain: epoch  1, batch     1 | loss: 5.7314997Losses:  5.5783514976501465 1.3183372020721436
CurrentTrain: epoch  1, batch     2 | loss: 6.8966885Losses:  5.084321022033691 0.7712084650993347
CurrentTrain: epoch  1, batch     3 | loss: 5.8555293Losses:  3.7512693405151367 0.9028375744819641
CurrentTrain: epoch  1, batch     4 | loss: 4.6541071Losses:  4.125079154968262 1.1442115306854248
CurrentTrain: epoch  1, batch     5 | loss: 5.2692909Losses:  4.004598140716553 0.1799437403678894
CurrentTrain: epoch  1, batch     6 | loss: 4.1845417Losses:  4.2889299392700195 1.0327872037887573
CurrentTrain: epoch  2, batch     0 | loss: 5.3217173Losses:  3.3661341667175293 0.7796818017959595
CurrentTrain: epoch  2, batch     1 | loss: 4.1458158Losses:  3.990138053894043 0.9747579097747803
CurrentTrain: epoch  2, batch     2 | loss: 4.9648962Losses:  3.9656999111175537 0.7050907611846924
CurrentTrain: epoch  2, batch     3 | loss: 4.6707907Losses:  4.13283634185791 0.8155910968780518
CurrentTrain: epoch  2, batch     4 | loss: 4.9484272Losses:  3.5237536430358887 0.7899467945098877
CurrentTrain: epoch  2, batch     5 | loss: 4.3137007Losses:  2.8571691513061523 0.2109195590019226
CurrentTrain: epoch  2, batch     6 | loss: 3.0680888Losses:  4.527826309204102 0.7599995136260986
CurrentTrain: epoch  3, batch     0 | loss: 5.2878256Losses:  3.0473623275756836 0.6683100461959839
CurrentTrain: epoch  3, batch     1 | loss: 3.7156725Losses:  3.6694369316101074 0.6835293173789978
CurrentTrain: epoch  3, batch     2 | loss: 4.3529663Losses:  2.8431997299194336 0.804875373840332
CurrentTrain: epoch  3, batch     3 | loss: 3.6480751Losses:  2.680906057357788 0.7201204299926758
CurrentTrain: epoch  3, batch     4 | loss: 3.4010265Losses:  3.568375587463379 0.5952692031860352
CurrentTrain: epoch  3, batch     5 | loss: 4.1636448Losses:  2.915571689605713 0.16139915585517883
CurrentTrain: epoch  3, batch     6 | loss: 3.0769708Losses:  2.739285469055176 0.4983444809913635
CurrentTrain: epoch  4, batch     0 | loss: 3.2376299Losses:  3.516613483428955 0.5718668699264526
CurrentTrain: epoch  4, batch     1 | loss: 4.0884805Losses:  2.9160547256469727 0.6259216070175171
CurrentTrain: epoch  4, batch     2 | loss: 3.5419765Losses:  2.3096487522125244 0.7194413542747498
CurrentTrain: epoch  4, batch     3 | loss: 3.0290902Losses:  2.8359932899475098 0.6150886416435242
CurrentTrain: epoch  4, batch     4 | loss: 3.4510820Losses:  2.5273003578186035 0.6131089925765991
CurrentTrain: epoch  4, batch     5 | loss: 3.1404095Losses:  1.883105993270874 0.05435509607195854
CurrentTrain: epoch  4, batch     6 | loss: 1.9374611Losses:  2.55250883102417 0.7573729753494263
CurrentTrain: epoch  5, batch     0 | loss: 3.3098817Losses:  2.311601161956787 0.3884883522987366
CurrentTrain: epoch  5, batch     1 | loss: 2.7000895Losses:  2.551387310028076 0.66584712266922
CurrentTrain: epoch  5, batch     2 | loss: 3.2172344Losses:  2.3594791889190674 0.42372286319732666
CurrentTrain: epoch  5, batch     3 | loss: 2.7832022Losses:  2.4991226196289062 0.6508704423904419
CurrentTrain: epoch  5, batch     4 | loss: 3.1499929Losses:  2.5336105823516846 0.472630113363266
CurrentTrain: epoch  5, batch     5 | loss: 3.0062406Losses:  2.0198397636413574 0.2986210286617279
CurrentTrain: epoch  5, batch     6 | loss: 2.3184607Losses:  2.666996479034424 0.7379756569862366
CurrentTrain: epoch  6, batch     0 | loss: 3.4049721Losses:  2.8263421058654785 0.43252724409103394
CurrentTrain: epoch  6, batch     1 | loss: 3.2588694Losses:  2.5861682891845703 0.7243026494979858
CurrentTrain: epoch  6, batch     2 | loss: 3.3104711Losses:  1.9850927591323853 0.42954468727111816
CurrentTrain: epoch  6, batch     3 | loss: 2.4146376Losses:  1.9437618255615234 0.43832749128341675
CurrentTrain: epoch  6, batch     4 | loss: 2.3820894Losses:  2.1611275672912598 0.35411396622657776
CurrentTrain: epoch  6, batch     5 | loss: 2.5152416Losses:  1.83526611328125 0.2206299901008606
CurrentTrain: epoch  6, batch     6 | loss: 2.0558960Losses:  2.17539119720459 0.4698619842529297
CurrentTrain: epoch  7, batch     0 | loss: 2.6452532Losses:  1.9325528144836426 0.39886903762817383
CurrentTrain: epoch  7, batch     1 | loss: 2.3314219Losses:  2.0483155250549316 0.5118516683578491
CurrentTrain: epoch  7, batch     2 | loss: 2.5601673Losses:  1.9690020084381104 0.30551600456237793
CurrentTrain: epoch  7, batch     3 | loss: 2.2745180Losses:  2.2191598415374756 0.42451998591423035
CurrentTrain: epoch  7, batch     4 | loss: 2.6436799Losses:  2.2402563095092773 0.5531907081604004
CurrentTrain: epoch  7, batch     5 | loss: 2.7934470Losses:  1.9898991584777832 0.08401660621166229
CurrentTrain: epoch  7, batch     6 | loss: 2.0739157Losses:  1.9462878704071045 0.49187374114990234
CurrentTrain: epoch  8, batch     0 | loss: 2.4381616Losses:  1.9394917488098145 0.1942482739686966
CurrentTrain: epoch  8, batch     1 | loss: 2.1337399Losses:  1.9625228643417358 0.4570506513118744
CurrentTrain: epoch  8, batch     2 | loss: 2.4195735Losses:  2.3140065670013428 0.2964321970939636
CurrentTrain: epoch  8, batch     3 | loss: 2.6104388Losses:  1.975614070892334 0.45253586769104004
CurrentTrain: epoch  8, batch     4 | loss: 2.4281499Losses:  1.796440601348877 0.38519901037216187
CurrentTrain: epoch  8, batch     5 | loss: 2.1816397Losses:  1.8442857265472412 0.28297361731529236
CurrentTrain: epoch  8, batch     6 | loss: 2.1272593Losses:  1.924941062927246 0.3385348916053772
CurrentTrain: epoch  9, batch     0 | loss: 2.2634759Losses:  1.7497165203094482 0.37302350997924805
CurrentTrain: epoch  9, batch     1 | loss: 2.1227400Losses:  1.8415955305099487 0.4533160328865051
CurrentTrain: epoch  9, batch     2 | loss: 2.2949116Losses:  1.9553266763687134 0.5373131036758423
CurrentTrain: epoch  9, batch     3 | loss: 2.4926398Losses:  1.9153705835342407 0.5765500664710999
CurrentTrain: epoch  9, batch     4 | loss: 2.4919207Losses:  1.8588635921478271 0.6892626285552979
CurrentTrain: epoch  9, batch     5 | loss: 2.5481262Losses:  2.050693988800049 0.27056294679641724
CurrentTrain: epoch  9, batch     6 | loss: 2.3212569
Losses:  1.4340605735778809 0.36453357338905334
MemoryTrain:  epoch  0, batch     0 | loss: 1.7985941Losses:  2.171340227127075 0.824657678604126
MemoryTrain:  epoch  0, batch     1 | loss: 2.9959979Losses:  2.36129093170166 0.37734949588775635
MemoryTrain:  epoch  0, batch     2 | loss: 2.7386403Losses:  1.3266632556915283 0.5015323758125305
MemoryTrain:  epoch  0, batch     3 | loss: 1.8281956Losses:  2.052454710006714 0.5003846287727356
MemoryTrain:  epoch  1, batch     0 | loss: 2.5528393Losses:  2.0060949325561523 0.431155800819397
MemoryTrain:  epoch  1, batch     1 | loss: 2.4372506Losses:  1.0315005779266357 0.4112437963485718
MemoryTrain:  epoch  1, batch     2 | loss: 1.4427444Losses:  1.4846800565719604 0.766913890838623
MemoryTrain:  epoch  1, batch     3 | loss: 2.2515941Losses:  0.9648078083992004 0.6665278673171997
MemoryTrain:  epoch  2, batch     0 | loss: 1.6313357Losses:  2.1263773441314697 0.6092011332511902
MemoryTrain:  epoch  2, batch     1 | loss: 2.7355785Losses:  0.19786635041236877 0.39728015661239624
MemoryTrain:  epoch  2, batch     2 | loss: 0.5951465Losses:  1.8631083965301514 0.39838576316833496
MemoryTrain:  epoch  2, batch     3 | loss: 2.2614942Losses:  0.7135156393051147 0.455630898475647
MemoryTrain:  epoch  3, batch     0 | loss: 1.1691465Losses:  0.5356186628341675 0.47773849964141846
MemoryTrain:  epoch  3, batch     1 | loss: 1.0133572Losses:  1.8572319746017456 0.4746870696544647
MemoryTrain:  epoch  3, batch     2 | loss: 2.3319190Losses:  0.43019458651542664 0.8089584708213806
MemoryTrain:  epoch  3, batch     3 | loss: 1.2391530Losses:  1.5897831916809082 0.6470715999603271
MemoryTrain:  epoch  4, batch     0 | loss: 2.2368548Losses:  0.4651745855808258 0.5638867020606995
MemoryTrain:  epoch  4, batch     1 | loss: 1.0290613Losses:  0.3982970714569092 0.41289758682250977
MemoryTrain:  epoch  4, batch     2 | loss: 0.8111947Losses:  1.06563138961792 0.4307120740413666
MemoryTrain:  epoch  4, batch     3 | loss: 1.4963435Losses:  1.479844093322754 0.6122381687164307
MemoryTrain:  epoch  5, batch     0 | loss: 2.0920823Losses:  0.14640723168849945 0.5383272171020508
MemoryTrain:  epoch  5, batch     1 | loss: 0.6847345Losses:  1.0112848281860352 0.43707066774368286
MemoryTrain:  epoch  5, batch     2 | loss: 1.4483554Losses:  0.06043766066431999 0.3371851444244385
MemoryTrain:  epoch  5, batch     3 | loss: 0.3976228Losses:  1.5211031436920166 0.45185956358909607
MemoryTrain:  epoch  6, batch     0 | loss: 1.9729627Losses:  0.45217564702033997 0.555188775062561
MemoryTrain:  epoch  6, batch     1 | loss: 1.0073644Losses:  0.180965393781662 0.4435946047306061
MemoryTrain:  epoch  6, batch     2 | loss: 0.6245600Losses:  0.12580585479736328 0.4164000451564789
MemoryTrain:  epoch  6, batch     3 | loss: 0.5422059Losses:  0.047809816896915436 0.4495547115802765
MemoryTrain:  epoch  7, batch     0 | loss: 0.4973645Losses:  0.1993919014930725 0.7216677665710449
MemoryTrain:  epoch  7, batch     1 | loss: 0.9210597Losses:  1.4190219640731812 0.6438369750976562
MemoryTrain:  epoch  7, batch     2 | loss: 2.0628591Losses:  0.03021518886089325 0.27098292112350464
MemoryTrain:  epoch  7, batch     3 | loss: 0.3011981Losses:  0.5906224250793457 0.3183397352695465
MemoryTrain:  epoch  8, batch     0 | loss: 0.9089621Losses:  0.49632662534713745 0.4891356825828552
MemoryTrain:  epoch  8, batch     1 | loss: 0.9854623Losses:  0.06529523432254791 0.7127448320388794
MemoryTrain:  epoch  8, batch     2 | loss: 0.7780401Losses:  0.025570359081029892 0.28759318590164185
MemoryTrain:  epoch  8, batch     3 | loss: 0.3131635Losses:  0.48543787002563477 0.3466378450393677
MemoryTrain:  epoch  9, batch     0 | loss: 0.8320757Losses:  0.16895845532417297 0.296927273273468
MemoryTrain:  epoch  9, batch     1 | loss: 0.4658857Losses:  0.043865855783224106 0.78277188539505
MemoryTrain:  epoch  9, batch     2 | loss: 0.8266377Losses:  0.02943362109363079 0.3924095332622528
MemoryTrain:  epoch  9, batch     3 | loss: 0.4218431
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.49%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 74.09%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.62%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.53%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.03%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 90.30%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 90.04%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 89.69%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 89.16%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 88.25%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 87.96%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 87.77%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 87.68%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.41%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 87.07%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 86.64%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 86.49%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 86.35%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 86.20%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 86.06%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.86%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 85.52%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 85.54%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 85.49%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 85.29%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 85.25%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.12%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.26%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 86.63%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 86.64%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 86.35%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 86.18%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 86.19%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.63%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 85.13%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 84.81%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 84.43%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 84.12%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 83.90%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.94%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.22%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 83.44%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 82.85%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 82.33%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 81.76%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 81.15%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 80.65%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 80.61%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 80.46%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 80.37%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 80.63%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 80.74%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 80.79%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 81.03%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 80.91%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 80.91%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 80.83%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 80.79%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 80.84%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 80.80%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 80.56%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 80.45%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 80.21%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 80.06%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 79.91%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 79.69%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 79.66%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 79.45%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 79.31%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 79.24%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 79.03%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 78.89%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 78.72%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 78.59%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 78.27%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 77.92%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 77.58%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 77.24%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 76.90%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 76.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 78.14%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 77.96%   
cur_acc:  ['0.9484', '0.7401', '0.7262']
his_acc:  ['0.9484', '0.8425', '0.7796']
Clustering into  19  clusters
Clusters:  [ 2  9 13  2  2  2 17  2 18  0 14  2 11  2 12 16 15  2  2  2  2  2  2 10
  2  0  2  7  5  8  2  3  2  2  4  2  2  2  1  6]
Losses:  7.253006935119629 1.2717173099517822
CurrentTrain: epoch  0, batch     0 | loss: 8.5247240Losses:  6.927801132202148 1.209291696548462
CurrentTrain: epoch  0, batch     1 | loss: 8.1370926Losses:  5.611532211303711 1.2517282962799072
CurrentTrain: epoch  0, batch     2 | loss: 6.8632603Losses:  5.7803635597229 0.9949134588241577
CurrentTrain: epoch  0, batch     3 | loss: 6.7752771Losses:  5.8331732749938965 0.8948197364807129
CurrentTrain: epoch  0, batch     4 | loss: 6.7279930Losses:  5.996723175048828 1.594717025756836
CurrentTrain: epoch  0, batch     5 | loss: 7.5914402Losses:  6.948124885559082 0.7190431356430054
CurrentTrain: epoch  0, batch     6 | loss: 7.6671681Losses:  5.596901893615723 1.1245882511138916
CurrentTrain: epoch  1, batch     0 | loss: 6.7214899Losses:  5.7623515129089355 1.1248178482055664
CurrentTrain: epoch  1, batch     1 | loss: 6.8871694Losses:  5.701722621917725 1.4766712188720703
CurrentTrain: epoch  1, batch     2 | loss: 7.1783938Losses:  5.2807841300964355 1.2745170593261719
CurrentTrain: epoch  1, batch     3 | loss: 6.5553012Losses:  4.777241230010986 1.176637887954712
CurrentTrain: epoch  1, batch     4 | loss: 5.9538794Losses:  3.539259433746338 1.1283137798309326
CurrentTrain: epoch  1, batch     5 | loss: 4.6675730Losses:  5.735631942749023 0.3034341335296631
CurrentTrain: epoch  1, batch     6 | loss: 6.0390663Losses:  4.812803268432617 0.8639596104621887
CurrentTrain: epoch  2, batch     0 | loss: 5.6767631Losses:  5.271484375 1.1383917331695557
CurrentTrain: epoch  2, batch     1 | loss: 6.4098759Losses:  4.6626877784729 1.2246685028076172
CurrentTrain: epoch  2, batch     2 | loss: 5.8873563Losses:  3.970365047454834 0.9300916194915771
CurrentTrain: epoch  2, batch     3 | loss: 4.9004564Losses:  4.438236236572266 0.876667857170105
CurrentTrain: epoch  2, batch     4 | loss: 5.3149042Losses:  3.3923604488372803 0.8961234092712402
CurrentTrain: epoch  2, batch     5 | loss: 4.2884836Losses:  4.393414497375488 0.7988255023956299
CurrentTrain: epoch  2, batch     6 | loss: 5.1922398Losses:  3.9180564880371094 1.0443305969238281
CurrentTrain: epoch  3, batch     0 | loss: 4.9623871Losses:  3.772279739379883 1.220685601234436
CurrentTrain: epoch  3, batch     1 | loss: 4.9929652Losses:  2.7875771522521973 1.1213401556015015
CurrentTrain: epoch  3, batch     2 | loss: 3.9089174Losses:  5.372198104858398 0.8929799795150757
CurrentTrain: epoch  3, batch     3 | loss: 6.2651782Losses:  2.954225540161133 1.0150747299194336
CurrentTrain: epoch  3, batch     4 | loss: 3.9693003Losses:  4.494251728057861 1.0336612462997437
CurrentTrain: epoch  3, batch     5 | loss: 5.5279131Losses:  4.595517158508301 0.509262204170227
CurrentTrain: epoch  3, batch     6 | loss: 5.1047792Losses:  3.19816255569458 0.8784661293029785
CurrentTrain: epoch  4, batch     0 | loss: 4.0766287Losses:  3.370525360107422 0.6175968050956726
CurrentTrain: epoch  4, batch     1 | loss: 3.9881222Losses:  4.059136867523193 0.9751052260398865
CurrentTrain: epoch  4, batch     2 | loss: 5.0342422Losses:  4.3285393714904785 1.163857102394104
CurrentTrain: epoch  4, batch     3 | loss: 5.4923964Losses:  3.4661402702331543 0.816936731338501
CurrentTrain: epoch  4, batch     4 | loss: 4.2830772Losses:  3.430793046951294 0.7801035642623901
CurrentTrain: epoch  4, batch     5 | loss: 4.2108965Losses:  3.598785400390625 0.389913409948349
CurrentTrain: epoch  4, batch     6 | loss: 3.9886987Losses:  2.880567789077759 0.8476888537406921
CurrentTrain: epoch  5, batch     0 | loss: 3.7282567Losses:  2.774618625640869 0.9466683864593506
CurrentTrain: epoch  5, batch     1 | loss: 3.7212870Losses:  3.746608257293701 0.9702260494232178
CurrentTrain: epoch  5, batch     2 | loss: 4.7168341Losses:  3.183187961578369 0.8527289628982544
CurrentTrain: epoch  5, batch     3 | loss: 4.0359168Losses:  4.103312015533447 0.7249783277511597
CurrentTrain: epoch  5, batch     4 | loss: 4.8282905Losses:  3.8491621017456055 1.0925542116165161
CurrentTrain: epoch  5, batch     5 | loss: 4.9417162Losses:  2.5947184562683105 0.1559763252735138
CurrentTrain: epoch  5, batch     6 | loss: 2.7506948Losses:  3.801828384399414 1.0970220565795898
CurrentTrain: epoch  6, batch     0 | loss: 4.8988504Losses:  3.8659143447875977 0.9506593346595764
CurrentTrain: epoch  6, batch     1 | loss: 4.8165736Losses:  2.7577266693115234 0.6157513856887817
CurrentTrain: epoch  6, batch     2 | loss: 3.3734779Losses:  2.7347683906555176 0.7114651799201965
CurrentTrain: epoch  6, batch     3 | loss: 3.4462335Losses:  2.6988749504089355 0.827954113483429
CurrentTrain: epoch  6, batch     4 | loss: 3.5268290Losses:  2.2694268226623535 0.7892947196960449
CurrentTrain: epoch  6, batch     5 | loss: 3.0587215Losses:  3.88242506980896 0.6921201348304749
CurrentTrain: epoch  6, batch     6 | loss: 4.5745454Losses:  2.8028111457824707 1.008118748664856
CurrentTrain: epoch  7, batch     0 | loss: 3.8109298Losses:  3.199800968170166 0.9955043196678162
CurrentTrain: epoch  7, batch     1 | loss: 4.1953053Losses:  3.2354979515075684 0.8314958214759827
CurrentTrain: epoch  7, batch     2 | loss: 4.0669937Losses:  3.0491366386413574 0.928558349609375
CurrentTrain: epoch  7, batch     3 | loss: 3.9776950Losses:  2.562544107437134 0.9411132335662842
CurrentTrain: epoch  7, batch     4 | loss: 3.5036573Losses:  2.093569278717041 0.5877196788787842
CurrentTrain: epoch  7, batch     5 | loss: 2.6812890Losses:  2.4388697147369385 0.07262853533029556
CurrentTrain: epoch  7, batch     6 | loss: 2.5114982Losses:  2.4822375774383545 0.6104742884635925
CurrentTrain: epoch  8, batch     0 | loss: 3.0927119Losses:  2.8013086318969727 0.7969799041748047
CurrentTrain: epoch  8, batch     1 | loss: 3.5982885Losses:  2.796834945678711 0.8838108777999878
CurrentTrain: epoch  8, batch     2 | loss: 3.6806459Losses:  2.8866028785705566 0.779464840888977
CurrentTrain: epoch  8, batch     3 | loss: 3.6660676Losses:  2.515035629272461 0.6279177665710449
CurrentTrain: epoch  8, batch     4 | loss: 3.1429534Losses:  2.1894567012786865 0.5511014461517334
CurrentTrain: epoch  8, batch     5 | loss: 2.7405581Losses:  1.8333351612091064 2.9802322387695312e-08
CurrentTrain: epoch  8, batch     6 | loss: 1.8333352Losses:  1.850874662399292 0.4177357256412506
CurrentTrain: epoch  9, batch     0 | loss: 2.2686105Losses:  2.0859389305114746 0.6204977631568909
CurrentTrain: epoch  9, batch     1 | loss: 2.7064366Losses:  2.478400707244873 0.5288156270980835
CurrentTrain: epoch  9, batch     2 | loss: 3.0072165Losses:  2.5639657974243164 0.9398400783538818
CurrentTrain: epoch  9, batch     3 | loss: 3.5038059Losses:  3.1846776008605957 0.5266830921173096
CurrentTrain: epoch  9, batch     4 | loss: 3.7113607Losses:  2.520878314971924 0.7703001499176025
CurrentTrain: epoch  9, batch     5 | loss: 3.2911785Losses:  2.311748504638672 0.039229631423950195
CurrentTrain: epoch  9, batch     6 | loss: 2.3509781
Losses:  0.768284797668457 0.43457871675491333
MemoryTrain:  epoch  0, batch     0 | loss: 1.2028635Losses:  0.3817961513996124 0.3589344620704651
MemoryTrain:  epoch  0, batch     1 | loss: 0.7407306Losses:  0.7697123289108276 0.81110680103302
MemoryTrain:  epoch  0, batch     2 | loss: 1.5808191Losses:  1.0116535425186157 0.8060382604598999
MemoryTrain:  epoch  0, batch     3 | loss: 1.8176918Losses:  0.2812945246696472 0.46782535314559937
MemoryTrain:  epoch  0, batch     4 | loss: 0.7491199Losses:  0.8575164079666138 0.7781525254249573
MemoryTrain:  epoch  1, batch     0 | loss: 1.6356690Losses:  1.417442798614502 0.5195528268814087
MemoryTrain:  epoch  1, batch     1 | loss: 1.9369956Losses:  1.2174948453903198 0.41709721088409424
MemoryTrain:  epoch  1, batch     2 | loss: 1.6345921Losses:  0.29291006922721863 0.5855310559272766
MemoryTrain:  epoch  1, batch     3 | loss: 0.8784411Losses:  1.3781577348709106 0.8099473714828491
MemoryTrain:  epoch  1, batch     4 | loss: 2.1881051Losses:  0.5698549747467041 0.4477168023586273
MemoryTrain:  epoch  2, batch     0 | loss: 1.0175718Losses:  0.1923293173313141 0.5582583546638489
MemoryTrain:  epoch  2, batch     1 | loss: 0.7505877Losses:  0.8772996664047241 0.5599613189697266
MemoryTrain:  epoch  2, batch     2 | loss: 1.4372610Losses:  0.39821308851242065 0.5497094392776489
MemoryTrain:  epoch  2, batch     3 | loss: 0.9479225Losses:  0.27827808260917664 0.9845559597015381
MemoryTrain:  epoch  2, batch     4 | loss: 1.2628341Losses:  0.13412773609161377 0.6186707615852356
MemoryTrain:  epoch  3, batch     0 | loss: 0.7527985Losses:  0.14621007442474365 0.4561324417591095
MemoryTrain:  epoch  3, batch     1 | loss: 0.6023425Losses:  0.38424447178840637 0.5751420259475708
MemoryTrain:  epoch  3, batch     2 | loss: 0.9593865Losses:  0.10231084376573563 0.5780045986175537
MemoryTrain:  epoch  3, batch     3 | loss: 0.6803154Losses:  0.10160806775093079 0.5899900197982788
MemoryTrain:  epoch  3, batch     4 | loss: 0.6915981Losses:  0.0877523124217987 0.5701136589050293
MemoryTrain:  epoch  4, batch     0 | loss: 0.6578660Losses:  0.1049804836511612 0.48324716091156006
MemoryTrain:  epoch  4, batch     1 | loss: 0.5882276Losses:  0.07205906510353088 0.6097643375396729
MemoryTrain:  epoch  4, batch     2 | loss: 0.6818234Losses:  0.49620598554611206 0.45984578132629395
MemoryTrain:  epoch  4, batch     3 | loss: 0.9560518Losses:  0.19321514666080475 0.6319018602371216
MemoryTrain:  epoch  4, batch     4 | loss: 0.8251170Losses:  0.07954797148704529 0.8158730268478394
MemoryTrain:  epoch  5, batch     0 | loss: 0.8954210Losses:  0.15048784017562866 0.5881052017211914
MemoryTrain:  epoch  5, batch     1 | loss: 0.7385930Losses:  0.08993411064147949 0.42184099555015564
MemoryTrain:  epoch  5, batch     2 | loss: 0.5117751Losses:  0.07511098682880402 0.34239357709884644
MemoryTrain:  epoch  5, batch     3 | loss: 0.4175045Losses:  0.03645865619182587 0.5682867765426636
MemoryTrain:  epoch  5, batch     4 | loss: 0.6047454Losses:  0.05346767604351044 0.8103005290031433
MemoryTrain:  epoch  6, batch     0 | loss: 0.8637682Losses:  0.05819439888000488 0.5524757504463196
MemoryTrain:  epoch  6, batch     1 | loss: 0.6106701Losses:  0.1488703191280365 0.4596386253833771
MemoryTrain:  epoch  6, batch     2 | loss: 0.6085089Losses:  0.041832007467746735 0.638975203037262
MemoryTrain:  epoch  6, batch     3 | loss: 0.6808072Losses:  0.041661329567432404 0.30221861600875854
MemoryTrain:  epoch  6, batch     4 | loss: 0.3438799Losses:  0.0834294930100441 0.46537843346595764
MemoryTrain:  epoch  7, batch     0 | loss: 0.5488079Losses:  0.043561916798353195 0.5666472911834717
MemoryTrain:  epoch  7, batch     1 | loss: 0.6102092Losses:  0.06999623030424118 0.690430760383606
MemoryTrain:  epoch  7, batch     2 | loss: 0.7604270Losses:  0.06553006172180176 0.5670819878578186
MemoryTrain:  epoch  7, batch     3 | loss: 0.6326120Losses:  0.03345988318324089 0.44736814498901367
MemoryTrain:  epoch  7, batch     4 | loss: 0.4808280Losses:  0.017219536006450653 0.4754446744918823
MemoryTrain:  epoch  8, batch     0 | loss: 0.4926642Losses:  0.05202958732843399 0.5349439382553101
MemoryTrain:  epoch  8, batch     1 | loss: 0.5869735Losses:  0.17679128050804138 0.6482281684875488
MemoryTrain:  epoch  8, batch     2 | loss: 0.8250195Losses:  0.05993854999542236 0.6012251973152161
MemoryTrain:  epoch  8, batch     3 | loss: 0.6611637Losses:  0.05948862433433533 0.39132100343704224
MemoryTrain:  epoch  8, batch     4 | loss: 0.4508096Losses:  0.038077063858509064 0.513913631439209
MemoryTrain:  epoch  9, batch     0 | loss: 0.5519907Losses:  0.05948089063167572 0.5284267067909241
MemoryTrain:  epoch  9, batch     1 | loss: 0.5879076Losses:  0.07840028405189514 0.44800934195518494
MemoryTrain:  epoch  9, batch     2 | loss: 0.5264096Losses:  0.04273875802755356 0.5963586568832397
MemoryTrain:  epoch  9, batch     3 | loss: 0.6390974Losses:  0.08463944494724274 0.3423847258090973
MemoryTrain:  epoch  9, batch     4 | loss: 0.4270242
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 45.70%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 44.49%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 45.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.28%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.45%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.83%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.65%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 91.45%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 90.84%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 90.15%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.90%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 88.49%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 86.25%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 85.13%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 82.90%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 82.25%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 82.13%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 81.77%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 81.51%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 81.33%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 81.33%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 81.02%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 80.96%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 81.98%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.80%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 82.66%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 82.34%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.78%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.85%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 80.40%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 79.91%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 79.98%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.34%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 79.69%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 79.03%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 77.85%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.27%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 76.70%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 76.43%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 76.11%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 76.05%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.80%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 76.65%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.73%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 76.62%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.69%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.62%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 76.51%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 76.50%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 76.51%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 76.49%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:  156 | acc: 31.25%,  total acc: 76.15%   [EVAL] batch:  157 | acc: 18.75%,  total acc: 75.79%   [EVAL] batch:  158 | acc: 18.75%,  total acc: 75.43%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 75.20%   [EVAL] batch:  160 | acc: 37.50%,  total acc: 74.96%   [EVAL] batch:  161 | acc: 12.50%,  total acc: 74.58%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 74.42%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 74.31%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 74.10%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 73.49%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.25%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.93%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 72.58%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 72.34%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 73.19%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 73.20%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 73.17%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 72.95%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 72.66%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 72.38%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 71.89%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 71.65%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  200 | acc: 0.00%,  total acc: 71.86%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 71.53%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 71.27%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 71.05%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:  205 | acc: 12.50%,  total acc: 70.51%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 71.30%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 72.47%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 72.12%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 72.05%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  243 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 72.82%   
cur_acc:  ['0.9484', '0.7401', '0.7262', '0.7083']
his_acc:  ['0.9484', '0.8425', '0.7796', '0.7282']
Clustering into  24  clusters
Clusters:  [ 3 23 16  4  3  3 22  3 18  1 17  3 14  3 15 21  8  3  3  3  3  3  3 12
  3  1  3 19 13  0  3 20  3  3  0  3  3  3 11  9  4 10  3  3  5  6  2  3
  3  7]
Losses:  4.266785621643066 0.9246134757995605
CurrentTrain: epoch  0, batch     0 | loss: 5.1913991Losses:  4.979404449462891 1.1260123252868652
CurrentTrain: epoch  0, batch     1 | loss: 6.1054168Losses:  5.3036088943481445 1.7207223176956177
CurrentTrain: epoch  0, batch     2 | loss: 7.0243311Losses:  4.2266387939453125 1.3229293823242188
CurrentTrain: epoch  0, batch     3 | loss: 5.5495682Losses:  5.7441606521606445 1.5787266492843628
CurrentTrain: epoch  0, batch     4 | loss: 7.3228874Losses:  4.5546674728393555 1.0386561155319214
CurrentTrain: epoch  0, batch     5 | loss: 5.5933237Losses:  8.147727012634277 1.1646692752838135
CurrentTrain: epoch  0, batch     6 | loss: 9.3123960Losses:  3.566434383392334 1.412418007850647
CurrentTrain: epoch  1, batch     0 | loss: 4.9788523Losses:  3.863574743270874 1.2941768169403076
CurrentTrain: epoch  1, batch     1 | loss: 5.1577516Losses:  4.29763126373291 1.1184539794921875
CurrentTrain: epoch  1, batch     2 | loss: 5.4160852Losses:  3.2453198432922363 1.2908616065979004
CurrentTrain: epoch  1, batch     3 | loss: 4.5361814Losses:  3.4959425926208496 0.721692681312561
CurrentTrain: epoch  1, batch     4 | loss: 4.2176352Losses:  4.6825079917907715 1.5083272457122803
CurrentTrain: epoch  1, batch     5 | loss: 6.1908350Losses:  1.9649873971939087 0.5709532499313354
CurrentTrain: epoch  1, batch     6 | loss: 2.5359406Losses:  4.073207378387451 1.0619845390319824
CurrentTrain: epoch  2, batch     0 | loss: 5.1351919Losses:  2.376582145690918 0.35892975330352783
CurrentTrain: epoch  2, batch     1 | loss: 2.7355118Losses:  3.682927370071411 0.9567431807518005
CurrentTrain: epoch  2, batch     2 | loss: 4.6396704Losses:  2.811065673828125 0.8481472134590149
CurrentTrain: epoch  2, batch     3 | loss: 3.6592128Losses:  3.3552260398864746 1.0863035917282104
CurrentTrain: epoch  2, batch     4 | loss: 4.4415298Losses:  3.455227851867676 1.1670161485671997
CurrentTrain: epoch  2, batch     5 | loss: 4.6222439Losses:  1.8956305980682373 0.08000176399946213
CurrentTrain: epoch  2, batch     6 | loss: 1.9756323Losses:  3.567267656326294 1.0034260749816895
CurrentTrain: epoch  3, batch     0 | loss: 4.5706940Losses:  2.798119306564331 0.909635066986084
CurrentTrain: epoch  3, batch     1 | loss: 3.7077544Losses:  2.568023204803467 0.7594282627105713
CurrentTrain: epoch  3, batch     2 | loss: 3.3274515Losses:  2.662038564682007 1.2102047204971313
CurrentTrain: epoch  3, batch     3 | loss: 3.8722434Losses:  2.555454730987549 0.7988504767417908
CurrentTrain: epoch  3, batch     4 | loss: 3.3543053Losses:  2.3606717586517334 0.6621782779693604
CurrentTrain: epoch  3, batch     5 | loss: 3.0228500Losses:  1.770719051361084 0.164653480052948
CurrentTrain: epoch  3, batch     6 | loss: 1.9353726Losses:  2.092519760131836 0.26909616589546204
CurrentTrain: epoch  4, batch     0 | loss: 2.3616159Losses:  2.314643383026123 0.7272341251373291
CurrentTrain: epoch  4, batch     1 | loss: 3.0418775Losses:  2.87528133392334 1.0191349983215332
CurrentTrain: epoch  4, batch     2 | loss: 3.8944163Losses:  1.8513684272766113 0.592621922492981
CurrentTrain: epoch  4, batch     3 | loss: 2.4439902Losses:  2.5386962890625 1.1205452680587769
CurrentTrain: epoch  4, batch     4 | loss: 3.6592417Losses:  2.6390480995178223 0.8721489310264587
CurrentTrain: epoch  4, batch     5 | loss: 3.5111971Losses:  5.176412582397461 8.94069742685133e-08
CurrentTrain: epoch  4, batch     6 | loss: 5.1764126Losses:  2.279712438583374 0.8630125522613525
CurrentTrain: epoch  5, batch     0 | loss: 3.1427250Losses:  2.30134654045105 0.975085437297821
CurrentTrain: epoch  5, batch     1 | loss: 3.2764320Losses:  1.9730465412139893 0.595917820930481
CurrentTrain: epoch  5, batch     2 | loss: 2.5689645Losses:  2.5237345695495605 0.7306089401245117
CurrentTrain: epoch  5, batch     3 | loss: 3.2543435Losses:  2.0701308250427246 0.503768265247345
CurrentTrain: epoch  5, batch     4 | loss: 2.5738990Losses:  1.9721181392669678 0.7894319295883179
CurrentTrain: epoch  5, batch     5 | loss: 2.7615499Losses:  1.8601878881454468 0.40415966510772705
CurrentTrain: epoch  5, batch     6 | loss: 2.2643476Losses:  2.009542942047119 0.418221652507782
CurrentTrain: epoch  6, batch     0 | loss: 2.4277647Losses:  2.146167278289795 0.8623256087303162
CurrentTrain: epoch  6, batch     1 | loss: 3.0084929Losses:  2.051539421081543 0.8409242630004883
CurrentTrain: epoch  6, batch     2 | loss: 2.8924637Losses:  1.9739935398101807 0.7545483112335205
CurrentTrain: epoch  6, batch     3 | loss: 2.7285419Losses:  2.064417600631714 0.4465639591217041
CurrentTrain: epoch  6, batch     4 | loss: 2.5109816Losses:  1.8053174018859863 0.6077029705047607
CurrentTrain: epoch  6, batch     5 | loss: 2.4130204Losses:  1.794521450996399 0.3089347779750824
CurrentTrain: epoch  6, batch     6 | loss: 2.1034563Losses:  2.006516456604004 0.6976391673088074
CurrentTrain: epoch  7, batch     0 | loss: 2.7041557Losses:  1.881606101989746 0.5777465105056763
CurrentTrain: epoch  7, batch     1 | loss: 2.4593525Losses:  2.001314640045166 0.7619508504867554
CurrentTrain: epoch  7, batch     2 | loss: 2.7632656Losses:  1.7468303442001343 0.6417131423950195
CurrentTrain: epoch  7, batch     3 | loss: 2.3885436Losses:  1.9865360260009766 0.78346848487854
CurrentTrain: epoch  7, batch     4 | loss: 2.7700045Losses:  1.8876712322235107 0.6475880742073059
CurrentTrain: epoch  7, batch     5 | loss: 2.5352592Losses:  1.9498682022094727 0.043238334357738495
CurrentTrain: epoch  7, batch     6 | loss: 1.9931065Losses:  1.8968420028686523 0.6624699831008911
CurrentTrain: epoch  8, batch     0 | loss: 2.5593119Losses:  1.9134817123413086 0.5549198389053345
CurrentTrain: epoch  8, batch     1 | loss: 2.4684014Losses:  1.8457438945770264 0.5086138844490051
CurrentTrain: epoch  8, batch     2 | loss: 2.3543577Losses:  1.8185688257217407 0.4130207300186157
CurrentTrain: epoch  8, batch     3 | loss: 2.2315896Losses:  1.8340709209442139 0.603147029876709
CurrentTrain: epoch  8, batch     4 | loss: 2.4372180Losses:  1.7529897689819336 0.44574350118637085
CurrentTrain: epoch  8, batch     5 | loss: 2.1987333Losses:  1.7475688457489014 0.11981530487537384
CurrentTrain: epoch  8, batch     6 | loss: 1.8673842Losses:  1.8054887056350708 0.35497957468032837
CurrentTrain: epoch  9, batch     0 | loss: 2.1604683Losses:  1.7554610967636108 0.6012634634971619
CurrentTrain: epoch  9, batch     1 | loss: 2.3567245Losses:  1.734379768371582 0.5285533666610718
CurrentTrain: epoch  9, batch     2 | loss: 2.2629333Losses:  1.79880952835083 0.516865611076355
CurrentTrain: epoch  9, batch     3 | loss: 2.3156753Losses:  1.8206396102905273 0.5822206735610962
CurrentTrain: epoch  9, batch     4 | loss: 2.4028602Losses:  1.792208194732666 0.6052128076553345
CurrentTrain: epoch  9, batch     5 | loss: 2.3974209Losses:  1.879072666168213 0.029091840609908104
CurrentTrain: epoch  9, batch     6 | loss: 1.9081645
Losses:  1.120524525642395 0.558671236038208
MemoryTrain:  epoch  0, batch     0 | loss: 1.6791958Losses:  0.23615145683288574 0.5791478753089905
MemoryTrain:  epoch  0, batch     1 | loss: 0.8152993Losses:  1.2103068828582764 0.5774611234664917
MemoryTrain:  epoch  0, batch     2 | loss: 1.7877680Losses:  0.8666262626647949 0.4829391837120056
MemoryTrain:  epoch  0, batch     3 | loss: 1.3495655Losses:  0.9686517715454102 0.5676401257514954
MemoryTrain:  epoch  0, batch     4 | loss: 1.5362918Losses:  0.7645325660705566 0.782569169998169
MemoryTrain:  epoch  0, batch     5 | loss: 1.5471017Losses:  0.058258648961782455 0.01624368503689766
MemoryTrain:  epoch  0, batch     6 | loss: 0.0745023Losses:  0.9612389802932739 0.6048645973205566
MemoryTrain:  epoch  1, batch     0 | loss: 1.5661036Losses:  0.34970614314079285 0.335064172744751
MemoryTrain:  epoch  1, batch     1 | loss: 0.6847703Losses:  0.637575626373291 0.38247793912887573
MemoryTrain:  epoch  1, batch     2 | loss: 1.0200536Losses:  1.4647135734558105 0.531644344329834
MemoryTrain:  epoch  1, batch     3 | loss: 1.9963579Losses:  1.0269147157669067 0.7002503871917725
MemoryTrain:  epoch  1, batch     4 | loss: 1.7271651Losses:  0.8779880404472351 0.727981686592102
MemoryTrain:  epoch  1, batch     5 | loss: 1.6059697Losses:  0.06762245297431946 0.35139304399490356
MemoryTrain:  epoch  1, batch     6 | loss: 0.4190155Losses:  0.2774505317211151 0.4995967447757721
MemoryTrain:  epoch  2, batch     0 | loss: 0.7770473Losses:  0.43539613485336304 0.4860461950302124
MemoryTrain:  epoch  2, batch     1 | loss: 0.9214423Losses:  0.41537967324256897 0.3963519036769867
MemoryTrain:  epoch  2, batch     2 | loss: 0.8117316Losses:  0.23014949262142181 0.7119230031967163
MemoryTrain:  epoch  2, batch     3 | loss: 0.9420725Losses:  0.19553931057453156 0.5158777236938477
MemoryTrain:  epoch  2, batch     4 | loss: 0.7114170Losses:  0.08383116871118546 0.5056631565093994
MemoryTrain:  epoch  2, batch     5 | loss: 0.5894943Losses:  0.869521975517273 0.0948958694934845
MemoryTrain:  epoch  2, batch     6 | loss: 0.9644178Losses:  0.146853506565094 0.5177091360092163
MemoryTrain:  epoch  3, batch     0 | loss: 0.6645626Losses:  0.2516704499721527 0.4105832576751709
MemoryTrain:  epoch  3, batch     1 | loss: 0.6622537Losses:  0.12150677293539047 0.4650874435901642
MemoryTrain:  epoch  3, batch     2 | loss: 0.5865942Losses:  0.12276503443717957 0.41884803771972656
MemoryTrain:  epoch  3, batch     3 | loss: 0.5416131Losses:  0.03373778238892555 0.3840157985687256
MemoryTrain:  epoch  3, batch     4 | loss: 0.4177536Losses:  0.08324466645717621 0.6869839429855347
MemoryTrain:  epoch  3, batch     5 | loss: 0.7702286Losses:  0.12615501880645752 0.31305524706840515
MemoryTrain:  epoch  3, batch     6 | loss: 0.4392103Losses:  0.038712821900844574 0.37596678733825684
MemoryTrain:  epoch  4, batch     0 | loss: 0.4146796Losses:  0.11320810765028 0.45498186349868774
MemoryTrain:  epoch  4, batch     1 | loss: 0.5681900Losses:  0.07302340120077133 0.5600782632827759
MemoryTrain:  epoch  4, batch     2 | loss: 0.6331016Losses:  0.11419905722141266 0.5505305528640747
MemoryTrain:  epoch  4, batch     3 | loss: 0.6647296Losses:  0.14789192378520966 0.7021644115447998
MemoryTrain:  epoch  4, batch     4 | loss: 0.8500564Losses:  0.07282509654760361 0.45329320430755615
MemoryTrain:  epoch  4, batch     5 | loss: 0.5261183Losses:  0.014460332691669464 0.012766577303409576
MemoryTrain:  epoch  4, batch     6 | loss: 0.0272269Losses:  0.028595909476280212 0.7160286903381348
MemoryTrain:  epoch  5, batch     0 | loss: 0.7446246Losses:  0.18151336908340454 0.48639774322509766
MemoryTrain:  epoch  5, batch     1 | loss: 0.6679111Losses:  0.11052720248699188 0.4597991108894348
MemoryTrain:  epoch  5, batch     2 | loss: 0.5703263Losses:  0.06284946203231812 0.5060147047042847
MemoryTrain:  epoch  5, batch     3 | loss: 0.5688642Losses:  0.05726911127567291 0.3790144622325897
MemoryTrain:  epoch  5, batch     4 | loss: 0.4362836Losses:  0.044405747205019 0.36915072798728943
MemoryTrain:  epoch  5, batch     5 | loss: 0.4135565Losses:  0.6182644367218018 0.043556150048971176
MemoryTrain:  epoch  5, batch     6 | loss: 0.6618206Losses:  0.05412072688341141 0.5412048101425171
MemoryTrain:  epoch  6, batch     0 | loss: 0.5953255Losses:  0.06536896526813507 0.3035842180252075
MemoryTrain:  epoch  6, batch     1 | loss: 0.3689532Losses:  0.040323927998542786 0.36272189021110535
MemoryTrain:  epoch  6, batch     2 | loss: 0.4030458Losses:  0.04141840711236 0.665959894657135
MemoryTrain:  epoch  6, batch     3 | loss: 0.7073783Losses:  0.04477253556251526 0.6788016557693481
MemoryTrain:  epoch  6, batch     4 | loss: 0.7235742Losses:  0.03847555071115494 0.46696382761001587
MemoryTrain:  epoch  6, batch     5 | loss: 0.5054394Losses:  0.03319160267710686 0.0488031730055809
MemoryTrain:  epoch  6, batch     6 | loss: 0.0819948Losses:  0.047450944781303406 0.7990744709968567
MemoryTrain:  epoch  7, batch     0 | loss: 0.8465254Losses:  0.05686459690332413 0.5577899217605591
MemoryTrain:  epoch  7, batch     1 | loss: 0.6146545Losses:  0.03478981554508209 0.4571950435638428
MemoryTrain:  epoch  7, batch     2 | loss: 0.4919848Losses:  0.08861446380615234 0.5299175977706909
MemoryTrain:  epoch  7, batch     3 | loss: 0.6185321Losses:  0.033826544880867004 0.25646236538887024
MemoryTrain:  epoch  7, batch     4 | loss: 0.2902889Losses:  0.05948672071099281 0.324481725692749
MemoryTrain:  epoch  7, batch     5 | loss: 0.3839684Losses:  0.0671907365322113 0.07362359762191772
MemoryTrain:  epoch  7, batch     6 | loss: 0.1408143Losses:  0.03586230427026749 0.2969425320625305
MemoryTrain:  epoch  8, batch     0 | loss: 0.3328048Losses:  0.054716806858778 0.6456658244132996
MemoryTrain:  epoch  8, batch     1 | loss: 0.7003826Losses:  0.05803738161921501 0.30620819330215454
MemoryTrain:  epoch  8, batch     2 | loss: 0.3642456Losses:  0.038818031549453735 0.4506170153617859
MemoryTrain:  epoch  8, batch     3 | loss: 0.4894350Losses:  0.03399345651268959 0.443087100982666
MemoryTrain:  epoch  8, batch     4 | loss: 0.4770806Losses:  0.03169946372509003 0.42750227451324463
MemoryTrain:  epoch  8, batch     5 | loss: 0.4592018Losses:  0.021969923749566078 0.29572731256484985
MemoryTrain:  epoch  8, batch     6 | loss: 0.3176972Losses:  0.03353505581617355 0.29572272300720215
MemoryTrain:  epoch  9, batch     0 | loss: 0.3292578Losses:  0.03803003579378128 0.4442082643508911
MemoryTrain:  epoch  9, batch     1 | loss: 0.4822383Losses:  0.04204854741692543 0.5937976837158203
MemoryTrain:  epoch  9, batch     2 | loss: 0.6358463Losses:  0.05577395111322403 0.5331097841262817
MemoryTrain:  epoch  9, batch     3 | loss: 0.5888838Losses:  0.0901661142706871 0.32136258482933044
MemoryTrain:  epoch  9, batch     4 | loss: 0.4115287Losses:  0.027406614273786545 0.38218700885772705
MemoryTrain:  epoch  9, batch     5 | loss: 0.4095936Losses:  0.022028649225831032 0.033517494797706604
MemoryTrain:  epoch  9, batch     6 | loss: 0.0555461
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 94.64%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 88.73%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.69%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 84.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.32%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 84.12%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.63%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 82.52%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 81.44%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 80.59%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 79.76%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 78.95%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 78.53%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 78.61%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 78.34%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 78.38%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 78.49%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 78.86%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 78.58%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 78.54%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 78.42%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 78.27%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 78.48%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 79.12%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 80.69%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 80.64%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 80.31%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.91%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 79.51%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.07%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 78.69%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 78.55%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 78.35%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.56%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 78.28%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 77.63%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 77.00%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 76.42%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 75.81%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 75.25%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.90%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 75.71%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.79%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 75.64%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.59%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 75.52%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 75.44%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 75.24%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 75.08%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 74.84%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 74.57%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 74.42%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 74.27%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.20%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 74.06%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.74%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 73.46%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 73.10%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.78%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 72.40%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 72.13%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 71.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 73.04%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 72.75%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 72.43%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 72.12%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 71.84%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 71.53%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 71.33%   [EVAL] batch:  194 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 71.95%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  200 | acc: 0.00%,  total acc: 71.67%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 71.38%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 71.09%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 70.86%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 70.70%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 70.48%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 72.27%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 72.46%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 72.15%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.86%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 72.72%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 72.65%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 72.49%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 72.48%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 72.35%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 73.43%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 73.53%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 73.50%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 73.39%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 73.35%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 73.29%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 73.20%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 73.94%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.53%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 75.72%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.64%   
cur_acc:  ['0.9484', '0.7401', '0.7262', '0.7083', '0.8869']
his_acc:  ['0.9484', '0.8425', '0.7796', '0.7282', '0.7564']
Clustering into  29  clusters
Clusters:  [ 0 27 18  0  0  0 23  0 21 28 14  0 15  0 16 20 19  0  0  0  0  0  0 25
  0  9  0 24  7 26  0 22  0 13 17  0  0  0 10  4  8 11  0  0 12  6  5  0
  0  2  3  0  0  0  0  1  0  0  0  0]
Losses:  5.815027236938477 1.1548576354980469
CurrentTrain: epoch  0, batch     0 | loss: 6.9698849Losses:  6.554692268371582 1.219728708267212
CurrentTrain: epoch  0, batch     1 | loss: 7.7744207Losses:  7.408238410949707 1.6947135925292969
CurrentTrain: epoch  0, batch     2 | loss: 9.1029520Losses:  5.873257160186768 1.5972719192504883
CurrentTrain: epoch  0, batch     3 | loss: 7.4705291Losses:  6.267210006713867 1.261997938156128
CurrentTrain: epoch  0, batch     4 | loss: 7.5292082Losses:  7.707289695739746 1.7274518013000488
CurrentTrain: epoch  0, batch     5 | loss: 9.4347420Losses:  8.244956016540527 0.47376924753189087
CurrentTrain: epoch  0, batch     6 | loss: 8.7187252Losses:  6.614346504211426 1.3001916408538818
CurrentTrain: epoch  1, batch     0 | loss: 7.9145384Losses:  5.248194694519043 1.5117073059082031
CurrentTrain: epoch  1, batch     1 | loss: 6.7599020Losses:  4.584354400634766 0.8293980360031128
CurrentTrain: epoch  1, batch     2 | loss: 5.4137526Losses:  5.850019454956055 1.5200823545455933
CurrentTrain: epoch  1, batch     3 | loss: 7.3701019Losses:  4.890583038330078 1.1270396709442139
CurrentTrain: epoch  1, batch     4 | loss: 6.0176229Losses:  4.7066168785095215 1.2794387340545654
CurrentTrain: epoch  1, batch     5 | loss: 5.9860554Losses:  4.3730316162109375 0.7444314956665039
CurrentTrain: epoch  1, batch     6 | loss: 5.1174631Losses:  5.69276762008667 1.0648878812789917
CurrentTrain: epoch  2, batch     0 | loss: 6.7576556Losses:  3.8906702995300293 1.3144350051879883
CurrentTrain: epoch  2, batch     1 | loss: 5.2051053Losses:  4.615335464477539 1.1200348138809204
CurrentTrain: epoch  2, batch     2 | loss: 5.7353702Losses:  3.7096846103668213 0.7671114206314087
CurrentTrain: epoch  2, batch     3 | loss: 4.4767962Losses:  3.9298083782196045 1.2692327499389648
CurrentTrain: epoch  2, batch     4 | loss: 5.1990414Losses:  4.141345024108887 1.4541683197021484
CurrentTrain: epoch  2, batch     5 | loss: 5.5955133Losses:  7.594959259033203 0.7896766662597656
CurrentTrain: epoch  2, batch     6 | loss: 8.3846359Losses:  4.150849342346191 1.3374764919281006
CurrentTrain: epoch  3, batch     0 | loss: 5.4883261Losses:  4.253232002258301 1.019632339477539
CurrentTrain: epoch  3, batch     1 | loss: 5.2728643Losses:  3.7369298934936523 1.175004005432129
CurrentTrain: epoch  3, batch     2 | loss: 4.9119339Losses:  2.9825844764709473 0.8784094452857971
CurrentTrain: epoch  3, batch     3 | loss: 3.8609939Losses:  3.7262489795684814 0.9173319935798645
CurrentTrain: epoch  3, batch     4 | loss: 4.6435809Losses:  4.959693908691406 1.0236221551895142
CurrentTrain: epoch  3, batch     5 | loss: 5.9833159Losses:  4.688385486602783 0.34472280740737915
CurrentTrain: epoch  3, batch     6 | loss: 5.0331082Losses:  4.576230525970459 1.2174686193466187
CurrentTrain: epoch  4, batch     0 | loss: 5.7936993Losses:  2.523289203643799 0.8671667575836182
CurrentTrain: epoch  4, batch     1 | loss: 3.3904560Losses:  4.5905656814575195 1.3010735511779785
CurrentTrain: epoch  4, batch     2 | loss: 5.8916392Losses:  3.5313899517059326 1.07652747631073
CurrentTrain: epoch  4, batch     3 | loss: 4.6079173Losses:  3.289102554321289 1.026808738708496
CurrentTrain: epoch  4, batch     4 | loss: 4.3159113Losses:  3.4077529907226562 0.8999354243278503
CurrentTrain: epoch  4, batch     5 | loss: 4.3076882Losses:  2.671417236328125 0.2394256740808487
CurrentTrain: epoch  4, batch     6 | loss: 2.9108429Losses:  2.620349884033203 0.8558551073074341
CurrentTrain: epoch  5, batch     0 | loss: 3.4762049Losses:  3.9712817668914795 1.120343565940857
CurrentTrain: epoch  5, batch     1 | loss: 5.0916252Losses:  2.981137990951538 0.9689418077468872
CurrentTrain: epoch  5, batch     2 | loss: 3.9500799Losses:  2.9889729022979736 0.8566712737083435
CurrentTrain: epoch  5, batch     3 | loss: 3.8456442Losses:  3.6928162574768066 1.0257186889648438
CurrentTrain: epoch  5, batch     4 | loss: 4.7185349Losses:  3.2804410457611084 1.1399476528167725
CurrentTrain: epoch  5, batch     5 | loss: 4.4203887Losses:  4.38605260848999 0.46400701999664307
CurrentTrain: epoch  5, batch     6 | loss: 4.8500595Losses:  3.822882652282715 0.9333289861679077
CurrentTrain: epoch  6, batch     0 | loss: 4.7562118Losses:  3.278785228729248 1.096207857131958
CurrentTrain: epoch  6, batch     1 | loss: 4.3749933Losses:  2.7678489685058594 0.6621005535125732
CurrentTrain: epoch  6, batch     2 | loss: 3.4299495Losses:  2.4297852516174316 1.018766164779663
CurrentTrain: epoch  6, batch     3 | loss: 3.4485514Losses:  2.824324131011963 0.6970072388648987
CurrentTrain: epoch  6, batch     4 | loss: 3.5213313Losses:  2.958336353302002 0.945689857006073
CurrentTrain: epoch  6, batch     5 | loss: 3.9040263Losses:  3.231309175491333 0.44200634956359863
CurrentTrain: epoch  6, batch     6 | loss: 3.6733155Losses:  3.9474422931671143 0.7866781949996948
CurrentTrain: epoch  7, batch     0 | loss: 4.7341204Losses:  2.2314422130584717 0.5689159631729126
CurrentTrain: epoch  7, batch     1 | loss: 2.8003583Losses:  3.0439014434814453 0.8566299676895142
CurrentTrain: epoch  7, batch     2 | loss: 3.9005313Losses:  2.1069936752319336 0.7194281816482544
CurrentTrain: epoch  7, batch     3 | loss: 2.8264217Losses:  3.1843037605285645 0.8705087304115295
CurrentTrain: epoch  7, batch     4 | loss: 4.0548124Losses:  3.0444703102111816 0.9819028377532959
CurrentTrain: epoch  7, batch     5 | loss: 4.0263729Losses:  2.337277412414551 0.2520461976528168
CurrentTrain: epoch  7, batch     6 | loss: 2.5893235Losses:  2.458533763885498 0.962591290473938
CurrentTrain: epoch  8, batch     0 | loss: 3.4211249Losses:  4.031649112701416 0.6067734956741333
CurrentTrain: epoch  8, batch     1 | loss: 4.6384225Losses:  2.434386730194092 0.9280695915222168
CurrentTrain: epoch  8, batch     2 | loss: 3.3624563Losses:  2.740856170654297 0.6311530470848083
CurrentTrain: epoch  8, batch     3 | loss: 3.3720093Losses:  2.05568528175354 0.6940401196479797
CurrentTrain: epoch  8, batch     4 | loss: 2.7497253Losses:  2.2784225940704346 0.7484497427940369
CurrentTrain: epoch  8, batch     5 | loss: 3.0268724Losses:  2.4613583087921143 0.2569059729576111
CurrentTrain: epoch  8, batch     6 | loss: 2.7182643Losses:  2.5808534622192383 0.9889212250709534
CurrentTrain: epoch  9, batch     0 | loss: 3.5697746Losses:  3.126681089401245 0.5614795088768005
CurrentTrain: epoch  9, batch     1 | loss: 3.6881607Losses:  2.2585842609405518 0.6129070520401001
CurrentTrain: epoch  9, batch     2 | loss: 2.8714914Losses:  2.900728940963745 0.8530386686325073
CurrentTrain: epoch  9, batch     3 | loss: 3.7537675Losses:  2.0299558639526367 0.6140310764312744
CurrentTrain: epoch  9, batch     4 | loss: 2.6439869Losses:  2.2372353076934814 0.6526985168457031
CurrentTrain: epoch  9, batch     5 | loss: 2.8899338Losses:  2.246687889099121 0.5541176199913025
CurrentTrain: epoch  9, batch     6 | loss: 2.8008056
Losses:  0.06331665068864822 0.5895246267318726
MemoryTrain:  epoch  0, batch     0 | loss: 0.6528413Losses:  0.15910795331001282 0.6087964773178101
MemoryTrain:  epoch  0, batch     1 | loss: 0.7679044Losses:  0.2519531846046448 0.5652766227722168
MemoryTrain:  epoch  0, batch     2 | loss: 0.8172298Losses:  1.4565865993499756 0.5774360299110413
MemoryTrain:  epoch  0, batch     3 | loss: 2.0340226Losses:  0.6155263781547546 0.6406071782112122
MemoryTrain:  epoch  0, batch     4 | loss: 1.2561336Losses:  0.48157596588134766 0.47858768701553345
MemoryTrain:  epoch  0, batch     5 | loss: 0.9601637Losses:  0.817119836807251 0.6044971942901611
MemoryTrain:  epoch  0, batch     6 | loss: 1.4216170Losses:  0.11914780735969543 0.35477015376091003
MemoryTrain:  epoch  0, batch     7 | loss: 0.4739180Losses:  0.5225076675415039 0.4916188418865204
MemoryTrain:  epoch  1, batch     0 | loss: 1.0141265Losses:  0.18505576252937317 0.36122167110443115
MemoryTrain:  epoch  1, batch     1 | loss: 0.5462774Losses:  1.1277610063552856 0.43333667516708374
MemoryTrain:  epoch  1, batch     2 | loss: 1.5610976Losses:  1.4412598609924316 0.9471582174301147
MemoryTrain:  epoch  1, batch     3 | loss: 2.3884182Losses:  0.1661490797996521 0.3698768615722656
MemoryTrain:  epoch  1, batch     4 | loss: 0.5360259Losses:  0.9282065033912659 0.5427776575088501
MemoryTrain:  epoch  1, batch     5 | loss: 1.4709842Losses:  0.1323719620704651 0.629319429397583
MemoryTrain:  epoch  1, batch     6 | loss: 0.7616914Losses:  0.11103282868862152 0.3881724774837494
MemoryTrain:  epoch  1, batch     7 | loss: 0.4992053Losses:  0.2294161021709442 0.7412294149398804
MemoryTrain:  epoch  2, batch     0 | loss: 0.9706455Losses:  0.7868586778640747 0.42602550983428955
MemoryTrain:  epoch  2, batch     1 | loss: 1.2128842Losses:  0.18942245841026306 0.719868004322052
MemoryTrain:  epoch  2, batch     2 | loss: 0.9092904Losses:  0.05424501746892929 0.35185009241104126
MemoryTrain:  epoch  2, batch     3 | loss: 0.4060951Losses:  0.4051090478897095 0.6191549301147461
MemoryTrain:  epoch  2, batch     4 | loss: 1.0242640Losses:  0.10875530540943146 0.4977830648422241
MemoryTrain:  epoch  2, batch     5 | loss: 0.6065384Losses:  0.12507307529449463 0.34771493077278137
MemoryTrain:  epoch  2, batch     6 | loss: 0.4727880Losses:  0.11473849415779114 0.14248664677143097
MemoryTrain:  epoch  2, batch     7 | loss: 0.2572252Losses:  0.1697102040052414 0.3866952657699585
MemoryTrain:  epoch  3, batch     0 | loss: 0.5564055Losses:  0.12333221733570099 0.4592876732349396
MemoryTrain:  epoch  3, batch     1 | loss: 0.5826199Losses:  0.10202872008085251 0.39254045486450195
MemoryTrain:  epoch  3, batch     2 | loss: 0.4945692Losses:  0.10075003653764725 0.5972508192062378
MemoryTrain:  epoch  3, batch     3 | loss: 0.6980008Losses:  0.07270745933055878 0.4777542054653168
MemoryTrain:  epoch  3, batch     4 | loss: 0.5504616Losses:  0.09392859041690826 0.42732179164886475
MemoryTrain:  epoch  3, batch     5 | loss: 0.5212504Losses:  0.2617042064666748 0.6144672632217407
MemoryTrain:  epoch  3, batch     6 | loss: 0.8761715Losses:  0.10618539899587631 0.38905319571495056
MemoryTrain:  epoch  3, batch     7 | loss: 0.4952386Losses:  0.07770611345767975 0.6787992715835571
MemoryTrain:  epoch  4, batch     0 | loss: 0.7565054Losses:  0.09896054118871689 0.7843601703643799
MemoryTrain:  epoch  4, batch     1 | loss: 0.8833207Losses:  0.050642549991607666 0.2733948230743408
MemoryTrain:  epoch  4, batch     2 | loss: 0.3240374Losses:  0.03751777485013008 0.40553510189056396
MemoryTrain:  epoch  4, batch     3 | loss: 0.4430529Losses:  0.07964577525854111 0.656013011932373
MemoryTrain:  epoch  4, batch     4 | loss: 0.7356588Losses:  0.10672055184841156 0.4141273498535156
MemoryTrain:  epoch  4, batch     5 | loss: 0.5208479Losses:  0.06877636909484863 0.5165445804595947
MemoryTrain:  epoch  4, batch     6 | loss: 0.5853209Losses:  0.0620342418551445 0.24482174217700958
MemoryTrain:  epoch  4, batch     7 | loss: 0.3068560Losses:  0.060426633805036545 0.37682342529296875
MemoryTrain:  epoch  5, batch     0 | loss: 0.4372500Losses:  0.10522212088108063 0.6956205368041992
MemoryTrain:  epoch  5, batch     1 | loss: 0.8008426Losses:  0.05169115215539932 0.5273252725601196
MemoryTrain:  epoch  5, batch     2 | loss: 0.5790164Losses:  0.08442133665084839 0.6377207040786743
MemoryTrain:  epoch  5, batch     3 | loss: 0.7221420Losses:  0.04404808580875397 0.6254925727844238
MemoryTrain:  epoch  5, batch     4 | loss: 0.6695406Losses:  0.03077896498143673 0.2673034965991974
MemoryTrain:  epoch  5, batch     5 | loss: 0.2980825Losses:  0.05588730797171593 0.47780710458755493
MemoryTrain:  epoch  5, batch     6 | loss: 0.5336944Losses:  0.08261129260063171 0.13685470819473267
MemoryTrain:  epoch  5, batch     7 | loss: 0.2194660Losses:  0.046974822878837585 0.3456222116947174
MemoryTrain:  epoch  6, batch     0 | loss: 0.3925970Losses:  0.04152793064713478 0.49802902340888977
MemoryTrain:  epoch  6, batch     1 | loss: 0.5395570Losses:  0.03718692809343338 0.4315270781517029
MemoryTrain:  epoch  6, batch     2 | loss: 0.4687140Losses:  0.04139234870672226 0.5712423324584961
MemoryTrain:  epoch  6, batch     3 | loss: 0.6126347Losses:  0.033034972846508026 0.29705673456192017
MemoryTrain:  epoch  6, batch     4 | loss: 0.3300917Losses:  0.05929192528128624 0.6844673156738281
MemoryTrain:  epoch  6, batch     5 | loss: 0.7437592Losses:  0.07720962911844254 0.5640297532081604
MemoryTrain:  epoch  6, batch     6 | loss: 0.6412394Losses:  0.04555036127567291 0.17220431566238403
MemoryTrain:  epoch  6, batch     7 | loss: 0.2177547Losses:  0.044364556670188904 0.45759493112564087
MemoryTrain:  epoch  7, batch     0 | loss: 0.5019595Losses:  0.022382376715540886 0.3984420597553253
MemoryTrain:  epoch  7, batch     1 | loss: 0.4208244Losses:  0.03405298292636871 0.3671279847621918
MemoryTrain:  epoch  7, batch     2 | loss: 0.4011810Losses:  0.029167072847485542 0.4075600504875183
MemoryTrain:  epoch  7, batch     3 | loss: 0.4367271Losses:  0.06564052402973175 0.3738936185836792
MemoryTrain:  epoch  7, batch     4 | loss: 0.4395341Losses:  0.027560370042920113 0.6567395925521851
MemoryTrain:  epoch  7, batch     5 | loss: 0.6842999Losses:  0.10467559099197388 0.5575986504554749
MemoryTrain:  epoch  7, batch     6 | loss: 0.6622742Losses:  0.0450441837310791 0.21019089221954346
MemoryTrain:  epoch  7, batch     7 | loss: 0.2552351Losses:  0.03923458978533745 0.4380307197570801
MemoryTrain:  epoch  8, batch     0 | loss: 0.4772653Losses:  0.03986778110265732 0.5367459058761597
MemoryTrain:  epoch  8, batch     1 | loss: 0.5766137Losses:  0.025112047791481018 0.3391687273979187
MemoryTrain:  epoch  8, batch     2 | loss: 0.3642808Losses:  0.03464619815349579 0.4823480248451233
MemoryTrain:  epoch  8, batch     3 | loss: 0.5169942Losses:  0.03867281228303909 0.3568370044231415
MemoryTrain:  epoch  8, batch     4 | loss: 0.3955098Losses:  0.026193376630544662 0.2638874650001526
MemoryTrain:  epoch  8, batch     5 | loss: 0.2900808Losses:  0.059833236038684845 0.4570121765136719
MemoryTrain:  epoch  8, batch     6 | loss: 0.5168454Losses:  0.060111697763204575 0.3402804732322693
MemoryTrain:  epoch  8, batch     7 | loss: 0.4003922Losses:  0.04487387835979462 0.3014320731163025
MemoryTrain:  epoch  9, batch     0 | loss: 0.3463060Losses:  0.02556793764233589 0.5012295246124268
MemoryTrain:  epoch  9, batch     1 | loss: 0.5267975Losses:  0.03519197180867195 0.47463512420654297
MemoryTrain:  epoch  9, batch     2 | loss: 0.5098271Losses:  0.035277530550956726 0.31814509630203247
MemoryTrain:  epoch  9, batch     3 | loss: 0.3534226Losses:  0.0706036388874054 0.5618629455566406
MemoryTrain:  epoch  9, batch     4 | loss: 0.6324666Losses:  0.02987077459692955 0.4949292540550232
MemoryTrain:  epoch  9, batch     5 | loss: 0.5248000Losses:  0.0413191094994545 0.4062936305999756
MemoryTrain:  epoch  9, batch     6 | loss: 0.4476127Losses:  0.021108796820044518 0.12579157948493958
MemoryTrain:  epoch  9, batch     7 | loss: 0.1469004
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 54.17%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.42%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 52.73%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 54.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 55.89%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 56.60%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 57.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 58.39%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 59.13%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 60.98%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 64.36%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 64.95%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.57%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 65.28%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.67%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 84.49%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.15%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 83.62%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 83.37%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 82.84%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 82.03%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 81.63%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 80.69%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 80.33%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 80.07%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 80.18%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 79.86%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 79.79%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 79.48%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.44%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.33%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.27%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 78.56%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 78.52%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 80.69%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 80.45%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 80.16%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 80.05%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.12%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 80.07%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.67%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 79.28%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 78.78%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 77.96%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 77.93%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.81%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 77.17%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 76.54%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 75.97%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 74.85%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 74.71%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.66%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 74.76%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.14%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 75.22%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 75.26%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 75.13%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.08%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.04%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 74.92%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 74.59%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 74.39%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 74.07%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 73.71%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 73.64%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 73.45%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 73.15%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 72.84%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 72.74%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 72.75%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 72.80%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 72.72%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.46%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 72.19%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 71.84%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 71.46%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 71.19%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 70.96%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 71.92%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 71.64%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 71.37%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 71.06%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 70.76%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 70.55%   [EVAL] batch:  194 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  200 | acc: 6.25%,  total acc: 70.99%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.67%   [EVAL] batch:  202 | acc: 6.25%,  total acc: 70.35%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 70.07%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 69.82%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 69.57%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 71.65%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 71.64%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 71.55%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 71.85%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 71.45%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.42%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 71.28%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 71.26%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 71.15%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 71.68%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 71.45%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 71.46%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 71.36%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 72.33%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 73.74%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 74.08%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 73.89%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 73.71%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 73.54%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 73.40%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 73.23%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 73.18%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 73.31%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 73.46%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  326 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:  330 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  331 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 73.67%   [EVAL] batch:  333 | acc: 43.75%,  total acc: 73.58%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 73.56%   [EVAL] batch:  335 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 73.42%   [EVAL] batch:  337 | acc: 37.50%,  total acc: 73.32%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 73.10%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 72.89%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 72.67%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 72.48%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 72.13%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.26%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 72.90%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 72.87%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 72.91%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 72.93%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 72.85%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.78%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 72.75%   
cur_acc:  ['0.9484', '0.7401', '0.7262', '0.7083', '0.8869', '0.6528']
his_acc:  ['0.9484', '0.8425', '0.7796', '0.7282', '0.7564', '0.7275']
Clustering into  34  clusters
Clusters:  [ 0 23 24  0  0  0 28  0 19 33 26  0 20  0 17 11  9  0  0  0  0  0  0 25
  0 16  0 27 31 32  0 29  0  0 22  0  0  0 12 21 18  5  0  0 13 30 14  0
  0 15 10  0  0  2  2  8  0  0  0  0  4  7  2  0  0  3  0  6  0  1]
Losses:  6.959245681762695 1.1366100311279297
CurrentTrain: epoch  0, batch     0 | loss: 8.0958557Losses:  6.0062971115112305 1.3857223987579346
CurrentTrain: epoch  0, batch     1 | loss: 7.3920193Losses:  6.120595932006836 1.1962565183639526
CurrentTrain: epoch  0, batch     2 | loss: 7.3168526Losses:  6.713922500610352 1.185211420059204
CurrentTrain: epoch  0, batch     3 | loss: 7.8991337Losses:  7.371741771697998 1.3676955699920654
CurrentTrain: epoch  0, batch     4 | loss: 8.7394371Losses:  6.555488586425781 1.175990104675293
CurrentTrain: epoch  0, batch     5 | loss: 7.7314787Losses:  7.897324085235596 0.5380349159240723
CurrentTrain: epoch  0, batch     6 | loss: 8.4353590Losses:  6.2439680099487305 0.795748770236969
CurrentTrain: epoch  1, batch     0 | loss: 7.0397167Losses:  6.091860771179199 0.9014149308204651
CurrentTrain: epoch  1, batch     1 | loss: 6.9932756Losses:  6.427119255065918 1.2970473766326904
CurrentTrain: epoch  1, batch     2 | loss: 7.7241669Losses:  4.6177978515625 1.0704816579818726
CurrentTrain: epoch  1, batch     3 | loss: 5.6882796Losses:  4.694555759429932 0.8423148393630981
CurrentTrain: epoch  1, batch     4 | loss: 5.5368705Losses:  5.385232925415039 1.0150115489959717
CurrentTrain: epoch  1, batch     5 | loss: 6.4002447Losses:  4.602246284484863 0.5551653504371643
CurrentTrain: epoch  1, batch     6 | loss: 5.1574116Losses:  4.669356346130371 0.9326889514923096
CurrentTrain: epoch  2, batch     0 | loss: 5.6020451Losses:  4.79699182510376 0.9688848257064819
CurrentTrain: epoch  2, batch     1 | loss: 5.7658768Losses:  4.411979675292969 0.8974148631095886
CurrentTrain: epoch  2, batch     2 | loss: 5.3093944Losses:  4.584897518157959 1.1067594289779663
CurrentTrain: epoch  2, batch     3 | loss: 5.6916571Losses:  3.6394338607788086 0.6356514692306519
CurrentTrain: epoch  2, batch     4 | loss: 4.2750854Losses:  5.540762901306152 1.1282824277877808
CurrentTrain: epoch  2, batch     5 | loss: 6.6690454Losses:  7.611650466918945 0.4192367196083069
CurrentTrain: epoch  2, batch     6 | loss: 8.0308876Losses:  3.2921576499938965 0.6399968862533569
CurrentTrain: epoch  3, batch     0 | loss: 3.9321547Losses:  4.990964889526367 1.3304479122161865
CurrentTrain: epoch  3, batch     1 | loss: 6.3214130Losses:  3.399730682373047 0.7700279951095581
CurrentTrain: epoch  3, batch     2 | loss: 4.1697588Losses:  3.3839056491851807 0.6909670829772949
CurrentTrain: epoch  3, batch     3 | loss: 4.0748730Losses:  5.354462146759033 0.7879923582077026
CurrentTrain: epoch  3, batch     4 | loss: 6.1424546Losses:  3.7883968353271484 0.9248865842819214
CurrentTrain: epoch  3, batch     5 | loss: 4.7132835Losses:  2.7231285572052 0.18651005625724792
CurrentTrain: epoch  3, batch     6 | loss: 2.9096386Losses:  3.921370506286621 0.947412371635437
CurrentTrain: epoch  4, batch     0 | loss: 4.8687830Losses:  3.2844924926757812 0.6615490913391113
CurrentTrain: epoch  4, batch     1 | loss: 3.9460416Losses:  5.037786483764648 0.8919443488121033
CurrentTrain: epoch  4, batch     2 | loss: 5.9297309Losses:  4.062565803527832 0.9841921925544739
CurrentTrain: epoch  4, batch     3 | loss: 5.0467582Losses:  3.312180995941162 0.8131822943687439
CurrentTrain: epoch  4, batch     4 | loss: 4.1253633Losses:  2.8860888481140137 0.6812566518783569
CurrentTrain: epoch  4, batch     5 | loss: 3.5673456Losses:  5.455841064453125 0.27690649032592773
CurrentTrain: epoch  4, batch     6 | loss: 5.7327476Losses:  3.74918270111084 1.0012544393539429
CurrentTrain: epoch  5, batch     0 | loss: 4.7504373Losses:  3.96104097366333 0.9648056030273438
CurrentTrain: epoch  5, batch     1 | loss: 4.9258466Losses:  3.322657346725464 0.7888359427452087
CurrentTrain: epoch  5, batch     2 | loss: 4.1114931Losses:  2.9034414291381836 0.6353657245635986
CurrentTrain: epoch  5, batch     3 | loss: 3.5388072Losses:  2.1827847957611084 0.5124167203903198
CurrentTrain: epoch  5, batch     4 | loss: 2.6952014Losses:  5.048018455505371 0.8280358910560608
CurrentTrain: epoch  5, batch     5 | loss: 5.8760543Losses:  4.240637302398682 0.23037204146385193
CurrentTrain: epoch  5, batch     6 | loss: 4.4710093Losses:  2.7759432792663574 0.793754518032074
CurrentTrain: epoch  6, batch     0 | loss: 3.5696979Losses:  3.6667733192443848 0.7000342011451721
CurrentTrain: epoch  6, batch     1 | loss: 4.3668075Losses:  2.9131805896759033 0.6652390956878662
CurrentTrain: epoch  6, batch     2 | loss: 3.5784197Losses:  2.627199172973633 0.5276232957839966
CurrentTrain: epoch  6, batch     3 | loss: 3.1548223Losses:  3.6162095069885254 0.7320818901062012
CurrentTrain: epoch  6, batch     4 | loss: 4.3482914Losses:  3.5668139457702637 0.9254866242408752
CurrentTrain: epoch  6, batch     5 | loss: 4.4923005Losses:  5.728306293487549 0.36921629309654236
CurrentTrain: epoch  6, batch     6 | loss: 6.0975227Losses:  2.46423077583313 0.627331554889679
CurrentTrain: epoch  7, batch     0 | loss: 3.0915623Losses:  3.0801546573638916 0.5012092590332031
CurrentTrain: epoch  7, batch     1 | loss: 3.5813639Losses:  3.8588647842407227 0.7943325042724609
CurrentTrain: epoch  7, batch     2 | loss: 4.6531973Losses:  2.2281229496002197 0.5448501706123352
CurrentTrain: epoch  7, batch     3 | loss: 2.7729731Losses:  3.2914721965789795 0.6616070866584778
CurrentTrain: epoch  7, batch     4 | loss: 3.9530792Losses:  4.187418460845947 0.6880168914794922
CurrentTrain: epoch  7, batch     5 | loss: 4.8754354Losses:  2.0850634574890137 1.1920928955078125e-07
CurrentTrain: epoch  7, batch     6 | loss: 2.0850635Losses:  3.1736178398132324 0.6269882917404175
CurrentTrain: epoch  8, batch     0 | loss: 3.8006063Losses:  3.0506575107574463 0.6022682189941406
CurrentTrain: epoch  8, batch     1 | loss: 3.6529257Losses:  2.857426643371582 0.5741140842437744
CurrentTrain: epoch  8, batch     2 | loss: 3.4315407Losses:  2.4263434410095215 0.42231810092926025
CurrentTrain: epoch  8, batch     3 | loss: 2.8486614Losses:  2.9147818088531494 0.6932876706123352
CurrentTrain: epoch  8, batch     4 | loss: 3.6080694Losses:  2.6668357849121094 0.652363657951355
CurrentTrain: epoch  8, batch     5 | loss: 3.3191996Losses:  3.374300003051758 0.0717194527387619
CurrentTrain: epoch  8, batch     6 | loss: 3.4460194Losses:  3.111206531524658 0.5975894927978516
CurrentTrain: epoch  9, batch     0 | loss: 3.7087960Losses:  3.077244520187378 0.6195123791694641
CurrentTrain: epoch  9, batch     1 | loss: 3.6967568Losses:  2.8501291275024414 0.6133320927619934
CurrentTrain: epoch  9, batch     2 | loss: 3.4634612Losses:  3.1933207511901855 0.7513371706008911
CurrentTrain: epoch  9, batch     3 | loss: 3.9446578Losses:  1.9323838949203491 0.3418857157230377
CurrentTrain: epoch  9, batch     4 | loss: 2.2742696Losses:  3.143277645111084 0.6831016540527344
CurrentTrain: epoch  9, batch     5 | loss: 3.8263793Losses:  1.750321626663208 0.09039555490016937
CurrentTrain: epoch  9, batch     6 | loss: 1.8407172
Losses:  1.4234561920166016 0.48539602756500244
MemoryTrain:  epoch  0, batch     0 | loss: 1.9088522Losses:  1.146228551864624 0.38640233874320984
MemoryTrain:  epoch  0, batch     1 | loss: 1.5326309Losses:  0.08561673015356064 0.48189282417297363
MemoryTrain:  epoch  0, batch     2 | loss: 0.5675095Losses:  0.6460644006729126 0.653048038482666
MemoryTrain:  epoch  0, batch     3 | loss: 1.2991124Losses:  0.6752303838729858 0.6039818525314331
MemoryTrain:  epoch  0, batch     4 | loss: 1.2792122Losses:  0.14963743090629578 0.4426755905151367
MemoryTrain:  epoch  0, batch     5 | loss: 0.5923131Losses:  0.02988964132964611 0.3417432904243469
MemoryTrain:  epoch  0, batch     6 | loss: 0.3716329Losses:  0.3572078049182892 0.5867797136306763
MemoryTrain:  epoch  0, batch     7 | loss: 0.9439875Losses:  0.058321401476860046 0.535969078540802
MemoryTrain:  epoch  0, batch     8 | loss: 0.5942905Losses:  0.18984279036521912 0.37887799739837646
MemoryTrain:  epoch  1, batch     0 | loss: 0.5687208Losses:  1.4572547674179077 0.4890735149383545
MemoryTrain:  epoch  1, batch     1 | loss: 1.9463283Losses:  0.9606208801269531 0.4756549000740051
MemoryTrain:  epoch  1, batch     2 | loss: 1.4362757Losses:  0.874930739402771 0.49114659428596497
MemoryTrain:  epoch  1, batch     3 | loss: 1.3660773Losses:  0.9491513967514038 0.6742419004440308
MemoryTrain:  epoch  1, batch     4 | loss: 1.6233933Losses:  0.28874143958091736 0.5020126104354858
MemoryTrain:  epoch  1, batch     5 | loss: 0.7907541Losses:  1.0619550943374634 0.3349875807762146
MemoryTrain:  epoch  1, batch     6 | loss: 1.3969426Losses:  0.3308205008506775 0.5336529016494751
MemoryTrain:  epoch  1, batch     7 | loss: 0.8644734Losses:  0.37179142236709595 0.2815276086330414
MemoryTrain:  epoch  1, batch     8 | loss: 0.6533190Losses:  0.6297729015350342 0.6777811050415039
MemoryTrain:  epoch  2, batch     0 | loss: 1.3075540Losses:  0.13091132044792175 0.40238162875175476
MemoryTrain:  epoch  2, batch     1 | loss: 0.5332929Losses:  0.07111009210348129 0.5083974003791809
MemoryTrain:  epoch  2, batch     2 | loss: 0.5795075Losses:  0.23516954481601715 0.6121345162391663
MemoryTrain:  epoch  2, batch     3 | loss: 0.8473040Losses:  0.42902305722236633 0.3098161816596985
MemoryTrain:  epoch  2, batch     4 | loss: 0.7388393Losses:  0.696448028087616 0.47494369745254517
MemoryTrain:  epoch  2, batch     5 | loss: 1.1713917Losses:  0.45411720871925354 0.44462624192237854
MemoryTrain:  epoch  2, batch     6 | loss: 0.8987435Losses:  0.9228528141975403 0.4883582890033722
MemoryTrain:  epoch  2, batch     7 | loss: 1.4112111Losses:  0.7621557116508484 0.3297745883464813
MemoryTrain:  epoch  2, batch     8 | loss: 1.0919303Losses:  0.04895191639661789 0.3980977535247803
MemoryTrain:  epoch  3, batch     0 | loss: 0.4470497Losses:  0.5710476040840149 0.4508174657821655
MemoryTrain:  epoch  3, batch     1 | loss: 1.0218651Losses:  0.2759215831756592 0.38592416048049927
MemoryTrain:  epoch  3, batch     2 | loss: 0.6618457Losses:  0.08827032893896103 0.29501670598983765
MemoryTrain:  epoch  3, batch     3 | loss: 0.3832870Losses:  0.06982038170099258 0.5574769973754883
MemoryTrain:  epoch  3, batch     4 | loss: 0.6272974Losses:  0.11554166674613953 0.51558917760849
MemoryTrain:  epoch  3, batch     5 | loss: 0.6311308Losses:  0.24104376137256622 0.5258016586303711
MemoryTrain:  epoch  3, batch     6 | loss: 0.7668454Losses:  0.06406064331531525 0.4521072208881378
MemoryTrain:  epoch  3, batch     7 | loss: 0.5161679Losses:  0.37804317474365234 0.40252161026000977
MemoryTrain:  epoch  3, batch     8 | loss: 0.7805648Losses:  0.18423527479171753 0.45090723037719727
MemoryTrain:  epoch  4, batch     0 | loss: 0.6351425Losses:  0.0905870795249939 0.41596487164497375
MemoryTrain:  epoch  4, batch     1 | loss: 0.5065520Losses:  0.04134783893823624 0.37451329827308655
MemoryTrain:  epoch  4, batch     2 | loss: 0.4158611Losses:  0.06657609343528748 0.5228085517883301
MemoryTrain:  epoch  4, batch     3 | loss: 0.5893847Losses:  0.10768253356218338 0.47942113876342773
MemoryTrain:  epoch  4, batch     4 | loss: 0.5871037Losses:  0.22534185647964478 0.44270387291908264
MemoryTrain:  epoch  4, batch     5 | loss: 0.6680458Losses:  0.03433576226234436 0.23827320337295532
MemoryTrain:  epoch  4, batch     6 | loss: 0.2726090Losses:  0.04654421657323837 0.4229772090911865
MemoryTrain:  epoch  4, batch     7 | loss: 0.4695214Losses:  0.10310865938663483 0.5220208168029785
MemoryTrain:  epoch  4, batch     8 | loss: 0.6251295Losses:  0.048249952495098114 0.4925362169742584
MemoryTrain:  epoch  5, batch     0 | loss: 0.5407861Losses:  0.13772545754909515 0.24712949991226196
MemoryTrain:  epoch  5, batch     1 | loss: 0.3848550Losses:  0.08072284609079361 0.44858330488204956
MemoryTrain:  epoch  5, batch     2 | loss: 0.5293062Losses:  0.023539112880825996 0.3402058184146881
MemoryTrain:  epoch  5, batch     3 | loss: 0.3637449Losses:  0.07649175077676773 0.4749472141265869
MemoryTrain:  epoch  5, batch     4 | loss: 0.5514390Losses:  0.12467889487743378 0.4617844820022583
MemoryTrain:  epoch  5, batch     5 | loss: 0.5864634Losses:  0.052648723125457764 0.4199190139770508
MemoryTrain:  epoch  5, batch     6 | loss: 0.4725677Losses:  0.04419321566820145 0.518811821937561
MemoryTrain:  epoch  5, batch     7 | loss: 0.5630050Losses:  0.07027053087949753 0.39852699637413025
MemoryTrain:  epoch  5, batch     8 | loss: 0.4687975Losses:  0.17532847821712494 0.41694197058677673
MemoryTrain:  epoch  6, batch     0 | loss: 0.5922704Losses:  0.03875567764043808 0.36037734150886536
MemoryTrain:  epoch  6, batch     1 | loss: 0.3991330Losses:  0.03744804114103317 0.3650984466075897
MemoryTrain:  epoch  6, batch     2 | loss: 0.4025465Losses:  0.045516639947891235 0.5932825803756714
MemoryTrain:  epoch  6, batch     3 | loss: 0.6387992Losses:  0.05573092773556709 0.5382817983627319
MemoryTrain:  epoch  6, batch     4 | loss: 0.5940127Losses:  0.09630976617336273 0.4374712109565735
MemoryTrain:  epoch  6, batch     5 | loss: 0.5337810Losses:  0.04696265980601311 0.4094927906990051
MemoryTrain:  epoch  6, batch     6 | loss: 0.4564554Losses:  0.06565704941749573 0.35847407579421997
MemoryTrain:  epoch  6, batch     7 | loss: 0.4241311Losses:  0.03964664414525032 0.2572496831417084
MemoryTrain:  epoch  6, batch     8 | loss: 0.2968963Losses:  0.054161496460437775 0.4696235656738281
MemoryTrain:  epoch  7, batch     0 | loss: 0.5237851Losses:  0.07819738984107971 0.4978257417678833
MemoryTrain:  epoch  7, batch     1 | loss: 0.5760231Losses:  0.10981268435716629 0.37964126467704773
MemoryTrain:  epoch  7, batch     2 | loss: 0.4894539Losses:  0.08002415299415588 0.46855419874191284
MemoryTrain:  epoch  7, batch     3 | loss: 0.5485784Losses:  0.06872596591711044 0.5058268904685974
MemoryTrain:  epoch  7, batch     4 | loss: 0.5745528Losses:  0.03046969696879387 0.25679177045822144
MemoryTrain:  epoch  7, batch     5 | loss: 0.2872615Losses:  0.03775960952043533 0.355233371257782
MemoryTrain:  epoch  7, batch     6 | loss: 0.3929930Losses:  0.049486979842185974 0.4281129240989685
MemoryTrain:  epoch  7, batch     7 | loss: 0.4775999Losses:  0.034330353140830994 0.3462112545967102
MemoryTrain:  epoch  7, batch     8 | loss: 0.3805416Losses:  0.08441288024187088 0.4800044298171997
MemoryTrain:  epoch  8, batch     0 | loss: 0.5644173Losses:  0.03971394896507263 0.41238418221473694
MemoryTrain:  epoch  8, batch     1 | loss: 0.4520981Losses:  0.0688275620341301 0.38740283250808716
MemoryTrain:  epoch  8, batch     2 | loss: 0.4562304Losses:  0.054267629981040955 0.33845412731170654
MemoryTrain:  epoch  8, batch     3 | loss: 0.3927218Losses:  0.047117654234170914 0.5957902073860168
MemoryTrain:  epoch  8, batch     4 | loss: 0.6429079Losses:  0.03250248730182648 0.58730149269104
MemoryTrain:  epoch  8, batch     5 | loss: 0.6198040Losses:  0.03988038748502731 0.25755298137664795
MemoryTrain:  epoch  8, batch     6 | loss: 0.2974334Losses:  0.056351736187934875 0.4757404327392578
MemoryTrain:  epoch  8, batch     7 | loss: 0.5320922Losses:  0.03973768278956413 0.19165539741516113
MemoryTrain:  epoch  8, batch     8 | loss: 0.2313931Losses:  0.16186238825321198 0.39671316742897034
MemoryTrain:  epoch  9, batch     0 | loss: 0.5585756Losses:  0.02382938750088215 0.27716192603111267
MemoryTrain:  epoch  9, batch     1 | loss: 0.3009913Losses:  0.02619478665292263 0.29723304510116577
MemoryTrain:  epoch  9, batch     2 | loss: 0.3234278Losses:  0.039033740758895874 0.25592464208602905
MemoryTrain:  epoch  9, batch     3 | loss: 0.2949584Losses:  0.03637009859085083 0.3373226821422577
MemoryTrain:  epoch  9, batch     4 | loss: 0.3736928Losses:  0.03925847262144089 0.42865628004074097
MemoryTrain:  epoch  9, batch     5 | loss: 0.4679148Losses:  0.042555056512355804 0.5272189378738403
MemoryTrain:  epoch  9, batch     6 | loss: 0.5697740Losses:  0.05123617500066757 0.39103326201438904
MemoryTrain:  epoch  9, batch     7 | loss: 0.4422694Losses:  0.1802334040403366 0.5045944452285767
MemoryTrain:  epoch  9, batch     8 | loss: 0.6848279
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 58.41%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 56.46%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 54.64%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 55.47%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 56.63%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 57.72%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 61.35%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 58.14%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 58.10%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 60.11%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 61.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 61.42%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 61.44%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 65.12%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 64.78%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 85.64%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 85.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 86.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 85.86%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 85.34%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.96%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.07%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.63%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 82.71%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 82.21%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 81.72%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 80.53%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 79.77%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.39%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 78.95%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 78.53%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 78.36%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.59%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 77.11%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 76.71%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 76.03%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 75.44%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 74.93%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 74.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 75.34%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 77.29%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 76.82%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.85%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 76.83%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.46%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 75.98%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 75.52%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 75.11%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 74.72%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.50%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.62%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.89%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.05%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.43%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 73.81%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.21%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 72.66%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 72.08%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 71.55%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 71.56%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.48%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 71.51%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 71.49%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.69%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 72.35%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 72.33%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 72.41%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 72.39%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 72.07%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 71.79%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 71.49%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 71.27%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 70.91%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 70.74%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 70.48%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 70.10%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 69.98%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 69.76%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 69.45%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 68.71%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 68.10%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 67.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 68.72%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 68.45%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.19%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 67.58%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 67.36%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 67.47%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  200 | acc: 12.50%,  total acc: 67.82%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 67.57%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 67.30%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 67.06%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 66.89%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 67.56%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  218 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.07%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 68.36%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 68.28%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 68.07%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 68.62%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 68.57%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 68.47%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 68.32%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 68.25%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 68.84%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 68.61%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 70.68%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 70.79%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 70.96%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 70.57%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 70.35%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 70.13%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.37%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 70.40%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 70.36%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 70.35%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 70.20%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 69.93%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 69.81%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 69.45%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 69.30%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 69.10%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 68.90%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.70%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 68.49%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.31%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 68.17%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  345 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  359 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.04%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 68.99%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 68.95%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 68.93%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 68.87%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 69.18%   [EVAL] batch:  388 | acc: 6.25%,  total acc: 69.02%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 68.89%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 68.73%   [EVAL] batch:  391 | acc: 12.50%,  total acc: 68.59%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 68.43%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 68.61%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.47%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 68.04%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:  413 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 68.13%   [EVAL] batch:  415 | acc: 18.75%,  total acc: 68.01%   [EVAL] batch:  416 | acc: 31.25%,  total acc: 67.93%   [EVAL] batch:  417 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 67.83%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 68.03%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 68.07%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 68.36%   
cur_acc:  ['0.9484', '0.7401', '0.7262', '0.7083', '0.8869', '0.6528', '0.6478']
his_acc:  ['0.9484', '0.8425', '0.7796', '0.7282', '0.7564', '0.7275', '0.6836']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0  0  0  0 12
  0 37  0 23 31 36  2 34  0  0 30  0  0  0 27 11 18 29  0  0 28 32 13  0
  0 17 16  0  0  1  1 20  0  0  0  0 15  9  1  0  0  8  0 14  0  7  5 10
  0  0  0  2  0  3  6  4]
Losses:  6.640834808349609 0.9801502823829651
CurrentTrain: epoch  0, batch     0 | loss: 7.6209850Losses:  6.19403076171875 0.9465367794036865
CurrentTrain: epoch  0, batch     1 | loss: 7.1405678Losses:  4.83211088180542 1.056318759918213
CurrentTrain: epoch  0, batch     2 | loss: 5.8884296Losses:  5.928876876831055 0.8810529708862305
CurrentTrain: epoch  0, batch     3 | loss: 6.8099298Losses:  6.3394927978515625 1.3569252490997314
CurrentTrain: epoch  0, batch     4 | loss: 7.6964178Losses:  4.798732280731201 1.0340852737426758
CurrentTrain: epoch  0, batch     5 | loss: 5.8328176Losses:  5.691891193389893 0.15270867943763733
CurrentTrain: epoch  0, batch     6 | loss: 5.8445997Losses:  4.525367259979248 0.931647539138794
CurrentTrain: epoch  1, batch     0 | loss: 5.4570150Losses:  4.565840721130371 0.9138101935386658
CurrentTrain: epoch  1, batch     1 | loss: 5.4796510Losses:  4.225886344909668 0.9320066571235657
CurrentTrain: epoch  1, batch     2 | loss: 5.1578932Losses:  5.363653182983398 1.093505859375
CurrentTrain: epoch  1, batch     3 | loss: 6.4571590Losses:  4.834529876708984 0.9633485078811646
CurrentTrain: epoch  1, batch     4 | loss: 5.7978783Losses:  3.5667471885681152 0.8277580738067627
CurrentTrain: epoch  1, batch     5 | loss: 4.3945055Losses:  3.3723723888397217 0.13228407502174377
CurrentTrain: epoch  1, batch     6 | loss: 3.5046566Losses:  5.249945640563965 1.0457673072814941
CurrentTrain: epoch  2, batch     0 | loss: 6.2957129Losses:  5.183646202087402 0.8747007846832275
CurrentTrain: epoch  2, batch     1 | loss: 6.0583467Losses:  2.309191942214966 0.5884321928024292
CurrentTrain: epoch  2, batch     2 | loss: 2.8976240Losses:  3.3009352684020996 0.8148412704467773
CurrentTrain: epoch  2, batch     3 | loss: 4.1157765Losses:  3.3350071907043457 0.9398008584976196
CurrentTrain: epoch  2, batch     4 | loss: 4.2748079Losses:  3.5869088172912598 0.7760045528411865
CurrentTrain: epoch  2, batch     5 | loss: 4.3629131Losses:  2.2658486366271973 0.020388010889291763
CurrentTrain: epoch  2, batch     6 | loss: 2.2862368Losses:  3.5633275508880615 0.49977582693099976
CurrentTrain: epoch  3, batch     0 | loss: 4.0631032Losses:  3.3365204334259033 0.8520402908325195
CurrentTrain: epoch  3, batch     1 | loss: 4.1885605Losses:  2.7638485431671143 0.48806142807006836
CurrentTrain: epoch  3, batch     2 | loss: 3.2519100Losses:  3.5346429347991943 0.7022014856338501
CurrentTrain: epoch  3, batch     3 | loss: 4.2368445Losses:  4.228254318237305 0.8901442885398865
CurrentTrain: epoch  3, batch     4 | loss: 5.1183987Losses:  2.67091703414917 0.8377139568328857
CurrentTrain: epoch  3, batch     5 | loss: 3.5086310Losses:  1.8144598007202148 0.2272447645664215
CurrentTrain: epoch  3, batch     6 | loss: 2.0417047Losses:  4.649181365966797 0.5510815978050232
CurrentTrain: epoch  4, batch     0 | loss: 5.2002630Losses:  2.4909725189208984 0.5634151697158813
CurrentTrain: epoch  4, batch     1 | loss: 3.0543876Losses:  2.9111714363098145 0.570581316947937
CurrentTrain: epoch  4, batch     2 | loss: 3.4817529Losses:  2.385876417160034 0.5702114105224609
CurrentTrain: epoch  4, batch     3 | loss: 2.9560878Losses:  3.009357452392578 1.0396174192428589
CurrentTrain: epoch  4, batch     4 | loss: 4.0489750Losses:  2.2803757190704346 0.6088371276855469
CurrentTrain: epoch  4, batch     5 | loss: 2.8892128Losses:  3.918820858001709 0.24666723608970642
CurrentTrain: epoch  4, batch     6 | loss: 4.1654882Losses:  3.059523344039917 0.8083103895187378
CurrentTrain: epoch  5, batch     0 | loss: 3.8678336Losses:  2.273740291595459 0.4144906997680664
CurrentTrain: epoch  5, batch     1 | loss: 2.6882310Losses:  2.6758153438568115 0.7584712505340576
CurrentTrain: epoch  5, batch     2 | loss: 3.4342866Losses:  2.749207019805908 0.8000988960266113
CurrentTrain: epoch  5, batch     3 | loss: 3.5493059Losses:  2.5575356483459473 0.7057276964187622
CurrentTrain: epoch  5, batch     4 | loss: 3.2632632Losses:  2.33229398727417 0.6861705780029297
CurrentTrain: epoch  5, batch     5 | loss: 3.0184646Losses:  1.8040646314620972 0.21200600266456604
CurrentTrain: epoch  5, batch     6 | loss: 2.0160706Losses:  2.4728660583496094 0.6712523698806763
CurrentTrain: epoch  6, batch     0 | loss: 3.1441183Losses:  2.409381866455078 0.6080746650695801
CurrentTrain: epoch  6, batch     1 | loss: 3.0174565Losses:  1.907414197921753 0.45882222056388855
CurrentTrain: epoch  6, batch     2 | loss: 2.3662364Losses:  2.093414783477783 0.7249235510826111
CurrentTrain: epoch  6, batch     3 | loss: 2.8183384Losses:  2.0778238773345947 0.40429386496543884
CurrentTrain: epoch  6, batch     4 | loss: 2.4821177Losses:  2.629812717437744 0.6902452707290649
CurrentTrain: epoch  6, batch     5 | loss: 3.3200579Losses:  1.9393000602722168 0.1590574085712433
CurrentTrain: epoch  6, batch     6 | loss: 2.0983574Losses:  2.076610565185547 0.3133479356765747
CurrentTrain: epoch  7, batch     0 | loss: 2.3899584Losses:  2.8039872646331787 0.5001958608627319
CurrentTrain: epoch  7, batch     1 | loss: 3.3041830Losses:  2.159420967102051 0.45075634121894836
CurrentTrain: epoch  7, batch     2 | loss: 2.6101773Losses:  1.990103006362915 0.7049364447593689
CurrentTrain: epoch  7, batch     3 | loss: 2.6950395Losses:  1.8785293102264404 0.50304114818573
CurrentTrain: epoch  7, batch     4 | loss: 2.3815703Losses:  2.0269460678100586 0.5286064743995667
CurrentTrain: epoch  7, batch     5 | loss: 2.5555525Losses:  1.9431158304214478 0.1905684471130371
CurrentTrain: epoch  7, batch     6 | loss: 2.1336842Losses:  2.19174861907959 0.5240117311477661
CurrentTrain: epoch  8, batch     0 | loss: 2.7157602Losses:  1.8365354537963867 0.255291223526001
CurrentTrain: epoch  8, batch     1 | loss: 2.0918267Losses:  1.9968032836914062 0.48413851857185364
CurrentTrain: epoch  8, batch     2 | loss: 2.4809418Losses:  2.1340479850769043 0.37918874621391296
CurrentTrain: epoch  8, batch     3 | loss: 2.5132368Losses:  2.0923120975494385 0.54710453748703
CurrentTrain: epoch  8, batch     4 | loss: 2.6394167Losses:  2.068530559539795 0.5613178014755249
CurrentTrain: epoch  8, batch     5 | loss: 2.6298485Losses:  1.7703428268432617 0.14491742849349976
CurrentTrain: epoch  8, batch     6 | loss: 1.9152603Losses:  1.8091638088226318 0.40141361951828003
CurrentTrain: epoch  9, batch     0 | loss: 2.2105775Losses:  1.8702811002731323 0.3835078775882721
CurrentTrain: epoch  9, batch     1 | loss: 2.2537889Losses:  2.0420289039611816 0.5361289978027344
CurrentTrain: epoch  9, batch     2 | loss: 2.5781579Losses:  1.9677047729492188 0.4994440972805023
CurrentTrain: epoch  9, batch     3 | loss: 2.4671488Losses:  2.078518867492676 0.5692712068557739
CurrentTrain: epoch  9, batch     4 | loss: 2.6477900Losses:  1.753079891204834 0.47411733865737915
CurrentTrain: epoch  9, batch     5 | loss: 2.2271972Losses:  1.733940839767456 0.2441507875919342
CurrentTrain: epoch  9, batch     6 | loss: 1.9780916
Losses:  0.8474365472793579 0.5837323665618896
MemoryTrain:  epoch  0, batch     0 | loss: 1.4311689Losses:  0.34277379512786865 0.4152941405773163
MemoryTrain:  epoch  0, batch     1 | loss: 0.7580680Losses:  0.6274821758270264 0.23922641575336456
MemoryTrain:  epoch  0, batch     2 | loss: 0.8667086Losses:  0.6573197245597839 0.6236400604248047
MemoryTrain:  epoch  0, batch     3 | loss: 1.2809598Losses:  0.9744219779968262 0.4836876094341278
MemoryTrain:  epoch  0, batch     4 | loss: 1.4581096Losses:  0.09939762949943542 0.2509387135505676
MemoryTrain:  epoch  0, batch     5 | loss: 0.3503363Losses:  0.3803021311759949 0.49107059836387634
MemoryTrain:  epoch  0, batch     6 | loss: 0.8713727Losses:  0.6167839169502258 0.7058837413787842
MemoryTrain:  epoch  0, batch     7 | loss: 1.3226676Losses:  0.28910163044929504 0.3980754315853119
MemoryTrain:  epoch  0, batch     8 | loss: 0.6871771Losses:  0.6905219554901123 0.5406194925308228
MemoryTrain:  epoch  0, batch     9 | loss: 1.2311414Losses:  0.27881118655204773 0.5357023477554321
MemoryTrain:  epoch  1, batch     0 | loss: 0.8145136Losses:  0.9511522054672241 0.32396799325942993
MemoryTrain:  epoch  1, batch     1 | loss: 1.2751203Losses:  0.6197792887687683 0.2927626967430115
MemoryTrain:  epoch  1, batch     2 | loss: 0.9125420Losses:  0.14618328213691711 0.4893394410610199
MemoryTrain:  epoch  1, batch     3 | loss: 0.6355227Losses:  0.29605045914649963 0.3561636209487915
MemoryTrain:  epoch  1, batch     4 | loss: 0.6522141Losses:  0.12170089781284332 0.1814529001712799
MemoryTrain:  epoch  1, batch     5 | loss: 0.3031538Losses:  1.192183494567871 0.7005016803741455
MemoryTrain:  epoch  1, batch     6 | loss: 1.8926852Losses:  0.38977861404418945 0.5660466551780701
MemoryTrain:  epoch  1, batch     7 | loss: 0.9558253Losses:  1.3743901252746582 0.666658878326416
MemoryTrain:  epoch  1, batch     8 | loss: 2.0410490Losses:  0.3337254524230957 0.47866296768188477
MemoryTrain:  epoch  1, batch     9 | loss: 0.8123884Losses:  0.34095102548599243 0.4285659193992615
MemoryTrain:  epoch  2, batch     0 | loss: 0.7695169Losses:  0.2317202091217041 0.4992281198501587
MemoryTrain:  epoch  2, batch     1 | loss: 0.7309483Losses:  0.16610248386859894 0.42683207988739014
MemoryTrain:  epoch  2, batch     2 | loss: 0.5929345Losses:  0.1462598741054535 0.5247126817703247
MemoryTrain:  epoch  2, batch     3 | loss: 0.6709726Losses:  0.21591730415821075 0.4719447195529938
MemoryTrain:  epoch  2, batch     4 | loss: 0.6878620Losses:  0.14262190461158752 0.5862100124359131
MemoryTrain:  epoch  2, batch     5 | loss: 0.7288319Losses:  0.11727390438318253 0.33266502618789673
MemoryTrain:  epoch  2, batch     6 | loss: 0.4499389Losses:  0.09715189039707184 0.36308425664901733
MemoryTrain:  epoch  2, batch     7 | loss: 0.4602361Losses:  0.2835947871208191 0.4647056758403778
MemoryTrain:  epoch  2, batch     8 | loss: 0.7483004Losses:  0.4182257652282715 0.2885264456272125
MemoryTrain:  epoch  2, batch     9 | loss: 0.7067522Losses:  0.08017581701278687 0.34978342056274414
MemoryTrain:  epoch  3, batch     0 | loss: 0.4299592Losses:  0.12007300555706024 0.34848088026046753
MemoryTrain:  epoch  3, batch     1 | loss: 0.4685539Losses:  0.08314573764801025 0.41909313201904297
MemoryTrain:  epoch  3, batch     2 | loss: 0.5022389Losses:  0.3992258310317993 0.4808603823184967
MemoryTrain:  epoch  3, batch     3 | loss: 0.8800862Losses:  0.05159466713666916 0.31654268503189087
MemoryTrain:  epoch  3, batch     4 | loss: 0.3681374Losses:  0.06675608456134796 0.47393009066581726
MemoryTrain:  epoch  3, batch     5 | loss: 0.5406862Losses:  0.3454565405845642 0.5924111604690552
MemoryTrain:  epoch  3, batch     6 | loss: 0.9378677Losses:  0.10887078940868378 0.3640612065792084
MemoryTrain:  epoch  3, batch     7 | loss: 0.4729320Losses:  0.05727957934141159 0.3623389005661011
MemoryTrain:  epoch  3, batch     8 | loss: 0.4196185Losses:  0.06568485498428345 0.3245554268360138
MemoryTrain:  epoch  3, batch     9 | loss: 0.3902403Losses:  0.10401732474565506 0.6952986717224121
MemoryTrain:  epoch  4, batch     0 | loss: 0.7993160Losses:  0.06240493804216385 0.35708367824554443
MemoryTrain:  epoch  4, batch     1 | loss: 0.4194886Losses:  0.040889933705329895 0.41584986448287964
MemoryTrain:  epoch  4, batch     2 | loss: 0.4567398Losses:  0.13807393610477448 0.37566235661506653
MemoryTrain:  epoch  4, batch     3 | loss: 0.5137363Losses:  0.15353240072727203 0.3376002907752991
MemoryTrain:  epoch  4, batch     4 | loss: 0.4911327Losses:  0.04266977310180664 0.2838965058326721
MemoryTrain:  epoch  4, batch     5 | loss: 0.3265663Losses:  0.3432198166847229 0.40259289741516113
MemoryTrain:  epoch  4, batch     6 | loss: 0.7458127Losses:  0.20831118524074554 0.30430281162261963
MemoryTrain:  epoch  4, batch     7 | loss: 0.5126140Losses:  0.22321414947509766 0.7430707216262817
MemoryTrain:  epoch  4, batch     8 | loss: 0.9662849Losses:  0.05820535123348236 0.3831227421760559
MemoryTrain:  epoch  4, batch     9 | loss: 0.4413281Losses:  0.06030862778425217 0.4092152416706085
MemoryTrain:  epoch  5, batch     0 | loss: 0.4695239Losses:  0.2089395821094513 0.43423035740852356
MemoryTrain:  epoch  5, batch     1 | loss: 0.6431699Losses:  0.09520833194255829 0.5061248540878296
MemoryTrain:  epoch  5, batch     2 | loss: 0.6013332Losses:  0.14430169761180878 0.5327513217926025
MemoryTrain:  epoch  5, batch     3 | loss: 0.6770530Losses:  0.07734189927577972 0.6315921545028687
MemoryTrain:  epoch  5, batch     4 | loss: 0.7089341Losses:  0.04670151323080063 0.2282106578350067
MemoryTrain:  epoch  5, batch     5 | loss: 0.2749122Losses:  0.23164120316505432 0.35909271240234375
MemoryTrain:  epoch  5, batch     6 | loss: 0.5907339Losses:  0.20967505872249603 0.4970787465572357
MemoryTrain:  epoch  5, batch     7 | loss: 0.7067538Losses:  0.05103585124015808 0.24306830763816833
MemoryTrain:  epoch  5, batch     8 | loss: 0.2941042Losses:  0.05704997479915619 0.2995099127292633
MemoryTrain:  epoch  5, batch     9 | loss: 0.3565599Losses:  0.08446440100669861 0.454115629196167
MemoryTrain:  epoch  6, batch     0 | loss: 0.5385801Losses:  0.08074883371591568 0.4156089723110199
MemoryTrain:  epoch  6, batch     1 | loss: 0.4963578Losses:  0.045567918568849564 0.24294331669807434
MemoryTrain:  epoch  6, batch     2 | loss: 0.2885112Losses:  0.03370092809200287 0.5047715306282043
MemoryTrain:  epoch  6, batch     3 | loss: 0.5384725Losses:  0.026852162554860115 0.29499322175979614
MemoryTrain:  epoch  6, batch     4 | loss: 0.3218454Losses:  0.05324801057577133 0.4047771394252777
MemoryTrain:  epoch  6, batch     5 | loss: 0.4580252Losses:  0.04995257779955864 0.26673778891563416
MemoryTrain:  epoch  6, batch     6 | loss: 0.3166904Losses:  0.06584695726633072 0.4984728693962097
MemoryTrain:  epoch  6, batch     7 | loss: 0.5643198Losses:  0.08125142753124237 0.5917580127716064
MemoryTrain:  epoch  6, batch     8 | loss: 0.6730095Losses:  0.9271619915962219 0.42616963386535645
MemoryTrain:  epoch  6, batch     9 | loss: 1.3533316Losses:  0.05206955596804619 0.44092050194740295
MemoryTrain:  epoch  7, batch     0 | loss: 0.4929900Losses:  0.03844783455133438 0.502085268497467
MemoryTrain:  epoch  7, batch     1 | loss: 0.5405331Losses:  0.17126499116420746 0.5163633823394775
MemoryTrain:  epoch  7, batch     2 | loss: 0.6876284Losses:  0.036590345203876495 0.41025421023368835
MemoryTrain:  epoch  7, batch     3 | loss: 0.4468445Losses:  0.029990224167704582 0.4209171533584595
MemoryTrain:  epoch  7, batch     4 | loss: 0.4509074Losses:  0.03783277049660683 0.3162347078323364
MemoryTrain:  epoch  7, batch     5 | loss: 0.3540675Losses:  0.039657577872276306 0.3140886425971985
MemoryTrain:  epoch  7, batch     6 | loss: 0.3537462Losses:  0.06155599281191826 0.230780690908432
MemoryTrain:  epoch  7, batch     7 | loss: 0.2923367Losses:  0.05184125155210495 0.248263418674469
MemoryTrain:  epoch  7, batch     8 | loss: 0.3001047Losses:  0.038911230862140656 0.49183112382888794
MemoryTrain:  epoch  7, batch     9 | loss: 0.5307423Losses:  0.05760157108306885 0.3100709021091461
MemoryTrain:  epoch  8, batch     0 | loss: 0.3676725Losses:  0.051320984959602356 0.38581162691116333
MemoryTrain:  epoch  8, batch     1 | loss: 0.4371326Losses:  0.045847758650779724 0.3938194811344147
MemoryTrain:  epoch  8, batch     2 | loss: 0.4396672Losses:  0.10234932601451874 0.5305343866348267
MemoryTrain:  epoch  8, batch     3 | loss: 0.6328837Losses:  0.027630917727947235 0.2796558141708374
MemoryTrain:  epoch  8, batch     4 | loss: 0.3072867Losses:  0.07950459420681 0.41650551557540894
MemoryTrain:  epoch  8, batch     5 | loss: 0.4960101Losses:  0.06875717639923096 0.3441419005393982
MemoryTrain:  epoch  8, batch     6 | loss: 0.4128991Losses:  0.11834433674812317 0.47310036420822144
MemoryTrain:  epoch  8, batch     7 | loss: 0.5914447Losses:  0.03626333177089691 0.2918916940689087
MemoryTrain:  epoch  8, batch     8 | loss: 0.3281550Losses:  0.05151356756687164 0.5747194290161133
MemoryTrain:  epoch  8, batch     9 | loss: 0.6262330Losses:  0.04941094294190407 0.6548701524734497
MemoryTrain:  epoch  9, batch     0 | loss: 0.7042811Losses:  0.04796638339757919 0.32294631004333496
MemoryTrain:  epoch  9, batch     1 | loss: 0.3709127Losses:  0.04976561665534973 0.3863866925239563
MemoryTrain:  epoch  9, batch     2 | loss: 0.4361523Losses:  0.1352449357509613 0.48232465982437134
MemoryTrain:  epoch  9, batch     3 | loss: 0.6175696Losses:  0.06217322126030922 0.20083492994308472
MemoryTrain:  epoch  9, batch     4 | loss: 0.2630081Losses:  0.036432962864637375 0.3064301311969757
MemoryTrain:  epoch  9, batch     5 | loss: 0.3428631Losses:  0.05181937664747238 0.3002830147743225
MemoryTrain:  epoch  9, batch     6 | loss: 0.3521024Losses:  0.0290526133030653 0.21049684286117554
MemoryTrain:  epoch  9, batch     7 | loss: 0.2395495Losses:  0.05418571084737778 0.48294952511787415
MemoryTrain:  epoch  9, batch     8 | loss: 0.5371352Losses:  0.04166097566485405 0.4082610607147217
MemoryTrain:  epoch  9, batch     9 | loss: 0.4499220
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 71.38%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.43%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.19%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 77.72%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 77.13%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 76.69%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 75.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.48%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 75.97%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.80%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 73.73%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 72.79%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 71.78%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 70.80%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 69.17%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 69.24%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 68.52%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 67.65%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 66.74%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 70.15%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 70.27%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 69.91%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 69.09%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 69.01%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 68.44%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 67.88%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 67.38%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 66.89%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.02%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 66.93%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 66.71%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 66.58%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 66.33%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 66.08%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 65.70%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 65.51%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 65.41%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 65.07%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 64.73%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 64.39%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 64.05%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 63.83%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 63.57%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 64.94%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 65.23%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 65.07%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 64.71%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 64.57%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  200 | acc: 6.25%,  total acc: 65.08%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 64.82%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 64.56%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 64.31%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 63.90%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 63.98%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 64.85%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 65.03%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 65.69%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 65.75%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 65.41%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 65.30%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 65.90%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 65.93%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 65.93%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.65%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 65.41%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 68.04%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 67.84%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 67.41%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 67.69%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.63%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  332 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  334 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 67.04%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 66.86%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.52%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 66.32%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 66.13%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 66.31%   [EVAL] batch:  358 | acc: 12.50%,  total acc: 66.16%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:  360 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:  362 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  368 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 65.50%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 65.63%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  388 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 65.17%   [EVAL] batch:  391 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:  392 | acc: 12.50%,  total acc: 64.90%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 64.80%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 65.13%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 64.99%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 64.84%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 64.76%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.44%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 64.75%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 64.85%   [EVAL] batch:  415 | acc: 37.50%,  total acc: 64.78%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 64.72%   [EVAL] batch:  417 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:  420 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  426 | acc: 50.00%,  total acc: 64.94%   [EVAL] batch:  427 | acc: 56.25%,  total acc: 64.92%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  437 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  438 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 65.47%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  441 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  442 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  443 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 65.76%   [EVAL] batch:  448 | acc: 37.50%,  total acc: 65.70%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  455 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  463 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  464 | acc: 50.00%,  total acc: 66.22%   [EVAL] batch:  465 | acc: 56.25%,  total acc: 66.20%   [EVAL] batch:  466 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  467 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:  468 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 66.21%   [EVAL] batch:  470 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  471 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  472 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  473 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  474 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  482 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  483 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 66.79%   
cur_acc:  ['0.9484', '0.7401', '0.7262', '0.7083', '0.8869', '0.6528', '0.6478', '0.7619']
his_acc:  ['0.9484', '0.8425', '0.7796', '0.7282', '0.7564', '0.7275', '0.6836', '0.6679']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.182470321655273 1.3374130725860596
CurrentTrain: epoch  0, batch     0 | loss: 11.5198832Losses:  9.806952476501465 1.2239339351654053
CurrentTrain: epoch  0, batch     1 | loss: 11.0308867Losses:  9.68058967590332 1.4440943002700806
CurrentTrain: epoch  0, batch     2 | loss: 11.1246843Losses:  9.992650985717773 1.4801727533340454
CurrentTrain: epoch  0, batch     3 | loss: 11.4728241Losses:  9.469728469848633 1.4136831760406494
CurrentTrain: epoch  0, batch     4 | loss: 10.8834114Losses:  10.14078140258789 0.9183521270751953
CurrentTrain: epoch  0, batch     5 | loss: 11.0591335Losses:  10.243729591369629 1.64797043800354
CurrentTrain: epoch  0, batch     6 | loss: 11.8916998Losses:  9.471637725830078 1.4859389066696167
CurrentTrain: epoch  0, batch     7 | loss: 10.9575768Losses:  9.546792984008789 1.645349144935608
CurrentTrain: epoch  0, batch     8 | loss: 11.1921425Losses:  8.82192325592041 1.4640023708343506
CurrentTrain: epoch  0, batch     9 | loss: 10.2859259Losses:  10.602928161621094 1.5133650302886963
CurrentTrain: epoch  0, batch    10 | loss: 12.1162930Losses:  9.28876781463623 1.2641496658325195
CurrentTrain: epoch  0, batch    11 | loss: 10.5529175Losses:  8.468521118164062 0.8951687812805176
CurrentTrain: epoch  0, batch    12 | loss: 9.3636894Losses:  8.584075927734375 1.7157251834869385
CurrentTrain: epoch  0, batch    13 | loss: 10.2998009Losses:  8.153253555297852 1.3924729824066162
CurrentTrain: epoch  0, batch    14 | loss: 9.5457268Losses:  9.406347274780273 1.382779598236084
CurrentTrain: epoch  0, batch    15 | loss: 10.7891273Losses:  9.092472076416016 1.5771523714065552
CurrentTrain: epoch  0, batch    16 | loss: 10.6696243Losses:  8.539677619934082 1.3865511417388916
CurrentTrain: epoch  0, batch    17 | loss: 9.9262285Losses:  8.142236709594727 1.5288231372833252
CurrentTrain: epoch  0, batch    18 | loss: 9.6710596Losses:  8.946807861328125 1.3772536516189575
CurrentTrain: epoch  0, batch    19 | loss: 10.3240614Losses:  9.473908424377441 1.1345524787902832
CurrentTrain: epoch  0, batch    20 | loss: 10.6084614Losses:  8.701848983764648 1.1782417297363281
CurrentTrain: epoch  0, batch    21 | loss: 9.8800907Losses:  7.765118598937988 1.3400647640228271
CurrentTrain: epoch  0, batch    22 | loss: 9.1051836Losses:  8.413764953613281 1.35829758644104
CurrentTrain: epoch  0, batch    23 | loss: 9.7720623Losses:  8.936073303222656 1.3343050479888916
CurrentTrain: epoch  0, batch    24 | loss: 10.2703781Losses:  7.969034194946289 1.1442359685897827
CurrentTrain: epoch  0, batch    25 | loss: 9.1132698Losses:  8.56814956665039 1.1822659969329834
CurrentTrain: epoch  0, batch    26 | loss: 9.7504158Losses:  8.151444435119629 1.0708155632019043
CurrentTrain: epoch  0, batch    27 | loss: 9.2222595Losses:  8.486419677734375 1.0383423566818237
CurrentTrain: epoch  0, batch    28 | loss: 9.5247622Losses:  7.8443684577941895 1.0971609354019165
CurrentTrain: epoch  0, batch    29 | loss: 8.9415293Losses:  7.787199974060059 0.8972084522247314
CurrentTrain: epoch  0, batch    30 | loss: 8.6844082Losses:  8.552117347717285 1.0722144842147827
CurrentTrain: epoch  0, batch    31 | loss: 9.6243315Losses:  8.250622749328613 1.2170934677124023
CurrentTrain: epoch  0, batch    32 | loss: 9.4677162Losses:  8.283106803894043 0.9573924541473389
CurrentTrain: epoch  0, batch    33 | loss: 9.2404995Losses:  8.459078788757324 1.1504781246185303
CurrentTrain: epoch  0, batch    34 | loss: 9.6095572Losses:  8.59623908996582 1.2106397151947021
CurrentTrain: epoch  0, batch    35 | loss: 9.8068790Losses:  8.635737419128418 0.9833710193634033
CurrentTrain: epoch  0, batch    36 | loss: 9.6191082Losses:  7.66623592376709 1.1106605529785156
CurrentTrain: epoch  0, batch    37 | loss: 8.7768965Losses:  8.729232788085938 0.8230847120285034
CurrentTrain: epoch  0, batch    38 | loss: 9.5523176Losses:  7.436619758605957 0.3324257731437683
CurrentTrain: epoch  0, batch    39 | loss: 7.7690454Losses:  9.294939994812012 1.4084434509277344
CurrentTrain: epoch  0, batch    40 | loss: 10.7033834Losses:  8.242630958557129 1.168912649154663
CurrentTrain: epoch  0, batch    41 | loss: 9.4115438Losses:  8.334474563598633 1.0249592065811157
CurrentTrain: epoch  0, batch    42 | loss: 9.3594341Losses:  7.936478614807129 0.9733035564422607
CurrentTrain: epoch  0, batch    43 | loss: 8.9097824Losses:  8.72419261932373 0.9869112372398376
CurrentTrain: epoch  0, batch    44 | loss: 9.7111034Losses:  8.435158729553223 1.0087051391601562
CurrentTrain: epoch  0, batch    45 | loss: 9.4438639Losses:  7.721770763397217 1.064815640449524
CurrentTrain: epoch  0, batch    46 | loss: 8.7865868Losses:  7.064823627471924 0.8136861324310303
CurrentTrain: epoch  0, batch    47 | loss: 7.8785095Losses:  9.161415100097656 1.1035536527633667
CurrentTrain: epoch  0, batch    48 | loss: 10.2649689Losses:  8.660235404968262 1.1372781991958618
CurrentTrain: epoch  0, batch    49 | loss: 9.7975140Losses:  8.018601417541504 0.662354588508606
CurrentTrain: epoch  0, batch    50 | loss: 8.6809559Losses:  7.768751621246338 0.8212143182754517
CurrentTrain: epoch  0, batch    51 | loss: 8.5899658Losses:  8.003408432006836 0.9225513935089111
CurrentTrain: epoch  0, batch    52 | loss: 8.9259596Losses:  7.269946098327637 1.13217294216156
CurrentTrain: epoch  0, batch    53 | loss: 8.4021187Losses:  8.172377586364746 0.7971465587615967
CurrentTrain: epoch  0, batch    54 | loss: 8.9695244Losses:  9.034051895141602 1.2185990810394287
CurrentTrain: epoch  0, batch    55 | loss: 10.2526512Losses:  8.180917739868164 1.0409481525421143
CurrentTrain: epoch  0, batch    56 | loss: 9.2218657Losses:  7.152186870574951 0.9872754812240601
CurrentTrain: epoch  0, batch    57 | loss: 8.1394625Losses:  8.20041275024414 0.8514975905418396
CurrentTrain: epoch  0, batch    58 | loss: 9.0519104Losses:  7.315182685852051 0.9832215905189514
CurrentTrain: epoch  0, batch    59 | loss: 8.2984047Losses:  8.317117691040039 1.3305120468139648
CurrentTrain: epoch  0, batch    60 | loss: 9.6476297Losses:  8.624801635742188 1.055624008178711
CurrentTrain: epoch  0, batch    61 | loss: 9.6804256Losses:  6.965549468994141 0.9391449093818665
CurrentTrain: epoch  0, batch    62 | loss: 7.9046946Losses:  6.770934104919434 0.5906527638435364
CurrentTrain: epoch  0, batch    63 | loss: 7.3615870Losses:  7.300777435302734 0.7829431295394897
CurrentTrain: epoch  0, batch    64 | loss: 8.0837202Losses:  7.226141452789307 0.9583708643913269
CurrentTrain: epoch  0, batch    65 | loss: 8.1845121Losses:  7.658267974853516 0.578571617603302
CurrentTrain: epoch  0, batch    66 | loss: 8.2368393Losses:  8.498193740844727 0.8559775352478027
CurrentTrain: epoch  0, batch    67 | loss: 9.3541718Losses:  7.261547565460205 0.9726125001907349
CurrentTrain: epoch  0, batch    68 | loss: 8.2341604Losses:  7.1210479736328125 0.8637006282806396
CurrentTrain: epoch  0, batch    69 | loss: 7.9847488Losses:  7.6864728927612305 0.8038972020149231
CurrentTrain: epoch  0, batch    70 | loss: 8.4903698Losses:  7.4200849533081055 0.9968418478965759
CurrentTrain: epoch  0, batch    71 | loss: 8.4169264Losses:  7.233867645263672 0.8965632915496826
CurrentTrain: epoch  0, batch    72 | loss: 8.1304312Losses:  7.98287296295166 0.8944579362869263
CurrentTrain: epoch  0, batch    73 | loss: 8.8773308Losses:  7.948385715484619 0.6922647356987
CurrentTrain: epoch  0, batch    74 | loss: 8.6406507Losses:  7.0850067138671875 0.9003615379333496
CurrentTrain: epoch  0, batch    75 | loss: 7.9853683Losses:  7.597091197967529 0.7024331092834473
CurrentTrain: epoch  0, batch    76 | loss: 8.2995243Losses:  7.6991119384765625 0.967244029045105
CurrentTrain: epoch  0, batch    77 | loss: 8.6663561Losses:  7.847560882568359 0.8820894360542297
CurrentTrain: epoch  0, batch    78 | loss: 8.7296505Losses:  8.369851112365723 0.9957029819488525
CurrentTrain: epoch  0, batch    79 | loss: 9.3655539Losses:  8.056229591369629 1.0954010486602783
CurrentTrain: epoch  0, batch    80 | loss: 9.1516304Losses:  6.76710319519043 0.8766946196556091
CurrentTrain: epoch  0, batch    81 | loss: 7.6437979Losses:  7.184323310852051 0.8733420968055725
CurrentTrain: epoch  0, batch    82 | loss: 8.0576658Losses:  8.232994079589844 0.850273847579956
CurrentTrain: epoch  0, batch    83 | loss: 9.0832682Losses:  7.086541175842285 0.873841404914856
CurrentTrain: epoch  0, batch    84 | loss: 7.9603825Losses:  6.685974597930908 0.5282703638076782
CurrentTrain: epoch  0, batch    85 | loss: 7.2142448Losses:  8.175704956054688 0.748705267906189
CurrentTrain: epoch  0, batch    86 | loss: 8.9244099Losses:  6.840371131896973 0.720955491065979
CurrentTrain: epoch  0, batch    87 | loss: 7.5613265Losses:  7.189243316650391 0.8458659648895264
CurrentTrain: epoch  0, batch    88 | loss: 8.0351095Losses:  7.862969398498535 0.8566014766693115
CurrentTrain: epoch  0, batch    89 | loss: 8.7195711Losses:  6.782325744628906 0.5231739282608032
CurrentTrain: epoch  0, batch    90 | loss: 7.3054996Losses:  7.830480575561523 0.691192090511322
CurrentTrain: epoch  0, batch    91 | loss: 8.5216722Losses:  7.472665309906006 0.5281455516815186
CurrentTrain: epoch  0, batch    92 | loss: 8.0008106Losses:  8.015690803527832 0.7598063945770264
CurrentTrain: epoch  0, batch    93 | loss: 8.7754974Losses:  7.577184200286865 0.7585349082946777
CurrentTrain: epoch  0, batch    94 | loss: 8.3357191Losses:  6.87277364730835 0.8044038414955139
CurrentTrain: epoch  0, batch    95 | loss: 7.6771774Losses:  5.933032035827637 0.7055177092552185
CurrentTrain: epoch  0, batch    96 | loss: 6.6385498Losses:  7.4319329261779785 0.8183269500732422
CurrentTrain: epoch  0, batch    97 | loss: 8.2502594Losses:  7.36783504486084 0.8623512983322144
CurrentTrain: epoch  0, batch    98 | loss: 8.2301865Losses:  7.387362480163574 0.686916708946228
CurrentTrain: epoch  0, batch    99 | loss: 8.0742788Losses:  7.080130577087402 0.6052427291870117
CurrentTrain: epoch  0, batch   100 | loss: 7.6853733Losses:  7.8313307762146 0.7258404493331909
CurrentTrain: epoch  0, batch   101 | loss: 8.5571709Losses:  6.260022163391113 0.7184187173843384
CurrentTrain: epoch  0, batch   102 | loss: 6.9784408Losses:  6.938658237457275 0.6544137001037598
CurrentTrain: epoch  0, batch   103 | loss: 7.5930719Losses:  6.356640815734863 0.8349264860153198
CurrentTrain: epoch  0, batch   104 | loss: 7.1915674Losses:  6.972785949707031 0.7785482406616211
CurrentTrain: epoch  0, batch   105 | loss: 7.7513342Losses:  6.295300006866455 0.6182628870010376
CurrentTrain: epoch  0, batch   106 | loss: 6.9135628Losses:  6.591726303100586 0.6835278272628784
CurrentTrain: epoch  0, batch   107 | loss: 7.2752542Losses:  6.944030284881592 0.4856247901916504
CurrentTrain: epoch  0, batch   108 | loss: 7.4296551Losses:  7.3355712890625 0.8302567005157471
CurrentTrain: epoch  0, batch   109 | loss: 8.1658278Losses:  6.122561931610107 0.7827495336532593
CurrentTrain: epoch  0, batch   110 | loss: 6.9053116Losses:  7.316991806030273 0.839907705783844
CurrentTrain: epoch  0, batch   111 | loss: 8.1568995Losses:  6.997958660125732 0.8112927079200745
CurrentTrain: epoch  0, batch   112 | loss: 7.8092513Losses:  7.823909759521484 0.8962739706039429
CurrentTrain: epoch  0, batch   113 | loss: 8.7201834Losses:  6.919246673583984 0.6222648620605469
CurrentTrain: epoch  0, batch   114 | loss: 7.5415115Losses:  7.246801853179932 0.5799267888069153
CurrentTrain: epoch  0, batch   115 | loss: 7.8267288Losses:  6.70416259765625 0.7386282682418823
CurrentTrain: epoch  0, batch   116 | loss: 7.4427910Losses:  7.006788730621338 0.5576860308647156
CurrentTrain: epoch  0, batch   117 | loss: 7.5644746Losses:  6.886568069458008 0.773046612739563
CurrentTrain: epoch  0, batch   118 | loss: 7.6596146Losses:  6.854423522949219 0.7079067230224609
CurrentTrain: epoch  0, batch   119 | loss: 7.5623302Losses:  6.0863261222839355 0.4121033549308777
CurrentTrain: epoch  0, batch   120 | loss: 6.4984293Losses:  7.25943660736084 0.7787516117095947
CurrentTrain: epoch  0, batch   121 | loss: 8.0381880Losses:  6.368753433227539 0.60210120677948
CurrentTrain: epoch  0, batch   122 | loss: 6.9708548Losses:  7.944512844085693 0.598862886428833
CurrentTrain: epoch  0, batch   123 | loss: 8.5433760Losses:  6.402979373931885 0.5969569087028503
CurrentTrain: epoch  0, batch   124 | loss: 6.9999361Losses:  5.74112606048584 0.5259902477264404
CurrentTrain: epoch  1, batch     0 | loss: 6.2671165Losses:  6.460357666015625 0.7305721640586853
CurrentTrain: epoch  1, batch     1 | loss: 7.1909299Losses:  6.31886100769043 0.5918446779251099
CurrentTrain: epoch  1, batch     2 | loss: 6.9107056Losses:  7.576169013977051 0.6207051873207092
CurrentTrain: epoch  1, batch     3 | loss: 8.1968746Losses:  5.849053859710693 0.720089852809906
CurrentTrain: epoch  1, batch     4 | loss: 6.5691438Losses:  6.575106620788574 0.5960968732833862
CurrentTrain: epoch  1, batch     5 | loss: 7.1712036Losses:  7.6380696296691895 0.554366946220398
CurrentTrain: epoch  1, batch     6 | loss: 8.1924362Losses:  6.702780723571777 0.6315609812736511
CurrentTrain: epoch  1, batch     7 | loss: 7.3343415Losses:  5.664580345153809 0.587171196937561
CurrentTrain: epoch  1, batch     8 | loss: 6.2517514Losses:  6.747122287750244 0.49803924560546875
CurrentTrain: epoch  1, batch     9 | loss: 7.2451615Losses:  6.876882076263428 0.4533635079860687
CurrentTrain: epoch  1, batch    10 | loss: 7.3302455Losses:  6.4616007804870605 0.6122797727584839
CurrentTrain: epoch  1, batch    11 | loss: 7.0738807Losses:  7.652658462524414 0.5792607069015503
CurrentTrain: epoch  1, batch    12 | loss: 8.2319193Losses:  7.035004615783691 0.792730987071991
CurrentTrain: epoch  1, batch    13 | loss: 7.8277354Losses:  6.380267143249512 0.4821324348449707
CurrentTrain: epoch  1, batch    14 | loss: 6.8623996Losses:  6.961108207702637 0.4719924032688141
CurrentTrain: epoch  1, batch    15 | loss: 7.4331007Losses:  7.324884414672852 0.707251250743866
CurrentTrain: epoch  1, batch    16 | loss: 8.0321360Losses:  5.971873760223389 0.6695019602775574
CurrentTrain: epoch  1, batch    17 | loss: 6.6413755Losses:  6.4215850830078125 0.7375087738037109
CurrentTrain: epoch  1, batch    18 | loss: 7.1590939Losses:  6.7760419845581055 0.49307310581207275
CurrentTrain: epoch  1, batch    19 | loss: 7.2691150Losses:  6.231578826904297 0.5104307532310486
CurrentTrain: epoch  1, batch    20 | loss: 6.7420096Losses:  6.996859550476074 0.7322981357574463
CurrentTrain: epoch  1, batch    21 | loss: 7.7291574Losses:  6.66713285446167 0.6107876300811768
CurrentTrain: epoch  1, batch    22 | loss: 7.2779207Losses:  5.808419704437256 0.5736836791038513
CurrentTrain: epoch  1, batch    23 | loss: 6.3821034Losses:  6.339206218719482 0.5966767072677612
CurrentTrain: epoch  1, batch    24 | loss: 6.9358830Losses:  6.892606735229492 0.7819145321846008
CurrentTrain: epoch  1, batch    25 | loss: 7.6745214Losses:  6.697746276855469 0.5720750689506531
CurrentTrain: epoch  1, batch    26 | loss: 7.2698212Losses:  6.55875301361084 0.7059285640716553
CurrentTrain: epoch  1, batch    27 | loss: 7.2646818Losses:  6.034757137298584 0.4482131004333496
CurrentTrain: epoch  1, batch    28 | loss: 6.4829702Losses:  6.03855037689209 0.3557998538017273
CurrentTrain: epoch  1, batch    29 | loss: 6.3943501Losses:  6.413740634918213 0.69374680519104
CurrentTrain: epoch  1, batch    30 | loss: 7.1074877Losses:  6.413145065307617 0.6322649717330933
CurrentTrain: epoch  1, batch    31 | loss: 7.0454102Losses:  6.505256175994873 0.6319241523742676
CurrentTrain: epoch  1, batch    32 | loss: 7.1371803Losses:  5.520286560058594 0.2792615592479706
CurrentTrain: epoch  1, batch    33 | loss: 5.7995481Losses:  5.972996711730957 0.4422012269496918
CurrentTrain: epoch  1, batch    34 | loss: 6.4151978Losses:  5.059528350830078 0.4123297929763794
CurrentTrain: epoch  1, batch    35 | loss: 5.4718580Losses:  5.710046291351318 0.4040558338165283
CurrentTrain: epoch  1, batch    36 | loss: 6.1141024Losses:  6.125739097595215 0.5491182208061218
CurrentTrain: epoch  1, batch    37 | loss: 6.6748571Losses:  6.1828203201293945 0.5982709527015686
CurrentTrain: epoch  1, batch    38 | loss: 6.7810912Losses:  5.65598726272583 0.6761817932128906
CurrentTrain: epoch  1, batch    39 | loss: 6.3321691Losses:  6.208592414855957 0.5846505165100098
CurrentTrain: epoch  1, batch    40 | loss: 6.7932429Losses:  5.591742515563965 0.3720857501029968
CurrentTrain: epoch  1, batch    41 | loss: 5.9638281Losses:  5.716592788696289 0.5898208618164062
CurrentTrain: epoch  1, batch    42 | loss: 6.3064137Losses:  5.568445205688477 0.5473083257675171
CurrentTrain: epoch  1, batch    43 | loss: 6.1157537Losses:  7.004640579223633 0.884155809879303
CurrentTrain: epoch  1, batch    44 | loss: 7.8887963Losses:  5.287590026855469 0.4685004949569702
CurrentTrain: epoch  1, batch    45 | loss: 5.7560906Losses:  5.377654075622559 0.535785973072052
CurrentTrain: epoch  1, batch    46 | loss: 5.9134402Losses:  6.454704284667969 0.2862061858177185
CurrentTrain: epoch  1, batch    47 | loss: 6.7409105Losses:  6.285133361816406 0.42137420177459717
CurrentTrain: epoch  1, batch    48 | loss: 6.7065077Losses:  6.064573287963867 0.5755065083503723
CurrentTrain: epoch  1, batch    49 | loss: 6.6400800Losses:  5.813297271728516 0.4537937343120575
CurrentTrain: epoch  1, batch    50 | loss: 6.2670908Losses:  7.08759880065918 0.5923287272453308
CurrentTrain: epoch  1, batch    51 | loss: 7.6799273Losses:  6.355125427246094 0.5134949684143066
CurrentTrain: epoch  1, batch    52 | loss: 6.8686204Losses:  6.134559631347656 0.3940124213695526
CurrentTrain: epoch  1, batch    53 | loss: 6.5285721Losses:  6.579966068267822 0.7029751539230347
CurrentTrain: epoch  1, batch    54 | loss: 7.2829413Losses:  6.7665557861328125 0.7265486717224121
CurrentTrain: epoch  1, batch    55 | loss: 7.4931045Losses:  6.513334274291992 0.47994405031204224
CurrentTrain: epoch  1, batch    56 | loss: 6.9932785Losses:  5.8174543380737305 0.30207571387290955
CurrentTrain: epoch  1, batch    57 | loss: 6.1195302Losses:  6.247328758239746 0.546623170375824
CurrentTrain: epoch  1, batch    58 | loss: 6.7939520Losses:  6.0470428466796875 0.6931105256080627
CurrentTrain: epoch  1, batch    59 | loss: 6.7401533Losses:  5.51392126083374 0.5922248363494873
CurrentTrain: epoch  1, batch    60 | loss: 6.1061459Losses:  5.335500240325928 0.3475582003593445
CurrentTrain: epoch  1, batch    61 | loss: 5.6830583Losses:  6.206680774688721 0.5013378858566284
CurrentTrain: epoch  1, batch    62 | loss: 6.7080188Losses:  6.040926456451416 0.571150004863739
CurrentTrain: epoch  1, batch    63 | loss: 6.6120763Losses:  5.3525896072387695 0.29045599699020386
CurrentTrain: epoch  1, batch    64 | loss: 5.6430454Losses:  5.5389509201049805 0.4817027449607849
CurrentTrain: epoch  1, batch    65 | loss: 6.0206537Losses:  5.36568021774292 0.30810749530792236
CurrentTrain: epoch  1, batch    66 | loss: 5.6737876Losses:  6.425112247467041 0.5924888849258423
CurrentTrain: epoch  1, batch    67 | loss: 7.0176010Losses:  6.170830726623535 0.43095511198043823
CurrentTrain: epoch  1, batch    68 | loss: 6.6017857Losses:  5.540278434753418 0.30590909719467163
CurrentTrain: epoch  1, batch    69 | loss: 5.8461876Losses:  6.883447647094727 0.570624828338623
CurrentTrain: epoch  1, batch    70 | loss: 7.4540725Losses:  6.980928421020508 0.2947767972946167
CurrentTrain: epoch  1, batch    71 | loss: 7.2757053Losses:  5.124382972717285 0.4769843518733978
CurrentTrain: epoch  1, batch    72 | loss: 5.6013675Losses:  5.646997451782227 0.4095042943954468
CurrentTrain: epoch  1, batch    73 | loss: 6.0565019Losses:  5.852067947387695 0.3885843753814697
CurrentTrain: epoch  1, batch    74 | loss: 6.2406521Losses:  5.361569404602051 0.2621532678604126
CurrentTrain: epoch  1, batch    75 | loss: 5.6237226Losses:  6.269330978393555 0.42722320556640625
CurrentTrain: epoch  1, batch    76 | loss: 6.6965542Losses:  6.873022079467773 0.5011731386184692
CurrentTrain: epoch  1, batch    77 | loss: 7.3741951Losses:  6.770366191864014 0.5520846843719482
CurrentTrain: epoch  1, batch    78 | loss: 7.3224506Losses:  5.676255226135254 0.5931485295295715
CurrentTrain: epoch  1, batch    79 | loss: 6.2694039Losses:  4.972002983093262 0.4071420431137085
CurrentTrain: epoch  1, batch    80 | loss: 5.3791451Losses:  6.283203125 0.5853691101074219
CurrentTrain: epoch  1, batch    81 | loss: 6.8685722Losses:  5.132149696350098 0.515372097492218
CurrentTrain: epoch  1, batch    82 | loss: 5.6475220Losses:  5.92642879486084 0.4807397127151489
CurrentTrain: epoch  1, batch    83 | loss: 6.4071684Losses:  5.2823944091796875 0.3086094260215759
CurrentTrain: epoch  1, batch    84 | loss: 5.5910039Losses:  5.5888991355896 0.424996018409729
CurrentTrain: epoch  1, batch    85 | loss: 6.0138950Losses:  5.594463348388672 0.4316720962524414
CurrentTrain: epoch  1, batch    86 | loss: 6.0261354Losses:  5.468808174133301 0.24262836575508118
CurrentTrain: epoch  1, batch    87 | loss: 5.7114367Losses:  6.899510383605957 0.3944544196128845
CurrentTrain: epoch  1, batch    88 | loss: 7.2939649Losses:  7.274299621582031 0.7077511548995972
CurrentTrain: epoch  1, batch    89 | loss: 7.9820509Losses:  6.783252716064453 0.654639482498169
CurrentTrain: epoch  1, batch    90 | loss: 7.4378920Losses:  6.844902038574219 0.6635376214981079
CurrentTrain: epoch  1, batch    91 | loss: 7.5084395Losses:  6.634618282318115 0.6021763682365417
CurrentTrain: epoch  1, batch    92 | loss: 7.2367945Losses:  5.609216690063477 0.2936994135379791
CurrentTrain: epoch  1, batch    93 | loss: 5.9029160Losses:  5.391767501831055 0.6535987854003906
CurrentTrain: epoch  1, batch    94 | loss: 6.0453663Losses:  6.62563419342041 0.6597533226013184
CurrentTrain: epoch  1, batch    95 | loss: 7.2853875Losses:  6.631600379943848 0.629996657371521
CurrentTrain: epoch  1, batch    96 | loss: 7.2615972Losses:  5.454648971557617 0.3107410669326782
CurrentTrain: epoch  1, batch    97 | loss: 5.7653899Losses:  6.841226100921631 0.6001923680305481
CurrentTrain: epoch  1, batch    98 | loss: 7.4414186Losses:  5.102380752563477 0.6077091693878174
CurrentTrain: epoch  1, batch    99 | loss: 5.7100897Losses:  5.979572296142578 0.2928130626678467
CurrentTrain: epoch  1, batch   100 | loss: 6.2723856Losses:  6.3836774826049805 0.49021387100219727
CurrentTrain: epoch  1, batch   101 | loss: 6.8738914Losses:  6.332803726196289 0.7415115833282471
CurrentTrain: epoch  1, batch   102 | loss: 7.0743151Losses:  5.493868827819824 0.3092460036277771
CurrentTrain: epoch  1, batch   103 | loss: 5.8031149Losses:  5.381260871887207 0.3332507312297821
CurrentTrain: epoch  1, batch   104 | loss: 5.7145114Losses:  5.365006446838379 0.39230677485466003
CurrentTrain: epoch  1, batch   105 | loss: 5.7573133Losses:  5.641792297363281 0.526403546333313
CurrentTrain: epoch  1, batch   106 | loss: 6.1681957Losses:  6.772058486938477 0.23269301652908325
CurrentTrain: epoch  1, batch   107 | loss: 7.0047517Losses:  5.126931190490723 0.3978561460971832
CurrentTrain: epoch  1, batch   108 | loss: 5.5247874Losses:  5.46351432800293 0.13010115921497345
CurrentTrain: epoch  1, batch   109 | loss: 5.5936155Losses:  5.497376918792725 0.3163832426071167
CurrentTrain: epoch  1, batch   110 | loss: 5.8137603Losses:  5.457743167877197 0.35283368825912476
CurrentTrain: epoch  1, batch   111 | loss: 5.8105769Losses:  6.3559980392456055 0.41963937878608704
CurrentTrain: epoch  1, batch   112 | loss: 6.7756376Losses:  5.891139984130859 0.5046306848526001
CurrentTrain: epoch  1, batch   113 | loss: 6.3957705Losses:  5.932755947113037 0.4421795606613159
CurrentTrain: epoch  1, batch   114 | loss: 6.3749356Losses:  5.888236045837402 0.5059961080551147
CurrentTrain: epoch  1, batch   115 | loss: 6.3942323Losses:  5.677731513977051 0.5119280815124512
CurrentTrain: epoch  1, batch   116 | loss: 6.1896596Losses:  5.844749450683594 0.4324263334274292
CurrentTrain: epoch  1, batch   117 | loss: 6.2771759Losses:  5.35489559173584 0.4679471552371979
CurrentTrain: epoch  1, batch   118 | loss: 5.8228426Losses:  4.869429111480713 0.5034923553466797
CurrentTrain: epoch  1, batch   119 | loss: 5.3729215Losses:  5.563690185546875 0.2948211431503296
CurrentTrain: epoch  1, batch   120 | loss: 5.8585114Losses:  5.840965270996094 0.5105094909667969
CurrentTrain: epoch  1, batch   121 | loss: 6.3514748Losses:  5.047292709350586 0.2758997082710266
CurrentTrain: epoch  1, batch   122 | loss: 5.3231926Losses:  5.40545654296875 0.3773200213909149
CurrentTrain: epoch  1, batch   123 | loss: 5.7827764Losses:  7.33488130569458 0.5478503704071045
CurrentTrain: epoch  1, batch   124 | loss: 7.8827314Losses:  4.725653648376465 0.3850994110107422
CurrentTrain: epoch  2, batch     0 | loss: 5.1107531Losses:  5.001496315002441 0.28502148389816284
CurrentTrain: epoch  2, batch     1 | loss: 5.2865176Losses:  5.221982002258301 0.29267823696136475
CurrentTrain: epoch  2, batch     2 | loss: 5.5146604Losses:  5.057938575744629 0.31607574224472046
CurrentTrain: epoch  2, batch     3 | loss: 5.3740144Losses:  4.857806205749512 0.4575190246105194
CurrentTrain: epoch  2, batch     4 | loss: 5.3153253Losses:  5.4616804122924805 0.25593674182891846
CurrentTrain: epoch  2, batch     5 | loss: 5.7176170Losses:  4.7675981521606445 0.24046796560287476
CurrentTrain: epoch  2, batch     6 | loss: 5.0080662Losses:  5.982311248779297 0.21336449682712555
CurrentTrain: epoch  2, batch     7 | loss: 6.1956758Losses:  5.420910358428955 0.3064483404159546
CurrentTrain: epoch  2, batch     8 | loss: 5.7273588Losses:  4.9951629638671875 0.45168617367744446
CurrentTrain: epoch  2, batch     9 | loss: 5.4468493Losses:  5.440543174743652 0.31472188234329224
CurrentTrain: epoch  2, batch    10 | loss: 5.7552652Losses:  4.8782525062561035 0.2332790046930313
CurrentTrain: epoch  2, batch    11 | loss: 5.1115317Losses:  5.057130336761475 0.4269091486930847
CurrentTrain: epoch  2, batch    12 | loss: 5.4840393Losses:  4.79753303527832 0.19118762016296387
CurrentTrain: epoch  2, batch    13 | loss: 4.9887209Losses:  4.788715362548828 0.41184481978416443
CurrentTrain: epoch  2, batch    14 | loss: 5.2005601Losses:  4.7389235496521 0.4614189863204956
CurrentTrain: epoch  2, batch    15 | loss: 5.2003427Losses:  4.642475128173828 0.20033250749111176
CurrentTrain: epoch  2, batch    16 | loss: 4.8428078Losses:  6.557055473327637 0.4465952515602112
CurrentTrain: epoch  2, batch    17 | loss: 7.0036507Losses:  4.562906265258789 0.3292728066444397
CurrentTrain: epoch  2, batch    18 | loss: 4.8921790Losses:  5.29310417175293 0.43670785427093506
CurrentTrain: epoch  2, batch    19 | loss: 5.7298121Losses:  4.493400573730469 0.314837783575058
CurrentTrain: epoch  2, batch    20 | loss: 4.8082385Losses:  6.260344505310059 0.5674020648002625
CurrentTrain: epoch  2, batch    21 | loss: 6.8277464Losses:  5.216026306152344 0.36260488629341125
CurrentTrain: epoch  2, batch    22 | loss: 5.5786314Losses:  5.176843643188477 0.15975052118301392
CurrentTrain: epoch  2, batch    23 | loss: 5.3365941Losses:  5.39544677734375 0.2889111042022705
CurrentTrain: epoch  2, batch    24 | loss: 5.6843576Losses:  4.536370754241943 0.3774378299713135
CurrentTrain: epoch  2, batch    25 | loss: 4.9138088Losses:  5.082271575927734 0.36053067445755005
CurrentTrain: epoch  2, batch    26 | loss: 5.4428024Losses:  5.981620788574219 0.25849729776382446
CurrentTrain: epoch  2, batch    27 | loss: 6.2401180Losses:  5.5928955078125 0.3125777542591095
CurrentTrain: epoch  2, batch    28 | loss: 5.9054732Losses:  5.597381114959717 0.16990284621715546
CurrentTrain: epoch  2, batch    29 | loss: 5.7672839Losses:  5.042181015014648 0.3250787556171417
CurrentTrain: epoch  2, batch    30 | loss: 5.3672600Losses:  4.695108890533447 0.21302953362464905
CurrentTrain: epoch  2, batch    31 | loss: 4.9081383Losses:  4.991878986358643 0.3712553381919861
CurrentTrain: epoch  2, batch    32 | loss: 5.3631344Losses:  5.039067268371582 0.3360916078090668
CurrentTrain: epoch  2, batch    33 | loss: 5.3751588Losses:  4.53231143951416 0.17841719090938568
CurrentTrain: epoch  2, batch    34 | loss: 4.7107286Losses:  5.066298007965088 0.304343044757843
CurrentTrain: epoch  2, batch    35 | loss: 5.3706412Losses:  5.379150390625 0.3510160744190216
CurrentTrain: epoch  2, batch    36 | loss: 5.7301664Losses:  5.241798400878906 0.4155184328556061
CurrentTrain: epoch  2, batch    37 | loss: 5.6573167Losses:  5.507490158081055 0.3081407845020294
CurrentTrain: epoch  2, batch    38 | loss: 5.8156309Losses:  4.853278636932373 0.22159220278263092
CurrentTrain: epoch  2, batch    39 | loss: 5.0748711Losses:  4.852892875671387 0.38773995637893677
CurrentTrain: epoch  2, batch    40 | loss: 5.2406330Losses:  5.568716049194336 0.40431898832321167
CurrentTrain: epoch  2, batch    41 | loss: 5.9730349Losses:  5.618535995483398 0.3267865777015686
CurrentTrain: epoch  2, batch    42 | loss: 5.9453225Losses:  6.471977710723877 0.5197296738624573
CurrentTrain: epoch  2, batch    43 | loss: 6.9917073Losses:  5.017011642456055 0.4327697157859802
CurrentTrain: epoch  2, batch    44 | loss: 5.4497814Losses:  6.386772155761719 0.6509544849395752
CurrentTrain: epoch  2, batch    45 | loss: 7.0377264Losses:  5.327652931213379 0.32858455181121826
CurrentTrain: epoch  2, batch    46 | loss: 5.6562376Losses:  5.256243705749512 0.21969005465507507
CurrentTrain: epoch  2, batch    47 | loss: 5.4759336Losses:  4.9299492835998535 0.3003787398338318
CurrentTrain: epoch  2, batch    48 | loss: 5.2303281Losses:  4.7678022384643555 0.3485759496688843
CurrentTrain: epoch  2, batch    49 | loss: 5.1163783Losses:  5.357549667358398 0.39196833968162537
CurrentTrain: epoch  2, batch    50 | loss: 5.7495179Losses:  4.400188446044922 0.32491594552993774
CurrentTrain: epoch  2, batch    51 | loss: 4.7251043Losses:  4.601078033447266 0.32555505633354187
CurrentTrain: epoch  2, batch    52 | loss: 4.9266329Losses:  5.219931125640869 0.35309505462646484
CurrentTrain: epoch  2, batch    53 | loss: 5.5730262Losses:  4.691673278808594 0.30260202288627625
CurrentTrain: epoch  2, batch    54 | loss: 4.9942751Losses:  5.083620071411133 0.377782940864563
CurrentTrain: epoch  2, batch    55 | loss: 5.4614029Losses:  4.951676368713379 0.2153768390417099
CurrentTrain: epoch  2, batch    56 | loss: 5.1670532Losses:  5.599050521850586 0.206885427236557
CurrentTrain: epoch  2, batch    57 | loss: 5.8059359Losses:  4.622307777404785 0.21400123834609985
CurrentTrain: epoch  2, batch    58 | loss: 4.8363090Losses:  5.026586532592773 0.221622496843338
CurrentTrain: epoch  2, batch    59 | loss: 5.2482090Losses:  5.101874351501465 0.30097997188568115
CurrentTrain: epoch  2, batch    60 | loss: 5.4028544Losses:  4.693549156188965 0.3938898742198944
CurrentTrain: epoch  2, batch    61 | loss: 5.0874391Losses:  6.5193562507629395 0.5650744438171387
CurrentTrain: epoch  2, batch    62 | loss: 7.0844307Losses:  5.502568244934082 0.2594414949417114
CurrentTrain: epoch  2, batch    63 | loss: 5.7620096Losses:  4.834526062011719 0.18327149748802185
CurrentTrain: epoch  2, batch    64 | loss: 5.0177975Losses:  5.17662239074707 0.2788303792476654
CurrentTrain: epoch  2, batch    65 | loss: 5.4554529Losses:  5.07020378112793 0.27499666810035706
CurrentTrain: epoch  2, batch    66 | loss: 5.3452005Losses:  4.895600318908691 0.31738853454589844
CurrentTrain: epoch  2, batch    67 | loss: 5.2129889Losses:  6.4726409912109375 0.32732003927230835
CurrentTrain: epoch  2, batch    68 | loss: 6.7999611Losses:  7.057344436645508 0.5010193586349487
CurrentTrain: epoch  2, batch    69 | loss: 7.5583639Losses:  4.84056282043457 0.28557947278022766
CurrentTrain: epoch  2, batch    70 | loss: 5.1261425Losses:  5.00932502746582 0.22854679822921753
CurrentTrain: epoch  2, batch    71 | loss: 5.2378716Losses:  5.226281642913818 0.22478225827217102
CurrentTrain: epoch  2, batch    72 | loss: 5.4510641Losses:  4.826597213745117 0.20529040694236755
CurrentTrain: epoch  2, batch    73 | loss: 5.0318875Losses:  4.947823524475098 0.1972018927335739
CurrentTrain: epoch  2, batch    74 | loss: 5.1450253Losses:  5.212338924407959 0.25296661257743835
CurrentTrain: epoch  2, batch    75 | loss: 5.4653053Losses:  4.804899215698242 0.2547280788421631
CurrentTrain: epoch  2, batch    76 | loss: 5.0596275Losses:  5.153470039367676 0.1488860547542572
CurrentTrain: epoch  2, batch    77 | loss: 5.3023562Losses:  4.760016441345215 0.31434011459350586
CurrentTrain: epoch  2, batch    78 | loss: 5.0743566Losses:  5.207990646362305 0.41913849115371704
CurrentTrain: epoch  2, batch    79 | loss: 5.6271291Losses:  5.539285659790039 0.27508801221847534
CurrentTrain: epoch  2, batch    80 | loss: 5.8143735Losses:  4.657176971435547 0.21079692244529724
CurrentTrain: epoch  2, batch    81 | loss: 4.8679738Losses:  4.69281005859375 0.35035282373428345
CurrentTrain: epoch  2, batch    82 | loss: 5.0431628Losses:  4.257852554321289 0.23953193426132202
CurrentTrain: epoch  2, batch    83 | loss: 4.4973845Losses:  5.15103006362915 0.4260151982307434
CurrentTrain: epoch  2, batch    84 | loss: 5.5770454Losses:  4.9604291915893555 0.15245646238327026
CurrentTrain: epoch  2, batch    85 | loss: 5.1128855Losses:  4.785837173461914 0.19979362189769745
CurrentTrain: epoch  2, batch    86 | loss: 4.9856310Losses:  4.742055892944336 0.294885516166687
CurrentTrain: epoch  2, batch    87 | loss: 5.0369415Losses:  4.827260971069336 0.23236624896526337
CurrentTrain: epoch  2, batch    88 | loss: 5.0596271Losses:  4.550137042999268 0.22690987586975098
CurrentTrain: epoch  2, batch    89 | loss: 4.7770472Losses:  5.056675910949707 0.2036477029323578
CurrentTrain: epoch  2, batch    90 | loss: 5.2603235Losses:  4.568982124328613 0.3254617154598236
CurrentTrain: epoch  2, batch    91 | loss: 4.8944440Losses:  6.4156880378723145 0.5235711336135864
CurrentTrain: epoch  2, batch    92 | loss: 6.9392591Losses:  4.8941330909729 0.3389434814453125
CurrentTrain: epoch  2, batch    93 | loss: 5.2330766Losses:  4.805108070373535 0.21729667484760284
CurrentTrain: epoch  2, batch    94 | loss: 5.0224047Losses:  4.911922454833984 0.22438004612922668
CurrentTrain: epoch  2, batch    95 | loss: 5.1363025Losses:  4.325679779052734 0.21450939774513245
CurrentTrain: epoch  2, batch    96 | loss: 4.5401893Losses:  4.408044815063477 0.16444048285484314
CurrentTrain: epoch  2, batch    97 | loss: 4.5724854Losses:  4.820323467254639 0.19482582807540894
CurrentTrain: epoch  2, batch    98 | loss: 5.0151491Losses:  4.406254768371582 0.4155512750148773
CurrentTrain: epoch  2, batch    99 | loss: 4.8218060Losses:  4.595578670501709 0.17071890830993652
CurrentTrain: epoch  2, batch   100 | loss: 4.7662973Losses:  4.67812442779541 0.3172713816165924
CurrentTrain: epoch  2, batch   101 | loss: 4.9953957Losses:  4.557830333709717 0.13463516533374786
CurrentTrain: epoch  2, batch   102 | loss: 4.6924653Losses:  4.351811408996582 0.317577600479126
CurrentTrain: epoch  2, batch   103 | loss: 4.6693888Losses:  5.018622875213623 0.3081952929496765
CurrentTrain: epoch  2, batch   104 | loss: 5.3268180Losses:  4.715279579162598 0.23480567336082458
CurrentTrain: epoch  2, batch   105 | loss: 4.9500852Losses:  4.965251922607422 0.31769776344299316
CurrentTrain: epoch  2, batch   106 | loss: 5.2829494Losses:  5.493808269500732 0.27788880467414856
CurrentTrain: epoch  2, batch   107 | loss: 5.7716970Losses:  4.614746570587158 0.23251016438007355
CurrentTrain: epoch  2, batch   108 | loss: 4.8472567Losses:  4.583205223083496 0.1931365430355072
CurrentTrain: epoch  2, batch   109 | loss: 4.7763419Losses:  4.728339195251465 0.2956165075302124
CurrentTrain: epoch  2, batch   110 | loss: 5.0239558Losses:  4.660476207733154 0.19174103438854218
CurrentTrain: epoch  2, batch   111 | loss: 4.8522172Losses:  4.696504592895508 0.3452756702899933
CurrentTrain: epoch  2, batch   112 | loss: 5.0417805Losses:  4.468685150146484 0.2223896086215973
CurrentTrain: epoch  2, batch   113 | loss: 4.6910748Losses:  4.949143886566162 0.27649813890457153
CurrentTrain: epoch  2, batch   114 | loss: 5.2256422Losses:  4.614361763000488 0.33139878511428833
CurrentTrain: epoch  2, batch   115 | loss: 4.9457607Losses:  4.792544364929199 0.2127605676651001
CurrentTrain: epoch  2, batch   116 | loss: 5.0053048Losses:  6.269940376281738 0.5026192665100098
CurrentTrain: epoch  2, batch   117 | loss: 6.7725596Losses:  4.485825538635254 0.2502157986164093
CurrentTrain: epoch  2, batch   118 | loss: 4.7360415Losses:  4.676751136779785 0.18377339839935303
CurrentTrain: epoch  2, batch   119 | loss: 4.8605247Losses:  4.405439376831055 0.24015185236930847
CurrentTrain: epoch  2, batch   120 | loss: 4.6455913Losses:  4.5472941398620605 0.3315054774284363
CurrentTrain: epoch  2, batch   121 | loss: 4.8787994Losses:  4.433610439300537 0.22953540086746216
CurrentTrain: epoch  2, batch   122 | loss: 4.6631460Losses:  4.76455545425415 0.30282098054885864
CurrentTrain: epoch  2, batch   123 | loss: 5.0673766Losses:  4.5813069343566895 0.2929374575614929
CurrentTrain: epoch  2, batch   124 | loss: 4.8742442Losses:  4.431612014770508 0.16715079545974731
CurrentTrain: epoch  3, batch     0 | loss: 4.5987630Losses:  4.348085403442383 0.17583473026752472
CurrentTrain: epoch  3, batch     1 | loss: 4.5239201Losses:  4.324994087219238 0.23108601570129395
CurrentTrain: epoch  3, batch     2 | loss: 4.5560799Losses:  4.604552268981934 0.16574278473854065
CurrentTrain: epoch  3, batch     3 | loss: 4.7702951Losses:  6.55639123916626 0.5767507553100586
CurrentTrain: epoch  3, batch     4 | loss: 7.1331420Losses:  4.824727535247803 0.21497133374214172
CurrentTrain: epoch  3, batch     5 | loss: 5.0396991Losses:  5.476191520690918 0.28322792053222656
CurrentTrain: epoch  3, batch     6 | loss: 5.7594194Losses:  4.5534257888793945 0.3646479845046997
CurrentTrain: epoch  3, batch     7 | loss: 4.9180737Losses:  5.031661033630371 0.35852235555648804
CurrentTrain: epoch  3, batch     8 | loss: 5.3901834Losses:  5.589974403381348 0.2536969482898712
CurrentTrain: epoch  3, batch     9 | loss: 5.8436713Losses:  4.37326192855835 0.14560958743095398
CurrentTrain: epoch  3, batch    10 | loss: 4.5188713Losses:  4.8381805419921875 0.1971794068813324
CurrentTrain: epoch  3, batch    11 | loss: 5.0353599Losses:  5.513522624969482 0.28843969106674194
CurrentTrain: epoch  3, batch    12 | loss: 5.8019624Losses:  4.470592021942139 0.21517495810985565
CurrentTrain: epoch  3, batch    13 | loss: 4.6857672Losses:  5.5531721115112305 0.17448130249977112
CurrentTrain: epoch  3, batch    14 | loss: 5.7276535Losses:  4.494266986846924 0.17174524068832397
CurrentTrain: epoch  3, batch    15 | loss: 4.6660123Losses:  4.851615905761719 0.23266440629959106
CurrentTrain: epoch  3, batch    16 | loss: 5.0842805Losses:  5.3205413818359375 0.20665204524993896
CurrentTrain: epoch  3, batch    17 | loss: 5.5271935Losses:  4.829001426696777 0.21406835317611694
CurrentTrain: epoch  3, batch    18 | loss: 5.0430698Losses:  4.4547247886657715 0.1859551966190338
CurrentTrain: epoch  3, batch    19 | loss: 4.6406798Losses:  4.527387619018555 0.2585545778274536
CurrentTrain: epoch  3, batch    20 | loss: 4.7859421Losses:  4.8899736404418945 0.2385474145412445
CurrentTrain: epoch  3, batch    21 | loss: 5.1285210Losses:  4.570917129516602 0.0886295884847641
CurrentTrain: epoch  3, batch    22 | loss: 4.6595469Losses:  4.336940288543701 0.22819101810455322
CurrentTrain: epoch  3, batch    23 | loss: 4.5651312Losses:  4.725472450256348 0.1605212241411209
CurrentTrain: epoch  3, batch    24 | loss: 4.8859935Losses:  4.451202392578125 0.09420983493328094
CurrentTrain: epoch  3, batch    25 | loss: 4.5454121Losses:  4.452005386352539 0.2976866364479065
CurrentTrain: epoch  3, batch    26 | loss: 4.7496920Losses:  4.5152788162231445 0.2328636199235916
CurrentTrain: epoch  3, batch    27 | loss: 4.7481422Losses:  5.052131652832031 0.17527058720588684
CurrentTrain: epoch  3, batch    28 | loss: 5.2274022Losses:  4.3943328857421875 0.14188778400421143
CurrentTrain: epoch  3, batch    29 | loss: 4.5362206Losses:  4.815699100494385 0.23222559690475464
CurrentTrain: epoch  3, batch    30 | loss: 5.0479245Losses:  4.419878005981445 0.29290685057640076
CurrentTrain: epoch  3, batch    31 | loss: 4.7127848Losses:  5.631028175354004 0.314725399017334
CurrentTrain: epoch  3, batch    32 | loss: 5.9457536Losses:  4.604129791259766 0.30323848128318787
CurrentTrain: epoch  3, batch    33 | loss: 4.9073682Losses:  4.367636680603027 0.18130800127983093
CurrentTrain: epoch  3, batch    34 | loss: 4.5489445Losses:  4.683131217956543 0.1267021745443344
CurrentTrain: epoch  3, batch    35 | loss: 4.8098335Losses:  4.679808616638184 0.18864920735359192
CurrentTrain: epoch  3, batch    36 | loss: 4.8684578Losses:  4.36732292175293 0.33623427152633667
CurrentTrain: epoch  3, batch    37 | loss: 4.7035570Losses:  4.669054985046387 0.05525543540716171
CurrentTrain: epoch  3, batch    38 | loss: 4.7243104Losses:  4.561228275299072 0.13082274794578552
CurrentTrain: epoch  3, batch    39 | loss: 4.6920509Losses:  4.620988368988037 0.25369197130203247
CurrentTrain: epoch  3, batch    40 | loss: 4.8746805Losses:  4.100137710571289 0.13557285070419312
CurrentTrain: epoch  3, batch    41 | loss: 4.2357106Losses:  4.517550468444824 0.1909167468547821
CurrentTrain: epoch  3, batch    42 | loss: 4.7084670Losses:  4.3558030128479 0.06262370198965073
CurrentTrain: epoch  3, batch    43 | loss: 4.4184265Losses:  4.464995384216309 0.21867844462394714
CurrentTrain: epoch  3, batch    44 | loss: 4.6836739Losses:  4.5362958908081055 0.3017524480819702
CurrentTrain: epoch  3, batch    45 | loss: 4.8380485Losses:  4.587204933166504 0.16462025046348572
CurrentTrain: epoch  3, batch    46 | loss: 4.7518253Losses:  4.899360656738281 0.38042864203453064
CurrentTrain: epoch  3, batch    47 | loss: 5.2797894Losses:  4.374987602233887 0.22345267236232758
CurrentTrain: epoch  3, batch    48 | loss: 4.5984402Losses:  4.629166603088379 0.24933673441410065
CurrentTrain: epoch  3, batch    49 | loss: 4.8785033Losses:  4.472740173339844 0.3341958820819855
CurrentTrain: epoch  3, batch    50 | loss: 4.8069363Losses:  4.375536918640137 0.2553529739379883
CurrentTrain: epoch  3, batch    51 | loss: 4.6308899Losses:  4.7346649169921875 0.16084320843219757
CurrentTrain: epoch  3, batch    52 | loss: 4.8955083Losses:  4.6178083419799805 0.1346333920955658
CurrentTrain: epoch  3, batch    53 | loss: 4.7524419Losses:  4.446956634521484 0.23478782176971436
CurrentTrain: epoch  3, batch    54 | loss: 4.6817446Losses:  4.551115989685059 0.26350924372673035
CurrentTrain: epoch  3, batch    55 | loss: 4.8146253Losses:  4.578895568847656 0.26650604605674744
CurrentTrain: epoch  3, batch    56 | loss: 4.8454018Losses:  4.620972633361816 0.22590136528015137
CurrentTrain: epoch  3, batch    57 | loss: 4.8468742Losses:  5.193098068237305 0.2250339835882187
CurrentTrain: epoch  3, batch    58 | loss: 5.4181318Losses:  4.273576736450195 0.19617736339569092
CurrentTrain: epoch  3, batch    59 | loss: 4.4697542Losses:  4.374691963195801 0.304088294506073
CurrentTrain: epoch  3, batch    60 | loss: 4.6787801Losses:  4.753598690032959 0.19192363321781158
CurrentTrain: epoch  3, batch    61 | loss: 4.9455223Losses:  4.56549072265625 0.31506434082984924
CurrentTrain: epoch  3, batch    62 | loss: 4.8805552Losses:  4.384500026702881 0.18704822659492493
CurrentTrain: epoch  3, batch    63 | loss: 4.5715485Losses:  4.3989996910095215 0.1555095613002777
CurrentTrain: epoch  3, batch    64 | loss: 4.5545092Losses:  4.568199157714844 0.19269314408302307
CurrentTrain: epoch  3, batch    65 | loss: 4.7608924Losses:  4.612817287445068 0.15003851056098938
CurrentTrain: epoch  3, batch    66 | loss: 4.7628560Losses:  4.479989528656006 0.23590724170207977
CurrentTrain: epoch  3, batch    67 | loss: 4.7158966Losses:  4.46781063079834 0.336044043302536
CurrentTrain: epoch  3, batch    68 | loss: 4.8038545Losses:  4.401054859161377 0.16904965043067932
CurrentTrain: epoch  3, batch    69 | loss: 4.5701046Losses:  4.427887916564941 0.26345521211624146
CurrentTrain: epoch  3, batch    70 | loss: 4.6913433Losses:  4.741416931152344 0.25844722986221313
CurrentTrain: epoch  3, batch    71 | loss: 4.9998641Losses:  4.568257808685303 0.30224692821502686
CurrentTrain: epoch  3, batch    72 | loss: 4.8705049Losses:  6.348859786987305 0.4005824327468872
CurrentTrain: epoch  3, batch    73 | loss: 6.7494421Losses:  4.806161403656006 0.2247762531042099
CurrentTrain: epoch  3, batch    74 | loss: 5.0309377Losses:  4.2939910888671875 0.18121939897537231
CurrentTrain: epoch  3, batch    75 | loss: 4.4752107Losses:  4.20817232131958 0.19753332436084747
CurrentTrain: epoch  3, batch    76 | loss: 4.4057055Losses:  4.800421714782715 0.18796873092651367
CurrentTrain: epoch  3, batch    77 | loss: 4.9883904Losses:  4.554320335388184 0.2009834498167038
CurrentTrain: epoch  3, batch    78 | loss: 4.7553039Losses:  4.599806785583496 0.174537792801857
CurrentTrain: epoch  3, batch    79 | loss: 4.7743444Losses:  4.473583698272705 0.07834671437740326
CurrentTrain: epoch  3, batch    80 | loss: 4.5519304Losses:  4.318076133728027 0.13008204102516174
CurrentTrain: epoch  3, batch    81 | loss: 4.4481583Losses:  5.023372173309326 0.21330085396766663
CurrentTrain: epoch  3, batch    82 | loss: 5.2366729Losses:  4.227532386779785 0.1573432832956314
CurrentTrain: epoch  3, batch    83 | loss: 4.3848758Losses:  4.771810054779053 0.21600094437599182
CurrentTrain: epoch  3, batch    84 | loss: 4.9878111Losses:  4.627227783203125 0.18041440844535828
CurrentTrain: epoch  3, batch    85 | loss: 4.8076420Losses:  4.32500696182251 0.17584311962127686
CurrentTrain: epoch  3, batch    86 | loss: 4.5008502Losses:  4.280874252319336 0.07077183574438095
CurrentTrain: epoch  3, batch    87 | loss: 4.3516459Losses:  5.313799858093262 0.29653775691986084
CurrentTrain: epoch  3, batch    88 | loss: 5.6103377Losses:  5.126455307006836 0.1501205861568451
CurrentTrain: epoch  3, batch    89 | loss: 5.2765760Losses:  4.378380298614502 0.11993978917598724
CurrentTrain: epoch  3, batch    90 | loss: 4.4983201Losses:  4.717679977416992 0.20811043679714203
CurrentTrain: epoch  3, batch    91 | loss: 4.9257903Losses:  4.750872611999512 0.17709606885910034
CurrentTrain: epoch  3, batch    92 | loss: 4.9279685Losses:  4.3257365226745605 0.15860216319561005
CurrentTrain: epoch  3, batch    93 | loss: 4.4843388Losses:  4.446985244750977 0.2196137011051178
CurrentTrain: epoch  3, batch    94 | loss: 4.6665988Losses:  4.3533501625061035 0.16516610980033875
CurrentTrain: epoch  3, batch    95 | loss: 4.5185161Losses:  4.304489612579346 0.20712651312351227
CurrentTrain: epoch  3, batch    96 | loss: 4.5116162Losses:  4.982699394226074 0.22178369760513306
CurrentTrain: epoch  3, batch    97 | loss: 5.2044830Losses:  4.5118865966796875 0.1991155594587326
CurrentTrain: epoch  3, batch    98 | loss: 4.7110023Losses:  4.539394855499268 0.22903166711330414
CurrentTrain: epoch  3, batch    99 | loss: 4.7684264Losses:  4.421292781829834 0.1306462585926056
CurrentTrain: epoch  3, batch   100 | loss: 4.5519390Losses:  4.501620292663574 0.09721247851848602
CurrentTrain: epoch  3, batch   101 | loss: 4.5988326Losses:  4.554699897766113 0.2602729797363281
CurrentTrain: epoch  3, batch   102 | loss: 4.8149729Losses:  4.196001052856445 0.18207621574401855
CurrentTrain: epoch  3, batch   103 | loss: 4.3780775Losses:  4.553332328796387 0.2497778683900833
CurrentTrain: epoch  3, batch   104 | loss: 4.8031101Losses:  4.41080379486084 0.2204241156578064
CurrentTrain: epoch  3, batch   105 | loss: 4.6312280Losses:  4.429896354675293 0.12478382140398026
CurrentTrain: epoch  3, batch   106 | loss: 4.5546803Losses:  4.413952827453613 0.1068660318851471
CurrentTrain: epoch  3, batch   107 | loss: 4.5208187Losses:  4.298343181610107 0.0753106027841568
CurrentTrain: epoch  3, batch   108 | loss: 4.3736539Losses:  4.369099140167236 0.11702986806631088
CurrentTrain: epoch  3, batch   109 | loss: 4.4861288Losses:  4.518197536468506 0.3108780086040497
CurrentTrain: epoch  3, batch   110 | loss: 4.8290753Losses:  4.092509746551514 0.19727295637130737
CurrentTrain: epoch  3, batch   111 | loss: 4.2897825Losses:  4.178918838500977 0.10046708583831787
CurrentTrain: epoch  3, batch   112 | loss: 4.2793860Losses:  4.567955017089844 0.16640716791152954
CurrentTrain: epoch  3, batch   113 | loss: 4.7343621Losses:  4.301839351654053 0.29587724804878235
CurrentTrain: epoch  3, batch   114 | loss: 4.5977168Losses:  4.814391613006592 0.22745442390441895
CurrentTrain: epoch  3, batch   115 | loss: 5.0418463Losses:  4.4030961990356445 0.2565866708755493
CurrentTrain: epoch  3, batch   116 | loss: 4.6596828Losses:  4.198428153991699 0.21470265090465546
CurrentTrain: epoch  3, batch   117 | loss: 4.4131308Losses:  4.13405704498291 0.09485605359077454
CurrentTrain: epoch  3, batch   118 | loss: 4.2289133Losses:  4.243626117706299 0.24030089378356934
CurrentTrain: epoch  3, batch   119 | loss: 4.4839268Losses:  4.3004560470581055 0.2717466950416565
CurrentTrain: epoch  3, batch   120 | loss: 4.5722027Losses:  4.20159387588501 0.15912312269210815
CurrentTrain: epoch  3, batch   121 | loss: 4.3607168Losses:  4.258845329284668 0.2926108241081238
CurrentTrain: epoch  3, batch   122 | loss: 4.5514560Losses:  4.409965515136719 0.2580333948135376
CurrentTrain: epoch  3, batch   123 | loss: 4.6679988Losses:  4.278285980224609 0.16201087832450867
CurrentTrain: epoch  3, batch   124 | loss: 4.4402966Losses:  4.502202033996582 0.17922726273536682
CurrentTrain: epoch  4, batch     0 | loss: 4.6814294Losses:  4.376293182373047 0.23157665133476257
CurrentTrain: epoch  4, batch     1 | loss: 4.6078696Losses:  4.328404426574707 0.1891549825668335
CurrentTrain: epoch  4, batch     2 | loss: 4.5175595Losses:  4.285670280456543 0.24914968013763428
CurrentTrain: epoch  4, batch     3 | loss: 4.5348201Losses:  4.321784019470215 0.16502757370471954
CurrentTrain: epoch  4, batch     4 | loss: 4.4868116Losses:  4.395148277282715 0.1674971580505371
CurrentTrain: epoch  4, batch     5 | loss: 4.5626454Losses:  4.262788772583008 0.2020266354084015
CurrentTrain: epoch  4, batch     6 | loss: 4.4648156Losses:  4.246971607208252 0.14677469432353973
CurrentTrain: epoch  4, batch     7 | loss: 4.3937464Losses:  4.219664096832275 0.16077297925949097
CurrentTrain: epoch  4, batch     8 | loss: 4.3804369Losses:  4.301576614379883 0.21251550316810608
CurrentTrain: epoch  4, batch     9 | loss: 4.5140920Losses:  4.345873832702637 0.1543198525905609
CurrentTrain: epoch  4, batch    10 | loss: 4.5001936Losses:  4.2518205642700195 0.1936223804950714
CurrentTrain: epoch  4, batch    11 | loss: 4.4454432Losses:  4.501166820526123 0.1977696716785431
CurrentTrain: epoch  4, batch    12 | loss: 4.6989365Losses:  4.044129371643066 0.0935603603720665
CurrentTrain: epoch  4, batch    13 | loss: 4.1376896Losses:  4.232883930206299 0.3496842682361603
CurrentTrain: epoch  4, batch    14 | loss: 4.5825682Losses:  4.461998462677002 0.20807799696922302
CurrentTrain: epoch  4, batch    15 | loss: 4.6700764Losses:  4.447717666625977 0.10554264485836029
CurrentTrain: epoch  4, batch    16 | loss: 4.5532603Losses:  4.326930046081543 0.1916702389717102
CurrentTrain: epoch  4, batch    17 | loss: 4.5186005Losses:  4.553134918212891 0.16480425000190735
CurrentTrain: epoch  4, batch    18 | loss: 4.7179394Losses:  4.2135114669799805 0.13275057077407837
CurrentTrain: epoch  4, batch    19 | loss: 4.3462620Losses:  4.217967987060547 0.1728610098361969
CurrentTrain: epoch  4, batch    20 | loss: 4.3908291Losses:  4.295504570007324 0.252692848443985
CurrentTrain: epoch  4, batch    21 | loss: 4.5481973Losses:  4.207131385803223 0.1825590431690216
CurrentTrain: epoch  4, batch    22 | loss: 4.3896904Losses:  4.262527942657471 0.1940148025751114
CurrentTrain: epoch  4, batch    23 | loss: 4.4565430Losses:  4.220531463623047 0.20761707425117493
CurrentTrain: epoch  4, batch    24 | loss: 4.4281487Losses:  4.1536993980407715 0.21566563844680786
CurrentTrain: epoch  4, batch    25 | loss: 4.3693652Losses:  4.315203666687012 0.121677465736866
CurrentTrain: epoch  4, batch    26 | loss: 4.4368811Losses:  4.326420783996582 0.22310903668403625
CurrentTrain: epoch  4, batch    27 | loss: 4.5495300Losses:  4.144481182098389 0.11388947814702988
CurrentTrain: epoch  4, batch    28 | loss: 4.2583709Losses:  4.331404685974121 0.21416069567203522
CurrentTrain: epoch  4, batch    29 | loss: 4.5455656Losses:  4.255273342132568 0.1808840036392212
CurrentTrain: epoch  4, batch    30 | loss: 4.4361572Losses:  4.228402614593506 0.16826897859573364
CurrentTrain: epoch  4, batch    31 | loss: 4.3966718Losses:  4.239775657653809 0.1329692006111145
CurrentTrain: epoch  4, batch    32 | loss: 4.3727450Losses:  5.983565330505371 0.21844062209129333
CurrentTrain: epoch  4, batch    33 | loss: 6.2020059Losses:  4.251137733459473 0.13374222815036774
CurrentTrain: epoch  4, batch    34 | loss: 4.3848801Losses:  4.201948165893555 0.08245411515235901
CurrentTrain: epoch  4, batch    35 | loss: 4.2844024Losses:  4.4790239334106445 0.127271369099617
CurrentTrain: epoch  4, batch    36 | loss: 4.6062951Losses:  4.181858062744141 0.1849145144224167
CurrentTrain: epoch  4, batch    37 | loss: 4.3667727Losses:  4.204029083251953 0.1335052251815796
CurrentTrain: epoch  4, batch    38 | loss: 4.3375344Losses:  4.528478622436523 0.21824516355991364
CurrentTrain: epoch  4, batch    39 | loss: 4.7467237Losses:  4.396458625793457 0.19061262905597687
CurrentTrain: epoch  4, batch    40 | loss: 4.5870714Losses:  4.065513610839844 0.19437271356582642
CurrentTrain: epoch  4, batch    41 | loss: 4.2598863Losses:  4.142999649047852 0.1387343555688858
CurrentTrain: epoch  4, batch    42 | loss: 4.2817340Losses:  4.230001449584961 0.22384695708751678
CurrentTrain: epoch  4, batch    43 | loss: 4.4538484Losses:  4.683777332305908 0.1371893286705017
CurrentTrain: epoch  4, batch    44 | loss: 4.8209667Losses:  4.234317302703857 0.21370521187782288
CurrentTrain: epoch  4, batch    45 | loss: 4.4480224Losses:  4.685689926147461 0.18328209221363068
CurrentTrain: epoch  4, batch    46 | loss: 4.8689718Losses:  4.233372211456299 0.1989068239927292
CurrentTrain: epoch  4, batch    47 | loss: 4.4322791Losses:  4.219047546386719 0.1593751609325409
CurrentTrain: epoch  4, batch    48 | loss: 4.3784227Losses:  4.121288776397705 0.12611648440361023
CurrentTrain: epoch  4, batch    49 | loss: 4.2474051Losses:  4.291576385498047 0.22502164542675018
CurrentTrain: epoch  4, batch    50 | loss: 4.5165982Losses:  4.682942867279053 0.17276209592819214
CurrentTrain: epoch  4, batch    51 | loss: 4.8557048Losses:  4.265655517578125 0.21479150652885437
CurrentTrain: epoch  4, batch    52 | loss: 4.4804468Losses:  4.3268914222717285 0.16622528433799744
CurrentTrain: epoch  4, batch    53 | loss: 4.4931169Losses:  4.409550666809082 0.18499907851219177
CurrentTrain: epoch  4, batch    54 | loss: 4.5945497Losses:  4.187772750854492 0.09532641619443893
CurrentTrain: epoch  4, batch    55 | loss: 4.2830992Losses:  4.411880970001221 0.16787144541740417
CurrentTrain: epoch  4, batch    56 | loss: 4.5797524Losses:  4.211119651794434 0.1631152480840683
CurrentTrain: epoch  4, batch    57 | loss: 4.3742347Losses:  4.581666946411133 0.17297597229480743
CurrentTrain: epoch  4, batch    58 | loss: 4.7546430Losses:  4.072694778442383 0.11881573498249054
CurrentTrain: epoch  4, batch    59 | loss: 4.1915107Losses:  4.168995380401611 0.12747527658939362
CurrentTrain: epoch  4, batch    60 | loss: 4.2964706Losses:  4.130850315093994 0.10336058586835861
CurrentTrain: epoch  4, batch    61 | loss: 4.2342110Losses:  4.351020812988281 0.1633160412311554
CurrentTrain: epoch  4, batch    62 | loss: 4.5143371Losses:  4.592011451721191 0.23081138730049133
CurrentTrain: epoch  4, batch    63 | loss: 4.8228230Losses:  4.313717365264893 0.20407313108444214
CurrentTrain: epoch  4, batch    64 | loss: 4.5177903Losses:  4.356404781341553 0.11555689573287964
CurrentTrain: epoch  4, batch    65 | loss: 4.4719615Losses:  4.104471206665039 0.10601292550563812
CurrentTrain: epoch  4, batch    66 | loss: 4.2104840Losses:  4.052914619445801 0.13941740989685059
CurrentTrain: epoch  4, batch    67 | loss: 4.1923323Losses:  4.301385879516602 0.23692572116851807
CurrentTrain: epoch  4, batch    68 | loss: 4.5383115Losses:  4.399787902832031 0.16375982761383057
CurrentTrain: epoch  4, batch    69 | loss: 4.5635476Losses:  4.1371355056762695 0.10279867053031921
CurrentTrain: epoch  4, batch    70 | loss: 4.2399340Losses:  4.391286849975586 0.1948738694190979
CurrentTrain: epoch  4, batch    71 | loss: 4.5861607Losses:  4.449188232421875 0.1901448369026184
CurrentTrain: epoch  4, batch    72 | loss: 4.6393332Losses:  4.21107292175293 0.08223146945238113
CurrentTrain: epoch  4, batch    73 | loss: 4.2933044Losses:  4.279071807861328 0.2064550817012787
CurrentTrain: epoch  4, batch    74 | loss: 4.4855270Losses:  4.466242790222168 0.13309118151664734
CurrentTrain: epoch  4, batch    75 | loss: 4.5993338Losses:  4.409376621246338 0.24215683341026306
CurrentTrain: epoch  4, batch    76 | loss: 4.6515336Losses:  4.356720447540283 0.26437491178512573
CurrentTrain: epoch  4, batch    77 | loss: 4.6210952Losses:  4.31117057800293 0.1383208930492401
CurrentTrain: epoch  4, batch    78 | loss: 4.4494915Losses:  4.200501441955566 0.11986144632101059
CurrentTrain: epoch  4, batch    79 | loss: 4.3203630Losses:  4.098649978637695 0.10212419927120209
CurrentTrain: epoch  4, batch    80 | loss: 4.2007742Losses:  4.1909403800964355 0.13852697610855103
CurrentTrain: epoch  4, batch    81 | loss: 4.3294673Losses:  4.272598743438721 0.20333534479141235
CurrentTrain: epoch  4, batch    82 | loss: 4.4759340Losses:  4.185457229614258 0.1595500409603119
CurrentTrain: epoch  4, batch    83 | loss: 4.3450074Losses:  4.2556376457214355 0.12673306465148926
CurrentTrain: epoch  4, batch    84 | loss: 4.3823709Losses:  4.188950538635254 0.13267526030540466
CurrentTrain: epoch  4, batch    85 | loss: 4.3216257Losses:  4.415257453918457 0.17426460981369019
CurrentTrain: epoch  4, batch    86 | loss: 4.5895219Losses:  4.923542499542236 0.3101920187473297
CurrentTrain: epoch  4, batch    87 | loss: 5.2337346Losses:  4.135437488555908 0.10265552997589111
CurrentTrain: epoch  4, batch    88 | loss: 4.2380929Losses:  4.208279132843018 0.11216581612825394
CurrentTrain: epoch  4, batch    89 | loss: 4.3204451Losses:  4.1719560623168945 0.07206624001264572
CurrentTrain: epoch  4, batch    90 | loss: 4.2440224Losses:  4.197970390319824 0.07271838933229446
CurrentTrain: epoch  4, batch    91 | loss: 4.2706890Losses:  4.061017990112305 0.06729499250650406
CurrentTrain: epoch  4, batch    92 | loss: 4.1283131Losses:  4.114894866943359 0.17724570631980896
CurrentTrain: epoch  4, batch    93 | loss: 4.2921405Losses:  4.054435729980469 0.1379697024822235
CurrentTrain: epoch  4, batch    94 | loss: 4.1924052Losses:  4.350370407104492 0.1497817039489746
CurrentTrain: epoch  4, batch    95 | loss: 4.5001521Losses:  4.240091323852539 0.1100236177444458
CurrentTrain: epoch  4, batch    96 | loss: 4.3501148Losses:  4.439662456512451 0.18704216182231903
CurrentTrain: epoch  4, batch    97 | loss: 4.6267047Losses:  4.369351863861084 0.12399809807538986
CurrentTrain: epoch  4, batch    98 | loss: 4.4933500Losses:  4.193136692047119 0.19888344407081604
CurrentTrain: epoch  4, batch    99 | loss: 4.3920202Losses:  4.158289432525635 0.12225461006164551
CurrentTrain: epoch  4, batch   100 | loss: 4.2805443Losses:  4.247171878814697 0.16504722833633423
CurrentTrain: epoch  4, batch   101 | loss: 4.4122190Losses:  4.255411148071289 0.18094787001609802
CurrentTrain: epoch  4, batch   102 | loss: 4.4363589Losses:  4.215444564819336 0.12089283764362335
CurrentTrain: epoch  4, batch   103 | loss: 4.3363376Losses:  4.354062080383301 0.1488071084022522
CurrentTrain: epoch  4, batch   104 | loss: 4.5028691Losses:  4.210786819458008 0.12926025688648224
CurrentTrain: epoch  4, batch   105 | loss: 4.3400469Losses:  4.1438984870910645 0.0590754933655262
CurrentTrain: epoch  4, batch   106 | loss: 4.2029738Losses:  4.169684410095215 0.09796265512704849
CurrentTrain: epoch  4, batch   107 | loss: 4.2676473Losses:  4.410813331604004 0.20966821908950806
CurrentTrain: epoch  4, batch   108 | loss: 4.6204815Losses:  4.170141220092773 0.1766088753938675
CurrentTrain: epoch  4, batch   109 | loss: 4.3467503Losses:  4.378140449523926 0.16817224025726318
CurrentTrain: epoch  4, batch   110 | loss: 4.5463128Losses:  4.196004867553711 0.14620491862297058
CurrentTrain: epoch  4, batch   111 | loss: 4.3422098Losses:  4.3050923347473145 0.11363138258457184
CurrentTrain: epoch  4, batch   112 | loss: 4.4187236Losses:  4.3310770988464355 0.16052758693695068
CurrentTrain: epoch  4, batch   113 | loss: 4.4916048Losses:  4.178613185882568 0.1784747838973999
CurrentTrain: epoch  4, batch   114 | loss: 4.3570881Losses:  4.153661251068115 0.1513771116733551
CurrentTrain: epoch  4, batch   115 | loss: 4.3050385Losses:  4.141758918762207 0.09834170341491699
CurrentTrain: epoch  4, batch   116 | loss: 4.2401009Losses:  4.3120927810668945 0.24816608428955078
CurrentTrain: epoch  4, batch   117 | loss: 4.5602589Losses:  4.356803894042969 0.1283094435930252
CurrentTrain: epoch  4, batch   118 | loss: 4.4851131Losses:  4.423672676086426 0.12425302714109421
CurrentTrain: epoch  4, batch   119 | loss: 4.5479255Losses:  4.180446624755859 0.10223330557346344
CurrentTrain: epoch  4, batch   120 | loss: 4.2826800Losses:  4.313546180725098 0.19050376117229462
CurrentTrain: epoch  4, batch   121 | loss: 4.5040498Losses:  4.124802112579346 0.10313158482313156
CurrentTrain: epoch  4, batch   122 | loss: 4.2279339Losses:  4.1734466552734375 0.10281320661306381
CurrentTrain: epoch  4, batch   123 | loss: 4.2762599Losses:  4.9397735595703125 0.2058609127998352
CurrentTrain: epoch  4, batch   124 | loss: 5.1456347Losses:  4.172940254211426 0.15399301052093506
CurrentTrain: epoch  5, batch     0 | loss: 4.3269334Losses:  4.192843437194824 0.20426401495933533
CurrentTrain: epoch  5, batch     1 | loss: 4.3971076Losses:  4.194973468780518 0.13466502726078033
CurrentTrain: epoch  5, batch     2 | loss: 4.3296385Losses:  4.215180397033691 0.20244306325912476
CurrentTrain: epoch  5, batch     3 | loss: 4.4176235Losses:  4.20465087890625 0.1958412528038025
CurrentTrain: epoch  5, batch     4 | loss: 4.4004922Losses:  4.458691596984863 0.2594180107116699
CurrentTrain: epoch  5, batch     5 | loss: 4.7181096Losses:  4.20527458190918 0.21904423832893372
CurrentTrain: epoch  5, batch     6 | loss: 4.4243188Losses:  4.468993663787842 0.20782384276390076
CurrentTrain: epoch  5, batch     7 | loss: 4.6768174Losses:  4.273801326751709 0.13405227661132812
CurrentTrain: epoch  5, batch     8 | loss: 4.4078536Losses:  4.163730621337891 0.20042118430137634
CurrentTrain: epoch  5, batch     9 | loss: 4.3641520Losses:  4.309548377990723 0.13352766633033752
CurrentTrain: epoch  5, batch    10 | loss: 4.4430761Losses:  4.395334243774414 0.1984051913022995
CurrentTrain: epoch  5, batch    11 | loss: 4.5937395Losses:  4.14251184463501 0.20187979936599731
CurrentTrain: epoch  5, batch    12 | loss: 4.3443918Losses:  4.242981910705566 0.176850825548172
CurrentTrain: epoch  5, batch    13 | loss: 4.4198327Losses:  4.20512580871582 0.24646705389022827
CurrentTrain: epoch  5, batch    14 | loss: 4.4515929Losses:  4.143319606781006 0.11930021643638611
CurrentTrain: epoch  5, batch    15 | loss: 4.2626200Losses:  4.160058498382568 0.11262275278568268
CurrentTrain: epoch  5, batch    16 | loss: 4.2726812Losses:  4.139919281005859 0.0748189240694046
CurrentTrain: epoch  5, batch    17 | loss: 4.2147384Losses:  4.167969703674316 0.10996358841657639
CurrentTrain: epoch  5, batch    18 | loss: 4.2779331Losses:  4.169558525085449 0.14007321000099182
CurrentTrain: epoch  5, batch    19 | loss: 4.3096318Losses:  4.169564247131348 0.10473361611366272
CurrentTrain: epoch  5, batch    20 | loss: 4.2742977Losses:  4.118709087371826 0.048709429800510406
CurrentTrain: epoch  5, batch    21 | loss: 4.1674185Losses:  4.183460712432861 0.13656599819660187
CurrentTrain: epoch  5, batch    22 | loss: 4.3200269Losses:  4.241054534912109 0.1651480793952942
CurrentTrain: epoch  5, batch    23 | loss: 4.4062028Losses:  4.158417701721191 0.18020623922348022
CurrentTrain: epoch  5, batch    24 | loss: 4.3386240Losses:  4.072451114654541 0.07435654103755951
CurrentTrain: epoch  5, batch    25 | loss: 4.1468077Losses:  4.173252105712891 0.08728183805942535
CurrentTrain: epoch  5, batch    26 | loss: 4.2605338Losses:  4.118714332580566 0.12256801873445511
CurrentTrain: epoch  5, batch    27 | loss: 4.2412825Losses:  4.155374050140381 0.12253864854574203
CurrentTrain: epoch  5, batch    28 | loss: 4.2779126Losses:  4.212131500244141 0.13335078954696655
CurrentTrain: epoch  5, batch    29 | loss: 4.3454823Losses:  4.293465614318848 0.12961962819099426
CurrentTrain: epoch  5, batch    30 | loss: 4.4230852Losses:  4.157252311706543 0.19443830847740173
CurrentTrain: epoch  5, batch    31 | loss: 4.3516908Losses:  4.1697916984558105 0.13242830336093903
CurrentTrain: epoch  5, batch    32 | loss: 4.3022199Losses:  4.227100372314453 0.18840715289115906
CurrentTrain: epoch  5, batch    33 | loss: 4.4155073Losses:  4.145852088928223 0.18239696323871613
CurrentTrain: epoch  5, batch    34 | loss: 4.3282490Losses:  4.04989767074585 0.12184306979179382
CurrentTrain: epoch  5, batch    35 | loss: 4.1717405Losses:  4.0830302238464355 0.11032794415950775
CurrentTrain: epoch  5, batch    36 | loss: 4.1933579Losses:  4.1448259353637695 0.13043513894081116
CurrentTrain: epoch  5, batch    37 | loss: 4.2752609Losses:  4.149083137512207 0.15965065360069275
CurrentTrain: epoch  5, batch    38 | loss: 4.3087339Losses:  4.259817600250244 0.12529975175857544
CurrentTrain: epoch  5, batch    39 | loss: 4.3851175Losses:  4.140240669250488 0.16674290597438812
CurrentTrain: epoch  5, batch    40 | loss: 4.3069835Losses:  4.04163932800293 0.07647258788347244
CurrentTrain: epoch  5, batch    41 | loss: 4.1181121Losses:  4.098513603210449 0.1284233033657074
CurrentTrain: epoch  5, batch    42 | loss: 4.2269368Losses:  4.115204811096191 0.07819929718971252
CurrentTrain: epoch  5, batch    43 | loss: 4.1934042Losses:  4.126934051513672 0.08919224888086319
CurrentTrain: epoch  5, batch    44 | loss: 4.2161264Losses:  4.170323848724365 0.1534244567155838
CurrentTrain: epoch  5, batch    45 | loss: 4.3237481Losses:  4.145402908325195 0.10896481573581696
CurrentTrain: epoch  5, batch    46 | loss: 4.2543678Losses:  4.120907783508301 0.13633066415786743
CurrentTrain: epoch  5, batch    47 | loss: 4.2572384Losses:  4.20037317276001 0.1920623779296875
CurrentTrain: epoch  5, batch    48 | loss: 4.3924356Losses:  4.111342906951904 0.08725188672542572
CurrentTrain: epoch  5, batch    49 | loss: 4.1985946Losses:  4.292393684387207 0.07394097745418549
CurrentTrain: epoch  5, batch    50 | loss: 4.3663344Losses:  4.0801849365234375 0.12910200655460358
CurrentTrain: epoch  5, batch    51 | loss: 4.2092872Losses:  4.155305862426758 0.17842158675193787
CurrentTrain: epoch  5, batch    52 | loss: 4.3337274Losses:  4.093447685241699 0.0750509649515152
CurrentTrain: epoch  5, batch    53 | loss: 4.1684985Losses:  4.116842746734619 0.158035546541214
CurrentTrain: epoch  5, batch    54 | loss: 4.2748785Losses:  4.070929527282715 0.18771187961101532
CurrentTrain: epoch  5, batch    55 | loss: 4.2586412Losses:  4.126314640045166 0.2318575084209442
CurrentTrain: epoch  5, batch    56 | loss: 4.3581719Losses:  4.155345916748047 0.10710607469081879
CurrentTrain: epoch  5, batch    57 | loss: 4.2624521Losses:  4.177871227264404 0.20019306242465973
CurrentTrain: epoch  5, batch    58 | loss: 4.3780642Losses:  4.151652812957764 0.13690125942230225
CurrentTrain: epoch  5, batch    59 | loss: 4.2885542Losses:  4.194803237915039 0.12109607458114624
CurrentTrain: epoch  5, batch    60 | loss: 4.3158994Losses:  4.085773468017578 0.14033108949661255
CurrentTrain: epoch  5, batch    61 | loss: 4.2261047Losses:  4.149317741394043 0.11244095861911774
CurrentTrain: epoch  5, batch    62 | loss: 4.2617588Losses:  4.13334846496582 0.13854414224624634
CurrentTrain: epoch  5, batch    63 | loss: 4.2718925Losses:  4.13778018951416 0.11039107292890549
CurrentTrain: epoch  5, batch    64 | loss: 4.2481713Losses:  4.027532577514648 0.09949608147144318
CurrentTrain: epoch  5, batch    65 | loss: 4.1270285Losses:  4.1265869140625 0.09587466716766357
CurrentTrain: epoch  5, batch    66 | loss: 4.2224617Losses:  4.101500511169434 0.046156249940395355
CurrentTrain: epoch  5, batch    67 | loss: 4.1476569Losses:  4.014029502868652 0.12945134937763214
CurrentTrain: epoch  5, batch    68 | loss: 4.1434808Losses:  4.232587814331055 0.08114460855722427
CurrentTrain: epoch  5, batch    69 | loss: 4.3137326Losses:  4.087402820587158 0.14760518074035645
CurrentTrain: epoch  5, batch    70 | loss: 4.2350082Losses:  4.063013076782227 0.16356012225151062
CurrentTrain: epoch  5, batch    71 | loss: 4.2265730Losses:  4.135483741760254 0.10605599731206894
CurrentTrain: epoch  5, batch    72 | loss: 4.2415400Losses:  4.114505290985107 0.08409517258405685
CurrentTrain: epoch  5, batch    73 | loss: 4.1986003Losses:  4.062438488006592 0.1263490617275238
CurrentTrain: epoch  5, batch    74 | loss: 4.1887875Losses:  4.128872871398926 0.11870744824409485
CurrentTrain: epoch  5, batch    75 | loss: 4.2475805Losses:  4.136359214782715 0.13165895640850067
CurrentTrain: epoch  5, batch    76 | loss: 4.2680182Losses:  4.081151962280273 0.09511768072843552
CurrentTrain: epoch  5, batch    77 | loss: 4.1762695Losses:  4.1680192947387695 0.1343775987625122
CurrentTrain: epoch  5, batch    78 | loss: 4.3023968Losses:  4.118157386779785 0.06490698456764221
CurrentTrain: epoch  5, batch    79 | loss: 4.1830645Losses:  4.127702713012695 0.12389398366212845
CurrentTrain: epoch  5, batch    80 | loss: 4.2515969Losses:  4.2844390869140625 0.12206093966960907
CurrentTrain: epoch  5, batch    81 | loss: 4.4064999Losses:  4.137232780456543 0.06293119490146637
CurrentTrain: epoch  5, batch    82 | loss: 4.2001638Losses:  4.143112659454346 0.16721099615097046
CurrentTrain: epoch  5, batch    83 | loss: 4.3103237Losses:  4.111442565917969 0.140354722738266
CurrentTrain: epoch  5, batch    84 | loss: 4.2517972Losses:  4.100088119506836 0.09780655056238174
CurrentTrain: epoch  5, batch    85 | loss: 4.1978946Losses:  4.064853668212891 0.10514632612466812
CurrentTrain: epoch  5, batch    86 | loss: 4.1700001Losses:  4.077090263366699 0.23281452059745789
CurrentTrain: epoch  5, batch    87 | loss: 4.3099046Losses:  4.084234237670898 0.132234126329422
CurrentTrain: epoch  5, batch    88 | loss: 4.2164683Losses:  4.004178047180176 0.15198594331741333
CurrentTrain: epoch  5, batch    89 | loss: 4.1561642Losses:  4.055377006530762 0.1521422564983368
CurrentTrain: epoch  5, batch    90 | loss: 4.2075191Losses:  4.136133193969727 0.13624925911426544
CurrentTrain: epoch  5, batch    91 | loss: 4.2723823Losses:  4.116644859313965 0.12004595249891281
CurrentTrain: epoch  5, batch    92 | loss: 4.2366910Losses:  4.10340690612793 0.1649066060781479
CurrentTrain: epoch  5, batch    93 | loss: 4.2683134Losses:  4.075498580932617 0.11061064153909683
CurrentTrain: epoch  5, batch    94 | loss: 4.1861091Losses:  4.072938919067383 0.13198870420455933
CurrentTrain: epoch  5, batch    95 | loss: 4.2049274Losses:  4.065011501312256 0.10255390405654907
CurrentTrain: epoch  5, batch    96 | loss: 4.1675653Losses:  4.060313701629639 0.11812590062618256
CurrentTrain: epoch  5, batch    97 | loss: 4.1784396Losses:  4.03164529800415 0.10431510955095291
CurrentTrain: epoch  5, batch    98 | loss: 4.1359606Losses:  4.124534606933594 0.16755808889865875
CurrentTrain: epoch  5, batch    99 | loss: 4.2920928Losses:  4.154702186584473 0.187105193734169
CurrentTrain: epoch  5, batch   100 | loss: 4.3418074Losses:  4.064776420593262 0.10497893393039703
CurrentTrain: epoch  5, batch   101 | loss: 4.1697555Losses:  3.951070785522461 0.06145128607749939
CurrentTrain: epoch  5, batch   102 | loss: 4.0125222Losses:  4.080368518829346 0.11111661791801453
CurrentTrain: epoch  5, batch   103 | loss: 4.1914849Losses:  4.036689758300781 0.1414136439561844
CurrentTrain: epoch  5, batch   104 | loss: 4.1781034Losses:  4.087592124938965 0.13069848716259003
CurrentTrain: epoch  5, batch   105 | loss: 4.2182908Losses:  4.101313591003418 0.1639300435781479
CurrentTrain: epoch  5, batch   106 | loss: 4.2652435Losses:  4.067592620849609 0.1294126808643341
CurrentTrain: epoch  5, batch   107 | loss: 4.1970053Losses:  4.091281414031982 0.15056057274341583
CurrentTrain: epoch  5, batch   108 | loss: 4.2418418Losses:  4.060351371765137 0.1180257797241211
CurrentTrain: epoch  5, batch   109 | loss: 4.1783772Losses:  4.0449442863464355 0.10389263927936554
CurrentTrain: epoch  5, batch   110 | loss: 4.1488371Losses:  4.047328948974609 0.1839233934879303
CurrentTrain: epoch  5, batch   111 | loss: 4.2312522Losses:  4.035920143127441 0.08215686678886414
CurrentTrain: epoch  5, batch   112 | loss: 4.1180768Losses:  4.082250595092773 0.20493735373020172
CurrentTrain: epoch  5, batch   113 | loss: 4.2871881Losses:  4.102847099304199 0.11572803556919098
CurrentTrain: epoch  5, batch   114 | loss: 4.2185750Losses:  4.08795166015625 0.0768033117055893
CurrentTrain: epoch  5, batch   115 | loss: 4.1647549Losses:  4.071559906005859 0.13577178120613098
CurrentTrain: epoch  5, batch   116 | loss: 4.2073317Losses:  4.059147834777832 0.12464317679405212
CurrentTrain: epoch  5, batch   117 | loss: 4.1837912Losses:  4.1026530265808105 0.16308201849460602
CurrentTrain: epoch  5, batch   118 | loss: 4.2657351Losses:  4.155444622039795 0.0660386011004448
CurrentTrain: epoch  5, batch   119 | loss: 4.2214832Losses:  4.084531784057617 0.11545659601688385
CurrentTrain: epoch  5, batch   120 | loss: 4.1999884Losses:  4.517669200897217 0.2066643089056015
CurrentTrain: epoch  5, batch   121 | loss: 4.7243333Losses:  3.973623752593994 0.08777625113725662
CurrentTrain: epoch  5, batch   122 | loss: 4.0613999Losses:  4.088884353637695 0.10519035160541534
CurrentTrain: epoch  5, batch   123 | loss: 4.1940746Losses:  4.0149054527282715 0.08188444375991821
CurrentTrain: epoch  5, batch   124 | loss: 4.0967898Losses:  4.177672386169434 0.0922631025314331
CurrentTrain: epoch  6, batch     0 | loss: 4.2699356Losses:  4.086126804351807 0.10737216472625732
CurrentTrain: epoch  6, batch     1 | loss: 4.1934991Losses:  4.043305397033691 0.06669500470161438
CurrentTrain: epoch  6, batch     2 | loss: 4.1100006Losses:  4.144245624542236 0.10620066523551941
CurrentTrain: epoch  6, batch     3 | loss: 4.2504463Losses:  4.008225917816162 0.10422651469707489
CurrentTrain: epoch  6, batch     4 | loss: 4.1124525Losses:  4.08334493637085 0.1386672705411911
CurrentTrain: epoch  6, batch     5 | loss: 4.2220120Losses:  4.087031841278076 0.09344729036092758
CurrentTrain: epoch  6, batch     6 | loss: 4.1804790Losses:  4.007915496826172 0.08214034140110016
CurrentTrain: epoch  6, batch     7 | loss: 4.0900559Losses:  4.190056800842285 0.13416223227977753
CurrentTrain: epoch  6, batch     8 | loss: 4.3242192Losses:  4.067780494689941 0.07183319330215454
CurrentTrain: epoch  6, batch     9 | loss: 4.1396136Losses:  4.050899028778076 0.19608281552791595
CurrentTrain: epoch  6, batch    10 | loss: 4.2469816Losses:  3.9933433532714844 0.09384539723396301
CurrentTrain: epoch  6, batch    11 | loss: 4.0871887Losses:  4.064425468444824 0.13413268327713013
CurrentTrain: epoch  6, batch    12 | loss: 4.1985583Losses:  4.041686058044434 0.12603788077831268
CurrentTrain: epoch  6, batch    13 | loss: 4.1677241Losses:  4.075933933258057 0.09366106241941452
CurrentTrain: epoch  6, batch    14 | loss: 4.1695948Losses:  4.05274772644043 0.06864961981773376
CurrentTrain: epoch  6, batch    15 | loss: 4.1213975Losses:  4.032783508300781 0.17824475467205048
CurrentTrain: epoch  6, batch    16 | loss: 4.2110281Losses:  4.05383825302124 0.15928581357002258
CurrentTrain: epoch  6, batch    17 | loss: 4.2131243Losses:  4.051119804382324 0.06801683455705643
CurrentTrain: epoch  6, batch    18 | loss: 4.1191368Losses:  4.088226318359375 0.09678196907043457
CurrentTrain: epoch  6, batch    19 | loss: 4.1850080Losses:  4.06399393081665 0.1137600839138031
CurrentTrain: epoch  6, batch    20 | loss: 4.1777539Losses:  4.008678913116455 0.08528810739517212
CurrentTrain: epoch  6, batch    21 | loss: 4.0939670Losses:  4.065311431884766 0.09314626455307007
CurrentTrain: epoch  6, batch    22 | loss: 4.1584578Losses:  4.13969612121582 0.08392906188964844
CurrentTrain: epoch  6, batch    23 | loss: 4.2236252Losses:  4.12817907333374 0.07599319517612457
CurrentTrain: epoch  6, batch    24 | loss: 4.2041721Losses:  4.07182502746582 0.08645389974117279
CurrentTrain: epoch  6, batch    25 | loss: 4.1582789Losses:  4.062322616577148 0.08550681173801422
CurrentTrain: epoch  6, batch    26 | loss: 4.1478295Losses:  4.153265476226807 0.10169120132923126
CurrentTrain: epoch  6, batch    27 | loss: 4.2549567Losses:  4.046931743621826 0.06217588484287262
CurrentTrain: epoch  6, batch    28 | loss: 4.1091075Losses:  4.009840488433838 0.14728881418704987
CurrentTrain: epoch  6, batch    29 | loss: 4.1571293Losses:  4.059245586395264 0.09598543494939804
CurrentTrain: epoch  6, batch    30 | loss: 4.1552310Losses:  4.070906639099121 0.15362954139709473
CurrentTrain: epoch  6, batch    31 | loss: 4.2245359Losses:  4.013998985290527 0.07513807713985443
CurrentTrain: epoch  6, batch    32 | loss: 4.0891371Losses:  4.043450832366943 0.17136666178703308
CurrentTrain: epoch  6, batch    33 | loss: 4.2148175Losses:  4.096383094787598 0.12323284149169922
CurrentTrain: epoch  6, batch    34 | loss: 4.2196159Losses:  4.11244535446167 0.14874514937400818
CurrentTrain: epoch  6, batch    35 | loss: 4.2611904Losses:  4.071969985961914 0.07774031162261963
CurrentTrain: epoch  6, batch    36 | loss: 4.1497102Losses:  4.019213676452637 0.1320914477109909
CurrentTrain: epoch  6, batch    37 | loss: 4.1513052Losses:  4.034627914428711 0.09655824303627014
CurrentTrain: epoch  6, batch    38 | loss: 4.1311860Losses:  4.0244646072387695 0.09401272982358932
CurrentTrain: epoch  6, batch    39 | loss: 4.1184773Losses:  4.054348468780518 0.13284873962402344
CurrentTrain: epoch  6, batch    40 | loss: 4.1871972Losses:  4.082476615905762 0.18222013115882874
CurrentTrain: epoch  6, batch    41 | loss: 4.2646966Losses:  4.052985668182373 0.10641054064035416
CurrentTrain: epoch  6, batch    42 | loss: 4.1593962Losses:  4.037813186645508 0.14259451627731323
CurrentTrain: epoch  6, batch    43 | loss: 4.1804075Losses:  4.011536121368408 0.10470173507928848
CurrentTrain: epoch  6, batch    44 | loss: 4.1162376Losses:  4.038549423217773 0.12944772839546204
CurrentTrain: epoch  6, batch    45 | loss: 4.1679974Losses:  4.0517730712890625 0.08142846822738647
CurrentTrain: epoch  6, batch    46 | loss: 4.1332016Losses:  4.068408012390137 0.09082513302564621
CurrentTrain: epoch  6, batch    47 | loss: 4.1592331Losses:  4.036993026733398 0.16476549208164215
CurrentTrain: epoch  6, batch    48 | loss: 4.2017584Losses:  4.06413459777832 0.1052994579076767
CurrentTrain: epoch  6, batch    49 | loss: 4.1694341Losses:  4.0427117347717285 0.13622310757637024
CurrentTrain: epoch  6, batch    50 | loss: 4.1789351Losses:  4.034958362579346 0.16082340478897095
CurrentTrain: epoch  6, batch    51 | loss: 4.1957817Losses:  4.079479694366455 0.11239815503358841
CurrentTrain: epoch  6, batch    52 | loss: 4.1918778Losses:  4.058345794677734 0.08796052634716034
CurrentTrain: epoch  6, batch    53 | loss: 4.1463065Losses:  4.141249656677246 0.11060792207717896
CurrentTrain: epoch  6, batch    54 | loss: 4.2518578Losses:  3.9885616302490234 0.07222007215023041
CurrentTrain: epoch  6, batch    55 | loss: 4.0607815Losses:  3.9948513507843018 0.18874961137771606
CurrentTrain: epoch  6, batch    56 | loss: 4.1836009Losses:  4.048611640930176 0.1148146316409111
CurrentTrain: epoch  6, batch    57 | loss: 4.1634264Losses:  4.377994537353516 0.13575485348701477
CurrentTrain: epoch  6, batch    58 | loss: 4.5137496Losses:  3.9290733337402344 0.049545906484127045
CurrentTrain: epoch  6, batch    59 | loss: 3.9786193Losses:  4.015987396240234 0.13263730704784393
CurrentTrain: epoch  6, batch    60 | loss: 4.1486249Losses:  4.090343475341797 0.13066765666007996
CurrentTrain: epoch  6, batch    61 | loss: 4.2210112Losses:  4.0739336013793945 0.08702635765075684
CurrentTrain: epoch  6, batch    62 | loss: 4.1609602Losses:  4.0221171379089355 0.08326362073421478
CurrentTrain: epoch  6, batch    63 | loss: 4.1053805Losses:  4.046802520751953 0.08144639432430267
CurrentTrain: epoch  6, batch    64 | loss: 4.1282487Losses:  4.086616516113281 0.1306551694869995
CurrentTrain: epoch  6, batch    65 | loss: 4.2172718Losses:  4.0335774421691895 0.17587000131607056
CurrentTrain: epoch  6, batch    66 | loss: 4.2094474Losses:  3.9984304904937744 0.05875614657998085
CurrentTrain: epoch  6, batch    67 | loss: 4.0571866Losses:  3.9876296520233154 0.07794178277254105
CurrentTrain: epoch  6, batch    68 | loss: 4.0655713Losses:  3.954817771911621 0.13037800788879395
CurrentTrain: epoch  6, batch    69 | loss: 4.0851955Losses:  4.067084312438965 0.10449934005737305
CurrentTrain: epoch  6, batch    70 | loss: 4.1715837Losses:  4.068362236022949 0.10733631998300552
CurrentTrain: epoch  6, batch    71 | loss: 4.1756988Losses:  4.0585551261901855 0.08991370350122452
CurrentTrain: epoch  6, batch    72 | loss: 4.1484690Losses:  4.043728828430176 0.049786388874053955
CurrentTrain: epoch  6, batch    73 | loss: 4.0935154Losses:  4.042686939239502 0.14021626114845276
CurrentTrain: epoch  6, batch    74 | loss: 4.1829033Losses:  4.041574001312256 0.12154196947813034
CurrentTrain: epoch  6, batch    75 | loss: 4.1631160Losses:  3.9723644256591797 0.17658650875091553
CurrentTrain: epoch  6, batch    76 | loss: 4.1489511Losses:  4.028031349182129 0.1751057505607605
CurrentTrain: epoch  6, batch    77 | loss: 4.2031369Losses:  4.091911315917969 0.08172044157981873
CurrentTrain: epoch  6, batch    78 | loss: 4.1736317Losses:  4.0291876792907715 0.08390369266271591
CurrentTrain: epoch  6, batch    79 | loss: 4.1130915Losses:  4.006155967712402 0.16036200523376465
CurrentTrain: epoch  6, batch    80 | loss: 4.1665182Losses:  4.041199684143066 0.09240198135375977
CurrentTrain: epoch  6, batch    81 | loss: 4.1336017Losses:  3.9388999938964844 0.06530077755451202
CurrentTrain: epoch  6, batch    82 | loss: 4.0042009Losses:  4.048510551452637 0.12100975960493088
CurrentTrain: epoch  6, batch    83 | loss: 4.1695204Losses:  4.076016426086426 0.08945155143737793
CurrentTrain: epoch  6, batch    84 | loss: 4.1654682Losses:  4.0557355880737305 0.10451540350914001
CurrentTrain: epoch  6, batch    85 | loss: 4.1602511Losses:  3.9531970024108887 0.08056305348873138
CurrentTrain: epoch  6, batch    86 | loss: 4.0337601Losses:  4.021172523498535 0.06915339827537537
CurrentTrain: epoch  6, batch    87 | loss: 4.0903258Losses:  4.015851020812988 0.09847652912139893
CurrentTrain: epoch  6, batch    88 | loss: 4.1143274Losses:  4.064455032348633 0.14576157927513123
CurrentTrain: epoch  6, batch    89 | loss: 4.2102165Losses:  3.970643997192383 0.1015956699848175
CurrentTrain: epoch  6, batch    90 | loss: 4.0722399Losses:  3.9653611183166504 0.07370162010192871
CurrentTrain: epoch  6, batch    91 | loss: 4.0390625Losses:  4.037409782409668 0.1662195324897766
CurrentTrain: epoch  6, batch    92 | loss: 4.2036295Losses:  4.004410743713379 0.14615437388420105
CurrentTrain: epoch  6, batch    93 | loss: 4.1505651Losses:  4.070652961730957 0.09311635792255402
CurrentTrain: epoch  6, batch    94 | loss: 4.1637692Losses:  4.062165260314941 0.08901652693748474
CurrentTrain: epoch  6, batch    95 | loss: 4.1511817Losses:  4.027856826782227 0.08501661568880081
CurrentTrain: epoch  6, batch    96 | loss: 4.1128736Losses:  4.0754008293151855 0.14886343479156494
CurrentTrain: epoch  6, batch    97 | loss: 4.2242641Losses:  4.003574371337891 0.11996997892856598
CurrentTrain: epoch  6, batch    98 | loss: 4.1235442Losses:  4.0190277099609375 0.1305525004863739
CurrentTrain: epoch  6, batch    99 | loss: 4.1495800Losses:  4.004856586456299 0.1331324577331543
CurrentTrain: epoch  6, batch   100 | loss: 4.1379890Losses:  4.036685943603516 0.14206336438655853
CurrentTrain: epoch  6, batch   101 | loss: 4.1787491Losses:  4.128887176513672 0.062179114669561386
CurrentTrain: epoch  6, batch   102 | loss: 4.1910663Losses:  4.011679172515869 0.0985570177435875
CurrentTrain: epoch  6, batch   103 | loss: 4.1102362Losses:  3.999553680419922 0.11477042734622955
CurrentTrain: epoch  6, batch   104 | loss: 4.1143241Losses:  4.094515323638916 0.06381244957447052
CurrentTrain: epoch  6, batch   105 | loss: 4.1583276Losses:  4.049223899841309 0.15711596608161926
CurrentTrain: epoch  6, batch   106 | loss: 4.2063398Losses:  4.10111141204834 0.10842469334602356
CurrentTrain: epoch  6, batch   107 | loss: 4.2095361Losses:  4.053709030151367 0.055335935205221176
CurrentTrain: epoch  6, batch   108 | loss: 4.1090450Losses:  4.081967353820801 0.06972573697566986
CurrentTrain: epoch  6, batch   109 | loss: 4.1516929Losses:  4.000260353088379 0.07478966563940048
CurrentTrain: epoch  6, batch   110 | loss: 4.0750499Losses:  4.046614646911621 0.08577820658683777
CurrentTrain: epoch  6, batch   111 | loss: 4.1323929Losses:  4.023080348968506 0.06945884227752686
CurrentTrain: epoch  6, batch   112 | loss: 4.0925393Losses:  4.095412254333496 0.10264753550291061
CurrentTrain: epoch  6, batch   113 | loss: 4.1980596Losses:  4.022695541381836 0.10677686333656311
CurrentTrain: epoch  6, batch   114 | loss: 4.1294723Losses:  3.9805846214294434 0.17210105061531067
CurrentTrain: epoch  6, batch   115 | loss: 4.1526856Losses:  4.036864757537842 0.13046233355998993
CurrentTrain: epoch  6, batch   116 | loss: 4.1673269Losses:  4.0328874588012695 0.09983488917350769
CurrentTrain: epoch  6, batch   117 | loss: 4.1327224Losses:  4.0204057693481445 0.08016417920589447
CurrentTrain: epoch  6, batch   118 | loss: 4.1005697Losses:  3.9885995388031006 0.05631660297513008
CurrentTrain: epoch  6, batch   119 | loss: 4.0449162Losses:  4.019063949584961 0.11488758772611618
CurrentTrain: epoch  6, batch   120 | loss: 4.1339517Losses:  4.031421661376953 0.08480630815029144
CurrentTrain: epoch  6, batch   121 | loss: 4.1162281Losses:  4.158307075500488 0.0831868126988411
CurrentTrain: epoch  6, batch   122 | loss: 4.2414937Losses:  3.9755842685699463 0.08417303115129471
CurrentTrain: epoch  6, batch   123 | loss: 4.0597572Losses:  4.05919075012207 0.08838419616222382
CurrentTrain: epoch  6, batch   124 | loss: 4.1475749Losses:  4.004147052764893 0.10861577093601227
CurrentTrain: epoch  7, batch     0 | loss: 4.1127629Losses:  4.005906581878662 0.09326762706041336
CurrentTrain: epoch  7, batch     1 | loss: 4.0991740Losses:  4.0105180740356445 0.08918297290802002
CurrentTrain: epoch  7, batch     2 | loss: 4.0997009Losses:  4.060064315795898 0.07080524414777756
CurrentTrain: epoch  7, batch     3 | loss: 4.1308694Losses:  4.023921012878418 0.07021147012710571
CurrentTrain: epoch  7, batch     4 | loss: 4.0941324Losses:  3.9593849182128906 0.05775608867406845
CurrentTrain: epoch  7, batch     5 | loss: 4.0171409Losses:  4.05540657043457 0.08089956641197205
CurrentTrain: epoch  7, batch     6 | loss: 4.1363063Losses:  3.942995309829712 0.06508840620517731
CurrentTrain: epoch  7, batch     7 | loss: 4.0080838Losses:  3.9810562133789062 0.11593443900346756
CurrentTrain: epoch  7, batch     8 | loss: 4.0969906Losses:  4.012895107269287 0.12360809743404388
CurrentTrain: epoch  7, batch     9 | loss: 4.1365032Losses:  3.989316940307617 0.0358114019036293
CurrentTrain: epoch  7, batch    10 | loss: 4.0251284Losses:  4.028176307678223 0.06836479902267456
CurrentTrain: epoch  7, batch    11 | loss: 4.0965409Losses:  4.0301513671875 0.12277917563915253
CurrentTrain: epoch  7, batch    12 | loss: 4.1529307Losses:  3.980311870574951 0.10971320420503616
CurrentTrain: epoch  7, batch    13 | loss: 4.0900249Losses:  4.007241249084473 0.07989072799682617
CurrentTrain: epoch  7, batch    14 | loss: 4.0871320Losses:  4.043999195098877 0.04754618555307388
CurrentTrain: epoch  7, batch    15 | loss: 4.0915456Losses:  4.056147575378418 0.08356955647468567
CurrentTrain: epoch  7, batch    16 | loss: 4.1397171Losses:  3.975203037261963 0.06418170034885406
CurrentTrain: epoch  7, batch    17 | loss: 4.0393848Losses:  4.097606658935547 0.06241539865732193
CurrentTrain: epoch  7, batch    18 | loss: 4.1600223Losses:  4.008296012878418 0.12849441170692444
CurrentTrain: epoch  7, batch    19 | loss: 4.1367903Losses:  4.001923084259033 0.1463092863559723
CurrentTrain: epoch  7, batch    20 | loss: 4.1482325Losses:  4.050501823425293 0.11835846304893494
CurrentTrain: epoch  7, batch    21 | loss: 4.1688604Losses:  4.029775619506836 0.09005843102931976
CurrentTrain: epoch  7, batch    22 | loss: 4.1198339Losses:  3.976839542388916 0.1147274374961853
CurrentTrain: epoch  7, batch    23 | loss: 4.0915670Losses:  3.977647066116333 0.07437296211719513
CurrentTrain: epoch  7, batch    24 | loss: 4.0520201Losses:  4.048994541168213 0.08905269205570221
CurrentTrain: epoch  7, batch    25 | loss: 4.1380472Losses:  4.008939743041992 0.030861448496580124
CurrentTrain: epoch  7, batch    26 | loss: 4.0398011Losses:  4.0276875495910645 0.11427456140518188
CurrentTrain: epoch  7, batch    27 | loss: 4.1419621Losses:  4.011002063751221 0.1282675862312317
CurrentTrain: epoch  7, batch    28 | loss: 4.1392698Losses:  3.987326145172119 0.083876833319664
CurrentTrain: epoch  7, batch    29 | loss: 4.0712028Losses:  4.000488758087158 0.10032524913549423
CurrentTrain: epoch  7, batch    30 | loss: 4.1008139Losses:  3.9623022079467773 0.05100949481129646
CurrentTrain: epoch  7, batch    31 | loss: 4.0133119Losses:  4.047273635864258 0.14311067759990692
CurrentTrain: epoch  7, batch    32 | loss: 4.1903844Losses:  4.011957168579102 0.1480104923248291
CurrentTrain: epoch  7, batch    33 | loss: 4.1599674Losses:  4.016269207000732 0.057324476540088654
CurrentTrain: epoch  7, batch    34 | loss: 4.0735936Losses:  4.033349990844727 0.06282706558704376
CurrentTrain: epoch  7, batch    35 | loss: 4.0961771Losses:  4.055826187133789 0.12518160045146942
CurrentTrain: epoch  7, batch    36 | loss: 4.1810079Losses:  4.020098686218262 0.08174454420804977
CurrentTrain: epoch  7, batch    37 | loss: 4.1018434Losses:  4.0256147384643555 0.11834360659122467
CurrentTrain: epoch  7, batch    38 | loss: 4.1439586Losses:  3.972872734069824 0.08017545938491821
CurrentTrain: epoch  7, batch    39 | loss: 4.0530481Losses:  4.022040843963623 0.08043172955513
CurrentTrain: epoch  7, batch    40 | loss: 4.1024728Losses:  3.9832916259765625 0.11041344702243805
CurrentTrain: epoch  7, batch    41 | loss: 4.0937052Losses:  4.037262916564941 0.09759433567523956
CurrentTrain: epoch  7, batch    42 | loss: 4.1348572Losses:  4.002496242523193 0.05759042501449585
CurrentTrain: epoch  7, batch    43 | loss: 4.0600867Losses:  4.045020580291748 0.10872739553451538
CurrentTrain: epoch  7, batch    44 | loss: 4.1537480Losses:  3.9959607124328613 0.07272335886955261
CurrentTrain: epoch  7, batch    45 | loss: 4.0686841Losses:  4.089621543884277 0.07347273826599121
CurrentTrain: epoch  7, batch    46 | loss: 4.1630945Losses:  4.038247108459473 0.10153155028820038
CurrentTrain: epoch  7, batch    47 | loss: 4.1397786Losses:  4.043378829956055 0.05029010772705078
CurrentTrain: epoch  7, batch    48 | loss: 4.0936689Losses:  3.9700608253479004 0.089053675532341
CurrentTrain: epoch  7, batch    49 | loss: 4.0591145Losses:  3.9588303565979004 0.10738931596279144
CurrentTrain: epoch  7, batch    50 | loss: 4.0662198Losses:  3.9959921836853027 0.15013280510902405
CurrentTrain: epoch  7, batch    51 | loss: 4.1461248Losses:  4.029163837432861 0.028477918356657028
CurrentTrain: epoch  7, batch    52 | loss: 4.0576420Losses:  3.940077304840088 0.04008956998586655
CurrentTrain: epoch  7, batch    53 | loss: 3.9801669Losses:  3.992645263671875 0.10536863654851913
CurrentTrain: epoch  7, batch    54 | loss: 4.0980139Losses:  4.052555084228516 0.04207848384976387
CurrentTrain: epoch  7, batch    55 | loss: 4.0946336Losses:  3.996771812438965 0.11096742749214172
CurrentTrain: epoch  7, batch    56 | loss: 4.1077394Losses:  3.9782919883728027 0.09239022433757782
CurrentTrain: epoch  7, batch    57 | loss: 4.0706820Losses:  3.9873337745666504 0.15935441851615906
CurrentTrain: epoch  7, batch    58 | loss: 4.1466880Losses:  4.067584037780762 0.11534488946199417
CurrentTrain: epoch  7, batch    59 | loss: 4.1829290Losses:  4.021806716918945 0.06567618250846863
CurrentTrain: epoch  7, batch    60 | loss: 4.0874829Losses:  4.050712585449219 0.11219442635774612
CurrentTrain: epoch  7, batch    61 | loss: 4.1629071Losses:  4.001034736633301 0.09627579152584076
CurrentTrain: epoch  7, batch    62 | loss: 4.0973105Losses:  4.010737419128418 0.04022446274757385
CurrentTrain: epoch  7, batch    63 | loss: 4.0509620Losses:  4.028608322143555 0.120364710688591
CurrentTrain: epoch  7, batch    64 | loss: 4.1489730Losses:  4.012013912200928 0.12092704325914383
CurrentTrain: epoch  7, batch    65 | loss: 4.1329408Losses:  3.9315128326416016 0.06058412790298462
CurrentTrain: epoch  7, batch    66 | loss: 3.9920969Losses:  3.9929399490356445 0.09737714380025864
CurrentTrain: epoch  7, batch    67 | loss: 4.0903172Losses:  4.0390729904174805 0.07009127736091614
CurrentTrain: epoch  7, batch    68 | loss: 4.1091642Losses:  4.0120344161987305 0.09841573238372803
CurrentTrain: epoch  7, batch    69 | loss: 4.1104503Losses:  3.967320680618286 0.08901330828666687
CurrentTrain: epoch  7, batch    70 | loss: 4.0563340Losses:  4.047447681427002 0.10438061505556107
CurrentTrain: epoch  7, batch    71 | loss: 4.1518283Losses:  4.026893615722656 0.08345680683851242
CurrentTrain: epoch  7, batch    72 | loss: 4.1103506Losses:  4.046117782592773 0.10240301489830017
CurrentTrain: epoch  7, batch    73 | loss: 4.1485209Losses:  3.9486396312713623 0.08538250625133514
CurrentTrain: epoch  7, batch    74 | loss: 4.0340223Losses:  4.023222923278809 0.08878082036972046
CurrentTrain: epoch  7, batch    75 | loss: 4.1120038Losses:  4.0363030433654785 0.0933351218700409
CurrentTrain: epoch  7, batch    76 | loss: 4.1296382Losses:  4.009721755981445 0.0821584165096283
CurrentTrain: epoch  7, batch    77 | loss: 4.0918803Losses:  3.953040361404419 0.10403518378734589
CurrentTrain: epoch  7, batch    78 | loss: 4.0570755Losses:  4.0210700035095215 0.09312392771244049
CurrentTrain: epoch  7, batch    79 | loss: 4.1141939Losses:  3.997771978378296 0.0730733722448349
CurrentTrain: epoch  7, batch    80 | loss: 4.0708451Losses:  4.5083723068237305 0.20699331164360046
CurrentTrain: epoch  7, batch    81 | loss: 4.7153654Losses:  3.940547466278076 0.07649657130241394
CurrentTrain: epoch  7, batch    82 | loss: 4.0170441Losses:  3.9393868446350098 0.07273644208908081
CurrentTrain: epoch  7, batch    83 | loss: 4.0121231Losses:  3.974689483642578 0.10504376888275146
CurrentTrain: epoch  7, batch    84 | loss: 4.0797334Losses:  4.030672073364258 0.14518509805202484
CurrentTrain: epoch  7, batch    85 | loss: 4.1758571Losses:  3.9817755222320557 0.09139181673526764
CurrentTrain: epoch  7, batch    86 | loss: 4.0731673Losses:  3.9993810653686523 0.0699690580368042
CurrentTrain: epoch  7, batch    87 | loss: 4.0693502Losses:  4.023123741149902 0.054827265441417694
CurrentTrain: epoch  7, batch    88 | loss: 4.0779510Losses:  3.993875503540039 0.09358153492212296
CurrentTrain: epoch  7, batch    89 | loss: 4.0874572Losses:  3.989866018295288 0.060773495584726334
CurrentTrain: epoch  7, batch    90 | loss: 4.0506396Losses:  4.028013706207275 0.10219874233007431
CurrentTrain: epoch  7, batch    91 | loss: 4.1302123Losses:  4.02557373046875 0.13528293371200562
CurrentTrain: epoch  7, batch    92 | loss: 4.1608567Losses:  4.012207984924316 0.05521390587091446
CurrentTrain: epoch  7, batch    93 | loss: 4.0674219Losses:  3.976654052734375 0.07319174706935883
CurrentTrain: epoch  7, batch    94 | loss: 4.0498457Losses:  4.073625087738037 0.07055723667144775
CurrentTrain: epoch  7, batch    95 | loss: 4.1441822Losses:  3.990943431854248 0.10589803010225296
CurrentTrain: epoch  7, batch    96 | loss: 4.0968413Losses:  4.035676002502441 0.1025480329990387
CurrentTrain: epoch  7, batch    97 | loss: 4.1382241Losses:  3.989962577819824 0.10251704603433609
CurrentTrain: epoch  7, batch    98 | loss: 4.0924797Losses:  4.027706623077393 0.13310551643371582
CurrentTrain: epoch  7, batch    99 | loss: 4.1608124Losses:  3.9956507682800293 0.09630466252565384
CurrentTrain: epoch  7, batch   100 | loss: 4.0919557Losses:  4.023038864135742 0.05784459784626961
CurrentTrain: epoch  7, batch   101 | loss: 4.0808835Losses:  4.0824995040893555 0.11289283633232117
CurrentTrain: epoch  7, batch   102 | loss: 4.1953921Losses:  3.9962825775146484 0.0899033322930336
CurrentTrain: epoch  7, batch   103 | loss: 4.0861859Losses:  4.0335235595703125 0.07230079174041748
CurrentTrain: epoch  7, batch   104 | loss: 4.1058245Losses:  4.131007194519043 0.047844722867012024
CurrentTrain: epoch  7, batch   105 | loss: 4.1788521Losses:  4.015328884124756 0.08158981055021286
CurrentTrain: epoch  7, batch   106 | loss: 4.0969186Losses:  3.9916887283325195 0.10482330620288849
CurrentTrain: epoch  7, batch   107 | loss: 4.0965118Losses:  4.023386001586914 0.14605450630187988
CurrentTrain: epoch  7, batch   108 | loss: 4.1694403Losses:  4.041731834411621 0.0990356057882309
CurrentTrain: epoch  7, batch   109 | loss: 4.1407676Losses:  3.9855992794036865 0.07950162887573242
CurrentTrain: epoch  7, batch   110 | loss: 4.0651007Losses:  4.037469863891602 0.13789516687393188
CurrentTrain: epoch  7, batch   111 | loss: 4.1753650Losses:  4.019423961639404 0.08361631631851196
CurrentTrain: epoch  7, batch   112 | loss: 4.1030402Losses:  4.031949996948242 0.06543591618537903
CurrentTrain: epoch  7, batch   113 | loss: 4.0973859Losses:  4.019449710845947 0.14808589220046997
CurrentTrain: epoch  7, batch   114 | loss: 4.1675358Losses:  4.008052349090576 0.10304126143455505
CurrentTrain: epoch  7, batch   115 | loss: 4.1110935Losses:  4.027684211730957 0.1282864212989807
CurrentTrain: epoch  7, batch   116 | loss: 4.1559706Losses:  4.0414652824401855 0.106730617582798
CurrentTrain: epoch  7, batch   117 | loss: 4.1481957Losses:  3.983898639678955 0.08655276149511337
CurrentTrain: epoch  7, batch   118 | loss: 4.0704513Losses:  4.045672416687012 0.051602207124233246
CurrentTrain: epoch  7, batch   119 | loss: 4.0972748Losses:  4.0215864181518555 0.1129910796880722
CurrentTrain: epoch  7, batch   120 | loss: 4.1345773Losses:  3.942077875137329 0.10895764827728271
CurrentTrain: epoch  7, batch   121 | loss: 4.0510354Losses:  4.015435218811035 0.06630605459213257
CurrentTrain: epoch  7, batch   122 | loss: 4.0817413Losses:  4.024234294891357 0.10828182101249695
CurrentTrain: epoch  7, batch   123 | loss: 4.1325159Losses:  4.023116111755371 0.09568871557712555
CurrentTrain: epoch  7, batch   124 | loss: 4.1188049Losses:  4.032571315765381 0.12342924624681473
CurrentTrain: epoch  8, batch     0 | loss: 4.1560006Losses:  4.02373743057251 0.10615865886211395
CurrentTrain: epoch  8, batch     1 | loss: 4.1298962Losses:  3.9904143810272217 0.14119328558444977
CurrentTrain: epoch  8, batch     2 | loss: 4.1316075Losses:  3.9986822605133057 0.05530901998281479
CurrentTrain: epoch  8, batch     3 | loss: 4.0539913Losses:  4.0277628898620605 0.10780438780784607
CurrentTrain: epoch  8, batch     4 | loss: 4.1355672Losses:  4.025417804718018 0.10624032467603683
CurrentTrain: epoch  8, batch     5 | loss: 4.1316581Losses:  4.023505210876465 0.10636722296476364
CurrentTrain: epoch  8, batch     6 | loss: 4.1298723Losses:  4.101980686187744 0.13390286266803741
CurrentTrain: epoch  8, batch     7 | loss: 4.2358837Losses:  4.0216755867004395 0.05074121057987213
CurrentTrain: epoch  8, batch     8 | loss: 4.0724168Losses:  4.036101818084717 0.10400132834911346
CurrentTrain: epoch  8, batch     9 | loss: 4.1401033Losses:  3.9795234203338623 0.15055610239505768
CurrentTrain: epoch  8, batch    10 | loss: 4.1300797Losses:  3.9799482822418213 0.07767334580421448
CurrentTrain: epoch  8, batch    11 | loss: 4.0576215Losses:  4.035580635070801 0.04064713418483734
CurrentTrain: epoch  8, batch    12 | loss: 4.0762277Losses:  4.0332159996032715 0.0964694619178772
CurrentTrain: epoch  8, batch    13 | loss: 4.1296854Losses:  4.014694690704346 0.08741322159767151
CurrentTrain: epoch  8, batch    14 | loss: 4.1021080Losses:  3.982678174972534 0.09780886769294739
CurrentTrain: epoch  8, batch    15 | loss: 4.0804873Losses:  3.994049072265625 0.15270504355430603
CurrentTrain: epoch  8, batch    16 | loss: 4.1467543Losses:  3.926384449005127 0.1044822409749031
CurrentTrain: epoch  8, batch    17 | loss: 4.0308666Losses:  3.9807021617889404 0.11934371292591095
CurrentTrain: epoch  8, batch    18 | loss: 4.1000457Losses:  4.041071891784668 0.06498271971940994
CurrentTrain: epoch  8, batch    19 | loss: 4.1060548Losses:  3.9701712131500244 0.09366606175899506
CurrentTrain: epoch  8, batch    20 | loss: 4.0638371Losses:  3.9961674213409424 0.09126138687133789
CurrentTrain: epoch  8, batch    21 | loss: 4.0874290Losses:  4.010936737060547 0.07579060643911362
CurrentTrain: epoch  8, batch    22 | loss: 4.0867271Losses:  3.9904966354370117 0.15279440581798553
CurrentTrain: epoch  8, batch    23 | loss: 4.1432910Losses:  3.9799413681030273 0.06706152856349945
CurrentTrain: epoch  8, batch    24 | loss: 4.0470028Losses:  4.089338302612305 0.05166708678007126
CurrentTrain: epoch  8, batch    25 | loss: 4.1410055Losses:  3.9441537857055664 0.09785483777523041
CurrentTrain: epoch  8, batch    26 | loss: 4.0420084Losses:  4.00117301940918 0.09738017618656158
CurrentTrain: epoch  8, batch    27 | loss: 4.0985532Losses:  4.021271705627441 0.047915272414684296
CurrentTrain: epoch  8, batch    28 | loss: 4.0691872Losses:  4.039026260375977 0.135178804397583
CurrentTrain: epoch  8, batch    29 | loss: 4.1742048Losses:  3.939053773880005 0.039904434233903885
CurrentTrain: epoch  8, batch    30 | loss: 3.9789581Losses:  4.0609822273254395 0.10646750032901764
CurrentTrain: epoch  8, batch    31 | loss: 4.1674500Losses:  4.008322715759277 0.09577619284391403
CurrentTrain: epoch  8, batch    32 | loss: 4.1040988Losses:  4.048787593841553 0.07615868747234344
CurrentTrain: epoch  8, batch    33 | loss: 4.1249461Losses:  3.9926652908325195 0.04149799421429634
CurrentTrain: epoch  8, batch    34 | loss: 4.0341635Losses:  3.9500651359558105 0.072446309030056
CurrentTrain: epoch  8, batch    35 | loss: 4.0225115Losses:  3.9358670711517334 0.034046873450279236
CurrentTrain: epoch  8, batch    36 | loss: 3.9699140Losses:  3.988680839538574 0.08706072717905045
CurrentTrain: epoch  8, batch    37 | loss: 4.0757418Losses:  3.9632487297058105 0.09168480336666107
CurrentTrain: epoch  8, batch    38 | loss: 4.0549335Losses:  4.028776168823242 0.09732624143362045
CurrentTrain: epoch  8, batch    39 | loss: 4.1261024Losses:  4.047599792480469 0.1113312616944313
CurrentTrain: epoch  8, batch    40 | loss: 4.1589313Losses:  4.000842571258545 0.09326265007257462
CurrentTrain: epoch  8, batch    41 | loss: 4.0941052Losses:  3.99832820892334 0.04917540401220322
CurrentTrain: epoch  8, batch    42 | loss: 4.0475035Losses:  4.025710105895996 0.08239905536174774
CurrentTrain: epoch  8, batch    43 | loss: 4.1081090Losses:  3.9834420680999756 0.07106373459100723
CurrentTrain: epoch  8, batch    44 | loss: 4.0545058Losses:  3.9695801734924316 0.059465859085321426
CurrentTrain: epoch  8, batch    45 | loss: 4.0290461Losses:  3.949648380279541 0.08753141760826111
CurrentTrain: epoch  8, batch    46 | loss: 4.0371799Losses:  3.9911293983459473 0.12751367688179016
CurrentTrain: epoch  8, batch    47 | loss: 4.1186433Losses:  3.9895782470703125 0.07733184099197388
CurrentTrain: epoch  8, batch    48 | loss: 4.0669103Losses:  3.974891185760498 0.06905613839626312
CurrentTrain: epoch  8, batch    49 | loss: 4.0439472Losses:  3.988778829574585 0.10056474804878235
CurrentTrain: epoch  8, batch    50 | loss: 4.0893435Losses:  3.9901585578918457 0.0807654857635498
CurrentTrain: epoch  8, batch    51 | loss: 4.0709238Losses:  3.979462146759033 0.10766489058732986
CurrentTrain: epoch  8, batch    52 | loss: 4.0871272Losses:  3.9333958625793457 0.09034231305122375
CurrentTrain: epoch  8, batch    53 | loss: 4.0237384Losses:  3.96342396736145 0.13784900307655334
CurrentTrain: epoch  8, batch    54 | loss: 4.1012731Losses:  3.991230010986328 0.09711581468582153
CurrentTrain: epoch  8, batch    55 | loss: 4.0883460Losses:  3.968384265899658 0.037074100226163864
CurrentTrain: epoch  8, batch    56 | loss: 4.0054584Losses:  3.9967713356018066 0.11745446920394897
CurrentTrain: epoch  8, batch    57 | loss: 4.1142259Losses:  3.973602771759033 0.1411292850971222
CurrentTrain: epoch  8, batch    58 | loss: 4.1147323Losses:  3.9647321701049805 0.10289643704891205
CurrentTrain: epoch  8, batch    59 | loss: 4.0676284Losses:  4.010746955871582 0.09582224488258362
CurrentTrain: epoch  8, batch    60 | loss: 4.1065693Losses:  3.9781277179718018 0.11205101013183594
CurrentTrain: epoch  8, batch    61 | loss: 4.0901785Losses:  3.9882001876831055 0.07775995135307312
CurrentTrain: epoch  8, batch    62 | loss: 4.0659599Losses:  4.008951663970947 0.10875242203474045
CurrentTrain: epoch  8, batch    63 | loss: 4.1177039Losses:  3.9710514545440674 0.11083067208528519
CurrentTrain: epoch  8, batch    64 | loss: 4.0818820Losses:  3.96610689163208 0.10583648085594177
CurrentTrain: epoch  8, batch    65 | loss: 4.0719433Losses:  3.973816394805908 0.0978614091873169
CurrentTrain: epoch  8, batch    66 | loss: 4.0716777Losses:  3.9555740356445312 0.08681916445493698
CurrentTrain: epoch  8, batch    67 | loss: 4.0423932Losses:  3.9855384826660156 0.1381760537624359
CurrentTrain: epoch  8, batch    68 | loss: 4.1237144Losses:  3.9645912647247314 0.08786620944738388
CurrentTrain: epoch  8, batch    69 | loss: 4.0524573Losses:  3.946854829788208 0.09610052406787872
CurrentTrain: epoch  8, batch    70 | loss: 4.0429554Losses:  4.00988245010376 0.045355066657066345
CurrentTrain: epoch  8, batch    71 | loss: 4.0552373Losses:  3.9450490474700928 0.06086074560880661
CurrentTrain: epoch  8, batch    72 | loss: 4.0059099Losses:  3.978705406188965 0.03254100680351257
CurrentTrain: epoch  8, batch    73 | loss: 4.0112462Losses:  3.9958157539367676 0.08202522248029709
CurrentTrain: epoch  8, batch    74 | loss: 4.0778408Losses:  3.9783389568328857 0.118734210729599
CurrentTrain: epoch  8, batch    75 | loss: 4.0970731Losses:  3.9503841400146484 0.13026422262191772
CurrentTrain: epoch  8, batch    76 | loss: 4.0806484Losses:  4.059863567352295 0.041792772710323334
CurrentTrain: epoch  8, batch    77 | loss: 4.1016564Losses:  3.971980094909668 0.03762898966670036
CurrentTrain: epoch  8, batch    78 | loss: 4.0096092Losses:  3.973771810531616 0.08801139146089554
CurrentTrain: epoch  8, batch    79 | loss: 4.0617833Losses:  3.9700560569763184 0.10046125948429108
CurrentTrain: epoch  8, batch    80 | loss: 4.0705175Losses:  3.9519832134246826 0.09556495398283005
CurrentTrain: epoch  8, batch    81 | loss: 4.0475483Losses:  4.013306617736816 0.050757016986608505
CurrentTrain: epoch  8, batch    82 | loss: 4.0640635Losses:  4.102574825286865 0.07767429202795029
CurrentTrain: epoch  8, batch    83 | loss: 4.1802492Losses:  3.961897373199463 0.03830474615097046
CurrentTrain: epoch  8, batch    84 | loss: 4.0002022Losses:  4.0531463623046875 0.0478181354701519
CurrentTrain: epoch  8, batch    85 | loss: 4.1009645Losses:  3.999213933944702 0.044550634920597076
CurrentTrain: epoch  8, batch    86 | loss: 4.0437646Losses:  3.9565277099609375 0.09875310957431793
CurrentTrain: epoch  8, batch    87 | loss: 4.0552807Losses:  3.9594950675964355 0.08328339457511902
CurrentTrain: epoch  8, batch    88 | loss: 4.0427785Losses:  3.9771013259887695 0.10293847322463989
CurrentTrain: epoch  8, batch    89 | loss: 4.0800400Losses:  3.988621234893799 0.05563473701477051
CurrentTrain: epoch  8, batch    90 | loss: 4.0442562Losses:  4.020868301391602 0.09657718241214752
CurrentTrain: epoch  8, batch    91 | loss: 4.1174455Losses:  3.9802443981170654 0.0692884624004364
CurrentTrain: epoch  8, batch    92 | loss: 4.0495329Losses:  4.021553039550781 0.0705571174621582
CurrentTrain: epoch  8, batch    93 | loss: 4.0921102Losses:  4.011297702789307 0.08553717285394669
CurrentTrain: epoch  8, batch    94 | loss: 4.0968347Losses:  4.039287567138672 0.06624351441860199
CurrentTrain: epoch  8, batch    95 | loss: 4.1055312Losses:  3.991727113723755 0.07207587361335754
CurrentTrain: epoch  8, batch    96 | loss: 4.0638032Losses:  4.013154983520508 0.10445671528577805
CurrentTrain: epoch  8, batch    97 | loss: 4.1176119Losses:  3.974365711212158 0.047477878630161285
CurrentTrain: epoch  8, batch    98 | loss: 4.0218434Losses:  3.9567880630493164 0.05692356452345848
CurrentTrain: epoch  8, batch    99 | loss: 4.0137115Losses:  3.978025436401367 0.062279440462589264
CurrentTrain: epoch  8, batch   100 | loss: 4.0403047Losses:  4.009882926940918 0.09577422589063644
CurrentTrain: epoch  8, batch   101 | loss: 4.1056571Losses:  3.922729015350342 0.07474280893802643
CurrentTrain: epoch  8, batch   102 | loss: 3.9974718Losses:  3.9748034477233887 0.056290823966264725
CurrentTrain: epoch  8, batch   103 | loss: 4.0310941Losses:  4.004129886627197 0.12627826631069183
CurrentTrain: epoch  8, batch   104 | loss: 4.1304083Losses:  3.9787023067474365 0.09545667469501495
CurrentTrain: epoch  8, batch   105 | loss: 4.0741591Losses:  3.984792709350586 0.08669206500053406
CurrentTrain: epoch  8, batch   106 | loss: 4.0714846Losses:  3.9749693870544434 0.037529319524765015
CurrentTrain: epoch  8, batch   107 | loss: 4.0124989Losses:  4.01112174987793 0.10901886969804764
CurrentTrain: epoch  8, batch   108 | loss: 4.1201406Losses:  3.9994168281555176 0.06957067549228668
CurrentTrain: epoch  8, batch   109 | loss: 4.0689874Losses:  3.922327995300293 0.08691811561584473
CurrentTrain: epoch  8, batch   110 | loss: 4.0092459Losses:  3.936969041824341 0.07914784550666809
CurrentTrain: epoch  8, batch   111 | loss: 4.0161171Losses:  3.9402410984039307 0.1067734882235527
CurrentTrain: epoch  8, batch   112 | loss: 4.0470147Losses:  3.9959006309509277 0.036294013261795044
CurrentTrain: epoch  8, batch   113 | loss: 4.0321946Losses:  3.966520071029663 0.08487454056739807
CurrentTrain: epoch  8, batch   114 | loss: 4.0513945Losses:  4.030447483062744 0.08997112512588501
CurrentTrain: epoch  8, batch   115 | loss: 4.1204185Losses:  3.9101037979125977 0.07394975423812866
CurrentTrain: epoch  8, batch   116 | loss: 3.9840536Losses:  3.9788882732391357 0.0797978937625885
CurrentTrain: epoch  8, batch   117 | loss: 4.0586863Losses:  4.040011405944824 0.05665726959705353
CurrentTrain: epoch  8, batch   118 | loss: 4.0966687Losses:  4.026245594024658 0.069451242685318
CurrentTrain: epoch  8, batch   119 | loss: 4.0956969Losses:  4.001681804656982 0.07692797482013702
CurrentTrain: epoch  8, batch   120 | loss: 4.0786099Losses:  4.040207862854004 0.08867868036031723
CurrentTrain: epoch  8, batch   121 | loss: 4.1288867Losses:  3.9896798133850098 0.09986497461795807
CurrentTrain: epoch  8, batch   122 | loss: 4.0895448Losses:  3.9849095344543457 0.10970237851142883
CurrentTrain: epoch  8, batch   123 | loss: 4.0946121Losses:  3.9591526985168457 0.07560928165912628
CurrentTrain: epoch  8, batch   124 | loss: 4.0347619Losses:  3.943765163421631 0.07047028839588165
CurrentTrain: epoch  9, batch     0 | loss: 4.0142355Losses:  3.9994704723358154 0.031532928347587585
CurrentTrain: epoch  9, batch     1 | loss: 4.0310035Losses:  3.9628002643585205 0.07334966212511063
CurrentTrain: epoch  9, batch     2 | loss: 4.0361500Losses:  3.9993319511413574 0.12452315539121628
CurrentTrain: epoch  9, batch     3 | loss: 4.1238551Losses:  4.042731285095215 0.09767160564661026
CurrentTrain: epoch  9, batch     4 | loss: 4.1404028Losses:  4.037196159362793 0.08666105568408966
CurrentTrain: epoch  9, batch     5 | loss: 4.1238570Losses:  3.9986770153045654 0.08628284186124802
CurrentTrain: epoch  9, batch     6 | loss: 4.0849600Losses:  4.0459675788879395 0.060355670750141144
CurrentTrain: epoch  9, batch     7 | loss: 4.1063232Losses:  3.953223466873169 0.07045657187700272
CurrentTrain: epoch  9, batch     8 | loss: 4.0236802Losses:  4.033797264099121 0.06254228949546814
CurrentTrain: epoch  9, batch     9 | loss: 4.0963397Losses:  3.948523759841919 0.06148888170719147
CurrentTrain: epoch  9, batch    10 | loss: 4.0100126Losses:  3.99837589263916 0.06836344301700592
CurrentTrain: epoch  9, batch    11 | loss: 4.0667396Losses:  3.9101881980895996 0.05161742493510246
CurrentTrain: epoch  9, batch    12 | loss: 3.9618056Losses:  4.004552841186523 0.06833675503730774
CurrentTrain: epoch  9, batch    13 | loss: 4.0728898Losses:  4.00289249420166 0.15442556142807007
CurrentTrain: epoch  9, batch    14 | loss: 4.1573181Losses:  3.9448866844177246 0.06330752372741699
CurrentTrain: epoch  9, batch    15 | loss: 4.0081940Losses:  3.99586820602417 0.10897496342658997
CurrentTrain: epoch  9, batch    16 | loss: 4.1048431Losses:  4.014401912689209 0.06417754292488098
CurrentTrain: epoch  9, batch    17 | loss: 4.0785794Losses:  4.016326427459717 0.05181014537811279
CurrentTrain: epoch  9, batch    18 | loss: 4.0681367Losses:  3.9824154376983643 0.1185598075389862
CurrentTrain: epoch  9, batch    19 | loss: 4.1009750Losses:  4.034715175628662 0.0788833275437355
CurrentTrain: epoch  9, batch    20 | loss: 4.1135983Losses:  4.0240888595581055 0.09127981215715408
CurrentTrain: epoch  9, batch    21 | loss: 4.1153688Losses:  4.021930694580078 0.0728481337428093
CurrentTrain: epoch  9, batch    22 | loss: 4.0947790Losses:  3.9906105995178223 0.07639196515083313
CurrentTrain: epoch  9, batch    23 | loss: 4.0670028Losses:  3.992368698120117 0.09831275045871735
CurrentTrain: epoch  9, batch    24 | loss: 4.0906816Losses:  3.9761548042297363 0.04802129790186882
CurrentTrain: epoch  9, batch    25 | loss: 4.0241761Losses:  3.9505844116210938 0.09061793237924576
CurrentTrain: epoch  9, batch    26 | loss: 4.0412025Losses:  3.959628105163574 0.11667986214160919
CurrentTrain: epoch  9, batch    27 | loss: 4.0763078Losses:  3.9922680854797363 0.07023321837186813
CurrentTrain: epoch  9, batch    28 | loss: 4.0625014Losses:  3.950005054473877 0.1218886449933052
CurrentTrain: epoch  9, batch    29 | loss: 4.0718937Losses:  3.988314390182495 0.0581299364566803
CurrentTrain: epoch  9, batch    30 | loss: 4.0464444Losses:  3.91469407081604 0.07891475409269333
CurrentTrain: epoch  9, batch    31 | loss: 3.9936087Losses:  4.001493453979492 0.08760868012905121
CurrentTrain: epoch  9, batch    32 | loss: 4.0891023Losses:  3.949706554412842 0.1367044448852539
CurrentTrain: epoch  9, batch    33 | loss: 4.0864110Losses:  3.9242470264434814 0.06731858104467392
CurrentTrain: epoch  9, batch    34 | loss: 3.9915657Losses:  4.000309467315674 0.0918746218085289
CurrentTrain: epoch  9, batch    35 | loss: 4.0921841Losses:  3.9562838077545166 0.059701837599277496
CurrentTrain: epoch  9, batch    36 | loss: 4.0159855Losses:  3.998016357421875 0.04206027835607529
CurrentTrain: epoch  9, batch    37 | loss: 4.0400767Losses:  3.9736223220825195 0.057250671088695526
CurrentTrain: epoch  9, batch    38 | loss: 4.0308728Losses:  4.043056488037109 0.04132717475295067
CurrentTrain: epoch  9, batch    39 | loss: 4.0843835Losses:  3.9104435443878174 0.10390834510326385
CurrentTrain: epoch  9, batch    40 | loss: 4.0143518Losses:  3.9337992668151855 0.07574958354234695
CurrentTrain: epoch  9, batch    41 | loss: 4.0095487Losses:  3.967217445373535 0.09621553122997284
CurrentTrain: epoch  9, batch    42 | loss: 4.0634332Losses:  3.985408306121826 0.07033555954694748
CurrentTrain: epoch  9, batch    43 | loss: 4.0557437Losses:  4.017685413360596 0.04702058807015419
CurrentTrain: epoch  9, batch    44 | loss: 4.0647058Losses:  3.9306392669677734 0.0564597025513649
CurrentTrain: epoch  9, batch    45 | loss: 3.9870989Losses:  3.9602556228637695 0.056376758962869644
CurrentTrain: epoch  9, batch    46 | loss: 4.0166326Losses:  3.983851432800293 0.05053548514842987
CurrentTrain: epoch  9, batch    47 | loss: 4.0343871Losses:  3.962651252746582 0.07490861415863037
CurrentTrain: epoch  9, batch    48 | loss: 4.0375600Losses:  4.025874137878418 0.06679773330688477
CurrentTrain: epoch  9, batch    49 | loss: 4.0926719Losses:  3.9510886669158936 0.045475102961063385
CurrentTrain: epoch  9, batch    50 | loss: 3.9965637Losses:  3.952451705932617 0.04462866112589836
CurrentTrain: epoch  9, batch    51 | loss: 3.9970803Losses:  3.9891624450683594 0.06728163361549377
CurrentTrain: epoch  9, batch    52 | loss: 4.0564442Losses:  3.8883965015411377 0.02865011990070343
CurrentTrain: epoch  9, batch    53 | loss: 3.9170465Losses:  3.9948782920837402 0.13914841413497925
CurrentTrain: epoch  9, batch    54 | loss: 4.1340265Losses:  3.981903553009033 0.06098064035177231
CurrentTrain: epoch  9, batch    55 | loss: 4.0428843Losses:  3.9539928436279297 0.05421449989080429
CurrentTrain: epoch  9, batch    56 | loss: 4.0082073Losses:  3.9793219566345215 0.10123288631439209
CurrentTrain: epoch  9, batch    57 | loss: 4.0805550Losses:  4.000682830810547 0.06237918138504028
CurrentTrain: epoch  9, batch    58 | loss: 4.0630622Losses:  3.9019322395324707 0.08696634322404861
CurrentTrain: epoch  9, batch    59 | loss: 3.9888985Losses:  4.048635005950928 0.07398343086242676
CurrentTrain: epoch  9, batch    60 | loss: 4.1226187Losses:  4.028239727020264 0.05355861037969589
CurrentTrain: epoch  9, batch    61 | loss: 4.0817986Losses:  4.0041022300720215 0.13128986954689026
CurrentTrain: epoch  9, batch    62 | loss: 4.1353922Losses:  3.958744764328003 0.07043682783842087
CurrentTrain: epoch  9, batch    63 | loss: 4.0291815Losses:  3.985273599624634 0.05426312983036041
CurrentTrain: epoch  9, batch    64 | loss: 4.0395370Losses:  3.9817399978637695 0.047501515597105026
CurrentTrain: epoch  9, batch    65 | loss: 4.0292416Losses:  3.998244285583496 0.12365198880434036
CurrentTrain: epoch  9, batch    66 | loss: 4.1218963Losses:  3.953519821166992 0.0978008359670639
CurrentTrain: epoch  9, batch    67 | loss: 4.0513206Losses:  4.021062850952148 0.039300885051488876
CurrentTrain: epoch  9, batch    68 | loss: 4.0603638Losses:  3.9120821952819824 0.07431954145431519
CurrentTrain: epoch  9, batch    69 | loss: 3.9864018Losses:  3.9590349197387695 0.1008414775133133
CurrentTrain: epoch  9, batch    70 | loss: 4.0598764Losses:  3.9666945934295654 0.06061622500419617
CurrentTrain: epoch  9, batch    71 | loss: 4.0273108Losses:  4.015014171600342 0.053342755883932114
CurrentTrain: epoch  9, batch    72 | loss: 4.0683570Losses:  3.9242825508117676 0.03368551284074783
CurrentTrain: epoch  9, batch    73 | loss: 3.9579680Losses:  3.9524753093719482 0.053223758935928345
CurrentTrain: epoch  9, batch    74 | loss: 4.0056992Losses:  3.9451937675476074 0.10741086304187775
CurrentTrain: epoch  9, batch    75 | loss: 4.0526047Losses:  3.976346969604492 0.08349624276161194
CurrentTrain: epoch  9, batch    76 | loss: 4.0598431Losses:  3.973926067352295 0.07989265024662018
CurrentTrain: epoch  9, batch    77 | loss: 4.0538187Losses:  3.960258960723877 0.10054843872785568
CurrentTrain: epoch  9, batch    78 | loss: 4.0608072Losses:  3.950206756591797 0.07936809957027435
CurrentTrain: epoch  9, batch    79 | loss: 4.0295749Losses:  3.9849205017089844 0.055924344807863235
CurrentTrain: epoch  9, batch    80 | loss: 4.0408449Losses:  3.956623077392578 0.11755333840847015
CurrentTrain: epoch  9, batch    81 | loss: 4.0741763Losses:  3.9036147594451904 0.11357645690441132
CurrentTrain: epoch  9, batch    82 | loss: 4.0171914Losses:  3.9939630031585693 0.07014191150665283
CurrentTrain: epoch  9, batch    83 | loss: 4.0641050Losses:  3.9638547897338867 0.0832376703619957
CurrentTrain: epoch  9, batch    84 | loss: 4.0470924Losses:  3.954110622406006 0.11836269497871399
CurrentTrain: epoch  9, batch    85 | loss: 4.0724735Losses:  3.9877777099609375 0.10384713113307953
CurrentTrain: epoch  9, batch    86 | loss: 4.0916247Losses:  3.9316420555114746 0.07308230549097061
CurrentTrain: epoch  9, batch    87 | loss: 4.0047245Losses:  3.9906582832336426 0.11695949733257294
CurrentTrain: epoch  9, batch    88 | loss: 4.1076179Losses:  4.017682075500488 0.07168218493461609
CurrentTrain: epoch  9, batch    89 | loss: 4.0893641Losses:  4.036351680755615 0.0671137124300003
CurrentTrain: epoch  9, batch    90 | loss: 4.1034656Losses:  4.011349201202393 0.10480160266160965
CurrentTrain: epoch  9, batch    91 | loss: 4.1161509Losses:  3.966294288635254 0.1074012890458107
CurrentTrain: epoch  9, batch    92 | loss: 4.0736957Losses:  3.991171360015869 0.03576945513486862
CurrentTrain: epoch  9, batch    93 | loss: 4.0269408Losses:  3.95338773727417 0.10562174767255783
CurrentTrain: epoch  9, batch    94 | loss: 4.0590096Losses:  3.975299835205078 0.054960817098617554
CurrentTrain: epoch  9, batch    95 | loss: 4.0302606Losses:  3.9556314945220947 0.09795771539211273
CurrentTrain: epoch  9, batch    96 | loss: 4.0535893Losses:  3.9739675521850586 0.05917206034064293
CurrentTrain: epoch  9, batch    97 | loss: 4.0331397Losses:  4.024971961975098 0.08503405749797821
CurrentTrain: epoch  9, batch    98 | loss: 4.1100059Losses:  3.98626971244812 0.08186542987823486
CurrentTrain: epoch  9, batch    99 | loss: 4.0681353Losses:  3.942322015762329 0.1333027482032776
CurrentTrain: epoch  9, batch   100 | loss: 4.0756249Losses:  3.96878719329834 0.06277160346508026
CurrentTrain: epoch  9, batch   101 | loss: 4.0315590Losses:  3.9149551391601562 0.07097151130437851
CurrentTrain: epoch  9, batch   102 | loss: 3.9859266Losses:  4.015617370605469 0.0658063068985939
CurrentTrain: epoch  9, batch   103 | loss: 4.0814238Losses:  3.989224910736084 0.13407766819000244
CurrentTrain: epoch  9, batch   104 | loss: 4.1233025Losses:  3.8781955242156982 0.05750715360045433
CurrentTrain: epoch  9, batch   105 | loss: 3.9357026Losses:  4.023883819580078 0.08979317545890808
CurrentTrain: epoch  9, batch   106 | loss: 4.1136770Losses:  4.025552749633789 0.07722877711057663
CurrentTrain: epoch  9, batch   107 | loss: 4.1027813Losses:  3.968972682952881 0.06849537789821625
CurrentTrain: epoch  9, batch   108 | loss: 4.0374680Losses:  3.918062686920166 0.08783584088087082
CurrentTrain: epoch  9, batch   109 | loss: 4.0058985Losses:  3.9779794216156006 0.09308614581823349
CurrentTrain: epoch  9, batch   110 | loss: 4.0710654Losses:  3.9883790016174316 0.042607516050338745
CurrentTrain: epoch  9, batch   111 | loss: 4.0309863Losses:  3.9126415252685547 0.07816619426012039
CurrentTrain: epoch  9, batch   112 | loss: 3.9908078Losses:  3.9764764308929443 0.0486580915749073
CurrentTrain: epoch  9, batch   113 | loss: 4.0251346Losses:  4.023911476135254 0.09985913336277008
CurrentTrain: epoch  9, batch   114 | loss: 4.1237707Losses:  3.927798271179199 0.07521301507949829
CurrentTrain: epoch  9, batch   115 | loss: 4.0030112Losses:  3.9863710403442383 0.06241700053215027
CurrentTrain: epoch  9, batch   116 | loss: 4.0487881Losses:  4.01449728012085 0.0836782455444336
CurrentTrain: epoch  9, batch   117 | loss: 4.0981755Losses:  3.9834132194519043 0.09169407188892365
CurrentTrain: epoch  9, batch   118 | loss: 4.0751071Losses:  3.9957737922668457 0.051884882152080536
CurrentTrain: epoch  9, batch   119 | loss: 4.0476584Losses:  4.006508827209473 0.07332699000835419
CurrentTrain: epoch  9, batch   120 | loss: 4.0798359Losses:  3.9352474212646484 0.02715495601296425
CurrentTrain: epoch  9, batch   121 | loss: 3.9624023Losses:  3.9784181118011475 0.09679433703422546
CurrentTrain: epoch  9, batch   122 | loss: 4.0752125Losses:  3.949308395385742 0.05255468562245369
CurrentTrain: epoch  9, batch   123 | loss: 4.0018630Losses:  4.041401386260986 0.04583561047911644
CurrentTrain: epoch  9, batch   124 | loss: 4.0872369
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
Clustering into  9  clusters
Clusters:  [0 5 4 0 0 0 0 0 8 7 0 3 0 0 0 6 0 1 0 2]
Losses:  9.941975593566895 1.2676417827606201
CurrentTrain: epoch  0, batch     0 | loss: 11.2096176Losses:  5.887940406799316 1.0243815183639526
CurrentTrain: epoch  0, batch     1 | loss: 6.9123220Losses:  7.417794704437256 1.4399000406265259
CurrentTrain: epoch  0, batch     2 | loss: 8.8576946Losses:  9.867284774780273 1.81917405128479
CurrentTrain: epoch  0, batch     3 | loss: 11.6864586Losses:  6.678884029388428 1.4297950267791748
CurrentTrain: epoch  0, batch     4 | loss: 8.1086788Losses:  8.034749984741211 1.4124693870544434
CurrentTrain: epoch  0, batch     5 | loss: 9.4472198Losses:  5.210465431213379 0.27195343375205994
CurrentTrain: epoch  0, batch     6 | loss: 5.4824190Losses:  7.756612300872803 1.6013641357421875
CurrentTrain: epoch  1, batch     0 | loss: 9.3579769Losses:  5.248466491699219 1.2537165880203247
CurrentTrain: epoch  1, batch     1 | loss: 6.5021830Losses:  6.041835784912109 0.8538063764572144
CurrentTrain: epoch  1, batch     2 | loss: 6.8956423Losses:  7.768003463745117 1.1143457889556885
CurrentTrain: epoch  1, batch     3 | loss: 8.8823490Losses:  6.696038722991943 1.5430984497070312
CurrentTrain: epoch  1, batch     4 | loss: 8.2391376Losses:  6.076784133911133 0.708048939704895
CurrentTrain: epoch  1, batch     5 | loss: 6.7848330Losses:  8.94820785522461 0.36624348163604736
CurrentTrain: epoch  1, batch     6 | loss: 9.3144512Losses:  5.111639022827148 1.310167908668518
CurrentTrain: epoch  2, batch     0 | loss: 6.4218068Losses:  5.713865756988525 1.1362743377685547
CurrentTrain: epoch  2, batch     1 | loss: 6.8501401Losses:  4.078630447387695 0.982422947883606
CurrentTrain: epoch  2, batch     2 | loss: 5.0610533Losses:  4.882247447967529 1.2268970012664795
CurrentTrain: epoch  2, batch     3 | loss: 6.1091442Losses:  6.910284042358398 0.8322104215621948
CurrentTrain: epoch  2, batch     4 | loss: 7.7424946Losses:  6.300206184387207 1.3800102472305298
CurrentTrain: epoch  2, batch     5 | loss: 7.6802163Losses:  3.682649612426758 0.33842897415161133
CurrentTrain: epoch  2, batch     6 | loss: 4.0210786Losses:  3.090778350830078 1.004742980003357
CurrentTrain: epoch  3, batch     0 | loss: 4.0955215Losses:  5.1197614669799805 1.2387937307357788
CurrentTrain: epoch  3, batch     1 | loss: 6.3585553Losses:  4.548961162567139 0.8593403100967407
CurrentTrain: epoch  3, batch     2 | loss: 5.4083014Losses:  4.936676502227783 1.0637123584747314
CurrentTrain: epoch  3, batch     3 | loss: 6.0003891Losses:  6.558225631713867 0.5735393166542053
CurrentTrain: epoch  3, batch     4 | loss: 7.1317649Losses:  4.087247371673584 1.2119193077087402
CurrentTrain: epoch  3, batch     5 | loss: 5.2991667Losses:  4.187046051025391 0.47922080755233765
CurrentTrain: epoch  3, batch     6 | loss: 4.6662669Losses:  4.2237396240234375 0.9723578095436096
CurrentTrain: epoch  4, batch     0 | loss: 5.1960974Losses:  4.960329532623291 1.1557800769805908
CurrentTrain: epoch  4, batch     1 | loss: 6.1161098Losses:  4.825432777404785 1.4536261558532715
CurrentTrain: epoch  4, batch     2 | loss: 6.2790589Losses:  5.142673492431641 1.4096335172653198
CurrentTrain: epoch  4, batch     3 | loss: 6.5523071Losses:  3.2110495567321777 0.8778665065765381
CurrentTrain: epoch  4, batch     4 | loss: 4.0889158Losses:  4.028685569763184 0.7834967374801636
CurrentTrain: epoch  4, batch     5 | loss: 4.8121824Losses:  5.06995964050293 0.27553388476371765
CurrentTrain: epoch  4, batch     6 | loss: 5.3454933Losses:  5.085468292236328 1.2230098247528076
CurrentTrain: epoch  5, batch     0 | loss: 6.3084784Losses:  4.029595851898193 1.0760750770568848
CurrentTrain: epoch  5, batch     1 | loss: 5.1056709Losses:  3.819051742553711 1.1208961009979248
CurrentTrain: epoch  5, batch     2 | loss: 4.9399481Losses:  4.692672252655029 1.1961292028427124
CurrentTrain: epoch  5, batch     3 | loss: 5.8888016Losses:  3.939136266708374 1.0467138290405273
CurrentTrain: epoch  5, batch     4 | loss: 4.9858503Losses:  3.2362771034240723 1.0510121583938599
CurrentTrain: epoch  5, batch     5 | loss: 4.2872891Losses:  2.1276583671569824 8.94069742685133e-08
CurrentTrain: epoch  5, batch     6 | loss: 2.1276584Losses:  3.9666874408721924 1.0506962537765503
CurrentTrain: epoch  6, batch     0 | loss: 5.0173836Losses:  2.2400474548339844 0.7255316376686096
CurrentTrain: epoch  6, batch     1 | loss: 2.9655790Losses:  3.7395334243774414 0.9855103492736816
CurrentTrain: epoch  6, batch     2 | loss: 4.7250438Losses:  4.239480018615723 1.0813448429107666
CurrentTrain: epoch  6, batch     3 | loss: 5.3208246Losses:  3.4674720764160156 0.9359039068222046
CurrentTrain: epoch  6, batch     4 | loss: 4.4033761Losses:  4.249737739562988 1.0468193292617798
CurrentTrain: epoch  6, batch     5 | loss: 5.2965569Losses:  3.9844837188720703 0.2796233892440796
CurrentTrain: epoch  6, batch     6 | loss: 4.2641072Losses:  1.9847829341888428 0.44025689363479614
CurrentTrain: epoch  7, batch     0 | loss: 2.4250398Losses:  2.9917898178100586 0.676479160785675
CurrentTrain: epoch  7, batch     1 | loss: 3.6682689Losses:  3.5126559734344482 0.9042906761169434
CurrentTrain: epoch  7, batch     2 | loss: 4.4169464Losses:  3.894266128540039 0.9190583229064941
CurrentTrain: epoch  7, batch     3 | loss: 4.8133245Losses:  3.822861433029175 0.9535096883773804
CurrentTrain: epoch  7, batch     4 | loss: 4.7763710Losses:  3.632148027420044 1.1258771419525146
CurrentTrain: epoch  7, batch     5 | loss: 4.7580252Losses:  3.2647571563720703 0.1749880462884903
CurrentTrain: epoch  7, batch     6 | loss: 3.4397452Losses:  3.208514928817749 1.0145384073257446
CurrentTrain: epoch  8, batch     0 | loss: 4.2230535Losses:  2.95403790473938 0.7625389099121094
CurrentTrain: epoch  8, batch     1 | loss: 3.7165768Losses:  3.874748468399048 0.8211723566055298
CurrentTrain: epoch  8, batch     2 | loss: 4.6959209Losses:  2.8865914344787598 0.7158082723617554
CurrentTrain: epoch  8, batch     3 | loss: 3.6023998Losses:  2.4955430030822754 0.6843632459640503
CurrentTrain: epoch  8, batch     4 | loss: 3.1799064Losses:  3.880091428756714 0.9986385107040405
CurrentTrain: epoch  8, batch     5 | loss: 4.8787298Losses:  1.668313980102539 8.94069742685133e-08
CurrentTrain: epoch  8, batch     6 | loss: 1.6683141Losses:  2.4269142150878906 0.6270961761474609
CurrentTrain: epoch  9, batch     0 | loss: 3.0540104Losses:  2.2224721908569336 0.5840647220611572
CurrentTrain: epoch  9, batch     1 | loss: 2.8065369Losses:  2.72711443901062 1.0800089836120605
CurrentTrain: epoch  9, batch     2 | loss: 3.8071234Losses:  3.5413882732391357 0.8597832918167114
CurrentTrain: epoch  9, batch     3 | loss: 4.4011717Losses:  2.912660837173462 0.7520860433578491
CurrentTrain: epoch  9, batch     4 | loss: 3.6647468Losses:  3.1317412853240967 0.7627975940704346
CurrentTrain: epoch  9, batch     5 | loss: 3.8945389Losses:  3.327100992202759 0.09165848791599274
CurrentTrain: epoch  9, batch     6 | loss: 3.4187596
Losses:  0.7883275747299194 0.9432539939880371
MemoryTrain:  epoch  0, batch     0 | loss: 1.7315816Losses:  0.5902190208435059 0.44082844257354736
MemoryTrain:  epoch  0, batch     1 | loss: 1.0310475Losses:  1.4881274700164795 0.13979218900203705
MemoryTrain:  epoch  0, batch     2 | loss: 1.6279197Losses:  0.48430415987968445 0.5408832430839539
MemoryTrain:  epoch  1, batch     0 | loss: 1.0251874Losses:  0.3230646848678589 0.5677089691162109
MemoryTrain:  epoch  1, batch     1 | loss: 0.8907737Losses:  1.6354258060455322 0.24818278849124908
MemoryTrain:  epoch  1, batch     2 | loss: 1.8836086Losses:  0.26530006527900696 0.38207173347473145
MemoryTrain:  epoch  2, batch     0 | loss: 0.6473718Losses:  0.2843453586101532 0.5880150198936462
MemoryTrain:  epoch  2, batch     1 | loss: 0.8723603Losses:  0.09198857843875885 0.3034719228744507
MemoryTrain:  epoch  2, batch     2 | loss: 0.3954605Losses:  0.14300435781478882 0.6110476851463318
MemoryTrain:  epoch  3, batch     0 | loss: 0.7540520Losses:  0.17247416079044342 0.532100260257721
MemoryTrain:  epoch  3, batch     1 | loss: 0.7045744Losses:  0.013042435981333256 0.25919875502586365
MemoryTrain:  epoch  3, batch     2 | loss: 0.2722412Losses:  0.04522177577018738 0.6466073989868164
MemoryTrain:  epoch  4, batch     0 | loss: 0.6918292Losses:  0.04664438217878342 0.45517241954803467
MemoryTrain:  epoch  4, batch     1 | loss: 0.5018168Losses:  0.014778505079448223 0.13250938057899475
MemoryTrain:  epoch  4, batch     2 | loss: 0.1472879Losses:  0.03478529676795006 0.5415711402893066
MemoryTrain:  epoch  5, batch     0 | loss: 0.5763564Losses:  0.06520533561706543 0.5481195449829102
MemoryTrain:  epoch  5, batch     1 | loss: 0.6133249Losses:  0.009752711281180382 0.08661017566919327
MemoryTrain:  epoch  5, batch     2 | loss: 0.0963629Losses:  0.04403643682599068 0.399000346660614
MemoryTrain:  epoch  6, batch     0 | loss: 0.4430368Losses:  0.029657157137989998 0.5562534332275391
MemoryTrain:  epoch  6, batch     1 | loss: 0.5859106Losses:  0.02205786667764187 0.1439756453037262
MemoryTrain:  epoch  6, batch     2 | loss: 0.1660335Losses:  0.015088530257344246 0.3572288751602173
MemoryTrain:  epoch  7, batch     0 | loss: 0.3723174Losses:  0.019148893654346466 0.5081040263175964
MemoryTrain:  epoch  7, batch     1 | loss: 0.5272529Losses:  0.019551020115613937 0.34675830602645874
MemoryTrain:  epoch  7, batch     2 | loss: 0.3663093Losses:  0.011542731896042824 0.38965702056884766
MemoryTrain:  epoch  8, batch     0 | loss: 0.4011998Losses:  0.022513551637530327 0.6191344261169434
MemoryTrain:  epoch  8, batch     1 | loss: 0.6416480Losses:  0.005434693768620491 0.142613023519516
MemoryTrain:  epoch  8, batch     2 | loss: 0.1480477Losses:  0.01857207715511322 0.4450701177120209
MemoryTrain:  epoch  9, batch     0 | loss: 0.4636422Losses:  0.009808974340558052 0.35201674699783325
MemoryTrain:  epoch  9, batch     1 | loss: 0.3618257Losses:  0.005983131937682629 0.14182883501052856
MemoryTrain:  epoch  9, batch     2 | loss: 0.1478120
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 64.87%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.30%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 73.70%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.60%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 94.06%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.95%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 92.97%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 92.31%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 91.95%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 90.62%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 90.49%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 90.80%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 91.17%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 90.38%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 89.53%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 88.54%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 87.82%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 87.27%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 86.50%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 86.92%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 86.17%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 85.62%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 85.03%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 84.44%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 83.53%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 83.11%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 83.31%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 84.70%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 84.81%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 84.83%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 84.74%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 84.71%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 84.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.73%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 84.70%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 84.69%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 84.61%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 84.76%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 84.73%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 84.55%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 84.53%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.55%   
cur_acc:  ['0.9474', '0.7460']
his_acc:  ['0.9474', '0.8455']
Clustering into  14  clusters
Clusters:  [ 2 11  9  2  2  2  0  2  8  7  0 13 10  2  2  6  2  1  2 12  2  5  2  2
  1  2  2  2  3  4]
Losses:  6.218531131744385 1.0451645851135254
CurrentTrain: epoch  0, batch     0 | loss: 7.2636957Losses:  5.843270778656006 1.1550136804580688
CurrentTrain: epoch  0, batch     1 | loss: 6.9982843Losses:  6.408403396606445 1.1927860975265503
CurrentTrain: epoch  0, batch     2 | loss: 7.6011896Losses:  6.312321662902832 1.1293702125549316
CurrentTrain: epoch  0, batch     3 | loss: 7.4416919Losses:  5.476663589477539 1.4083302021026611
CurrentTrain: epoch  0, batch     4 | loss: 6.8849936Losses:  6.041578769683838 1.271793007850647
CurrentTrain: epoch  0, batch     5 | loss: 7.3133717Losses:  5.021515846252441 0.6066620349884033
CurrentTrain: epoch  0, batch     6 | loss: 5.6281776Losses:  5.8025593757629395 1.0401396751403809
CurrentTrain: epoch  1, batch     0 | loss: 6.8426991Losses:  6.160797595977783 1.179118275642395
CurrentTrain: epoch  1, batch     1 | loss: 7.3399158Losses:  5.96940803527832 1.0046899318695068
CurrentTrain: epoch  1, batch     2 | loss: 6.9740982Losses:  4.212038516998291 1.1052480936050415
CurrentTrain: epoch  1, batch     3 | loss: 5.3172865Losses:  4.610580921173096 0.890757143497467
CurrentTrain: epoch  1, batch     4 | loss: 5.5013380Losses:  4.655716896057129 0.9847899675369263
CurrentTrain: epoch  1, batch     5 | loss: 5.6405067Losses:  4.698443412780762 8.94069742685133e-08
CurrentTrain: epoch  1, batch     6 | loss: 4.6984434Losses:  5.191547393798828 1.1318614482879639
CurrentTrain: epoch  2, batch     0 | loss: 6.3234091Losses:  4.107813835144043 1.2177382707595825
CurrentTrain: epoch  2, batch     1 | loss: 5.3255520Losses:  4.913688659667969 1.1947145462036133
CurrentTrain: epoch  2, batch     2 | loss: 6.1084032Losses:  4.218606948852539 1.2591875791549683
CurrentTrain: epoch  2, batch     3 | loss: 5.4777946Losses:  3.907693386077881 1.2552101612091064
CurrentTrain: epoch  2, batch     4 | loss: 5.1629038Losses:  3.8889541625976562 1.1430368423461914
CurrentTrain: epoch  2, batch     5 | loss: 5.0319910Losses:  1.988747477531433 0.2744145393371582
CurrentTrain: epoch  2, batch     6 | loss: 2.2631621Losses:  5.110747814178467 1.159402847290039
CurrentTrain: epoch  3, batch     0 | loss: 6.2701507Losses:  2.620558261871338 0.7067716121673584
CurrentTrain: epoch  3, batch     1 | loss: 3.3273299Losses:  4.0808587074279785 1.1389881372451782
CurrentTrain: epoch  3, batch     2 | loss: 5.2198467Losses:  3.5834767818450928 1.0929681062698364
CurrentTrain: epoch  3, batch     3 | loss: 4.6764450Losses:  4.77390718460083 0.9719719886779785
CurrentTrain: epoch  3, batch     4 | loss: 5.7458792Losses:  3.3323771953582764 0.731960654258728
CurrentTrain: epoch  3, batch     5 | loss: 4.0643377Losses:  3.592968463897705 0.3764651417732239
CurrentTrain: epoch  3, batch     6 | loss: 3.9694335Losses:  3.401174306869507 0.9911016225814819
CurrentTrain: epoch  4, batch     0 | loss: 4.3922758Losses:  4.069314956665039 0.9303140640258789
CurrentTrain: epoch  4, batch     1 | loss: 4.9996290Losses:  4.804018497467041 0.8397074937820435
CurrentTrain: epoch  4, batch     2 | loss: 5.6437259Losses:  2.4122092723846436 0.8340096473693848
CurrentTrain: epoch  4, batch     3 | loss: 3.2462189Losses:  3.6382715702056885 1.1764215230941772
CurrentTrain: epoch  4, batch     4 | loss: 4.8146930Losses:  3.2747669219970703 0.5375665426254272
CurrentTrain: epoch  4, batch     5 | loss: 3.8123336Losses:  4.567017078399658 0.50102698802948
CurrentTrain: epoch  4, batch     6 | loss: 5.0680442Losses:  2.4569482803344727 0.7964349389076233
CurrentTrain: epoch  5, batch     0 | loss: 3.2533832Losses:  3.942777395248413 0.9130142331123352
CurrentTrain: epoch  5, batch     1 | loss: 4.8557916Losses:  3.551891565322876 1.0447694063186646
CurrentTrain: epoch  5, batch     2 | loss: 4.5966611Losses:  3.8867945671081543 1.1466952562332153
CurrentTrain: epoch  5, batch     3 | loss: 5.0334897Losses:  2.48470401763916 0.6001800298690796
CurrentTrain: epoch  5, batch     4 | loss: 3.0848842Losses:  3.5379552841186523 0.4592743515968323
CurrentTrain: epoch  5, batch     5 | loss: 3.9972296Losses:  3.495168685913086 0.19182530045509338
CurrentTrain: epoch  5, batch     6 | loss: 3.6869941Losses:  2.7705087661743164 0.7418698072433472
CurrentTrain: epoch  6, batch     0 | loss: 3.5123787Losses:  3.275773763656616 0.6800093650817871
CurrentTrain: epoch  6, batch     1 | loss: 3.9557831Losses:  2.705611228942871 0.8635368347167969
CurrentTrain: epoch  6, batch     2 | loss: 3.5691481Losses:  2.770120859146118 0.8954013586044312
CurrentTrain: epoch  6, batch     3 | loss: 3.6655221Losses:  3.058239698410034 0.9829825162887573
CurrentTrain: epoch  6, batch     4 | loss: 4.0412221Losses:  2.706045389175415 0.5736081600189209
CurrentTrain: epoch  6, batch     5 | loss: 3.2796535Losses:  1.9347424507141113 0.1844135820865631
CurrentTrain: epoch  6, batch     6 | loss: 2.1191561Losses:  2.750689744949341 0.8736735582351685
CurrentTrain: epoch  7, batch     0 | loss: 3.6243634Losses:  3.069206714630127 0.9386650919914246
CurrentTrain: epoch  7, batch     1 | loss: 4.0078716Losses:  3.078792095184326 0.9923360347747803
CurrentTrain: epoch  7, batch     2 | loss: 4.0711279Losses:  2.695751190185547 0.7556749582290649
CurrentTrain: epoch  7, batch     3 | loss: 3.4514260Losses:  1.9456366300582886 0.46043089032173157
CurrentTrain: epoch  7, batch     4 | loss: 2.4060676Losses:  2.2003302574157715 0.6730128526687622
CurrentTrain: epoch  7, batch     5 | loss: 2.8733430Losses:  2.7149529457092285 8.94069742685133e-08
CurrentTrain: epoch  7, batch     6 | loss: 2.7149529Losses:  2.2966814041137695 0.704110860824585
CurrentTrain: epoch  8, batch     0 | loss: 3.0007923Losses:  2.946815013885498 1.0151485204696655
CurrentTrain: epoch  8, batch     1 | loss: 3.9619637Losses:  2.5187835693359375 0.6568248867988586
CurrentTrain: epoch  8, batch     2 | loss: 3.1756084Losses:  2.2122385501861572 0.46174079179763794
CurrentTrain: epoch  8, batch     3 | loss: 2.6739793Losses:  2.899935722351074 0.8144457340240479
CurrentTrain: epoch  8, batch     4 | loss: 3.7143815Losses:  2.044224500656128 0.6136814951896667
CurrentTrain: epoch  8, batch     5 | loss: 2.6579061Losses:  1.8589924573898315 0.20285442471504211
CurrentTrain: epoch  8, batch     6 | loss: 2.0618470Losses:  2.9570517539978027 0.7130659818649292
CurrentTrain: epoch  9, batch     0 | loss: 3.6701179Losses:  2.4673564434051514 0.6286156177520752
CurrentTrain: epoch  9, batch     1 | loss: 3.0959721Losses:  1.9880080223083496 0.6257057189941406
CurrentTrain: epoch  9, batch     2 | loss: 2.6137137Losses:  2.25260591506958 0.5953237414360046
CurrentTrain: epoch  9, batch     3 | loss: 2.8479297Losses:  2.2277708053588867 0.6242483854293823
CurrentTrain: epoch  9, batch     4 | loss: 2.8520193Losses:  1.9684476852416992 0.5902238488197327
CurrentTrain: epoch  9, batch     5 | loss: 2.5586715Losses:  2.025482654571533 8.94069742685133e-08
CurrentTrain: epoch  9, batch     6 | loss: 2.0254827
Losses:  2.018754720687866 0.7188630700111389
MemoryTrain:  epoch  0, batch     0 | loss: 2.7376177Losses:  1.0321073532104492 0.8062944412231445
MemoryTrain:  epoch  0, batch     1 | loss: 1.8384018Losses:  0.5799927711486816 0.33557718992233276
MemoryTrain:  epoch  0, batch     2 | loss: 0.9155700Losses:  0.7062370181083679 0.3579235076904297
MemoryTrain:  epoch  0, batch     3 | loss: 1.0641606Losses:  1.6944942474365234 0.5570333003997803
MemoryTrain:  epoch  1, batch     0 | loss: 2.2515275Losses:  1.0241317749023438 0.7648192644119263
MemoryTrain:  epoch  1, batch     1 | loss: 1.7889510Losses:  1.1336493492126465 0.35425329208374023
MemoryTrain:  epoch  1, batch     2 | loss: 1.4879026Losses:  1.0668847560882568 0.4559440016746521
MemoryTrain:  epoch  1, batch     3 | loss: 1.5228288Losses:  0.1935030072927475 0.9491955041885376
MemoryTrain:  epoch  2, batch     0 | loss: 1.1426985Losses:  1.2847782373428345 0.3325863778591156
MemoryTrain:  epoch  2, batch     1 | loss: 1.6173646Losses:  0.47959330677986145 0.6419339179992676
MemoryTrain:  epoch  2, batch     2 | loss: 1.1215272Losses:  0.8103212118148804 0.46578407287597656
MemoryTrain:  epoch  2, batch     3 | loss: 1.2761053Losses:  0.1133359968662262 0.5989865660667419
MemoryTrain:  epoch  3, batch     0 | loss: 0.7123226Losses:  0.44566142559051514 0.37004995346069336
MemoryTrain:  epoch  3, batch     1 | loss: 0.8157114Losses:  0.10698960721492767 0.40733736753463745
MemoryTrain:  epoch  3, batch     2 | loss: 0.5143270Losses:  1.045156717300415 0.6460139155387878
MemoryTrain:  epoch  3, batch     3 | loss: 1.6911707Losses:  0.14401249587535858 0.5895328521728516
MemoryTrain:  epoch  4, batch     0 | loss: 0.7335454Losses:  0.12688207626342773 0.5593720078468323
MemoryTrain:  epoch  4, batch     1 | loss: 0.6862541Losses:  0.3861751854419708 0.6421312093734741
MemoryTrain:  epoch  4, batch     2 | loss: 1.0283064Losses:  0.036063045263290405 0.2928169071674347
MemoryTrain:  epoch  4, batch     3 | loss: 0.3288800Losses:  0.02186686359345913 0.4447118639945984
MemoryTrain:  epoch  5, batch     0 | loss: 0.4665787Losses:  0.2564549744129181 0.3794442117214203
MemoryTrain:  epoch  5, batch     1 | loss: 0.6358992Losses:  0.06326029449701309 0.6576896905899048
MemoryTrain:  epoch  5, batch     2 | loss: 0.7209500Losses:  0.21444134414196014 0.474433034658432
MemoryTrain:  epoch  5, batch     3 | loss: 0.6888744Losses:  0.11638961732387543 0.5132827758789062
MemoryTrain:  epoch  6, batch     0 | loss: 0.6296724Losses:  0.02539568394422531 0.6362080574035645
MemoryTrain:  epoch  6, batch     1 | loss: 0.6616037Losses:  0.06962048262357712 0.4590774178504944
MemoryTrain:  epoch  6, batch     2 | loss: 0.5286979Losses:  0.08773168176412582 0.336631715297699
MemoryTrain:  epoch  6, batch     3 | loss: 0.4243634Losses:  0.06979617476463318 0.5305230021476746
MemoryTrain:  epoch  7, batch     0 | loss: 0.6003191Losses:  0.20060645043849945 0.4741259217262268
MemoryTrain:  epoch  7, batch     1 | loss: 0.6747324Losses:  0.08026978373527527 0.5184767246246338
MemoryTrain:  epoch  7, batch     2 | loss: 0.5987465Losses:  0.05051843449473381 0.3472744822502136
MemoryTrain:  epoch  7, batch     3 | loss: 0.3977929Losses:  0.05396628379821777 0.5333549976348877
MemoryTrain:  epoch  8, batch     0 | loss: 0.5873213Losses:  0.04607769846916199 0.5430464744567871
MemoryTrain:  epoch  8, batch     1 | loss: 0.5891242Losses:  0.11768947541713715 0.49771296977996826
MemoryTrain:  epoch  8, batch     2 | loss: 0.6154025Losses:  0.026950176805257797 0.4132261276245117
MemoryTrain:  epoch  8, batch     3 | loss: 0.4401763Losses:  0.06043554097414017 0.446478009223938
MemoryTrain:  epoch  9, batch     0 | loss: 0.5069135Losses:  0.015754085034132004 0.2756865322589874
MemoryTrain:  epoch  9, batch     1 | loss: 0.2914406Losses:  0.07264815270900726 0.5055415630340576
MemoryTrain:  epoch  9, batch     2 | loss: 0.5781897Losses:  0.05946817249059677 0.5016592144966125
MemoryTrain:  epoch  9, batch     3 | loss: 0.5611274
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 82.84%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 82.33%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 80.80%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 80.26%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 80.06%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 79.17%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 91.95%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 91.46%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 90.38%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 89.75%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 89.33%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 87.78%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 87.59%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.76%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 88.25%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 87.42%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 86.69%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 85.82%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 85.28%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 84.77%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 84.03%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 83.99%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 84.59%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 83.92%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 83.33%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 82.76%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 82.27%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.45%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 81.05%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 81.18%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 81.32%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.38%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 81.69%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 82.48%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 82.06%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 81.65%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 81.31%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 81.02%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 80.52%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.37%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.41%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 80.63%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.65%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 80.65%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 80.14%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 79.91%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.92%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.28%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.34%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 80.22%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 80.04%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 79.83%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 79.71%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 79.72%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:  145 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 80.17%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 80.35%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 80.35%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 80.53%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 80.57%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 81.10%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 81.06%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 80.84%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 80.84%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 81.36%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 81.21%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 81.00%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 80.77%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 80.60%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 80.53%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 80.37%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 80.37%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 80.45%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 80.22%   
cur_acc:  ['0.9474', '0.7460', '0.7917']
his_acc:  ['0.9474', '0.8455', '0.8022']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15 17 16  2  1  1 12  1 18  1  8  1 14  1  1
  7  1  1  1  3  6  2  1  5  1 10  0  4  1  1  1]
Losses:  5.387310028076172 1.549224853515625
CurrentTrain: epoch  0, batch     0 | loss: 6.9365349Losses:  6.0394816398620605 1.1248338222503662
CurrentTrain: epoch  0, batch     1 | loss: 7.1643152Losses:  5.180957794189453 1.2369945049285889
CurrentTrain: epoch  0, batch     2 | loss: 6.4179525Losses:  5.581492900848389 1.1060842275619507
CurrentTrain: epoch  0, batch     3 | loss: 6.6875772Losses:  8.565056800842285 1.1845457553863525
CurrentTrain: epoch  0, batch     4 | loss: 9.7496023Losses:  6.8653364181518555 1.242337703704834
CurrentTrain: epoch  0, batch     5 | loss: 8.1076736Losses:  4.136858940124512 0.3344842493534088
CurrentTrain: epoch  0, batch     6 | loss: 4.4713430Losses:  5.690188884735107 0.8456602692604065
CurrentTrain: epoch  1, batch     0 | loss: 6.5358491Losses:  5.484918594360352 1.4023069143295288
CurrentTrain: epoch  1, batch     1 | loss: 6.8872256Losses:  5.072873592376709 1.2008622884750366
CurrentTrain: epoch  1, batch     2 | loss: 6.2737360Losses:  4.796608924865723 1.1786713600158691
CurrentTrain: epoch  1, batch     3 | loss: 5.9752803Losses:  4.5637736320495605 1.0179924964904785
CurrentTrain: epoch  1, batch     4 | loss: 5.5817661Losses:  5.156844139099121 1.232771635055542
CurrentTrain: epoch  1, batch     5 | loss: 6.3896160Losses:  5.081830024719238 0.19771656394004822
CurrentTrain: epoch  1, batch     6 | loss: 5.2795467Losses:  4.869844436645508 1.0727113485336304
CurrentTrain: epoch  2, batch     0 | loss: 5.9425559Losses:  5.232712268829346 1.0844930410385132
CurrentTrain: epoch  2, batch     1 | loss: 6.3172054Losses:  5.21146297454834 1.4463015794754028
CurrentTrain: epoch  2, batch     2 | loss: 6.6577644Losses:  4.572251319885254 0.7798973321914673
CurrentTrain: epoch  2, batch     3 | loss: 5.3521485Losses:  4.757615089416504 1.103270411491394
CurrentTrain: epoch  2, batch     4 | loss: 5.8608856Losses:  3.603888988494873 0.6212162971496582
CurrentTrain: epoch  2, batch     5 | loss: 4.2251053Losses:  5.109692573547363 0.29788923263549805
CurrentTrain: epoch  2, batch     6 | loss: 5.4075818Losses:  5.029202461242676 1.0420931577682495
CurrentTrain: epoch  3, batch     0 | loss: 6.0712957Losses:  5.207335472106934 1.0497742891311646
CurrentTrain: epoch  3, batch     1 | loss: 6.2571096Losses:  4.494885444641113 1.0921980142593384
CurrentTrain: epoch  3, batch     2 | loss: 5.5870833Losses:  3.4466006755828857 0.8525424003601074
CurrentTrain: epoch  3, batch     3 | loss: 4.2991428Losses:  4.638261318206787 0.9372556805610657
CurrentTrain: epoch  3, batch     4 | loss: 5.5755172Losses:  2.363135814666748 0.5743812322616577
CurrentTrain: epoch  3, batch     5 | loss: 2.9375172Losses:  4.462489604949951 0.5640282034873962
CurrentTrain: epoch  3, batch     6 | loss: 5.0265179Losses:  4.502365589141846 1.0076911449432373
CurrentTrain: epoch  4, batch     0 | loss: 5.5100565Losses:  4.7804365158081055 1.0218828916549683
CurrentTrain: epoch  4, batch     1 | loss: 5.8023195Losses:  2.863194704055786 0.6419112086296082
CurrentTrain: epoch  4, batch     2 | loss: 3.5051060Losses:  3.737473487854004 0.896598756313324
CurrentTrain: epoch  4, batch     3 | loss: 4.6340723Losses:  3.40090012550354 0.7132323980331421
CurrentTrain: epoch  4, batch     4 | loss: 4.1141324Losses:  4.205355644226074 1.088903784751892
CurrentTrain: epoch  4, batch     5 | loss: 5.2942595Losses:  1.7797099351882935 2.9802322387695312e-08
CurrentTrain: epoch  4, batch     6 | loss: 1.7797099Losses:  4.56241512298584 0.8121135234832764
CurrentTrain: epoch  5, batch     0 | loss: 5.3745289Losses:  3.250774383544922 0.582542896270752
CurrentTrain: epoch  5, batch     1 | loss: 3.8333173Losses:  3.9154000282287598 0.9626215696334839
CurrentTrain: epoch  5, batch     2 | loss: 4.8780217Losses:  4.001865386962891 0.7690755128860474
CurrentTrain: epoch  5, batch     3 | loss: 4.7709408Losses:  1.8839489221572876 0.5464448928833008
CurrentTrain: epoch  5, batch     4 | loss: 2.4303937Losses:  3.4411063194274902 0.6821972131729126
CurrentTrain: epoch  5, batch     5 | loss: 4.1233034Losses:  5.750230312347412 1.1920930376163597e-07
CurrentTrain: epoch  5, batch     6 | loss: 5.7502303Losses:  2.97507905960083 0.7365151643753052
CurrentTrain: epoch  6, batch     0 | loss: 3.7115941Losses:  2.9468772411346436 0.7067779898643494
CurrentTrain: epoch  6, batch     1 | loss: 3.6536553Losses:  3.266648530960083 0.9512373208999634
CurrentTrain: epoch  6, batch     2 | loss: 4.2178860Losses:  2.612062692642212 0.6420380473136902
CurrentTrain: epoch  6, batch     3 | loss: 3.2541008Losses:  4.173112869262695 0.7742947936058044
CurrentTrain: epoch  6, batch     4 | loss: 4.9474077Losses:  3.4915175437927246 1.1270921230316162
CurrentTrain: epoch  6, batch     5 | loss: 4.6186094Losses:  2.13958740234375 0.1504477858543396
CurrentTrain: epoch  6, batch     6 | loss: 2.2900352Losses:  3.506842613220215 0.524955153465271
CurrentTrain: epoch  7, batch     0 | loss: 4.0317979Losses:  2.9094326496124268 0.8167855143547058
CurrentTrain: epoch  7, batch     1 | loss: 3.7262182Losses:  2.3799936771392822 0.7618395090103149
CurrentTrain: epoch  7, batch     2 | loss: 3.1418333Losses:  2.8549323081970215 0.6437416076660156
CurrentTrain: epoch  7, batch     3 | loss: 3.4986739Losses:  3.29990816116333 0.8689224123954773
CurrentTrain: epoch  7, batch     4 | loss: 4.1688304Losses:  2.7021307945251465 0.6598836183547974
CurrentTrain: epoch  7, batch     5 | loss: 3.3620143Losses:  2.1160078048706055 0.16508693993091583
CurrentTrain: epoch  7, batch     6 | loss: 2.2810948Losses:  2.9584975242614746 0.9379261136054993
CurrentTrain: epoch  8, batch     0 | loss: 3.8964236Losses:  2.145104169845581 0.6507867574691772
CurrentTrain: epoch  8, batch     1 | loss: 2.7958908Losses:  2.459463357925415 0.6579165458679199
CurrentTrain: epoch  8, batch     2 | loss: 3.1173799Losses:  3.0435357093811035 0.8367878198623657
CurrentTrain: epoch  8, batch     3 | loss: 3.8803234Losses:  2.3121910095214844 0.7790400385856628
CurrentTrain: epoch  8, batch     4 | loss: 3.0912311Losses:  2.0978291034698486 0.5567989945411682
CurrentTrain: epoch  8, batch     5 | loss: 2.6546280Losses:  1.7948944568634033 0.1498403400182724
CurrentTrain: epoch  8, batch     6 | loss: 1.9447348Losses:  2.214310646057129 0.6054920554161072
CurrentTrain: epoch  9, batch     0 | loss: 2.8198028Losses:  2.6235008239746094 0.8756732940673828
CurrentTrain: epoch  9, batch     1 | loss: 3.4991741Losses:  2.2853317260742188 0.5884004235267639
CurrentTrain: epoch  9, batch     2 | loss: 2.8737321Losses:  2.0834174156188965 0.5496989488601685
CurrentTrain: epoch  9, batch     3 | loss: 2.6331162Losses:  2.704803705215454 0.664725124835968
CurrentTrain: epoch  9, batch     4 | loss: 3.3695288Losses:  2.0959911346435547 0.8035295605659485
CurrentTrain: epoch  9, batch     5 | loss: 2.8995206Losses:  1.877611756324768 0.12603184580802917
CurrentTrain: epoch  9, batch     6 | loss: 2.0036435
Losses:  0.7931151390075684 0.7998984456062317
MemoryTrain:  epoch  0, batch     0 | loss: 1.5930135Losses:  1.0925570726394653 0.7274138331413269
MemoryTrain:  epoch  0, batch     1 | loss: 1.8199708Losses:  1.2481378316879272 0.5273352265357971
MemoryTrain:  epoch  0, batch     2 | loss: 1.7754731Losses:  0.49689263105392456 0.7656906843185425
MemoryTrain:  epoch  0, batch     3 | loss: 1.2625833Losses:  0.4216165840625763 0.32821565866470337
MemoryTrain:  epoch  0, batch     4 | loss: 0.7498323Losses:  0.22762516140937805 0.6270683407783508
MemoryTrain:  epoch  1, batch     0 | loss: 0.8546935Losses:  0.5547083020210266 0.5532879829406738
MemoryTrain:  epoch  1, batch     1 | loss: 1.1079962Losses:  1.2280720472335815 0.3997080624103546
MemoryTrain:  epoch  1, batch     2 | loss: 1.6277801Losses:  1.2140326499938965 0.5693281888961792
MemoryTrain:  epoch  1, batch     3 | loss: 1.7833608Losses:  1.0469255447387695 0.8234128952026367
MemoryTrain:  epoch  1, batch     4 | loss: 1.8703384Losses:  0.1945524513721466 0.6216809749603271
MemoryTrain:  epoch  2, batch     0 | loss: 0.8162334Losses:  0.2710499167442322 0.7499656677246094
MemoryTrain:  epoch  2, batch     1 | loss: 1.0210156Losses:  0.2501092255115509 0.5132764577865601
MemoryTrain:  epoch  2, batch     2 | loss: 0.7633857Losses:  0.6532053351402283 0.4899134039878845
MemoryTrain:  epoch  2, batch     3 | loss: 1.1431187Losses:  0.7686485052108765 0.6522371768951416
MemoryTrain:  epoch  2, batch     4 | loss: 1.4208857Losses:  0.3645169734954834 0.375958114862442
MemoryTrain:  epoch  3, batch     0 | loss: 0.7404751Losses:  0.5049271583557129 0.5941030979156494
MemoryTrain:  epoch  3, batch     1 | loss: 1.0990303Losses:  0.12795065343379974 0.5944803953170776
MemoryTrain:  epoch  3, batch     2 | loss: 0.7224311Losses:  0.44286802411079407 0.9885007739067078
MemoryTrain:  epoch  3, batch     3 | loss: 1.4313688Losses:  0.14970295131206512 0.33048903942108154
MemoryTrain:  epoch  3, batch     4 | loss: 0.4801920Losses:  0.09014010429382324 0.4483228623867035
MemoryTrain:  epoch  4, batch     0 | loss: 0.5384630Losses:  0.14464186131954193 0.6084079742431641
MemoryTrain:  epoch  4, batch     1 | loss: 0.7530499Losses:  0.1204383447766304 0.5043175220489502
MemoryTrain:  epoch  4, batch     2 | loss: 0.6247559Losses:  0.06956995278596878 0.6379398107528687
MemoryTrain:  epoch  4, batch     3 | loss: 0.7075098Losses:  0.09663038700819016 0.7887226343154907
MemoryTrain:  epoch  4, batch     4 | loss: 0.8853530Losses:  0.0711001306772232 0.6264832019805908
MemoryTrain:  epoch  5, batch     0 | loss: 0.6975833Losses:  0.1120108887553215 0.620009183883667
MemoryTrain:  epoch  5, batch     1 | loss: 0.7320201Losses:  0.0838877409696579 0.3480200469493866
MemoryTrain:  epoch  5, batch     2 | loss: 0.4319078Losses:  0.18886815011501312 0.6400969624519348
MemoryTrain:  epoch  5, batch     3 | loss: 0.8289651Losses:  0.048009008169174194 0.5395848751068115
MemoryTrain:  epoch  5, batch     4 | loss: 0.5875939Losses:  0.09535375982522964 0.426408052444458
MemoryTrain:  epoch  6, batch     0 | loss: 0.5217618Losses:  0.09490391612052917 0.6334338188171387
MemoryTrain:  epoch  6, batch     1 | loss: 0.7283378Losses:  0.049713701009750366 0.48705774545669556
MemoryTrain:  epoch  6, batch     2 | loss: 0.5367714Losses:  0.09840171039104462 0.6072934865951538
MemoryTrain:  epoch  6, batch     3 | loss: 0.7056952Losses:  0.07854175567626953 0.463981568813324
MemoryTrain:  epoch  6, batch     4 | loss: 0.5425233Losses:  0.043056126683950424 0.5746380090713501
MemoryTrain:  epoch  7, batch     0 | loss: 0.6176941Losses:  0.05673675984144211 0.4001384377479553
MemoryTrain:  epoch  7, batch     1 | loss: 0.4568752Losses:  0.026595573872327805 0.5179435014724731
MemoryTrain:  epoch  7, batch     2 | loss: 0.5445391Losses:  0.058206506073474884 0.6729325652122498
MemoryTrain:  epoch  7, batch     3 | loss: 0.7311391Losses:  0.04030056297779083 0.49837926030158997
MemoryTrain:  epoch  7, batch     4 | loss: 0.5386798Losses:  0.04305703938007355 0.2872116267681122
MemoryTrain:  epoch  8, batch     0 | loss: 0.3302687Losses:  0.024261528626084328 0.5460726022720337
MemoryTrain:  epoch  8, batch     1 | loss: 0.5703341Losses:  0.02454311028122902 0.5364584922790527
MemoryTrain:  epoch  8, batch     2 | loss: 0.5610016Losses:  0.0720791295170784 0.41205736994743347
MemoryTrain:  epoch  8, batch     3 | loss: 0.4841365Losses:  0.06946144253015518 0.7857004404067993
MemoryTrain:  epoch  8, batch     4 | loss: 0.8551619Losses:  0.04548097401857376 0.35648226737976074
MemoryTrain:  epoch  9, batch     0 | loss: 0.4019632Losses:  0.0326639786362648 0.4232157766819
MemoryTrain:  epoch  9, batch     1 | loss: 0.4558797Losses:  0.1601860225200653 0.7550162672996521
MemoryTrain:  epoch  9, batch     2 | loss: 0.9152023Losses:  0.06211775541305542 0.6581190824508667
MemoryTrain:  epoch  9, batch     3 | loss: 0.7202368Losses:  0.03840266913175583 0.40668755769729614
MemoryTrain:  epoch  9, batch     4 | loss: 0.4450902
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 53.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 63.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.28%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 67.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 65.57%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 63.89%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 92.21%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 91.16%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 89.90%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.01%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 87.79%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 86.85%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 86.31%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 86.14%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 86.28%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 86.30%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 85.69%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 84.74%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 83.89%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 83.31%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 82.66%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 81.87%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 81.86%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.60%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 81.81%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 81.11%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 80.49%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 79.89%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 79.10%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 78.59%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 78.99%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.21%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.02%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 80.04%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 79.98%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 79.32%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.00%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 78.26%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 77.85%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 77.59%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 77.30%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 76.96%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 77.18%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 77.27%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 77.17%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 76.95%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 76.94%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 76.59%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 76.34%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 76.23%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.35%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 76.54%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 76.08%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 75.76%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 75.44%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 75.09%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 74.74%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 74.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 75.38%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 75.65%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 75.73%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 76.69%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 76.65%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 76.49%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 76.48%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 77.10%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.98%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 76.83%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.78%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 76.77%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 76.59%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 76.29%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 76.27%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 75.76%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 75.49%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 75.20%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 74.81%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 74.61%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 74.65%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 74.72%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 74.75%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.78%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 74.82%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 74.67%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 74.46%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.93%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.73%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 75.14%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 74.95%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 74.50%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  244 | acc: 0.00%,  total acc: 74.49%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 74.21%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 73.91%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 73.71%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 73.49%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 73.28%   
cur_acc:  ['0.9474', '0.7460', '0.7917', '0.6389']
his_acc:  ['0.9474', '0.8455', '0.8022', '0.7328']
Clustering into  24  clusters
Clusters:  [ 4 23 19  4  4  4  0  4 15 16  1 21  3  4  4 12  4 22  4 10  4 17  4  4
 11  4  4  4 20 18  3  4 13  4 14  0  6  4  4  4  7  8  4  4  5  1  9  4
  4  2]
Losses:  4.7097907066345215 1.360790491104126
CurrentTrain: epoch  0, batch     0 | loss: 6.0705814Losses:  4.56011962890625 1.4572207927703857
CurrentTrain: epoch  0, batch     1 | loss: 6.0173407Losses:  5.744385719299316 1.134434700012207
CurrentTrain: epoch  0, batch     2 | loss: 6.8788204Losses:  3.6533660888671875 1.0745466947555542
CurrentTrain: epoch  0, batch     3 | loss: 4.7279129Losses:  5.88118839263916 1.3422287702560425
CurrentTrain: epoch  0, batch     4 | loss: 7.2234173Losses:  5.020272254943848 1.5871045589447021
CurrentTrain: epoch  0, batch     5 | loss: 6.6073771Losses:  4.598723888397217 0.3816875219345093
CurrentTrain: epoch  0, batch     6 | loss: 4.9804115Losses:  4.852827072143555 1.482811689376831
CurrentTrain: epoch  1, batch     0 | loss: 6.3356390Losses:  3.9568376541137695 0.9270613193511963
CurrentTrain: epoch  1, batch     1 | loss: 4.8838987Losses:  2.763650417327881 0.8073357343673706
CurrentTrain: epoch  1, batch     2 | loss: 3.5709863Losses:  3.359684467315674 0.7796844840049744
CurrentTrain: epoch  1, batch     3 | loss: 4.1393690Losses:  4.3586320877075195 0.9945688247680664
CurrentTrain: epoch  1, batch     4 | loss: 5.3532009Losses:  2.6820435523986816 1.1859276294708252
CurrentTrain: epoch  1, batch     5 | loss: 3.8679712Losses:  4.753800868988037 0.44070908427238464
CurrentTrain: epoch  1, batch     6 | loss: 5.1945100Losses:  3.4279346466064453 1.167079210281372
CurrentTrain: epoch  2, batch     0 | loss: 4.5950136Losses:  2.8584156036376953 1.1164355278015137
CurrentTrain: epoch  2, batch     1 | loss: 3.9748511Losses:  3.7395145893096924 1.0451555252075195
CurrentTrain: epoch  2, batch     2 | loss: 4.7846699Losses:  3.3440327644348145 1.1231560707092285
CurrentTrain: epoch  2, batch     3 | loss: 4.4671888Losses:  2.914139747619629 1.2969512939453125
CurrentTrain: epoch  2, batch     4 | loss: 4.2110910Losses:  2.373198986053467 0.7821985483169556
CurrentTrain: epoch  2, batch     5 | loss: 3.1553974Losses:  1.895293951034546 0.253328412771225
CurrentTrain: epoch  2, batch     6 | loss: 2.1486223Losses:  2.6406986713409424 1.2027487754821777
CurrentTrain: epoch  3, batch     0 | loss: 3.8434474Losses:  2.8231863975524902 1.0228575468063354
CurrentTrain: epoch  3, batch     1 | loss: 3.8460441Losses:  3.0170347690582275 1.265284776687622
CurrentTrain: epoch  3, batch     2 | loss: 4.2823195Losses:  3.4195549488067627 0.6963692903518677
CurrentTrain: epoch  3, batch     3 | loss: 4.1159244Losses:  2.6455817222595215 0.5936186909675598
CurrentTrain: epoch  3, batch     4 | loss: 3.2392004Losses:  2.3548452854156494 0.9309428930282593
CurrentTrain: epoch  3, batch     5 | loss: 3.2857881Losses:  2.6366994380950928 0.41456031799316406
CurrentTrain: epoch  3, batch     6 | loss: 3.0512598Losses:  3.4285712242126465 0.7691494226455688
CurrentTrain: epoch  4, batch     0 | loss: 4.1977205Losses:  2.1798317432403564 1.0491734743118286
CurrentTrain: epoch  4, batch     1 | loss: 3.2290053Losses:  2.443108081817627 0.8418856859207153
CurrentTrain: epoch  4, batch     2 | loss: 3.2849936Losses:  2.1964640617370605 0.6160405874252319
CurrentTrain: epoch  4, batch     3 | loss: 2.8125048Losses:  2.657468795776367 0.6612531542778015
CurrentTrain: epoch  4, batch     4 | loss: 3.3187220Losses:  2.749783515930176 0.6187973022460938
CurrentTrain: epoch  4, batch     5 | loss: 3.3685808Losses:  2.367499589920044 0.2596258521080017
CurrentTrain: epoch  4, batch     6 | loss: 2.6271255Losses:  2.768965721130371 0.7676957249641418
CurrentTrain: epoch  5, batch     0 | loss: 3.5366614Losses:  2.4563260078430176 0.5325319766998291
CurrentTrain: epoch  5, batch     1 | loss: 2.9888580Losses:  2.2316102981567383 0.7887928485870361
CurrentTrain: epoch  5, batch     2 | loss: 3.0204031Losses:  2.084639072418213 0.4317839741706848
CurrentTrain: epoch  5, batch     3 | loss: 2.5164230Losses:  1.8468581438064575 0.5003104209899902
CurrentTrain: epoch  5, batch     4 | loss: 2.3471684Losses:  2.5775182247161865 1.127713680267334
CurrentTrain: epoch  5, batch     5 | loss: 3.7052319Losses:  1.6640024185180664 0.19491246342658997
CurrentTrain: epoch  5, batch     6 | loss: 1.8589149Losses:  1.8291363716125488 0.4306901693344116
CurrentTrain: epoch  6, batch     0 | loss: 2.2598267Losses:  2.149322509765625 0.7211143970489502
CurrentTrain: epoch  6, batch     1 | loss: 2.8704369Losses:  2.0147252082824707 0.6909652948379517
CurrentTrain: epoch  6, batch     2 | loss: 2.7056904Losses:  2.2888011932373047 0.7783021330833435
CurrentTrain: epoch  6, batch     3 | loss: 3.0671034Losses:  1.8764455318450928 0.4938373565673828
CurrentTrain: epoch  6, batch     4 | loss: 2.3702829Losses:  2.862504243850708 0.5335795283317566
CurrentTrain: epoch  6, batch     5 | loss: 3.3960838Losses:  4.268336296081543 0.4013068675994873
CurrentTrain: epoch  6, batch     6 | loss: 4.6696434Losses:  2.0005135536193848 0.5710527896881104
CurrentTrain: epoch  7, batch     0 | loss: 2.5715663Losses:  2.2351458072662354 0.8146284222602844
CurrentTrain: epoch  7, batch     1 | loss: 3.0497742Losses:  1.8691586256027222 0.811223030090332
CurrentTrain: epoch  7, batch     2 | loss: 2.6803818Losses:  2.6008331775665283 0.7411706447601318
CurrentTrain: epoch  7, batch     3 | loss: 3.3420038Losses:  2.3689985275268555 0.5210385322570801
CurrentTrain: epoch  7, batch     4 | loss: 2.8900371Losses:  1.8968982696533203 0.41791534423828125
CurrentTrain: epoch  7, batch     5 | loss: 2.3148136Losses:  1.9972699880599976 0.17534375190734863
CurrentTrain: epoch  7, batch     6 | loss: 2.1726136Losses:  1.9078986644744873 0.7455808520317078
CurrentTrain: epoch  8, batch     0 | loss: 2.6534796Losses:  2.1032605171203613 0.7276170253753662
CurrentTrain: epoch  8, batch     1 | loss: 2.8308775Losses:  2.0063796043395996 0.3667040169239044
CurrentTrain: epoch  8, batch     2 | loss: 2.3730836Losses:  2.060368061065674 0.7035125494003296
CurrentTrain: epoch  8, batch     3 | loss: 2.7638807Losses:  1.8309025764465332 0.643081545829773
CurrentTrain: epoch  8, batch     4 | loss: 2.4739842Losses:  2.2398252487182617 0.699543833732605
CurrentTrain: epoch  8, batch     5 | loss: 2.9393692Losses:  1.9556951522827148 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.9556952Losses:  1.9495148658752441 0.636677622795105
CurrentTrain: epoch  9, batch     0 | loss: 2.5861926Losses:  1.8580782413482666 0.3400404751300812
CurrentTrain: epoch  9, batch     1 | loss: 2.1981187Losses:  1.7930591106414795 0.42778289318084717
CurrentTrain: epoch  9, batch     2 | loss: 2.2208419Losses:  1.9262276887893677 0.46073341369628906
CurrentTrain: epoch  9, batch     3 | loss: 2.3869610Losses:  2.07523775100708 0.6744328737258911
CurrentTrain: epoch  9, batch     4 | loss: 2.7496705Losses:  2.043285608291626 0.4314635694026947
CurrentTrain: epoch  9, batch     5 | loss: 2.4747491Losses:  1.7573820352554321 0.07778863608837128
CurrentTrain: epoch  9, batch     6 | loss: 1.8351706
Losses:  0.26455727219581604 0.7254121899604797
MemoryTrain:  epoch  0, batch     0 | loss: 0.9899695Losses:  1.0310654640197754 0.4791787266731262
MemoryTrain:  epoch  0, batch     1 | loss: 1.5102441Losses:  0.7622628211975098 0.7557975053787231
MemoryTrain:  epoch  0, batch     2 | loss: 1.5180603Losses:  1.6293352842330933 0.8548080921173096
MemoryTrain:  epoch  0, batch     3 | loss: 2.4841433Losses:  0.6648786664009094 0.5544570684432983
MemoryTrain:  epoch  0, batch     4 | loss: 1.2193358Losses:  0.6629849672317505 0.5141360759735107
MemoryTrain:  epoch  0, batch     5 | loss: 1.1771210Losses:  2.2267260551452637 0.02745838463306427
MemoryTrain:  epoch  0, batch     6 | loss: 2.2541845Losses:  0.9820067286491394 0.3986920118331909
MemoryTrain:  epoch  1, batch     0 | loss: 1.3806987Losses:  0.14116156101226807 0.3759825527667999
MemoryTrain:  epoch  1, batch     1 | loss: 0.5171441Losses:  1.6750797033309937 0.8985030651092529
MemoryTrain:  epoch  1, batch     2 | loss: 2.5735826Losses:  0.8867053985595703 0.5668593645095825
MemoryTrain:  epoch  1, batch     3 | loss: 1.4535648Losses:  2.199662208557129 0.6845594644546509
MemoryTrain:  epoch  1, batch     4 | loss: 2.8842216Losses:  0.8494316935539246 0.4739008843898773
MemoryTrain:  epoch  1, batch     5 | loss: 1.3233325Losses:  2.5949161052703857 0.10058678686618805
MemoryTrain:  epoch  1, batch     6 | loss: 2.6955030Losses:  0.595086932182312 0.6667068600654602
MemoryTrain:  epoch  2, batch     0 | loss: 1.2617939Losses:  0.18124721944332123 0.4580514430999756
MemoryTrain:  epoch  2, batch     1 | loss: 0.6392987Losses:  0.21687722206115723 0.4105616509914398
MemoryTrain:  epoch  2, batch     2 | loss: 0.6274389Losses:  0.6500182747840881 0.6055952310562134
MemoryTrain:  epoch  2, batch     3 | loss: 1.2556136Losses:  1.0067260265350342 0.5116032361984253
MemoryTrain:  epoch  2, batch     4 | loss: 1.5183293Losses:  0.20142106711864471 0.5627652406692505
MemoryTrain:  epoch  2, batch     5 | loss: 0.7641863Losses:  0.12423014640808105 0.09047931432723999
MemoryTrain:  epoch  2, batch     6 | loss: 0.2147095Losses:  1.162804365158081 0.5585095286369324
MemoryTrain:  epoch  3, batch     0 | loss: 1.7213140Losses:  0.3530258238315582 0.6362140774726868
MemoryTrain:  epoch  3, batch     1 | loss: 0.9892399Losses:  0.0907021015882492 0.5584560632705688
MemoryTrain:  epoch  3, batch     2 | loss: 0.6491582Losses:  0.06874646246433258 0.29901039600372314
MemoryTrain:  epoch  3, batch     3 | loss: 0.3677568Losses:  0.39445239305496216 0.558185338973999
MemoryTrain:  epoch  3, batch     4 | loss: 0.9526377Losses:  0.19119302928447723 0.4809868633747101
MemoryTrain:  epoch  3, batch     5 | loss: 0.6721799Losses:  0.13920901715755463 0.08347383886575699
MemoryTrain:  epoch  3, batch     6 | loss: 0.2226829Losses:  0.2721608281135559 0.6168488264083862
MemoryTrain:  epoch  4, batch     0 | loss: 0.8890097Losses:  0.06945990025997162 0.6316813230514526
MemoryTrain:  epoch  4, batch     1 | loss: 0.7011412Losses:  0.07954124361276627 0.46347951889038086
MemoryTrain:  epoch  4, batch     2 | loss: 0.5430208Losses:  0.20861504971981049 0.38798701763153076
MemoryTrain:  epoch  4, batch     3 | loss: 0.5966021Losses:  0.07130765169858932 0.5019693970680237
MemoryTrain:  epoch  4, batch     4 | loss: 0.5732771Losses:  0.20692132413387299 0.4470764696598053
MemoryTrain:  epoch  4, batch     5 | loss: 0.6539978Losses:  0.10108636319637299 0.08805758506059647
MemoryTrain:  epoch  4, batch     6 | loss: 0.1891440Losses:  0.12520557641983032 0.3514077663421631
MemoryTrain:  epoch  5, batch     0 | loss: 0.4766133Losses:  0.10480749607086182 0.4957965612411499
MemoryTrain:  epoch  5, batch     1 | loss: 0.6006041Losses:  0.09855281561613083 0.5287876725196838
MemoryTrain:  epoch  5, batch     2 | loss: 0.6273405Losses:  0.03774210810661316 0.4169060289859772
MemoryTrain:  epoch  5, batch     3 | loss: 0.4546481Losses:  0.06125979870557785 0.3313223719596863
MemoryTrain:  epoch  5, batch     4 | loss: 0.3925822Losses:  0.05791304633021355 0.6261050701141357
MemoryTrain:  epoch  5, batch     5 | loss: 0.6840181Losses:  0.08263042569160461 0.23680144548416138
MemoryTrain:  epoch  5, batch     6 | loss: 0.3194319Losses:  0.11968648433685303 0.4348238706588745
MemoryTrain:  epoch  6, batch     0 | loss: 0.5545104Losses:  0.04937327653169632 0.44880256056785583
MemoryTrain:  epoch  6, batch     1 | loss: 0.4981758Losses:  0.06972744315862656 0.4849510192871094
MemoryTrain:  epoch  6, batch     2 | loss: 0.5546784Losses:  0.03848988562822342 0.3813909888267517
MemoryTrain:  epoch  6, batch     3 | loss: 0.4198809Losses:  0.09572014957666397 0.525255560874939
MemoryTrain:  epoch  6, batch     4 | loss: 0.6209757Losses:  0.05957484245300293 0.5903439521789551
MemoryTrain:  epoch  6, batch     5 | loss: 0.6499188Losses:  0.04618499055504799 0.19204717874526978
MemoryTrain:  epoch  6, batch     6 | loss: 0.2382322Losses:  0.035981837660074234 0.4064764380455017
MemoryTrain:  epoch  7, batch     0 | loss: 0.4424583Losses:  0.04516132175922394 0.3152116537094116
MemoryTrain:  epoch  7, batch     1 | loss: 0.3603730Losses:  0.06653117388486862 0.44850265979766846
MemoryTrain:  epoch  7, batch     2 | loss: 0.5150338Losses:  0.03837413713335991 0.5193318128585815
MemoryTrain:  epoch  7, batch     3 | loss: 0.5577059Losses:  0.08488522469997406 0.6466379165649414
MemoryTrain:  epoch  7, batch     4 | loss: 0.7315232Losses:  0.06029508262872696 0.44735318422317505
MemoryTrain:  epoch  7, batch     5 | loss: 0.5076483Losses:  0.13456454873085022 0.04560954496264458
MemoryTrain:  epoch  7, batch     6 | loss: 0.1801741Losses:  0.07384860515594482 0.6489793062210083
MemoryTrain:  epoch  8, batch     0 | loss: 0.7228279Losses:  0.050753891468048096 0.4267842471599579
MemoryTrain:  epoch  8, batch     1 | loss: 0.4775381Losses:  0.03752794489264488 0.38834038376808167
MemoryTrain:  epoch  8, batch     2 | loss: 0.4258683Losses:  0.039090730249881744 0.29711025953292847
MemoryTrain:  epoch  8, batch     3 | loss: 0.3362010Losses:  0.030774379149079323 0.3775213658809662
MemoryTrain:  epoch  8, batch     4 | loss: 0.4082958Losses:  0.05249593406915665 0.4956668019294739
MemoryTrain:  epoch  8, batch     5 | loss: 0.5481628Losses:  0.13268201053142548 0.05352985858917236
MemoryTrain:  epoch  8, batch     6 | loss: 0.1862119Losses:  0.04261345788836479 0.49369537830352783
MemoryTrain:  epoch  9, batch     0 | loss: 0.5363088Losses:  0.034941308200359344 0.4517970383167267
MemoryTrain:  epoch  9, batch     1 | loss: 0.4867384Losses:  0.029538890346884727 0.30464014410972595
MemoryTrain:  epoch  9, batch     2 | loss: 0.3341790Losses:  0.05834160000085831 0.7552061080932617
MemoryTrain:  epoch  9, batch     3 | loss: 0.8135477Losses:  0.06151548773050308 0.4918190538883209
MemoryTrain:  epoch  9, batch     4 | loss: 0.5533345Losses:  0.07016744464635849 0.4091094136238098
MemoryTrain:  epoch  9, batch     5 | loss: 0.4792769Losses:  0.005466529168188572 0.013315856456756592
MemoryTrain:  epoch  9, batch     6 | loss: 0.0187824
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.24%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 79.46%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 89.04%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 87.61%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 87.29%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 86.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 86.21%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 85.64%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 84.81%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 84.05%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 83.55%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.54%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 83.78%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 83.31%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 82.63%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 81.73%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 81.09%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 80.55%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 79.86%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 80.68%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 79.92%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 79.10%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 78.43%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 77.79%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 77.02%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 76.53%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 76.87%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 76.86%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 76.47%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 75.97%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 75.66%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 74.94%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 74.71%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 74.48%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 74.25%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 73.98%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.82%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.49%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 72.81%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 72.41%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 72.17%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 72.49%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 72.04%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 71.33%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 71.26%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 71.34%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 71.01%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 70.37%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 71.61%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 72.68%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 72.69%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 72.54%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.56%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 73.43%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 73.15%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 72.86%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 72.63%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 72.51%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 72.06%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 71.91%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 71.46%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 71.32%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 70.84%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.93%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 70.98%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 71.07%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 70.88%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 70.66%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 70.42%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 70.23%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 70.08%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 69.98%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 71.64%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 71.85%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 71.62%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 71.42%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 71.28%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 71.21%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 71.04%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 71.44%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 71.22%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 70.99%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 70.82%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 70.69%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 70.46%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 70.40%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.74%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 71.58%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 71.38%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 71.11%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 70.87%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 70.35%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 70.12%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 69.89%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 70.78%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  303 | acc: 68.75%,  total acc: 71.92%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 72.22%   
cur_acc:  ['0.9474', '0.7460', '0.7917', '0.6389', '0.7946']
his_acc:  ['0.9474', '0.8455', '0.8022', '0.7328', '0.7222']
Clustering into  28  clusters
Clusters:  [ 5 19 22  5  5  5 26  5 18  1  0 24  4  5  5 15  5 25  5 27  5 20  5  5
  2  5  5  5 17 21  4  5 16  5  8 14  7  5  5  5  3 10  5  5 13  0 23  5
  5  9  5  5  5 12  5  1  5  6 11  2]
Losses:  5.916498184204102 1.2644380331039429
CurrentTrain: epoch  0, batch     0 | loss: 7.1809363Losses:  6.031406879425049 0.8638248443603516
CurrentTrain: epoch  0, batch     1 | loss: 6.8952317Losses:  5.9108052253723145 0.9485868811607361
CurrentTrain: epoch  0, batch     2 | loss: 6.8593922Losses:  6.63249397277832 0.9972256422042847
CurrentTrain: epoch  0, batch     3 | loss: 7.6297197Losses:  6.32353401184082 1.1491625308990479
CurrentTrain: epoch  0, batch     4 | loss: 7.4726963Losses:  4.759264945983887 0.46433863043785095
CurrentTrain: epoch  0, batch     5 | loss: 5.2236037Losses:  7.94854736328125 0.2673155665397644
CurrentTrain: epoch  0, batch     6 | loss: 8.2158632Losses:  5.447504043579102 0.9231995344161987
CurrentTrain: epoch  1, batch     0 | loss: 6.3707037Losses:  4.25575065612793 0.962556004524231
CurrentTrain: epoch  1, batch     1 | loss: 5.2183065Losses:  4.769051551818848 0.9429869055747986
CurrentTrain: epoch  1, batch     2 | loss: 5.7120385Losses:  5.998023509979248 0.9044263958930969
CurrentTrain: epoch  1, batch     3 | loss: 6.9024501Losses:  3.4081461429595947 0.5552562475204468
CurrentTrain: epoch  1, batch     4 | loss: 3.9634023Losses:  5.255656719207764 0.8742908239364624
CurrentTrain: epoch  1, batch     5 | loss: 6.1299477Losses:  3.785311698913574 0.05758915841579437
CurrentTrain: epoch  1, batch     6 | loss: 3.8429008Losses:  3.7687883377075195 0.9355679154396057
CurrentTrain: epoch  2, batch     0 | loss: 4.7043562Losses:  4.155288219451904 0.5819632411003113
CurrentTrain: epoch  2, batch     1 | loss: 4.7372513Losses:  3.6904287338256836 0.6772111058235168
CurrentTrain: epoch  2, batch     2 | loss: 4.3676400Losses:  3.816873550415039 0.5792628526687622
CurrentTrain: epoch  2, batch     3 | loss: 4.3961363Losses:  3.235348701477051 0.7167690396308899
CurrentTrain: epoch  2, batch     4 | loss: 3.9521177Losses:  2.959845781326294 0.5427924394607544
CurrentTrain: epoch  2, batch     5 | loss: 3.5026383Losses:  3.9116621017456055 0.35647985339164734
CurrentTrain: epoch  2, batch     6 | loss: 4.2681417Losses:  3.4967384338378906 0.8576091527938843
CurrentTrain: epoch  3, batch     0 | loss: 4.3543477Losses:  3.163564682006836 0.8110518455505371
CurrentTrain: epoch  3, batch     1 | loss: 3.9746165Losses:  2.5474796295166016 0.42146381735801697
CurrentTrain: epoch  3, batch     2 | loss: 2.9689434Losses:  3.2083301544189453 0.5958669185638428
CurrentTrain: epoch  3, batch     3 | loss: 3.8041971Losses:  2.841104507446289 0.569306492805481
CurrentTrain: epoch  3, batch     4 | loss: 3.4104109Losses:  2.5584282875061035 0.5190316438674927
CurrentTrain: epoch  3, batch     5 | loss: 3.0774598Losses:  3.4263477325439453 0.2237556427717209
CurrentTrain: epoch  3, batch     6 | loss: 3.6501033Losses:  2.6431238651275635 0.6230517625808716
CurrentTrain: epoch  4, batch     0 | loss: 3.2661757Losses:  2.8159992694854736 0.5842492580413818
CurrentTrain: epoch  4, batch     1 | loss: 3.4002485Losses:  2.4574599266052246 0.47905969619750977
CurrentTrain: epoch  4, batch     2 | loss: 2.9365196Losses:  2.9239354133605957 0.743260383605957
CurrentTrain: epoch  4, batch     3 | loss: 3.6671958Losses:  2.7939252853393555 0.4259387254714966
CurrentTrain: epoch  4, batch     4 | loss: 3.2198639Losses:  2.84753155708313 0.4190523028373718
CurrentTrain: epoch  4, batch     5 | loss: 3.2665839Losses:  1.8225163221359253 0.048355426639318466
CurrentTrain: epoch  4, batch     6 | loss: 1.8708718Losses:  2.802523612976074 0.4516746997833252
CurrentTrain: epoch  5, batch     0 | loss: 3.2541983Losses:  2.367074489593506 0.3886236846446991
CurrentTrain: epoch  5, batch     1 | loss: 2.7556982Losses:  2.379749298095703 0.5783448219299316
CurrentTrain: epoch  5, batch     2 | loss: 2.9580941Losses:  2.568431854248047 0.545846164226532
CurrentTrain: epoch  5, batch     3 | loss: 3.1142781Losses:  2.2412142753601074 0.5256354212760925
CurrentTrain: epoch  5, batch     4 | loss: 2.7668498Losses:  2.339351177215576 0.4412284791469574
CurrentTrain: epoch  5, batch     5 | loss: 2.7805796Losses:  2.18943190574646 0.06681857258081436
CurrentTrain: epoch  5, batch     6 | loss: 2.2562504Losses:  2.4663796424865723 0.5125042796134949
CurrentTrain: epoch  6, batch     0 | loss: 2.9788840Losses:  2.875685691833496 0.4636569023132324
CurrentTrain: epoch  6, batch     1 | loss: 3.3393426Losses:  2.067042350769043 0.43218910694122314
CurrentTrain: epoch  6, batch     2 | loss: 2.4992313Losses:  1.918097734451294 0.4478257894515991
CurrentTrain: epoch  6, batch     3 | loss: 2.3659234Losses:  2.0307366847991943 0.4201449751853943
CurrentTrain: epoch  6, batch     4 | loss: 2.4508817Losses:  2.030834197998047 0.29279667139053345
CurrentTrain: epoch  6, batch     5 | loss: 2.3236308Losses:  1.856940507888794 0.196012943983078
CurrentTrain: epoch  6, batch     6 | loss: 2.0529535Losses:  2.0379438400268555 0.5239332914352417
CurrentTrain: epoch  7, batch     0 | loss: 2.5618773Losses:  2.175236225128174 0.7033131122589111
CurrentTrain: epoch  7, batch     1 | loss: 2.8785493Losses:  2.2869389057159424 0.269497811794281
CurrentTrain: epoch  7, batch     2 | loss: 2.5564368Losses:  2.168179750442505 0.37512317299842834
CurrentTrain: epoch  7, batch     3 | loss: 2.5433030Losses:  1.9791678190231323 0.4682810306549072
CurrentTrain: epoch  7, batch     4 | loss: 2.4474487Losses:  1.8891950845718384 0.2623633146286011
CurrentTrain: epoch  7, batch     5 | loss: 2.1515584Losses:  1.7575430870056152 0.15260116755962372
CurrentTrain: epoch  7, batch     6 | loss: 1.9101442Losses:  1.863956093788147 0.2437758445739746
CurrentTrain: epoch  8, batch     0 | loss: 2.1077318Losses:  1.92818284034729 0.3595263957977295
CurrentTrain: epoch  8, batch     1 | loss: 2.2877092Losses:  2.311497688293457 0.5855085849761963
CurrentTrain: epoch  8, batch     2 | loss: 2.8970063Losses:  1.8763883113861084 0.48193931579589844
CurrentTrain: epoch  8, batch     3 | loss: 2.3583276Losses:  1.9085949659347534 0.43375062942504883
CurrentTrain: epoch  8, batch     4 | loss: 2.3423457Losses:  1.9348833560943604 0.4189433455467224
CurrentTrain: epoch  8, batch     5 | loss: 2.3538268Losses:  2.1140360832214355 0.10250590741634369
CurrentTrain: epoch  8, batch     6 | loss: 2.2165420Losses:  1.867124319076538 0.34550002217292786
CurrentTrain: epoch  9, batch     0 | loss: 2.2126243Losses:  1.8019776344299316 0.2687718868255615
CurrentTrain: epoch  9, batch     1 | loss: 2.0707495Losses:  1.9846112728118896 0.3491017818450928
CurrentTrain: epoch  9, batch     2 | loss: 2.3337131Losses:  2.034351348876953 0.25725123286247253
CurrentTrain: epoch  9, batch     3 | loss: 2.2916026Losses:  1.9427578449249268 0.5801256895065308
CurrentTrain: epoch  9, batch     4 | loss: 2.5228834Losses:  1.9024577140808105 0.3313475251197815
CurrentTrain: epoch  9, batch     5 | loss: 2.2338052Losses:  1.7047922611236572 0.019144324585795403
CurrentTrain: epoch  9, batch     6 | loss: 1.7239366
Losses:  1.481903076171875 0.49192166328430176
MemoryTrain:  epoch  0, batch     0 | loss: 1.9738247Losses:  0.4286246597766876 0.5089446306228638
MemoryTrain:  epoch  0, batch     1 | loss: 0.9375693Losses:  0.7869248390197754 0.6135272979736328
MemoryTrain:  epoch  0, batch     2 | loss: 1.4004521Losses:  0.8780665993690491 0.5853830575942993
MemoryTrain:  epoch  0, batch     3 | loss: 1.4634497Losses:  1.2851777076721191 0.39964768290519714
MemoryTrain:  epoch  0, batch     4 | loss: 1.6848254Losses:  0.5662309527397156 0.4730956554412842
MemoryTrain:  epoch  0, batch     5 | loss: 1.0393267Losses:  0.8884992599487305 0.45056861639022827
MemoryTrain:  epoch  0, batch     6 | loss: 1.3390679Losses:  0.6431106925010681 0.2965998649597168
MemoryTrain:  epoch  0, batch     7 | loss: 0.9397106Losses:  0.38714128732681274 0.46619686484336853
MemoryTrain:  epoch  1, batch     0 | loss: 0.8533381Losses:  3.1952147483825684 0.5761055946350098
MemoryTrain:  epoch  1, batch     1 | loss: 3.7713203Losses:  1.2177608013153076 0.39453357458114624
MemoryTrain:  epoch  1, batch     2 | loss: 1.6122944Losses:  0.47430419921875 0.3702234625816345
MemoryTrain:  epoch  1, batch     3 | loss: 0.8445277Losses:  0.6213446259498596 0.4241516888141632
MemoryTrain:  epoch  1, batch     4 | loss: 1.0454963Losses:  0.3220098912715912 0.39723390340805054
MemoryTrain:  epoch  1, batch     5 | loss: 0.7192438Losses:  0.9162818193435669 0.6817566156387329
MemoryTrain:  epoch  1, batch     6 | loss: 1.5980384Losses:  1.0433526039123535 0.23855067789554596
MemoryTrain:  epoch  1, batch     7 | loss: 1.2819033Losses:  0.20805075764656067 0.4137791097164154
MemoryTrain:  epoch  2, batch     0 | loss: 0.6218299Losses:  0.8693302869796753 0.531538724899292
MemoryTrain:  epoch  2, batch     1 | loss: 1.4008690Losses:  0.9184510707855225 0.4035903215408325
MemoryTrain:  epoch  2, batch     2 | loss: 1.3220414Losses:  0.35572850704193115 0.5593058466911316
MemoryTrain:  epoch  2, batch     3 | loss: 0.9150344Losses:  0.7277785539627075 0.33992570638656616
MemoryTrain:  epoch  2, batch     4 | loss: 1.0677042Losses:  0.6940709948539734 0.33600714802742004
MemoryTrain:  epoch  2, batch     5 | loss: 1.0300782Losses:  0.42149144411087036 0.44579505920410156
MemoryTrain:  epoch  2, batch     6 | loss: 0.8672865Losses:  0.10387393832206726 0.3193892538547516
MemoryTrain:  epoch  2, batch     7 | loss: 0.4232632Losses:  0.11243197321891785 0.49895668029785156
MemoryTrain:  epoch  3, batch     0 | loss: 0.6113887Losses:  1.4366971254348755 0.4417799711227417
MemoryTrain:  epoch  3, batch     1 | loss: 1.8784771Losses:  0.6946886777877808 0.3128703236579895
MemoryTrain:  epoch  3, batch     2 | loss: 1.0075591Losses:  0.08572286367416382 0.501574695110321
MemoryTrain:  epoch  3, batch     3 | loss: 0.5872976Losses:  0.10275697708129883 0.5466365814208984
MemoryTrain:  epoch  3, batch     4 | loss: 0.6493936Losses:  0.3692830801010132 0.5622093677520752
MemoryTrain:  epoch  3, batch     5 | loss: 0.9314924Losses:  0.21734794974327087 0.46328192949295044
MemoryTrain:  epoch  3, batch     6 | loss: 0.6806298Losses:  0.10383699834346771 0.21478664875030518
MemoryTrain:  epoch  3, batch     7 | loss: 0.3186237Losses:  0.08622881770133972 0.30778998136520386
MemoryTrain:  epoch  4, batch     0 | loss: 0.3940188Losses:  0.07907813787460327 0.45493197441101074
MemoryTrain:  epoch  4, batch     1 | loss: 0.5340101Losses:  0.19277943670749664 0.35629984736442566
MemoryTrain:  epoch  4, batch     2 | loss: 0.5490793Losses:  0.059098560363054276 0.4576531946659088
MemoryTrain:  epoch  4, batch     3 | loss: 0.5167518Losses:  0.2208099365234375 0.548129677772522
MemoryTrain:  epoch  4, batch     4 | loss: 0.7689396Losses:  0.4923653304576874 0.5387017726898193
MemoryTrain:  epoch  4, batch     5 | loss: 1.0310671Losses:  0.22425471246242523 0.3426283299922943
MemoryTrain:  epoch  4, batch     6 | loss: 0.5668830Losses:  0.041428010910749435 0.15765167772769928
MemoryTrain:  epoch  4, batch     7 | loss: 0.1990797Losses:  0.1983824074268341 0.3543468117713928
MemoryTrain:  epoch  5, batch     0 | loss: 0.5527292Losses:  0.18238669633865356 0.4486020803451538
MemoryTrain:  epoch  5, batch     1 | loss: 0.6309888Losses:  0.13733120262622833 0.5575569868087769
MemoryTrain:  epoch  5, batch     2 | loss: 0.6948882Losses:  0.0596722736954689 0.41041719913482666
MemoryTrain:  epoch  5, batch     3 | loss: 0.4700895Losses:  0.04372425377368927 0.32871872186660767
MemoryTrain:  epoch  5, batch     4 | loss: 0.3724430Losses:  0.08877494186162949 0.39264097809791565
MemoryTrain:  epoch  5, batch     5 | loss: 0.4814159Losses:  0.030623063445091248 0.3858799338340759
MemoryTrain:  epoch  5, batch     6 | loss: 0.4165030Losses:  0.27751603722572327 0.20332539081573486
MemoryTrain:  epoch  5, batch     7 | loss: 0.4808414Losses:  0.22542518377304077 0.36758601665496826
MemoryTrain:  epoch  6, batch     0 | loss: 0.5930112Losses:  0.08089765906333923 0.8064867258071899
MemoryTrain:  epoch  6, batch     1 | loss: 0.8873844Losses:  0.11648108810186386 0.32387998700141907
MemoryTrain:  epoch  6, batch     2 | loss: 0.4403611Losses:  0.033814504742622375 0.27726030349731445
MemoryTrain:  epoch  6, batch     3 | loss: 0.3110748Losses:  0.06521525979042053 0.33679020404815674
MemoryTrain:  epoch  6, batch     4 | loss: 0.4020055Losses:  0.20699414610862732 0.4761476516723633
MemoryTrain:  epoch  6, batch     5 | loss: 0.6831418Losses:  0.1292308270931244 0.38813531398773193
MemoryTrain:  epoch  6, batch     6 | loss: 0.5173662Losses:  0.06823261827230453 0.359220415353775
MemoryTrain:  epoch  6, batch     7 | loss: 0.4274530Losses:  0.060350507497787476 0.4882698655128479
MemoryTrain:  epoch  7, batch     0 | loss: 0.5486203Losses:  0.075658418238163 0.25018996000289917
MemoryTrain:  epoch  7, batch     1 | loss: 0.3258484Losses:  0.07559960335493088 0.5228588581085205
MemoryTrain:  epoch  7, batch     2 | loss: 0.5984585Losses:  0.0711110532283783 0.5513865947723389
MemoryTrain:  epoch  7, batch     3 | loss: 0.6224977Losses:  0.05354375019669533 0.4232577085494995
MemoryTrain:  epoch  7, batch     4 | loss: 0.4768015Losses:  0.4681069850921631 0.4556863605976105
MemoryTrain:  epoch  7, batch     5 | loss: 0.9237933Losses:  0.3112461268901825 0.28555214405059814
MemoryTrain:  epoch  7, batch     6 | loss: 0.5967983Losses:  0.18408562242984772 0.21800218522548676
MemoryTrain:  epoch  7, batch     7 | loss: 0.4020878Losses:  0.04443991184234619 0.42619040608406067
MemoryTrain:  epoch  8, batch     0 | loss: 0.4706303Losses:  0.36448121070861816 0.3617773652076721
MemoryTrain:  epoch  8, batch     1 | loss: 0.7262586Losses:  0.16275639832019806 0.29793480038642883
MemoryTrain:  epoch  8, batch     2 | loss: 0.4606912Losses:  0.10547640174627304 0.49887245893478394
MemoryTrain:  epoch  8, batch     3 | loss: 0.6043488Losses:  0.03201017156243324 0.4345337450504303
MemoryTrain:  epoch  8, batch     4 | loss: 0.4665439Losses:  0.1661001443862915 0.6760002374649048
MemoryTrain:  epoch  8, batch     5 | loss: 0.8421004Losses:  0.03308453783392906 0.3341622054576874
MemoryTrain:  epoch  8, batch     6 | loss: 0.3672467Losses:  0.045131370425224304 0.1443435251712799
MemoryTrain:  epoch  8, batch     7 | loss: 0.1894749Losses:  0.03238997608423233 0.4627496600151062
MemoryTrain:  epoch  9, batch     0 | loss: 0.4951396Losses:  0.27535679936408997 0.45005783438682556
MemoryTrain:  epoch  9, batch     1 | loss: 0.7254146Losses:  0.14943119883537292 0.3797053098678589
MemoryTrain:  epoch  9, batch     2 | loss: 0.5291365Losses:  0.04469902068376541 0.43770042061805725
MemoryTrain:  epoch  9, batch     3 | loss: 0.4823994Losses:  0.023130251094698906 0.33370622992515564
MemoryTrain:  epoch  9, batch     4 | loss: 0.3568365Losses:  0.44468817114830017 0.6181114315986633
MemoryTrain:  epoch  9, batch     5 | loss: 1.0627996Losses:  0.025563040748238564 0.27025479078292847
MemoryTrain:  epoch  9, batch     6 | loss: 0.2958178Losses:  0.021587945520877838 0.24923661351203918
MemoryTrain:  epoch  9, batch     7 | loss: 0.2708246
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 72.52%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.70%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 86.10%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 85.38%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 84.90%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 83.63%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 83.11%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 82.40%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 81.62%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.34%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.34%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.42%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.59%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 79.57%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 78.96%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 78.28%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 77.55%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 77.59%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 77.74%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 76.94%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 76.24%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 74.80%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 74.87%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 74.45%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 73.73%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 73.41%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.25%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 72.64%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 72.45%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 72.20%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 72.01%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 71.72%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 71.64%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 72.13%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 72.42%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 72.15%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 71.78%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 71.56%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 71.15%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 70.80%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.09%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 70.67%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 70.26%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 69.41%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 71.30%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 71.06%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 70.86%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 70.62%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 70.28%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 70.43%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 70.70%   [EVAL] batch:  176 | acc: 6.25%,  total acc: 70.34%   [EVAL] batch:  177 | acc: 6.25%,  total acc: 69.98%   [EVAL] batch:  178 | acc: 18.75%,  total acc: 69.69%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 69.41%   [EVAL] batch:  180 | acc: 6.25%,  total acc: 69.06%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 68.58%   [EVAL] batch:  184 | acc: 25.00%,  total acc: 68.34%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 68.22%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.05%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 67.37%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 67.28%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 67.20%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.10%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 67.20%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 67.07%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 66.39%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 67.97%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 67.82%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.64%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 67.46%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 67.70%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 67.48%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 66.92%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 66.77%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  259 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.07%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 67.78%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 67.57%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 67.35%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 66.68%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 67.70%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 67.73%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  300 | acc: 25.00%,  total acc: 68.92%   [EVAL] batch:  301 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  302 | acc: 43.75%,  total acc: 68.77%   [EVAL] batch:  303 | acc: 37.50%,  total acc: 68.67%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  305 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  314 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 69.26%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 69.73%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 69.68%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 69.64%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 69.67%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 69.67%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:  345 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:  347 | acc: 81.25%,  total acc: 69.70%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 69.73%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 69.65%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 69.58%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 69.50%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 69.27%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.77%   
cur_acc:  ['0.9474', '0.7460', '0.7917', '0.6389', '0.7946', '0.7252']
his_acc:  ['0.9474', '0.8455', '0.8022', '0.7328', '0.7222', '0.6977']
Clustering into  34  clusters
Clusters:  [ 0  9 20  3  0  0 29  0 22  1 31 27  4  0  0 17  0 28  0 33  0 24  0  0
 16  0  0  0 21 25  4  0 18  0 19 26  8  0  0  0  3 13  0  0 30 14 23  0
  0 32  0  0  0 10  0  1  0  6 12 15  9 11  0  0  0  5  0  0  2  7]
Losses:  8.161478042602539 1.0833148956298828
CurrentTrain: epoch  0, batch     0 | loss: 9.2447929Losses:  5.101980209350586 0.8036290407180786
CurrentTrain: epoch  0, batch     1 | loss: 5.9056091Losses:  5.672468185424805 1.2193894386291504
CurrentTrain: epoch  0, batch     2 | loss: 6.8918576Losses:  5.479655742645264 1.4550644159317017
CurrentTrain: epoch  0, batch     3 | loss: 6.9347200Losses:  5.714077949523926 0.9027242660522461
CurrentTrain: epoch  0, batch     4 | loss: 6.6168022Losses:  4.988506317138672 1.0347599983215332
CurrentTrain: epoch  0, batch     5 | loss: 6.0232663Losses:  7.733685493469238 0.5221984386444092
CurrentTrain: epoch  0, batch     6 | loss: 8.2558842Losses:  5.592850685119629 0.9934849143028259
CurrentTrain: epoch  1, batch     0 | loss: 6.5863357Losses:  4.840275764465332 0.8775227069854736
CurrentTrain: epoch  1, batch     1 | loss: 5.7177982Losses:  3.6433262825012207 0.6930072903633118
CurrentTrain: epoch  1, batch     2 | loss: 4.3363338Losses:  4.68515682220459 0.6462197303771973
CurrentTrain: epoch  1, batch     3 | loss: 5.3313766Losses:  4.857188701629639 0.8989567160606384
CurrentTrain: epoch  1, batch     4 | loss: 5.7561455Losses:  3.4063987731933594 0.869221031665802
CurrentTrain: epoch  1, batch     5 | loss: 4.2756200Losses:  2.0007667541503906 0.2699315547943115
CurrentTrain: epoch  1, batch     6 | loss: 2.2706983Losses:  3.3312017917633057 0.7309207916259766
CurrentTrain: epoch  2, batch     0 | loss: 4.0621223Losses:  3.5412089824676514 0.683605968952179
CurrentTrain: epoch  2, batch     1 | loss: 4.2248149Losses:  3.0506200790405273 0.8386982679367065
CurrentTrain: epoch  2, batch     2 | loss: 3.8893185Losses:  4.058012962341309 0.8753557205200195
CurrentTrain: epoch  2, batch     3 | loss: 4.9333687Losses:  2.6092963218688965 0.5730642080307007
CurrentTrain: epoch  2, batch     4 | loss: 3.1823606Losses:  5.214144229888916 0.6847007274627686
CurrentTrain: epoch  2, batch     5 | loss: 5.8988447Losses:  2.876558303833008 0.15662537515163422
CurrentTrain: epoch  2, batch     6 | loss: 3.0331836Losses:  3.2686867713928223 0.8954758644104004
CurrentTrain: epoch  3, batch     0 | loss: 4.1641626Losses:  3.766893148422241 0.9163353443145752
CurrentTrain: epoch  3, batch     1 | loss: 4.6832285Losses:  2.8909363746643066 0.45669299364089966
CurrentTrain: epoch  3, batch     2 | loss: 3.3476293Losses:  3.637997627258301 0.8071863055229187
CurrentTrain: epoch  3, batch     3 | loss: 4.4451838Losses:  2.724302291870117 0.7966049313545227
CurrentTrain: epoch  3, batch     4 | loss: 3.5209072Losses:  2.8233633041381836 0.4993271827697754
CurrentTrain: epoch  3, batch     5 | loss: 3.3226905Losses:  3.0107967853546143 0.07019397616386414
CurrentTrain: epoch  3, batch     6 | loss: 3.0809908Losses:  3.4904212951660156 0.8730135560035706
CurrentTrain: epoch  4, batch     0 | loss: 4.3634348Losses:  3.2521657943725586 0.902405321598053
CurrentTrain: epoch  4, batch     1 | loss: 4.1545711Losses:  2.1678802967071533 0.49482792615890503
CurrentTrain: epoch  4, batch     2 | loss: 2.6627083Losses:  2.9802310466766357 0.7396122217178345
CurrentTrain: epoch  4, batch     3 | loss: 3.7198434Losses:  2.572960376739502 0.7099169492721558
CurrentTrain: epoch  4, batch     4 | loss: 3.2828774Losses:  2.5901412963867188 0.4829910397529602
CurrentTrain: epoch  4, batch     5 | loss: 3.0731323Losses:  2.309173107147217 0.18907740712165833
CurrentTrain: epoch  4, batch     6 | loss: 2.4982505Losses:  2.2212464809417725 0.5812082290649414
CurrentTrain: epoch  5, batch     0 | loss: 2.8024547Losses:  2.588470220565796 0.5959388017654419
CurrentTrain: epoch  5, batch     1 | loss: 3.1844091Losses:  2.1750221252441406 0.6634840369224548
CurrentTrain: epoch  5, batch     2 | loss: 2.8385062Losses:  2.6236183643341064 0.5760959386825562
CurrentTrain: epoch  5, batch     3 | loss: 3.1997142Losses:  2.778090476989746 0.8165333271026611
CurrentTrain: epoch  5, batch     4 | loss: 3.5946238Losses:  2.1541600227355957 0.6385617256164551
CurrentTrain: epoch  5, batch     5 | loss: 2.7927217Losses:  5.232924938201904 8.94069742685133e-08
CurrentTrain: epoch  5, batch     6 | loss: 5.2329249Losses:  2.3322362899780273 0.8135834336280823
CurrentTrain: epoch  6, batch     0 | loss: 3.1458197Losses:  2.647124767303467 0.7182434797286987
CurrentTrain: epoch  6, batch     1 | loss: 3.3653684Losses:  2.230774402618408 0.6867815852165222
CurrentTrain: epoch  6, batch     2 | loss: 2.9175560Losses:  2.1051406860351562 0.6061327457427979
CurrentTrain: epoch  6, batch     3 | loss: 2.7112734Losses:  2.3593907356262207 0.6384344100952148
CurrentTrain: epoch  6, batch     4 | loss: 2.9978251Losses:  2.133615255355835 0.5298242568969727
CurrentTrain: epoch  6, batch     5 | loss: 2.6634395Losses:  3.1930384635925293 0.3925984501838684
CurrentTrain: epoch  6, batch     6 | loss: 3.5856369Losses:  2.362532138824463 0.6621608734130859
CurrentTrain: epoch  7, batch     0 | loss: 3.0246930Losses:  1.9314812421798706 0.47960808873176575
CurrentTrain: epoch  7, batch     1 | loss: 2.4110894Losses:  2.3640215396881104 0.5757588148117065
CurrentTrain: epoch  7, batch     2 | loss: 2.9397802Losses:  2.3860630989074707 0.9019910097122192
CurrentTrain: epoch  7, batch     3 | loss: 3.2880540Losses:  1.981676459312439 0.45590144395828247
CurrentTrain: epoch  7, batch     4 | loss: 2.4375780Losses:  1.9082390069961548 0.48766762018203735
CurrentTrain: epoch  7, batch     5 | loss: 2.3959067Losses:  1.941311001777649 0.12373751401901245
CurrentTrain: epoch  7, batch     6 | loss: 2.0650485Losses:  2.1165390014648438 0.5026796460151672
CurrentTrain: epoch  8, batch     0 | loss: 2.6192186Losses:  1.9828805923461914 0.5086023807525635
CurrentTrain: epoch  8, batch     1 | loss: 2.4914830Losses:  2.0742383003234863 0.5063756108283997
CurrentTrain: epoch  8, batch     2 | loss: 2.5806139Losses:  1.9689466953277588 0.4214278757572174
CurrentTrain: epoch  8, batch     3 | loss: 2.3903747Losses:  2.033763885498047 0.768399715423584
CurrentTrain: epoch  8, batch     4 | loss: 2.8021636Losses:  2.004302978515625 0.5216947197914124
CurrentTrain: epoch  8, batch     5 | loss: 2.5259976Losses:  2.104379177093506 0.13597071170806885
CurrentTrain: epoch  8, batch     6 | loss: 2.2403498Losses:  1.9873443841934204 0.7239110469818115
CurrentTrain: epoch  9, batch     0 | loss: 2.7112556Losses:  1.9217084646224976 0.5644776821136475
CurrentTrain: epoch  9, batch     1 | loss: 2.4861860Losses:  1.8776061534881592 0.5769210457801819
CurrentTrain: epoch  9, batch     2 | loss: 2.4545271Losses:  1.8398051261901855 0.36869582533836365
CurrentTrain: epoch  9, batch     3 | loss: 2.2085009Losses:  1.8695380687713623 0.15706519782543182
CurrentTrain: epoch  9, batch     4 | loss: 2.0266032Losses:  2.065204620361328 0.6235947608947754
CurrentTrain: epoch  9, batch     5 | loss: 2.6887994Losses:  1.8569037914276123 0.08759529888629913
CurrentTrain: epoch  9, batch     6 | loss: 1.9444991
Losses:  0.20236459374427795 0.45315998792648315
MemoryTrain:  epoch  0, batch     0 | loss: 0.6555246Losses:  0.8820050358772278 0.6472398042678833
MemoryTrain:  epoch  0, batch     1 | loss: 1.5292449Losses:  0.327455073595047 0.3571307361125946
MemoryTrain:  epoch  0, batch     2 | loss: 0.6845858Losses:  0.9855778217315674 0.406158447265625
MemoryTrain:  epoch  0, batch     3 | loss: 1.3917363Losses:  0.7756413817405701 0.5178091526031494
MemoryTrain:  epoch  0, batch     4 | loss: 1.2934506Losses:  1.5243353843688965 0.695356547832489
MemoryTrain:  epoch  0, batch     5 | loss: 2.2196920Losses:  0.8098275661468506 0.5165297389030457
MemoryTrain:  epoch  0, batch     6 | loss: 1.3263574Losses:  0.9193066358566284 0.4364314079284668
MemoryTrain:  epoch  0, batch     7 | loss: 1.3557380Losses:  0.2811049818992615 0.3073875308036804
MemoryTrain:  epoch  0, batch     8 | loss: 0.5884925Losses:  0.43387284874916077 0.4141756594181061
MemoryTrain:  epoch  1, batch     0 | loss: 0.8480485Losses:  0.381051242351532 0.4737999439239502
MemoryTrain:  epoch  1, batch     1 | loss: 0.8548512Losses:  1.2087516784667969 0.4140036106109619
MemoryTrain:  epoch  1, batch     2 | loss: 1.6227553Losses:  0.12254849821329117 0.2952578365802765
MemoryTrain:  epoch  1, batch     3 | loss: 0.4178063Losses:  0.6023844480514526 0.5178529620170593
MemoryTrain:  epoch  1, batch     4 | loss: 1.1202374Losses:  1.1442545652389526 0.4629894495010376
MemoryTrain:  epoch  1, batch     5 | loss: 1.6072440Losses:  1.298872947692871 0.47957926988601685
MemoryTrain:  epoch  1, batch     6 | loss: 1.7784522Losses:  0.8739509582519531 0.47014591097831726
MemoryTrain:  epoch  1, batch     7 | loss: 1.3440969Losses:  0.33513590693473816 0.340148389339447
MemoryTrain:  epoch  1, batch     8 | loss: 0.6752843Losses:  0.05617663636803627 0.4079018235206604
MemoryTrain:  epoch  2, batch     0 | loss: 0.4640785Losses:  0.5421590209007263 0.5457492470741272
MemoryTrain:  epoch  2, batch     1 | loss: 1.0879083Losses:  0.21867839992046356 0.44340837001800537
MemoryTrain:  epoch  2, batch     2 | loss: 0.6620868Losses:  0.24866989254951477 0.4370579421520233
MemoryTrain:  epoch  2, batch     3 | loss: 0.6857278Losses:  1.0353631973266602 0.7223291397094727
MemoryTrain:  epoch  2, batch     4 | loss: 1.7576923Losses:  0.15935340523719788 0.36388981342315674
MemoryTrain:  epoch  2, batch     5 | loss: 0.5232432Losses:  0.697693407535553 0.3145866394042969
MemoryTrain:  epoch  2, batch     6 | loss: 1.0122800Losses:  0.4321412146091461 0.3619406819343567
MemoryTrain:  epoch  2, batch     7 | loss: 0.7940819Losses:  0.22530412673950195 0.6766647100448608
MemoryTrain:  epoch  2, batch     8 | loss: 0.9019688Losses:  0.18748882412910461 0.47445106506347656
MemoryTrain:  epoch  3, batch     0 | loss: 0.6619399Losses:  0.5407836437225342 0.3397281765937805
MemoryTrain:  epoch  3, batch     1 | loss: 0.8805118Losses:  0.2703510820865631 0.5672165155410767
MemoryTrain:  epoch  3, batch     2 | loss: 0.8375676Losses:  0.11596282571554184 0.34146249294281006
MemoryTrain:  epoch  3, batch     3 | loss: 0.4574253Losses:  0.06657247990369797 0.4603574573993683
MemoryTrain:  epoch  3, batch     4 | loss: 0.5269299Losses:  0.24044641852378845 0.6912572979927063
MemoryTrain:  epoch  3, batch     5 | loss: 0.9317037Losses:  0.23937754333019257 0.46037864685058594
MemoryTrain:  epoch  3, batch     6 | loss: 0.6997562Losses:  0.08861890435218811 0.5091648101806641
MemoryTrain:  epoch  3, batch     7 | loss: 0.5977837Losses:  0.24120591580867767 0.24867211282253265
MemoryTrain:  epoch  3, batch     8 | loss: 0.4898780Losses:  0.12066502869129181 0.5209481716156006
MemoryTrain:  epoch  4, batch     0 | loss: 0.6416132Losses:  0.19040954113006592 0.3763909339904785
MemoryTrain:  epoch  4, batch     1 | loss: 0.5668005Losses:  0.21254396438598633 0.5678237676620483
MemoryTrain:  epoch  4, batch     2 | loss: 0.7803677Losses:  0.051796942949295044 0.47705718874931335
MemoryTrain:  epoch  4, batch     3 | loss: 0.5288541Losses:  0.12669993937015533 0.39271193742752075
MemoryTrain:  epoch  4, batch     4 | loss: 0.5194119Losses:  0.04897858947515488 0.29417669773101807
MemoryTrain:  epoch  4, batch     5 | loss: 0.3431553Losses:  0.08598894625902176 0.37035173177719116
MemoryTrain:  epoch  4, batch     6 | loss: 0.4563407Losses:  0.11253059655427933 0.5954000949859619
MemoryTrain:  epoch  4, batch     7 | loss: 0.7079307Losses:  0.11389943957328796 0.38254404067993164
MemoryTrain:  epoch  4, batch     8 | loss: 0.4964435Losses:  0.05164095014333725 0.3301677107810974
MemoryTrain:  epoch  5, batch     0 | loss: 0.3818087Losses:  0.08506068587303162 0.57986980676651
MemoryTrain:  epoch  5, batch     1 | loss: 0.6649305Losses:  0.09010683000087738 0.5201643705368042
MemoryTrain:  epoch  5, batch     2 | loss: 0.6102712Losses:  0.06216941401362419 0.3408305048942566
MemoryTrain:  epoch  5, batch     3 | loss: 0.4029999Losses:  0.07071982324123383 0.3678361475467682
MemoryTrain:  epoch  5, batch     4 | loss: 0.4385560Losses:  0.35034188628196716 0.49243608117103577
MemoryTrain:  epoch  5, batch     5 | loss: 0.8427780Losses:  0.10726895928382874 0.38287338614463806
MemoryTrain:  epoch  5, batch     6 | loss: 0.4901423Losses:  0.06560000032186508 0.3542666733264923
MemoryTrain:  epoch  5, batch     7 | loss: 0.4198667Losses:  0.030475899577140808 0.45071280002593994
MemoryTrain:  epoch  5, batch     8 | loss: 0.4811887Losses:  0.05734848976135254 0.5167973041534424
MemoryTrain:  epoch  6, batch     0 | loss: 0.5741458Losses:  0.04179194197058678 0.3963262736797333
MemoryTrain:  epoch  6, batch     1 | loss: 0.4381182Losses:  0.03185229003429413 0.21694430708885193
MemoryTrain:  epoch  6, batch     2 | loss: 0.2487966Losses:  0.19391080737113953 0.4518369436264038
MemoryTrain:  epoch  6, batch     3 | loss: 0.6457478Losses:  0.061183344572782516 0.380558580160141
MemoryTrain:  epoch  6, batch     4 | loss: 0.4417419Losses:  0.11045399308204651 0.4763258695602417
MemoryTrain:  epoch  6, batch     5 | loss: 0.5867798Losses:  0.02988228015601635 0.49531152844429016
MemoryTrain:  epoch  6, batch     6 | loss: 0.5251938Losses:  0.048791028559207916 0.31797704100608826
MemoryTrain:  epoch  6, batch     7 | loss: 0.3667681Losses:  0.0637865737080574 0.4219208359718323
MemoryTrain:  epoch  6, batch     8 | loss: 0.4857074Losses:  0.059749092906713486 0.36913633346557617
MemoryTrain:  epoch  7, batch     0 | loss: 0.4288854Losses:  0.03390077129006386 0.3585793972015381
MemoryTrain:  epoch  7, batch     1 | loss: 0.3924802Losses:  0.07620567083358765 0.39997929334640503
MemoryTrain:  epoch  7, batch     2 | loss: 0.4761850Losses:  0.0500996857881546 0.40230464935302734
MemoryTrain:  epoch  7, batch     3 | loss: 0.4524043Losses:  0.06656572967767715 0.335340678691864
MemoryTrain:  epoch  7, batch     4 | loss: 0.4019064Losses:  0.03309177979826927 0.48702317476272583
MemoryTrain:  epoch  7, batch     5 | loss: 0.5201150Losses:  0.0318899005651474 0.27562716603279114
MemoryTrain:  epoch  7, batch     6 | loss: 0.3075171Losses:  0.08435395359992981 0.4188673496246338
MemoryTrain:  epoch  7, batch     7 | loss: 0.5032213Losses:  0.04554164037108421 0.3793867528438568
MemoryTrain:  epoch  7, batch     8 | loss: 0.4249284Losses:  0.047911860048770905 0.35995179414749146
MemoryTrain:  epoch  8, batch     0 | loss: 0.4078636Losses:  0.03245089203119278 0.42042598128318787
MemoryTrain:  epoch  8, batch     1 | loss: 0.4528769Losses:  0.032887015491724014 0.34086382389068604
MemoryTrain:  epoch  8, batch     2 | loss: 0.3737508Losses:  0.03731989115476608 0.29707014560699463
MemoryTrain:  epoch  8, batch     3 | loss: 0.3343900Losses:  0.0507471039891243 0.4840339422225952
MemoryTrain:  epoch  8, batch     4 | loss: 0.5347810Losses:  0.05636073276400566 0.35833629965782166
MemoryTrain:  epoch  8, batch     5 | loss: 0.4146970Losses:  0.040656931698322296 0.35505080223083496
MemoryTrain:  epoch  8, batch     6 | loss: 0.3957077Losses:  0.05267386510968208 0.36811769008636475
MemoryTrain:  epoch  8, batch     7 | loss: 0.4207916Losses:  0.059642888605594635 0.6247317790985107
MemoryTrain:  epoch  8, batch     8 | loss: 0.6843747Losses:  0.03296717628836632 0.3969954550266266
MemoryTrain:  epoch  9, batch     0 | loss: 0.4299626Losses:  0.03619003668427467 0.4080788791179657
MemoryTrain:  epoch  9, batch     1 | loss: 0.4442689Losses:  0.03709608316421509 0.350737065076828
MemoryTrain:  epoch  9, batch     2 | loss: 0.3878331Losses:  0.032364070415496826 0.21311722695827484
MemoryTrain:  epoch  9, batch     3 | loss: 0.2454813Losses:  0.03432879596948624 0.366238534450531
MemoryTrain:  epoch  9, batch     4 | loss: 0.4005673Losses:  0.08441686630249023 0.6980440020561218
MemoryTrain:  epoch  9, batch     5 | loss: 0.7824609Losses:  0.041021525859832764 0.31339210271835327
MemoryTrain:  epoch  9, batch     6 | loss: 0.3544136Losses:  0.03659668192267418 0.28687119483947754
MemoryTrain:  epoch  9, batch     7 | loss: 0.3234679Losses:  0.07160909473896027 0.5267612338066101
MemoryTrain:  epoch  9, batch     8 | loss: 0.5983703
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 25.00%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 69.85%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.70%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.83%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.07%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 79.09%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 76.92%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 76.69%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 76.07%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 75.48%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 75.47%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 74.91%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 75.09%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 75.09%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 74.92%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 74.11%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 73.32%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 72.78%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 71.68%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 71.72%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 73.15%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 72.33%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 71.60%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 70.38%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 69.93%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 69.74%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 68.64%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 68.53%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 67.95%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 68.41%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 69.34%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 68.88%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 67.78%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 69.74%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 69.39%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 69.20%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 68.79%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 69.35%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 69.14%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  178 | acc: 25.00%,  total acc: 68.61%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:  180 | acc: 12.50%,  total acc: 68.02%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 67.24%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 67.18%   [EVAL] batch:  187 | acc: 12.50%,  total acc: 66.89%   [EVAL] batch:  188 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:  189 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 65.84%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  192 | acc: 0.00%,  total acc: 65.16%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.92%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.97%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 64.89%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 64.82%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 64.71%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 64.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 64.92%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 64.78%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 64.68%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 64.46%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 66.31%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 66.19%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 65.88%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 65.69%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.52%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 65.34%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 65.22%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  259 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 66.30%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 65.73%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 65.56%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 65.39%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  314 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.08%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:  345 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  347 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 69.05%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 68.82%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 68.59%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 68.47%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 68.42%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  375 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  378 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 69.50%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 69.50%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 69.43%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 69.35%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 69.81%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  403 | acc: 37.50%,  total acc: 69.63%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  407 | acc: 25.00%,  total acc: 69.42%   [EVAL] batch:  408 | acc: 0.00%,  total acc: 69.25%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 69.12%   [EVAL] batch:  410 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 68.80%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 68.77%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 68.81%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.55%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.58%   
cur_acc:  ['0.9474', '0.7460', '0.7917', '0.6389', '0.7946', '0.7252', '0.7183']
his_acc:  ['0.9474', '0.8455', '0.8022', '0.7328', '0.7222', '0.6977', '0.6958']
Clustering into  38  clusters
Clusters:  [ 1  5 24  1  1  1 33  1 21  0 32 27  3  1  1 37  1 29  1 34  2 25  1  1
 36  1  1  1 20 23 26  1 22  1 19 31 35  1  1  1 17 30  1  1 28 15 11  1
  1 18  1  1  1 12  1  0  1 13 16 14  5 10  1  1  1  2  1  8  6  9  7  1
  1  3  3  4  1  1  1  1]
Losses:  6.3683085441589355 1.2025471925735474
CurrentTrain: epoch  0, batch     0 | loss: 7.5708556Losses:  6.4387526512146 1.532392978668213
CurrentTrain: epoch  0, batch     1 | loss: 7.9711456Losses:  5.737920761108398 1.1593084335327148
CurrentTrain: epoch  0, batch     2 | loss: 6.8972292Losses:  7.324875831604004 1.4327608346939087
CurrentTrain: epoch  0, batch     3 | loss: 8.7576370Losses:  6.826520919799805 1.0143940448760986
CurrentTrain: epoch  0, batch     4 | loss: 7.8409147Losses:  6.883075714111328 1.283912181854248
CurrentTrain: epoch  0, batch     5 | loss: 8.1669884Losses:  7.384359359741211 0.427497923374176
CurrentTrain: epoch  0, batch     6 | loss: 7.8118572Losses:  6.258729934692383 1.3039612770080566
CurrentTrain: epoch  1, batch     0 | loss: 7.5626912Losses:  6.625907897949219 1.196845531463623
CurrentTrain: epoch  1, batch     1 | loss: 7.8227534Losses:  5.564717769622803 1.2689783573150635
CurrentTrain: epoch  1, batch     2 | loss: 6.8336964Losses:  4.408547401428223 1.0032048225402832
CurrentTrain: epoch  1, batch     3 | loss: 5.4117522Losses:  6.322413444519043 1.0342566967010498
CurrentTrain: epoch  1, batch     4 | loss: 7.3566704Losses:  3.9159083366394043 1.21836256980896
CurrentTrain: epoch  1, batch     5 | loss: 5.1342707Losses:  5.900237083435059 0.4009983539581299
CurrentTrain: epoch  1, batch     6 | loss: 6.3012352Losses:  4.3770060539245605 0.8810736536979675
CurrentTrain: epoch  2, batch     0 | loss: 5.2580795Losses:  6.236433029174805 1.0184142589569092
CurrentTrain: epoch  2, batch     1 | loss: 7.2548475Losses:  4.212662696838379 0.9882664084434509
CurrentTrain: epoch  2, batch     2 | loss: 5.2009292Losses:  4.963986396789551 1.1043709516525269
CurrentTrain: epoch  2, batch     3 | loss: 6.0683575Losses:  4.948204040527344 1.1792547702789307
CurrentTrain: epoch  2, batch     4 | loss: 6.1274586Losses:  3.734973430633545 0.9860097169876099
CurrentTrain: epoch  2, batch     5 | loss: 4.7209830Losses:  3.0568933486938477 0.2811668813228607
CurrentTrain: epoch  2, batch     6 | loss: 3.3380601Losses:  4.109539985656738 0.8658308386802673
CurrentTrain: epoch  3, batch     0 | loss: 4.9753709Losses:  4.098672866821289 1.2049089670181274
CurrentTrain: epoch  3, batch     1 | loss: 5.3035817Losses:  4.089346885681152 1.3007408380508423
CurrentTrain: epoch  3, batch     2 | loss: 5.3900876Losses:  4.044605255126953 1.0894339084625244
CurrentTrain: epoch  3, batch     3 | loss: 5.1340389Losses:  4.083712577819824 0.8954888582229614
CurrentTrain: epoch  3, batch     4 | loss: 4.9792013Losses:  4.05570125579834 0.8088942766189575
CurrentTrain: epoch  3, batch     5 | loss: 4.8645954Losses:  3.771514892578125 0.11525877565145493
CurrentTrain: epoch  3, batch     6 | loss: 3.8867736Losses:  2.813976764678955 0.9468454122543335
CurrentTrain: epoch  4, batch     0 | loss: 3.7608223Losses:  2.7425858974456787 0.988078236579895
CurrentTrain: epoch  4, batch     1 | loss: 3.7306643Losses:  3.8652799129486084 1.1135166883468628
CurrentTrain: epoch  4, batch     2 | loss: 4.9787965Losses:  4.212477207183838 1.3782879114151
CurrentTrain: epoch  4, batch     3 | loss: 5.5907650Losses:  4.3418169021606445 0.9406896233558655
CurrentTrain: epoch  4, batch     4 | loss: 5.2825065Losses:  3.426448345184326 0.6535741090774536
CurrentTrain: epoch  4, batch     5 | loss: 4.0800223Losses:  3.268576145172119 0.10929466784000397
CurrentTrain: epoch  4, batch     6 | loss: 3.3778708Losses:  3.113367795944214 0.8848048448562622
CurrentTrain: epoch  5, batch     0 | loss: 3.9981728Losses:  2.641735315322876 0.6098321676254272
CurrentTrain: epoch  5, batch     1 | loss: 3.2515674Losses:  2.8909802436828613 0.769659161567688
CurrentTrain: epoch  5, batch     2 | loss: 3.6606393Losses:  3.239346742630005 0.9981672167778015
CurrentTrain: epoch  5, batch     3 | loss: 4.2375140Losses:  4.311537742614746 0.9380627870559692
CurrentTrain: epoch  5, batch     4 | loss: 5.2496004Losses:  3.2809648513793945 0.9679571390151978
CurrentTrain: epoch  5, batch     5 | loss: 4.2489219Losses:  2.272987127304077 0.20507600903511047
CurrentTrain: epoch  5, batch     6 | loss: 2.4780631Losses:  2.9974937438964844 0.7305746078491211
CurrentTrain: epoch  6, batch     0 | loss: 3.7280684Losses:  3.3791730403900146 0.9129371643066406
CurrentTrain: epoch  6, batch     1 | loss: 4.2921104Losses:  3.9020934104919434 0.6990424394607544
CurrentTrain: epoch  6, batch     2 | loss: 4.6011357Losses:  2.3524818420410156 0.7485641241073608
CurrentTrain: epoch  6, batch     3 | loss: 3.1010461Losses:  2.415949821472168 0.8159914612770081
CurrentTrain: epoch  6, batch     4 | loss: 3.2319412Losses:  2.3741745948791504 0.6022641062736511
CurrentTrain: epoch  6, batch     5 | loss: 2.9764388Losses:  2.297959804534912 0.10542947053909302
CurrentTrain: epoch  6, batch     6 | loss: 2.4033892Losses:  2.566157817840576 1.0452061891555786
CurrentTrain: epoch  7, batch     0 | loss: 3.6113639Losses:  2.926783323287964 0.6875777244567871
CurrentTrain: epoch  7, batch     1 | loss: 3.6143610Losses:  2.220247507095337 0.5849014520645142
CurrentTrain: epoch  7, batch     2 | loss: 2.8051491Losses:  2.3515923023223877 0.862197995185852
CurrentTrain: epoch  7, batch     3 | loss: 3.2137904Losses:  3.875426769256592 0.43483635783195496
CurrentTrain: epoch  7, batch     4 | loss: 4.3102632Losses:  2.0451087951660156 0.3919667899608612
CurrentTrain: epoch  7, batch     5 | loss: 2.4370756Losses:  2.054309844970703 0.4479236602783203
CurrentTrain: epoch  7, batch     6 | loss: 2.5022335Losses:  2.623967170715332 0.6816495060920715
CurrentTrain: epoch  8, batch     0 | loss: 3.3056166Losses:  2.886415719985962 0.732673168182373
CurrentTrain: epoch  8, batch     1 | loss: 3.6190889Losses:  2.358042001724243 0.8137030005455017
CurrentTrain: epoch  8, batch     2 | loss: 3.1717451Losses:  1.9373795986175537 0.8715113401412964
CurrentTrain: epoch  8, batch     3 | loss: 2.8088908Losses:  2.6696009635925293 0.9121948480606079
CurrentTrain: epoch  8, batch     4 | loss: 3.5817957Losses:  2.258840560913086 0.8521175980567932
CurrentTrain: epoch  8, batch     5 | loss: 3.1109581Losses:  2.267040729522705 0.12884065508842468
CurrentTrain: epoch  8, batch     6 | loss: 2.3958814Losses:  2.450223922729492 0.9853106141090393
CurrentTrain: epoch  9, batch     0 | loss: 3.4355345Losses:  2.497617483139038 0.8628007769584656
CurrentTrain: epoch  9, batch     1 | loss: 3.3604183Losses:  1.997550368309021 0.9334211945533752
CurrentTrain: epoch  9, batch     2 | loss: 2.9309716Losses:  2.0195212364196777 0.5977932810783386
CurrentTrain: epoch  9, batch     3 | loss: 2.6173146Losses:  2.077970504760742 0.7753299474716187
CurrentTrain: epoch  9, batch     4 | loss: 2.8533006Losses:  2.1858575344085693 0.9189977645874023
CurrentTrain: epoch  9, batch     5 | loss: 3.1048553Losses:  2.877863883972168 0.19910317659378052
CurrentTrain: epoch  9, batch     6 | loss: 3.0769670
Losses:  0.30435872077941895 0.5507733821868896
MemoryTrain:  epoch  0, batch     0 | loss: 0.8551321Losses:  0.08248835057020187 0.47036099433898926
MemoryTrain:  epoch  0, batch     1 | loss: 0.5528494Losses:  0.5390934348106384 0.5659109354019165
MemoryTrain:  epoch  0, batch     2 | loss: 1.1050043Losses:  1.221100926399231 0.6887565851211548
MemoryTrain:  epoch  0, batch     3 | loss: 1.9098575Losses:  1.264225721359253 0.6498023867607117
MemoryTrain:  epoch  0, batch     4 | loss: 1.9140282Losses:  0.36050546169281006 0.4054318070411682
MemoryTrain:  epoch  0, batch     5 | loss: 0.7659373Losses:  0.9823647737503052 0.5871425867080688
MemoryTrain:  epoch  0, batch     6 | loss: 1.5695074Losses:  1.2530487775802612 0.28465530276298523
MemoryTrain:  epoch  0, batch     7 | loss: 1.5377041Losses:  0.2508907914161682 0.3908517360687256
MemoryTrain:  epoch  0, batch     8 | loss: 0.6417425Losses:  0.5433210730552673 0.3722060024738312
MemoryTrain:  epoch  0, batch     9 | loss: 0.9155271Losses:  1.0938987731933594 0.6424335241317749
MemoryTrain:  epoch  1, batch     0 | loss: 1.7363323Losses:  0.09106971323490143 0.28932470083236694
MemoryTrain:  epoch  1, batch     1 | loss: 0.3803944Losses:  0.34749582409858704 0.301835834980011
MemoryTrain:  epoch  1, batch     2 | loss: 0.6493317Losses:  0.5800215005874634 0.463543176651001
MemoryTrain:  epoch  1, batch     3 | loss: 1.0435647Losses:  1.3687974214553833 0.5639318227767944
MemoryTrain:  epoch  1, batch     4 | loss: 1.9327292Losses:  0.5593652725219727 0.2919798195362091
MemoryTrain:  epoch  1, batch     5 | loss: 0.8513451Losses:  1.110182285308838 0.49883517622947693
MemoryTrain:  epoch  1, batch     6 | loss: 1.6090175Losses:  0.6734640598297119 0.5035260915756226
MemoryTrain:  epoch  1, batch     7 | loss: 1.1769902Losses:  1.180680513381958 0.5105102062225342
MemoryTrain:  epoch  1, batch     8 | loss: 1.6911907Losses:  0.7266008257865906 0.47537076473236084
MemoryTrain:  epoch  1, batch     9 | loss: 1.2019715Losses:  0.944998025894165 0.4814569652080536
MemoryTrain:  epoch  2, batch     0 | loss: 1.4264550Losses:  0.42075324058532715 0.646521806716919
MemoryTrain:  epoch  2, batch     1 | loss: 1.0672750Losses:  0.19607658684253693 0.3508301377296448
MemoryTrain:  epoch  2, batch     2 | loss: 0.5469067Losses:  0.25649183988571167 0.3047499656677246
MemoryTrain:  epoch  2, batch     3 | loss: 0.5612418Losses:  0.08824924379587173 0.453522652387619
MemoryTrain:  epoch  2, batch     4 | loss: 0.5417719Losses:  0.05601639300584793 0.47865787148475647
MemoryTrain:  epoch  2, batch     5 | loss: 0.5346743Losses:  0.3379881978034973 0.44573086500167847
MemoryTrain:  epoch  2, batch     6 | loss: 0.7837191Losses:  0.6831159591674805 0.30947649478912354
MemoryTrain:  epoch  2, batch     7 | loss: 0.9925925Losses:  0.3263091444969177 0.4802165925502777
MemoryTrain:  epoch  2, batch     8 | loss: 0.8065257Losses:  0.3397303521633148 0.6109025478363037
MemoryTrain:  epoch  2, batch     9 | loss: 0.9506329Losses:  0.08761483430862427 0.3307030200958252
MemoryTrain:  epoch  3, batch     0 | loss: 0.4183179Losses:  0.2348756492137909 0.32755398750305176
MemoryTrain:  epoch  3, batch     1 | loss: 0.5624297Losses:  0.07867872714996338 0.37057799100875854
MemoryTrain:  epoch  3, batch     2 | loss: 0.4492567Losses:  0.0842960774898529 0.4921826720237732
MemoryTrain:  epoch  3, batch     3 | loss: 0.5764787Losses:  0.5163644552230835 0.6162892580032349
MemoryTrain:  epoch  3, batch     4 | loss: 1.1326537Losses:  0.23031190037727356 0.504898726940155
MemoryTrain:  epoch  3, batch     5 | loss: 0.7352107Losses:  0.08018295466899872 0.5292240381240845
MemoryTrain:  epoch  3, batch     6 | loss: 0.6094070Losses:  0.06537181884050369 0.38601431250572205
MemoryTrain:  epoch  3, batch     7 | loss: 0.4513861Losses:  0.10977024585008621 0.2907390296459198
MemoryTrain:  epoch  3, batch     8 | loss: 0.4005093Losses:  0.3479423224925995 0.5828538537025452
MemoryTrain:  epoch  3, batch     9 | loss: 0.9307961Losses:  0.09742196649312973 0.623306393623352
MemoryTrain:  epoch  4, batch     0 | loss: 0.7207283Losses:  0.4923570156097412 0.5621891021728516
MemoryTrain:  epoch  4, batch     1 | loss: 1.0545461Losses:  0.06346669048070908 0.3818559944629669
MemoryTrain:  epoch  4, batch     2 | loss: 0.4453227Losses:  0.02859320491552353 0.4364992082118988
MemoryTrain:  epoch  4, batch     3 | loss: 0.4650924Losses:  0.31999456882476807 0.5247125625610352
MemoryTrain:  epoch  4, batch     4 | loss: 0.8447071Losses:  0.0510924831032753 0.5769872069358826
MemoryTrain:  epoch  4, batch     5 | loss: 0.6280797Losses:  0.09658943116664886 0.532085657119751
MemoryTrain:  epoch  4, batch     6 | loss: 0.6286751Losses:  0.11561984568834305 0.39916589856147766
MemoryTrain:  epoch  4, batch     7 | loss: 0.5147858Losses:  0.08709084987640381 0.26764681935310364
MemoryTrain:  epoch  4, batch     8 | loss: 0.3547377Losses:  0.28398796916007996 0.41639024019241333
MemoryTrain:  epoch  4, batch     9 | loss: 0.7003782Losses:  0.03622489422559738 0.39026346802711487
MemoryTrain:  epoch  5, batch     0 | loss: 0.4264884Losses:  0.07434673607349396 0.4175322353839874
MemoryTrain:  epoch  5, batch     1 | loss: 0.4918790Losses:  0.07409609854221344 0.45594775676727295
MemoryTrain:  epoch  5, batch     2 | loss: 0.5300438Losses:  0.5647000670433044 0.5184977054595947
MemoryTrain:  epoch  5, batch     3 | loss: 1.0831978Losses:  0.024362141266465187 0.29650527238845825
MemoryTrain:  epoch  5, batch     4 | loss: 0.3208674Losses:  0.07301786541938782 0.6665675640106201
MemoryTrain:  epoch  5, batch     5 | loss: 0.7395854Losses:  0.10270451009273529 0.3488813042640686
MemoryTrain:  epoch  5, batch     6 | loss: 0.4515858Losses:  0.10887092351913452 0.4637000858783722
MemoryTrain:  epoch  5, batch     7 | loss: 0.5725710Losses:  0.12608152627944946 0.5425766706466675
MemoryTrain:  epoch  5, batch     8 | loss: 0.6686582Losses:  0.07113245874643326 0.3194218575954437
MemoryTrain:  epoch  5, batch     9 | loss: 0.3905543Losses:  0.0879950299859047 0.3712541460990906
MemoryTrain:  epoch  6, batch     0 | loss: 0.4592492Losses:  0.03889504075050354 0.39028674364089966
MemoryTrain:  epoch  6, batch     1 | loss: 0.4291818Losses:  0.07000228762626648 0.4723353385925293
MemoryTrain:  epoch  6, batch     2 | loss: 0.5423377Losses:  0.06365346908569336 0.28932368755340576
MemoryTrain:  epoch  6, batch     3 | loss: 0.3529772Losses:  0.04322486370801926 0.5207975506782532
MemoryTrain:  epoch  6, batch     4 | loss: 0.5640224Losses:  0.026480915024876595 0.34069159626960754
MemoryTrain:  epoch  6, batch     5 | loss: 0.3671725Losses:  0.06503376364707947 0.5167750120162964
MemoryTrain:  epoch  6, batch     6 | loss: 0.5818088Losses:  0.06761634349822998 0.501288652420044
MemoryTrain:  epoch  6, batch     7 | loss: 0.5689050Losses:  0.3530045747756958 0.39274007081985474
MemoryTrain:  epoch  6, batch     8 | loss: 0.7457446Losses:  0.04729979485273361 0.37770700454711914
MemoryTrain:  epoch  6, batch     9 | loss: 0.4250068Losses:  0.02934422716498375 0.358569473028183
MemoryTrain:  epoch  7, batch     0 | loss: 0.3879137Losses:  0.052085667848587036 0.48838990926742554
MemoryTrain:  epoch  7, batch     1 | loss: 0.5404756Losses:  0.026001039892435074 0.3522604703903198
MemoryTrain:  epoch  7, batch     2 | loss: 0.3782615Losses:  0.04050541669130325 0.4501122236251831
MemoryTrain:  epoch  7, batch     3 | loss: 0.4906176Losses:  0.07333973795175552 0.47558099031448364
MemoryTrain:  epoch  7, batch     4 | loss: 0.5489208Losses:  0.060915507376194 0.413349449634552
MemoryTrain:  epoch  7, batch     5 | loss: 0.4742649Losses:  0.05598412826657295 0.4147748351097107
MemoryTrain:  epoch  7, batch     6 | loss: 0.4707590Losses:  0.06481672823429108 0.2745245099067688
MemoryTrain:  epoch  7, batch     7 | loss: 0.3393412Losses:  0.09016913175582886 0.5290887355804443
MemoryTrain:  epoch  7, batch     8 | loss: 0.6192579Losses:  0.03589492291212082 0.3349262475967407
MemoryTrain:  epoch  7, batch     9 | loss: 0.3708212Losses:  0.09650617837905884 0.3417840600013733
MemoryTrain:  epoch  8, batch     0 | loss: 0.4382902Losses:  0.04572322964668274 0.4360908269882202
MemoryTrain:  epoch  8, batch     1 | loss: 0.4818141Losses:  0.07933792471885681 0.5324348211288452
MemoryTrain:  epoch  8, batch     2 | loss: 0.6117728Losses:  0.09730686247348785 0.4790291488170624
MemoryTrain:  epoch  8, batch     3 | loss: 0.5763360Losses:  0.038596250116825104 0.3603939414024353
MemoryTrain:  epoch  8, batch     4 | loss: 0.3989902Losses:  0.03659672662615776 0.30979031324386597
MemoryTrain:  epoch  8, batch     5 | loss: 0.3463870Losses:  0.05636448413133621 0.5613728761672974
MemoryTrain:  epoch  8, batch     6 | loss: 0.6177374Losses:  0.16253264248371124 0.4625729024410248
MemoryTrain:  epoch  8, batch     7 | loss: 0.6251056Losses:  0.02437783032655716 0.27575695514678955
MemoryTrain:  epoch  8, batch     8 | loss: 0.3001348Losses:  0.04911684989929199 0.40196430683135986
MemoryTrain:  epoch  8, batch     9 | loss: 0.4510812Losses:  0.061931781470775604 0.24012553691864014
MemoryTrain:  epoch  9, batch     0 | loss: 0.3020573Losses:  0.04437614977359772 0.39203229546546936
MemoryTrain:  epoch  9, batch     1 | loss: 0.4364085Losses:  0.0437488779425621 0.5438960790634155
MemoryTrain:  epoch  9, batch     2 | loss: 0.5876449Losses:  0.06797555088996887 0.4219146966934204
MemoryTrain:  epoch  9, batch     3 | loss: 0.4898902Losses:  0.029388301074504852 0.28365838527679443
MemoryTrain:  epoch  9, batch     4 | loss: 0.3130467Losses:  0.06300877034664154 0.47955262660980225
MemoryTrain:  epoch  9, batch     5 | loss: 0.5425614Losses:  0.028047684580087662 0.4804060459136963
MemoryTrain:  epoch  9, batch     6 | loss: 0.5084537Losses:  0.06810365617275238 0.4195023775100708
MemoryTrain:  epoch  9, batch     7 | loss: 0.4876060Losses:  0.03241695463657379 0.2621975839138031
MemoryTrain:  epoch  9, batch     8 | loss: 0.2946146Losses:  0.03045184351503849 0.24180877208709717
MemoryTrain:  epoch  9, batch     9 | loss: 0.2722606
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 56.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.09%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 52.55%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 50.67%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 48.92%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 47.71%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 46.37%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 47.07%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 48.30%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 49.26%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 52.53%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 53.62%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 54.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 58.95%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 58.47%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 57.34%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 56.38%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 55.47%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 55.10%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 54.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 54.29%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.05%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 55.54%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 56.82%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 57.25%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 57.35%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 57.44%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 57.42%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 57.71%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.58%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 57.96%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 57.44%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 47.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 79.82%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.12%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 76.37%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 75.77%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 75.76%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 75.37%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 75.09%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 75.09%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 75.09%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 74.42%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 73.70%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 72.39%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 71.80%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 71.06%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 71.11%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 72.59%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.77%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 70.97%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 70.33%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 69.77%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 69.02%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 69.31%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 68.93%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 68.51%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 68.03%   [EVAL] batch:  104 | acc: 25.00%,  total acc: 67.62%   [EVAL] batch:  105 | acc: 25.00%,  total acc: 67.22%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 66.50%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 65.54%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 65.01%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 64.83%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 64.76%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 65.82%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 65.45%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 65.09%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 64.68%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 64.28%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 63.93%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 64.43%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 64.11%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 63.83%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 63.42%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 63.11%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 62.93%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 65.80%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 65.19%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 64.97%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 64.89%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 64.33%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 64.02%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 63.42%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 63.18%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 62.69%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 62.76%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 62.69%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 62.60%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 62.53%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 62.53%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 62.59%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 62.62%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 62.78%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.74%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 62.62%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 62.35%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 62.20%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 62.09%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 64.13%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 64.23%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 64.44%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 64.60%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 64.43%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 64.29%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.09%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 63.95%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 64.31%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 64.08%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 63.87%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 63.68%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 63.48%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 63.40%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 64.15%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 64.21%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 64.28%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 64.30%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 64.23%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 64.15%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 63.96%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 63.72%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 63.53%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 63.30%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 63.16%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 62.95%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 63.79%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 63.93%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 63.89%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 65.30%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 65.31%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 66.57%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 66.52%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  345 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 66.60%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 66.46%   [EVAL] batch:  358 | acc: 12.50%,  total acc: 66.31%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 66.23%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 66.12%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 66.92%   [EVAL] batch:  375 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 67.13%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 67.37%   [EVAL] batch:  403 | acc: 18.75%,  total acc: 67.25%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 67.16%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  406 | acc: 31.25%,  total acc: 66.97%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 66.90%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 66.75%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 66.62%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 66.47%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 66.32%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 66.38%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  424 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  438 | acc: 6.25%,  total acc: 66.97%   [EVAL] batch:  439 | acc: 12.50%,  total acc: 66.85%   [EVAL] batch:  440 | acc: 0.00%,  total acc: 66.70%   [EVAL] batch:  441 | acc: 0.00%,  total acc: 66.54%   [EVAL] batch:  442 | acc: 0.00%,  total acc: 66.39%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 66.31%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  446 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:  452 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 66.90%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  458 | acc: 37.50%,  total acc: 66.79%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 66.77%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  461 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  462 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 66.39%   [EVAL] batch:  464 | acc: 6.25%,  total acc: 66.26%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 66.12%   [EVAL] batch:  466 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 65.88%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:  469 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  473 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.32%   [EVAL] batch:  483 | acc: 12.50%,  total acc: 66.21%   [EVAL] batch:  484 | acc: 12.50%,  total acc: 66.10%   [EVAL] batch:  485 | acc: 18.75%,  total acc: 66.00%   [EVAL] batch:  486 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  487 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  489 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  490 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  494 | acc: 50.00%,  total acc: 66.04%   [EVAL] batch:  495 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  498 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 66.03%   
cur_acc:  ['0.9474', '0.7460', '0.7917', '0.6389', '0.7946', '0.7252', '0.7183', '0.5744']
his_acc:  ['0.9474', '0.8455', '0.8022', '0.7328', '0.7222', '0.6977', '0.6958', '0.6603']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.597838401794434 1.4164063930511475
CurrentTrain: epoch  0, batch     0 | loss: 11.0142450Losses:  8.894979476928711 1.3646032810211182
CurrentTrain: epoch  0, batch     1 | loss: 10.2595825Losses:  9.398592948913574 1.5140857696533203
CurrentTrain: epoch  0, batch     2 | loss: 10.9126787Losses:  9.588027954101562 1.688605546951294
CurrentTrain: epoch  0, batch     3 | loss: 11.2766333Losses:  9.40865421295166 1.867559552192688
CurrentTrain: epoch  0, batch     4 | loss: 11.2762136Losses:  9.27719497680664 1.4889273643493652
CurrentTrain: epoch  0, batch     5 | loss: 10.7661228Losses:  9.461359024047852 1.4798709154129028
CurrentTrain: epoch  0, batch     6 | loss: 10.9412298Losses:  9.857559204101562 1.4645615816116333
CurrentTrain: epoch  0, batch     7 | loss: 11.3221207Losses:  8.524374961853027 1.3649654388427734
CurrentTrain: epoch  0, batch     8 | loss: 9.8893404Losses:  9.865089416503906 1.374183177947998
CurrentTrain: epoch  0, batch     9 | loss: 11.2392731Losses:  10.682893753051758 1.8071553707122803
CurrentTrain: epoch  0, batch    10 | loss: 12.4900494Losses:  10.436809539794922 1.5816354751586914
CurrentTrain: epoch  0, batch    11 | loss: 12.0184450Losses:  7.873903274536133 1.0724128484725952
CurrentTrain: epoch  0, batch    12 | loss: 8.9463158Losses:  8.99251651763916 1.201221227645874
CurrentTrain: epoch  0, batch    13 | loss: 10.1937380Losses:  7.828715801239014 1.2191553115844727
CurrentTrain: epoch  0, batch    14 | loss: 9.0478706Losses:  8.75223159790039 1.2957303524017334
CurrentTrain: epoch  0, batch    15 | loss: 10.0479622Losses:  8.035406112670898 1.1463854312896729
CurrentTrain: epoch  0, batch    16 | loss: 9.1817913Losses:  7.661079406738281 1.4058821201324463
CurrentTrain: epoch  0, batch    17 | loss: 9.0669613Losses:  8.403745651245117 1.233642816543579
CurrentTrain: epoch  0, batch    18 | loss: 9.6373882Losses:  8.239029884338379 1.3469139337539673
CurrentTrain: epoch  0, batch    19 | loss: 9.5859442Losses:  9.207815170288086 1.2188376188278198
CurrentTrain: epoch  0, batch    20 | loss: 10.4266529Losses:  8.98965072631836 1.4324049949645996
CurrentTrain: epoch  0, batch    21 | loss: 10.4220562Losses:  9.012864112854004 1.1743354797363281
CurrentTrain: epoch  0, batch    22 | loss: 10.1871996Losses:  8.658565521240234 1.3311607837677002
CurrentTrain: epoch  0, batch    23 | loss: 9.9897261Losses:  8.55103588104248 1.2256646156311035
CurrentTrain: epoch  0, batch    24 | loss: 9.7767010Losses:  8.813549041748047 1.5495600700378418
CurrentTrain: epoch  0, batch    25 | loss: 10.3631096Losses:  6.902692794799805 0.6883804798126221
CurrentTrain: epoch  0, batch    26 | loss: 7.5910730Losses:  8.833139419555664 1.2703766822814941
CurrentTrain: epoch  0, batch    27 | loss: 10.1035156Losses:  7.5369038581848145 1.226381778717041
CurrentTrain: epoch  0, batch    28 | loss: 8.7632856Losses:  9.081003189086914 1.2503480911254883
CurrentTrain: epoch  0, batch    29 | loss: 10.3313513Losses:  8.05051040649414 0.8923457264900208
CurrentTrain: epoch  0, batch    30 | loss: 8.9428558Losses:  8.480740547180176 1.1339486837387085
CurrentTrain: epoch  0, batch    31 | loss: 9.6146889Losses:  8.664555549621582 1.0400495529174805
CurrentTrain: epoch  0, batch    32 | loss: 9.7046051Losses:  8.461345672607422 1.1008179187774658
CurrentTrain: epoch  0, batch    33 | loss: 9.5621634Losses:  8.339775085449219 1.5360562801361084
CurrentTrain: epoch  0, batch    34 | loss: 9.8758316Losses:  9.203018188476562 1.4451537132263184
CurrentTrain: epoch  0, batch    35 | loss: 10.6481724Losses:  7.654363632202148 1.0387215614318848
CurrentTrain: epoch  0, batch    36 | loss: 8.6930847Losses:  8.799416542053223 1.254610538482666
CurrentTrain: epoch  0, batch    37 | loss: 10.0540276Losses:  8.40475845336914 0.9223110675811768
CurrentTrain: epoch  0, batch    38 | loss: 9.3270693Losses:  9.120172500610352 1.4156498908996582
CurrentTrain: epoch  0, batch    39 | loss: 10.5358219Losses:  7.100127220153809 0.7291601896286011
CurrentTrain: epoch  0, batch    40 | loss: 7.8292875Losses:  7.518287658691406 1.0706205368041992
CurrentTrain: epoch  0, batch    41 | loss: 8.5889082Losses:  8.824756622314453 0.815666139125824
CurrentTrain: epoch  0, batch    42 | loss: 9.6404228Losses:  8.24908447265625 1.0434966087341309
CurrentTrain: epoch  0, batch    43 | loss: 9.2925816Losses:  9.964326858520508 1.0987337827682495
CurrentTrain: epoch  0, batch    44 | loss: 11.0630608Losses:  7.507594108581543 0.9196699857711792
CurrentTrain: epoch  0, batch    45 | loss: 8.4272642Losses:  7.46405029296875 0.9696453213691711
CurrentTrain: epoch  0, batch    46 | loss: 8.4336958Losses:  9.838842391967773 1.1277042627334595
CurrentTrain: epoch  0, batch    47 | loss: 10.9665470Losses:  7.351634979248047 0.8992847204208374
CurrentTrain: epoch  0, batch    48 | loss: 8.2509193Losses:  8.237552642822266 0.7574259042739868
CurrentTrain: epoch  0, batch    49 | loss: 8.9949789Losses:  8.986493110656738 0.9383224844932556
CurrentTrain: epoch  0, batch    50 | loss: 9.9248152Losses:  8.015434265136719 0.9665043354034424
CurrentTrain: epoch  0, batch    51 | loss: 8.9819384Losses:  8.004034042358398 0.9217289686203003
CurrentTrain: epoch  0, batch    52 | loss: 8.9257631Losses:  7.135709762573242 1.0757505893707275
CurrentTrain: epoch  0, batch    53 | loss: 8.2114601Losses:  6.855114936828613 1.1999627351760864
CurrentTrain: epoch  0, batch    54 | loss: 8.0550776Losses:  8.713102340698242 1.2534428834915161
CurrentTrain: epoch  0, batch    55 | loss: 9.9665451Losses:  8.044361114501953 1.1532599925994873
CurrentTrain: epoch  0, batch    56 | loss: 9.1976213Losses:  8.674664497375488 0.7624186873435974
CurrentTrain: epoch  0, batch    57 | loss: 9.4370832Losses:  8.444587707519531 1.1090457439422607
CurrentTrain: epoch  0, batch    58 | loss: 9.5536337Losses:  8.223745346069336 1.0976526737213135
CurrentTrain: epoch  0, batch    59 | loss: 9.3213978Losses:  7.35546875 0.8899168968200684
CurrentTrain: epoch  0, batch    60 | loss: 8.2453861Losses:  8.272421836853027 0.8059508204460144
CurrentTrain: epoch  0, batch    61 | loss: 9.0783730Losses:  7.69320011138916 0.9910369515419006
CurrentTrain: epoch  0, batch    62 | loss: 8.6842375Losses:  8.142986297607422 1.1374764442443848
CurrentTrain: epoch  0, batch    63 | loss: 9.2804623Losses:  8.49383544921875 0.9650989174842834
CurrentTrain: epoch  0, batch    64 | loss: 9.4589348Losses:  6.4200944900512695 0.5523016452789307
CurrentTrain: epoch  0, batch    65 | loss: 6.9723959Losses:  8.199460983276367 1.3228862285614014
CurrentTrain: epoch  0, batch    66 | loss: 9.5223475Losses:  8.430354118347168 0.7589655518531799
CurrentTrain: epoch  0, batch    67 | loss: 9.1893196Losses:  6.574224472045898 0.7412279844284058
CurrentTrain: epoch  0, batch    68 | loss: 7.3154526Losses:  7.718901634216309 1.0414563417434692
CurrentTrain: epoch  0, batch    69 | loss: 8.7603579Losses:  8.918270111083984 0.7488877177238464
CurrentTrain: epoch  0, batch    70 | loss: 9.6671581Losses:  7.266404151916504 0.8305894136428833
CurrentTrain: epoch  0, batch    71 | loss: 8.0969934Losses:  7.343870639801025 0.936700701713562
CurrentTrain: epoch  0, batch    72 | loss: 8.2805710Losses:  7.578603267669678 0.9255034327507019
CurrentTrain: epoch  0, batch    73 | loss: 8.5041065Losses:  7.900091171264648 0.9010607004165649
CurrentTrain: epoch  0, batch    74 | loss: 8.8011522Losses:  6.665496826171875 0.9910656213760376
CurrentTrain: epoch  0, batch    75 | loss: 7.6565623Losses:  8.009510040283203 0.6282854676246643
CurrentTrain: epoch  0, batch    76 | loss: 8.6377954Losses:  8.336054801940918 0.9381387829780579
CurrentTrain: epoch  0, batch    77 | loss: 9.2741938Losses:  8.837360382080078 1.1330467462539673
CurrentTrain: epoch  0, batch    78 | loss: 9.9704075Losses:  7.249988555908203 1.1148087978363037
CurrentTrain: epoch  0, batch    79 | loss: 8.3647976Losses:  7.090046405792236 0.4469749331474304
CurrentTrain: epoch  0, batch    80 | loss: 7.5370212Losses:  7.6019415855407715 0.8238532543182373
CurrentTrain: epoch  0, batch    81 | loss: 8.4257946Losses:  7.823665618896484 0.9830712080001831
CurrentTrain: epoch  0, batch    82 | loss: 8.8067369Losses:  6.412297248840332 0.7126400470733643
CurrentTrain: epoch  0, batch    83 | loss: 7.1249371Losses:  7.56173038482666 0.9064381122589111
CurrentTrain: epoch  0, batch    84 | loss: 8.4681683Losses:  7.079034805297852 0.8407672047615051
CurrentTrain: epoch  0, batch    85 | loss: 7.9198022Losses:  7.9680891036987305 0.9425441026687622
CurrentTrain: epoch  0, batch    86 | loss: 8.9106331Losses:  8.131128311157227 0.8091957569122314
CurrentTrain: epoch  0, batch    87 | loss: 8.9403238Losses:  7.706543922424316 0.687187910079956
CurrentTrain: epoch  0, batch    88 | loss: 8.3937321Losses:  8.049067497253418 0.9547563791275024
CurrentTrain: epoch  0, batch    89 | loss: 9.0038242Losses:  6.468841552734375 0.8194370269775391
CurrentTrain: epoch  0, batch    90 | loss: 7.2882786Losses:  7.091827869415283 0.7160021066665649
CurrentTrain: epoch  0, batch    91 | loss: 7.8078299Losses:  7.17855978012085 0.9512914419174194
CurrentTrain: epoch  0, batch    92 | loss: 8.1298513Losses:  6.869239330291748 0.7594355344772339
CurrentTrain: epoch  0, batch    93 | loss: 7.6286750Losses:  7.553431510925293 0.8611282110214233
CurrentTrain: epoch  0, batch    94 | loss: 8.4145594Losses:  8.37771224975586 0.3629906177520752
CurrentTrain: epoch  0, batch    95 | loss: 8.7407026Losses:  8.041814804077148 0.8561093807220459
CurrentTrain: epoch  0, batch    96 | loss: 8.8979244Losses:  7.484299182891846 0.7518080472946167
CurrentTrain: epoch  0, batch    97 | loss: 8.2361069Losses:  7.144219398498535 0.4758383631706238
CurrentTrain: epoch  0, batch    98 | loss: 7.6200576Losses:  8.386752128601074 0.8070777654647827
CurrentTrain: epoch  0, batch    99 | loss: 9.1938295Losses:  8.395700454711914 0.9130442142486572
CurrentTrain: epoch  0, batch   100 | loss: 9.3087444Losses:  6.568811893463135 0.8852570056915283
CurrentTrain: epoch  0, batch   101 | loss: 7.4540691Losses:  6.7691450119018555 0.6694334745407104
CurrentTrain: epoch  0, batch   102 | loss: 7.4385786Losses:  6.8109636306762695 0.7751072645187378
CurrentTrain: epoch  0, batch   103 | loss: 7.5860710Losses:  6.732949733734131 0.7400450706481934
CurrentTrain: epoch  0, batch   104 | loss: 7.4729948Losses:  6.674526691436768 0.8363968133926392
CurrentTrain: epoch  0, batch   105 | loss: 7.5109234Losses:  6.760887622833252 1.032663106918335
CurrentTrain: epoch  0, batch   106 | loss: 7.7935505Losses:  6.333828926086426 0.5993958711624146
CurrentTrain: epoch  0, batch   107 | loss: 6.9332247Losses:  8.60154914855957 0.9377176761627197
CurrentTrain: epoch  0, batch   108 | loss: 9.5392666Losses:  6.63617467880249 0.8166860938072205
CurrentTrain: epoch  0, batch   109 | loss: 7.4528608Losses:  7.477928161621094 0.8795933127403259
CurrentTrain: epoch  0, batch   110 | loss: 8.3575211Losses:  6.286282539367676 0.6475486159324646
CurrentTrain: epoch  0, batch   111 | loss: 6.9338312Losses:  6.482396125793457 0.5724837779998779
CurrentTrain: epoch  0, batch   112 | loss: 7.0548801Losses:  6.072492599487305 0.8039196133613586
CurrentTrain: epoch  0, batch   113 | loss: 6.8764124Losses:  6.509423732757568 0.4143298864364624
CurrentTrain: epoch  0, batch   114 | loss: 6.9237537Losses:  7.29879093170166 0.6395213603973389
CurrentTrain: epoch  0, batch   115 | loss: 7.9383125Losses:  6.873592376708984 0.8610149621963501
CurrentTrain: epoch  0, batch   116 | loss: 7.7346072Losses:  7.130929946899414 0.8607343435287476
CurrentTrain: epoch  0, batch   117 | loss: 7.9916644Losses:  7.214893341064453 0.5633061528205872
CurrentTrain: epoch  0, batch   118 | loss: 7.7781997Losses:  6.676713943481445 0.6443682909011841
CurrentTrain: epoch  0, batch   119 | loss: 7.3210821Losses:  7.559796333312988 0.9236093163490295
CurrentTrain: epoch  0, batch   120 | loss: 8.4834061Losses:  7.361254692077637 0.6147977709770203
CurrentTrain: epoch  0, batch   121 | loss: 7.9760523Losses:  6.606261253356934 0.7992541790008545
CurrentTrain: epoch  0, batch   122 | loss: 7.4055157Losses:  5.397095680236816 0.3140248954296112
CurrentTrain: epoch  0, batch   123 | loss: 5.7111206Losses:  6.817373275756836 0.4707653820514679
CurrentTrain: epoch  0, batch   124 | loss: 7.2881389Losses:  7.013315677642822 0.5485826730728149
CurrentTrain: epoch  1, batch     0 | loss: 7.5618982Losses:  7.134897708892822 0.7779728174209595
CurrentTrain: epoch  1, batch     1 | loss: 7.9128704Losses:  5.997281074523926 0.6642389893531799
CurrentTrain: epoch  1, batch     2 | loss: 6.6615200Losses:  6.745724678039551 0.6736800074577332
CurrentTrain: epoch  1, batch     3 | loss: 7.4194045Losses:  7.187105655670166 0.7106492519378662
CurrentTrain: epoch  1, batch     4 | loss: 7.8977547Losses:  6.543759346008301 0.7672332525253296
CurrentTrain: epoch  1, batch     5 | loss: 7.3109927Losses:  7.45869779586792 0.509878396987915
CurrentTrain: epoch  1, batch     6 | loss: 7.9685764Losses:  5.9072370529174805 0.6545028686523438
CurrentTrain: epoch  1, batch     7 | loss: 6.5617399Losses:  6.83601188659668 0.7693357467651367
CurrentTrain: epoch  1, batch     8 | loss: 7.6053476Losses:  5.8809614181518555 0.2954182028770447
CurrentTrain: epoch  1, batch     9 | loss: 6.1763797Losses:  5.869009017944336 0.587958574295044
CurrentTrain: epoch  1, batch    10 | loss: 6.4569674Losses:  6.518813133239746 0.5695755481719971
CurrentTrain: epoch  1, batch    11 | loss: 7.0883884Losses:  6.067491054534912 0.5640969276428223
CurrentTrain: epoch  1, batch    12 | loss: 6.6315880Losses:  6.086735248565674 0.6839960217475891
CurrentTrain: epoch  1, batch    13 | loss: 6.7707314Losses:  6.898439407348633 0.4680640399456024
CurrentTrain: epoch  1, batch    14 | loss: 7.3665032Losses:  7.119935512542725 0.6172783374786377
CurrentTrain: epoch  1, batch    15 | loss: 7.7372141Losses:  5.564785480499268 0.3338158130645752
CurrentTrain: epoch  1, batch    16 | loss: 5.8986015Losses:  6.0513691902160645 0.6502895355224609
CurrentTrain: epoch  1, batch    17 | loss: 6.7016587Losses:  7.0052103996276855 0.42021945118904114
CurrentTrain: epoch  1, batch    18 | loss: 7.4254298Losses:  6.665046691894531 0.5487704277038574
CurrentTrain: epoch  1, batch    19 | loss: 7.2138171Losses:  7.850931644439697 0.625378429889679
CurrentTrain: epoch  1, batch    20 | loss: 8.4763098Losses:  6.073744773864746 0.6881895661354065
CurrentTrain: epoch  1, batch    21 | loss: 6.7619343Losses:  6.202151775360107 0.509711503982544
CurrentTrain: epoch  1, batch    22 | loss: 6.7118635Losses:  6.340141773223877 0.5714231133460999
CurrentTrain: epoch  1, batch    23 | loss: 6.9115648Losses:  6.598222732543945 0.8121016025543213
CurrentTrain: epoch  1, batch    24 | loss: 7.4103241Losses:  6.898349285125732 0.5308897495269775
CurrentTrain: epoch  1, batch    25 | loss: 7.4292393Losses:  6.744948387145996 0.548758864402771
CurrentTrain: epoch  1, batch    26 | loss: 7.2937074Losses:  6.64384651184082 0.6117298603057861
CurrentTrain: epoch  1, batch    27 | loss: 7.2555761Losses:  7.348275184631348 0.7019169330596924
CurrentTrain: epoch  1, batch    28 | loss: 8.0501919Losses:  5.940922737121582 0.5077674984931946
CurrentTrain: epoch  1, batch    29 | loss: 6.4486904Losses:  5.498074531555176 0.2971947193145752
CurrentTrain: epoch  1, batch    30 | loss: 5.7952690Losses:  6.8382368087768555 0.42016512155532837
CurrentTrain: epoch  1, batch    31 | loss: 7.2584019Losses:  7.427233695983887 0.5393235683441162
CurrentTrain: epoch  1, batch    32 | loss: 7.9665575Losses:  7.237129211425781 0.6185244917869568
CurrentTrain: epoch  1, batch    33 | loss: 7.8556538Losses:  6.131715774536133 0.5854867696762085
CurrentTrain: epoch  1, batch    34 | loss: 6.7172027Losses:  7.873321533203125 1.0651553869247437
CurrentTrain: epoch  1, batch    35 | loss: 8.9384766Losses:  8.275115966796875 0.9823952913284302
CurrentTrain: epoch  1, batch    36 | loss: 9.2575111Losses:  6.804623126983643 0.6915217041969299
CurrentTrain: epoch  1, batch    37 | loss: 7.4961448Losses:  7.85573673248291 0.46872425079345703
CurrentTrain: epoch  1, batch    38 | loss: 8.3244610Losses:  7.1992340087890625 0.5783860683441162
CurrentTrain: epoch  1, batch    39 | loss: 7.7776203Losses:  5.685654640197754 0.3989253044128418
CurrentTrain: epoch  1, batch    40 | loss: 6.0845799Losses:  6.721585273742676 0.35419991612434387
CurrentTrain: epoch  1, batch    41 | loss: 7.0757852Losses:  6.566988945007324 0.6110563278198242
CurrentTrain: epoch  1, batch    42 | loss: 7.1780453Losses:  6.57328462600708 0.8306863307952881
CurrentTrain: epoch  1, batch    43 | loss: 7.4039707Losses:  6.120051860809326 0.5548869967460632
CurrentTrain: epoch  1, batch    44 | loss: 6.6749387Losses:  5.662990093231201 0.31701692938804626
CurrentTrain: epoch  1, batch    45 | loss: 5.9800072Losses:  6.222016334533691 0.4872858226299286
CurrentTrain: epoch  1, batch    46 | loss: 6.7093019Losses:  6.020825386047363 0.6817253232002258
CurrentTrain: epoch  1, batch    47 | loss: 6.7025509Losses:  6.022449016571045 0.42744195461273193
CurrentTrain: epoch  1, batch    48 | loss: 6.4498911Losses:  6.868865013122559 0.7007585763931274
CurrentTrain: epoch  1, batch    49 | loss: 7.5696235Losses:  6.1659393310546875 0.4589441120624542
CurrentTrain: epoch  1, batch    50 | loss: 6.6248837Losses:  6.208639144897461 0.5968537330627441
CurrentTrain: epoch  1, batch    51 | loss: 6.8054929Losses:  6.689702033996582 0.5982193946838379
CurrentTrain: epoch  1, batch    52 | loss: 7.2879214Losses:  5.38767147064209 0.3178246021270752
CurrentTrain: epoch  1, batch    53 | loss: 5.7054958Losses:  6.281600475311279 0.7012761831283569
CurrentTrain: epoch  1, batch    54 | loss: 6.9828768Losses:  5.365866184234619 0.637080192565918
CurrentTrain: epoch  1, batch    55 | loss: 6.0029464Losses:  5.724225997924805 0.393166720867157
CurrentTrain: epoch  1, batch    56 | loss: 6.1173925Losses:  5.87822961807251 0.49290531873703003
CurrentTrain: epoch  1, batch    57 | loss: 6.3711348Losses:  6.430388927459717 0.3703242540359497
CurrentTrain: epoch  1, batch    58 | loss: 6.8007131Losses:  6.625031471252441 0.5744900107383728
CurrentTrain: epoch  1, batch    59 | loss: 7.1995215Losses:  6.3278489112854 0.42353206872940063
CurrentTrain: epoch  1, batch    60 | loss: 6.7513809Losses:  5.899456977844238 0.35645580291748047
CurrentTrain: epoch  1, batch    61 | loss: 6.2559128Losses:  5.477352142333984 0.30282139778137207
CurrentTrain: epoch  1, batch    62 | loss: 5.7801733Losses:  6.080068588256836 0.5673588514328003
CurrentTrain: epoch  1, batch    63 | loss: 6.6474276Losses:  5.716633319854736 0.4622870683670044
CurrentTrain: epoch  1, batch    64 | loss: 6.1789203Losses:  5.739279747009277 0.47050711512565613
CurrentTrain: epoch  1, batch    65 | loss: 6.2097869Losses:  5.869935035705566 0.5606804490089417
CurrentTrain: epoch  1, batch    66 | loss: 6.4306154Losses:  5.180290222167969 0.34211108088493347
CurrentTrain: epoch  1, batch    67 | loss: 5.5224013Losses:  4.999670028686523 0.5419601798057556
CurrentTrain: epoch  1, batch    68 | loss: 5.5416303Losses:  5.467227935791016 0.2553423047065735
CurrentTrain: epoch  1, batch    69 | loss: 5.7225704Losses:  5.828963279724121 0.3428989052772522
CurrentTrain: epoch  1, batch    70 | loss: 6.1718621Losses:  5.849477291107178 0.5391736030578613
CurrentTrain: epoch  1, batch    71 | loss: 6.3886509Losses:  6.137948036193848 0.5119374394416809
CurrentTrain: epoch  1, batch    72 | loss: 6.6498857Losses:  5.303062438964844 0.5107817649841309
CurrentTrain: epoch  1, batch    73 | loss: 5.8138442Losses:  6.145610809326172 0.38381868600845337
CurrentTrain: epoch  1, batch    74 | loss: 6.5294294Losses:  6.941421031951904 0.45134246349334717
CurrentTrain: epoch  1, batch    75 | loss: 7.3927636Losses:  5.575693130493164 0.3980022072792053
CurrentTrain: epoch  1, batch    76 | loss: 5.9736953Losses:  5.814238548278809 0.3209391236305237
CurrentTrain: epoch  1, batch    77 | loss: 6.1351776Losses:  5.91551399230957 0.3161427974700928
CurrentTrain: epoch  1, batch    78 | loss: 6.2316570Losses:  5.68609619140625 0.3479255437850952
CurrentTrain: epoch  1, batch    79 | loss: 6.0340219Losses:  6.249411106109619 0.5744607448577881
CurrentTrain: epoch  1, batch    80 | loss: 6.8238716Losses:  6.449556350708008 0.5191025733947754
CurrentTrain: epoch  1, batch    81 | loss: 6.9686589Losses:  7.63024377822876 0.5652201175689697
CurrentTrain: epoch  1, batch    82 | loss: 8.1954641Losses:  5.513164520263672 0.4316061735153198
CurrentTrain: epoch  1, batch    83 | loss: 5.9447708Losses:  6.41503381729126 0.8207690715789795
CurrentTrain: epoch  1, batch    84 | loss: 7.2358027Losses:  5.93009614944458 0.518126368522644
CurrentTrain: epoch  1, batch    85 | loss: 6.4482226Losses:  5.9767961502075195 0.39511001110076904
CurrentTrain: epoch  1, batch    86 | loss: 6.3719063Losses:  4.914069175720215 0.39815327525138855
CurrentTrain: epoch  1, batch    87 | loss: 5.3122225Losses:  5.885241508483887 0.5152952671051025
CurrentTrain: epoch  1, batch    88 | loss: 6.4005365Losses:  5.488806247711182 0.4855690598487854
CurrentTrain: epoch  1, batch    89 | loss: 5.9743752Losses:  5.554708003997803 0.29211902618408203
CurrentTrain: epoch  1, batch    90 | loss: 5.8468270Losses:  5.44224739074707 0.3200162649154663
CurrentTrain: epoch  1, batch    91 | loss: 5.7622638Losses:  6.6192193031311035 0.6126868724822998
CurrentTrain: epoch  1, batch    92 | loss: 7.2319059Losses:  5.297430992126465 0.5317294001579285
CurrentTrain: epoch  1, batch    93 | loss: 5.8291602Losses:  4.86606502532959 0.2444053292274475
CurrentTrain: epoch  1, batch    94 | loss: 5.1104703Losses:  5.276953220367432 0.41482776403427124
CurrentTrain: epoch  1, batch    95 | loss: 5.6917810Losses:  5.219710350036621 0.5092880725860596
CurrentTrain: epoch  1, batch    96 | loss: 5.7289982Losses:  5.393762588500977 0.517570436000824
CurrentTrain: epoch  1, batch    97 | loss: 5.9113331Losses:  5.690187454223633 0.5064675807952881
CurrentTrain: epoch  1, batch    98 | loss: 6.1966553Losses:  5.054413318634033 0.47059324383735657
CurrentTrain: epoch  1, batch    99 | loss: 5.5250068Losses:  4.923833847045898 0.41040676832199097
CurrentTrain: epoch  1, batch   100 | loss: 5.3342404Losses:  5.344733238220215 0.3884568214416504
CurrentTrain: epoch  1, batch   101 | loss: 5.7331901Losses:  5.288858413696289 0.4776429831981659
CurrentTrain: epoch  1, batch   102 | loss: 5.7665014Losses:  6.098049163818359 0.36820805072784424
CurrentTrain: epoch  1, batch   103 | loss: 6.4662571Losses:  5.163262367248535 0.24552719295024872
CurrentTrain: epoch  1, batch   104 | loss: 5.4087896Losses:  5.725038528442383 0.6038233041763306
CurrentTrain: epoch  1, batch   105 | loss: 6.3288617Losses:  5.429311752319336 0.5200194120407104
CurrentTrain: epoch  1, batch   106 | loss: 5.9493313Losses:  5.377326011657715 0.39602130651474
CurrentTrain: epoch  1, batch   107 | loss: 5.7733474Losses:  5.198159217834473 0.3183186948299408
CurrentTrain: epoch  1, batch   108 | loss: 5.5164781Losses:  5.131936073303223 0.448070228099823
CurrentTrain: epoch  1, batch   109 | loss: 5.5800061Losses:  4.655768394470215 0.3588348627090454
CurrentTrain: epoch  1, batch   110 | loss: 5.0146031Losses:  5.530508041381836 0.46427929401397705
CurrentTrain: epoch  1, batch   111 | loss: 5.9947872Losses:  5.90472412109375 0.5493013858795166
CurrentTrain: epoch  1, batch   112 | loss: 6.4540253Losses:  6.164514541625977 0.5831078290939331
CurrentTrain: epoch  1, batch   113 | loss: 6.7476225Losses:  6.06153678894043 0.34349989891052246
CurrentTrain: epoch  1, batch   114 | loss: 6.4050369Losses:  5.058535099029541 0.47159481048583984
CurrentTrain: epoch  1, batch   115 | loss: 5.5301299Losses:  6.01509952545166 0.36492741107940674
CurrentTrain: epoch  1, batch   116 | loss: 6.3800268Losses:  5.643619060516357 0.39251619577407837
CurrentTrain: epoch  1, batch   117 | loss: 6.0361352Losses:  4.827023506164551 0.3863750100135803
CurrentTrain: epoch  1, batch   118 | loss: 5.2133985Losses:  5.424361228942871 0.4546419084072113
CurrentTrain: epoch  1, batch   119 | loss: 5.8790030Losses:  5.623470306396484 0.3023342490196228
CurrentTrain: epoch  1, batch   120 | loss: 5.9258046Losses:  5.327835559844971 0.2358098179101944
CurrentTrain: epoch  1, batch   121 | loss: 5.5636454Losses:  4.893588066101074 0.32640665769577026
CurrentTrain: epoch  1, batch   122 | loss: 5.2199945Losses:  5.555126190185547 0.5470778942108154
CurrentTrain: epoch  1, batch   123 | loss: 6.1022043Losses:  5.213064193725586 0.5323349833488464
CurrentTrain: epoch  1, batch   124 | loss: 5.7453990Losses:  5.5132856369018555 0.38068753480911255
CurrentTrain: epoch  2, batch     0 | loss: 5.8939734Losses:  4.895638465881348 0.29745274782180786
CurrentTrain: epoch  2, batch     1 | loss: 5.1930914Losses:  4.961445331573486 0.307309627532959
CurrentTrain: epoch  2, batch     2 | loss: 5.2687550Losses:  5.106507301330566 0.2869490385055542
CurrentTrain: epoch  2, batch     3 | loss: 5.3934565Losses:  4.998909950256348 0.27657079696655273
CurrentTrain: epoch  2, batch     4 | loss: 5.2754807Losses:  5.405950546264648 0.3604007363319397
CurrentTrain: epoch  2, batch     5 | loss: 5.7663512Losses:  4.5664167404174805 0.3031421899795532
CurrentTrain: epoch  2, batch     6 | loss: 4.8695588Losses:  4.861964225769043 0.2071368545293808
CurrentTrain: epoch  2, batch     7 | loss: 5.0691009Losses:  4.744639873504639 0.38115647435188293
CurrentTrain: epoch  2, batch     8 | loss: 5.1257963Losses:  5.698431968688965 0.3585175573825836
CurrentTrain: epoch  2, batch     9 | loss: 6.0569496Losses:  4.629218101501465 0.3229672908782959
CurrentTrain: epoch  2, batch    10 | loss: 4.9521856Losses:  5.289061546325684 0.35693812370300293
CurrentTrain: epoch  2, batch    11 | loss: 5.6459999Losses:  6.427148818969727 0.4020472764968872
CurrentTrain: epoch  2, batch    12 | loss: 6.8291960Losses:  4.473155498504639 0.21280215680599213
CurrentTrain: epoch  2, batch    13 | loss: 4.6859574Losses:  5.236594200134277 0.3572191596031189
CurrentTrain: epoch  2, batch    14 | loss: 5.5938134Losses:  4.560083866119385 0.22998923063278198
CurrentTrain: epoch  2, batch    15 | loss: 4.7900729Losses:  5.680599212646484 0.5518963932991028
CurrentTrain: epoch  2, batch    16 | loss: 6.2324958Losses:  4.457249641418457 0.2960737347602844
CurrentTrain: epoch  2, batch    17 | loss: 4.7533236Losses:  4.844085693359375 0.5016839504241943
CurrentTrain: epoch  2, batch    18 | loss: 5.3457699Losses:  5.332681655883789 0.21656997501850128
CurrentTrain: epoch  2, batch    19 | loss: 5.5492516Losses:  5.0806169509887695 0.3070985674858093
CurrentTrain: epoch  2, batch    20 | loss: 5.3877153Losses:  5.029600620269775 0.33574679493904114
CurrentTrain: epoch  2, batch    21 | loss: 5.3653474Losses:  6.190451622009277 0.44466838240623474
CurrentTrain: epoch  2, batch    22 | loss: 6.6351199Losses:  4.549047470092773 0.41607579588890076
CurrentTrain: epoch  2, batch    23 | loss: 4.9651232Losses:  4.800504684448242 0.23880967497825623
CurrentTrain: epoch  2, batch    24 | loss: 5.0393143Losses:  4.853094100952148 0.3141673803329468
CurrentTrain: epoch  2, batch    25 | loss: 5.1672616Losses:  4.7983293533325195 0.19355988502502441
CurrentTrain: epoch  2, batch    26 | loss: 4.9918890Losses:  4.962856292724609 0.3251960575580597
CurrentTrain: epoch  2, batch    27 | loss: 5.2880526Losses:  4.95407247543335 0.1892000138759613
CurrentTrain: epoch  2, batch    28 | loss: 5.1432724Losses:  4.8016204833984375 0.20450100302696228
CurrentTrain: epoch  2, batch    29 | loss: 5.0061216Losses:  5.22563362121582 0.2922680675983429
CurrentTrain: epoch  2, batch    30 | loss: 5.5179019Losses:  4.819289207458496 0.3874994218349457
CurrentTrain: epoch  2, batch    31 | loss: 5.2067885Losses:  4.743784427642822 0.3540154695510864
CurrentTrain: epoch  2, batch    32 | loss: 5.0977998Losses:  5.330792427062988 0.4134521484375
CurrentTrain: epoch  2, batch    33 | loss: 5.7442446Losses:  4.735532760620117 0.25217729806900024
CurrentTrain: epoch  2, batch    34 | loss: 4.9877100Losses:  5.161862373352051 0.321601927280426
CurrentTrain: epoch  2, batch    35 | loss: 5.4834642Losses:  4.953242301940918 0.25453823804855347
CurrentTrain: epoch  2, batch    36 | loss: 5.2077804Losses:  5.309365272521973 0.2947537899017334
CurrentTrain: epoch  2, batch    37 | loss: 5.6041193Losses:  5.629136085510254 0.31742554903030396
CurrentTrain: epoch  2, batch    38 | loss: 5.9465618Losses:  5.022879123687744 0.1806141436100006
CurrentTrain: epoch  2, batch    39 | loss: 5.2034931Losses:  4.560957908630371 0.20882338285446167
CurrentTrain: epoch  2, batch    40 | loss: 4.7697811Losses:  4.8279008865356445 0.2952266335487366
CurrentTrain: epoch  2, batch    41 | loss: 5.1231275Losses:  4.5956315994262695 0.28221070766448975
CurrentTrain: epoch  2, batch    42 | loss: 4.8778424Losses:  4.941070079803467 0.3808874189853668
CurrentTrain: epoch  2, batch    43 | loss: 5.3219576Losses:  5.130271911621094 0.260994553565979
CurrentTrain: epoch  2, batch    44 | loss: 5.3912663Losses:  4.826229095458984 0.38786089420318604
CurrentTrain: epoch  2, batch    45 | loss: 5.2140899Losses:  5.2518439292907715 0.2973877489566803
CurrentTrain: epoch  2, batch    46 | loss: 5.5492315Losses:  4.61597204208374 0.1086275652050972
CurrentTrain: epoch  2, batch    47 | loss: 4.7245998Losses:  4.80650520324707 0.24190722405910492
CurrentTrain: epoch  2, batch    48 | loss: 5.0484123Losses:  5.506372451782227 0.273853063583374
CurrentTrain: epoch  2, batch    49 | loss: 5.7802258Losses:  4.935579299926758 0.24311591684818268
CurrentTrain: epoch  2, batch    50 | loss: 5.1786952Losses:  4.572565078735352 0.3475567102432251
CurrentTrain: epoch  2, batch    51 | loss: 4.9201217Losses:  4.670413970947266 0.3440250754356384
CurrentTrain: epoch  2, batch    52 | loss: 5.0144391Losses:  4.972892761230469 0.2385164499282837
CurrentTrain: epoch  2, batch    53 | loss: 5.2114091Losses:  4.668591499328613 0.29140567779541016
CurrentTrain: epoch  2, batch    54 | loss: 4.9599972Losses:  6.733761787414551 0.23675695061683655
CurrentTrain: epoch  2, batch    55 | loss: 6.9705186Losses:  4.8444952964782715 0.26079899072647095
CurrentTrain: epoch  2, batch    56 | loss: 5.1052942Losses:  4.406196117401123 0.36734798550605774
CurrentTrain: epoch  2, batch    57 | loss: 4.7735443Losses:  4.545209884643555 0.3688412308692932
CurrentTrain: epoch  2, batch    58 | loss: 4.9140511Losses:  4.61588191986084 0.240056574344635
CurrentTrain: epoch  2, batch    59 | loss: 4.8559384Losses:  7.0492682456970215 0.1780375838279724
CurrentTrain: epoch  2, batch    60 | loss: 7.2273059Losses:  4.6948418617248535 0.20091311633586884
CurrentTrain: epoch  2, batch    61 | loss: 4.8957548Losses:  5.4243011474609375 0.307955265045166
CurrentTrain: epoch  2, batch    62 | loss: 5.7322564Losses:  5.083970069885254 0.34806597232818604
CurrentTrain: epoch  2, batch    63 | loss: 5.4320359Losses:  4.896978378295898 0.21791589260101318
CurrentTrain: epoch  2, batch    64 | loss: 5.1148944Losses:  5.166898250579834 0.2568909525871277
CurrentTrain: epoch  2, batch    65 | loss: 5.4237890Losses:  4.74689245223999 0.14150375127792358
CurrentTrain: epoch  2, batch    66 | loss: 4.8883963Losses:  4.735372543334961 0.28544026613235474
CurrentTrain: epoch  2, batch    67 | loss: 5.0208130Losses:  5.336827278137207 0.23190924525260925
CurrentTrain: epoch  2, batch    68 | loss: 5.5687366Losses:  4.703394889831543 0.31828737258911133
CurrentTrain: epoch  2, batch    69 | loss: 5.0216823Losses:  5.407745838165283 0.3420863449573517
CurrentTrain: epoch  2, batch    70 | loss: 5.7498322Losses:  5.273439884185791 0.2994537651538849
CurrentTrain: epoch  2, batch    71 | loss: 5.5728936Losses:  4.694899082183838 0.3083573579788208
CurrentTrain: epoch  2, batch    72 | loss: 5.0032563Losses:  4.809741020202637 0.35636818408966064
CurrentTrain: epoch  2, batch    73 | loss: 5.1661091Losses:  5.607176780700684 0.47673240303993225
CurrentTrain: epoch  2, batch    74 | loss: 6.0839090Losses:  4.613552093505859 0.2890418767929077
CurrentTrain: epoch  2, batch    75 | loss: 4.9025941Losses:  4.808399677276611 0.3420519232749939
CurrentTrain: epoch  2, batch    76 | loss: 5.1504517Losses:  4.657279968261719 0.4702354073524475
CurrentTrain: epoch  2, batch    77 | loss: 5.1275153Losses:  4.76678991317749 0.26633358001708984
CurrentTrain: epoch  2, batch    78 | loss: 5.0331235Losses:  4.950578212738037 0.238864004611969
CurrentTrain: epoch  2, batch    79 | loss: 5.1894422Losses:  4.45729923248291 0.1876010000705719
CurrentTrain: epoch  2, batch    80 | loss: 4.6449003Losses:  4.807038307189941 0.18885350227355957
CurrentTrain: epoch  2, batch    81 | loss: 4.9958916Losses:  4.988414764404297 0.2595493197441101
CurrentTrain: epoch  2, batch    82 | loss: 5.2479639Losses:  4.7144951820373535 0.24394221603870392
CurrentTrain: epoch  2, batch    83 | loss: 4.9584374Losses:  4.672938346862793 0.12536725401878357
CurrentTrain: epoch  2, batch    84 | loss: 4.7983055Losses:  4.447335243225098 0.18916094303131104
CurrentTrain: epoch  2, batch    85 | loss: 4.6364961Losses:  4.638861656188965 0.1763794720172882
CurrentTrain: epoch  2, batch    86 | loss: 4.8152413Losses:  4.8119049072265625 0.3598920404911041
CurrentTrain: epoch  2, batch    87 | loss: 5.1717968Losses:  4.53812837600708 0.182901993393898
CurrentTrain: epoch  2, batch    88 | loss: 4.7210302Losses:  4.343569278717041 0.29321402311325073
CurrentTrain: epoch  2, batch    89 | loss: 4.6367831Losses:  5.031674861907959 0.18677040934562683
CurrentTrain: epoch  2, batch    90 | loss: 5.2184453Losses:  4.6247100830078125 0.16957971453666687
CurrentTrain: epoch  2, batch    91 | loss: 4.7942896Losses:  4.5952982902526855 0.31303834915161133
CurrentTrain: epoch  2, batch    92 | loss: 4.9083366Losses:  5.4838972091674805 0.36179590225219727
CurrentTrain: epoch  2, batch    93 | loss: 5.8456931Losses:  4.93776798248291 0.3616969585418701
CurrentTrain: epoch  2, batch    94 | loss: 5.2994652Losses:  5.223050594329834 0.26831895112991333
CurrentTrain: epoch  2, batch    95 | loss: 5.4913697Losses:  4.921063423156738 0.30935126543045044
CurrentTrain: epoch  2, batch    96 | loss: 5.2304149Losses:  4.825182914733887 0.3268222212791443
CurrentTrain: epoch  2, batch    97 | loss: 5.1520052Losses:  4.789188861846924 0.26265180110931396
CurrentTrain: epoch  2, batch    98 | loss: 5.0518408Losses:  5.056377410888672 0.2972277104854584
CurrentTrain: epoch  2, batch    99 | loss: 5.3536053Losses:  5.505025863647461 0.22331836819648743
CurrentTrain: epoch  2, batch   100 | loss: 5.7283444Losses:  4.9527201652526855 0.15714821219444275
CurrentTrain: epoch  2, batch   101 | loss: 5.1098685Losses:  5.378986358642578 0.3427763879299164
CurrentTrain: epoch  2, batch   102 | loss: 5.7217627Losses:  4.787961959838867 0.20936506986618042
CurrentTrain: epoch  2, batch   103 | loss: 4.9973269Losses:  5.510743141174316 0.3268548846244812
CurrentTrain: epoch  2, batch   104 | loss: 5.8375978Losses:  4.868129730224609 0.18234124779701233
CurrentTrain: epoch  2, batch   105 | loss: 5.0504708Losses:  4.622034072875977 0.21765229105949402
CurrentTrain: epoch  2, batch   106 | loss: 4.8396864Losses:  4.942900657653809 0.20866012573242188
CurrentTrain: epoch  2, batch   107 | loss: 5.1515608Losses:  4.676458358764648 0.11920695006847382
CurrentTrain: epoch  2, batch   108 | loss: 4.7956653Losses:  4.4930925369262695 0.2143852263689041
CurrentTrain: epoch  2, batch   109 | loss: 4.7074776Losses:  5.912039279937744 0.42825815081596375
CurrentTrain: epoch  2, batch   110 | loss: 6.3402972Losses:  4.74481201171875 0.2627914249897003
CurrentTrain: epoch  2, batch   111 | loss: 5.0076036Losses:  4.650176048278809 0.2039496898651123
CurrentTrain: epoch  2, batch   112 | loss: 4.8541260Losses:  4.439363956451416 0.21523651480674744
CurrentTrain: epoch  2, batch   113 | loss: 4.6546006Losses:  4.374852180480957 0.22545546293258667
CurrentTrain: epoch  2, batch   114 | loss: 4.6003075Losses:  5.373977184295654 0.2726590633392334
CurrentTrain: epoch  2, batch   115 | loss: 5.6466360Losses:  4.292237758636475 0.35162365436553955
CurrentTrain: epoch  2, batch   116 | loss: 4.6438613Losses:  7.273042678833008 0.6262721419334412
CurrentTrain: epoch  2, batch   117 | loss: 7.8993149Losses:  5.596490859985352 0.3157283067703247
CurrentTrain: epoch  2, batch   118 | loss: 5.9122190Losses:  4.510159492492676 0.14734351634979248
CurrentTrain: epoch  2, batch   119 | loss: 4.6575031Losses:  4.439553260803223 0.28958219289779663
CurrentTrain: epoch  2, batch   120 | loss: 4.7291355Losses:  4.47620153427124 0.38985341787338257
CurrentTrain: epoch  2, batch   121 | loss: 4.8660550Losses:  4.496879577636719 0.23783855140209198
CurrentTrain: epoch  2, batch   122 | loss: 4.7347183Losses:  4.566653251647949 0.17460350692272186
CurrentTrain: epoch  2, batch   123 | loss: 4.7412567Losses:  5.189432144165039 0.23605501651763916
CurrentTrain: epoch  2, batch   124 | loss: 5.4254870Losses:  4.412517547607422 0.13823308050632477
CurrentTrain: epoch  3, batch     0 | loss: 4.5507507Losses:  4.646827697753906 0.26054897904396057
CurrentTrain: epoch  3, batch     1 | loss: 4.9073768Losses:  5.306560516357422 0.32558220624923706
CurrentTrain: epoch  3, batch     2 | loss: 5.6321425Losses:  4.890219211578369 0.2812628149986267
CurrentTrain: epoch  3, batch     3 | loss: 5.1714821Losses:  4.637481689453125 0.18569941818714142
CurrentTrain: epoch  3, batch     4 | loss: 4.8231812Losses:  5.336130142211914 0.19573548436164856
CurrentTrain: epoch  3, batch     5 | loss: 5.5318656Losses:  4.729511260986328 0.3164229989051819
CurrentTrain: epoch  3, batch     6 | loss: 5.0459342Losses:  5.069965839385986 0.17275531589984894
CurrentTrain: epoch  3, batch     7 | loss: 5.2427211Losses:  4.401362419128418 0.20319625735282898
CurrentTrain: epoch  3, batch     8 | loss: 4.6045585Losses:  4.232014179229736 0.22725918889045715
CurrentTrain: epoch  3, batch     9 | loss: 4.4592733Losses:  4.5718536376953125 0.33039066195487976
CurrentTrain: epoch  3, batch    10 | loss: 4.9022441Losses:  4.347033500671387 0.1832105815410614
CurrentTrain: epoch  3, batch    11 | loss: 4.5302439Losses:  4.346853733062744 0.17218324542045593
CurrentTrain: epoch  3, batch    12 | loss: 4.5190368Losses:  4.677789211273193 0.2030385434627533
CurrentTrain: epoch  3, batch    13 | loss: 4.8808279Losses:  4.460735321044922 0.23950380086898804
CurrentTrain: epoch  3, batch    14 | loss: 4.7002392Losses:  4.618016242980957 0.2730954885482788
CurrentTrain: epoch  3, batch    15 | loss: 4.8911119Losses:  4.4934916496276855 0.3487774729728699
CurrentTrain: epoch  3, batch    16 | loss: 4.8422689Losses:  4.433215141296387 0.2371641844511032
CurrentTrain: epoch  3, batch    17 | loss: 4.6703792Losses:  4.559723854064941 0.2112988829612732
CurrentTrain: epoch  3, batch    18 | loss: 4.7710228Losses:  4.564740180969238 0.3304438591003418
CurrentTrain: epoch  3, batch    19 | loss: 4.8951840Losses:  4.41836404800415 0.15773189067840576
CurrentTrain: epoch  3, batch    20 | loss: 4.5760961Losses:  4.427183151245117 0.24483247101306915
CurrentTrain: epoch  3, batch    21 | loss: 4.6720157Losses:  4.3353776931762695 0.31663399934768677
CurrentTrain: epoch  3, batch    22 | loss: 4.6520119Losses:  4.68778657913208 0.2955029010772705
CurrentTrain: epoch  3, batch    23 | loss: 4.9832897Losses:  6.205357551574707 0.3166840970516205
CurrentTrain: epoch  3, batch    24 | loss: 6.5220418Losses:  4.322874069213867 0.1598808467388153
CurrentTrain: epoch  3, batch    25 | loss: 4.4827547Losses:  4.976958751678467 0.17912128567695618
CurrentTrain: epoch  3, batch    26 | loss: 5.1560802Losses:  4.764190673828125 0.17886888980865479
CurrentTrain: epoch  3, batch    27 | loss: 4.9430594Losses:  4.560935974121094 0.23286962509155273
CurrentTrain: epoch  3, batch    28 | loss: 4.7938056Losses:  4.464033126831055 0.3079374432563782
CurrentTrain: epoch  3, batch    29 | loss: 4.7719707Losses:  4.4660515785217285 0.2122519463300705
CurrentTrain: epoch  3, batch    30 | loss: 4.6783037Losses:  5.333447456359863 0.31247565150260925
CurrentTrain: epoch  3, batch    31 | loss: 5.6459231Losses:  4.499573707580566 0.22696615755558014
CurrentTrain: epoch  3, batch    32 | loss: 4.7265401Losses:  4.452944755554199 0.12375475466251373
CurrentTrain: epoch  3, batch    33 | loss: 4.5766997Losses:  5.021894454956055 0.17537835240364075
CurrentTrain: epoch  3, batch    34 | loss: 5.1972728Losses:  4.403087615966797 0.33242926001548767
CurrentTrain: epoch  3, batch    35 | loss: 4.7355170Losses:  4.422192573547363 0.22044697403907776
CurrentTrain: epoch  3, batch    36 | loss: 4.6426396Losses:  4.489687442779541 0.26193469762802124
CurrentTrain: epoch  3, batch    37 | loss: 4.7516222Losses:  4.4175567626953125 0.14848458766937256
CurrentTrain: epoch  3, batch    38 | loss: 4.5660415Losses:  4.218880653381348 0.15812750160694122
CurrentTrain: epoch  3, batch    39 | loss: 4.3770080Losses:  5.152413368225098 0.40774375200271606
CurrentTrain: epoch  3, batch    40 | loss: 5.5601573Losses:  4.640401840209961 0.2680472433567047
CurrentTrain: epoch  3, batch    41 | loss: 4.9084492Losses:  4.323029041290283 0.2717367708683014
CurrentTrain: epoch  3, batch    42 | loss: 4.5947657Losses:  4.349576950073242 0.18841883540153503
CurrentTrain: epoch  3, batch    43 | loss: 4.5379958Losses:  5.6089911460876465 0.23556838929653168
CurrentTrain: epoch  3, batch    44 | loss: 5.8445597Losses:  4.276795387268066 0.15844520926475525
CurrentTrain: epoch  3, batch    45 | loss: 4.4352407Losses:  4.391254425048828 0.12743648886680603
CurrentTrain: epoch  3, batch    46 | loss: 4.5186911Losses:  4.35156774520874 0.14688412845134735
CurrentTrain: epoch  3, batch    47 | loss: 4.4984517Losses:  4.361503601074219 0.20905549824237823
CurrentTrain: epoch  3, batch    48 | loss: 4.5705590Losses:  4.407607078552246 0.23015281558036804
CurrentTrain: epoch  3, batch    49 | loss: 4.6377597Losses:  4.393817901611328 0.13000750541687012
CurrentTrain: epoch  3, batch    50 | loss: 4.5238256Losses:  4.408824443817139 0.1372578740119934
CurrentTrain: epoch  3, batch    51 | loss: 4.5460825Losses:  4.666101455688477 0.19945994019508362
CurrentTrain: epoch  3, batch    52 | loss: 4.8655615Losses:  4.486921310424805 0.1958014965057373
CurrentTrain: epoch  3, batch    53 | loss: 4.6827230Losses:  4.403473854064941 0.17192994058132172
CurrentTrain: epoch  3, batch    54 | loss: 4.5754037Losses:  4.507979869842529 0.19937542080879211
CurrentTrain: epoch  3, batch    55 | loss: 4.7073555Losses:  4.5431718826293945 0.22120878100395203
CurrentTrain: epoch  3, batch    56 | loss: 4.7643805Losses:  4.38503885269165 0.1437116265296936
CurrentTrain: epoch  3, batch    57 | loss: 4.5287504Losses:  4.447997570037842 0.15256455540657043
CurrentTrain: epoch  3, batch    58 | loss: 4.6005621Losses:  4.303937911987305 0.2359180450439453
CurrentTrain: epoch  3, batch    59 | loss: 4.5398560Losses:  4.1992363929748535 0.18376706540584564
CurrentTrain: epoch  3, batch    60 | loss: 4.3830032Losses:  4.46694278717041 0.14235666394233704
CurrentTrain: epoch  3, batch    61 | loss: 4.6092997Losses:  4.485100269317627 0.3240795135498047
CurrentTrain: epoch  3, batch    62 | loss: 4.8091798Losses:  5.064644813537598 0.11006072163581848
CurrentTrain: epoch  3, batch    63 | loss: 5.1747055Losses:  4.317485809326172 0.13270686566829681
CurrentTrain: epoch  3, batch    64 | loss: 4.4501925Losses:  4.272741317749023 0.17626404762268066
CurrentTrain: epoch  3, batch    65 | loss: 4.4490051Losses:  4.525798797607422 0.1505274623632431
CurrentTrain: epoch  3, batch    66 | loss: 4.6763263Losses:  4.315962791442871 0.29938578605651855
CurrentTrain: epoch  3, batch    67 | loss: 4.6153488Losses:  4.7882537841796875 0.21186888217926025
CurrentTrain: epoch  3, batch    68 | loss: 5.0001225Losses:  4.362698554992676 0.11882326006889343
CurrentTrain: epoch  3, batch    69 | loss: 4.4815216Losses:  4.566963195800781 0.1354714334011078
CurrentTrain: epoch  3, batch    70 | loss: 4.7024345Losses:  4.464019775390625 0.10470548272132874
CurrentTrain: epoch  3, batch    71 | loss: 4.5687251Losses:  4.239733695983887 0.21001744270324707
CurrentTrain: epoch  3, batch    72 | loss: 4.4497509Losses:  4.785370826721191 0.28504878282546997
CurrentTrain: epoch  3, batch    73 | loss: 5.0704198Losses:  4.331048965454102 0.199758380651474
CurrentTrain: epoch  3, batch    74 | loss: 4.5308075Losses:  4.342740058898926 0.21012729406356812
CurrentTrain: epoch  3, batch    75 | loss: 4.5528674Losses:  4.3219313621521 0.2930174469947815
CurrentTrain: epoch  3, batch    76 | loss: 4.6149487Losses:  4.134695053100586 0.1930953860282898
CurrentTrain: epoch  3, batch    77 | loss: 4.3277903Losses:  4.257340431213379 0.16685926914215088
CurrentTrain: epoch  3, batch    78 | loss: 4.4241996Losses:  4.479841232299805 0.2644907236099243
CurrentTrain: epoch  3, batch    79 | loss: 4.7443318Losses:  4.947206497192383 0.18885529041290283
CurrentTrain: epoch  3, batch    80 | loss: 5.1360617Losses:  4.751968860626221 0.2218155562877655
CurrentTrain: epoch  3, batch    81 | loss: 4.9737844Losses:  4.521081924438477 0.2523280382156372
CurrentTrain: epoch  3, batch    82 | loss: 4.7734098Losses:  4.285030364990234 0.12748514115810394
CurrentTrain: epoch  3, batch    83 | loss: 4.4125156Losses:  4.752708911895752 0.23377317190170288
CurrentTrain: epoch  3, batch    84 | loss: 4.9864821Losses:  4.171555519104004 0.062319621443748474
CurrentTrain: epoch  3, batch    85 | loss: 4.2338753Losses:  4.248480796813965 0.07372848689556122
CurrentTrain: epoch  3, batch    86 | loss: 4.3222094Losses:  4.373359203338623 0.1309136003255844
CurrentTrain: epoch  3, batch    87 | loss: 4.5042729Losses:  4.476665496826172 0.2152877002954483
CurrentTrain: epoch  3, batch    88 | loss: 4.6919532Losses:  4.57533073425293 0.19309864938259125
CurrentTrain: epoch  3, batch    89 | loss: 4.7684293Losses:  4.171935558319092 0.15753859281539917
CurrentTrain: epoch  3, batch    90 | loss: 4.3294740Losses:  4.483983516693115 0.15751831233501434
CurrentTrain: epoch  3, batch    91 | loss: 4.6415019Losses:  4.268250465393066 0.15523234009742737
CurrentTrain: epoch  3, batch    92 | loss: 4.4234829Losses:  4.704989433288574 0.19611965119838715
CurrentTrain: epoch  3, batch    93 | loss: 4.9011092Losses:  4.266942501068115 0.18245995044708252
CurrentTrain: epoch  3, batch    94 | loss: 4.4494023Losses:  4.196755409240723 0.27998143434524536
CurrentTrain: epoch  3, batch    95 | loss: 4.4767370Losses:  4.538383483886719 0.19107937812805176
CurrentTrain: epoch  3, batch    96 | loss: 4.7294626Losses:  4.643011569976807 0.25575417280197144
CurrentTrain: epoch  3, batch    97 | loss: 4.8987656Losses:  4.41951847076416 0.13781771063804626
CurrentTrain: epoch  3, batch    98 | loss: 4.5573363Losses:  4.381551742553711 0.09096403419971466
CurrentTrain: epoch  3, batch    99 | loss: 4.4725156Losses:  4.2621893882751465 0.09234605729579926
CurrentTrain: epoch  3, batch   100 | loss: 4.3545356Losses:  4.369007587432861 0.1602059006690979
CurrentTrain: epoch  3, batch   101 | loss: 4.5292134Losses:  4.416136264801025 0.21006986498832703
CurrentTrain: epoch  3, batch   102 | loss: 4.6262059Losses:  4.31494140625 0.18177448213100433
CurrentTrain: epoch  3, batch   103 | loss: 4.4967160Losses:  4.241128444671631 0.21893543004989624
CurrentTrain: epoch  3, batch   104 | loss: 4.4600639Losses:  4.20938777923584 0.1762971580028534
CurrentTrain: epoch  3, batch   105 | loss: 4.3856850Losses:  4.257979393005371 0.17254044115543365
CurrentTrain: epoch  3, batch   106 | loss: 4.4305201Losses:  4.292430400848389 0.14203289151191711
CurrentTrain: epoch  3, batch   107 | loss: 4.4344635Losses:  4.420314311981201 0.2534927725791931
CurrentTrain: epoch  3, batch   108 | loss: 4.6738071Losses:  4.463633060455322 0.2838263511657715
CurrentTrain: epoch  3, batch   109 | loss: 4.7474594Losses:  4.812801837921143 0.35571038722991943
CurrentTrain: epoch  3, batch   110 | loss: 5.1685123Losses:  4.327184200286865 0.29657724499702454
CurrentTrain: epoch  3, batch   111 | loss: 4.6237617Losses:  4.547647476196289 0.16983100771903992
CurrentTrain: epoch  3, batch   112 | loss: 4.7174783Losses:  4.380790710449219 0.1933620274066925
CurrentTrain: epoch  3, batch   113 | loss: 4.5741529Losses:  4.342832088470459 0.05602418631315231
CurrentTrain: epoch  3, batch   114 | loss: 4.3988562Losses:  4.940053939819336 0.34237170219421387
CurrentTrain: epoch  3, batch   115 | loss: 5.2824259Losses:  4.4409871101379395 0.23913642764091492
CurrentTrain: epoch  3, batch   116 | loss: 4.6801233Losses:  4.269015789031982 0.18087875843048096
CurrentTrain: epoch  3, batch   117 | loss: 4.4498944Losses:  4.255205154418945 0.15920203924179077
CurrentTrain: epoch  3, batch   118 | loss: 4.4144073Losses:  4.713441848754883 0.3307496905326843
CurrentTrain: epoch  3, batch   119 | loss: 5.0441914Losses:  4.2626752853393555 0.17991548776626587
CurrentTrain: epoch  3, batch   120 | loss: 4.4425907Losses:  4.282773971557617 0.21010847389698029
CurrentTrain: epoch  3, batch   121 | loss: 4.4928823Losses:  4.647184371948242 0.16760730743408203
CurrentTrain: epoch  3, batch   122 | loss: 4.8147917Losses:  4.295345306396484 0.1683686524629593
CurrentTrain: epoch  3, batch   123 | loss: 4.4637141Losses:  4.337309837341309 0.133144348859787
CurrentTrain: epoch  3, batch   124 | loss: 4.4704542Losses:  4.450563907623291 0.1653749942779541
CurrentTrain: epoch  4, batch     0 | loss: 4.6159391Losses:  4.118841171264648 0.16454780101776123
CurrentTrain: epoch  4, batch     1 | loss: 4.2833891Losses:  4.266925811767578 0.2498108446598053
CurrentTrain: epoch  4, batch     2 | loss: 4.5167365Losses:  4.387212753295898 0.21508753299713135
CurrentTrain: epoch  4, batch     3 | loss: 4.6023002Losses:  4.875158309936523 0.4109199047088623
CurrentTrain: epoch  4, batch     4 | loss: 5.2860785Losses:  4.290922164916992 0.2455107867717743
CurrentTrain: epoch  4, batch     5 | loss: 4.5364327Losses:  4.307308197021484 0.26839524507522583
CurrentTrain: epoch  4, batch     6 | loss: 4.5757036Losses:  4.303223609924316 0.1542234718799591
CurrentTrain: epoch  4, batch     7 | loss: 4.4574471Losses:  4.474155902862549 0.30228790640830994
CurrentTrain: epoch  4, batch     8 | loss: 4.7764440Losses:  4.310129165649414 0.21691690385341644
CurrentTrain: epoch  4, batch     9 | loss: 4.5270462Losses:  4.367068290710449 0.20210909843444824
CurrentTrain: epoch  4, batch    10 | loss: 4.5691776Losses:  4.166805267333984 0.21869541704654694
CurrentTrain: epoch  4, batch    11 | loss: 4.3855009Losses:  4.425609588623047 0.12846845388412476
CurrentTrain: epoch  4, batch    12 | loss: 4.5540781Losses:  4.270978927612305 0.19182975590229034
CurrentTrain: epoch  4, batch    13 | loss: 4.4628086Losses:  4.316861152648926 0.14591056108474731
CurrentTrain: epoch  4, batch    14 | loss: 4.4627719Losses:  4.235860824584961 0.17996089160442352
CurrentTrain: epoch  4, batch    15 | loss: 4.4158216Losses:  4.555021286010742 0.1488099992275238
CurrentTrain: epoch  4, batch    16 | loss: 4.7038312Losses:  4.299929618835449 0.1650933474302292
CurrentTrain: epoch  4, batch    17 | loss: 4.4650230Losses:  4.230483531951904 0.157613143324852
CurrentTrain: epoch  4, batch    18 | loss: 4.3880968Losses:  4.164836406707764 0.11583467572927475
CurrentTrain: epoch  4, batch    19 | loss: 4.2806711Losses:  4.322338104248047 0.18277592957019806
CurrentTrain: epoch  4, batch    20 | loss: 4.5051141Losses:  4.354667663574219 0.20374076068401337
CurrentTrain: epoch  4, batch    21 | loss: 4.5584083Losses:  4.185647010803223 0.21261748671531677
CurrentTrain: epoch  4, batch    22 | loss: 4.3982644Losses:  4.200040817260742 0.10543800890445709
CurrentTrain: epoch  4, batch    23 | loss: 4.3054790Losses:  4.2691144943237305 0.14198243618011475
CurrentTrain: epoch  4, batch    24 | loss: 4.4110970Losses:  4.391913890838623 0.13297775387763977
CurrentTrain: epoch  4, batch    25 | loss: 4.5248919Losses:  4.243068695068359 0.20508936047554016
CurrentTrain: epoch  4, batch    26 | loss: 4.4481583Losses:  4.362495422363281 0.22609403729438782
CurrentTrain: epoch  4, batch    27 | loss: 4.5885897Losses:  4.267997741699219 0.18521323800086975
CurrentTrain: epoch  4, batch    28 | loss: 4.4532108Losses:  4.20536994934082 0.10398896783590317
CurrentTrain: epoch  4, batch    29 | loss: 4.3093591Losses:  4.187046051025391 0.11867248266935349
CurrentTrain: epoch  4, batch    30 | loss: 4.3057184Losses:  4.314813137054443 0.27793747186660767
CurrentTrain: epoch  4, batch    31 | loss: 4.5927505Losses:  4.301074981689453 0.24893398582935333
CurrentTrain: epoch  4, batch    32 | loss: 4.5500088Losses:  4.252900123596191 0.1667758822441101
CurrentTrain: epoch  4, batch    33 | loss: 4.4196758Losses:  4.208327293395996 0.11821147799491882
CurrentTrain: epoch  4, batch    34 | loss: 4.3265386Losses:  4.230966567993164 0.09463818371295929
CurrentTrain: epoch  4, batch    35 | loss: 4.3256049Losses:  4.253289699554443 0.18985378742218018
CurrentTrain: epoch  4, batch    36 | loss: 4.4431434Losses:  4.126753807067871 0.28620246052742004
CurrentTrain: epoch  4, batch    37 | loss: 4.4129562Losses:  4.218341827392578 0.12984047830104828
CurrentTrain: epoch  4, batch    38 | loss: 4.3481822Losses:  4.31024694442749 0.19435209035873413
CurrentTrain: epoch  4, batch    39 | loss: 4.5045991Losses:  4.2688398361206055 0.20942869782447815
CurrentTrain: epoch  4, batch    40 | loss: 4.4782686Losses:  4.214367866516113 0.12988752126693726
CurrentTrain: epoch  4, batch    41 | loss: 4.3442554Losses:  4.252739429473877 0.14500582218170166
CurrentTrain: epoch  4, batch    42 | loss: 4.3977451Losses:  4.221120357513428 0.13125869631767273
CurrentTrain: epoch  4, batch    43 | loss: 4.3523788Losses:  4.1821088790893555 0.1733001172542572
CurrentTrain: epoch  4, batch    44 | loss: 4.3554091Losses:  4.099993705749512 0.12565380334854126
CurrentTrain: epoch  4, batch    45 | loss: 4.2256474Losses:  4.309700965881348 0.18211711943149567
CurrentTrain: epoch  4, batch    46 | loss: 4.4918180Losses:  4.341909408569336 0.1121557205915451
CurrentTrain: epoch  4, batch    47 | loss: 4.4540653Losses:  4.1995038986206055 0.13392594456672668
CurrentTrain: epoch  4, batch    48 | loss: 4.3334298Losses:  4.28467321395874 0.2541382908821106
CurrentTrain: epoch  4, batch    49 | loss: 4.5388117Losses:  4.19746732711792 0.11297059804201126
CurrentTrain: epoch  4, batch    50 | loss: 4.3104382Losses:  4.205993175506592 0.09354700148105621
CurrentTrain: epoch  4, batch    51 | loss: 4.2995400Losses:  4.2302446365356445 0.13071657717227936
CurrentTrain: epoch  4, batch    52 | loss: 4.3609614Losses:  4.303622245788574 0.17547529935836792
CurrentTrain: epoch  4, batch    53 | loss: 4.4790974Losses:  4.318609237670898 0.16020192205905914
CurrentTrain: epoch  4, batch    54 | loss: 4.4788113Losses:  4.170788764953613 0.19755439460277557
CurrentTrain: epoch  4, batch    55 | loss: 4.3683434Losses:  4.1460957527160645 0.13358011841773987
CurrentTrain: epoch  4, batch    56 | loss: 4.2796760Losses:  4.2148590087890625 0.16544313728809357
CurrentTrain: epoch  4, batch    57 | loss: 4.3803020Losses:  4.230299949645996 0.12373711913824081
CurrentTrain: epoch  4, batch    58 | loss: 4.3540373Losses:  4.317172050476074 0.15760774910449982
CurrentTrain: epoch  4, batch    59 | loss: 4.4747796Losses:  4.253047943115234 0.10712294280529022
CurrentTrain: epoch  4, batch    60 | loss: 4.3601708Losses:  4.082416534423828 0.20351269841194153
CurrentTrain: epoch  4, batch    61 | loss: 4.2859292Losses:  4.1952972412109375 0.20383094251155853
CurrentTrain: epoch  4, batch    62 | loss: 4.3991280Losses:  4.2491278648376465 0.16312605142593384
CurrentTrain: epoch  4, batch    63 | loss: 4.4122539Losses:  4.161159515380859 0.13290810585021973
CurrentTrain: epoch  4, batch    64 | loss: 4.2940674Losses:  4.180864334106445 0.13133566081523895
CurrentTrain: epoch  4, batch    65 | loss: 4.3122001Losses:  4.238761901855469 0.21893620491027832
CurrentTrain: epoch  4, batch    66 | loss: 4.4576979Losses:  4.201794147491455 0.1432170271873474
CurrentTrain: epoch  4, batch    67 | loss: 4.3450112Losses:  4.20710563659668 0.1298375129699707
CurrentTrain: epoch  4, batch    68 | loss: 4.3369431Losses:  4.1730146408081055 0.13599088788032532
CurrentTrain: epoch  4, batch    69 | loss: 4.3090057Losses:  4.105711460113525 0.17032283544540405
CurrentTrain: epoch  4, batch    70 | loss: 4.2760344Losses:  4.186647415161133 0.10931004583835602
CurrentTrain: epoch  4, batch    71 | loss: 4.2959576Losses:  4.1695237159729 0.08171863108873367
CurrentTrain: epoch  4, batch    72 | loss: 4.2512422Losses:  4.103769302368164 0.2119329273700714
CurrentTrain: epoch  4, batch    73 | loss: 4.3157024Losses:  4.176541328430176 0.09638009965419769
CurrentTrain: epoch  4, batch    74 | loss: 4.2729216Losses:  4.2219557762146 0.0974484235048294
CurrentTrain: epoch  4, batch    75 | loss: 4.3194041Losses:  5.0116190910339355 0.18108130991458893
CurrentTrain: epoch  4, batch    76 | loss: 5.1927004Losses:  4.141748428344727 0.17855612933635712
CurrentTrain: epoch  4, batch    77 | loss: 4.3203044Losses:  4.226920127868652 0.1787574589252472
CurrentTrain: epoch  4, batch    78 | loss: 4.4056778Losses:  4.231265544891357 0.18945764005184174
CurrentTrain: epoch  4, batch    79 | loss: 4.4207230Losses:  4.239799976348877 0.1473863124847412
CurrentTrain: epoch  4, batch    80 | loss: 4.3871861Losses:  4.147741317749023 0.17233122885227203
CurrentTrain: epoch  4, batch    81 | loss: 4.3200727Losses:  4.371866703033447 0.16642092168331146
CurrentTrain: epoch  4, batch    82 | loss: 4.5382876Losses:  4.198115348815918 0.10169917345046997
CurrentTrain: epoch  4, batch    83 | loss: 4.2998147Losses:  4.256357192993164 0.13076043128967285
CurrentTrain: epoch  4, batch    84 | loss: 4.3871174Losses:  4.240735054016113 0.24891987442970276
CurrentTrain: epoch  4, batch    85 | loss: 4.4896550Losses:  4.1729536056518555 0.2460324913263321
CurrentTrain: epoch  4, batch    86 | loss: 4.4189863Losses:  4.193966865539551 0.19005875289440155
CurrentTrain: epoch  4, batch    87 | loss: 4.3840256Losses:  4.283232688903809 0.0928293988108635
CurrentTrain: epoch  4, batch    88 | loss: 4.3760619Losses:  4.133535861968994 0.12218890339136124
CurrentTrain: epoch  4, batch    89 | loss: 4.2557249Losses:  4.09396505355835 0.11559158563613892
CurrentTrain: epoch  4, batch    90 | loss: 4.2095566Losses:  4.193164825439453 0.041427627205848694
CurrentTrain: epoch  4, batch    91 | loss: 4.2345924Losses:  4.187072277069092 0.10564737766981125
CurrentTrain: epoch  4, batch    92 | loss: 4.2927198Losses:  4.329023361206055 0.16621623933315277
CurrentTrain: epoch  4, batch    93 | loss: 4.4952397Losses:  4.156313896179199 0.13274608552455902
CurrentTrain: epoch  4, batch    94 | loss: 4.2890601Losses:  4.166746616363525 0.16570349037647247
CurrentTrain: epoch  4, batch    95 | loss: 4.3324499Losses:  4.150785446166992 0.12526541948318481
CurrentTrain: epoch  4, batch    96 | loss: 4.2760510Losses:  4.212672710418701 0.1813594400882721
CurrentTrain: epoch  4, batch    97 | loss: 4.3940320Losses:  4.127703666687012 0.15206901729106903
CurrentTrain: epoch  4, batch    98 | loss: 4.2797728Losses:  4.11378288269043 0.17293010652065277
CurrentTrain: epoch  4, batch    99 | loss: 4.2867131Losses:  4.199434757232666 0.11025567352771759
CurrentTrain: epoch  4, batch   100 | loss: 4.3096905Losses:  4.135497093200684 0.1603909432888031
CurrentTrain: epoch  4, batch   101 | loss: 4.2958879Losses:  4.195140361785889 0.16038411855697632
CurrentTrain: epoch  4, batch   102 | loss: 4.3555245Losses:  4.181026458740234 0.26371249556541443
CurrentTrain: epoch  4, batch   103 | loss: 4.4447389Losses:  4.121326923370361 0.17803767323493958
CurrentTrain: epoch  4, batch   104 | loss: 4.2993646Losses:  4.194344997406006 0.15019086003303528
CurrentTrain: epoch  4, batch   105 | loss: 4.3445358Losses:  4.131616592407227 0.14060914516448975
CurrentTrain: epoch  4, batch   106 | loss: 4.2722259Losses:  4.1670684814453125 0.10008388757705688
CurrentTrain: epoch  4, batch   107 | loss: 4.2671523Losses:  4.11180305480957 0.13399337232112885
CurrentTrain: epoch  4, batch   108 | loss: 4.2457962Losses:  4.173162937164307 0.14402548968791962
CurrentTrain: epoch  4, batch   109 | loss: 4.3171883Losses:  4.220071792602539 0.09903070330619812
CurrentTrain: epoch  4, batch   110 | loss: 4.3191023Losses:  4.424956321716309 0.1064053475856781
CurrentTrain: epoch  4, batch   111 | loss: 4.5313616Losses:  4.144106864929199 0.18222038447856903
CurrentTrain: epoch  4, batch   112 | loss: 4.3263273Losses:  4.16498327255249 0.08969233930110931
CurrentTrain: epoch  4, batch   113 | loss: 4.2546754Losses:  4.113631725311279 0.09966573119163513
CurrentTrain: epoch  4, batch   114 | loss: 4.2132974Losses:  4.108192443847656 0.20149806141853333
CurrentTrain: epoch  4, batch   115 | loss: 4.3096905Losses:  4.124940395355225 0.16015253961086273
CurrentTrain: epoch  4, batch   116 | loss: 4.2850928Losses:  4.1996307373046875 0.12946921586990356
CurrentTrain: epoch  4, batch   117 | loss: 4.3291001Losses:  4.14792013168335 0.1177385225892067
CurrentTrain: epoch  4, batch   118 | loss: 4.2656589Losses:  4.177042007446289 0.1798664629459381
CurrentTrain: epoch  4, batch   119 | loss: 4.3569083Losses:  4.135254383087158 0.10825763642787933
CurrentTrain: epoch  4, batch   120 | loss: 4.2435122Losses:  4.014747142791748 0.10107335448265076
CurrentTrain: epoch  4, batch   121 | loss: 4.1158204Losses:  4.041863441467285 0.1255071759223938
CurrentTrain: epoch  4, batch   122 | loss: 4.1673708Losses:  4.071642875671387 0.18966637551784515
CurrentTrain: epoch  4, batch   123 | loss: 4.2613091Losses:  4.038237571716309 0.107002854347229
CurrentTrain: epoch  4, batch   124 | loss: 4.1452403Losses:  4.07915735244751 0.13431312143802643
CurrentTrain: epoch  5, batch     0 | loss: 4.2134705Losses:  4.122920036315918 0.0592498704791069
CurrentTrain: epoch  5, batch     1 | loss: 4.1821699Losses:  4.11230993270874 0.1004706472158432
CurrentTrain: epoch  5, batch     2 | loss: 4.2127805Losses:  4.1559247970581055 0.14599096775054932
CurrentTrain: epoch  5, batch     3 | loss: 4.3019156Losses:  4.0839033126831055 0.1834423840045929
CurrentTrain: epoch  5, batch     4 | loss: 4.2673459Losses:  4.056266784667969 0.14471378922462463
CurrentTrain: epoch  5, batch     5 | loss: 4.2009807Losses:  4.1168670654296875 0.0792965292930603
CurrentTrain: epoch  5, batch     6 | loss: 4.1961637Losses:  4.14893913269043 0.15564438700675964
CurrentTrain: epoch  5, batch     7 | loss: 4.3045835Losses:  4.138245582580566 0.14930491149425507
CurrentTrain: epoch  5, batch     8 | loss: 4.2875504Losses:  4.1368913650512695 0.16097034513950348
CurrentTrain: epoch  5, batch     9 | loss: 4.2978616Losses:  4.141777515411377 0.19164510071277618
CurrentTrain: epoch  5, batch    10 | loss: 4.3334227Losses:  4.092682838439941 0.11787236481904984
CurrentTrain: epoch  5, batch    11 | loss: 4.2105551Losses:  4.036969184875488 0.2089833766222
CurrentTrain: epoch  5, batch    12 | loss: 4.2459526Losses:  4.079441070556641 0.09875057637691498
CurrentTrain: epoch  5, batch    13 | loss: 4.1781917Losses:  4.110619068145752 0.10176083445549011
CurrentTrain: epoch  5, batch    14 | loss: 4.2123799Losses:  4.147458076477051 0.08562293648719788
CurrentTrain: epoch  5, batch    15 | loss: 4.2330809Losses:  4.186412334442139 0.07180476188659668
CurrentTrain: epoch  5, batch    16 | loss: 4.2582169Losses:  4.1021013259887695 0.2034839391708374
CurrentTrain: epoch  5, batch    17 | loss: 4.3055854Losses:  4.069356441497803 0.17073699831962585
CurrentTrain: epoch  5, batch    18 | loss: 4.2400932Losses:  4.121296405792236 0.1711919903755188
CurrentTrain: epoch  5, batch    19 | loss: 4.2924886Losses:  4.079306602478027 0.24270570278167725
CurrentTrain: epoch  5, batch    20 | loss: 4.3220124Losses:  4.135786056518555 0.1796596795320511
CurrentTrain: epoch  5, batch    21 | loss: 4.3154459Losses:  4.061655044555664 0.11192157864570618
CurrentTrain: epoch  5, batch    22 | loss: 4.1735768Losses:  4.094189167022705 0.19212433695793152
CurrentTrain: epoch  5, batch    23 | loss: 4.2863135Losses:  4.090631484985352 0.10640119016170502
CurrentTrain: epoch  5, batch    24 | loss: 4.1970325Losses:  4.056703090667725 0.2310078889131546
CurrentTrain: epoch  5, batch    25 | loss: 4.2877111Losses:  4.092148780822754 0.15064112842082977
CurrentTrain: epoch  5, batch    26 | loss: 4.2427897Losses:  4.185572624206543 0.14832784235477448
CurrentTrain: epoch  5, batch    27 | loss: 4.3339005Losses:  4.126172065734863 0.2413332164287567
CurrentTrain: epoch  5, batch    28 | loss: 4.3675051Losses:  4.046485900878906 0.13081766664981842
CurrentTrain: epoch  5, batch    29 | loss: 4.1773038Losses:  4.112608909606934 0.17435067892074585
CurrentTrain: epoch  5, batch    30 | loss: 4.2869596Losses:  4.133644104003906 0.10467167943716049
CurrentTrain: epoch  5, batch    31 | loss: 4.2383156Losses:  4.15970516204834 0.17782217264175415
CurrentTrain: epoch  5, batch    32 | loss: 4.3375273Losses:  4.205165863037109 0.2227802276611328
CurrentTrain: epoch  5, batch    33 | loss: 4.4279461Losses:  4.091063499450684 0.10078492015600204
CurrentTrain: epoch  5, batch    34 | loss: 4.1918483Losses:  4.055535316467285 0.14123594760894775
CurrentTrain: epoch  5, batch    35 | loss: 4.1967711Losses:  4.162038803100586 0.14893195033073425
CurrentTrain: epoch  5, batch    36 | loss: 4.3109708Losses:  4.08612585067749 0.1759314239025116
CurrentTrain: epoch  5, batch    37 | loss: 4.2620573Losses:  3.9929251670837402 0.12658077478408813
CurrentTrain: epoch  5, batch    38 | loss: 4.1195059Losses:  4.144279479980469 0.03718291223049164
CurrentTrain: epoch  5, batch    39 | loss: 4.1814623Losses:  4.10856294631958 0.20645813643932343
CurrentTrain: epoch  5, batch    40 | loss: 4.3150210Losses:  4.066603183746338 0.07111939787864685
CurrentTrain: epoch  5, batch    41 | loss: 4.1377225Losses:  4.107570648193359 0.1599419265985489
CurrentTrain: epoch  5, batch    42 | loss: 4.2675128Losses:  4.084312438964844 0.10677986592054367
CurrentTrain: epoch  5, batch    43 | loss: 4.1910925Losses:  4.147923469543457 0.1548854410648346
CurrentTrain: epoch  5, batch    44 | loss: 4.3028088Losses:  4.094985008239746 0.12908941507339478
CurrentTrain: epoch  5, batch    45 | loss: 4.2240744Losses:  4.156350135803223 0.11823107302188873
CurrentTrain: epoch  5, batch    46 | loss: 4.2745814Losses:  4.124818801879883 0.11003152281045914
CurrentTrain: epoch  5, batch    47 | loss: 4.2348504Losses:  4.1446533203125 0.11108824610710144
CurrentTrain: epoch  5, batch    48 | loss: 4.2557416Losses:  4.100940704345703 0.15426944196224213
CurrentTrain: epoch  5, batch    49 | loss: 4.2552099Losses:  4.1573381423950195 0.13017630577087402
CurrentTrain: epoch  5, batch    50 | loss: 4.2875147Losses:  4.122932434082031 0.22668445110321045
CurrentTrain: epoch  5, batch    51 | loss: 4.3496170Losses:  4.068869590759277 0.2030167579650879
CurrentTrain: epoch  5, batch    52 | loss: 4.2718863Losses:  4.019969940185547 0.13148704171180725
CurrentTrain: epoch  5, batch    53 | loss: 4.1514568Losses:  4.104194641113281 0.09484560787677765
CurrentTrain: epoch  5, batch    54 | loss: 4.1990404Losses:  4.130439281463623 0.12712904810905457
CurrentTrain: epoch  5, batch    55 | loss: 4.2575684Losses:  4.086312770843506 0.05315747857093811
CurrentTrain: epoch  5, batch    56 | loss: 4.1394701Losses:  4.07382869720459 0.1530056893825531
CurrentTrain: epoch  5, batch    57 | loss: 4.2268343Losses:  4.041755676269531 0.10289576649665833
CurrentTrain: epoch  5, batch    58 | loss: 4.1446514Losses:  4.143620014190674 0.15592515468597412
CurrentTrain: epoch  5, batch    59 | loss: 4.2995453Losses:  4.0644025802612305 0.06406724452972412
CurrentTrain: epoch  5, batch    60 | loss: 4.1284699Losses:  4.026557445526123 0.14406614005565643
CurrentTrain: epoch  5, batch    61 | loss: 4.1706238Losses:  4.08698844909668 0.07615390419960022
CurrentTrain: epoch  5, batch    62 | loss: 4.1631422Losses:  4.125865459442139 0.10612267255783081
CurrentTrain: epoch  5, batch    63 | loss: 4.2319880Losses:  4.096067428588867 0.18137642741203308
CurrentTrain: epoch  5, batch    64 | loss: 4.2774439Losses:  4.105637550354004 0.06075580045580864
CurrentTrain: epoch  5, batch    65 | loss: 4.1663933Losses:  4.105369567871094 0.07305673509836197
CurrentTrain: epoch  5, batch    66 | loss: 4.1784263Losses:  4.2095441818237305 0.07306224852800369
CurrentTrain: epoch  5, batch    67 | loss: 4.2826066Losses:  4.034422397613525 0.11827468127012253
CurrentTrain: epoch  5, batch    68 | loss: 4.1526971Losses:  4.161386489868164 0.2008916437625885
CurrentTrain: epoch  5, batch    69 | loss: 4.3622780Losses:  4.120391845703125 0.08394093811511993
CurrentTrain: epoch  5, batch    70 | loss: 4.2043328Losses:  4.092577934265137 0.14701160788536072
CurrentTrain: epoch  5, batch    71 | loss: 4.2395897Losses:  4.094105243682861 0.06904695928096771
CurrentTrain: epoch  5, batch    72 | loss: 4.1631522Losses:  4.2144317626953125 0.22088700532913208
CurrentTrain: epoch  5, batch    73 | loss: 4.4353189Losses:  4.1021342277526855 0.08295774459838867
CurrentTrain: epoch  5, batch    74 | loss: 4.1850920Losses:  4.0007171630859375 0.07053612172603607
CurrentTrain: epoch  5, batch    75 | loss: 4.0712533Losses:  4.045072555541992 0.13250023126602173
CurrentTrain: epoch  5, batch    76 | loss: 4.1775727Losses:  4.129827976226807 0.14278250932693481
CurrentTrain: epoch  5, batch    77 | loss: 4.2726107Losses:  4.003880500793457 0.09852559119462967
CurrentTrain: epoch  5, batch    78 | loss: 4.1024060Losses:  4.053997039794922 0.20195156335830688
CurrentTrain: epoch  5, batch    79 | loss: 4.2559485Losses:  4.028112411499023 0.10842835903167725
CurrentTrain: epoch  5, batch    80 | loss: 4.1365409Losses:  4.0156145095825195 0.06577129662036896
CurrentTrain: epoch  5, batch    81 | loss: 4.0813856Losses:  3.994373321533203 0.1115357056260109
CurrentTrain: epoch  5, batch    82 | loss: 4.1059089Losses:  4.058793067932129 0.12352108955383301
CurrentTrain: epoch  5, batch    83 | loss: 4.1823139Losses:  4.085915565490723 0.14400453865528107
CurrentTrain: epoch  5, batch    84 | loss: 4.2299199Losses:  3.9831347465515137 0.12476666271686554
CurrentTrain: epoch  5, batch    85 | loss: 4.1079016Losses:  4.015749931335449 0.13935844600200653
CurrentTrain: epoch  5, batch    86 | loss: 4.1551085Losses:  4.060690879821777 0.11709628254175186
CurrentTrain: epoch  5, batch    87 | loss: 4.1777873Losses:  4.047092437744141 0.08099252730607986
CurrentTrain: epoch  5, batch    88 | loss: 4.1280851Losses:  4.048851013183594 0.07743524014949799
CurrentTrain: epoch  5, batch    89 | loss: 4.1262860Losses:  4.067193984985352 0.13509143888950348
CurrentTrain: epoch  5, batch    90 | loss: 4.2022853Losses:  4.042046546936035 0.07149835675954819
CurrentTrain: epoch  5, batch    91 | loss: 4.1135449Losses:  4.100132942199707 0.08193768560886383
CurrentTrain: epoch  5, batch    92 | loss: 4.1820707Losses:  4.032785415649414 0.12401954084634781
CurrentTrain: epoch  5, batch    93 | loss: 4.1568050Losses:  4.039869785308838 0.0550866536796093
CurrentTrain: epoch  5, batch    94 | loss: 4.0949564Losses:  4.063169002532959 0.194833442568779
CurrentTrain: epoch  5, batch    95 | loss: 4.2580023Losses:  4.107461929321289 0.07423112541437149
CurrentTrain: epoch  5, batch    96 | loss: 4.1816931Losses:  4.023037433624268 0.18545979261398315
CurrentTrain: epoch  5, batch    97 | loss: 4.2084970Losses:  4.2120513916015625 0.14344540238380432
CurrentTrain: epoch  5, batch    98 | loss: 4.3554969Losses:  4.016035079956055 0.12807318568229675
CurrentTrain: epoch  5, batch    99 | loss: 4.1441083Losses:  4.054121494293213 0.1016649454832077
CurrentTrain: epoch  5, batch   100 | loss: 4.1557865Losses:  4.089065074920654 0.14570757746696472
CurrentTrain: epoch  5, batch   101 | loss: 4.2347727Losses:  3.9874494075775146 0.0679979994893074
CurrentTrain: epoch  5, batch   102 | loss: 4.0554476Losses:  4.059419631958008 0.10163882374763489
CurrentTrain: epoch  5, batch   103 | loss: 4.1610584Losses:  4.033449649810791 0.16738663613796234
CurrentTrain: epoch  5, batch   104 | loss: 4.2008362Losses:  4.076474666595459 0.1429367959499359
CurrentTrain: epoch  5, batch   105 | loss: 4.2194114Losses:  4.067897796630859 0.14227867126464844
CurrentTrain: epoch  5, batch   106 | loss: 4.2101765Losses:  4.016542911529541 0.12015272676944733
CurrentTrain: epoch  5, batch   107 | loss: 4.1366959Losses:  4.0807647705078125 0.1019788384437561
CurrentTrain: epoch  5, batch   108 | loss: 4.1827435Losses:  3.985456943511963 0.10353857278823853
CurrentTrain: epoch  5, batch   109 | loss: 4.0889955Losses:  4.065546989440918 0.12106102705001831
CurrentTrain: epoch  5, batch   110 | loss: 4.1866078Losses:  4.049320697784424 0.12900391221046448
CurrentTrain: epoch  5, batch   111 | loss: 4.1783247Losses:  4.1261701583862305 0.12273109704256058
CurrentTrain: epoch  5, batch   112 | loss: 4.2489014Losses:  4.087929725646973 0.12310345470905304
CurrentTrain: epoch  5, batch   113 | loss: 4.2110333Losses:  4.027846336364746 0.1134951189160347
CurrentTrain: epoch  5, batch   114 | loss: 4.1413417Losses:  4.105347633361816 0.11577139794826508
CurrentTrain: epoch  5, batch   115 | loss: 4.2211189Losses:  4.040921211242676 0.12062010914087296
CurrentTrain: epoch  5, batch   116 | loss: 4.1615415Losses:  4.042784690856934 0.03073456883430481
CurrentTrain: epoch  5, batch   117 | loss: 4.0735192Losses:  4.016054630279541 0.17537516355514526
CurrentTrain: epoch  5, batch   118 | loss: 4.1914296Losses:  4.037668228149414 0.09857513755559921
CurrentTrain: epoch  5, batch   119 | loss: 4.1362433Losses:  4.043245315551758 0.05090947821736336
CurrentTrain: epoch  5, batch   120 | loss: 4.0941548Losses:  3.996891498565674 0.1791962832212448
CurrentTrain: epoch  5, batch   121 | loss: 4.1760879Losses:  4.132049560546875 0.1358354687690735
CurrentTrain: epoch  5, batch   122 | loss: 4.2678852Losses:  4.082157135009766 0.09481976926326752
CurrentTrain: epoch  5, batch   123 | loss: 4.1769767Losses:  4.057024002075195 0.1552233248949051
CurrentTrain: epoch  5, batch   124 | loss: 4.2122474Losses:  4.047135353088379 0.1503540575504303
CurrentTrain: epoch  6, batch     0 | loss: 4.1974893Losses:  4.014556407928467 0.17593678832054138
CurrentTrain: epoch  6, batch     1 | loss: 4.1904931Losses:  4.041722297668457 0.08178415894508362
CurrentTrain: epoch  6, batch     2 | loss: 4.1235065Losses:  4.070583343505859 0.1296490728855133
CurrentTrain: epoch  6, batch     3 | loss: 4.2002325Losses:  4.058934211730957 0.12620265781879425
CurrentTrain: epoch  6, batch     4 | loss: 4.1851368Losses:  4.05052375793457 0.05983863025903702
CurrentTrain: epoch  6, batch     5 | loss: 4.1103625Losses:  4.015805244445801 0.10324057936668396
CurrentTrain: epoch  6, batch     6 | loss: 4.1190457Losses:  4.098128318786621 0.07482217252254486
CurrentTrain: epoch  6, batch     7 | loss: 4.1729503Losses:  4.074856281280518 0.09645682573318481
CurrentTrain: epoch  6, batch     8 | loss: 4.1713133Losses:  4.063453197479248 0.1502687782049179
CurrentTrain: epoch  6, batch     9 | loss: 4.2137218Losses:  4.120899677276611 0.06620323657989502
CurrentTrain: epoch  6, batch    10 | loss: 4.1871028Losses:  4.091969966888428 0.09177768975496292
CurrentTrain: epoch  6, batch    11 | loss: 4.1837478Losses:  4.101493835449219 0.14921793341636658
CurrentTrain: epoch  6, batch    12 | loss: 4.2507119Losses:  3.9895012378692627 0.16059717535972595
CurrentTrain: epoch  6, batch    13 | loss: 4.1500983Losses:  4.045584678649902 0.17740368843078613
CurrentTrain: epoch  6, batch    14 | loss: 4.2229881Losses:  4.060826301574707 0.10284693539142609
CurrentTrain: epoch  6, batch    15 | loss: 4.1636734Losses:  4.012080669403076 0.18862247467041016
CurrentTrain: epoch  6, batch    16 | loss: 4.2007031Losses:  4.079134941101074 0.1026511937379837
CurrentTrain: epoch  6, batch    17 | loss: 4.1817861Losses:  4.099422931671143 0.06083376333117485
CurrentTrain: epoch  6, batch    18 | loss: 4.1602569Losses:  4.107821941375732 0.11736918985843658
CurrentTrain: epoch  6, batch    19 | loss: 4.2251911Losses:  4.038511276245117 0.09643521904945374
CurrentTrain: epoch  6, batch    20 | loss: 4.1349463Losses:  4.036293029785156 0.1289394646883011
CurrentTrain: epoch  6, batch    21 | loss: 4.1652327Losses:  4.07053279876709 0.1001150906085968
CurrentTrain: epoch  6, batch    22 | loss: 4.1706481Losses:  3.9753870964050293 0.12773333489894867
CurrentTrain: epoch  6, batch    23 | loss: 4.1031203Losses:  3.993598222732544 0.11171624809503555
CurrentTrain: epoch  6, batch    24 | loss: 4.1053143Losses:  4.0206708908081055 0.12509097158908844
CurrentTrain: epoch  6, batch    25 | loss: 4.1457620Losses:  4.019475936889648 0.11870544403791428
CurrentTrain: epoch  6, batch    26 | loss: 4.1381812Losses:  4.1869707107543945 0.07709167897701263
CurrentTrain: epoch  6, batch    27 | loss: 4.2640624Losses:  4.026072025299072 0.17127497494220734
CurrentTrain: epoch  6, batch    28 | loss: 4.1973472Losses:  4.007234573364258 0.11443446576595306
CurrentTrain: epoch  6, batch    29 | loss: 4.1216688Losses:  4.023841857910156 0.07014452666044235
CurrentTrain: epoch  6, batch    30 | loss: 4.0939865Losses:  4.030015468597412 0.05261433869600296
CurrentTrain: epoch  6, batch    31 | loss: 4.0826297Losses:  3.9916296005249023 0.15006545186042786
CurrentTrain: epoch  6, batch    32 | loss: 4.1416950Losses:  4.019105911254883 0.1567809283733368
CurrentTrain: epoch  6, batch    33 | loss: 4.1758866Losses:  4.0522966384887695 0.1581428050994873
CurrentTrain: epoch  6, batch    34 | loss: 4.2104397Losses:  4.012655258178711 0.12175455689430237
CurrentTrain: epoch  6, batch    35 | loss: 4.1344099Losses:  4.0766377449035645 0.1094839796423912
CurrentTrain: epoch  6, batch    36 | loss: 4.1861219Losses:  3.9763944149017334 0.08555279672145844
CurrentTrain: epoch  6, batch    37 | loss: 4.0619473Losses:  3.992025375366211 0.08357702195644379
CurrentTrain: epoch  6, batch    38 | loss: 4.0756025Losses:  4.026569366455078 0.12864750623703003
CurrentTrain: epoch  6, batch    39 | loss: 4.1552167Losses:  3.9899327754974365 0.1398136019706726
CurrentTrain: epoch  6, batch    40 | loss: 4.1297464Losses:  3.9781088829040527 0.16611820459365845
CurrentTrain: epoch  6, batch    41 | loss: 4.1442270Losses:  4.007132530212402 0.09617647528648376
CurrentTrain: epoch  6, batch    42 | loss: 4.1033092Losses:  4.049502372741699 0.16733500361442566
CurrentTrain: epoch  6, batch    43 | loss: 4.2168374Losses:  3.9877395629882812 0.1382874995470047
CurrentTrain: epoch  6, batch    44 | loss: 4.1260271Losses:  3.997642755508423 0.05758229270577431
CurrentTrain: epoch  6, batch    45 | loss: 4.0552249Losses:  4.026085376739502 0.15508727729320526
CurrentTrain: epoch  6, batch    46 | loss: 4.1811728Losses:  4.052100658416748 0.10041002929210663
CurrentTrain: epoch  6, batch    47 | loss: 4.1525106Losses:  4.096986770629883 0.15410849452018738
CurrentTrain: epoch  6, batch    48 | loss: 4.2510953Losses:  3.9504942893981934 0.13178329169750214
CurrentTrain: epoch  6, batch    49 | loss: 4.0822778Losses:  4.016580581665039 0.053425244987010956
CurrentTrain: epoch  6, batch    50 | loss: 4.0700059Losses:  4.03055477142334 0.07587297260761261
CurrentTrain: epoch  6, batch    51 | loss: 4.1064277Losses:  4.052219390869141 0.08084851503372192
CurrentTrain: epoch  6, batch    52 | loss: 4.1330681Losses:  4.043766498565674 0.05308924615383148
CurrentTrain: epoch  6, batch    53 | loss: 4.0968556Losses:  4.01308536529541 0.2138461470603943
CurrentTrain: epoch  6, batch    54 | loss: 4.2269316Losses:  3.9924232959747314 0.11038218438625336
CurrentTrain: epoch  6, batch    55 | loss: 4.1028056Losses:  4.026443004608154 0.04011924937367439
CurrentTrain: epoch  6, batch    56 | loss: 4.0665622Losses:  3.9926888942718506 0.09502791613340378
CurrentTrain: epoch  6, batch    57 | loss: 4.0877166Losses:  4.0015869140625 0.09265494346618652
CurrentTrain: epoch  6, batch    58 | loss: 4.0942421Losses:  4.010824203491211 0.1294979453086853
CurrentTrain: epoch  6, batch    59 | loss: 4.1403222Losses:  3.9695184230804443 0.09159664809703827
CurrentTrain: epoch  6, batch    60 | loss: 4.0611153Losses:  4.00883674621582 0.10304982960224152
CurrentTrain: epoch  6, batch    61 | loss: 4.1118865Losses:  4.103617191314697 0.15199095010757446
CurrentTrain: epoch  6, batch    62 | loss: 4.2556081Losses:  3.990455389022827 0.15525655448436737
CurrentTrain: epoch  6, batch    63 | loss: 4.1457119Losses:  3.997673988342285 0.1417425572872162
CurrentTrain: epoch  6, batch    64 | loss: 4.1394167Losses:  4.132814407348633 0.07791431248188019
CurrentTrain: epoch  6, batch    65 | loss: 4.2107286Losses:  4.022298812866211 0.11997540295124054
CurrentTrain: epoch  6, batch    66 | loss: 4.1422744Losses:  4.059299468994141 0.06516160815954208
CurrentTrain: epoch  6, batch    67 | loss: 4.1244612Losses:  4.055204391479492 0.07957759499549866
CurrentTrain: epoch  6, batch    68 | loss: 4.1347818Losses:  4.096662998199463 0.13340477645397186
CurrentTrain: epoch  6, batch    69 | loss: 4.2300677Losses:  4.0333251953125 0.060386233031749725
CurrentTrain: epoch  6, batch    70 | loss: 4.0937114Losses:  4.034660339355469 0.057992059737443924
CurrentTrain: epoch  6, batch    71 | loss: 4.0926523Losses:  4.019994735717773 0.08249074220657349
CurrentTrain: epoch  6, batch    72 | loss: 4.1024857Losses:  4.055807113647461 0.09951774030923843
CurrentTrain: epoch  6, batch    73 | loss: 4.1553249Losses:  4.070354461669922 0.1034885123372078
CurrentTrain: epoch  6, batch    74 | loss: 4.1738429Losses:  4.0464768409729 0.08941600471735
CurrentTrain: epoch  6, batch    75 | loss: 4.1358929Losses:  4.0190606117248535 0.10571134090423584
CurrentTrain: epoch  6, batch    76 | loss: 4.1247721Losses:  3.9938712120056152 0.09707550704479218
CurrentTrain: epoch  6, batch    77 | loss: 4.0909467Losses:  4.09849739074707 0.031120561063289642
CurrentTrain: epoch  6, batch    78 | loss: 4.1296182Losses:  4.036190032958984 0.11069419980049133
CurrentTrain: epoch  6, batch    79 | loss: 4.1468844Losses:  4.115077018737793 0.10835044085979462
CurrentTrain: epoch  6, batch    80 | loss: 4.2234273Losses:  4.107072353363037 0.1058388203382492
CurrentTrain: epoch  6, batch    81 | loss: 4.2129111Losses:  4.003811836242676 0.035454899072647095
CurrentTrain: epoch  6, batch    82 | loss: 4.0392666Losses:  4.021864891052246 0.11850102990865707
CurrentTrain: epoch  6, batch    83 | loss: 4.1403661Losses:  4.044745922088623 0.13905031979084015
CurrentTrain: epoch  6, batch    84 | loss: 4.1837964Losses:  3.9957797527313232 0.11407442390918732
CurrentTrain: epoch  6, batch    85 | loss: 4.1098542Losses:  4.089299201965332 0.07094615697860718
CurrentTrain: epoch  6, batch    86 | loss: 4.1602454Losses:  4.034717559814453 0.14018531143665314
CurrentTrain: epoch  6, batch    87 | loss: 4.1749029Losses:  4.041695594787598 0.06281575560569763
CurrentTrain: epoch  6, batch    88 | loss: 4.1045113Losses:  4.065621376037598 0.10872301459312439
CurrentTrain: epoch  6, batch    89 | loss: 4.1743445Losses:  4.058955192565918 0.043997861444950104
CurrentTrain: epoch  6, batch    90 | loss: 4.1029530Losses:  4.001628875732422 0.08850124478340149
CurrentTrain: epoch  6, batch    91 | loss: 4.0901303Losses:  3.983663558959961 0.129172682762146
CurrentTrain: epoch  6, batch    92 | loss: 4.1128364Losses:  3.988257884979248 0.14298208057880402
CurrentTrain: epoch  6, batch    93 | loss: 4.1312399Losses:  4.116641044616699 0.18921776115894318
CurrentTrain: epoch  6, batch    94 | loss: 4.3058586Losses:  4.028892517089844 0.14857414364814758
CurrentTrain: epoch  6, batch    95 | loss: 4.1774669Losses:  4.04644250869751 0.19332775473594666
CurrentTrain: epoch  6, batch    96 | loss: 4.2397704Losses:  4.051644325256348 0.07420042157173157
CurrentTrain: epoch  6, batch    97 | loss: 4.1258450Losses:  4.098578453063965 0.1097678393125534
CurrentTrain: epoch  6, batch    98 | loss: 4.2083464Losses:  3.930866003036499 0.0803268626332283
CurrentTrain: epoch  6, batch    99 | loss: 4.0111928Losses:  4.084481239318848 0.1389082968235016
CurrentTrain: epoch  6, batch   100 | loss: 4.2233896Losses:  4.060928821563721 0.1234968900680542
CurrentTrain: epoch  6, batch   101 | loss: 4.1844258Losses:  3.980705976486206 0.11981029063463211
CurrentTrain: epoch  6, batch   102 | loss: 4.1005163Losses:  3.969163417816162 0.10077108442783356
CurrentTrain: epoch  6, batch   103 | loss: 4.0699344Losses:  4.026615619659424 0.1082887277007103
CurrentTrain: epoch  6, batch   104 | loss: 4.1349044Losses:  4.000310897827148 0.1408088356256485
CurrentTrain: epoch  6, batch   105 | loss: 4.1411200Losses:  4.050379753112793 0.1457165777683258
CurrentTrain: epoch  6, batch   106 | loss: 4.1960964Losses:  3.9910778999328613 0.1622200906276703
CurrentTrain: epoch  6, batch   107 | loss: 4.1532979Losses:  4.016858100891113 0.0936061292886734
CurrentTrain: epoch  6, batch   108 | loss: 4.1104641Losses:  4.022037506103516 0.11737728118896484
CurrentTrain: epoch  6, batch   109 | loss: 4.1394148Losses:  3.9984049797058105 0.07570961117744446
CurrentTrain: epoch  6, batch   110 | loss: 4.0741148Losses:  4.029295444488525 0.09371800720691681
CurrentTrain: epoch  6, batch   111 | loss: 4.1230135Losses:  4.029475212097168 0.09750057011842728
CurrentTrain: epoch  6, batch   112 | loss: 4.1269760Losses:  4.029275417327881 0.1024455651640892
CurrentTrain: epoch  6, batch   113 | loss: 4.1317210Losses:  4.06351900100708 0.0690203383564949
CurrentTrain: epoch  6, batch   114 | loss: 4.1325393Losses:  4.031503677368164 0.11550559848546982
CurrentTrain: epoch  6, batch   115 | loss: 4.1470094Losses:  3.9552316665649414 0.05099580064415932
CurrentTrain: epoch  6, batch   116 | loss: 4.0062275Losses:  3.994480609893799 0.12058047205209732
CurrentTrain: epoch  6, batch   117 | loss: 4.1150613Losses:  4.014307022094727 0.07927574217319489
CurrentTrain: epoch  6, batch   118 | loss: 4.0935826Losses:  3.9777145385742188 0.14061269164085388
CurrentTrain: epoch  6, batch   119 | loss: 4.1183271Losses:  4.462308883666992 0.08595634996891022
CurrentTrain: epoch  6, batch   120 | loss: 4.5482655Losses:  4.043512344360352 0.0975990742444992
CurrentTrain: epoch  6, batch   121 | loss: 4.1411114Losses:  4.078548431396484 0.09434264898300171
CurrentTrain: epoch  6, batch   122 | loss: 4.1728911Losses:  4.075964450836182 0.08936816453933716
CurrentTrain: epoch  6, batch   123 | loss: 4.1653328Losses:  3.9045205116271973 0.09962090849876404
CurrentTrain: epoch  6, batch   124 | loss: 4.0041413Losses:  3.994755268096924 0.07733966410160065
CurrentTrain: epoch  7, batch     0 | loss: 4.0720949Losses:  4.008241176605225 0.10940534621477127
CurrentTrain: epoch  7, batch     1 | loss: 4.1176467Losses:  4.057243347167969 0.024908680468797684
CurrentTrain: epoch  7, batch     2 | loss: 4.0821519Losses:  4.000018119812012 0.11048062890768051
CurrentTrain: epoch  7, batch     3 | loss: 4.1104989Losses:  3.982046365737915 0.062230974435806274
CurrentTrain: epoch  7, batch     4 | loss: 4.0442772Losses:  4.0397491455078125 0.013004623353481293
CurrentTrain: epoch  7, batch     5 | loss: 4.0527539Losses:  4.0402092933654785 0.09333354234695435
CurrentTrain: epoch  7, batch     6 | loss: 4.1335430Losses:  4.028327941894531 0.11490355432033539
CurrentTrain: epoch  7, batch     7 | loss: 4.1432314Losses:  4.020810604095459 0.13320091366767883
CurrentTrain: epoch  7, batch     8 | loss: 4.1540117Losses:  4.060823917388916 0.13787563145160675
CurrentTrain: epoch  7, batch     9 | loss: 4.1986995Losses:  3.9614782333374023 0.10692042112350464
CurrentTrain: epoch  7, batch    10 | loss: 4.0683985Losses:  4.038098335266113 0.08593923598527908
CurrentTrain: epoch  7, batch    11 | loss: 4.1240377Losses:  4.070323944091797 0.05226016044616699
CurrentTrain: epoch  7, batch    12 | loss: 4.1225843Losses:  3.966698169708252 0.10338091850280762
CurrentTrain: epoch  7, batch    13 | loss: 4.0700788Losses:  4.027135848999023 0.1334504634141922
CurrentTrain: epoch  7, batch    14 | loss: 4.1605864Losses:  4.061583518981934 0.08959151804447174
CurrentTrain: epoch  7, batch    15 | loss: 4.1511750Losses:  3.9808950424194336 0.09936146438121796
CurrentTrain: epoch  7, batch    16 | loss: 4.0802565Losses:  3.9668796062469482 0.054094262421131134
CurrentTrain: epoch  7, batch    17 | loss: 4.0209737Losses:  4.087536811828613 0.16810131072998047
CurrentTrain: epoch  7, batch    18 | loss: 4.2556381Losses:  4.0367112159729 0.09226321429014206
CurrentTrain: epoch  7, batch    19 | loss: 4.1289744Losses:  4.047623634338379 0.09796997159719467
CurrentTrain: epoch  7, batch    20 | loss: 4.1455936Losses:  4.039520263671875 0.1702328324317932
CurrentTrain: epoch  7, batch    21 | loss: 4.2097530Losses:  4.0142669677734375 0.06510605663061142
CurrentTrain: epoch  7, batch    22 | loss: 4.0793729Losses:  4.040534019470215 0.13159820437431335
CurrentTrain: epoch  7, batch    23 | loss: 4.1721320Losses:  4.0431742668151855 0.05617136508226395
CurrentTrain: epoch  7, batch    24 | loss: 4.0993457Losses:  3.995272159576416 0.13877694308757782
CurrentTrain: epoch  7, batch    25 | loss: 4.1340489Losses:  3.9924001693725586 0.11343580484390259
CurrentTrain: epoch  7, batch    26 | loss: 4.1058359Losses:  3.9782795906066895 0.10931491106748581
CurrentTrain: epoch  7, batch    27 | loss: 4.0875945Losses:  4.0511298179626465 0.1224004253745079
CurrentTrain: epoch  7, batch    28 | loss: 4.1735301Losses:  3.9434547424316406 0.11335936188697815
CurrentTrain: epoch  7, batch    29 | loss: 4.0568142Losses:  4.0193657875061035 0.06935979425907135
CurrentTrain: epoch  7, batch    30 | loss: 4.0887256Losses:  3.9884676933288574 0.1126445084810257
CurrentTrain: epoch  7, batch    31 | loss: 4.1011124Losses:  4.079336166381836 0.04517363756895065
CurrentTrain: epoch  7, batch    32 | loss: 4.1245098Losses:  3.9813523292541504 0.12898321449756622
CurrentTrain: epoch  7, batch    33 | loss: 4.1103354Losses:  4.032558441162109 0.1200597882270813
CurrentTrain: epoch  7, batch    34 | loss: 4.1526184Losses:  3.9659359455108643 0.10259106755256653
CurrentTrain: epoch  7, batch    35 | loss: 4.0685272Losses:  4.010038375854492 0.1255076676607132
CurrentTrain: epoch  7, batch    36 | loss: 4.1355462Losses:  4.035356521606445 0.07193834334611893
CurrentTrain: epoch  7, batch    37 | loss: 4.1072950Losses:  4.025907516479492 0.12000949680805206
CurrentTrain: epoch  7, batch    38 | loss: 4.1459169Losses:  4.003841400146484 0.14025986194610596
CurrentTrain: epoch  7, batch    39 | loss: 4.1441011Losses:  3.985978126525879 0.08609434217214584
CurrentTrain: epoch  7, batch    40 | loss: 4.0720725Losses:  4.007425308227539 0.09740443527698517
CurrentTrain: epoch  7, batch    41 | loss: 4.1048298Losses:  4.025821685791016 0.15264186263084412
CurrentTrain: epoch  7, batch    42 | loss: 4.1784635Losses:  3.974813222885132 0.1365056037902832
CurrentTrain: epoch  7, batch    43 | loss: 4.1113186Losses:  4.089522361755371 0.08826605975627899
CurrentTrain: epoch  7, batch    44 | loss: 4.1777883Losses:  4.037171363830566 0.06491275876760483
CurrentTrain: epoch  7, batch    45 | loss: 4.1020842Losses:  3.9912776947021484 0.09874377399682999
CurrentTrain: epoch  7, batch    46 | loss: 4.0900216Losses:  3.995509624481201 0.10134716331958771
CurrentTrain: epoch  7, batch    47 | loss: 4.0968566Losses:  3.9484758377075195 0.08486740291118622
CurrentTrain: epoch  7, batch    48 | loss: 4.0333433Losses:  3.9903271198272705 0.07061450183391571
CurrentTrain: epoch  7, batch    49 | loss: 4.0609417Losses:  4.005256652832031 0.08442150056362152
CurrentTrain: epoch  7, batch    50 | loss: 4.0896783Losses:  4.001487731933594 0.13289766013622284
CurrentTrain: epoch  7, batch    51 | loss: 4.1343856Losses:  4.013408184051514 0.05452708899974823
CurrentTrain: epoch  7, batch    52 | loss: 4.0679355Losses:  4.068638801574707 0.07333029061555862
CurrentTrain: epoch  7, batch    53 | loss: 4.1419692Losses:  4.037970542907715 0.12033174932003021
CurrentTrain: epoch  7, batch    54 | loss: 4.1583023Losses:  3.9964873790740967 0.12665671110153198
CurrentTrain: epoch  7, batch    55 | loss: 4.1231441Losses:  4.003408432006836 0.09283946454524994
CurrentTrain: epoch  7, batch    56 | loss: 4.0962477Losses:  3.9694876670837402 0.11419273167848587
CurrentTrain: epoch  7, batch    57 | loss: 4.0836806Losses:  3.9951748847961426 0.11850704252719879
CurrentTrain: epoch  7, batch    58 | loss: 4.1136818Losses:  3.9940216541290283 0.12127377092838287
CurrentTrain: epoch  7, batch    59 | loss: 4.1152954Losses:  3.9865341186523438 0.11302404850721359
CurrentTrain: epoch  7, batch    60 | loss: 4.0995584Losses:  4.0546441078186035 0.08049459010362625
CurrentTrain: epoch  7, batch    61 | loss: 4.1351385Losses:  4.059910774230957 0.09191896766424179
CurrentTrain: epoch  7, batch    62 | loss: 4.1518297Losses:  4.023113250732422 0.06602339446544647
CurrentTrain: epoch  7, batch    63 | loss: 4.0891366Losses:  4.0457305908203125 0.09432929754257202
CurrentTrain: epoch  7, batch    64 | loss: 4.1400599Losses:  4.030935287475586 0.10318552702665329
CurrentTrain: epoch  7, batch    65 | loss: 4.1341209Losses:  4.049750328063965 0.08074231445789337
CurrentTrain: epoch  7, batch    66 | loss: 4.1304927Losses:  4.052519798278809 0.07653041183948517
CurrentTrain: epoch  7, batch    67 | loss: 4.1290503Losses:  4.010128974914551 0.08165997266769409
CurrentTrain: epoch  7, batch    68 | loss: 4.0917888Losses:  3.9927351474761963 0.09858787059783936
CurrentTrain: epoch  7, batch    69 | loss: 4.0913229Losses:  4.043585777282715 0.0893896296620369
CurrentTrain: epoch  7, batch    70 | loss: 4.1329756Losses:  4.000080108642578 0.0977194532752037
CurrentTrain: epoch  7, batch    71 | loss: 4.0977998Losses:  4.036679267883301 0.10404974967241287
CurrentTrain: epoch  7, batch    72 | loss: 4.1407290Losses:  4.020896911621094 0.10842310637235641
CurrentTrain: epoch  7, batch    73 | loss: 4.1293201Losses:  4.004265785217285 0.045292407274246216
CurrentTrain: epoch  7, batch    74 | loss: 4.0495582Losses:  3.96203351020813 0.10842414200305939
CurrentTrain: epoch  7, batch    75 | loss: 4.0704575Losses:  3.931119680404663 0.0885791927576065
CurrentTrain: epoch  7, batch    76 | loss: 4.0196991Losses:  4.043529987335205 0.09984683990478516
CurrentTrain: epoch  7, batch    77 | loss: 4.1433768Losses:  4.009543418884277 0.10570865869522095
CurrentTrain: epoch  7, batch    78 | loss: 4.1152520Losses:  4.070131301879883 0.03892761468887329
CurrentTrain: epoch  7, batch    79 | loss: 4.1090589Losses:  4.026183128356934 0.13435566425323486
CurrentTrain: epoch  7, batch    80 | loss: 4.1605387Losses:  4.008301258087158 0.062345974147319794
CurrentTrain: epoch  7, batch    81 | loss: 4.0706472Losses:  3.9945881366729736 0.1302131712436676
CurrentTrain: epoch  7, batch    82 | loss: 4.1248012Losses:  4.018746376037598 0.08707325160503387
CurrentTrain: epoch  7, batch    83 | loss: 4.1058197Losses:  4.064319133758545 0.10561975091695786
CurrentTrain: epoch  7, batch    84 | loss: 4.1699390Losses:  4.026305675506592 0.09441132098436356
CurrentTrain: epoch  7, batch    85 | loss: 4.1207170Losses:  3.996896982192993 0.05986206978559494
CurrentTrain: epoch  7, batch    86 | loss: 4.0567589Losses:  4.009613990783691 0.07285718619823456
CurrentTrain: epoch  7, batch    87 | loss: 4.0824714Losses:  4.006996154785156 0.09931626915931702
CurrentTrain: epoch  7, batch    88 | loss: 4.1063123Losses:  4.000817775726318 0.11858417093753815
CurrentTrain: epoch  7, batch    89 | loss: 4.1194019Losses:  4.04966926574707 0.06102060154080391
CurrentTrain: epoch  7, batch    90 | loss: 4.1106896Losses:  3.9731554985046387 0.0970500037074089
CurrentTrain: epoch  7, batch    91 | loss: 4.0702057Losses:  4.013415336608887 0.04120630770921707
CurrentTrain: epoch  7, batch    92 | loss: 4.0546217Losses:  4.039888381958008 0.12270662933588028
CurrentTrain: epoch  7, batch    93 | loss: 4.1625948Losses:  4.009003162384033 0.05346297472715378
CurrentTrain: epoch  7, batch    94 | loss: 4.0624661Losses:  3.983400821685791 0.12724396586418152
CurrentTrain: epoch  7, batch    95 | loss: 4.1106448Losses:  3.9548027515411377 0.03593187779188156
CurrentTrain: epoch  7, batch    96 | loss: 3.9907346Losses:  4.022319316864014 0.06148774176836014
CurrentTrain: epoch  7, batch    97 | loss: 4.0838070Losses:  3.988856315612793 0.06624890118837357
CurrentTrain: epoch  7, batch    98 | loss: 4.0551052Losses:  3.9768903255462646 0.1206904724240303
CurrentTrain: epoch  7, batch    99 | loss: 4.0975809Losses:  4.02308464050293 0.09311475604772568
CurrentTrain: epoch  7, batch   100 | loss: 4.1161995Losses:  3.9976069927215576 0.0594487190246582
CurrentTrain: epoch  7, batch   101 | loss: 4.0570555Losses:  3.9895169734954834 0.1299600899219513
CurrentTrain: epoch  7, batch   102 | loss: 4.1194773Losses:  4.009788990020752 0.10950709134340286
CurrentTrain: epoch  7, batch   103 | loss: 4.1192961Losses:  4.043001651763916 0.10868243873119354
CurrentTrain: epoch  7, batch   104 | loss: 4.1516843Losses:  3.901024341583252 0.10349860787391663
CurrentTrain: epoch  7, batch   105 | loss: 4.0045228Losses:  3.9578750133514404 0.11752835661172867
CurrentTrain: epoch  7, batch   106 | loss: 4.0754032Losses:  3.9414889812469482 0.06614526361227036
CurrentTrain: epoch  7, batch   107 | loss: 4.0076342Losses:  4.059647083282471 0.1083497479557991
CurrentTrain: epoch  7, batch   108 | loss: 4.1679969Losses:  3.995699882507324 0.08257655799388885
CurrentTrain: epoch  7, batch   109 | loss: 4.0782766Losses:  3.9836153984069824 0.08345945179462433
CurrentTrain: epoch  7, batch   110 | loss: 4.0670748Losses:  3.9874842166900635 0.1288750320672989
CurrentTrain: epoch  7, batch   111 | loss: 4.1163592Losses:  3.9714040756225586 0.12498627603054047
CurrentTrain: epoch  7, batch   112 | loss: 4.0963902Losses:  3.977269172668457 0.10424768924713135
CurrentTrain: epoch  7, batch   113 | loss: 4.0815167Losses:  4.025328159332275 0.09267103672027588
CurrentTrain: epoch  7, batch   114 | loss: 4.1179991Losses:  4.001220703125 0.056857138872146606
CurrentTrain: epoch  7, batch   115 | loss: 4.0580778Losses:  4.014615058898926 0.09788890182971954
CurrentTrain: epoch  7, batch   116 | loss: 4.1125040Losses:  4.015297889709473 0.06366933882236481
CurrentTrain: epoch  7, batch   117 | loss: 4.0789671Losses:  3.932856559753418 0.09056437760591507
CurrentTrain: epoch  7, batch   118 | loss: 4.0234208Losses:  4.0547776222229 0.025031089782714844
CurrentTrain: epoch  7, batch   119 | loss: 4.0798087Losses:  4.056768894195557 0.08147962391376495
CurrentTrain: epoch  7, batch   120 | loss: 4.1382484Losses:  3.962559700012207 0.06702591478824615
CurrentTrain: epoch  7, batch   121 | loss: 4.0295858Losses:  4.019360065460205 0.07478825747966766
CurrentTrain: epoch  7, batch   122 | loss: 4.0941482Losses:  4.039311408996582 0.06595062464475632
CurrentTrain: epoch  7, batch   123 | loss: 4.1052618Losses:  3.982862949371338 0.10500659793615341
CurrentTrain: epoch  7, batch   124 | loss: 4.0878696Losses:  4.014313220977783 0.12411364912986755
CurrentTrain: epoch  8, batch     0 | loss: 4.1384268Losses:  4.009112358093262 0.10417427122592926
CurrentTrain: epoch  8, batch     1 | loss: 4.1132865Losses:  4.011913299560547 0.09856594353914261
CurrentTrain: epoch  8, batch     2 | loss: 4.1104794Losses:  3.9861104488372803 0.08187646418809891
CurrentTrain: epoch  8, batch     3 | loss: 4.0679870Losses:  3.9710192680358887 0.04436125606298447
CurrentTrain: epoch  8, batch     4 | loss: 4.0153804Losses:  3.976341724395752 0.06521531194448471
CurrentTrain: epoch  8, batch     5 | loss: 4.0415568Losses:  3.9157028198242188 0.04243965074419975
CurrentTrain: epoch  8, batch     6 | loss: 3.9581425Losses:  4.057088851928711 0.0549028143286705
CurrentTrain: epoch  8, batch     7 | loss: 4.1119919Losses:  4.036853790283203 0.08160349726676941
CurrentTrain: epoch  8, batch     8 | loss: 4.1184573Losses:  3.9679317474365234 0.09108700603246689
CurrentTrain: epoch  8, batch     9 | loss: 4.0590186Losses:  3.9997215270996094 0.10168787837028503
CurrentTrain: epoch  8, batch    10 | loss: 4.1014094Losses:  4.042598724365234 0.09132882207632065
CurrentTrain: epoch  8, batch    11 | loss: 4.1339273Losses:  4.018131732940674 0.07795362174510956
CurrentTrain: epoch  8, batch    12 | loss: 4.0960855Losses:  3.997941732406616 0.1331484317779541
CurrentTrain: epoch  8, batch    13 | loss: 4.1310902Losses:  4.010863304138184 0.13011357188224792
CurrentTrain: epoch  8, batch    14 | loss: 4.1409769Losses:  3.9520087242126465 0.09243623167276382
CurrentTrain: epoch  8, batch    15 | loss: 4.0444450Losses:  4.027709007263184 0.07809349149465561
CurrentTrain: epoch  8, batch    16 | loss: 4.1058025Losses:  3.971714496612549 0.08182752132415771
CurrentTrain: epoch  8, batch    17 | loss: 4.0535421Losses:  4.000230312347412 0.05951371788978577
CurrentTrain: epoch  8, batch    18 | loss: 4.0597439Losses:  4.040337562561035 0.06742806732654572
CurrentTrain: epoch  8, batch    19 | loss: 4.1077657Losses:  3.9839370250701904 0.05642511695623398
CurrentTrain: epoch  8, batch    20 | loss: 4.0403624Losses:  3.962739944458008 0.04797353595495224
CurrentTrain: epoch  8, batch    21 | loss: 4.0107136Losses:  4.0087761878967285 0.10727080702781677
CurrentTrain: epoch  8, batch    22 | loss: 4.1160469Losses:  4.060945510864258 0.061240121722221375
CurrentTrain: epoch  8, batch    23 | loss: 4.1221857Losses:  4.0258870124816895 0.05921228229999542
CurrentTrain: epoch  8, batch    24 | loss: 4.0850992Losses:  4.017704963684082 0.062458474189043045
CurrentTrain: epoch  8, batch    25 | loss: 4.0801635Losses:  4.004772663116455 0.10992080718278885
CurrentTrain: epoch  8, batch    26 | loss: 4.1146936Losses:  3.9734678268432617 0.07266632467508316
CurrentTrain: epoch  8, batch    27 | loss: 4.0461340Losses:  3.9555351734161377 0.0416547954082489
CurrentTrain: epoch  8, batch    28 | loss: 3.9971900Losses:  3.994372606277466 0.06903772801160812
CurrentTrain: epoch  8, batch    29 | loss: 4.0634103Losses:  4.02398681640625 0.08551956713199615
CurrentTrain: epoch  8, batch    30 | loss: 4.1095066Losses:  3.9481844902038574 0.07735484838485718
CurrentTrain: epoch  8, batch    31 | loss: 4.0255394Losses:  3.953986644744873 0.08655434846878052
CurrentTrain: epoch  8, batch    32 | loss: 4.0405412Losses:  4.008589267730713 0.11186806857585907
CurrentTrain: epoch  8, batch    33 | loss: 4.1204572Losses:  3.9567623138427734 0.08365349471569061
CurrentTrain: epoch  8, batch    34 | loss: 4.0404158Losses:  4.00021505355835 0.032575830817222595
CurrentTrain: epoch  8, batch    35 | loss: 4.0327907Losses:  4.0169477462768555 0.11105481535196304
CurrentTrain: epoch  8, batch    36 | loss: 4.1280026Losses:  3.966710090637207 0.07053503394126892
CurrentTrain: epoch  8, batch    37 | loss: 4.0372453Losses:  4.00590181350708 0.1265798807144165
CurrentTrain: epoch  8, batch    38 | loss: 4.1324816Losses:  4.009402275085449 0.1115926206111908
CurrentTrain: epoch  8, batch    39 | loss: 4.1209950Losses:  3.9669504165649414 0.08551393449306488
CurrentTrain: epoch  8, batch    40 | loss: 4.0524645Losses:  3.9941699504852295 0.1357426643371582
CurrentTrain: epoch  8, batch    41 | loss: 4.1299124Losses:  3.9936227798461914 0.04597090929746628
CurrentTrain: epoch  8, batch    42 | loss: 4.0395937Losses:  4.071552753448486 0.09263977408409119
CurrentTrain: epoch  8, batch    43 | loss: 4.1641927Losses:  3.936215877532959 0.04294900968670845
CurrentTrain: epoch  8, batch    44 | loss: 3.9791648Losses:  3.9908697605133057 0.1106819361448288
CurrentTrain: epoch  8, batch    45 | loss: 4.1015515Losses:  3.939500331878662 0.10867205262184143
CurrentTrain: epoch  8, batch    46 | loss: 4.0481725Losses:  3.9719631671905518 0.06949856877326965
CurrentTrain: epoch  8, batch    47 | loss: 4.0414619Losses:  4.009446620941162 0.09962067008018494
CurrentTrain: epoch  8, batch    48 | loss: 4.1090674Losses:  3.9731898307800293 0.08847188949584961
CurrentTrain: epoch  8, batch    49 | loss: 4.0616617Losses:  3.99015474319458 0.12196893990039825
CurrentTrain: epoch  8, batch    50 | loss: 4.1121235Losses:  3.9164671897888184 0.07134852558374405
CurrentTrain: epoch  8, batch    51 | loss: 3.9878156Losses:  4.027285099029541 0.06322432309389114
CurrentTrain: epoch  8, batch    52 | loss: 4.0905094Losses:  3.9732489585876465 0.10990900546312332
CurrentTrain: epoch  8, batch    53 | loss: 4.0831580Losses:  3.9951422214508057 0.11709602922201157
CurrentTrain: epoch  8, batch    54 | loss: 4.1122384Losses:  4.018959045410156 0.03686877712607384
CurrentTrain: epoch  8, batch    55 | loss: 4.0558276Losses:  3.921043634414673 0.06655909866094589
CurrentTrain: epoch  8, batch    56 | loss: 3.9876027Losses:  3.9885706901550293 0.10913605988025665
CurrentTrain: epoch  8, batch    57 | loss: 4.0977068Losses:  3.993760108947754 0.10281819105148315
CurrentTrain: epoch  8, batch    58 | loss: 4.0965781Losses:  4.016782283782959 0.049903713166713715
CurrentTrain: epoch  8, batch    59 | loss: 4.0666862Losses:  3.9243764877319336 0.061242423951625824
CurrentTrain: epoch  8, batch    60 | loss: 3.9856188Losses:  4.023675441741943 0.10410549491643906
CurrentTrain: epoch  8, batch    61 | loss: 4.1277809Losses:  3.9917807579040527 0.043584272265434265
CurrentTrain: epoch  8, batch    62 | loss: 4.0353651Losses:  3.996751070022583 0.12421677261590958
CurrentTrain: epoch  8, batch    63 | loss: 4.1209679Losses:  4.00202751159668 0.05156869813799858
CurrentTrain: epoch  8, batch    64 | loss: 4.0535960Losses:  4.036823272705078 0.07016311585903168
CurrentTrain: epoch  8, batch    65 | loss: 4.1069865Losses:  3.995264768600464 0.05713202804327011
CurrentTrain: epoch  8, batch    66 | loss: 4.0523968Losses:  4.028589248657227 0.10182467848062515
CurrentTrain: epoch  8, batch    67 | loss: 4.1304140Losses:  4.009219169616699 0.0837460607290268
CurrentTrain: epoch  8, batch    68 | loss: 4.0929651Losses:  3.976511240005493 0.08721676468849182
CurrentTrain: epoch  8, batch    69 | loss: 4.0637279Losses:  4.02577018737793 0.10247422754764557
CurrentTrain: epoch  8, batch    70 | loss: 4.1282444Losses:  3.9954957962036133 0.058734357357025146
CurrentTrain: epoch  8, batch    71 | loss: 4.0542302Losses:  3.9952731132507324 0.09812707453966141
CurrentTrain: epoch  8, batch    72 | loss: 4.0934000Losses:  4.002501010894775 0.0778682604432106
CurrentTrain: epoch  8, batch    73 | loss: 4.0803695Losses:  4.007433891296387 0.07815288007259369
CurrentTrain: epoch  8, batch    74 | loss: 4.0855865Losses:  4.008559226989746 0.09590552002191544
CurrentTrain: epoch  8, batch    75 | loss: 4.1044645Losses:  4.0200395584106445 0.0807126984000206
CurrentTrain: epoch  8, batch    76 | loss: 4.1007524Losses:  3.9439706802368164 0.10576732456684113
CurrentTrain: epoch  8, batch    77 | loss: 4.0497379Losses:  3.980440139770508 0.08603817969560623
CurrentTrain: epoch  8, batch    78 | loss: 4.0664783Losses:  3.995082378387451 0.05468986928462982
CurrentTrain: epoch  8, batch    79 | loss: 4.0497723Losses:  4.003091812133789 0.045040201395750046
CurrentTrain: epoch  8, batch    80 | loss: 4.0481319Losses:  3.970350742340088 0.0588744580745697
CurrentTrain: epoch  8, batch    81 | loss: 4.0292253Losses:  4.021850109100342 0.05068562924861908
CurrentTrain: epoch  8, batch    82 | loss: 4.0725355Losses:  3.9746415615081787 0.060592446476221085
CurrentTrain: epoch  8, batch    83 | loss: 4.0352340Losses:  4.018886566162109 0.0458107590675354
CurrentTrain: epoch  8, batch    84 | loss: 4.0646973Losses:  4.006081581115723 0.10366439819335938
CurrentTrain: epoch  8, batch    85 | loss: 4.1097460Losses:  3.993474245071411 0.08553849160671234
CurrentTrain: epoch  8, batch    86 | loss: 4.0790129Losses:  3.9687001705169678 0.05543246120214462
CurrentTrain: epoch  8, batch    87 | loss: 4.0241327Losses:  3.9547011852264404 0.1027817502617836
CurrentTrain: epoch  8, batch    88 | loss: 4.0574827Losses:  3.9874062538146973 0.07561405003070831
CurrentTrain: epoch  8, batch    89 | loss: 4.0630202Losses:  3.946168899536133 0.08664460480213165
CurrentTrain: epoch  8, batch    90 | loss: 4.0328135Losses:  4.021160125732422 0.07032056152820587
CurrentTrain: epoch  8, batch    91 | loss: 4.0914807Losses:  3.9611220359802246 0.11742492020130157
CurrentTrain: epoch  8, batch    92 | loss: 4.0785470Losses:  4.00242805480957 0.07783240079879761
CurrentTrain: epoch  8, batch    93 | loss: 4.0802603Losses:  3.9516448974609375 0.08847572654485703
CurrentTrain: epoch  8, batch    94 | loss: 4.0401206Losses:  4.079802513122559 0.07040556520223618
CurrentTrain: epoch  8, batch    95 | loss: 4.1502080Losses:  3.9284865856170654 0.06086895614862442
CurrentTrain: epoch  8, batch    96 | loss: 3.9893556Losses:  4.007992267608643 0.06461372971534729
CurrentTrain: epoch  8, batch    97 | loss: 4.0726061Losses:  4.014310359954834 0.06511025875806808
CurrentTrain: epoch  8, batch    98 | loss: 4.0794206Losses:  3.9398717880249023 0.05843795835971832
CurrentTrain: epoch  8, batch    99 | loss: 3.9983099Losses:  3.987339973449707 0.07486037909984589
CurrentTrain: epoch  8, batch   100 | loss: 4.0622005Losses:  3.9997801780700684 0.06548641622066498
CurrentTrain: epoch  8, batch   101 | loss: 4.0652666Losses:  3.9674923419952393 0.08671717345714569
CurrentTrain: epoch  8, batch   102 | loss: 4.0542097Losses:  3.949265480041504 0.04233670234680176
CurrentTrain: epoch  8, batch   103 | loss: 3.9916022Losses:  3.970078468322754 0.06642624735832214
CurrentTrain: epoch  8, batch   104 | loss: 4.0365047Losses:  3.993492603302002 0.06895941495895386
CurrentTrain: epoch  8, batch   105 | loss: 4.0624518Losses:  4.005985736846924 0.05803177133202553
CurrentTrain: epoch  8, batch   106 | loss: 4.0640173Losses:  3.967726707458496 0.12088252604007721
CurrentTrain: epoch  8, batch   107 | loss: 4.0886092Losses:  3.9406518936157227 0.09466391801834106
CurrentTrain: epoch  8, batch   108 | loss: 4.0353160Losses:  3.986834764480591 0.07236276566982269
CurrentTrain: epoch  8, batch   109 | loss: 4.0591974Losses:  3.9031691551208496 0.029799822717905045
CurrentTrain: epoch  8, batch   110 | loss: 3.9329691Losses:  3.993502616882324 0.09848707914352417
CurrentTrain: epoch  8, batch   111 | loss: 4.0919895Losses:  3.9245176315307617 0.0643351674079895
CurrentTrain: epoch  8, batch   112 | loss: 3.9888527Losses:  3.9598796367645264 0.10134734213352203
CurrentTrain: epoch  8, batch   113 | loss: 4.0612268Losses:  3.965839385986328 0.0925869345664978
CurrentTrain: epoch  8, batch   114 | loss: 4.0584264Losses:  3.9702467918395996 0.05401390790939331
CurrentTrain: epoch  8, batch   115 | loss: 4.0242605Losses:  3.9097633361816406 0.09942624717950821
CurrentTrain: epoch  8, batch   116 | loss: 4.0091896Losses:  3.958245038986206 0.11954168975353241
CurrentTrain: epoch  8, batch   117 | loss: 4.0777869Losses:  4.019408226013184 0.1255236268043518
CurrentTrain: epoch  8, batch   118 | loss: 4.1449318Losses:  3.9985196590423584 0.09142272174358368
CurrentTrain: epoch  8, batch   119 | loss: 4.0899425Losses:  3.9749011993408203 0.1429668813943863
CurrentTrain: epoch  8, batch   120 | loss: 4.1178679Losses:  3.992560386657715 0.0453430712223053
CurrentTrain: epoch  8, batch   121 | loss: 4.0379033Losses:  3.9562747478485107 0.11349080502986908
CurrentTrain: epoch  8, batch   122 | loss: 4.0697656Losses:  4.029491424560547 0.040634557604789734
CurrentTrain: epoch  8, batch   123 | loss: 4.0701261Losses:  3.9513821601867676 0.07327263057231903
CurrentTrain: epoch  8, batch   124 | loss: 4.0246549Losses:  3.925630569458008 0.09705419838428497
CurrentTrain: epoch  9, batch     0 | loss: 4.0226846Losses:  3.986769914627075 0.07639160752296448
CurrentTrain: epoch  9, batch     1 | loss: 4.0631614Losses:  3.9530677795410156 0.05290182679891586
CurrentTrain: epoch  9, batch     2 | loss: 4.0059695Losses:  3.9551279544830322 0.1328398436307907
CurrentTrain: epoch  9, batch     3 | loss: 4.0879679Losses:  3.9830689430236816 0.026323210448026657
CurrentTrain: epoch  9, batch     4 | loss: 4.0093923Losses:  3.9854798316955566 0.07404603809118271
CurrentTrain: epoch  9, batch     5 | loss: 4.0595260Losses:  3.9516751766204834 0.1212422102689743
CurrentTrain: epoch  9, batch     6 | loss: 4.0729175Losses:  3.980464220046997 0.0823512002825737
CurrentTrain: epoch  9, batch     7 | loss: 4.0628152Losses:  3.9751152992248535 0.07418255507946014
CurrentTrain: epoch  9, batch     8 | loss: 4.0492978Losses:  3.9862351417541504 0.10445614904165268
CurrentTrain: epoch  9, batch     9 | loss: 4.0906911Losses:  3.968545913696289 0.05043940246105194
CurrentTrain: epoch  9, batch    10 | loss: 4.0189853Losses:  3.941476583480835 0.07195857167243958
CurrentTrain: epoch  9, batch    11 | loss: 4.0134354Losses:  3.9866819381713867 0.0890444740653038
CurrentTrain: epoch  9, batch    12 | loss: 4.0757265Losses:  3.9785642623901367 0.1211751401424408
CurrentTrain: epoch  9, batch    13 | loss: 4.0997396Losses:  3.9211463928222656 0.045952215790748596
CurrentTrain: epoch  9, batch    14 | loss: 3.9670987Losses:  4.015469551086426 0.07498273998498917
CurrentTrain: epoch  9, batch    15 | loss: 4.0904522Losses:  4.054689407348633 0.050523802638053894
CurrentTrain: epoch  9, batch    16 | loss: 4.1052132Losses:  4.069595813751221 0.05404666066169739
CurrentTrain: epoch  9, batch    17 | loss: 4.1236424Losses:  3.9770255088806152 0.07060486823320389
CurrentTrain: epoch  9, batch    18 | loss: 4.0476303Losses:  3.9859399795532227 0.06063145026564598
CurrentTrain: epoch  9, batch    19 | loss: 4.0465713Losses:  3.9259591102600098 0.03452020138502121
CurrentTrain: epoch  9, batch    20 | loss: 3.9604793Losses:  3.9585063457489014 0.07927990704774857
CurrentTrain: epoch  9, batch    21 | loss: 4.0377865Losses:  3.976029872894287 0.08235637098550797
CurrentTrain: epoch  9, batch    22 | loss: 4.0583863Losses:  3.982652187347412 0.09811078757047653
CurrentTrain: epoch  9, batch    23 | loss: 4.0807629Losses:  3.9590766429901123 0.03341725096106529
CurrentTrain: epoch  9, batch    24 | loss: 3.9924939Losses:  4.009485721588135 0.08188077807426453
CurrentTrain: epoch  9, batch    25 | loss: 4.0913663Losses:  3.9716224670410156 0.06696158647537231
CurrentTrain: epoch  9, batch    26 | loss: 4.0385842Losses:  3.933260440826416 0.1215488389134407
CurrentTrain: epoch  9, batch    27 | loss: 4.0548091Losses:  3.9198756217956543 0.0799439400434494
CurrentTrain: epoch  9, batch    28 | loss: 3.9998195Losses:  3.985269784927368 0.08540041744709015
CurrentTrain: epoch  9, batch    29 | loss: 4.0706701Losses:  3.9748458862304688 0.0797329992055893
CurrentTrain: epoch  9, batch    30 | loss: 4.0545788Losses:  3.941648483276367 0.07001275569200516
CurrentTrain: epoch  9, batch    31 | loss: 4.0116611Losses:  3.9366183280944824 0.03783702850341797
CurrentTrain: epoch  9, batch    32 | loss: 3.9744554Losses:  3.96539306640625 0.10979674756526947
CurrentTrain: epoch  9, batch    33 | loss: 4.0751896Losses:  3.989666223526001 0.05872069671750069
CurrentTrain: epoch  9, batch    34 | loss: 4.0483871Losses:  3.980772018432617 0.08518340438604355
CurrentTrain: epoch  9, batch    35 | loss: 4.0659556Losses:  3.9677340984344482 0.082596555352211
CurrentTrain: epoch  9, batch    36 | loss: 4.0503306Losses:  3.9538702964782715 0.07859701663255692
CurrentTrain: epoch  9, batch    37 | loss: 4.0324674Losses:  3.9607746601104736 0.06300484389066696
CurrentTrain: epoch  9, batch    38 | loss: 4.0237794Losses:  3.969318389892578 0.04252680763602257
CurrentTrain: epoch  9, batch    39 | loss: 4.0118451Losses:  4.00400972366333 0.04706393927335739
CurrentTrain: epoch  9, batch    40 | loss: 4.0510736Losses:  3.9722812175750732 0.08039525896310806
CurrentTrain: epoch  9, batch    41 | loss: 4.0526767Losses:  3.994520664215088 0.0948718935251236
CurrentTrain: epoch  9, batch    42 | loss: 4.0893927Losses:  4.008786201477051 0.07225099205970764
CurrentTrain: epoch  9, batch    43 | loss: 4.0810370Losses:  4.008081912994385 0.05352234095335007
CurrentTrain: epoch  9, batch    44 | loss: 4.0616040Losses:  4.000357627868652 0.05890202522277832
CurrentTrain: epoch  9, batch    45 | loss: 4.0592594Losses:  3.9784555435180664 0.05866829305887222
CurrentTrain: epoch  9, batch    46 | loss: 4.0371237Losses:  3.9711670875549316 0.10045064985752106
CurrentTrain: epoch  9, batch    47 | loss: 4.0716176Losses:  3.960659980773926 0.0966590940952301
CurrentTrain: epoch  9, batch    48 | loss: 4.0573192Losses:  3.848376750946045 0.08556511998176575
CurrentTrain: epoch  9, batch    49 | loss: 3.9339418Losses:  3.9529635906219482 0.047074250876903534
CurrentTrain: epoch  9, batch    50 | loss: 4.0000377Losses:  3.9690890312194824 0.08955350518226624
CurrentTrain: epoch  9, batch    51 | loss: 4.0586424Losses:  3.900479316711426 0.04676976054906845
CurrentTrain: epoch  9, batch    52 | loss: 3.9472492Losses:  3.986794948577881 0.06863163411617279
CurrentTrain: epoch  9, batch    53 | loss: 4.0554266Losses:  3.946807861328125 0.06470920145511627
CurrentTrain: epoch  9, batch    54 | loss: 4.0115170Losses:  4.031742095947266 0.07855609804391861
CurrentTrain: epoch  9, batch    55 | loss: 4.1102982Losses:  3.9665451049804688 0.09669408947229385
CurrentTrain: epoch  9, batch    56 | loss: 4.0632391Losses:  3.9410812854766846 0.07191599905490875
CurrentTrain: epoch  9, batch    57 | loss: 4.0129972Losses:  3.989147186279297 0.08993226289749146
CurrentTrain: epoch  9, batch    58 | loss: 4.0790796Losses:  3.9970195293426514 0.08385514467954636
CurrentTrain: epoch  9, batch    59 | loss: 4.0808744Losses:  3.9333548545837402 0.09796705842018127
CurrentTrain: epoch  9, batch    60 | loss: 4.0313220Losses:  3.986990213394165 0.03981635347008705
CurrentTrain: epoch  9, batch    61 | loss: 4.0268064Losses:  3.992102861404419 0.12424404174089432
CurrentTrain: epoch  9, batch    62 | loss: 4.1163468Losses:  3.9739882946014404 0.05286354571580887
CurrentTrain: epoch  9, batch    63 | loss: 4.0268517Losses:  3.9498403072357178 0.06361323595046997
CurrentTrain: epoch  9, batch    64 | loss: 4.0134535Losses:  3.9599251747131348 0.10069459676742554
CurrentTrain: epoch  9, batch    65 | loss: 4.0606198Losses:  3.9999642372131348 0.058093130588531494
CurrentTrain: epoch  9, batch    66 | loss: 4.0580573Losses:  3.9524168968200684 0.06460708379745483
CurrentTrain: epoch  9, batch    67 | loss: 4.0170240Losses:  4.02475643157959 0.04112144559621811
CurrentTrain: epoch  9, batch    68 | loss: 4.0658779Losses:  3.9545836448669434 0.09213992953300476
CurrentTrain: epoch  9, batch    69 | loss: 4.0467234Losses:  4.0064802169799805 0.06556686758995056
CurrentTrain: epoch  9, batch    70 | loss: 4.0720472Losses:  3.9693164825439453 0.06517471373081207
CurrentTrain: epoch  9, batch    71 | loss: 4.0344911Losses:  3.9546332359313965 0.07753382623195648
CurrentTrain: epoch  9, batch    72 | loss: 4.0321670Losses:  3.98748779296875 0.06275512278079987
CurrentTrain: epoch  9, batch    73 | loss: 4.0502429Losses:  4.0321879386901855 0.05386616662144661
CurrentTrain: epoch  9, batch    74 | loss: 4.0860543Losses:  4.022955894470215 0.0815175473690033
CurrentTrain: epoch  9, batch    75 | loss: 4.1044736Losses:  3.9211268424987793 0.09856750071048737
CurrentTrain: epoch  9, batch    76 | loss: 4.0196943Losses:  3.9608311653137207 0.02988426759839058
CurrentTrain: epoch  9, batch    77 | loss: 3.9907155Losses:  3.9670512676239014 0.08455300331115723
CurrentTrain: epoch  9, batch    78 | loss: 4.0516043Losses:  3.9286489486694336 0.08863385021686554
CurrentTrain: epoch  9, batch    79 | loss: 4.0172830Losses:  3.9910659790039062 0.04475894570350647
CurrentTrain: epoch  9, batch    80 | loss: 4.0358248Losses:  3.955425262451172 0.10258099436759949
CurrentTrain: epoch  9, batch    81 | loss: 4.0580063Losses:  3.9567785263061523 0.055615246295928955
CurrentTrain: epoch  9, batch    82 | loss: 4.0123940Losses:  3.904143810272217 0.07030527293682098
CurrentTrain: epoch  9, batch    83 | loss: 3.9744492Losses:  4.0226216316223145 0.08759620785713196
CurrentTrain: epoch  9, batch    84 | loss: 4.1102180Losses:  3.9521782398223877 0.11710464954376221
CurrentTrain: epoch  9, batch    85 | loss: 4.0692830Losses:  3.9723572731018066 0.040666334331035614
CurrentTrain: epoch  9, batch    86 | loss: 4.0130234Losses:  3.9718449115753174 0.07196216285228729
CurrentTrain: epoch  9, batch    87 | loss: 4.0438070Losses:  3.9888739585876465 0.07449060678482056
CurrentTrain: epoch  9, batch    88 | loss: 4.0633645Losses:  3.894808053970337 0.12285119295120239
CurrentTrain: epoch  9, batch    89 | loss: 4.0176592Losses:  3.964726448059082 0.04804602637887001
CurrentTrain: epoch  9, batch    90 | loss: 4.0127726Losses:  3.944754123687744 0.0670003592967987
CurrentTrain: epoch  9, batch    91 | loss: 4.0117545Losses:  3.979780435562134 0.0659525915980339
CurrentTrain: epoch  9, batch    92 | loss: 4.0457330Losses:  3.9789092540740967 0.05330558121204376
CurrentTrain: epoch  9, batch    93 | loss: 4.0322146Losses:  3.978301525115967 0.09789019823074341
CurrentTrain: epoch  9, batch    94 | loss: 4.0761919Losses:  3.901944637298584 0.04846258461475372
CurrentTrain: epoch  9, batch    95 | loss: 3.9504073Losses:  3.9949374198913574 0.11926545202732086
CurrentTrain: epoch  9, batch    96 | loss: 4.1142030Losses:  3.9490737915039062 0.1380588710308075
CurrentTrain: epoch  9, batch    97 | loss: 4.0871325Losses:  3.9820661544799805 0.08873820304870605
CurrentTrain: epoch  9, batch    98 | loss: 4.0708046Losses:  3.969064712524414 0.09085357189178467
CurrentTrain: epoch  9, batch    99 | loss: 4.0599184Losses:  3.991426706314087 0.037094056606292725
CurrentTrain: epoch  9, batch   100 | loss: 4.0285206Losses:  3.9706945419311523 0.07377168536186218
CurrentTrain: epoch  9, batch   101 | loss: 4.0444660Losses:  3.9585559368133545 0.07328928261995316
CurrentTrain: epoch  9, batch   102 | loss: 4.0318451Losses:  3.9960670471191406 0.07476668059825897
CurrentTrain: epoch  9, batch   103 | loss: 4.0708337Losses:  3.940523147583008 0.04868021979928017
CurrentTrain: epoch  9, batch   104 | loss: 3.9892035Losses:  3.964808702468872 0.04197511821985245
CurrentTrain: epoch  9, batch   105 | loss: 4.0067840Losses:  4.021548748016357 0.04814249277114868
CurrentTrain: epoch  9, batch   106 | loss: 4.0696912Losses:  3.9871878623962402 0.059293996542692184
CurrentTrain: epoch  9, batch   107 | loss: 4.0464821Losses:  3.94232177734375 0.08887627720832825
CurrentTrain: epoch  9, batch   108 | loss: 4.0311980Losses:  3.9864745140075684 0.06986173987388611
CurrentTrain: epoch  9, batch   109 | loss: 4.0563364Losses:  3.9927806854248047 0.07959664613008499
CurrentTrain: epoch  9, batch   110 | loss: 4.0723772Losses:  3.9812088012695312 0.06062759459018707
CurrentTrain: epoch  9, batch   111 | loss: 4.0418363Losses:  3.986706495285034 0.058997735381126404
CurrentTrain: epoch  9, batch   112 | loss: 4.0457044Losses:  4.069440841674805 0.07261084765195847
CurrentTrain: epoch  9, batch   113 | loss: 4.1420517Losses:  3.983595132827759 0.07412627339363098
CurrentTrain: epoch  9, batch   114 | loss: 4.0577216Losses:  3.988670825958252 0.05703040212392807
CurrentTrain: epoch  9, batch   115 | loss: 4.0457010Losses:  3.99289608001709 0.047371722757816315
CurrentTrain: epoch  9, batch   116 | loss: 4.0402679Losses:  3.9856748580932617 0.05997970700263977
CurrentTrain: epoch  9, batch   117 | loss: 4.0456548Losses:  3.928525447845459 0.08837870508432388
CurrentTrain: epoch  9, batch   118 | loss: 4.0169044Losses:  3.909268617630005 0.0411517396569252
CurrentTrain: epoch  9, batch   119 | loss: 3.9504204Losses:  3.9307305812835693 0.10963361710309982
CurrentTrain: epoch  9, batch   120 | loss: 4.0403643Losses:  3.95603609085083 0.0905904620885849
CurrentTrain: epoch  9, batch   121 | loss: 4.0466266Losses:  3.9382925033569336 0.08062857389450073
CurrentTrain: epoch  9, batch   122 | loss: 4.0189209Losses:  3.9634921550750732 0.04841071739792824
CurrentTrain: epoch  9, batch   123 | loss: 4.0119028Losses:  3.9250121116638184 0.06144427880644798
CurrentTrain: epoch  9, batch   124 | loss: 3.9864564
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [2 7 8 1 2 2 2 2 0 5 1 6 2 2 2 0 4 2 2 3]
Losses:  6.386231422424316 1.3600913286209106
CurrentTrain: epoch  0, batch     0 | loss: 7.7463226Losses:  6.307920932769775 1.0975794792175293
CurrentTrain: epoch  0, batch     1 | loss: 7.4055004Losses:  7.1123881340026855 1.2445521354675293
CurrentTrain: epoch  0, batch     2 | loss: 8.3569403Losses:  6.118361473083496 1.2221314907073975
CurrentTrain: epoch  0, batch     3 | loss: 7.3404932Losses:  6.821659088134766 1.5767995119094849
CurrentTrain: epoch  0, batch     4 | loss: 8.3984585Losses:  8.452815055847168 1.5262773036956787
CurrentTrain: epoch  0, batch     5 | loss: 9.9790926Losses:  3.490180015563965 0.4469934105873108
CurrentTrain: epoch  0, batch     6 | loss: 3.9371734Losses:  5.8131937980651855 1.3381056785583496
CurrentTrain: epoch  1, batch     0 | loss: 7.1512995Losses:  5.825926780700684 1.4136899709701538
CurrentTrain: epoch  1, batch     1 | loss: 7.2396169Losses:  4.979579925537109 1.2085357904434204
CurrentTrain: epoch  1, batch     2 | loss: 6.1881156Losses:  5.395808219909668 1.3518595695495605
CurrentTrain: epoch  1, batch     3 | loss: 6.7476678Losses:  4.690889358520508 1.0474212169647217
CurrentTrain: epoch  1, batch     4 | loss: 5.7383108Losses:  5.132101535797119 1.0728938579559326
CurrentTrain: epoch  1, batch     5 | loss: 6.2049952Losses:  3.6247007846832275 0.2588185667991638
CurrentTrain: epoch  1, batch     6 | loss: 3.8835194Losses:  4.882179260253906 1.2316449880599976
CurrentTrain: epoch  2, batch     0 | loss: 6.1138244Losses:  3.534269332885742 0.40749499201774597
CurrentTrain: epoch  2, batch     1 | loss: 3.9417644Losses:  3.798224449157715 1.4913047552108765
CurrentTrain: epoch  2, batch     2 | loss: 5.2895293Losses:  3.879329204559326 0.878466010093689
CurrentTrain: epoch  2, batch     3 | loss: 4.7577953Losses:  2.3586177825927734 0.9565526247024536
CurrentTrain: epoch  2, batch     4 | loss: 3.3151703Losses:  3.258183002471924 1.182002067565918
CurrentTrain: epoch  2, batch     5 | loss: 4.4401851Losses:  8.27479362487793 0.33231669664382935
CurrentTrain: epoch  2, batch     6 | loss: 8.6071100Losses:  3.388068914413452 0.6448702216148376
CurrentTrain: epoch  3, batch     0 | loss: 4.0329390Losses:  2.992217540740967 0.8816007375717163
CurrentTrain: epoch  3, batch     1 | loss: 3.8738184Losses:  5.071621894836426 1.4350181818008423
CurrentTrain: epoch  3, batch     2 | loss: 6.5066400Losses:  2.1686203479766846 0.8802332878112793
CurrentTrain: epoch  3, batch     3 | loss: 3.0488536Losses:  2.7564568519592285 0.9707251787185669
CurrentTrain: epoch  3, batch     4 | loss: 3.7271819Losses:  2.285127639770508 0.802692174911499
CurrentTrain: epoch  3, batch     5 | loss: 3.0878198Losses:  3.5294456481933594 0.2082463800907135
CurrentTrain: epoch  3, batch     6 | loss: 3.7376921Losses:  3.302999973297119 1.2599952220916748
CurrentTrain: epoch  4, batch     0 | loss: 4.5629950Losses:  2.962008237838745 1.2967225313186646
CurrentTrain: epoch  4, batch     1 | loss: 4.2587309Losses:  1.8050475120544434 0.5691393613815308
CurrentTrain: epoch  4, batch     2 | loss: 2.3741870Losses:  3.495737075805664 1.3751516342163086
CurrentTrain: epoch  4, batch     3 | loss: 4.8708887Losses:  2.593870162963867 0.6459296345710754
CurrentTrain: epoch  4, batch     4 | loss: 3.2397997Losses:  3.223180055618286 0.7856192588806152
CurrentTrain: epoch  4, batch     5 | loss: 4.0087996Losses:  3.204665184020996 0.4435655474662781
CurrentTrain: epoch  4, batch     6 | loss: 3.6482308Losses:  2.8453235626220703 0.8308660984039307
CurrentTrain: epoch  5, batch     0 | loss: 3.6761897Losses:  2.778723955154419 0.8725647926330566
CurrentTrain: epoch  5, batch     1 | loss: 3.6512887Losses:  2.205289602279663 0.8378286361694336
CurrentTrain: epoch  5, batch     2 | loss: 3.0431182Losses:  2.6679182052612305 0.9113880395889282
CurrentTrain: epoch  5, batch     3 | loss: 3.5793061Losses:  2.371539831161499 0.8709735870361328
CurrentTrain: epoch  5, batch     4 | loss: 3.2425134Losses:  2.723623275756836 0.7752104997634888
CurrentTrain: epoch  5, batch     5 | loss: 3.4988337Losses:  3.1512675285339355 0.27734375
CurrentTrain: epoch  5, batch     6 | loss: 3.4286113Losses:  2.4712486267089844 0.7503325343132019
CurrentTrain: epoch  6, batch     0 | loss: 3.2215812Losses:  2.7987589836120605 1.0599932670593262
CurrentTrain: epoch  6, batch     1 | loss: 3.8587523Losses:  1.883248209953308 0.6439915299415588
CurrentTrain: epoch  6, batch     2 | loss: 2.5272398Losses:  2.413987159729004 0.5175230503082275
CurrentTrain: epoch  6, batch     3 | loss: 2.9315102Losses:  2.1725287437438965 0.6526787281036377
CurrentTrain: epoch  6, batch     4 | loss: 2.8252075Losses:  2.734544277191162 0.7472373247146606
CurrentTrain: epoch  6, batch     5 | loss: 3.4817815Losses:  3.6325647830963135 0.27880772948265076
CurrentTrain: epoch  6, batch     6 | loss: 3.9113724Losses:  2.471734046936035 0.817165732383728
CurrentTrain: epoch  7, batch     0 | loss: 3.2888999Losses:  2.3359055519104004 1.013627529144287
CurrentTrain: epoch  7, batch     1 | loss: 3.3495331Losses:  2.1550745964050293 0.6614370942115784
CurrentTrain: epoch  7, batch     2 | loss: 2.8165116Losses:  2.111438512802124 0.595496654510498
CurrentTrain: epoch  7, batch     3 | loss: 2.7069352Losses:  2.8230390548706055 0.8120365142822266
CurrentTrain: epoch  7, batch     4 | loss: 3.6350756Losses:  1.9438951015472412 0.7435901165008545
CurrentTrain: epoch  7, batch     5 | loss: 2.6874852Losses:  1.79592764377594 0.08115885406732559
CurrentTrain: epoch  7, batch     6 | loss: 1.8770865Losses:  2.335317611694336 0.7645170092582703
CurrentTrain: epoch  8, batch     0 | loss: 3.0998347Losses:  2.232041835784912 0.6341657042503357
CurrentTrain: epoch  8, batch     1 | loss: 2.8662076Losses:  1.8923230171203613 0.5563019514083862
CurrentTrain: epoch  8, batch     2 | loss: 2.4486251Losses:  1.8929657936096191 0.6444091796875
CurrentTrain: epoch  8, batch     3 | loss: 2.5373750Losses:  1.9393011331558228 0.6115660667419434
CurrentTrain: epoch  8, batch     4 | loss: 2.5508671Losses:  2.0756537914276123 0.49945199489593506
CurrentTrain: epoch  8, batch     5 | loss: 2.5751057Losses:  4.002227783203125 0.2742757499217987
CurrentTrain: epoch  8, batch     6 | loss: 4.2765036Losses:  2.3423426151275635 0.7294708490371704
CurrentTrain: epoch  9, batch     0 | loss: 3.0718136Losses:  2.132052421569824 0.41991159319877625
CurrentTrain: epoch  9, batch     1 | loss: 2.5519640Losses:  1.8212822675704956 0.626119077205658
CurrentTrain: epoch  9, batch     2 | loss: 2.4474013Losses:  1.8854938745498657 0.6470726728439331
CurrentTrain: epoch  9, batch     3 | loss: 2.5325665Losses:  2.0506057739257812 0.4255174994468689
CurrentTrain: epoch  9, batch     4 | loss: 2.4761233Losses:  2.0893187522888184 0.5096854567527771
CurrentTrain: epoch  9, batch     5 | loss: 2.5990043Losses:  1.7338100671768188 0.11826954036951065
CurrentTrain: epoch  9, batch     6 | loss: 1.8520796
Losses:  1.2636739015579224 0.3640345633029938
MemoryTrain:  epoch  0, batch     0 | loss: 1.6277084Losses:  1.7697709798812866 0.6443634033203125
MemoryTrain:  epoch  0, batch     1 | loss: 2.4141345Losses:  1.7260334491729736 0.10164538025856018
MemoryTrain:  epoch  0, batch     2 | loss: 1.8276788Losses:  2.701266288757324 0.7391120195388794
MemoryTrain:  epoch  1, batch     0 | loss: 3.4403782Losses:  0.43910250067710876 0.35158276557922363
MemoryTrain:  epoch  1, batch     1 | loss: 0.7906853Losses:  0.5962271690368652 0.05654739588499069
MemoryTrain:  epoch  1, batch     2 | loss: 0.6527746Losses:  0.07542581111192703 0.5391208529472351
MemoryTrain:  epoch  2, batch     0 | loss: 0.6145467Losses:  1.6134214401245117 0.5146037340164185
MemoryTrain:  epoch  2, batch     1 | loss: 2.1280251Losses:  0.3790338933467865 0.22585757076740265
MemoryTrain:  epoch  2, batch     2 | loss: 0.6048915Losses:  0.14331063628196716 0.5606268048286438
MemoryTrain:  epoch  3, batch     0 | loss: 0.7039374Losses:  0.5332181453704834 0.35697293281555176
MemoryTrain:  epoch  3, batch     1 | loss: 0.8901911Losses:  1.5067516565322876 0.28109627962112427
MemoryTrain:  epoch  3, batch     2 | loss: 1.7878480Losses:  0.08883205056190491 0.5226834416389465
MemoryTrain:  epoch  4, batch     0 | loss: 0.6115155Losses:  0.32562190294265747 0.6204650402069092
MemoryTrain:  epoch  4, batch     1 | loss: 0.9460869Losses:  0.055951591581106186 0.03718338906764984
MemoryTrain:  epoch  4, batch     2 | loss: 0.0931350Losses:  0.052309781312942505 0.6026601791381836
MemoryTrain:  epoch  5, batch     0 | loss: 0.6549699Losses:  0.05638372525572777 0.2601540982723236
MemoryTrain:  epoch  5, batch     1 | loss: 0.3165378Losses:  0.2999846935272217 0.5394856929779053
MemoryTrain:  epoch  5, batch     2 | loss: 0.8394704Losses:  0.08504297584295273 0.3962293863296509
MemoryTrain:  epoch  6, batch     0 | loss: 0.4812724Losses:  0.03924785926938057 0.3063966631889343
MemoryTrain:  epoch  6, batch     1 | loss: 0.3456445Losses:  0.026371855288743973 0.36830446124076843
MemoryTrain:  epoch  6, batch     2 | loss: 0.3946763Losses:  0.015074649825692177 0.19095098972320557
MemoryTrain:  epoch  7, batch     0 | loss: 0.2060256Losses:  0.0503152571618557 0.586891770362854
MemoryTrain:  epoch  7, batch     1 | loss: 0.6372070Losses:  0.02384861558675766 0.16950875520706177
MemoryTrain:  epoch  7, batch     2 | loss: 0.1933574Losses:  0.028372425585985184 0.32816627621650696
MemoryTrain:  epoch  8, batch     0 | loss: 0.3565387Losses:  0.015949171036481857 0.3979136347770691
MemoryTrain:  epoch  8, batch     1 | loss: 0.4138628Losses:  0.04668097943067551 0.2886064350605011
MemoryTrain:  epoch  8, batch     2 | loss: 0.3352874Losses:  0.037606846541166306 0.47851815819740295
MemoryTrain:  epoch  9, batch     0 | loss: 0.5161250Losses:  0.01167226955294609 0.3373502492904663
MemoryTrain:  epoch  9, batch     1 | loss: 0.3490225Losses:  0.0238991416990757 0.16835027933120728
MemoryTrain:  epoch  9, batch     2 | loss: 0.1922494
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.23%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 83.40%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.47%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 82.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 92.12%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.89%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.09%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 92.35%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 92.40%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 92.26%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 92.37%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 92.52%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 92.23%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 91.69%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.25%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 90.97%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 90.17%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 89.23%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 88.32%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 87.43%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 86.63%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 85.92%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 85.65%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 86.45%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 86.34%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 86.24%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 86.12%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 86.27%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.73%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.44%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 87.56%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 88.03%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 88.07%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 88.06%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 88.11%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 88.05%   
cur_acc:  ['0.9484', '0.8264']
his_acc:  ['0.9484', '0.8805']
Clustering into  14  clusters
Clusters:  [ 0  7  9  0  0  0 11  0 10 12  8  3  0  0  5 13  4  0  0  6  2  0  0  0
  0  1  0  0  0  0]
Losses:  5.969059467315674 0.9540377855300903
CurrentTrain: epoch  0, batch     0 | loss: 6.9230971Losses:  6.75308895111084 1.7465670108795166
CurrentTrain: epoch  0, batch     1 | loss: 8.4996557Losses:  7.018899917602539 1.4892685413360596
CurrentTrain: epoch  0, batch     2 | loss: 8.5081682Losses:  6.320188045501709 1.2680244445800781
CurrentTrain: epoch  0, batch     3 | loss: 7.5882125Losses:  5.7440032958984375 1.5483191013336182
CurrentTrain: epoch  0, batch     4 | loss: 7.2923222Losses:  6.576272010803223 1.9408483505249023
CurrentTrain: epoch  0, batch     5 | loss: 8.5171204Losses:  5.520213603973389 0.6092718839645386
CurrentTrain: epoch  0, batch     6 | loss: 6.1294856Losses:  5.669374465942383 1.3338885307312012
CurrentTrain: epoch  1, batch     0 | loss: 7.0032630Losses:  5.946924209594727 1.7625948190689087
CurrentTrain: epoch  1, batch     1 | loss: 7.7095189Losses:  4.7209320068359375 1.6195284128189087
CurrentTrain: epoch  1, batch     2 | loss: 6.3404603Losses:  5.356024742126465 1.5035673379898071
CurrentTrain: epoch  1, batch     3 | loss: 6.8595920Losses:  4.9365034103393555 1.6355737447738647
CurrentTrain: epoch  1, batch     4 | loss: 6.5720773Losses:  4.376357078552246 1.4841582775115967
CurrentTrain: epoch  1, batch     5 | loss: 5.8605156Losses:  4.55048942565918 0.17818182706832886
CurrentTrain: epoch  1, batch     6 | loss: 4.7286711Losses:  5.181880474090576 1.0122504234313965
CurrentTrain: epoch  2, batch     0 | loss: 6.1941309Losses:  4.394096374511719 1.517956256866455
CurrentTrain: epoch  2, batch     1 | loss: 5.9120526Losses:  5.618294715881348 1.5879840850830078
CurrentTrain: epoch  2, batch     2 | loss: 7.2062788Losses:  3.492722511291504 1.50038743019104
CurrentTrain: epoch  2, batch     3 | loss: 4.9931097Losses:  3.851945638656616 1.3924585580825806
CurrentTrain: epoch  2, batch     4 | loss: 5.2444043Losses:  3.447157382965088 1.4038426876068115
CurrentTrain: epoch  2, batch     5 | loss: 4.8509998Losses:  3.443211555480957 0.651908278465271
CurrentTrain: epoch  2, batch     6 | loss: 4.0951200Losses:  4.99949312210083 1.2435373067855835
CurrentTrain: epoch  3, batch     0 | loss: 6.2430305Losses:  2.9319276809692383 1.1422579288482666
CurrentTrain: epoch  3, batch     1 | loss: 4.0741854Losses:  4.677981376647949 1.303777813911438
CurrentTrain: epoch  3, batch     2 | loss: 5.9817591Losses:  4.106755256652832 1.6199204921722412
CurrentTrain: epoch  3, batch     3 | loss: 5.7266760Losses:  2.998424530029297 1.3938348293304443
CurrentTrain: epoch  3, batch     4 | loss: 4.3922596Losses:  4.1589741706848145 1.6668968200683594
CurrentTrain: epoch  3, batch     5 | loss: 5.8258710Losses:  3.277705669403076 0.5657640695571899
CurrentTrain: epoch  3, batch     6 | loss: 3.8434696Losses:  4.744824409484863 1.1341748237609863
CurrentTrain: epoch  4, batch     0 | loss: 5.8789992Losses:  3.7841737270355225 1.1987831592559814
CurrentTrain: epoch  4, batch     1 | loss: 4.9829569Losses:  2.6747961044311523 0.739457905292511
CurrentTrain: epoch  4, batch     2 | loss: 3.4142540Losses:  4.33656644821167 1.423107385635376
CurrentTrain: epoch  4, batch     3 | loss: 5.7596741Losses:  3.3768420219421387 1.4090248346328735
CurrentTrain: epoch  4, batch     4 | loss: 4.7858667Losses:  2.968646764755249 1.2963922023773193
CurrentTrain: epoch  4, batch     5 | loss: 4.2650390Losses:  2.182460308074951 0.5339220762252808
CurrentTrain: epoch  4, batch     6 | loss: 2.7163825Losses:  2.950789451599121 1.3088756799697876
CurrentTrain: epoch  5, batch     0 | loss: 4.2596650Losses:  3.677823781967163 1.1245180368423462
CurrentTrain: epoch  5, batch     1 | loss: 4.8023419Losses:  2.3017754554748535 1.2431714534759521
CurrentTrain: epoch  5, batch     2 | loss: 3.5449469Losses:  3.483607769012451 1.1145939826965332
CurrentTrain: epoch  5, batch     3 | loss: 4.5982018Losses:  3.21224308013916 1.1285268068313599
CurrentTrain: epoch  5, batch     4 | loss: 4.3407698Losses:  4.179555892944336 1.3363621234893799
CurrentTrain: epoch  5, batch     5 | loss: 5.5159178Losses:  1.9645999670028687 0.20723341405391693
CurrentTrain: epoch  5, batch     6 | loss: 2.1718333Losses:  3.6724143028259277 1.1730536222457886
CurrentTrain: epoch  6, batch     0 | loss: 4.8454680Losses:  2.4688057899475098 1.0229496955871582
CurrentTrain: epoch  6, batch     1 | loss: 3.4917555Losses:  3.1477463245391846 0.991365909576416
CurrentTrain: epoch  6, batch     2 | loss: 4.1391125Losses:  2.474729537963867 1.1706314086914062
CurrentTrain: epoch  6, batch     3 | loss: 3.6453609Losses:  4.535378456115723 0.9697233438491821
CurrentTrain: epoch  6, batch     4 | loss: 5.5051017Losses:  2.1094186305999756 0.7189478278160095
CurrentTrain: epoch  6, batch     5 | loss: 2.8283665Losses:  2.1458733081817627 0.4661175012588501
CurrentTrain: epoch  6, batch     6 | loss: 2.6119909Losses:  2.9429008960723877 1.2765084505081177
CurrentTrain: epoch  7, batch     0 | loss: 4.2194095Losses:  2.7461793422698975 1.0449347496032715
CurrentTrain: epoch  7, batch     1 | loss: 3.7911141Losses:  2.1749091148376465 1.1287693977355957
CurrentTrain: epoch  7, batch     2 | loss: 3.3036785Losses:  3.8018507957458496 0.934717059135437
CurrentTrain: epoch  7, batch     3 | loss: 4.7365680Losses:  2.3058021068573 1.0025286674499512
CurrentTrain: epoch  7, batch     4 | loss: 3.3083308Losses:  2.549030065536499 1.148384690284729
CurrentTrain: epoch  7, batch     5 | loss: 3.6974149Losses:  5.024436950683594 0.3612389862537384
CurrentTrain: epoch  7, batch     6 | loss: 5.3856759Losses:  2.785221576690674 0.9368786811828613
CurrentTrain: epoch  8, batch     0 | loss: 3.7221003Losses:  2.7841134071350098 0.8616732358932495
CurrentTrain: epoch  8, batch     1 | loss: 3.6457868Losses:  3.2156877517700195 0.7442907094955444
CurrentTrain: epoch  8, batch     2 | loss: 3.9599786Losses:  3.032524585723877 1.2614052295684814
CurrentTrain: epoch  8, batch     3 | loss: 4.2939301Losses:  2.2698276042938232 0.8408257961273193
CurrentTrain: epoch  8, batch     4 | loss: 3.1106534Losses:  2.0687761306762695 1.0523449182510376
CurrentTrain: epoch  8, batch     5 | loss: 3.1211209Losses:  1.8295772075653076 0.3937583565711975
CurrentTrain: epoch  8, batch     6 | loss: 2.2233355Losses:  1.9087183475494385 0.6465023756027222
CurrentTrain: epoch  9, batch     0 | loss: 2.5552206Losses:  2.797673225402832 1.02974271774292
CurrentTrain: epoch  9, batch     1 | loss: 3.8274159Losses:  2.1319308280944824 1.0077846050262451
CurrentTrain: epoch  9, batch     2 | loss: 3.1397154Losses:  3.3232038021087646 0.8453531861305237
CurrentTrain: epoch  9, batch     3 | loss: 4.1685572Losses:  2.972029685974121 0.8218839168548584
CurrentTrain: epoch  9, batch     4 | loss: 3.7939136Losses:  2.528562545776367 0.8469088077545166
CurrentTrain: epoch  9, batch     5 | loss: 3.3754714Losses:  1.9354219436645508 0.1212993860244751
CurrentTrain: epoch  9, batch     6 | loss: 2.0567212
Losses:  1.5420265197753906 0.6497820615768433
MemoryTrain:  epoch  0, batch     0 | loss: 2.1918087Losses:  1.3581658601760864 0.9997602701187134
MemoryTrain:  epoch  0, batch     1 | loss: 2.3579261Losses:  0.344112366437912 0.439839631319046
MemoryTrain:  epoch  0, batch     2 | loss: 0.7839520Losses:  0.8158687949180603 0.42657679319381714
MemoryTrain:  epoch  0, batch     3 | loss: 1.2424456Losses:  1.228141188621521 0.6287677884101868
MemoryTrain:  epoch  1, batch     0 | loss: 1.8569090Losses:  1.247593879699707 0.8066861033439636
MemoryTrain:  epoch  1, batch     1 | loss: 2.0542800Losses:  0.5816132426261902 0.5787895321846008
MemoryTrain:  epoch  1, batch     2 | loss: 1.1604028Losses:  0.8146865367889404 0.5946142673492432
MemoryTrain:  epoch  1, batch     3 | loss: 1.4093008Losses:  0.36728373169898987 0.5270896553993225
MemoryTrain:  epoch  2, batch     0 | loss: 0.8943734Losses:  0.419979453086853 0.5156533718109131
MemoryTrain:  epoch  2, batch     1 | loss: 0.9356328Losses:  0.12616294622421265 0.5531319379806519
MemoryTrain:  epoch  2, batch     2 | loss: 0.6792949Losses:  0.7398101687431335 0.6015993356704712
MemoryTrain:  epoch  2, batch     3 | loss: 1.3414094Losses:  0.140016108751297 0.6073701977729797
MemoryTrain:  epoch  3, batch     0 | loss: 0.7473863Losses:  0.1365293264389038 0.6718701124191284
MemoryTrain:  epoch  3, batch     1 | loss: 0.8083994Losses:  0.1754208505153656 0.4562187194824219
MemoryTrain:  epoch  3, batch     2 | loss: 0.6316396Losses:  0.15474030375480652 0.4315263032913208
MemoryTrain:  epoch  3, batch     3 | loss: 0.5862666Losses:  0.0910988450050354 0.5573379993438721
MemoryTrain:  epoch  4, batch     0 | loss: 0.6484368Losses:  0.09764041006565094 0.5137529373168945
MemoryTrain:  epoch  4, batch     1 | loss: 0.6113933Losses:  0.05497891828417778 0.6992408037185669
MemoryTrain:  epoch  4, batch     2 | loss: 0.7542197Losses:  0.04687124490737915 0.4161252975463867
MemoryTrain:  epoch  4, batch     3 | loss: 0.4629965Losses:  0.03715655207633972 0.7084291577339172
MemoryTrain:  epoch  5, batch     0 | loss: 0.7455857Losses:  0.05998092517256737 0.5724899768829346
MemoryTrain:  epoch  5, batch     1 | loss: 0.6324709Losses:  0.02913525328040123 0.6631951928138733
MemoryTrain:  epoch  5, batch     2 | loss: 0.6923304Losses:  0.018190620467066765 0.21863417327404022
MemoryTrain:  epoch  5, batch     3 | loss: 0.2368248Losses:  0.027098100632429123 0.5151092410087585
MemoryTrain:  epoch  6, batch     0 | loss: 0.5422074Losses:  0.08085396140813828 0.5651907920837402
MemoryTrain:  epoch  6, batch     1 | loss: 0.6460447Losses:  0.0166971143335104 0.4250955581665039
MemoryTrain:  epoch  6, batch     2 | loss: 0.4417927Losses:  0.03263735771179199 0.5617637634277344
MemoryTrain:  epoch  6, batch     3 | loss: 0.5944011Losses:  0.016903052106499672 0.36277127265930176
MemoryTrain:  epoch  7, batch     0 | loss: 0.3796743Losses:  0.029439929872751236 0.5016642808914185
MemoryTrain:  epoch  7, batch     1 | loss: 0.5311042Losses:  0.04555581510066986 0.6367521286010742
MemoryTrain:  epoch  7, batch     2 | loss: 0.6823080Losses:  0.04256279766559601 0.6418904662132263
MemoryTrain:  epoch  7, batch     3 | loss: 0.6844532Losses:  0.034586258232593536 0.5503676533699036
MemoryTrain:  epoch  8, batch     0 | loss: 0.5849539Losses:  0.02858319692313671 0.4646557569503784
MemoryTrain:  epoch  8, batch     1 | loss: 0.4932390Losses:  0.02076491340994835 0.6326229572296143
MemoryTrain:  epoch  8, batch     2 | loss: 0.6533878Losses:  0.02215339057147503 0.4260047376155853
MemoryTrain:  epoch  8, batch     3 | loss: 0.4481581Losses:  0.0251917727291584 0.5280776023864746
MemoryTrain:  epoch  9, batch     0 | loss: 0.5532694Losses:  0.06766152381896973 0.6977805495262146
MemoryTrain:  epoch  9, batch     1 | loss: 0.7654421Losses:  0.019765205681324005 0.40839359164237976
MemoryTrain:  epoch  9, batch     2 | loss: 0.4281588Losses:  0.01729028858244419 0.24999278783798218
MemoryTrain:  epoch  9, batch     3 | loss: 0.2672831
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 57.11%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 55.42%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.63%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 53.91%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.92%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 58.28%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.05%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 59.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 61.43%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.08%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 90.85%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.90%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.84%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.27%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.11%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 90.86%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 90.54%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 90.26%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 89.90%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 89.00%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 88.67%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 88.27%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 87.50%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 86.67%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 85.64%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 84.78%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 83.94%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 82.97%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.31%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 83.79%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 83.83%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 83.78%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.40%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 85.33%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 85.40%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 85.31%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 85.28%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 85.43%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 85.69%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 85.22%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 84.74%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 83.91%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 83.61%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 83.16%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 83.14%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 83.18%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 83.50%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 83.49%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 83.51%   [EVAL] batch:  138 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 83.87%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 83.58%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 83.26%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 83.02%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 82.76%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 82.46%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.95%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 81.41%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 80.88%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 80.36%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 79.88%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 79.37%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 79.32%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.45%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 79.52%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 79.57%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 79.74%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.92%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 80.01%   [EVAL] batch:  172 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 80.10%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 79.97%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 79.94%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 79.92%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.83%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 79.70%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 79.58%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 79.42%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 79.32%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 78.82%   
cur_acc:  ['0.9484', '0.8264', '0.6508']
his_acc:  ['0.9484', '0.8805', '0.7882']
Clustering into  19  clusters
Clusters:  [ 0  3 12  1  0  0 16  0 13 11 10 17  1  0 14  6 15  0  0 18  7  1  0  0
  0  9  0  0  0  0  3  8  1  1  0  5  0  0  4  2]
Losses:  5.581223964691162 0.7993124723434448
CurrentTrain: epoch  0, batch     0 | loss: 6.3805366Losses:  5.303782939910889 1.060506820678711
CurrentTrain: epoch  0, batch     1 | loss: 6.3642898Losses:  6.695477485656738 1.5553665161132812
CurrentTrain: epoch  0, batch     2 | loss: 8.2508440Losses:  6.398372650146484 1.2393062114715576
CurrentTrain: epoch  0, batch     3 | loss: 7.6376791Losses:  4.890886306762695 0.9838303327560425
CurrentTrain: epoch  0, batch     4 | loss: 5.8747168Losses:  7.077773094177246 1.4575519561767578
CurrentTrain: epoch  0, batch     5 | loss: 8.5353251Losses:  6.569573402404785 0.5164667367935181
CurrentTrain: epoch  0, batch     6 | loss: 7.0860400Losses:  4.462072372436523 1.538266658782959
CurrentTrain: epoch  1, batch     0 | loss: 6.0003390Losses:  5.800003528594971 0.9564532041549683
CurrentTrain: epoch  1, batch     1 | loss: 6.7564569Losses:  5.854503631591797 0.6901313066482544
CurrentTrain: epoch  1, batch     2 | loss: 6.5446348Losses:  6.061845302581787 1.282022476196289
CurrentTrain: epoch  1, batch     3 | loss: 7.3438678Losses:  4.510283470153809 0.9834111332893372
CurrentTrain: epoch  1, batch     4 | loss: 5.4936948Losses:  3.990870952606201 0.9548221826553345
CurrentTrain: epoch  1, batch     5 | loss: 4.9456930Losses:  2.006852388381958 0.14154121279716492
CurrentTrain: epoch  1, batch     6 | loss: 2.1483936Losses:  4.144578456878662 1.1239092350006104
CurrentTrain: epoch  2, batch     0 | loss: 5.2684879Losses:  3.0481791496276855 0.7521065473556519
CurrentTrain: epoch  2, batch     1 | loss: 3.8002858Losses:  3.9825632572174072 0.9721463322639465
CurrentTrain: epoch  2, batch     2 | loss: 4.9547095Losses:  5.719121932983398 0.7339297533035278
CurrentTrain: epoch  2, batch     3 | loss: 6.4530516Losses:  4.691402435302734 1.145483374595642
CurrentTrain: epoch  2, batch     4 | loss: 5.8368859Losses:  4.260071754455566 0.9912245273590088
CurrentTrain: epoch  2, batch     5 | loss: 5.2512960Losses:  2.7017829418182373 0.22916853427886963
CurrentTrain: epoch  2, batch     6 | loss: 2.9309516Losses:  5.155351161956787 0.763688325881958
CurrentTrain: epoch  3, batch     0 | loss: 5.9190397Losses:  2.703366756439209 0.7386143207550049
CurrentTrain: epoch  3, batch     1 | loss: 3.4419811Losses:  3.0586395263671875 1.0423729419708252
CurrentTrain: epoch  3, batch     2 | loss: 4.1010122Losses:  3.669074535369873 0.7977406978607178
CurrentTrain: epoch  3, batch     3 | loss: 4.4668150Losses:  4.903240203857422 0.7992444634437561
CurrentTrain: epoch  3, batch     4 | loss: 5.7024846Losses:  3.2805392742156982 0.8134526014328003
CurrentTrain: epoch  3, batch     5 | loss: 4.0939918Losses:  5.264299392700195 0.325711727142334
CurrentTrain: epoch  3, batch     6 | loss: 5.5900111Losses:  2.632248640060425 0.6396670341491699
CurrentTrain: epoch  4, batch     0 | loss: 3.2719157Losses:  2.9892380237579346 0.8281533718109131
CurrentTrain: epoch  4, batch     1 | loss: 3.8173914Losses:  4.773524761199951 0.8156545758247375
CurrentTrain: epoch  4, batch     2 | loss: 5.5891795Losses:  2.6138226985931396 0.9432637691497803
CurrentTrain: epoch  4, batch     3 | loss: 3.5570865Losses:  2.815061092376709 0.8640614151954651
CurrentTrain: epoch  4, batch     4 | loss: 3.6791224Losses:  3.8024187088012695 1.1227515935897827
CurrentTrain: epoch  4, batch     5 | loss: 4.9251704Losses:  3.7003209590911865 0.13574765622615814
CurrentTrain: epoch  4, batch     6 | loss: 3.8360686Losses:  2.4030566215515137 0.8839129209518433
CurrentTrain: epoch  5, batch     0 | loss: 3.2869697Losses:  2.6335930824279785 0.7445589303970337
CurrentTrain: epoch  5, batch     1 | loss: 3.3781519Losses:  2.8889236450195312 0.6189461946487427
CurrentTrain: epoch  5, batch     2 | loss: 3.5078697Losses:  2.8310983180999756 0.887802004814148
CurrentTrain: epoch  5, batch     3 | loss: 3.7189002Losses:  3.6066341400146484 0.6885247826576233
CurrentTrain: epoch  5, batch     4 | loss: 4.2951589Losses:  3.158283233642578 0.737726092338562
CurrentTrain: epoch  5, batch     5 | loss: 3.8960094Losses:  2.537661552429199 0.40135735273361206
CurrentTrain: epoch  5, batch     6 | loss: 2.9390190Losses:  2.934846878051758 0.9155315160751343
CurrentTrain: epoch  6, batch     0 | loss: 3.8503785Losses:  2.158724308013916 0.6174684762954712
CurrentTrain: epoch  6, batch     1 | loss: 2.7761927Losses:  2.714850664138794 0.8813810348510742
CurrentTrain: epoch  6, batch     2 | loss: 3.5962317Losses:  2.6319589614868164 0.9255395531654358
CurrentTrain: epoch  6, batch     3 | loss: 3.5574985Losses:  3.092557430267334 0.8743848204612732
CurrentTrain: epoch  6, batch     4 | loss: 3.9669423Losses:  2.1539113521575928 0.5361254215240479
CurrentTrain: epoch  6, batch     5 | loss: 2.6900368Losses:  3.492588996887207 0.2545132637023926
CurrentTrain: epoch  6, batch     6 | loss: 3.7471023Losses:  2.9031262397766113 0.8644602298736572
CurrentTrain: epoch  7, batch     0 | loss: 3.7675865Losses:  2.425912857055664 0.8553777933120728
CurrentTrain: epoch  7, batch     1 | loss: 3.2812905Losses:  2.210939884185791 0.5341984629631042
CurrentTrain: epoch  7, batch     2 | loss: 2.7451384Losses:  2.8189618587493896 0.49962860345840454
CurrentTrain: epoch  7, batch     3 | loss: 3.3185904Losses:  2.0612854957580566 0.8474739193916321
CurrentTrain: epoch  7, batch     4 | loss: 2.9087594Losses:  1.861332654953003 0.6017495393753052
CurrentTrain: epoch  7, batch     5 | loss: 2.4630823Losses:  2.118943214416504 0.2701259255409241
CurrentTrain: epoch  7, batch     6 | loss: 2.3890691Losses:  2.378701686859131 0.528493344783783
CurrentTrain: epoch  8, batch     0 | loss: 2.9071951Losses:  2.1304233074188232 0.6194422245025635
CurrentTrain: epoch  8, batch     1 | loss: 2.7498655Losses:  2.919569969177246 0.5928958058357239
CurrentTrain: epoch  8, batch     2 | loss: 3.5124657Losses:  1.8340795040130615 0.28243255615234375
CurrentTrain: epoch  8, batch     3 | loss: 2.1165121Losses:  1.8821017742156982 0.556914210319519
CurrentTrain: epoch  8, batch     4 | loss: 2.4390159Losses:  1.932996153831482 0.5575706958770752
CurrentTrain: epoch  8, batch     5 | loss: 2.4905667Losses:  4.228198051452637 1.1920930376163597e-07
CurrentTrain: epoch  8, batch     6 | loss: 4.2281981Losses:  2.4122610092163086 0.6081589460372925
CurrentTrain: epoch  9, batch     0 | loss: 3.0204201Losses:  2.005251884460449 0.6277486085891724
CurrentTrain: epoch  9, batch     1 | loss: 2.6330004Losses:  1.988252878189087 0.7131690979003906
CurrentTrain: epoch  9, batch     2 | loss: 2.7014220Losses:  2.109872341156006 0.6507604122161865
CurrentTrain: epoch  9, batch     3 | loss: 2.7606328Losses:  2.145291328430176 0.746446967124939
CurrentTrain: epoch  9, batch     4 | loss: 2.8917384Losses:  1.8446483612060547 0.6026113629341125
CurrentTrain: epoch  9, batch     5 | loss: 2.4472597Losses:  1.8496052026748657 0.10938777774572372
CurrentTrain: epoch  9, batch     6 | loss: 1.9589930
Losses:  1.0841840505599976 0.6865590810775757
MemoryTrain:  epoch  0, batch     0 | loss: 1.7707431Losses:  1.2321207523345947 0.7494236826896667
MemoryTrain:  epoch  0, batch     1 | loss: 1.9815445Losses:  0.7038806676864624 0.42447134852409363
MemoryTrain:  epoch  0, batch     2 | loss: 1.1283520Losses:  0.43920183181762695 0.5633187890052795
MemoryTrain:  epoch  0, batch     3 | loss: 1.0025206Losses:  0.6490134000778198 0.6874252557754517
MemoryTrain:  epoch  0, batch     4 | loss: 1.3364387Losses:  1.7843271493911743 0.6873123645782471
MemoryTrain:  epoch  1, batch     0 | loss: 2.4716396Losses:  0.8480464816093445 0.6076095104217529
MemoryTrain:  epoch  1, batch     1 | loss: 1.4556561Losses:  0.7710869908332825 0.5310857892036438
MemoryTrain:  epoch  1, batch     2 | loss: 1.3021728Losses:  0.42145252227783203 0.4859825372695923
MemoryTrain:  epoch  1, batch     3 | loss: 0.9074351Losses:  0.2580990493297577 0.7367523908615112
MemoryTrain:  epoch  1, batch     4 | loss: 0.9948515Losses:  0.2990792393684387 0.402094304561615
MemoryTrain:  epoch  2, batch     0 | loss: 0.7011735Losses:  0.5574435591697693 0.6563037037849426
MemoryTrain:  epoch  2, batch     1 | loss: 1.2137473Losses:  0.5489989519119263 0.7055054903030396
MemoryTrain:  epoch  2, batch     2 | loss: 1.2545044Losses:  0.19728592038154602 0.5080394744873047
MemoryTrain:  epoch  2, batch     3 | loss: 0.7053254Losses:  0.07906362414360046 0.5899205207824707
MemoryTrain:  epoch  2, batch     4 | loss: 0.6689842Losses:  0.14785784482955933 0.5917948484420776
MemoryTrain:  epoch  3, batch     0 | loss: 0.7396527Losses:  0.09517569094896317 0.4531904458999634
MemoryTrain:  epoch  3, batch     1 | loss: 0.5483661Losses:  0.08813892304897308 0.504727303981781
MemoryTrain:  epoch  3, batch     2 | loss: 0.5928662Losses:  0.45110759139060974 0.5818231701850891
MemoryTrain:  epoch  3, batch     3 | loss: 1.0329307Losses:  0.16546989977359772 0.6371496915817261
MemoryTrain:  epoch  3, batch     4 | loss: 0.8026196Losses:  0.348792165517807 0.533206045627594
MemoryTrain:  epoch  4, batch     0 | loss: 0.8819982Losses:  0.026196900755167007 0.6793909072875977
MemoryTrain:  epoch  4, batch     1 | loss: 0.7055878Losses:  0.11343049257993698 0.5136179327964783
MemoryTrain:  epoch  4, batch     2 | loss: 0.6270484Losses:  0.029958536848425865 0.4480116665363312
MemoryTrain:  epoch  4, batch     3 | loss: 0.4779702Losses:  0.14417262375354767 0.7029483318328857
MemoryTrain:  epoch  4, batch     4 | loss: 0.8471209Losses:  0.24973542988300323 0.21126215159893036
MemoryTrain:  epoch  5, batch     0 | loss: 0.4609976Losses:  0.14519847929477692 0.7118788957595825
MemoryTrain:  epoch  5, batch     1 | loss: 0.8570774Losses:  0.12149536609649658 0.5998529195785522
MemoryTrain:  epoch  5, batch     2 | loss: 0.7213483Losses:  0.07310149073600769 0.6232572197914124
MemoryTrain:  epoch  5, batch     3 | loss: 0.6963587Losses:  0.1305980235338211 0.4866105914115906
MemoryTrain:  epoch  5, batch     4 | loss: 0.6172086Losses:  0.08893828094005585 0.657569169998169
MemoryTrain:  epoch  6, batch     0 | loss: 0.7465075Losses:  0.04714783653616905 0.5388996601104736
MemoryTrain:  epoch  6, batch     1 | loss: 0.5860475Losses:  0.17327550053596497 0.6834486722946167
MemoryTrain:  epoch  6, batch     2 | loss: 0.8567241Losses:  0.0920049399137497 0.36871546506881714
MemoryTrain:  epoch  6, batch     3 | loss: 0.4607204Losses:  0.025563709437847137 0.445292204618454
MemoryTrain:  epoch  6, batch     4 | loss: 0.4708559Losses:  0.09729079157114029 0.6171138286590576
MemoryTrain:  epoch  7, batch     0 | loss: 0.7144046Losses:  0.04889634996652603 0.5485494136810303
MemoryTrain:  epoch  7, batch     1 | loss: 0.5974458Losses:  0.016375688835978508 0.3461172580718994
MemoryTrain:  epoch  7, batch     2 | loss: 0.3624929Losses:  0.02527095563709736 0.33354246616363525
MemoryTrain:  epoch  7, batch     3 | loss: 0.3588134Losses:  0.06191340088844299 0.5562703609466553
MemoryTrain:  epoch  7, batch     4 | loss: 0.6181837Losses:  0.02586623653769493 0.587720513343811
MemoryTrain:  epoch  8, batch     0 | loss: 0.6135867Losses:  0.043659113347530365 0.6606377363204956
MemoryTrain:  epoch  8, batch     1 | loss: 0.7042968Losses:  0.02308349683880806 0.38046368956565857
MemoryTrain:  epoch  8, batch     2 | loss: 0.4035472Losses:  0.024289149791002274 0.3481059670448303
MemoryTrain:  epoch  8, batch     3 | loss: 0.3723951Losses:  0.04767470806837082 0.5488890409469604
MemoryTrain:  epoch  8, batch     4 | loss: 0.5965638Losses:  0.02404031902551651 0.4377700090408325
MemoryTrain:  epoch  9, batch     0 | loss: 0.4618103Losses:  0.022761093452572823 0.6267000436782837
MemoryTrain:  epoch  9, batch     1 | loss: 0.6494612Losses:  0.030135756358504295 0.5477218627929688
MemoryTrain:  epoch  9, batch     2 | loss: 0.5778576Losses:  0.03598780184984207 0.5264304876327515
MemoryTrain:  epoch  9, batch     3 | loss: 0.5624183Losses:  0.03528403863310814 0.6523721218109131
MemoryTrain:  epoch  9, batch     4 | loss: 0.6876562
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 74.87%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 75.13%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.46%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 88.77%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 88.41%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 88.06%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.05%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.48%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 88.27%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.26%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.29%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 88.26%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 87.91%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.58%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 87.03%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 86.42%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 84.94%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 84.00%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 83.24%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 82.56%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 81.75%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 81.53%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 82.55%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 82.47%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 82.33%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 83.90%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 83.83%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 83.84%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 83.71%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 83.53%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 83.56%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 83.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 83.43%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 82.92%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 82.42%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 81.83%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 81.44%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 80.92%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 80.78%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 80.78%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 81.11%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.16%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 81.21%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 81.29%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 81.47%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 81.12%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 80.74%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 80.61%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 80.32%   [EVAL] batch:  148 | acc: 18.75%,  total acc: 79.91%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 79.62%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 79.14%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 78.66%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 78.15%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 77.64%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 77.22%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.72%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 76.63%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 77.01%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 77.13%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 77.17%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 76.72%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 76.34%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 75.98%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 75.54%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 75.11%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 74.96%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 74.93%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 74.83%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 74.66%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 74.40%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.36%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 74.24%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 74.52%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.31%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 74.97%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 75.41%   [EVAL] batch:  214 | acc: 25.00%,  total acc: 75.17%   [EVAL] batch:  215 | acc: 50.00%,  total acc: 75.06%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 74.88%   [EVAL] batch:  217 | acc: 37.50%,  total acc: 74.71%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 74.49%   [EVAL] batch:  222 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:  223 | acc: 62.50%,  total acc: 74.33%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 74.36%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 74.36%   [EVAL] batch:  234 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 74.44%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 75.72%   
cur_acc:  ['0.9484', '0.8264', '0.6508', '0.7946']
his_acc:  ['0.9484', '0.8805', '0.7882', '0.7572']
Clustering into  24  clusters
Clusters:  [ 0  1 17  2  0  0 21  0 18 23 12 11  2  0 19 14 15  0  0 22  8  2  0  0
  0 20  0  0  0  0  1 13  2  0  0  6  0  0 10  9 16  0  4  0  5  7  3  0
  0  0]
Losses:  6.737321853637695 1.2586772441864014
CurrentTrain: epoch  0, batch     0 | loss: 7.9959993Losses:  3.3860654830932617 0.6647065877914429
CurrentTrain: epoch  0, batch     1 | loss: 4.0507722Losses:  5.821507453918457 1.2114320993423462
CurrentTrain: epoch  0, batch     2 | loss: 7.0329394Losses:  7.35078239440918 1.401301622390747
CurrentTrain: epoch  0, batch     3 | loss: 8.7520838Losses:  7.392152786254883 0.9293816089630127
CurrentTrain: epoch  0, batch     4 | loss: 8.3215342Losses:  5.042057037353516 0.9843674898147583
CurrentTrain: epoch  0, batch     5 | loss: 6.0264244Losses:  5.146422386169434 0.2907083034515381
CurrentTrain: epoch  0, batch     6 | loss: 5.4371309Losses:  7.549919128417969 0.8677679300308228
CurrentTrain: epoch  1, batch     0 | loss: 8.4176874Losses:  5.066375732421875 1.0406501293182373
CurrentTrain: epoch  1, batch     1 | loss: 6.1070261Losses:  3.6319878101348877 1.075471043586731
CurrentTrain: epoch  1, batch     2 | loss: 4.7074590Losses:  4.568828582763672 1.009943962097168
CurrentTrain: epoch  1, batch     3 | loss: 5.5787725Losses:  4.176850318908691 1.227975606918335
CurrentTrain: epoch  1, batch     4 | loss: 5.4048262Losses:  5.539501667022705 1.3539724349975586
CurrentTrain: epoch  1, batch     5 | loss: 6.8934741Losses:  3.6320366859436035 0.21123820543289185
CurrentTrain: epoch  1, batch     6 | loss: 3.8432748Losses:  3.896730661392212 1.007744550704956
CurrentTrain: epoch  2, batch     0 | loss: 4.9044752Losses:  4.9026384353637695 1.3158289194107056
CurrentTrain: epoch  2, batch     1 | loss: 6.2184672Losses:  5.72897481918335 0.862104058265686
CurrentTrain: epoch  2, batch     2 | loss: 6.5910788Losses:  3.590243101119995 1.0095778703689575
CurrentTrain: epoch  2, batch     3 | loss: 4.5998211Losses:  4.474740982055664 1.0275497436523438
CurrentTrain: epoch  2, batch     4 | loss: 5.5022907Losses:  4.72178316116333 0.8408857583999634
CurrentTrain: epoch  2, batch     5 | loss: 5.5626688Losses:  2.1081576347351074 0.287526398897171
CurrentTrain: epoch  2, batch     6 | loss: 2.3956840Losses:  2.545642852783203 0.6665157079696655
CurrentTrain: epoch  3, batch     0 | loss: 3.2121587Losses:  4.201474666595459 0.596903383731842
CurrentTrain: epoch  3, batch     1 | loss: 4.7983780Losses:  4.280509948730469 1.145740032196045
CurrentTrain: epoch  3, batch     2 | loss: 5.4262500Losses:  5.547531604766846 1.0472362041473389
CurrentTrain: epoch  3, batch     3 | loss: 6.5947676Losses:  5.171202659606934 1.074316143989563
CurrentTrain: epoch  3, batch     4 | loss: 6.2455187Losses:  3.153830051422119 1.153864860534668
CurrentTrain: epoch  3, batch     5 | loss: 4.3076949Losses:  4.407984256744385 0.8086479902267456
CurrentTrain: epoch  3, batch     6 | loss: 5.2166324Losses:  3.638643264770508 0.775562047958374
CurrentTrain: epoch  4, batch     0 | loss: 4.4142056Losses:  4.716536521911621 0.966569185256958
CurrentTrain: epoch  4, batch     1 | loss: 5.6831055Losses:  3.1387104988098145 0.8923043012619019
CurrentTrain: epoch  4, batch     2 | loss: 4.0310149Losses:  3.6654810905456543 0.8576425313949585
CurrentTrain: epoch  4, batch     3 | loss: 4.5231237Losses:  3.5456652641296387 1.2072205543518066
CurrentTrain: epoch  4, batch     4 | loss: 4.7528858Losses:  3.443308115005493 0.987680196762085
CurrentTrain: epoch  4, batch     5 | loss: 4.4309883Losses:  4.041316986083984 0.10148545354604721
CurrentTrain: epoch  4, batch     6 | loss: 4.1428022Losses:  4.8126044273376465 0.9612031579017639
CurrentTrain: epoch  5, batch     0 | loss: 5.7738075Losses:  3.7762370109558105 0.8128243684768677
CurrentTrain: epoch  5, batch     1 | loss: 4.5890613Losses:  3.623270034790039 0.8015874624252319
CurrentTrain: epoch  5, batch     2 | loss: 4.4248576Losses:  2.7499477863311768 0.8643442392349243
CurrentTrain: epoch  5, batch     3 | loss: 3.6142921Losses:  1.9217545986175537 0.6680676937103271
CurrentTrain: epoch  5, batch     4 | loss: 2.5898223Losses:  2.8708600997924805 0.8500878810882568
CurrentTrain: epoch  5, batch     5 | loss: 3.7209480Losses:  4.4928131103515625 0.31985005736351013
CurrentTrain: epoch  5, batch     6 | loss: 4.8126631Losses:  2.6653666496276855 0.7306921482086182
CurrentTrain: epoch  6, batch     0 | loss: 3.3960588Losses:  2.8080809116363525 0.7572939395904541
CurrentTrain: epoch  6, batch     1 | loss: 3.5653749Losses:  3.006026029586792 0.7017711997032166
CurrentTrain: epoch  6, batch     2 | loss: 3.7077973Losses:  2.320122241973877 0.5274185538291931
CurrentTrain: epoch  6, batch     3 | loss: 2.8475409Losses:  2.7301087379455566 0.7700690627098083
CurrentTrain: epoch  6, batch     4 | loss: 3.5001779Losses:  3.051913261413574 0.9355830550193787
CurrentTrain: epoch  6, batch     5 | loss: 3.9874964Losses:  2.684171199798584 0.18392249941825867
CurrentTrain: epoch  6, batch     6 | loss: 2.8680937Losses:  2.824549913406372 0.8457379341125488
CurrentTrain: epoch  7, batch     0 | loss: 3.6702878Losses:  2.017764091491699 0.7085903882980347
CurrentTrain: epoch  7, batch     1 | loss: 2.7263546Losses:  2.8852109909057617 0.8522395491600037
CurrentTrain: epoch  7, batch     2 | loss: 3.7374506Losses:  2.436659812927246 0.9129654169082642
CurrentTrain: epoch  7, batch     3 | loss: 3.3496251Losses:  2.361964464187622 0.7042828798294067
CurrentTrain: epoch  7, batch     4 | loss: 3.0662475Losses:  2.4478871822357178 0.8502185344696045
CurrentTrain: epoch  7, batch     5 | loss: 3.2981057Losses:  1.7809813022613525 0.12648643553256989
CurrentTrain: epoch  7, batch     6 | loss: 1.9074677Losses:  2.8757801055908203 0.8502318263053894
CurrentTrain: epoch  8, batch     0 | loss: 3.7260120Losses:  1.9644479751586914 0.4276456832885742
CurrentTrain: epoch  8, batch     1 | loss: 2.3920937Losses:  2.223012924194336 0.5479779243469238
CurrentTrain: epoch  8, batch     2 | loss: 2.7709908Losses:  1.9900541305541992 0.6008505821228027
CurrentTrain: epoch  8, batch     3 | loss: 2.5909047Losses:  2.4442524909973145 0.7917188405990601
CurrentTrain: epoch  8, batch     4 | loss: 3.2359715Losses:  2.2192273139953613 0.6209325790405273
CurrentTrain: epoch  8, batch     5 | loss: 2.8401599Losses:  1.8422441482543945 0.1120796948671341
CurrentTrain: epoch  8, batch     6 | loss: 1.9543239Losses:  2.0519328117370605 0.40914422273635864
CurrentTrain: epoch  9, batch     0 | loss: 2.4610770Losses:  1.8728835582733154 0.6101032495498657
CurrentTrain: epoch  9, batch     1 | loss: 2.4829869Losses:  2.444087028503418 0.5613126158714294
CurrentTrain: epoch  9, batch     2 | loss: 3.0053997Losses:  2.0214896202087402 0.5642845630645752
CurrentTrain: epoch  9, batch     3 | loss: 2.5857742Losses:  2.073930263519287 0.6979473829269409
CurrentTrain: epoch  9, batch     4 | loss: 2.7718778Losses:  2.0535614490509033 0.6176275610923767
CurrentTrain: epoch  9, batch     5 | loss: 2.6711891Losses:  2.0041251182556152 0.1427578181028366
CurrentTrain: epoch  9, batch     6 | loss: 2.1468830
Losses:  0.23654818534851074 0.42402541637420654
MemoryTrain:  epoch  0, batch     0 | loss: 0.6605736Losses:  0.22367867827415466 0.33071255683898926
MemoryTrain:  epoch  0, batch     1 | loss: 0.5543913Losses:  0.6501711010932922 0.645841658115387
MemoryTrain:  epoch  0, batch     2 | loss: 1.2960128Losses:  0.7935150265693665 0.5914615392684937
MemoryTrain:  epoch  0, batch     3 | loss: 1.3849766Losses:  0.9733201265335083 0.606371283531189
MemoryTrain:  epoch  0, batch     4 | loss: 1.5796914Losses:  0.22801488637924194 0.750891923904419
MemoryTrain:  epoch  0, batch     5 | loss: 0.9789068Losses:  0.44020795822143555 0.254042387008667
MemoryTrain:  epoch  0, batch     6 | loss: 0.6942503Losses:  0.6259708404541016 0.5718698501586914
MemoryTrain:  epoch  1, batch     0 | loss: 1.1978407Losses:  0.38340672850608826 0.45986518263816833
MemoryTrain:  epoch  1, batch     1 | loss: 0.8432719Losses:  0.29215383529663086 0.6088849902153015
MemoryTrain:  epoch  1, batch     2 | loss: 0.9010388Losses:  0.6717722415924072 0.4272593557834625
MemoryTrain:  epoch  1, batch     3 | loss: 1.0990316Losses:  0.5359612703323364 0.6021809577941895
MemoryTrain:  epoch  1, batch     4 | loss: 1.1381422Losses:  0.6980797052383423 0.7422113418579102
MemoryTrain:  epoch  1, batch     5 | loss: 1.4402910Losses:  0.10968142747879028 0.05242856591939926
MemoryTrain:  epoch  1, batch     6 | loss: 0.1621100Losses:  0.4407302141189575 0.561802089214325
MemoryTrain:  epoch  2, batch     0 | loss: 1.0025322Losses:  0.2141096591949463 0.7086151838302612
MemoryTrain:  epoch  2, batch     1 | loss: 0.9227248Losses:  0.11131349951028824 0.36513376235961914
MemoryTrain:  epoch  2, batch     2 | loss: 0.4764473Losses:  0.5067051649093628 0.7115578651428223
MemoryTrain:  epoch  2, batch     3 | loss: 1.2182630Losses:  0.23134402930736542 0.5419086813926697
MemoryTrain:  epoch  2, batch     4 | loss: 0.7732527Losses:  0.0988328754901886 0.3940233588218689
MemoryTrain:  epoch  2, batch     5 | loss: 0.4928562Losses:  0.9239900708198547 0.08142495155334473
MemoryTrain:  epoch  2, batch     6 | loss: 1.0054150Losses:  0.14215624332427979 0.6266058683395386
MemoryTrain:  epoch  3, batch     0 | loss: 0.7687621Losses:  0.30758213996887207 0.5044946670532227
MemoryTrain:  epoch  3, batch     1 | loss: 0.8120768Losses:  0.19741863012313843 0.6615545749664307
MemoryTrain:  epoch  3, batch     2 | loss: 0.8589732Losses:  0.18307743966579437 0.46133577823638916
MemoryTrain:  epoch  3, batch     3 | loss: 0.6444132Losses:  0.1747916340827942 0.6310855150222778
MemoryTrain:  epoch  3, batch     4 | loss: 0.8058771Losses:  0.053260646760463715 0.4606055021286011
MemoryTrain:  epoch  3, batch     5 | loss: 0.5138661Losses:  0.0827624499797821 0.07614526152610779
MemoryTrain:  epoch  3, batch     6 | loss: 0.1589077Losses:  0.040167734026908875 0.49211734533309937
MemoryTrain:  epoch  4, batch     0 | loss: 0.5322851Losses:  0.11305247992277145 0.6839730739593506
MemoryTrain:  epoch  4, batch     1 | loss: 0.7970256Losses:  0.05687620863318443 0.3728143274784088
MemoryTrain:  epoch  4, batch     2 | loss: 0.4296905Losses:  0.057471420615911484 0.45315223932266235
MemoryTrain:  epoch  4, batch     3 | loss: 0.5106236Losses:  0.06985581666231155 0.5174261331558228
MemoryTrain:  epoch  4, batch     4 | loss: 0.5872819Losses:  0.335738867521286 0.7602988481521606
MemoryTrain:  epoch  4, batch     5 | loss: 1.0960377Losses:  0.043531037867069244 0.13222837448120117
MemoryTrain:  epoch  4, batch     6 | loss: 0.1757594Losses:  0.15125198662281036 0.5190464854240417
MemoryTrain:  epoch  5, batch     0 | loss: 0.6702985Losses:  0.04694344475865364 0.4546785354614258
MemoryTrain:  epoch  5, batch     1 | loss: 0.5016220Losses:  0.032669369131326675 0.3535957932472229
MemoryTrain:  epoch  5, batch     2 | loss: 0.3862652Losses:  0.12333952635526657 0.45985978841781616
MemoryTrain:  epoch  5, batch     3 | loss: 0.5831993Losses:  0.031223969534039497 0.35374557971954346
MemoryTrain:  epoch  5, batch     4 | loss: 0.3849696Losses:  0.10772466659545898 0.7999231815338135
MemoryTrain:  epoch  5, batch     5 | loss: 0.9076478Losses:  0.042801178991794586 0.04145246744155884
MemoryTrain:  epoch  5, batch     6 | loss: 0.0842536Losses:  0.05096981301903725 0.594489336013794
MemoryTrain:  epoch  6, batch     0 | loss: 0.6454592Losses:  0.05635543167591095 0.557354211807251
MemoryTrain:  epoch  6, batch     1 | loss: 0.6137096Losses:  0.05321505665779114 0.5665318369865417
MemoryTrain:  epoch  6, batch     2 | loss: 0.6197469Losses:  0.08326463401317596 0.4108417332172394
MemoryTrain:  epoch  6, batch     3 | loss: 0.4941064Losses:  0.05083891376852989 0.4506770372390747
MemoryTrain:  epoch  6, batch     4 | loss: 0.5015159Losses:  0.03570210933685303 0.40720638632774353
MemoryTrain:  epoch  6, batch     5 | loss: 0.4429085Losses:  0.046740949153900146 0.057673774659633636
MemoryTrain:  epoch  6, batch     6 | loss: 0.1044147Losses:  0.055002838373184204 0.4920734763145447
MemoryTrain:  epoch  7, batch     0 | loss: 0.5470763Losses:  0.06381745636463165 0.5730270147323608
MemoryTrain:  epoch  7, batch     1 | loss: 0.6368445Losses:  0.045975543558597565 0.5018445253372192
MemoryTrain:  epoch  7, batch     2 | loss: 0.5478201Losses:  0.06324107944965363 0.48490309715270996
MemoryTrain:  epoch  7, batch     3 | loss: 0.5481442Losses:  0.022884797304868698 0.3997122645378113
MemoryTrain:  epoch  7, batch     4 | loss: 0.4225971Losses:  0.020256809890270233 0.401777446269989
MemoryTrain:  epoch  7, batch     5 | loss: 0.4220343Losses:  0.008857913315296173 0.027824116870760918
MemoryTrain:  epoch  7, batch     6 | loss: 0.0366820Losses:  0.03483286127448082 0.5478899478912354
MemoryTrain:  epoch  8, batch     0 | loss: 0.5827228Losses:  0.032748669385910034 0.4493008852005005
MemoryTrain:  epoch  8, batch     1 | loss: 0.4820496Losses:  0.024483369663357735 0.32005733251571655
MemoryTrain:  epoch  8, batch     2 | loss: 0.3445407Losses:  0.030612241476774216 0.36293092370033264
MemoryTrain:  epoch  8, batch     3 | loss: 0.3935432Losses:  0.062152888625860214 0.5674346089363098
MemoryTrain:  epoch  8, batch     4 | loss: 0.6295875Losses:  0.030705099925398827 0.5253664255142212
MemoryTrain:  epoch  8, batch     5 | loss: 0.5560715Losses:  0.032537683844566345 0.07859444618225098
MemoryTrain:  epoch  8, batch     6 | loss: 0.1111321Losses:  0.03006519004702568 0.5084962844848633
MemoryTrain:  epoch  9, batch     0 | loss: 0.5385615Losses:  0.022621311247348785 0.5188435316085815
MemoryTrain:  epoch  9, batch     1 | loss: 0.5414649Losses:  0.022530315443873405 0.28429362177848816
MemoryTrain:  epoch  9, batch     2 | loss: 0.3068239Losses:  0.034918203949928284 0.46093815565109253
MemoryTrain:  epoch  9, batch     3 | loss: 0.4958563Losses:  0.01988595724105835 0.4630924463272095
MemoryTrain:  epoch  9, batch     4 | loss: 0.4829784Losses:  0.03382588550448418 0.48655861616134644
MemoryTrain:  epoch  9, batch     5 | loss: 0.5203845Losses:  0.030866894870996475 0.07009495049715042
MemoryTrain:  epoch  9, batch     6 | loss: 0.1009618
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 82.77%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 79.62%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.65%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.35%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 79.53%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.37%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.01%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 86.78%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 86.11%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.43%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.73%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 84.92%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 84.51%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 84.77%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.81%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 85.02%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 84.21%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 83.70%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 82.62%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 81.70%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 80.88%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 80.07%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 79.29%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 78.38%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 78.05%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 79.45%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 79.34%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 79.36%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 79.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.03%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 80.70%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 80.54%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 80.18%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 80.03%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 79.88%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 79.96%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 80.02%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 80.13%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.19%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 80.20%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 79.66%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 79.23%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 78.76%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 78.25%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 77.88%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 77.48%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 77.37%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 77.90%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 77.99%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 78.17%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 77.89%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 77.57%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 76.98%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 76.72%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 76.46%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 75.99%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 75.49%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 74.51%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.11%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.64%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.73%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 74.03%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.51%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 74.59%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 74.70%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 74.30%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 73.87%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 73.51%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 73.09%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 72.67%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 72.25%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 71.95%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.79%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 71.72%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 71.52%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 71.40%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 71.27%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 71.21%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 71.64%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 71.51%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 71.64%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 71.64%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 71.60%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 71.69%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  213 | acc: 18.75%,  total acc: 72.11%   [EVAL] batch:  214 | acc: 18.75%,  total acc: 71.86%   [EVAL] batch:  215 | acc: 25.00%,  total acc: 71.64%   [EVAL] batch:  216 | acc: 12.50%,  total acc: 71.37%   [EVAL] batch:  217 | acc: 18.75%,  total acc: 71.13%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 71.00%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 70.94%   [EVAL] batch:  220 | acc: 43.75%,  total acc: 70.81%   [EVAL] batch:  221 | acc: 37.50%,  total acc: 70.66%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 70.49%   [EVAL] batch:  223 | acc: 31.25%,  total acc: 70.31%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 70.41%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 70.43%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.48%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 71.99%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 72.41%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 72.31%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 72.27%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 73.45%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 73.43%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 73.43%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 73.28%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 73.10%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 73.04%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 72.91%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 73.22%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 73.34%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 73.39%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 73.43%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.40%   
cur_acc:  ['0.9484', '0.8264', '0.6508', '0.7946', '0.7937']
his_acc:  ['0.9484', '0.8805', '0.7882', '0.7572', '0.7340']
Clustering into  29  clusters
Clusters:  [ 0  2 19  0  0  0 24  0 20 15 25 23  0  0 11 27  9  0  0 26 22  0  0  0
  0 17  0  0  0  0  2 21  0  0  0  1  0 28 10 12 18  0 14  0 16  8 13  0
  0  0  1  4  0  6  7  0  0  0  5  3]
Losses:  6.08909273147583 1.2677830457687378
CurrentTrain: epoch  0, batch     0 | loss: 7.3568759Losses:  7.2628583908081055 1.1009860038757324
CurrentTrain: epoch  0, batch     1 | loss: 8.3638439Losses:  6.082277297973633 1.085165023803711
CurrentTrain: epoch  0, batch     2 | loss: 7.1674423Losses:  7.316080093383789 1.5080246925354004
CurrentTrain: epoch  0, batch     3 | loss: 8.8241043Losses:  6.612603187561035 1.0543292760849
CurrentTrain: epoch  0, batch     4 | loss: 7.6669326Losses:  6.132607460021973 1.277078628540039
CurrentTrain: epoch  0, batch     5 | loss: 7.4096861Losses:  4.9999589920043945 0.496027410030365
CurrentTrain: epoch  0, batch     6 | loss: 5.4959865Losses:  7.125659465789795 1.3974074125289917
CurrentTrain: epoch  1, batch     0 | loss: 8.5230665Losses:  5.288718223571777 1.024662733078003
CurrentTrain: epoch  1, batch     1 | loss: 6.3133812Losses:  5.016814231872559 1.2012778520584106
CurrentTrain: epoch  1, batch     2 | loss: 6.2180920Losses:  4.021902084350586 1.0915111303329468
CurrentTrain: epoch  1, batch     3 | loss: 5.1134133Losses:  5.845675468444824 0.894772469997406
CurrentTrain: epoch  1, batch     4 | loss: 6.7404480Losses:  5.290502071380615 1.0278598070144653
CurrentTrain: epoch  1, batch     5 | loss: 6.3183618Losses:  2.2686376571655273 0.0
CurrentTrain: epoch  1, batch     6 | loss: 2.2686377Losses:  5.003426551818848 1.2244246006011963
CurrentTrain: epoch  2, batch     0 | loss: 6.2278509Losses:  4.366111755371094 0.7227603197097778
CurrentTrain: epoch  2, batch     1 | loss: 5.0888720Losses:  6.022637367248535 1.465766191482544
CurrentTrain: epoch  2, batch     2 | loss: 7.4884033Losses:  3.120903491973877 0.777397096157074
CurrentTrain: epoch  2, batch     3 | loss: 3.8983006Losses:  5.350600719451904 1.085689663887024
CurrentTrain: epoch  2, batch     4 | loss: 6.4362903Losses:  3.317775249481201 0.637103796005249
CurrentTrain: epoch  2, batch     5 | loss: 3.9548790Losses:  2.8751220703125 0.3875586986541748
CurrentTrain: epoch  2, batch     6 | loss: 3.2626808Losses:  5.239132881164551 1.045767903327942
CurrentTrain: epoch  3, batch     0 | loss: 6.2849007Losses:  2.949707269668579 0.8112295866012573
CurrentTrain: epoch  3, batch     1 | loss: 3.7609367Losses:  4.090363502502441 1.0380024909973145
CurrentTrain: epoch  3, batch     2 | loss: 5.1283660Losses:  4.497649192810059 1.249139666557312
CurrentTrain: epoch  3, batch     3 | loss: 5.7467890Losses:  3.379551410675049 0.990416944026947
CurrentTrain: epoch  3, batch     4 | loss: 4.3699684Losses:  3.4216976165771484 1.0800845623016357
CurrentTrain: epoch  3, batch     5 | loss: 4.5017824Losses:  1.8919322490692139 0.1010945588350296
CurrentTrain: epoch  3, batch     6 | loss: 1.9930269Losses:  2.890573501586914 1.0193005800247192
CurrentTrain: epoch  4, batch     0 | loss: 3.9098740Losses:  3.771881580352783 1.232992172241211
CurrentTrain: epoch  4, batch     1 | loss: 5.0048738Losses:  3.213089942932129 0.7556172609329224
CurrentTrain: epoch  4, batch     2 | loss: 3.9687071Losses:  3.1011080741882324 0.858060896396637
CurrentTrain: epoch  4, batch     3 | loss: 3.9591689Losses:  3.836246967315674 0.8943373560905457
CurrentTrain: epoch  4, batch     4 | loss: 4.7305841Losses:  3.0227043628692627 0.8784216046333313
CurrentTrain: epoch  4, batch     5 | loss: 3.9011259Losses:  5.039825439453125 0.6523846983909607
CurrentTrain: epoch  4, batch     6 | loss: 5.6922102Losses:  3.9829118251800537 1.0375092029571533
CurrentTrain: epoch  5, batch     0 | loss: 5.0204210Losses:  3.647320508956909 1.1236375570297241
CurrentTrain: epoch  5, batch     1 | loss: 4.7709579Losses:  2.8544206619262695 0.8799321055412292
CurrentTrain: epoch  5, batch     2 | loss: 3.7343528Losses:  3.067523956298828 0.666962742805481
CurrentTrain: epoch  5, batch     3 | loss: 3.7344866Losses:  2.627884864807129 0.7086437940597534
CurrentTrain: epoch  5, batch     4 | loss: 3.3365288Losses:  3.218209743499756 0.8810243606567383
CurrentTrain: epoch  5, batch     5 | loss: 4.0992341Losses:  1.8981215953826904 0.138453871011734
CurrentTrain: epoch  5, batch     6 | loss: 2.0365756Losses:  3.731334686279297 1.0796996355056763
CurrentTrain: epoch  6, batch     0 | loss: 4.8110342Losses:  3.379955768585205 1.043560266494751
CurrentTrain: epoch  6, batch     1 | loss: 4.4235163Losses:  2.4806389808654785 0.7320057153701782
CurrentTrain: epoch  6, batch     2 | loss: 3.2126446Losses:  2.831808567047119 0.6829478144645691
CurrentTrain: epoch  6, batch     3 | loss: 3.5147564Losses:  2.9921860694885254 0.7814154624938965
CurrentTrain: epoch  6, batch     4 | loss: 3.7736015Losses:  2.3323864936828613 0.5517653226852417
CurrentTrain: epoch  6, batch     5 | loss: 2.8841519Losses:  2.3307876586914062 0.3158591687679291
CurrentTrain: epoch  6, batch     6 | loss: 2.6466467Losses:  2.211207151412964 0.8669559955596924
CurrentTrain: epoch  7, batch     0 | loss: 3.0781631Losses:  2.363016128540039 0.45588499307632446
CurrentTrain: epoch  7, batch     1 | loss: 2.8189011Losses:  3.4647912979125977 0.6724053025245667
CurrentTrain: epoch  7, batch     2 | loss: 4.1371965Losses:  2.556110382080078 0.9421722888946533
CurrentTrain: epoch  7, batch     3 | loss: 3.4982827Losses:  2.8034090995788574 0.5932668447494507
CurrentTrain: epoch  7, batch     4 | loss: 3.3966761Losses:  2.4308297634124756 0.6404139399528503
CurrentTrain: epoch  7, batch     5 | loss: 3.0712438Losses:  2.048130989074707 0.17588204145431519
CurrentTrain: epoch  7, batch     6 | loss: 2.2240131Losses:  2.654362678527832 0.9057283401489258
CurrentTrain: epoch  8, batch     0 | loss: 3.5600910Losses:  1.9995410442352295 0.6634036302566528
CurrentTrain: epoch  8, batch     1 | loss: 2.6629448Losses:  2.0894150733947754 0.6467300653457642
CurrentTrain: epoch  8, batch     2 | loss: 2.7361450Losses:  2.557013511657715 0.7846555709838867
CurrentTrain: epoch  8, batch     3 | loss: 3.3416691Losses:  2.933584451675415 0.8864710330963135
CurrentTrain: epoch  8, batch     4 | loss: 3.8200555Losses:  2.4984965324401855 0.8142950534820557
CurrentTrain: epoch  8, batch     5 | loss: 3.3127916Losses:  2.2150869369506836 0.2558976411819458
CurrentTrain: epoch  8, batch     6 | loss: 2.4709845Losses:  2.775451183319092 0.7798410654067993
CurrentTrain: epoch  9, batch     0 | loss: 3.5552921Losses:  2.312370777130127 0.8413404822349548
CurrentTrain: epoch  9, batch     1 | loss: 3.1537113Losses:  2.1893322467803955 0.519931435585022
CurrentTrain: epoch  9, batch     2 | loss: 2.7092638Losses:  2.014399290084839 0.6353077292442322
CurrentTrain: epoch  9, batch     3 | loss: 2.6497071Losses:  2.3823764324188232 0.8552227020263672
CurrentTrain: epoch  9, batch     4 | loss: 3.2375991Losses:  1.897289752960205 0.5438593626022339
CurrentTrain: epoch  9, batch     5 | loss: 2.4411492Losses:  2.0826971530914307 0.17910602688789368
CurrentTrain: epoch  9, batch     6 | loss: 2.2618032
Losses:  0.7666310667991638 0.40419647097587585
MemoryTrain:  epoch  0, batch     0 | loss: 1.1708275Losses:  0.3269232511520386 0.4299997091293335
MemoryTrain:  epoch  0, batch     1 | loss: 0.7569230Losses:  0.41248106956481934 0.41707515716552734
MemoryTrain:  epoch  0, batch     2 | loss: 0.8295562Losses:  1.1003011465072632 0.580995500087738
MemoryTrain:  epoch  0, batch     3 | loss: 1.6812966Losses:  0.6679874658584595 0.533003032207489
MemoryTrain:  epoch  0, batch     4 | loss: 1.2009904Losses:  0.06158965080976486 0.7305927276611328
MemoryTrain:  epoch  0, batch     5 | loss: 0.7921824Losses:  0.2896077632904053 0.5978702902793884
MemoryTrain:  epoch  0, batch     6 | loss: 0.8874781Losses:  1.0531573295593262 0.4105634093284607
MemoryTrain:  epoch  0, batch     7 | loss: 1.4637208Losses:  0.3023654520511627 0.7221329212188721
MemoryTrain:  epoch  1, batch     0 | loss: 1.0244983Losses:  0.9948471784591675 0.3940662145614624
MemoryTrain:  epoch  1, batch     1 | loss: 1.3889134Losses:  0.7160447239875793 0.6356590390205383
MemoryTrain:  epoch  1, batch     2 | loss: 1.3517038Losses:  0.9156342148780823 0.4812851548194885
MemoryTrain:  epoch  1, batch     3 | loss: 1.3969194Losses:  0.9194467067718506 0.5000981092453003
MemoryTrain:  epoch  1, batch     4 | loss: 1.4195448Losses:  1.2002718448638916 0.4669892191886902
MemoryTrain:  epoch  1, batch     5 | loss: 1.6672611Losses:  0.2823787331581116 0.42869722843170166
MemoryTrain:  epoch  1, batch     6 | loss: 0.7110760Losses:  0.07107056677341461 0.14045186340808868
MemoryTrain:  epoch  1, batch     7 | loss: 0.2115224Losses:  0.4635094106197357 0.6172804832458496
MemoryTrain:  epoch  2, batch     0 | loss: 1.0807899Losses:  0.2513464689254761 0.5613933801651001
MemoryTrain:  epoch  2, batch     1 | loss: 0.8127398Losses:  0.25843167304992676 0.6051030158996582
MemoryTrain:  epoch  2, batch     2 | loss: 0.8635347Losses:  0.11593730747699738 0.3647010028362274
MemoryTrain:  epoch  2, batch     3 | loss: 0.4806383Losses:  0.06770969927310944 0.5641798973083496
MemoryTrain:  epoch  2, batch     4 | loss: 0.6318896Losses:  0.4579378366470337 0.6169038414955139
MemoryTrain:  epoch  2, batch     5 | loss: 1.0748417Losses:  0.2704140543937683 0.4246003031730652
MemoryTrain:  epoch  2, batch     6 | loss: 0.6950144Losses:  0.04718809574842453 0.24943923950195312
MemoryTrain:  epoch  2, batch     7 | loss: 0.2966273Losses:  0.1396634876728058 0.47809937596321106
MemoryTrain:  epoch  3, batch     0 | loss: 0.6177629Losses:  0.15255147218704224 0.5474623441696167
MemoryTrain:  epoch  3, batch     1 | loss: 0.7000138Losses:  0.18218989670276642 0.3227114677429199
MemoryTrain:  epoch  3, batch     2 | loss: 0.5049013Losses:  0.06032009795308113 0.43949195742607117
MemoryTrain:  epoch  3, batch     3 | loss: 0.4998121Losses:  0.13493892550468445 0.4710479974746704
MemoryTrain:  epoch  3, batch     4 | loss: 0.6059870Losses:  0.05079275369644165 0.5320841073989868
MemoryTrain:  epoch  3, batch     5 | loss: 0.5828769Losses:  0.391539067029953 0.6484485268592834
MemoryTrain:  epoch  3, batch     6 | loss: 1.0399876Losses:  0.07307292520999908 0.3192605674266815
MemoryTrain:  epoch  3, batch     7 | loss: 0.3923335Losses:  0.09098149091005325 0.3274115025997162
MemoryTrain:  epoch  4, batch     0 | loss: 0.4183930Losses:  0.0543297603726387 0.5397881269454956
MemoryTrain:  epoch  4, batch     1 | loss: 0.5941179Losses:  0.3211064636707306 0.5191572904586792
MemoryTrain:  epoch  4, batch     2 | loss: 0.8402637Losses:  0.18177664279937744 0.8859052658081055
MemoryTrain:  epoch  4, batch     3 | loss: 1.0676819Losses:  0.08149386942386627 0.3283471465110779
MemoryTrain:  epoch  4, batch     4 | loss: 0.4098410Losses:  0.043298348784446716 0.6262164115905762
MemoryTrain:  epoch  4, batch     5 | loss: 0.6695148Losses:  0.13760897517204285 0.3974858224391937
MemoryTrain:  epoch  4, batch     6 | loss: 0.5350948Losses:  0.0811660885810852 0.22661660611629486
MemoryTrain:  epoch  4, batch     7 | loss: 0.3077827Losses:  0.11663220077753067 0.5061068534851074
MemoryTrain:  epoch  5, batch     0 | loss: 0.6227391Losses:  0.06327488273382187 0.49605605006217957
MemoryTrain:  epoch  5, batch     1 | loss: 0.5593309Losses:  0.06259921193122864 0.5485159158706665
MemoryTrain:  epoch  5, batch     2 | loss: 0.6111151Losses:  0.17475666105747223 0.35095781087875366
MemoryTrain:  epoch  5, batch     3 | loss: 0.5257145Losses:  0.12777698040008545 0.5846287608146667
MemoryTrain:  epoch  5, batch     4 | loss: 0.7124057Losses:  0.04719354584813118 0.31543517112731934
MemoryTrain:  epoch  5, batch     5 | loss: 0.3626287Losses:  0.1047956794500351 0.550906240940094
MemoryTrain:  epoch  5, batch     6 | loss: 0.6557019Losses:  0.06775692105293274 0.36476922035217285
MemoryTrain:  epoch  5, batch     7 | loss: 0.4325261Losses:  0.020462151616811752 0.29192477464675903
MemoryTrain:  epoch  6, batch     0 | loss: 0.3123869Losses:  0.061386942863464355 0.5403786897659302
MemoryTrain:  epoch  6, batch     1 | loss: 0.6017656Losses:  0.05142354965209961 0.6199562549591064
MemoryTrain:  epoch  6, batch     2 | loss: 0.6713798Losses:  0.1895773708820343 0.3525587320327759
MemoryTrain:  epoch  6, batch     3 | loss: 0.5421361Losses:  0.05143414065241814 0.7175611257553101
MemoryTrain:  epoch  6, batch     4 | loss: 0.7689953Losses:  0.04124974086880684 0.2971101403236389
MemoryTrain:  epoch  6, batch     5 | loss: 0.3383599Losses:  0.21182790398597717 0.5225263237953186
MemoryTrain:  epoch  6, batch     6 | loss: 0.7343543Losses:  0.01755141094326973 0.11881782114505768
MemoryTrain:  epoch  6, batch     7 | loss: 0.1363692Losses:  0.08967234939336777 0.4496018886566162
MemoryTrain:  epoch  7, batch     0 | loss: 0.5392742Losses:  0.06310652196407318 0.5598165392875671
MemoryTrain:  epoch  7, batch     1 | loss: 0.6229231Losses:  0.046419743448495865 0.4274190068244934
MemoryTrain:  epoch  7, batch     2 | loss: 0.4738387Losses:  0.043436698615550995 0.299335777759552
MemoryTrain:  epoch  7, batch     3 | loss: 0.3427725Losses:  0.03220962733030319 0.48026779294013977
MemoryTrain:  epoch  7, batch     4 | loss: 0.5124774Losses:  0.03822030872106552 0.4819789230823517
MemoryTrain:  epoch  7, batch     5 | loss: 0.5201992Losses:  0.06018126755952835 0.4665829539299011
MemoryTrain:  epoch  7, batch     6 | loss: 0.5267642Losses:  0.0811208114027977 0.2785398066043854
MemoryTrain:  epoch  7, batch     7 | loss: 0.3596606Losses:  0.037441834807395935 0.30124974250793457
MemoryTrain:  epoch  8, batch     0 | loss: 0.3386916Losses:  0.1344863772392273 0.6223481893539429
MemoryTrain:  epoch  8, batch     1 | loss: 0.7568346Losses:  0.047208212316036224 0.4253385663032532
MemoryTrain:  epoch  8, batch     2 | loss: 0.4725468Losses:  0.05085077881813049 0.5664836764335632
MemoryTrain:  epoch  8, batch     3 | loss: 0.6173345Losses:  0.1229739785194397 0.5686227083206177
MemoryTrain:  epoch  8, batch     4 | loss: 0.6915967Losses:  0.04169454798102379 0.3905128538608551
MemoryTrain:  epoch  8, batch     5 | loss: 0.4322074Losses:  0.027145586907863617 0.4414059817790985
MemoryTrain:  epoch  8, batch     6 | loss: 0.4685516Losses:  0.020490242168307304 0.31879425048828125
MemoryTrain:  epoch  8, batch     7 | loss: 0.3392845Losses:  0.03700357675552368 0.3642362058162689
MemoryTrain:  epoch  9, batch     0 | loss: 0.4012398Losses:  0.03220054879784584 0.40320998430252075
MemoryTrain:  epoch  9, batch     1 | loss: 0.4354105Losses:  0.048046842217445374 0.5039736032485962
MemoryTrain:  epoch  9, batch     2 | loss: 0.5520204Losses:  0.04726844280958176 0.49177372455596924
MemoryTrain:  epoch  9, batch     3 | loss: 0.5390422Losses:  0.09717793762683868 0.5098326206207275
MemoryTrain:  epoch  9, batch     4 | loss: 0.6070105Losses:  0.025084372609853745 0.3212372660636902
MemoryTrain:  epoch  9, batch     5 | loss: 0.3463216Losses:  0.04979176074266434 0.3372279107570648
MemoryTrain:  epoch  9, batch     6 | loss: 0.3870197Losses:  0.04889197275042534 0.13311032950878143
MemoryTrain:  epoch  9, batch     7 | loss: 0.1820023
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 74.85%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 74.78%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 73.79%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 72.75%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 72.02%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 85.82%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 85.50%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 84.20%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.90%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.46%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 80.75%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 80.87%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 80.97%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 80.78%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.34%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.34%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 81.42%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 81.67%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 81.33%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 80.93%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 80.69%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 80.06%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.84%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 79.32%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.04%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 78.24%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 77.46%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 76.84%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 76.16%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 75.29%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 76.71%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 76.93%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 76.91%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 76.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.93%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 78.62%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 78.37%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 78.23%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 77.99%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 77.81%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 77.73%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 77.31%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 76.86%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 76.31%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 75.96%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 75.52%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 75.43%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 75.47%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.87%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 76.03%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 75.99%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 75.68%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 75.47%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 75.21%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 74.92%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.25%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 73.77%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.28%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 72.81%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 72.38%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 71.92%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 72.48%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 72.72%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 72.30%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 71.95%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 71.57%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 71.16%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 70.79%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 70.60%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 70.47%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 70.11%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 70.41%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 71.19%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 71.15%   [EVAL] batch:  216 | acc: 18.75%,  total acc: 70.91%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 70.79%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 70.66%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 70.33%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 70.10%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 69.93%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 69.64%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 69.36%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 68.93%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 70.24%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 70.67%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 70.53%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 71.64%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 71.65%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:  294 | acc: 18.75%,  total acc: 71.44%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 71.24%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 71.06%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 70.95%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 70.86%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 70.73%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.14%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 70.90%   [EVAL] batch:  308 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:  309 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:  311 | acc: 31.25%,  total acc: 70.53%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 70.40%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 70.32%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 70.23%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 70.09%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  325 | acc: 6.25%,  total acc: 70.40%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 70.20%   [EVAL] batch:  327 | acc: 0.00%,  total acc: 69.99%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 69.81%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 69.68%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 69.54%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 70.99%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 71.00%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 71.03%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 70.90%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.26%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 71.10%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 71.01%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 70.95%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 70.91%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 70.87%   
cur_acc:  ['0.9484', '0.8264', '0.6508', '0.7946', '0.7937', '0.7202']
his_acc:  ['0.9484', '0.8805', '0.7882', '0.7572', '0.7340', '0.7087']
Clustering into  34  clusters
Clusters:  [ 0  5 22  0  0  0 27  0 23 31 33 28  0  0 26 32 24  0  0 29 15  0  0  0
  0 19  0  0  0  0  5 21  0  0  0  2  0 16 25 13 20  0 17  0 18  9  8  0
  0  0  2 11  0  7 30  0  0  0 10  4  0  0  0 14  0 12  0  6  3  1]
Losses:  6.084132194519043 1.2226365804672241
CurrentTrain: epoch  0, batch     0 | loss: 7.3067689Losses:  7.093594551086426 0.9814565181732178
CurrentTrain: epoch  0, batch     1 | loss: 8.0750513Losses:  5.992308616638184 0.8526147603988647
CurrentTrain: epoch  0, batch     2 | loss: 6.8449235Losses:  7.482665061950684 0.9385441541671753
CurrentTrain: epoch  0, batch     3 | loss: 8.4212093Losses:  6.5710296630859375 1.2969558238983154
CurrentTrain: epoch  0, batch     4 | loss: 7.8679857Losses:  6.90067195892334 0.9053726196289062
CurrentTrain: epoch  0, batch     5 | loss: 7.8060446Losses:  3.8082616329193115 2.9802322387695312e-08
CurrentTrain: epoch  0, batch     6 | loss: 3.8082616Losses:  3.9873993396759033 0.7249630689620972
CurrentTrain: epoch  1, batch     0 | loss: 4.7123623Losses:  5.156846046447754 0.9007425308227539
CurrentTrain: epoch  1, batch     1 | loss: 6.0575886Losses:  5.422591686248779 1.0354580879211426
CurrentTrain: epoch  1, batch     2 | loss: 6.4580498Losses:  4.011670112609863 0.48142868280410767
CurrentTrain: epoch  1, batch     3 | loss: 4.4930987Losses:  6.204840660095215 0.849570631980896
CurrentTrain: epoch  1, batch     4 | loss: 7.0544114Losses:  4.640777111053467 1.0942028760910034
CurrentTrain: epoch  1, batch     5 | loss: 5.7349801Losses:  5.281103610992432 0.09363381564617157
CurrentTrain: epoch  1, batch     6 | loss: 5.3747373Losses:  4.515020370483398 0.6381648778915405
CurrentTrain: epoch  2, batch     0 | loss: 5.1531854Losses:  4.172177314758301 0.9250044822692871
CurrentTrain: epoch  2, batch     1 | loss: 5.0971818Losses:  4.003460884094238 0.8281700611114502
CurrentTrain: epoch  2, batch     2 | loss: 4.8316307Losses:  3.9506783485412598 0.40569332242012024
CurrentTrain: epoch  2, batch     3 | loss: 4.3563719Losses:  4.44554328918457 0.6163489818572998
CurrentTrain: epoch  2, batch     4 | loss: 5.0618925Losses:  3.4663684368133545 0.5943638682365417
CurrentTrain: epoch  2, batch     5 | loss: 4.0607324Losses:  4.360414981842041 0.07024981081485748
CurrentTrain: epoch  2, batch     6 | loss: 4.4306650Losses:  3.8843061923980713 0.8465059995651245
CurrentTrain: epoch  3, batch     0 | loss: 4.7308121Losses:  3.3107028007507324 0.8316345810890198
CurrentTrain: epoch  3, batch     1 | loss: 4.1423373Losses:  3.2382750511169434 0.4291694760322571
CurrentTrain: epoch  3, batch     2 | loss: 3.6674445Losses:  4.452380180358887 0.7909178137779236
CurrentTrain: epoch  3, batch     3 | loss: 5.2432981Losses:  3.4683878421783447 0.645110011100769
CurrentTrain: epoch  3, batch     4 | loss: 4.1134977Losses:  3.280181407928467 0.7359788417816162
CurrentTrain: epoch  3, batch     5 | loss: 4.0161600Losses:  4.099287033081055 8.94069742685133e-08
CurrentTrain: epoch  3, batch     6 | loss: 4.0992870Losses:  4.233142375946045 0.6120020151138306
CurrentTrain: epoch  4, batch     0 | loss: 4.8451443Losses:  3.0681569576263428 0.6852575540542603
CurrentTrain: epoch  4, batch     1 | loss: 3.7534146Losses:  2.811342239379883 0.7596499919891357
CurrentTrain: epoch  4, batch     2 | loss: 3.5709922Losses:  3.157593250274658 0.6643753051757812
CurrentTrain: epoch  4, batch     3 | loss: 3.8219686Losses:  2.5602073669433594 0.5904586315155029
CurrentTrain: epoch  4, batch     4 | loss: 3.1506660Losses:  2.8715457916259766 0.4470266103744507
CurrentTrain: epoch  4, batch     5 | loss: 3.3185725Losses:  2.0095324516296387 0.0
CurrentTrain: epoch  4, batch     6 | loss: 2.0095325Losses:  2.643920421600342 0.516087532043457
CurrentTrain: epoch  5, batch     0 | loss: 3.1600080Losses:  3.093939781188965 0.5675870776176453
CurrentTrain: epoch  5, batch     1 | loss: 3.6615269Losses:  2.6563029289245605 0.6602120399475098
CurrentTrain: epoch  5, batch     2 | loss: 3.3165150Losses:  2.357067346572876 0.5665899515151978
CurrentTrain: epoch  5, batch     3 | loss: 2.9236574Losses:  2.4603474140167236 0.6462249755859375
CurrentTrain: epoch  5, batch     4 | loss: 3.1065724Losses:  2.966830253601074 0.6498929262161255
CurrentTrain: epoch  5, batch     5 | loss: 3.6167231Losses:  2.631870985031128 0.1262613981962204
CurrentTrain: epoch  5, batch     6 | loss: 2.7581325Losses:  2.2172279357910156 0.5069040060043335
CurrentTrain: epoch  6, batch     0 | loss: 2.7241321Losses:  1.9915879964828491 0.5847908854484558
CurrentTrain: epoch  6, batch     1 | loss: 2.5763788Losses:  2.2990646362304688 0.527228832244873
CurrentTrain: epoch  6, batch     2 | loss: 2.8262935Losses:  2.594165802001953 0.3355560898780823
CurrentTrain: epoch  6, batch     3 | loss: 2.9297218Losses:  2.396111249923706 0.28820565342903137
CurrentTrain: epoch  6, batch     4 | loss: 2.6843169Losses:  2.6924378871917725 0.333129346370697
CurrentTrain: epoch  6, batch     5 | loss: 3.0255673Losses:  1.8952171802520752 0.04027669131755829
CurrentTrain: epoch  6, batch     6 | loss: 1.9354938Losses:  2.204638719558716 0.48632335662841797
CurrentTrain: epoch  7, batch     0 | loss: 2.6909621Losses:  2.1903367042541504 0.4523186981678009
CurrentTrain: epoch  7, batch     1 | loss: 2.6426554Losses:  2.2975375652313232 0.6068568229675293
CurrentTrain: epoch  7, batch     2 | loss: 2.9043944Losses:  1.990034818649292 0.41343826055526733
CurrentTrain: epoch  7, batch     3 | loss: 2.4034731Losses:  2.019355058670044 0.41296643018722534
CurrentTrain: epoch  7, batch     4 | loss: 2.4323215Losses:  2.119230270385742 0.23686379194259644
CurrentTrain: epoch  7, batch     5 | loss: 2.3560941Losses:  1.8205058574676514 0.03298569470643997
CurrentTrain: epoch  7, batch     6 | loss: 1.8534915Losses:  2.1138503551483154 0.5917999744415283
CurrentTrain: epoch  8, batch     0 | loss: 2.7056503Losses:  2.144848346710205 0.43111371994018555
CurrentTrain: epoch  8, batch     1 | loss: 2.5759621Losses:  1.9871630668640137 0.3639393746852875
CurrentTrain: epoch  8, batch     2 | loss: 2.3511024Losses:  1.9034233093261719 0.42681747674942017
CurrentTrain: epoch  8, batch     3 | loss: 2.3302407Losses:  2.0608766078948975 0.37386423349380493
CurrentTrain: epoch  8, batch     4 | loss: 2.4347408Losses:  1.8389613628387451 0.3929218649864197
CurrentTrain: epoch  8, batch     5 | loss: 2.2318833Losses:  1.674077033996582 0.05752452462911606
CurrentTrain: epoch  8, batch     6 | loss: 1.7316016Losses:  2.170949935913086 0.32917872071266174
CurrentTrain: epoch  9, batch     0 | loss: 2.5001287Losses:  2.0707297325134277 0.3258610963821411
CurrentTrain: epoch  9, batch     1 | loss: 2.3965907Losses:  1.9161872863769531 0.3733813464641571
CurrentTrain: epoch  9, batch     2 | loss: 2.2895687Losses:  1.9576265811920166 0.3939584493637085
CurrentTrain: epoch  9, batch     3 | loss: 2.3515849Losses:  2.003279685974121 0.4659646153450012
CurrentTrain: epoch  9, batch     4 | loss: 2.4692442Losses:  1.8264529705047607 0.3872750401496887
CurrentTrain: epoch  9, batch     5 | loss: 2.2137280Losses:  1.744020700454712 0.05832623690366745
CurrentTrain: epoch  9, batch     6 | loss: 1.8023469
Losses:  0.3616403639316559 0.6404978036880493
MemoryTrain:  epoch  0, batch     0 | loss: 1.0021381Losses:  1.532091498374939 0.6998260021209717
MemoryTrain:  epoch  0, batch     1 | loss: 2.2319174Losses:  0.2259598672389984 0.5286361575126648
MemoryTrain:  epoch  0, batch     2 | loss: 0.7545960Losses:  0.775175154209137 0.3051469922065735
MemoryTrain:  epoch  0, batch     3 | loss: 1.0803221Losses:  0.27369359135627747 0.5388743877410889
MemoryTrain:  epoch  0, batch     4 | loss: 0.8125679Losses:  0.3554155230522156 0.6128189563751221
MemoryTrain:  epoch  0, batch     5 | loss: 0.9682345Losses:  1.0341490507125854 0.3829886317253113
MemoryTrain:  epoch  0, batch     6 | loss: 1.4171376Losses:  1.307356357574463 0.42465120553970337
MemoryTrain:  epoch  0, batch     7 | loss: 1.7320075Losses:  0.956098735332489 0.22522227466106415
MemoryTrain:  epoch  0, batch     8 | loss: 1.1813210Losses:  2.1525402069091797 0.4562443792819977
MemoryTrain:  epoch  1, batch     0 | loss: 2.6087847Losses:  1.240462064743042 0.6091368198394775
MemoryTrain:  epoch  1, batch     1 | loss: 1.8495989Losses:  1.2513096332550049 0.5320940613746643
MemoryTrain:  epoch  1, batch     2 | loss: 1.7834036Losses:  0.36595064401626587 0.5261011123657227
MemoryTrain:  epoch  1, batch     3 | loss: 0.8920518Losses:  0.19190596044063568 0.36457228660583496
MemoryTrain:  epoch  1, batch     4 | loss: 0.5564783Losses:  0.5359362363815308 0.5152048468589783
MemoryTrain:  epoch  1, batch     5 | loss: 1.0511410Losses:  0.15178027749061584 0.4658760726451874
MemoryTrain:  epoch  1, batch     6 | loss: 0.6176564Losses:  0.3095780909061432 0.3760150372982025
MemoryTrain:  epoch  1, batch     7 | loss: 0.6855931Losses:  0.7822794914245605 0.2582723796367645
MemoryTrain:  epoch  1, batch     8 | loss: 1.0405519Losses:  0.9105637669563293 0.384518027305603
MemoryTrain:  epoch  2, batch     0 | loss: 1.2950819Losses:  0.6098819375038147 0.408885657787323
MemoryTrain:  epoch  2, batch     1 | loss: 1.0187676Losses:  0.13493141531944275 0.4077203869819641
MemoryTrain:  epoch  2, batch     2 | loss: 0.5426518Losses:  0.09729579836130142 0.4689157009124756
MemoryTrain:  epoch  2, batch     3 | loss: 0.5662115Losses:  0.8060994148254395 0.5310026407241821
MemoryTrain:  epoch  2, batch     4 | loss: 1.3371021Losses:  0.9787760972976685 0.5063347220420837
MemoryTrain:  epoch  2, batch     5 | loss: 1.4851108Losses:  0.2786213159561157 0.569298267364502
MemoryTrain:  epoch  2, batch     6 | loss: 0.8479196Losses:  0.06472928076982498 0.3892925977706909
MemoryTrain:  epoch  2, batch     7 | loss: 0.4540219Losses:  0.21698516607284546 0.22377602756023407
MemoryTrain:  epoch  2, batch     8 | loss: 0.4407612Losses:  0.5942285060882568 0.7164332270622253
MemoryTrain:  epoch  3, batch     0 | loss: 1.3106618Losses:  0.10556961596012115 0.4738236665725708
MemoryTrain:  epoch  3, batch     1 | loss: 0.5793933Losses:  0.09722060710191727 0.38354218006134033
MemoryTrain:  epoch  3, batch     2 | loss: 0.4807628Losses:  0.057393185794353485 0.3633706569671631
MemoryTrain:  epoch  3, batch     3 | loss: 0.4207639Losses:  0.1264151781797409 0.7894779443740845
MemoryTrain:  epoch  3, batch     4 | loss: 0.9158931Losses:  0.043772608041763306 0.28594785928726196
MemoryTrain:  epoch  3, batch     5 | loss: 0.3297205Losses:  0.1017586886882782 0.38741111755371094
MemoryTrain:  epoch  3, batch     6 | loss: 0.4891698Losses:  0.45738059282302856 0.35633522272109985
MemoryTrain:  epoch  3, batch     7 | loss: 0.8137158Losses:  0.22658972442150116 0.33871909976005554
MemoryTrain:  epoch  3, batch     8 | loss: 0.5653088Losses:  0.04560711234807968 0.33641737699508667
MemoryTrain:  epoch  4, batch     0 | loss: 0.3820245Losses:  0.08216612040996552 0.4427329897880554
MemoryTrain:  epoch  4, batch     1 | loss: 0.5248991Losses:  0.11949753761291504 0.41300904750823975
MemoryTrain:  epoch  4, batch     2 | loss: 0.5325066Losses:  0.049366772174835205 0.5484305620193481
MemoryTrain:  epoch  4, batch     3 | loss: 0.5977973Losses:  0.04164949804544449 0.41003838181495667
MemoryTrain:  epoch  4, batch     4 | loss: 0.4516879Losses:  0.33120793104171753 0.4099099636077881
MemoryTrain:  epoch  4, batch     5 | loss: 0.7411179Losses:  0.18226148188114166 0.5934965014457703
MemoryTrain:  epoch  4, batch     6 | loss: 0.7757580Losses:  0.18988540768623352 0.4446862041950226
MemoryTrain:  epoch  4, batch     7 | loss: 0.6345716Losses:  0.11037236452102661 0.439873069524765
MemoryTrain:  epoch  4, batch     8 | loss: 0.5502454Losses:  0.10642869025468826 0.4404183626174927
MemoryTrain:  epoch  5, batch     0 | loss: 0.5468470Losses:  0.09678376466035843 0.48359984159469604
MemoryTrain:  epoch  5, batch     1 | loss: 0.5803836Losses:  0.04989742115139961 0.5780212879180908
MemoryTrain:  epoch  5, batch     2 | loss: 0.6279187Losses:  0.1172238290309906 0.4117851257324219
MemoryTrain:  epoch  5, batch     3 | loss: 0.5290090Losses:  0.028732536360621452 0.3275600075721741
MemoryTrain:  epoch  5, batch     4 | loss: 0.3562925Losses:  0.5649585723876953 0.48399618268013
MemoryTrain:  epoch  5, batch     5 | loss: 1.0489547Losses:  0.07357633113861084 0.4521597623825073
MemoryTrain:  epoch  5, batch     6 | loss: 0.5257361Losses:  0.12429477274417877 0.3662220537662506
MemoryTrain:  epoch  5, batch     7 | loss: 0.4905168Losses:  0.11778607219457626 0.2730625569820404
MemoryTrain:  epoch  5, batch     8 | loss: 0.3908486Losses:  0.04530007392168045 0.3563864827156067
MemoryTrain:  epoch  6, batch     0 | loss: 0.4016865Losses:  0.06831710040569305 0.2838441729545593
MemoryTrain:  epoch  6, batch     1 | loss: 0.3521613Losses:  0.042726390063762665 0.4077569544315338
MemoryTrain:  epoch  6, batch     2 | loss: 0.4504834Losses:  0.07116875052452087 0.6117856502532959
MemoryTrain:  epoch  6, batch     3 | loss: 0.6829544Losses:  0.10814415663480759 0.42599380016326904
MemoryTrain:  epoch  6, batch     4 | loss: 0.5341380Losses:  0.12103387713432312 0.30940428376197815
MemoryTrain:  epoch  6, batch     5 | loss: 0.4304382Losses:  0.045520372688770294 0.3424171507358551
MemoryTrain:  epoch  6, batch     6 | loss: 0.3879375Losses:  0.07258050888776779 0.4016062021255493
MemoryTrain:  epoch  6, batch     7 | loss: 0.4741867Losses:  0.040294162929058075 0.3332022428512573
MemoryTrain:  epoch  6, batch     8 | loss: 0.3734964Losses:  0.032334890216588974 0.4248150885105133
MemoryTrain:  epoch  7, batch     0 | loss: 0.4571500Losses:  0.08181318640708923 0.554751992225647
MemoryTrain:  epoch  7, batch     1 | loss: 0.6365652Losses:  0.1067478135228157 0.4283400774002075
MemoryTrain:  epoch  7, batch     2 | loss: 0.5350879Losses:  0.04789675027132034 0.4616624116897583
MemoryTrain:  epoch  7, batch     3 | loss: 0.5095592Losses:  0.17023447155952454 0.4076703190803528
MemoryTrain:  epoch  7, batch     4 | loss: 0.5779048Losses:  0.024165641516447067 0.4077749252319336
MemoryTrain:  epoch  7, batch     5 | loss: 0.4319406Losses:  0.02609628438949585 0.24727971851825714
MemoryTrain:  epoch  7, batch     6 | loss: 0.2733760Losses:  0.04229506105184555 0.47874757647514343
MemoryTrain:  epoch  7, batch     7 | loss: 0.5210426Losses:  0.08978849649429321 0.26384299993515015
MemoryTrain:  epoch  7, batch     8 | loss: 0.3536315Losses:  0.048801448196172714 0.3156994879245758
MemoryTrain:  epoch  8, batch     0 | loss: 0.3645009Losses:  0.02531701698899269 0.2876129150390625
MemoryTrain:  epoch  8, batch     1 | loss: 0.3129299Losses:  0.09931935369968414 0.41950130462646484
MemoryTrain:  epoch  8, batch     2 | loss: 0.5188206Losses:  0.06348728388547897 0.39516472816467285
MemoryTrain:  epoch  8, batch     3 | loss: 0.4586520Losses:  0.1691492646932602 0.49078088998794556
MemoryTrain:  epoch  8, batch     4 | loss: 0.6599302Losses:  0.11044533550739288 0.3829158544540405
MemoryTrain:  epoch  8, batch     5 | loss: 0.4933612Losses:  0.03201994672417641 0.3781132102012634
MemoryTrain:  epoch  8, batch     6 | loss: 0.4101332Losses:  0.03658619895577431 0.4719442129135132
MemoryTrain:  epoch  8, batch     7 | loss: 0.5085304Losses:  0.03606131672859192 0.1761748492717743
MemoryTrain:  epoch  8, batch     8 | loss: 0.2122362Losses:  0.05173907056450844 0.4380161166191101
MemoryTrain:  epoch  9, batch     0 | loss: 0.4897552Losses:  0.04542012885212898 0.3196141719818115
MemoryTrain:  epoch  9, batch     1 | loss: 0.3650343Losses:  0.0436100997030735 0.273417592048645
MemoryTrain:  epoch  9, batch     2 | loss: 0.3170277Losses:  0.05169883742928505 0.3683050572872162
MemoryTrain:  epoch  9, batch     3 | loss: 0.4200039Losses:  0.07636677473783493 0.5376649498939514
MemoryTrain:  epoch  9, batch     4 | loss: 0.6140317Losses:  0.06062893196940422 0.50218266248703
MemoryTrain:  epoch  9, batch     5 | loss: 0.5628116Losses:  0.03489188477396965 0.3430263102054596
MemoryTrain:  epoch  9, batch     6 | loss: 0.3779182Losses:  0.08119324594736099 0.3459406793117523
MemoryTrain:  epoch  9, batch     7 | loss: 0.4271339Losses:  0.034031741321086884 0.2286367118358612
MemoryTrain:  epoch  9, batch     8 | loss: 0.2626685
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 72.14%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 70.93%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 85.33%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 85.46%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 84.84%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 84.20%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 83.71%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.89%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.78%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 81.86%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.09%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 81.41%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 80.54%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 80.31%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 79.86%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 79.50%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 78.77%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 76.74%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 76.01%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 77.51%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 77.49%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 77.46%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 77.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.36%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.31%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 78.95%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 78.70%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 78.45%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 78.15%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 77.91%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 77.88%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 77.31%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 76.76%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 76.16%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.67%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 75.14%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 75.63%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 75.74%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 75.34%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 74.87%   [EVAL] batch:  146 | acc: 0.00%,  total acc: 74.36%   [EVAL] batch:  147 | acc: 12.50%,  total acc: 73.94%   [EVAL] batch:  148 | acc: 0.00%,  total acc: 73.45%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 73.00%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.56%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.08%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.61%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 71.14%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 70.73%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.27%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  169 | acc: 0.00%,  total acc: 71.18%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.42%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 70.01%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 69.61%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 69.21%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.03%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 69.07%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 69.10%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 69.02%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 68.92%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 69.07%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 68.81%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 68.93%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 69.74%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 69.56%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  218 | acc: 37.50%,  total acc: 69.09%   [EVAL] batch:  219 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  220 | acc: 18.75%,  total acc: 68.72%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 68.44%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 68.19%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 67.64%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 67.76%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 67.93%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 67.89%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 67.47%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 67.37%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 68.68%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 68.77%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 68.63%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 68.52%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 68.45%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 69.82%   [EVAL] batch:  292 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  294 | acc: 18.75%,  total acc: 69.66%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 69.47%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 69.30%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 68.98%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 69.26%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 69.07%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 68.69%   [EVAL] batch:  310 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 68.31%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 68.17%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 68.11%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.98%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.15%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  325 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 67.81%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  328 | acc: 6.25%,  total acc: 67.44%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 67.31%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 67.15%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 68.78%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 68.78%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 68.73%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 68.61%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 68.55%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 68.50%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 68.52%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  387 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 69.10%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 69.05%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  398 | acc: 43.75%,  total acc: 68.83%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 68.78%   [EVAL] batch:  400 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  401 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  402 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  403 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  404 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 68.81%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 68.41%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 68.29%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 68.18%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 68.56%   
cur_acc:  ['0.9484', '0.8264', '0.6508', '0.7946', '0.7937', '0.7202', '0.6895']
his_acc:  ['0.9484', '0.8805', '0.7882', '0.7572', '0.7340', '0.7087', '0.6856']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 37 29  0  0 28 32 23  0  0 35 34  0  0  1
  1 19  0  0  0  0  5 22  0  0  0  2  0 18 27 20 26  0 10  0  9 25 36  0
  0  0  2 16  0  0 17  0  0  0 13 11  0  0  0 12  0 31  0 30 15 14  7  4
  1  0  0  8  0  6  0  3]
Losses:  6.92884635925293 1.114376187324524
CurrentTrain: epoch  0, batch     0 | loss: 8.0432224Losses:  6.707798004150391 1.0169751644134521
CurrentTrain: epoch  0, batch     1 | loss: 7.7247734Losses:  5.070059776306152 1.1013116836547852
CurrentTrain: epoch  0, batch     2 | loss: 6.1713715Losses:  6.33396577835083 1.0631299018859863
CurrentTrain: epoch  0, batch     3 | loss: 7.3970957Losses:  6.602887153625488 0.9713131785392761
CurrentTrain: epoch  0, batch     4 | loss: 7.5742002Losses:  6.943896293640137 1.3638408184051514
CurrentTrain: epoch  0, batch     5 | loss: 8.3077374Losses:  4.650740146636963 0.30975857377052307
CurrentTrain: epoch  0, batch     6 | loss: 4.9604988Losses:  5.894087791442871 0.7995215654373169
CurrentTrain: epoch  1, batch     0 | loss: 6.6936092Losses:  5.0944929122924805 0.9340638518333435
CurrentTrain: epoch  1, batch     1 | loss: 6.0285568Losses:  5.1252593994140625 0.8192510008811951
CurrentTrain: epoch  1, batch     2 | loss: 5.9445105Losses:  6.627645015716553 1.008853554725647
CurrentTrain: epoch  1, batch     3 | loss: 7.6364985Losses:  4.429340839385986 0.9286823272705078
CurrentTrain: epoch  1, batch     4 | loss: 5.3580232Losses:  4.436600685119629 0.8047472834587097
CurrentTrain: epoch  1, batch     5 | loss: 5.2413478Losses:  5.553367614746094 0.6053869724273682
CurrentTrain: epoch  1, batch     6 | loss: 6.1587543Losses:  3.437164306640625 0.607906699180603
CurrentTrain: epoch  2, batch     0 | loss: 4.0450711Losses:  5.837795257568359 1.0973546504974365
CurrentTrain: epoch  2, batch     1 | loss: 6.9351501Losses:  4.304126739501953 1.0075654983520508
CurrentTrain: epoch  2, batch     2 | loss: 5.3116922Losses:  4.140445709228516 0.7921019792556763
CurrentTrain: epoch  2, batch     3 | loss: 4.9325476Losses:  5.073566436767578 0.8120646476745605
CurrentTrain: epoch  2, batch     4 | loss: 5.8856311Losses:  4.740289688110352 0.7202863693237305
CurrentTrain: epoch  2, batch     5 | loss: 5.4605761Losses:  2.4843387603759766 0.1683211624622345
CurrentTrain: epoch  2, batch     6 | loss: 2.6526599Losses:  4.804298400878906 0.6919269561767578
CurrentTrain: epoch  3, batch     0 | loss: 5.4962254Losses:  5.378708839416504 1.0358136892318726
CurrentTrain: epoch  3, batch     1 | loss: 6.4145226Losses:  2.9584641456604004 0.6071376800537109
CurrentTrain: epoch  3, batch     2 | loss: 3.5656018Losses:  4.442707061767578 0.8931366205215454
CurrentTrain: epoch  3, batch     3 | loss: 5.3358436Losses:  3.509935140609741 0.7510615587234497
CurrentTrain: epoch  3, batch     4 | loss: 4.2609968Losses:  3.965982675552368 0.5802711248397827
CurrentTrain: epoch  3, batch     5 | loss: 4.5462537Losses:  1.9356191158294678 0.08678276836872101
CurrentTrain: epoch  3, batch     6 | loss: 2.0224018Losses:  4.015501976013184 0.554710865020752
CurrentTrain: epoch  4, batch     0 | loss: 4.5702128Losses:  3.7224173545837402 0.7583565711975098
CurrentTrain: epoch  4, batch     1 | loss: 4.4807739Losses:  4.682382583618164 0.8631382584571838
CurrentTrain: epoch  4, batch     2 | loss: 5.5455208Losses:  3.1522533893585205 0.6825023293495178
CurrentTrain: epoch  4, batch     3 | loss: 3.8347557Losses:  2.615692377090454 0.43355900049209595
CurrentTrain: epoch  4, batch     4 | loss: 3.0492513Losses:  3.6212689876556396 0.6603403091430664
CurrentTrain: epoch  4, batch     5 | loss: 4.2816095Losses:  3.9579150676727295 0.11892753839492798
CurrentTrain: epoch  4, batch     6 | loss: 4.0768428Losses:  3.3468985557556152 0.5429657697677612
CurrentTrain: epoch  5, batch     0 | loss: 3.8898644Losses:  3.4104044437408447 0.8183894157409668
CurrentTrain: epoch  5, batch     1 | loss: 4.2287941Losses:  4.258049964904785 0.5558663606643677
CurrentTrain: epoch  5, batch     2 | loss: 4.8139162Losses:  2.9037532806396484 0.5836841464042664
CurrentTrain: epoch  5, batch     3 | loss: 3.4874375Losses:  3.446143865585327 0.5317261219024658
CurrentTrain: epoch  5, batch     4 | loss: 3.9778700Losses:  3.854825019836426 0.5222100019454956
CurrentTrain: epoch  5, batch     5 | loss: 4.3770351Losses:  3.680375576019287 0.11223077028989792
CurrentTrain: epoch  5, batch     6 | loss: 3.7926064Losses:  3.5381274223327637 0.46376633644104004
CurrentTrain: epoch  6, batch     0 | loss: 4.0018940Losses:  3.264183521270752 0.7271466255187988
CurrentTrain: epoch  6, batch     1 | loss: 3.9913301Losses:  4.19222354888916 0.920436441898346
CurrentTrain: epoch  6, batch     2 | loss: 5.1126599Losses:  3.004669427871704 0.5820943713188171
CurrentTrain: epoch  6, batch     3 | loss: 3.5867639Losses:  2.688343048095703 0.4736067056655884
CurrentTrain: epoch  6, batch     4 | loss: 3.1619496Losses:  2.6193037033081055 0.5879507064819336
CurrentTrain: epoch  6, batch     5 | loss: 3.2072544Losses:  3.212749481201172 0.25202515721321106
CurrentTrain: epoch  6, batch     6 | loss: 3.4647746Losses:  3.4318792819976807 0.7295973896980286
CurrentTrain: epoch  7, batch     0 | loss: 4.1614766Losses:  2.334617853164673 0.4543819725513458
CurrentTrain: epoch  7, batch     1 | loss: 2.7889998Losses:  3.436291217803955 0.7644959688186646
CurrentTrain: epoch  7, batch     2 | loss: 4.2007871Losses:  3.2256762981414795 0.72871994972229
CurrentTrain: epoch  7, batch     3 | loss: 3.9543962Losses:  2.8838162422180176 0.7143251895904541
CurrentTrain: epoch  7, batch     4 | loss: 3.5981414Losses:  2.861717462539673 0.6687527298927307
CurrentTrain: epoch  7, batch     5 | loss: 3.5304701Losses:  1.8178715705871582 2.9802322387695312e-08
CurrentTrain: epoch  7, batch     6 | loss: 1.8178716Losses:  2.4121482372283936 0.45779654383659363
CurrentTrain: epoch  8, batch     0 | loss: 2.8699448Losses:  2.3567941188812256 0.4385797679424286
CurrentTrain: epoch  8, batch     1 | loss: 2.7953739Losses:  4.253732204437256 0.5739494562149048
CurrentTrain: epoch  8, batch     2 | loss: 4.8276815Losses:  2.851940631866455 0.680608868598938
CurrentTrain: epoch  8, batch     3 | loss: 3.5325494Losses:  2.6087026596069336 0.49022066593170166
CurrentTrain: epoch  8, batch     4 | loss: 3.0989232Losses:  2.6530797481536865 0.36947929859161377
CurrentTrain: epoch  8, batch     5 | loss: 3.0225592Losses:  1.7499415874481201 0.05637167766690254
CurrentTrain: epoch  8, batch     6 | loss: 1.8063133Losses:  2.4233174324035645 0.5406367182731628
CurrentTrain: epoch  9, batch     0 | loss: 2.9639542Losses:  2.3245689868927 0.42015278339385986
CurrentTrain: epoch  9, batch     1 | loss: 2.7447219Losses:  2.0010180473327637 0.3197364807128906
CurrentTrain: epoch  9, batch     2 | loss: 2.3207545Losses:  2.5121052265167236 0.4017643332481384
CurrentTrain: epoch  9, batch     3 | loss: 2.9138696Losses:  2.787485361099243 0.5869570970535278
CurrentTrain: epoch  9, batch     4 | loss: 3.3744426Losses:  3.7985219955444336 0.7226905822753906
CurrentTrain: epoch  9, batch     5 | loss: 4.5212126Losses:  1.7344270944595337 0.0
CurrentTrain: epoch  9, batch     6 | loss: 1.7344271
Losses:  0.8682658076286316 0.48104003071784973
MemoryTrain:  epoch  0, batch     0 | loss: 1.3493059Losses:  0.5574901700019836 0.5561573505401611
MemoryTrain:  epoch  0, batch     1 | loss: 1.1136475Losses:  0.6009688377380371 0.511967658996582
MemoryTrain:  epoch  0, batch     2 | loss: 1.1129365Losses:  2.2124342918395996 0.6324396729469299
MemoryTrain:  epoch  0, batch     3 | loss: 2.8448739Losses:  0.42209112644195557 0.3311951756477356
MemoryTrain:  epoch  0, batch     4 | loss: 0.7532863Losses:  0.35129836201667786 0.3303132653236389
MemoryTrain:  epoch  0, batch     5 | loss: 0.6816117Losses:  0.016306132078170776 0.3830462694168091
MemoryTrain:  epoch  0, batch     6 | loss: 0.3993524Losses:  0.1765894591808319 0.7252517342567444
MemoryTrain:  epoch  0, batch     7 | loss: 0.9018412Losses:  0.5252077579498291 0.5551540851593018
MemoryTrain:  epoch  0, batch     8 | loss: 1.0803618Losses:  0.25887903571128845 0.31806495785713196
MemoryTrain:  epoch  0, batch     9 | loss: 0.5769440Losses:  0.6473903059959412 0.35492080450057983
MemoryTrain:  epoch  1, batch     0 | loss: 1.0023111Losses:  0.15932539105415344 0.4589774012565613
MemoryTrain:  epoch  1, batch     1 | loss: 0.6183028Losses:  1.667211890220642 0.5496094822883606
MemoryTrain:  epoch  1, batch     2 | loss: 2.2168214Losses:  0.12354488670825958 0.2919809818267822
MemoryTrain:  epoch  1, batch     3 | loss: 0.4155259Losses:  0.4407501220703125 0.6527214050292969
MemoryTrain:  epoch  1, batch     4 | loss: 1.0934715Losses:  0.11417801678180695 0.2661898136138916
MemoryTrain:  epoch  1, batch     5 | loss: 0.3803678Losses:  0.48606836795806885 0.49091917276382446
MemoryTrain:  epoch  1, batch     6 | loss: 0.9769875Losses:  0.3878917992115021 0.6469929218292236
MemoryTrain:  epoch  1, batch     7 | loss: 1.0348847Losses:  1.1348166465759277 0.4541375935077667
MemoryTrain:  epoch  1, batch     8 | loss: 1.5889542Losses:  0.7255969047546387 0.42565152049064636
MemoryTrain:  epoch  1, batch     9 | loss: 1.1512485Losses:  0.805898904800415 0.31113043427467346
MemoryTrain:  epoch  2, batch     0 | loss: 1.1170293Losses:  0.14288249611854553 0.3796302080154419
MemoryTrain:  epoch  2, batch     1 | loss: 0.5225127Losses:  0.09510962665081024 0.3191106617450714
MemoryTrain:  epoch  2, batch     2 | loss: 0.4142203Losses:  0.18592652678489685 0.4356367588043213
MemoryTrain:  epoch  2, batch     3 | loss: 0.6215633Losses:  0.264818012714386 0.3677081763744354
MemoryTrain:  epoch  2, batch     4 | loss: 0.6325262Losses:  0.25914931297302246 0.3294547498226166
MemoryTrain:  epoch  2, batch     5 | loss: 0.5886041Losses:  0.1420685052871704 0.52584308385849
MemoryTrain:  epoch  2, batch     6 | loss: 0.6679116Losses:  0.2600516080856323 0.5989722013473511
MemoryTrain:  epoch  2, batch     7 | loss: 0.8590238Losses:  0.728183388710022 0.48857519030570984
MemoryTrain:  epoch  2, batch     8 | loss: 1.2167586Losses:  0.6051012873649597 0.3811362385749817
MemoryTrain:  epoch  2, batch     9 | loss: 0.9862375Losses:  0.06123744323849678 0.34440290927886963
MemoryTrain:  epoch  3, batch     0 | loss: 0.4056404Losses:  0.14873620867729187 0.3639141917228699
MemoryTrain:  epoch  3, batch     1 | loss: 0.5126504Losses:  0.07918832451105118 0.3544073700904846
MemoryTrain:  epoch  3, batch     2 | loss: 0.4335957Losses:  0.16338327527046204 0.5559044480323792
MemoryTrain:  epoch  3, batch     3 | loss: 0.7192878Losses:  0.18577376008033752 0.34161174297332764
MemoryTrain:  epoch  3, batch     4 | loss: 0.5273855Losses:  0.16221380233764648 0.6925548315048218
MemoryTrain:  epoch  3, batch     5 | loss: 0.8547686Losses:  0.2318771779537201 0.41158849000930786
MemoryTrain:  epoch  3, batch     6 | loss: 0.6434656Losses:  0.11900593340396881 0.3524375259876251
MemoryTrain:  epoch  3, batch     7 | loss: 0.4714435Losses:  0.09455043077468872 0.2595495283603668
MemoryTrain:  epoch  3, batch     8 | loss: 0.3541000Losses:  0.16615751385688782 0.6187986731529236
MemoryTrain:  epoch  3, batch     9 | loss: 0.7849562Losses:  0.07270564138889313 0.4674423336982727
MemoryTrain:  epoch  4, batch     0 | loss: 0.5401480Losses:  0.05187734588980675 0.35196027159690857
MemoryTrain:  epoch  4, batch     1 | loss: 0.4038376Losses:  0.08028784394264221 0.4421049654483795
MemoryTrain:  epoch  4, batch     2 | loss: 0.5223928Losses:  0.06649179756641388 0.35399293899536133
MemoryTrain:  epoch  4, batch     3 | loss: 0.4204847Losses:  0.11911187320947647 0.4722077548503876
MemoryTrain:  epoch  4, batch     4 | loss: 0.5913196Losses:  0.19868676364421844 0.4071410298347473
MemoryTrain:  epoch  4, batch     5 | loss: 0.6058278Losses:  0.0819573625922203 0.5235916376113892
MemoryTrain:  epoch  4, batch     6 | loss: 0.6055490Losses:  0.16143764555454254 0.4014509916305542
MemoryTrain:  epoch  4, batch     7 | loss: 0.5628886Losses:  0.04393790662288666 0.3641777038574219
MemoryTrain:  epoch  4, batch     8 | loss: 0.4081156Losses:  0.04016564041376114 0.405830055475235
MemoryTrain:  epoch  4, batch     9 | loss: 0.4459957Losses:  0.04985436797142029 0.29093635082244873
MemoryTrain:  epoch  5, batch     0 | loss: 0.3407907Losses:  0.05054197460412979 0.5710758566856384
MemoryTrain:  epoch  5, batch     1 | loss: 0.6216179Losses:  0.04554738104343414 0.43523380160331726
MemoryTrain:  epoch  5, batch     2 | loss: 0.4807812Losses:  0.20035943388938904 0.5960236191749573
MemoryTrain:  epoch  5, batch     3 | loss: 0.7963830Losses:  0.059433840215206146 0.4038160741329193
MemoryTrain:  epoch  5, batch     4 | loss: 0.4632499Losses:  0.061178646981716156 0.3860971927642822
MemoryTrain:  epoch  5, batch     5 | loss: 0.4472758Losses:  0.1311202198266983 0.6723555326461792
MemoryTrain:  epoch  5, batch     6 | loss: 0.8034757Losses:  0.07220907509326935 0.4347708821296692
MemoryTrain:  epoch  5, batch     7 | loss: 0.5069799Losses:  0.03328938037157059 0.3059893846511841
MemoryTrain:  epoch  5, batch     8 | loss: 0.3392788Losses:  0.03479037061333656 0.3294948935508728
MemoryTrain:  epoch  5, batch     9 | loss: 0.3642853Losses:  0.08339207619428635 0.3147508203983307
MemoryTrain:  epoch  6, batch     0 | loss: 0.3981429Losses:  0.05072501674294472 0.44173315167427063
MemoryTrain:  epoch  6, batch     1 | loss: 0.4924582Losses:  0.041785433888435364 0.31802821159362793
MemoryTrain:  epoch  6, batch     2 | loss: 0.3598136Losses:  0.06435409188270569 0.36622393131256104
MemoryTrain:  epoch  6, batch     3 | loss: 0.4305780Losses:  0.041468869894742966 0.36189740896224976
MemoryTrain:  epoch  6, batch     4 | loss: 0.4033663Losses:  0.03093506209552288 0.3426892161369324
MemoryTrain:  epoch  6, batch     5 | loss: 0.3736243Losses:  0.1778242439031601 0.5717973709106445
MemoryTrain:  epoch  6, batch     6 | loss: 0.7496216Losses:  0.06972790509462357 0.3609258532524109
MemoryTrain:  epoch  6, batch     7 | loss: 0.4306538Losses:  0.058051303029060364 0.41163015365600586
MemoryTrain:  epoch  6, batch     8 | loss: 0.4696814Losses:  0.05274277180433273 0.40435653924942017
MemoryTrain:  epoch  6, batch     9 | loss: 0.4570993Losses:  0.06795152276754379 0.3265596628189087
MemoryTrain:  epoch  7, batch     0 | loss: 0.3945112Losses:  0.0575108677148819 0.33567649126052856
MemoryTrain:  epoch  7, batch     1 | loss: 0.3931873Losses:  0.08513107895851135 0.4734674394130707
MemoryTrain:  epoch  7, batch     2 | loss: 0.5585985Losses:  0.06284812837839127 0.3190714716911316
MemoryTrain:  epoch  7, batch     3 | loss: 0.3819196Losses:  0.0732581615447998 0.32491379976272583
MemoryTrain:  epoch  7, batch     4 | loss: 0.3981720Losses:  0.038607463240623474 0.4113371670246124
MemoryTrain:  epoch  7, batch     5 | loss: 0.4499446Losses:  0.06864937394857407 0.28561437129974365
MemoryTrain:  epoch  7, batch     6 | loss: 0.3542638Losses:  0.0738707035779953 0.3793081045150757
MemoryTrain:  epoch  7, batch     7 | loss: 0.4531788Losses:  0.04513805732131004 0.35868048667907715
MemoryTrain:  epoch  7, batch     8 | loss: 0.4038185Losses:  0.05582529306411743 0.5486047863960266
MemoryTrain:  epoch  7, batch     9 | loss: 0.6044301Losses:  0.026056746020913124 0.2023065835237503
MemoryTrain:  epoch  8, batch     0 | loss: 0.2283633Losses:  0.05436687171459198 0.36591970920562744
MemoryTrain:  epoch  8, batch     1 | loss: 0.4202866Losses:  0.05763915181159973 0.2772761285305023
MemoryTrain:  epoch  8, batch     2 | loss: 0.3349153Losses:  0.0316942036151886 0.34847456216812134
MemoryTrain:  epoch  8, batch     3 | loss: 0.3801688Losses:  0.050367336720228195 0.4498457908630371
MemoryTrain:  epoch  8, batch     4 | loss: 0.5002131Losses:  0.06351288408041 0.43834489583969116
MemoryTrain:  epoch  8, batch     5 | loss: 0.5018578Losses:  0.03267762437462807 0.35769110918045044
MemoryTrain:  epoch  8, batch     6 | loss: 0.3903687Losses:  0.10436443239450455 0.3891947269439697
MemoryTrain:  epoch  8, batch     7 | loss: 0.4935592Losses:  0.029007557779550552 0.33729711174964905
MemoryTrain:  epoch  8, batch     8 | loss: 0.3663047Losses:  0.07957682013511658 0.5149081945419312
MemoryTrain:  epoch  8, batch     9 | loss: 0.5944850Losses:  0.03650695085525513 0.2928529679775238
MemoryTrain:  epoch  9, batch     0 | loss: 0.3293599Losses:  0.2139188051223755 0.36901217699050903
MemoryTrain:  epoch  9, batch     1 | loss: 0.5829310Losses:  0.03369523957371712 0.5197033882141113
MemoryTrain:  epoch  9, batch     2 | loss: 0.5533986Losses:  0.04559861868619919 0.33960431814193726
MemoryTrain:  epoch  9, batch     3 | loss: 0.3852029Losses:  0.01884441077709198 0.2251751869916916
MemoryTrain:  epoch  9, batch     4 | loss: 0.2440196Losses:  0.031367287039756775 0.3152024745941162
MemoryTrain:  epoch  9, batch     5 | loss: 0.3465698Losses:  0.05521891266107559 0.3765024244785309
MemoryTrain:  epoch  9, batch     6 | loss: 0.4317213Losses:  0.07161866128444672 0.26086604595184326
MemoryTrain:  epoch  9, batch     7 | loss: 0.3324847Losses:  0.028598647564649582 0.2646133303642273
MemoryTrain:  epoch  9, batch     8 | loss: 0.2932120Losses:  0.05670487880706787 0.5530258417129517
MemoryTrain:  epoch  9, batch     9 | loss: 0.6097307
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 60.34%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 55.17%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 53.33%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 51.61%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 53.60%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 54.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 58.11%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 59.84%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 60.21%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.36%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 61.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 63.80%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 64.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 66.07%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 85.03%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 85.34%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 81.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 80.75%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 81.06%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 81.34%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 80.62%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 79.75%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 79.25%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 78.21%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 77.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 77.63%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 76.50%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 76.33%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 76.00%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 75.53%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 74.85%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 74.26%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 73.68%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 72.97%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.20%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 72.09%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 73.88%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 73.92%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 73.93%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 73.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.22%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 75.88%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 75.60%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 75.21%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 75.05%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 75.30%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 74.75%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 74.17%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 73.59%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.03%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 72.47%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 72.93%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 72.71%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 72.64%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.35%   [EVAL] batch:  144 | acc: 6.25%,  total acc: 71.90%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 71.45%   [EVAL] batch:  146 | acc: 0.00%,  total acc: 70.96%   [EVAL] batch:  147 | acc: 12.50%,  total acc: 70.57%   [EVAL] batch:  148 | acc: 0.00%,  total acc: 70.09%   [EVAL] batch:  149 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.21%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 67.86%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 67.46%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.04%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 67.63%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  169 | acc: 0.00%,  total acc: 68.20%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 67.44%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 67.05%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 66.29%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 66.05%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 66.10%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 66.01%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 65.71%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 65.59%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 66.11%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 65.90%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 65.66%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 65.43%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 65.29%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 65.84%   [EVAL] batch:  215 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 65.41%   [EVAL] batch:  219 | acc: 31.25%,  total acc: 65.26%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 65.07%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 64.86%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 64.66%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 64.21%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 64.32%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 64.12%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 63.98%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 65.61%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 65.67%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 65.65%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 65.50%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  272 | acc: 12.50%,  total acc: 65.22%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.08%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  290 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  293 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 66.17%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 65.98%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 65.65%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 65.94%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  308 | acc: 0.00%,  total acc: 65.57%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 65.40%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 65.25%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 64.93%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 64.65%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 64.52%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 64.87%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 64.66%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 64.50%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 64.42%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 64.36%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 64.24%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 64.83%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 65.69%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  354 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  370 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:  372 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  374 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  394 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:  395 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 65.85%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:  400 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  401 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  402 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  403 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  404 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 66.09%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 66.05%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 65.93%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 65.90%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  415 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 65.86%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 65.74%   [EVAL] batch:  421 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 65.48%   [EVAL] batch:  423 | acc: 12.50%,  total acc: 65.36%   [EVAL] batch:  424 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:  432 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  441 | acc: 31.25%,  total acc: 65.72%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  450 | acc: 12.50%,  total acc: 65.87%   [EVAL] batch:  451 | acc: 12.50%,  total acc: 65.75%   [EVAL] batch:  452 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  453 | acc: 25.00%,  total acc: 65.53%   [EVAL] batch:  454 | acc: 12.50%,  total acc: 65.41%   [EVAL] batch:  455 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 65.63%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  464 | acc: 6.25%,  total acc: 65.36%   [EVAL] batch:  465 | acc: 18.75%,  total acc: 65.26%   [EVAL] batch:  466 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:  467 | acc: 0.00%,  total acc: 65.00%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 64.91%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  478 | acc: 56.25%,  total acc: 65.37%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  481 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  482 | acc: 62.50%,  total acc: 65.48%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 65.71%   [EVAL] batch:  494 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  497 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.96%   
cur_acc:  ['0.9484', '0.8264', '0.6508', '0.7946', '0.7937', '0.7202', '0.6895', '0.6607']
his_acc:  ['0.9484', '0.8805', '0.7882', '0.7572', '0.7340', '0.7087', '0.6856', '0.6596']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.906390190124512 1.918205738067627
CurrentTrain: epoch  0, batch     0 | loss: 11.8245964Losses:  10.13147258758545 1.2344272136688232
CurrentTrain: epoch  0, batch     1 | loss: 11.3659000Losses:  10.629199028015137 1.7534624338150024
CurrentTrain: epoch  0, batch     2 | loss: 12.3826618Losses:  9.47661018371582 1.6688048839569092
CurrentTrain: epoch  0, batch     3 | loss: 11.1454153Losses:  8.966544151306152 1.4082833528518677
CurrentTrain: epoch  0, batch     4 | loss: 10.3748274Losses:  9.79304313659668 1.554557204246521
CurrentTrain: epoch  0, batch     5 | loss: 11.3476000Losses:  9.640920639038086 1.19254469871521
CurrentTrain: epoch  0, batch     6 | loss: 10.8334656Losses:  9.228251457214355 1.6391288042068481
CurrentTrain: epoch  0, batch     7 | loss: 10.8673801Losses:  8.50534439086914 1.1615080833435059
CurrentTrain: epoch  0, batch     8 | loss: 9.6668530Losses:  9.964668273925781 1.4352710247039795
CurrentTrain: epoch  0, batch     9 | loss: 11.3999395Losses:  8.131898880004883 1.1988275051116943
CurrentTrain: epoch  0, batch    10 | loss: 9.3307266Losses:  8.89590072631836 1.5492770671844482
CurrentTrain: epoch  0, batch    11 | loss: 10.4451780Losses:  9.27431869506836 2.0496273040771484
CurrentTrain: epoch  0, batch    12 | loss: 11.3239460Losses:  8.381067276000977 1.148295521736145
CurrentTrain: epoch  0, batch    13 | loss: 9.5293627Losses:  8.577627182006836 1.3437789678573608
CurrentTrain: epoch  0, batch    14 | loss: 9.9214058Losses:  9.60499382019043 1.1909188032150269
CurrentTrain: epoch  0, batch    15 | loss: 10.7959127Losses:  7.474540710449219 1.0028104782104492
CurrentTrain: epoch  0, batch    16 | loss: 8.4773512Losses:  8.930224418640137 1.2246708869934082
CurrentTrain: epoch  0, batch    17 | loss: 10.1548958Losses:  8.681519508361816 1.00191330909729
CurrentTrain: epoch  0, batch    18 | loss: 9.6834326Losses:  9.508764266967773 1.5170952081680298
CurrentTrain: epoch  0, batch    19 | loss: 11.0258598Losses:  7.863681793212891 1.2861658334732056
CurrentTrain: epoch  0, batch    20 | loss: 9.1498480Losses:  9.210464477539062 1.2114248275756836
CurrentTrain: epoch  0, batch    21 | loss: 10.4218893Losses:  8.854351043701172 1.150235652923584
CurrentTrain: epoch  0, batch    22 | loss: 10.0045872Losses:  8.790180206298828 1.3527615070343018
CurrentTrain: epoch  0, batch    23 | loss: 10.1429415Losses:  7.8511457443237305 0.9892570972442627
CurrentTrain: epoch  0, batch    24 | loss: 8.8404026Losses:  8.666498184204102 1.050142526626587
CurrentTrain: epoch  0, batch    25 | loss: 9.7166405Losses:  8.833105087280273 1.6173639297485352
CurrentTrain: epoch  0, batch    26 | loss: 10.4504690Losses:  9.556010246276855 1.0009467601776123
CurrentTrain: epoch  0, batch    27 | loss: 10.5569572Losses:  8.4918212890625 0.9963032007217407
CurrentTrain: epoch  0, batch    28 | loss: 9.4881248Losses:  8.013345718383789 0.9924620389938354
CurrentTrain: epoch  0, batch    29 | loss: 9.0058079Losses:  8.7468843460083 1.171382188796997
CurrentTrain: epoch  0, batch    30 | loss: 9.9182663Losses:  9.365623474121094 1.1562180519104004
CurrentTrain: epoch  0, batch    31 | loss: 10.5218410Losses:  8.340200424194336 1.0876867771148682
CurrentTrain: epoch  0, batch    32 | loss: 9.4278870Losses:  8.0928373336792 1.5348122119903564
CurrentTrain: epoch  0, batch    33 | loss: 9.6276493Losses:  9.428401947021484 1.0602679252624512
CurrentTrain: epoch  0, batch    34 | loss: 10.4886703Losses:  8.968063354492188 1.0952556133270264
CurrentTrain: epoch  0, batch    35 | loss: 10.0633192Losses:  8.788248062133789 1.0482832193374634
CurrentTrain: epoch  0, batch    36 | loss: 9.8365316Losses:  8.363584518432617 1.147316575050354
CurrentTrain: epoch  0, batch    37 | loss: 9.5109015Losses:  7.52182674407959 0.9918888807296753
CurrentTrain: epoch  0, batch    38 | loss: 8.5137157Losses:  9.554078102111816 1.110211968421936
CurrentTrain: epoch  0, batch    39 | loss: 10.6642904Losses:  7.613120079040527 1.0522842407226562
CurrentTrain: epoch  0, batch    40 | loss: 8.6654043Losses:  9.194665908813477 0.8471096754074097
CurrentTrain: epoch  0, batch    41 | loss: 10.0417757Losses:  7.399130821228027 1.0374386310577393
CurrentTrain: epoch  0, batch    42 | loss: 8.4365692Losses:  8.108144760131836 1.2529957294464111
CurrentTrain: epoch  0, batch    43 | loss: 9.3611403Losses:  8.065956115722656 0.9592694044113159
CurrentTrain: epoch  0, batch    44 | loss: 9.0252256Losses:  8.210721969604492 1.1041333675384521
CurrentTrain: epoch  0, batch    45 | loss: 9.3148556Losses:  8.328351974487305 0.6042944192886353
CurrentTrain: epoch  0, batch    46 | loss: 8.9326468Losses:  8.723247528076172 0.9176311492919922
CurrentTrain: epoch  0, batch    47 | loss: 9.6408787Losses:  7.415260314941406 0.8821567893028259
CurrentTrain: epoch  0, batch    48 | loss: 8.2974167Losses:  8.105051040649414 1.1234028339385986
CurrentTrain: epoch  0, batch    49 | loss: 9.2284536Losses:  7.843590259552002 0.9592852592468262
CurrentTrain: epoch  0, batch    50 | loss: 8.8028755Losses:  8.395509719848633 0.9045414328575134
CurrentTrain: epoch  0, batch    51 | loss: 9.3000507Losses:  8.640499114990234 1.1125812530517578
CurrentTrain: epoch  0, batch    52 | loss: 9.7530804Losses:  7.995882034301758 1.0768110752105713
CurrentTrain: epoch  0, batch    53 | loss: 9.0726929Losses:  7.449514389038086 0.9188461303710938
CurrentTrain: epoch  0, batch    54 | loss: 8.3683605Losses:  7.424877166748047 0.5954297780990601
CurrentTrain: epoch  0, batch    55 | loss: 8.0203066Losses:  7.503485202789307 0.9696458578109741
CurrentTrain: epoch  0, batch    56 | loss: 8.4731312Losses:  7.823292255401611 1.02327561378479
CurrentTrain: epoch  0, batch    57 | loss: 8.8465681Losses:  6.743075370788574 0.9475896954536438
CurrentTrain: epoch  0, batch    58 | loss: 7.6906652Losses:  6.88213586807251 0.9397515058517456
CurrentTrain: epoch  0, batch    59 | loss: 7.8218875Losses:  7.854134559631348 1.0313254594802856
CurrentTrain: epoch  0, batch    60 | loss: 8.8854599Losses:  7.022700786590576 0.6547122001647949
CurrentTrain: epoch  0, batch    61 | loss: 7.6774130Losses:  7.367363929748535 1.0445711612701416
CurrentTrain: epoch  0, batch    62 | loss: 8.4119349Losses:  6.84615421295166 0.45307236909866333
CurrentTrain: epoch  0, batch    63 | loss: 7.2992268Losses:  7.992046356201172 1.3213448524475098
CurrentTrain: epoch  0, batch    64 | loss: 9.3133907Losses:  8.752314567565918 0.629256010055542
CurrentTrain: epoch  0, batch    65 | loss: 9.3815708Losses:  7.739032745361328 1.094132423400879
CurrentTrain: epoch  0, batch    66 | loss: 8.8331652Losses:  7.667273044586182 0.8789687156677246
CurrentTrain: epoch  0, batch    67 | loss: 8.5462418Losses:  6.988833427429199 1.0086207389831543
CurrentTrain: epoch  0, batch    68 | loss: 7.9974542Losses:  8.039831161499023 0.8659632205963135
CurrentTrain: epoch  0, batch    69 | loss: 8.9057941Losses:  7.879133701324463 0.6761741638183594
CurrentTrain: epoch  0, batch    70 | loss: 8.5553074Losses:  6.909219264984131 0.7393931150436401
CurrentTrain: epoch  0, batch    71 | loss: 7.6486125Losses:  8.262151718139648 0.7098945379257202
CurrentTrain: epoch  0, batch    72 | loss: 8.9720459Losses:  7.803126335144043 1.121424674987793
CurrentTrain: epoch  0, batch    73 | loss: 8.9245510Losses:  7.452856063842773 0.7914888858795166
CurrentTrain: epoch  0, batch    74 | loss: 8.2443447Losses:  7.906831741333008 1.2050280570983887
CurrentTrain: epoch  0, batch    75 | loss: 9.1118603Losses:  7.999449729919434 1.1333184242248535
CurrentTrain: epoch  0, batch    76 | loss: 9.1327686Losses:  6.487787246704102 0.6814638376235962
CurrentTrain: epoch  0, batch    77 | loss: 7.1692510Losses:  6.598780155181885 0.43475091457366943
CurrentTrain: epoch  0, batch    78 | loss: 7.0335312Losses:  7.553171157836914 0.6746147871017456
CurrentTrain: epoch  0, batch    79 | loss: 8.2277861Losses:  6.764580726623535 0.910024881362915
CurrentTrain: epoch  0, batch    80 | loss: 7.6746054Losses:  7.386828422546387 0.8559911251068115
CurrentTrain: epoch  0, batch    81 | loss: 8.2428198Losses:  7.358636379241943 0.7735922336578369
CurrentTrain: epoch  0, batch    82 | loss: 8.1322289Losses:  8.009051322937012 0.44100552797317505
CurrentTrain: epoch  0, batch    83 | loss: 8.4500570Losses:  7.153954029083252 0.7028462886810303
CurrentTrain: epoch  0, batch    84 | loss: 7.8568001Losses:  6.180390357971191 0.5766966342926025
CurrentTrain: epoch  0, batch    85 | loss: 6.7570868Losses:  7.81267786026001 1.0913809537887573
CurrentTrain: epoch  0, batch    86 | loss: 8.9040585Losses:  7.8492584228515625 0.7887470126152039
CurrentTrain: epoch  0, batch    87 | loss: 8.6380053Losses:  7.594605922698975 1.1113624572753906
CurrentTrain: epoch  0, batch    88 | loss: 8.7059689Losses:  8.202000617980957 1.0918028354644775
CurrentTrain: epoch  0, batch    89 | loss: 9.2938032Losses:  7.406506061553955 0.8739954233169556
CurrentTrain: epoch  0, batch    90 | loss: 8.2805014Losses:  7.271592140197754 0.8247963190078735
CurrentTrain: epoch  0, batch    91 | loss: 8.0963888Losses:  6.596896171569824 0.8116620779037476
CurrentTrain: epoch  0, batch    92 | loss: 7.4085584Losses:  10.065523147583008 0.8007000684738159
CurrentTrain: epoch  0, batch    93 | loss: 10.8662233Losses:  7.171897888183594 0.5367339253425598
CurrentTrain: epoch  0, batch    94 | loss: 7.7086320Losses:  7.052360534667969 0.8272587060928345
CurrentTrain: epoch  0, batch    95 | loss: 7.8796191Losses:  6.834805488586426 0.5719624161720276
CurrentTrain: epoch  0, batch    96 | loss: 7.4067678Losses:  5.7136549949646 0.5192645192146301
CurrentTrain: epoch  0, batch    97 | loss: 6.2329197Losses:  7.3805036544799805 0.8343847393989563
CurrentTrain: epoch  0, batch    98 | loss: 8.2148886Losses:  6.981050491333008 0.9291477203369141
CurrentTrain: epoch  0, batch    99 | loss: 7.9101982Losses:  7.074385643005371 0.7273932695388794
CurrentTrain: epoch  0, batch   100 | loss: 7.8017788Losses:  7.337667465209961 0.5493078231811523
CurrentTrain: epoch  0, batch   101 | loss: 7.8869753Losses:  6.136904239654541 0.7046264410018921
CurrentTrain: epoch  0, batch   102 | loss: 6.8415308Losses:  7.607880115509033 0.67868572473526
CurrentTrain: epoch  0, batch   103 | loss: 8.2865658Losses:  7.924041271209717 0.7068076133728027
CurrentTrain: epoch  0, batch   104 | loss: 8.6308489Losses:  7.216963768005371 0.7731813192367554
CurrentTrain: epoch  0, batch   105 | loss: 7.9901452Losses:  8.057197570800781 0.8558863401412964
CurrentTrain: epoch  0, batch   106 | loss: 8.9130840Losses:  8.307144165039062 0.7205692529678345
CurrentTrain: epoch  0, batch   107 | loss: 9.0277138Losses:  6.127756118774414 0.7600106000900269
CurrentTrain: epoch  0, batch   108 | loss: 6.8877668Losses:  6.825006484985352 0.9063571691513062
CurrentTrain: epoch  0, batch   109 | loss: 7.7313638Losses:  6.905184268951416 0.919338583946228
CurrentTrain: epoch  0, batch   110 | loss: 7.8245230Losses:  5.929183006286621 0.802059531211853
CurrentTrain: epoch  0, batch   111 | loss: 6.7312427Losses:  7.112384796142578 0.8210263252258301
CurrentTrain: epoch  0, batch   112 | loss: 7.9334111Losses:  7.79480504989624 0.9547932147979736
CurrentTrain: epoch  0, batch   113 | loss: 8.7495985Losses:  7.297920227050781 0.828106164932251
CurrentTrain: epoch  0, batch   114 | loss: 8.1260262Losses:  7.644248962402344 0.8903290033340454
CurrentTrain: epoch  0, batch   115 | loss: 8.5345783Losses:  6.686613082885742 0.8886762857437134
CurrentTrain: epoch  0, batch   116 | loss: 7.5752892Losses:  7.345952033996582 0.48553013801574707
CurrentTrain: epoch  0, batch   117 | loss: 7.8314819Losses:  6.627817630767822 0.7264798879623413
CurrentTrain: epoch  0, batch   118 | loss: 7.3542976Losses:  6.23516845703125 0.5052832365036011
CurrentTrain: epoch  0, batch   119 | loss: 6.7404518Losses:  8.332189559936523 0.8081478476524353
CurrentTrain: epoch  0, batch   120 | loss: 9.1403370Losses:  6.763157844543457 0.8442814946174622
CurrentTrain: epoch  0, batch   121 | loss: 7.6074395Losses:  6.673892974853516 0.9028645753860474
CurrentTrain: epoch  0, batch   122 | loss: 7.5767574Losses:  6.64826774597168 0.5297949910163879
CurrentTrain: epoch  0, batch   123 | loss: 7.1780629Losses:  6.469194412231445 0.7488299012184143
CurrentTrain: epoch  0, batch   124 | loss: 7.2180243Losses:  5.717113018035889 0.3128722310066223
CurrentTrain: epoch  1, batch     0 | loss: 6.0299854Losses:  7.223067760467529 0.6363152265548706
CurrentTrain: epoch  1, batch     1 | loss: 7.8593831Losses:  6.272193908691406 0.7292672395706177
CurrentTrain: epoch  1, batch     2 | loss: 7.0014610Losses:  6.318127632141113 0.6584741473197937
CurrentTrain: epoch  1, batch     3 | loss: 6.9766016Losses:  6.590361595153809 0.5473294854164124
CurrentTrain: epoch  1, batch     4 | loss: 7.1376910Losses:  8.603059768676758 0.6841427683830261
CurrentTrain: epoch  1, batch     5 | loss: 9.2872028Losses:  6.7881855964660645 0.6067276000976562
CurrentTrain: epoch  1, batch     6 | loss: 7.3949132Losses:  6.345121383666992 0.4159858226776123
CurrentTrain: epoch  1, batch     7 | loss: 6.7611074Losses:  8.868008613586426 0.8772355914115906
CurrentTrain: epoch  1, batch     8 | loss: 9.7452440Losses:  6.661808013916016 0.8108839988708496
CurrentTrain: epoch  1, batch     9 | loss: 7.4726920Losses:  6.385977745056152 0.497260183095932
CurrentTrain: epoch  1, batch    10 | loss: 6.8832378Losses:  6.928953170776367 0.6658085584640503
CurrentTrain: epoch  1, batch    11 | loss: 7.5947618Losses:  6.758730888366699 0.719902753829956
CurrentTrain: epoch  1, batch    12 | loss: 7.4786339Losses:  7.0737481117248535 0.6034004092216492
CurrentTrain: epoch  1, batch    13 | loss: 7.6771483Losses:  7.351044654846191 0.5015707612037659
CurrentTrain: epoch  1, batch    14 | loss: 7.8526154Losses:  6.778628349304199 0.6693097949028015
CurrentTrain: epoch  1, batch    15 | loss: 7.4479380Losses:  7.488284111022949 0.5580372214317322
CurrentTrain: epoch  1, batch    16 | loss: 8.0463209Losses:  8.259276390075684 0.7950372099876404
CurrentTrain: epoch  1, batch    17 | loss: 9.0543137Losses:  6.934814453125 0.6998227834701538
CurrentTrain: epoch  1, batch    18 | loss: 7.6346374Losses:  6.6803364753723145 0.5256313681602478
CurrentTrain: epoch  1, batch    19 | loss: 7.2059679Losses:  6.057750701904297 0.41842448711395264
CurrentTrain: epoch  1, batch    20 | loss: 6.4761753Losses:  6.308932781219482 0.7119327783584595
CurrentTrain: epoch  1, batch    21 | loss: 7.0208654Losses:  6.7376508712768555 0.5198491811752319
CurrentTrain: epoch  1, batch    22 | loss: 7.2575002Losses:  6.575839042663574 0.4659460186958313
CurrentTrain: epoch  1, batch    23 | loss: 7.0417852Losses:  7.0406494140625 0.5200592279434204
CurrentTrain: epoch  1, batch    24 | loss: 7.5607085Losses:  5.722606658935547 0.6176847815513611
CurrentTrain: epoch  1, batch    25 | loss: 6.3402915Losses:  6.321490287780762 0.5641059875488281
CurrentTrain: epoch  1, batch    26 | loss: 6.8855963Losses:  6.9955596923828125 0.38917306065559387
CurrentTrain: epoch  1, batch    27 | loss: 7.3847327Losses:  5.948528289794922 0.471701055765152
CurrentTrain: epoch  1, batch    28 | loss: 6.4202294Losses:  7.136717796325684 0.55406653881073
CurrentTrain: epoch  1, batch    29 | loss: 7.6907845Losses:  6.990574836730957 0.397754967212677
CurrentTrain: epoch  1, batch    30 | loss: 7.3883300Losses:  6.531340599060059 0.5160220861434937
CurrentTrain: epoch  1, batch    31 | loss: 7.0473628Losses:  6.2986297607421875 0.6312055587768555
CurrentTrain: epoch  1, batch    32 | loss: 6.9298353Losses:  6.732086658477783 0.7255954146385193
CurrentTrain: epoch  1, batch    33 | loss: 7.4576821Losses:  5.7379584312438965 0.6008248925209045
CurrentTrain: epoch  1, batch    34 | loss: 6.3387833Losses:  5.299077987670898 0.4737585783004761
CurrentTrain: epoch  1, batch    35 | loss: 5.7728367Losses:  7.215372562408447 0.7432330846786499
CurrentTrain: epoch  1, batch    36 | loss: 7.9586058Losses:  6.173556327819824 0.42135176062583923
CurrentTrain: epoch  1, batch    37 | loss: 6.5949082Losses:  5.5494465827941895 0.5954204201698303
CurrentTrain: epoch  1, batch    38 | loss: 6.1448669Losses:  6.417446136474609 0.52942955493927
CurrentTrain: epoch  1, batch    39 | loss: 6.9468756Losses:  6.239500045776367 0.3486328423023224
CurrentTrain: epoch  1, batch    40 | loss: 6.5881329Losses:  6.641631603240967 0.6429585218429565
CurrentTrain: epoch  1, batch    41 | loss: 7.2845902Losses:  5.707962989807129 0.4759124219417572
CurrentTrain: epoch  1, batch    42 | loss: 6.1838756Losses:  5.416559219360352 0.23282214999198914
CurrentTrain: epoch  1, batch    43 | loss: 5.6493812Losses:  5.980250835418701 0.6844244003295898
CurrentTrain: epoch  1, batch    44 | loss: 6.6646752Losses:  6.029193878173828 0.6549739837646484
CurrentTrain: epoch  1, batch    45 | loss: 6.6841679Losses:  5.92793083190918 0.4731913208961487
CurrentTrain: epoch  1, batch    46 | loss: 6.4011221Losses:  6.972243785858154 0.4463525116443634
CurrentTrain: epoch  1, batch    47 | loss: 7.4185963Losses:  6.657958030700684 0.6836181282997131
CurrentTrain: epoch  1, batch    48 | loss: 7.3415761Losses:  6.204236030578613 0.6744436025619507
CurrentTrain: epoch  1, batch    49 | loss: 6.8786798Losses:  5.442429542541504 0.5382790565490723
CurrentTrain: epoch  1, batch    50 | loss: 5.9807086Losses:  6.1183600425720215 0.6107023358345032
CurrentTrain: epoch  1, batch    51 | loss: 6.7290626Losses:  5.843575477600098 0.5807743072509766
CurrentTrain: epoch  1, batch    52 | loss: 6.4243498Losses:  6.2510151863098145 0.41350293159484863
CurrentTrain: epoch  1, batch    53 | loss: 6.6645184Losses:  6.066679000854492 0.34534382820129395
CurrentTrain: epoch  1, batch    54 | loss: 6.4120226Losses:  8.210622787475586 0.3715148866176605
CurrentTrain: epoch  1, batch    55 | loss: 8.5821381Losses:  6.684205055236816 0.7042580246925354
CurrentTrain: epoch  1, batch    56 | loss: 7.3884630Losses:  6.151308536529541 0.5524052381515503
CurrentTrain: epoch  1, batch    57 | loss: 6.7037139Losses:  6.515984535217285 0.8155003786087036
CurrentTrain: epoch  1, batch    58 | loss: 7.3314848Losses:  6.136805534362793 0.35131600499153137
CurrentTrain: epoch  1, batch    59 | loss: 6.4881215Losses:  5.316156387329102 0.25859278440475464
CurrentTrain: epoch  1, batch    60 | loss: 5.5747490Losses:  7.346561431884766 0.735069751739502
CurrentTrain: epoch  1, batch    61 | loss: 8.0816307Losses:  5.976438999176025 0.5354915261268616
CurrentTrain: epoch  1, batch    62 | loss: 6.5119305Losses:  5.938740253448486 0.3505503833293915
CurrentTrain: epoch  1, batch    63 | loss: 6.2892904Losses:  5.94674015045166 0.3877432346343994
CurrentTrain: epoch  1, batch    64 | loss: 6.3344831Losses:  5.836627006530762 0.40098950266838074
CurrentTrain: epoch  1, batch    65 | loss: 6.2376165Losses:  6.4836578369140625 0.7245715856552124
CurrentTrain: epoch  1, batch    66 | loss: 7.2082295Losses:  6.747113227844238 0.6132299900054932
CurrentTrain: epoch  1, batch    67 | loss: 7.3603430Losses:  5.959982395172119 0.4420928955078125
CurrentTrain: epoch  1, batch    68 | loss: 6.4020753Losses:  5.590106964111328 0.5054084062576294
CurrentTrain: epoch  1, batch    69 | loss: 6.0955153Losses:  7.240041732788086 0.7928632497787476
CurrentTrain: epoch  1, batch    70 | loss: 8.0329046Losses:  6.697807312011719 0.5716649889945984
CurrentTrain: epoch  1, batch    71 | loss: 7.2694721Losses:  5.880482196807861 0.5017329454421997
CurrentTrain: epoch  1, batch    72 | loss: 6.3822150Losses:  5.333469867706299 0.43305128812789917
CurrentTrain: epoch  1, batch    73 | loss: 5.7665210Losses:  6.315363883972168 0.4945034086704254
CurrentTrain: epoch  1, batch    74 | loss: 6.8098674Losses:  7.637688159942627 0.69028639793396
CurrentTrain: epoch  1, batch    75 | loss: 8.3279743Losses:  5.7735185623168945 0.5054137706756592
CurrentTrain: epoch  1, batch    76 | loss: 6.2789326Losses:  6.610598564147949 0.4367749094963074
CurrentTrain: epoch  1, batch    77 | loss: 7.0473733Losses:  6.144419193267822 0.44237685203552246
CurrentTrain: epoch  1, batch    78 | loss: 6.5867958Losses:  6.255442142486572 0.3207356333732605
CurrentTrain: epoch  1, batch    79 | loss: 6.5761776Losses:  6.402998447418213 0.4821230173110962
CurrentTrain: epoch  1, batch    80 | loss: 6.8851213Losses:  5.760868549346924 0.4930339753627777
CurrentTrain: epoch  1, batch    81 | loss: 6.2539024Losses:  6.1450324058532715 0.5163397789001465
CurrentTrain: epoch  1, batch    82 | loss: 6.6613722Losses:  5.263442039489746 0.35076719522476196
CurrentTrain: epoch  1, batch    83 | loss: 5.6142092Losses:  5.918594837188721 0.5794984698295593
CurrentTrain: epoch  1, batch    84 | loss: 6.4980931Losses:  5.63870906829834 0.3364099860191345
CurrentTrain: epoch  1, batch    85 | loss: 5.9751191Losses:  6.166243076324463 0.4219931960105896
CurrentTrain: epoch  1, batch    86 | loss: 6.5882363Losses:  5.888571739196777 0.36903685331344604
CurrentTrain: epoch  1, batch    87 | loss: 6.2576084Losses:  6.75318717956543 0.46014636754989624
CurrentTrain: epoch  1, batch    88 | loss: 7.2133336Losses:  6.304422378540039 0.7689759731292725
CurrentTrain: epoch  1, batch    89 | loss: 7.0733986Losses:  6.839343070983887 0.24404402077198029
CurrentTrain: epoch  1, batch    90 | loss: 7.0833869Losses:  5.125964164733887 0.4898463785648346
CurrentTrain: epoch  1, batch    91 | loss: 5.6158104Losses:  5.582676410675049 0.6496545672416687
CurrentTrain: epoch  1, batch    92 | loss: 6.2323308Losses:  5.25405216217041 0.4306696057319641
CurrentTrain: epoch  1, batch    93 | loss: 5.6847219Losses:  6.573733329772949 0.4339260458946228
CurrentTrain: epoch  1, batch    94 | loss: 7.0076594Losses:  7.028084754943848 0.6928863525390625
CurrentTrain: epoch  1, batch    95 | loss: 7.7209711Losses:  5.600790977478027 0.2999597191810608
CurrentTrain: epoch  1, batch    96 | loss: 5.9007506Losses:  5.766501426696777 0.5212094187736511
CurrentTrain: epoch  1, batch    97 | loss: 6.2877107Losses:  5.691877365112305 0.5658757090568542
CurrentTrain: epoch  1, batch    98 | loss: 6.2577529Losses:  5.866689682006836 0.49701574444770813
CurrentTrain: epoch  1, batch    99 | loss: 6.3637056Losses:  5.658937454223633 0.4443581998348236
CurrentTrain: epoch  1, batch   100 | loss: 6.1032958Losses:  5.77979850769043 0.6216274499893188
CurrentTrain: epoch  1, batch   101 | loss: 6.4014258Losses:  5.183514595031738 0.5204319953918457
CurrentTrain: epoch  1, batch   102 | loss: 5.7039466Losses:  5.574592590332031 0.45587924122810364
CurrentTrain: epoch  1, batch   103 | loss: 6.0304718Losses:  5.025226593017578 0.42116138339042664
CurrentTrain: epoch  1, batch   104 | loss: 5.4463878Losses:  6.08494758605957 0.43368566036224365
CurrentTrain: epoch  1, batch   105 | loss: 6.5186334Losses:  5.236078262329102 0.27238553762435913
CurrentTrain: epoch  1, batch   106 | loss: 5.5084639Losses:  5.823539733886719 0.3693787753582001
CurrentTrain: epoch  1, batch   107 | loss: 6.1929183Losses:  6.158844947814941 0.45563167333602905
CurrentTrain: epoch  1, batch   108 | loss: 6.6144767Losses:  5.165684700012207 0.6086704730987549
CurrentTrain: epoch  1, batch   109 | loss: 5.7743549Losses:  5.167072772979736 0.405860036611557
CurrentTrain: epoch  1, batch   110 | loss: 5.5729327Losses:  5.6706743240356445 0.4591168761253357
CurrentTrain: epoch  1, batch   111 | loss: 6.1297913Losses:  5.007094860076904 0.5499691367149353
CurrentTrain: epoch  1, batch   112 | loss: 5.5570641Losses:  6.507218360900879 0.5113096237182617
CurrentTrain: epoch  1, batch   113 | loss: 7.0185280Losses:  5.4170732498168945 0.3006319999694824
CurrentTrain: epoch  1, batch   114 | loss: 5.7177052Losses:  5.155178070068359 0.48915520310401917
CurrentTrain: epoch  1, batch   115 | loss: 5.6443334Losses:  6.559510231018066 0.644310712814331
CurrentTrain: epoch  1, batch   116 | loss: 7.2038212Losses:  6.202938079833984 0.2441813051700592
CurrentTrain: epoch  1, batch   117 | loss: 6.4471192Losses:  6.188024520874023 0.43771663308143616
CurrentTrain: epoch  1, batch   118 | loss: 6.6257410Losses:  5.8386945724487305 0.39493799209594727
CurrentTrain: epoch  1, batch   119 | loss: 6.2336326Losses:  4.680830955505371 0.25008314847946167
CurrentTrain: epoch  1, batch   120 | loss: 4.9309139Losses:  4.872234344482422 0.5053710341453552
CurrentTrain: epoch  1, batch   121 | loss: 5.3776054Losses:  5.11257791519165 0.2184177041053772
CurrentTrain: epoch  1, batch   122 | loss: 5.3309956Losses:  4.864849090576172 0.42116212844848633
CurrentTrain: epoch  1, batch   123 | loss: 5.2860112Losses:  4.694139003753662 0.25794968008995056
CurrentTrain: epoch  1, batch   124 | loss: 4.9520888Losses:  6.075690746307373 0.5009241104125977
CurrentTrain: epoch  2, batch     0 | loss: 6.5766149Losses:  5.020007133483887 0.3631148636341095
CurrentTrain: epoch  2, batch     1 | loss: 5.3831220Losses:  5.16147518157959 0.5353803038597107
CurrentTrain: epoch  2, batch     2 | loss: 5.6968555Losses:  6.10575008392334 0.44782140851020813
CurrentTrain: epoch  2, batch     3 | loss: 6.5535717Losses:  5.730016231536865 0.3735021948814392
CurrentTrain: epoch  2, batch     4 | loss: 6.1035185Losses:  5.30645751953125 0.3463127017021179
CurrentTrain: epoch  2, batch     5 | loss: 5.6527700Losses:  5.196989059448242 0.3240600824356079
CurrentTrain: epoch  2, batch     6 | loss: 5.5210490Losses:  5.539051055908203 0.3469531536102295
CurrentTrain: epoch  2, batch     7 | loss: 5.8860044Losses:  4.645324230194092 0.3574235439300537
CurrentTrain: epoch  2, batch     8 | loss: 5.0027475Losses:  4.773012638092041 0.4027169644832611
CurrentTrain: epoch  2, batch     9 | loss: 5.1757298Losses:  7.093201637268066 0.2832982838153839
CurrentTrain: epoch  2, batch    10 | loss: 7.3765001Losses:  5.447699546813965 0.3307027220726013
CurrentTrain: epoch  2, batch    11 | loss: 5.7784023Losses:  5.3927836418151855 0.41520872712135315
CurrentTrain: epoch  2, batch    12 | loss: 5.8079925Losses:  5.320614337921143 0.3869486451148987
CurrentTrain: epoch  2, batch    13 | loss: 5.7075629Losses:  4.937112331390381 0.2539075016975403
CurrentTrain: epoch  2, batch    14 | loss: 5.1910200Losses:  4.832332134246826 0.3827475309371948
CurrentTrain: epoch  2, batch    15 | loss: 5.2150798Losses:  5.081393241882324 0.3148835599422455
CurrentTrain: epoch  2, batch    16 | loss: 5.3962770Losses:  4.453499794006348 0.1785619556903839
CurrentTrain: epoch  2, batch    17 | loss: 4.6320620Losses:  4.950655937194824 0.16058433055877686
CurrentTrain: epoch  2, batch    18 | loss: 5.1112404Losses:  5.053711891174316 0.2937788963317871
CurrentTrain: epoch  2, batch    19 | loss: 5.3474908Losses:  4.675797939300537 0.21988272666931152
CurrentTrain: epoch  2, batch    20 | loss: 4.8956804Losses:  5.067086219787598 0.20234528183937073
CurrentTrain: epoch  2, batch    21 | loss: 5.2694316Losses:  4.552548408508301 0.2803606688976288
CurrentTrain: epoch  2, batch    22 | loss: 4.8329091Losses:  6.501892566680908 0.49579575657844543
CurrentTrain: epoch  2, batch    23 | loss: 6.9976883Losses:  5.5331268310546875 0.2995230555534363
CurrentTrain: epoch  2, batch    24 | loss: 5.8326497Losses:  6.320089340209961 0.3497423231601715
CurrentTrain: epoch  2, batch    25 | loss: 6.6698318Losses:  5.822741508483887 0.2828936278820038
CurrentTrain: epoch  2, batch    26 | loss: 6.1056352Losses:  4.970423221588135 0.36717647314071655
CurrentTrain: epoch  2, batch    27 | loss: 5.3375998Losses:  5.79622745513916 0.3010052442550659
CurrentTrain: epoch  2, batch    28 | loss: 6.0972328Losses:  4.331593990325928 0.2428036779165268
CurrentTrain: epoch  2, batch    29 | loss: 4.5743976Losses:  5.262578010559082 0.38609713315963745
CurrentTrain: epoch  2, batch    30 | loss: 5.6486750Losses:  4.957966327667236 0.3386539816856384
CurrentTrain: epoch  2, batch    31 | loss: 5.2966204Losses:  5.582405090332031 0.447035551071167
CurrentTrain: epoch  2, batch    32 | loss: 6.0294409Losses:  5.364290237426758 0.36647605895996094
CurrentTrain: epoch  2, batch    33 | loss: 5.7307663Losses:  4.882728576660156 0.3713637590408325
CurrentTrain: epoch  2, batch    34 | loss: 5.2540922Losses:  5.151195049285889 0.41802242398262024
CurrentTrain: epoch  2, batch    35 | loss: 5.5692177Losses:  4.852484703063965 0.4206196665763855
CurrentTrain: epoch  2, batch    36 | loss: 5.2731042Losses:  4.862259864807129 0.314719021320343
CurrentTrain: epoch  2, batch    37 | loss: 5.1769791Losses:  5.007145881652832 0.32635629177093506
CurrentTrain: epoch  2, batch    38 | loss: 5.3335023Losses:  5.472301483154297 0.48132210969924927
CurrentTrain: epoch  2, batch    39 | loss: 5.9536238Losses:  5.730029106140137 0.4769379794597626
CurrentTrain: epoch  2, batch    40 | loss: 6.2069669Losses:  5.831557273864746 0.3990437984466553
CurrentTrain: epoch  2, batch    41 | loss: 6.2306013Losses:  4.8128204345703125 0.31019285321235657
CurrentTrain: epoch  2, batch    42 | loss: 5.1230135Losses:  4.728738784790039 0.3543285131454468
CurrentTrain: epoch  2, batch    43 | loss: 5.0830674Losses:  5.246121406555176 0.280714213848114
CurrentTrain: epoch  2, batch    44 | loss: 5.5268354Losses:  5.3184309005737305 0.2862025499343872
CurrentTrain: epoch  2, batch    45 | loss: 5.6046333Losses:  5.625972270965576 0.4761626124382019
CurrentTrain: epoch  2, batch    46 | loss: 6.1021347Losses:  4.8990302085876465 0.18872776627540588
CurrentTrain: epoch  2, batch    47 | loss: 5.0877581Losses:  4.765082359313965 0.1995372325181961
CurrentTrain: epoch  2, batch    48 | loss: 4.9646196Losses:  5.340087890625 0.21632838249206543
CurrentTrain: epoch  2, batch    49 | loss: 5.5564165Losses:  5.181448936462402 0.3686479926109314
CurrentTrain: epoch  2, batch    50 | loss: 5.5500970Losses:  4.491795539855957 0.18522711098194122
CurrentTrain: epoch  2, batch    51 | loss: 4.6770225Losses:  5.604878902435303 0.27476826310157776
CurrentTrain: epoch  2, batch    52 | loss: 5.8796473Losses:  4.979236125946045 0.29665452241897583
CurrentTrain: epoch  2, batch    53 | loss: 5.2758908Losses:  5.490079879760742 0.32027941942214966
CurrentTrain: epoch  2, batch    54 | loss: 5.8103595Losses:  4.462541580200195 0.24872365593910217
CurrentTrain: epoch  2, batch    55 | loss: 4.7112651Losses:  5.127096176147461 0.2639756202697754
CurrentTrain: epoch  2, batch    56 | loss: 5.3910718Losses:  5.2711381912231445 0.4310245215892792
CurrentTrain: epoch  2, batch    57 | loss: 5.7021627Losses:  6.300022602081299 0.44535815715789795
CurrentTrain: epoch  2, batch    58 | loss: 6.7453809Losses:  4.513033866882324 0.23594410717487335
CurrentTrain: epoch  2, batch    59 | loss: 4.7489781Losses:  5.086620330810547 0.23738785088062286
CurrentTrain: epoch  2, batch    60 | loss: 5.3240080Losses:  5.186359405517578 0.4337809085845947
CurrentTrain: epoch  2, batch    61 | loss: 5.6201401Losses:  5.50954532623291 0.2900262475013733
CurrentTrain: epoch  2, batch    62 | loss: 5.7995715Losses:  4.981958389282227 0.35698914527893066
CurrentTrain: epoch  2, batch    63 | loss: 5.3389473Losses:  4.929406642913818 0.15252244472503662
CurrentTrain: epoch  2, batch    64 | loss: 5.0819292Losses:  5.252679824829102 0.17920377850532532
CurrentTrain: epoch  2, batch    65 | loss: 5.4318838Losses:  5.053348064422607 0.32697418332099915
CurrentTrain: epoch  2, batch    66 | loss: 5.3803225Losses:  4.73635196685791 0.25507694482803345
CurrentTrain: epoch  2, batch    67 | loss: 4.9914289Losses:  5.0506181716918945 0.36906909942626953
CurrentTrain: epoch  2, batch    68 | loss: 5.4196873Losses:  4.6676249504089355 0.17261841893196106
CurrentTrain: epoch  2, batch    69 | loss: 4.8402433Losses:  4.943487644195557 0.4190714657306671
CurrentTrain: epoch  2, batch    70 | loss: 5.3625593Losses:  4.849427223205566 0.26358336210250854
CurrentTrain: epoch  2, batch    71 | loss: 5.1130104Losses:  5.214361190795898 0.3626730144023895
CurrentTrain: epoch  2, batch    72 | loss: 5.5770340Losses:  4.773618698120117 0.3254193961620331
CurrentTrain: epoch  2, batch    73 | loss: 5.0990381Losses:  4.645369529724121 0.39251214265823364
CurrentTrain: epoch  2, batch    74 | loss: 5.0378819Losses:  4.814023971557617 0.17634320259094238
CurrentTrain: epoch  2, batch    75 | loss: 4.9903669Losses:  4.7075300216674805 0.2241336703300476
CurrentTrain: epoch  2, batch    76 | loss: 4.9316635Losses:  4.921910285949707 0.38340994715690613
CurrentTrain: epoch  2, batch    77 | loss: 5.3053203Losses:  4.8485822677612305 0.24949485063552856
CurrentTrain: epoch  2, batch    78 | loss: 5.0980773Losses:  4.597955226898193 0.19289246201515198
CurrentTrain: epoch  2, batch    79 | loss: 4.7908478Losses:  5.033686637878418 0.3784865438938141
CurrentTrain: epoch  2, batch    80 | loss: 5.4121733Losses:  5.251970291137695 0.38809943199157715
CurrentTrain: epoch  2, batch    81 | loss: 5.6400700Losses:  4.819908142089844 0.2801700532436371
CurrentTrain: epoch  2, batch    82 | loss: 5.1000781Losses:  5.209512710571289 0.2884402573108673
CurrentTrain: epoch  2, batch    83 | loss: 5.4979529Losses:  5.5561203956604 0.44590914249420166
CurrentTrain: epoch  2, batch    84 | loss: 6.0020294Losses:  4.441460132598877 0.28542011976242065
CurrentTrain: epoch  2, batch    85 | loss: 4.7268801Losses:  4.944971084594727 0.23745286464691162
CurrentTrain: epoch  2, batch    86 | loss: 5.1824241Losses:  4.841674327850342 0.19804570078849792
CurrentTrain: epoch  2, batch    87 | loss: 5.0397201Losses:  4.9754767417907715 0.22474396228790283
CurrentTrain: epoch  2, batch    88 | loss: 5.2002206Losses:  4.374101161956787 0.2302454560995102
CurrentTrain: epoch  2, batch    89 | loss: 4.6043468Losses:  5.944540977478027 0.4600760340690613
CurrentTrain: epoch  2, batch    90 | loss: 6.4046168Losses:  5.107308387756348 0.24495863914489746
CurrentTrain: epoch  2, batch    91 | loss: 5.3522673Losses:  5.033496379852295 0.3602461516857147
CurrentTrain: epoch  2, batch    92 | loss: 5.3937426Losses:  5.096609115600586 0.37975072860717773
CurrentTrain: epoch  2, batch    93 | loss: 5.4763598Losses:  4.503793716430664 0.16987216472625732
CurrentTrain: epoch  2, batch    94 | loss: 4.6736660Losses:  4.651500701904297 0.3036339282989502
CurrentTrain: epoch  2, batch    95 | loss: 4.9551344Losses:  4.834662437438965 0.16942287981510162
CurrentTrain: epoch  2, batch    96 | loss: 5.0040855Losses:  4.503914833068848 0.27474433183670044
CurrentTrain: epoch  2, batch    97 | loss: 4.7786593Losses:  4.567625522613525 0.18881633877754211
CurrentTrain: epoch  2, batch    98 | loss: 4.7564421Losses:  5.296470642089844 0.3915256857872009
CurrentTrain: epoch  2, batch    99 | loss: 5.6879964Losses:  7.490512847900391 0.4413135051727295
CurrentTrain: epoch  2, batch   100 | loss: 7.9318266Losses:  6.291945457458496 0.2586057186126709
CurrentTrain: epoch  2, batch   101 | loss: 6.5505514Losses:  5.756039142608643 0.29411593079566956
CurrentTrain: epoch  2, batch   102 | loss: 6.0501552Losses:  4.57105016708374 0.20468898117542267
CurrentTrain: epoch  2, batch   103 | loss: 4.7757392Losses:  5.089350700378418 0.2455437332391739
CurrentTrain: epoch  2, batch   104 | loss: 5.3348947Losses:  4.896258354187012 0.29255691170692444
CurrentTrain: epoch  2, batch   105 | loss: 5.1888151Losses:  6.364527702331543 0.3077525496482849
CurrentTrain: epoch  2, batch   106 | loss: 6.6722803Losses:  4.9358930587768555 0.23390930891036987
CurrentTrain: epoch  2, batch   107 | loss: 5.1698022Losses:  5.433567047119141 0.3385446071624756
CurrentTrain: epoch  2, batch   108 | loss: 5.7721119Losses:  4.735001564025879 0.22359706461429596
CurrentTrain: epoch  2, batch   109 | loss: 4.9585986Losses:  5.402413845062256 0.20384624600410461
CurrentTrain: epoch  2, batch   110 | loss: 5.6062603Losses:  4.852449893951416 0.27138543128967285
CurrentTrain: epoch  2, batch   111 | loss: 5.1238356Losses:  5.198908805847168 0.35637611150741577
CurrentTrain: epoch  2, batch   112 | loss: 5.5552850Losses:  4.6206817626953125 0.16998505592346191
CurrentTrain: epoch  2, batch   113 | loss: 4.7906666Losses:  4.745335578918457 0.2723888158798218
CurrentTrain: epoch  2, batch   114 | loss: 5.0177245Losses:  5.170550346374512 0.3386619985103607
CurrentTrain: epoch  2, batch   115 | loss: 5.5092125Losses:  4.9766950607299805 0.14678406715393066
CurrentTrain: epoch  2, batch   116 | loss: 5.1234789Losses:  4.488772392272949 0.3062508702278137
CurrentTrain: epoch  2, batch   117 | loss: 4.7950234Losses:  4.576210975646973 0.1410338580608368
CurrentTrain: epoch  2, batch   118 | loss: 4.7172446Losses:  5.460867404937744 0.3147375285625458
CurrentTrain: epoch  2, batch   119 | loss: 5.7756047Losses:  5.063712120056152 0.29121634364128113
CurrentTrain: epoch  2, batch   120 | loss: 5.3549285Losses:  4.768028736114502 0.2506757378578186
CurrentTrain: epoch  2, batch   121 | loss: 5.0187044Losses:  5.466396331787109 0.2579997181892395
CurrentTrain: epoch  2, batch   122 | loss: 5.7243962Losses:  4.571611404418945 0.30118516087532043
CurrentTrain: epoch  2, batch   123 | loss: 4.8727965Losses:  4.7647199630737305 0.36412402987480164
CurrentTrain: epoch  2, batch   124 | loss: 5.1288438Losses:  4.851921558380127 0.28189995884895325
CurrentTrain: epoch  3, batch     0 | loss: 5.1338215Losses:  4.409808158874512 0.3134567439556122
CurrentTrain: epoch  3, batch     1 | loss: 4.7232647Losses:  4.81307315826416 0.33744746446609497
CurrentTrain: epoch  3, batch     2 | loss: 5.1505208Losses:  4.789039134979248 0.2551022171974182
CurrentTrain: epoch  3, batch     3 | loss: 5.0441413Losses:  4.641178607940674 0.10900355875492096
CurrentTrain: epoch  3, batch     4 | loss: 4.7501822Losses:  4.853289604187012 0.26028361916542053
CurrentTrain: epoch  3, batch     5 | loss: 5.1135731Losses:  4.607086658477783 0.1462801694869995
CurrentTrain: epoch  3, batch     6 | loss: 4.7533669Losses:  4.442946910858154 0.2322276085615158
CurrentTrain: epoch  3, batch     7 | loss: 4.6751747Losses:  4.94854736328125 0.45879077911376953
CurrentTrain: epoch  3, batch     8 | loss: 5.4073381Losses:  4.302815914154053 0.14426690340042114
CurrentTrain: epoch  3, batch     9 | loss: 4.4470830Losses:  4.249618053436279 0.17628905177116394
CurrentTrain: epoch  3, batch    10 | loss: 4.4259071Losses:  4.594404220581055 0.2523864507675171
CurrentTrain: epoch  3, batch    11 | loss: 4.8467908Losses:  4.461983680725098 0.29647374153137207
CurrentTrain: epoch  3, batch    12 | loss: 4.7584572Losses:  4.754319667816162 0.2712450325489044
CurrentTrain: epoch  3, batch    13 | loss: 5.0255647Losses:  4.836004257202148 0.24767130613327026
CurrentTrain: epoch  3, batch    14 | loss: 5.0836754Losses:  4.689788818359375 0.29495519399642944
CurrentTrain: epoch  3, batch    15 | loss: 4.9847441Losses:  4.647294521331787 0.31207340955734253
CurrentTrain: epoch  3, batch    16 | loss: 4.9593678Losses:  4.80155086517334 0.19994421303272247
CurrentTrain: epoch  3, batch    17 | loss: 5.0014949Losses:  4.3643388748168945 0.2927965521812439
CurrentTrain: epoch  3, batch    18 | loss: 4.6571355Losses:  4.954023361206055 0.3563840389251709
CurrentTrain: epoch  3, batch    19 | loss: 5.3104076Losses:  4.614606857299805 0.2108055055141449
CurrentTrain: epoch  3, batch    20 | loss: 4.8254123Losses:  4.502909183502197 0.22604462504386902
CurrentTrain: epoch  3, batch    21 | loss: 4.7289538Losses:  4.565112113952637 0.2976645231246948
CurrentTrain: epoch  3, batch    22 | loss: 4.8627768Losses:  4.714249610900879 0.18625611066818237
CurrentTrain: epoch  3, batch    23 | loss: 4.9005055Losses:  4.454676151275635 0.32198119163513184
CurrentTrain: epoch  3, batch    24 | loss: 4.7766571Losses:  4.491719722747803 0.235258549451828
CurrentTrain: epoch  3, batch    25 | loss: 4.7269783Losses:  4.7421417236328125 0.11256588250398636
CurrentTrain: epoch  3, batch    26 | loss: 4.8547077Losses:  4.543853759765625 0.30530375242233276
CurrentTrain: epoch  3, batch    27 | loss: 4.8491573Losses:  4.3861494064331055 0.1701350212097168
CurrentTrain: epoch  3, batch    28 | loss: 4.5562844Losses:  4.566812515258789 0.21718034148216248
CurrentTrain: epoch  3, batch    29 | loss: 4.7839928Losses:  4.581914901733398 0.21108923852443695
CurrentTrain: epoch  3, batch    30 | loss: 4.7930040Losses:  4.885548114776611 0.19632349908351898
CurrentTrain: epoch  3, batch    31 | loss: 5.0818715Losses:  4.763598442077637 0.28718647360801697
CurrentTrain: epoch  3, batch    32 | loss: 5.0507851Losses:  4.434263706207275 0.07013524323701859
CurrentTrain: epoch  3, batch    33 | loss: 4.5043988Losses:  4.773345947265625 0.17486590147018433
CurrentTrain: epoch  3, batch    34 | loss: 4.9482117Losses:  4.438511848449707 0.23182713985443115
CurrentTrain: epoch  3, batch    35 | loss: 4.6703391Losses:  4.297542572021484 0.2695194482803345
CurrentTrain: epoch  3, batch    36 | loss: 4.5670619Losses:  6.519025802612305 0.6173270344734192
CurrentTrain: epoch  3, batch    37 | loss: 7.1363530Losses:  4.311296463012695 0.29158884286880493
CurrentTrain: epoch  3, batch    38 | loss: 4.6028852Losses:  4.479280471801758 0.3306317925453186
CurrentTrain: epoch  3, batch    39 | loss: 4.8099122Losses:  4.466115951538086 0.2725808918476105
CurrentTrain: epoch  3, batch    40 | loss: 4.7386971Losses:  4.930366516113281 0.31223970651626587
CurrentTrain: epoch  3, batch    41 | loss: 5.2426062Losses:  5.533244609832764 0.33840301632881165
CurrentTrain: epoch  3, batch    42 | loss: 5.8716478Losses:  4.390285491943359 0.26464158296585083
CurrentTrain: epoch  3, batch    43 | loss: 4.6549273Losses:  4.4649763107299805 0.24989037215709686
CurrentTrain: epoch  3, batch    44 | loss: 4.7148666Losses:  4.559311866760254 0.3521002531051636
CurrentTrain: epoch  3, batch    45 | loss: 4.9114122Losses:  4.465456962585449 0.13199833035469055
CurrentTrain: epoch  3, batch    46 | loss: 4.5974555Losses:  4.546499252319336 0.169642373919487
CurrentTrain: epoch  3, batch    47 | loss: 4.7161417Losses:  4.656878471374512 0.2659800052642822
CurrentTrain: epoch  3, batch    48 | loss: 4.9228582Losses:  4.631379127502441 0.36499980092048645
CurrentTrain: epoch  3, batch    49 | loss: 4.9963789Losses:  4.334308624267578 0.09962183982133865
CurrentTrain: epoch  3, batch    50 | loss: 4.4339304Losses:  4.261353015899658 0.17316387593746185
CurrentTrain: epoch  3, batch    51 | loss: 4.4345169Losses:  4.916444778442383 0.3418673574924469
CurrentTrain: epoch  3, batch    52 | loss: 5.2583122Losses:  4.659601211547852 0.34766191244125366
CurrentTrain: epoch  3, batch    53 | loss: 5.0072632Losses:  5.369338035583496 0.29199784994125366
CurrentTrain: epoch  3, batch    54 | loss: 5.6613359Losses:  4.363015174865723 0.19078180193901062
CurrentTrain: epoch  3, batch    55 | loss: 4.5537968Losses:  4.727383613586426 0.27892130613327026
CurrentTrain: epoch  3, batch    56 | loss: 5.0063047Losses:  4.609169006347656 0.3213537633419037
CurrentTrain: epoch  3, batch    57 | loss: 4.9305229Losses:  4.478172779083252 0.35455644130706787
CurrentTrain: epoch  3, batch    58 | loss: 4.8327293Losses:  4.420171737670898 0.31319713592529297
CurrentTrain: epoch  3, batch    59 | loss: 4.7333689Losses:  4.897517204284668 0.3766377568244934
CurrentTrain: epoch  3, batch    60 | loss: 5.2741551Losses:  4.418210029602051 0.2867088317871094
CurrentTrain: epoch  3, batch    61 | loss: 4.7049189Losses:  4.636742115020752 0.18829423189163208
CurrentTrain: epoch  3, batch    62 | loss: 4.8250365Losses:  4.517965793609619 0.14241650700569153
CurrentTrain: epoch  3, batch    63 | loss: 4.6603823Losses:  4.781303405761719 0.2599515914916992
CurrentTrain: epoch  3, batch    64 | loss: 5.0412550Losses:  4.43727970123291 0.15681582689285278
CurrentTrain: epoch  3, batch    65 | loss: 4.5940957Losses:  4.455655097961426 0.16065703332424164
CurrentTrain: epoch  3, batch    66 | loss: 4.6163120Losses:  4.519845485687256 0.19511985778808594
CurrentTrain: epoch  3, batch    67 | loss: 4.7149653Losses:  4.709033966064453 0.2071412205696106
CurrentTrain: epoch  3, batch    68 | loss: 4.9161754Losses:  4.39475154876709 0.3328447937965393
CurrentTrain: epoch  3, batch    69 | loss: 4.7275963Losses:  4.395167827606201 0.13299629092216492
CurrentTrain: epoch  3, batch    70 | loss: 4.5281639Losses:  4.439643859863281 0.15496936440467834
CurrentTrain: epoch  3, batch    71 | loss: 4.5946131Losses:  4.966236591339111 0.29181891679763794
CurrentTrain: epoch  3, batch    72 | loss: 5.2580557Losses:  4.845654487609863 0.2973673343658447
CurrentTrain: epoch  3, batch    73 | loss: 5.1430216Losses:  4.466318130493164 0.20727714896202087
CurrentTrain: epoch  3, batch    74 | loss: 4.6735954Losses:  5.158997535705566 0.1486383080482483
CurrentTrain: epoch  3, batch    75 | loss: 5.3076358Losses:  4.476973056793213 0.27907267212867737
CurrentTrain: epoch  3, batch    76 | loss: 4.7560458Losses:  4.958855628967285 0.21202601492404938
CurrentTrain: epoch  3, batch    77 | loss: 5.1708817Losses:  4.486750602722168 0.3781017065048218
CurrentTrain: epoch  3, batch    78 | loss: 4.8648524Losses:  4.53521728515625 0.2744070589542389
CurrentTrain: epoch  3, batch    79 | loss: 4.8096242Losses:  5.539656639099121 0.398021936416626
CurrentTrain: epoch  3, batch    80 | loss: 5.9376783Losses:  4.750107765197754 0.300121545791626
CurrentTrain: epoch  3, batch    81 | loss: 5.0502291Losses:  4.322597026824951 0.1475699096918106
CurrentTrain: epoch  3, batch    82 | loss: 4.4701672Losses:  4.532182216644287 0.176020085811615
CurrentTrain: epoch  3, batch    83 | loss: 4.7082024Losses:  4.563295841217041 0.2198696881532669
CurrentTrain: epoch  3, batch    84 | loss: 4.7831655Losses:  4.3266706466674805 0.3034714460372925
CurrentTrain: epoch  3, batch    85 | loss: 4.6301422Losses:  4.568469047546387 0.24422289431095123
CurrentTrain: epoch  3, batch    86 | loss: 4.8126922Losses:  4.30845832824707 0.15251170098781586
CurrentTrain: epoch  3, batch    87 | loss: 4.4609699Losses:  4.356260299682617 0.1211220994591713
CurrentTrain: epoch  3, batch    88 | loss: 4.4773822Losses:  4.366000175476074 0.18887194991111755
CurrentTrain: epoch  3, batch    89 | loss: 4.5548720Losses:  4.407170295715332 0.12194880843162537
CurrentTrain: epoch  3, batch    90 | loss: 4.5291190Losses:  5.785730838775635 0.32725536823272705
CurrentTrain: epoch  3, batch    91 | loss: 6.1129861Losses:  4.420660972595215 0.12692907452583313
CurrentTrain: epoch  3, batch    92 | loss: 4.5475903Losses:  4.444746971130371 0.24534259736537933
CurrentTrain: epoch  3, batch    93 | loss: 4.6900897Losses:  4.429792404174805 0.3324914574623108
CurrentTrain: epoch  3, batch    94 | loss: 4.7622838Losses:  4.4496259689331055 0.27123698592185974
CurrentTrain: epoch  3, batch    95 | loss: 4.7208629Losses:  4.461645126342773 0.1497173309326172
CurrentTrain: epoch  3, batch    96 | loss: 4.6113625Losses:  4.31322717666626 0.15276536345481873
CurrentTrain: epoch  3, batch    97 | loss: 4.4659925Losses:  4.494627952575684 0.2654404640197754
CurrentTrain: epoch  3, batch    98 | loss: 4.7600684Losses:  4.654618263244629 0.3146263360977173
CurrentTrain: epoch  3, batch    99 | loss: 4.9692445Losses:  4.6836700439453125 0.17064374685287476
CurrentTrain: epoch  3, batch   100 | loss: 4.8543139Losses:  4.343710899353027 0.2295568287372589
CurrentTrain: epoch  3, batch   101 | loss: 4.5732679Losses:  4.660702705383301 0.2426721751689911
CurrentTrain: epoch  3, batch   102 | loss: 4.9033747Losses:  4.439079284667969 0.18951915204524994
CurrentTrain: epoch  3, batch   103 | loss: 4.6285982Losses:  4.30765438079834 0.3275589942932129
CurrentTrain: epoch  3, batch   104 | loss: 4.6352134Losses:  4.20186710357666 0.17730195820331573
CurrentTrain: epoch  3, batch   105 | loss: 4.3791690Losses:  4.517851829528809 0.0992782860994339
CurrentTrain: epoch  3, batch   106 | loss: 4.6171303Losses:  4.57804012298584 0.2579538822174072
CurrentTrain: epoch  3, batch   107 | loss: 4.8359938Losses:  4.454545021057129 0.15475377440452576
CurrentTrain: epoch  3, batch   108 | loss: 4.6092987Losses:  4.465645790100098 0.17210043966770172
CurrentTrain: epoch  3, batch   109 | loss: 4.6377463Losses:  4.260515213012695 0.142812117934227
CurrentTrain: epoch  3, batch   110 | loss: 4.4033275Losses:  4.262221336364746 0.11007444560527802
CurrentTrain: epoch  3, batch   111 | loss: 4.3722959Losses:  4.234112739562988 0.13106371462345123
CurrentTrain: epoch  3, batch   112 | loss: 4.3651767Losses:  4.306491851806641 0.06220361217856407
CurrentTrain: epoch  3, batch   113 | loss: 4.3686953Losses:  4.29701042175293 0.09296827018260956
CurrentTrain: epoch  3, batch   114 | loss: 4.3899789Losses:  4.314190864562988 0.21283075213432312
CurrentTrain: epoch  3, batch   115 | loss: 4.5270214Losses:  4.298257827758789 0.0746307224035263
CurrentTrain: epoch  3, batch   116 | loss: 4.3728886Losses:  4.306674957275391 0.23780272901058197
CurrentTrain: epoch  3, batch   117 | loss: 4.5444775Losses:  4.135622978210449 0.06504769623279572
CurrentTrain: epoch  3, batch   118 | loss: 4.2006707Losses:  4.16532039642334 0.3250386714935303
CurrentTrain: epoch  3, batch   119 | loss: 4.4903593Losses:  4.36432409286499 0.2628728747367859
CurrentTrain: epoch  3, batch   120 | loss: 4.6271968Losses:  4.0889387130737305 0.13532540202140808
CurrentTrain: epoch  3, batch   121 | loss: 4.2242641Losses:  4.886270523071289 0.17343786358833313
CurrentTrain: epoch  3, batch   122 | loss: 5.0597086Losses:  4.869292259216309 0.1585710197687149
CurrentTrain: epoch  3, batch   123 | loss: 5.0278635Losses:  4.369921684265137 0.2004949152469635
CurrentTrain: epoch  3, batch   124 | loss: 4.5704165Losses:  4.293098449707031 0.1351163238286972
CurrentTrain: epoch  4, batch     0 | loss: 4.4282146Losses:  4.332510948181152 0.28060299158096313
CurrentTrain: epoch  4, batch     1 | loss: 4.6131139Losses:  4.4236907958984375 0.13909929990768433
CurrentTrain: epoch  4, batch     2 | loss: 4.5627899Losses:  4.264123916625977 0.16223205626010895
CurrentTrain: epoch  4, batch     3 | loss: 4.4263558Losses:  4.288738250732422 0.09573616832494736
CurrentTrain: epoch  4, batch     4 | loss: 4.3844743Losses:  4.422816276550293 0.17835356295108795
CurrentTrain: epoch  4, batch     5 | loss: 4.6011701Losses:  4.495563507080078 0.14942161738872528
CurrentTrain: epoch  4, batch     6 | loss: 4.6449852Losses:  4.263871669769287 0.23971347510814667
CurrentTrain: epoch  4, batch     7 | loss: 4.5035853Losses:  4.374432563781738 0.213850200176239
CurrentTrain: epoch  4, batch     8 | loss: 4.5882826Losses:  4.225062370300293 0.1507578194141388
CurrentTrain: epoch  4, batch     9 | loss: 4.3758202Losses:  4.180867671966553 0.26766490936279297
CurrentTrain: epoch  4, batch    10 | loss: 4.4485326Losses:  4.28184700012207 0.10366412997245789
CurrentTrain: epoch  4, batch    11 | loss: 4.3855109Losses:  4.316723346710205 0.2081632912158966
CurrentTrain: epoch  4, batch    12 | loss: 4.5248866Losses:  4.324728012084961 0.15357811748981476
CurrentTrain: epoch  4, batch    13 | loss: 4.4783063Losses:  4.382274150848389 0.15889310836791992
CurrentTrain: epoch  4, batch    14 | loss: 4.5411673Losses:  4.352301597595215 0.21450665593147278
CurrentTrain: epoch  4, batch    15 | loss: 4.5668082Losses:  4.337604999542236 0.1656482070684433
CurrentTrain: epoch  4, batch    16 | loss: 4.5032530Losses:  4.279888153076172 0.2538910508155823
CurrentTrain: epoch  4, batch    17 | loss: 4.5337791Losses:  4.324559688568115 0.2103147953748703
CurrentTrain: epoch  4, batch    18 | loss: 4.5348744Losses:  4.3084516525268555 0.25049158930778503
CurrentTrain: epoch  4, batch    19 | loss: 4.5589433Losses:  4.24820613861084 0.16429930925369263
CurrentTrain: epoch  4, batch    20 | loss: 4.4125056Losses:  4.2880401611328125 0.10915932804346085
CurrentTrain: epoch  4, batch    21 | loss: 4.3971996Losses:  4.269811153411865 0.2004261463880539
CurrentTrain: epoch  4, batch    22 | loss: 4.4702373Losses:  4.38093376159668 0.23829904198646545
CurrentTrain: epoch  4, batch    23 | loss: 4.6192327Losses:  4.430357933044434 0.2267138659954071
CurrentTrain: epoch  4, batch    24 | loss: 4.6570716Losses:  4.26114559173584 0.14698591828346252
CurrentTrain: epoch  4, batch    25 | loss: 4.4081316Losses:  4.280742168426514 0.13098523020744324
CurrentTrain: epoch  4, batch    26 | loss: 4.4117274Losses:  4.232881546020508 0.1701798290014267
CurrentTrain: epoch  4, batch    27 | loss: 4.4030614Losses:  4.526987075805664 0.2781693935394287
CurrentTrain: epoch  4, batch    28 | loss: 4.8051567Losses:  4.319668769836426 0.2369876205921173
CurrentTrain: epoch  4, batch    29 | loss: 4.5566564Losses:  4.304208755493164 0.24248862266540527
CurrentTrain: epoch  4, batch    30 | loss: 4.5466976Losses:  4.741535186767578 0.12479569762945175
CurrentTrain: epoch  4, batch    31 | loss: 4.8663311Losses:  4.307447910308838 0.21408957242965698
CurrentTrain: epoch  4, batch    32 | loss: 4.5215373Losses:  4.262878894805908 0.21470269560813904
CurrentTrain: epoch  4, batch    33 | loss: 4.4775815Losses:  4.240245819091797 0.1255963295698166
CurrentTrain: epoch  4, batch    34 | loss: 4.3658423Losses:  4.243766784667969 0.09417599439620972
CurrentTrain: epoch  4, batch    35 | loss: 4.3379426Losses:  4.392608165740967 0.21065984666347504
CurrentTrain: epoch  4, batch    36 | loss: 4.6032681Losses:  4.423841953277588 0.19139480590820312
CurrentTrain: epoch  4, batch    37 | loss: 4.6152368Losses:  4.398069381713867 0.20917320251464844
CurrentTrain: epoch  4, batch    38 | loss: 4.6072426Losses:  4.239386081695557 0.29302191734313965
CurrentTrain: epoch  4, batch    39 | loss: 4.5324078Losses:  4.212747573852539 0.19864729046821594
CurrentTrain: epoch  4, batch    40 | loss: 4.4113951Losses:  4.21572208404541 0.1249704509973526
CurrentTrain: epoch  4, batch    41 | loss: 4.3406925Losses:  4.310799598693848 0.11839692294597626
CurrentTrain: epoch  4, batch    42 | loss: 4.4291964Losses:  4.3126220703125 0.12652215361595154
CurrentTrain: epoch  4, batch    43 | loss: 4.4391441Losses:  4.17495059967041 0.14526039361953735
CurrentTrain: epoch  4, batch    44 | loss: 4.3202109Losses:  4.22327995300293 0.10053454339504242
CurrentTrain: epoch  4, batch    45 | loss: 4.3238144Losses:  4.723208904266357 0.20181216299533844
CurrentTrain: epoch  4, batch    46 | loss: 4.9250212Losses:  4.271886825561523 0.17256011068820953
CurrentTrain: epoch  4, batch    47 | loss: 4.4444470Losses:  4.3079915046691895 0.20946836471557617
CurrentTrain: epoch  4, batch    48 | loss: 4.5174599Losses:  4.241649627685547 0.16249293088912964
CurrentTrain: epoch  4, batch    49 | loss: 4.4041424Losses:  4.244032382965088 0.12408939003944397
CurrentTrain: epoch  4, batch    50 | loss: 4.3681216Losses:  4.400082111358643 0.2669403851032257
CurrentTrain: epoch  4, batch    51 | loss: 4.6670227Losses:  4.3354058265686035 0.10878217220306396
CurrentTrain: epoch  4, batch    52 | loss: 4.4441881Losses:  4.198894500732422 0.09143427759408951
CurrentTrain: epoch  4, batch    53 | loss: 4.2903290Losses:  4.129190444946289 0.23137003183364868
CurrentTrain: epoch  4, batch    54 | loss: 4.3605604Losses:  4.215097427368164 0.22445696592330933
CurrentTrain: epoch  4, batch    55 | loss: 4.4395542Losses:  4.292556285858154 0.18652330338954926
CurrentTrain: epoch  4, batch    56 | loss: 4.4790797Losses:  4.245142936706543 0.22082556784152985
CurrentTrain: epoch  4, batch    57 | loss: 4.4659686Losses:  4.175888538360596 0.16910788416862488
CurrentTrain: epoch  4, batch    58 | loss: 4.3449965Losses:  4.157012462615967 0.06165776401758194
CurrentTrain: epoch  4, batch    59 | loss: 4.2186704Losses:  4.195594787597656 0.14911310374736786
CurrentTrain: epoch  4, batch    60 | loss: 4.3447080Losses:  4.383284568786621 0.16535653173923492
CurrentTrain: epoch  4, batch    61 | loss: 4.5486412Losses:  6.375909805297852 0.5681071281433105
CurrentTrain: epoch  4, batch    62 | loss: 6.9440169Losses:  4.271924018859863 0.16444280743598938
CurrentTrain: epoch  4, batch    63 | loss: 4.4363670Losses:  4.1118669509887695 0.1308058202266693
CurrentTrain: epoch  4, batch    64 | loss: 4.2426729Losses:  4.773726463317871 0.14350619912147522
CurrentTrain: epoch  4, batch    65 | loss: 4.9172325Losses:  4.846451282501221 0.11737577617168427
CurrentTrain: epoch  4, batch    66 | loss: 4.9638271Losses:  4.237821578979492 0.20576855540275574
CurrentTrain: epoch  4, batch    67 | loss: 4.4435902Losses:  4.223865509033203 0.17191758751869202
CurrentTrain: epoch  4, batch    68 | loss: 4.3957829Losses:  4.548139572143555 0.22125324606895447
CurrentTrain: epoch  4, batch    69 | loss: 4.7693930Losses:  4.311213493347168 0.07133269309997559
CurrentTrain: epoch  4, batch    70 | loss: 4.3825464Losses:  4.885308265686035 0.2090785801410675
CurrentTrain: epoch  4, batch    71 | loss: 5.0943871Losses:  4.318944454193115 0.13901543617248535
CurrentTrain: epoch  4, batch    72 | loss: 4.4579601Losses:  4.143929481506348 0.23371846973896027
CurrentTrain: epoch  4, batch    73 | loss: 4.3776479Losses:  4.089813232421875 0.13440628349781036
CurrentTrain: epoch  4, batch    74 | loss: 4.2242193Losses:  4.1591925621032715 0.1071726605296135
CurrentTrain: epoch  4, batch    75 | loss: 4.2663651Losses:  4.192549705505371 0.10589112341403961
CurrentTrain: epoch  4, batch    76 | loss: 4.2984409Losses:  4.419113636016846 0.2646872401237488
CurrentTrain: epoch  4, batch    77 | loss: 4.6838007Losses:  4.219411849975586 0.13832440972328186
CurrentTrain: epoch  4, batch    78 | loss: 4.3577361Losses:  4.457089424133301 0.19837313890457153
CurrentTrain: epoch  4, batch    79 | loss: 4.6554627Losses:  4.3854451179504395 0.1444840133190155
CurrentTrain: epoch  4, batch    80 | loss: 4.5299292Losses:  4.156075477600098 0.11270234733819962
CurrentTrain: epoch  4, batch    81 | loss: 4.2687778Losses:  4.204585075378418 0.09288343042135239
CurrentTrain: epoch  4, batch    82 | loss: 4.2974687Losses:  4.217623233795166 0.14264297485351562
CurrentTrain: epoch  4, batch    83 | loss: 4.3602662Losses:  4.091711044311523 0.16789060831069946
CurrentTrain: epoch  4, batch    84 | loss: 4.2596016Losses:  4.335020542144775 0.19796310365200043
CurrentTrain: epoch  4, batch    85 | loss: 4.5329838Losses:  4.263409614562988 0.0656072273850441
CurrentTrain: epoch  4, batch    86 | loss: 4.3290167Losses:  4.138075828552246 0.10375617444515228
CurrentTrain: epoch  4, batch    87 | loss: 4.2418318Losses:  4.2057695388793945 0.08984309434890747
CurrentTrain: epoch  4, batch    88 | loss: 4.2956128Losses:  4.256099700927734 0.22582991421222687
CurrentTrain: epoch  4, batch    89 | loss: 4.4819298Losses:  4.127444744110107 0.153543159365654
CurrentTrain: epoch  4, batch    90 | loss: 4.2809877Losses:  4.259052276611328 0.09868873655796051
CurrentTrain: epoch  4, batch    91 | loss: 4.3577409Losses:  4.1496782302856445 0.11991697549819946
CurrentTrain: epoch  4, batch    92 | loss: 4.2695951Losses:  4.124251365661621 0.14701366424560547
CurrentTrain: epoch  4, batch    93 | loss: 4.2712650Losses:  4.204492568969727 0.13827534019947052
CurrentTrain: epoch  4, batch    94 | loss: 4.3427677Losses:  4.291912078857422 0.21493837237358093
CurrentTrain: epoch  4, batch    95 | loss: 4.5068502Losses:  4.2220611572265625 0.17506152391433716
CurrentTrain: epoch  4, batch    96 | loss: 4.3971229Losses:  4.24077033996582 0.18582141399383545
CurrentTrain: epoch  4, batch    97 | loss: 4.4265919Losses:  4.107357025146484 0.14094603061676025
CurrentTrain: epoch  4, batch    98 | loss: 4.2483029Losses:  4.069836616516113 0.07726690173149109
CurrentTrain: epoch  4, batch    99 | loss: 4.1471033Losses:  4.173131942749023 0.13420218229293823
CurrentTrain: epoch  4, batch   100 | loss: 4.3073339Losses:  4.256139755249023 0.19366075098514557
CurrentTrain: epoch  4, batch   101 | loss: 4.4498005Losses:  4.057766437530518 0.13213808834552765
CurrentTrain: epoch  4, batch   102 | loss: 4.1899047Losses:  4.160708904266357 0.13061895966529846
CurrentTrain: epoch  4, batch   103 | loss: 4.2913280Losses:  5.053077697753906 0.29423201084136963
CurrentTrain: epoch  4, batch   104 | loss: 5.3473096Losses:  4.21463680267334 0.1115621030330658
CurrentTrain: epoch  4, batch   105 | loss: 4.3261991Losses:  4.109574794769287 0.06261365115642548
CurrentTrain: epoch  4, batch   106 | loss: 4.1721883Losses:  4.118539333343506 0.06739766150712967
CurrentTrain: epoch  4, batch   107 | loss: 4.1859369Losses:  4.101946830749512 0.11609097570180893
CurrentTrain: epoch  4, batch   108 | loss: 4.2180376Losses:  4.298611640930176 0.09806650876998901
CurrentTrain: epoch  4, batch   109 | loss: 4.3966780Losses:  4.154831409454346 0.17757809162139893
CurrentTrain: epoch  4, batch   110 | loss: 4.3324094Losses:  4.116878509521484 0.13553307950496674
CurrentTrain: epoch  4, batch   111 | loss: 4.2524114Losses:  4.151504039764404 0.21177369356155396
CurrentTrain: epoch  4, batch   112 | loss: 4.3632779Losses:  4.160734176635742 0.16550655663013458
CurrentTrain: epoch  4, batch   113 | loss: 4.3262405Losses:  4.189866065979004 0.14525872468948364
CurrentTrain: epoch  4, batch   114 | loss: 4.3351250Losses:  4.096130847930908 0.16872084140777588
CurrentTrain: epoch  4, batch   115 | loss: 4.2648516Losses:  4.141024589538574 0.14542503654956818
CurrentTrain: epoch  4, batch   116 | loss: 4.2864494Losses:  4.210248947143555 0.21846021711826324
CurrentTrain: epoch  4, batch   117 | loss: 4.4287090Losses:  4.172712802886963 0.13507817685604095
CurrentTrain: epoch  4, batch   118 | loss: 4.3077908Losses:  4.418282508850098 0.10711979866027832
CurrentTrain: epoch  4, batch   119 | loss: 4.5254021Losses:  4.156659126281738 0.0755356028676033
CurrentTrain: epoch  4, batch   120 | loss: 4.2321949Losses:  4.186257839202881 0.09713976085186005
CurrentTrain: epoch  4, batch   121 | loss: 4.2833977Losses:  4.100275039672852 0.151884987950325
CurrentTrain: epoch  4, batch   122 | loss: 4.2521601Losses:  4.104953289031982 0.056284040212631226
CurrentTrain: epoch  4, batch   123 | loss: 4.1612372Losses:  4.1745381355285645 0.1584799885749817
CurrentTrain: epoch  4, batch   124 | loss: 4.3330183Losses:  4.2774152755737305 0.1570367068052292
CurrentTrain: epoch  5, batch     0 | loss: 4.4344521Losses:  4.167348861694336 0.21147756278514862
CurrentTrain: epoch  5, batch     1 | loss: 4.3788266Losses:  4.105458736419678 0.14359310269355774
CurrentTrain: epoch  5, batch     2 | loss: 4.2490520Losses:  4.008700370788574 0.16138076782226562
CurrentTrain: epoch  5, batch     3 | loss: 4.1700811Losses:  4.148728370666504 0.17674854397773743
CurrentTrain: epoch  5, batch     4 | loss: 4.3254771Losses:  4.065398216247559 0.06641505658626556
CurrentTrain: epoch  5, batch     5 | loss: 4.1318130Losses:  4.159757614135742 0.07850813865661621
CurrentTrain: epoch  5, batch     6 | loss: 4.2382660Losses:  4.168008327484131 0.12032521516084671
CurrentTrain: epoch  5, batch     7 | loss: 4.2883334Losses:  4.088062763214111 0.13345691561698914
CurrentTrain: epoch  5, batch     8 | loss: 4.2215195Losses:  4.133474349975586 0.15587222576141357
CurrentTrain: epoch  5, batch     9 | loss: 4.2893467Losses:  4.1340131759643555 0.12394560873508453
CurrentTrain: epoch  5, batch    10 | loss: 4.2579589Losses:  4.088313102722168 0.11864222586154938
CurrentTrain: epoch  5, batch    11 | loss: 4.2069554Losses:  4.175325393676758 0.08010002225637436
CurrentTrain: epoch  5, batch    12 | loss: 4.2554255Losses:  4.203991889953613 0.09771066904067993
CurrentTrain: epoch  5, batch    13 | loss: 4.3017025Losses:  4.096946716308594 0.1169266477227211
CurrentTrain: epoch  5, batch    14 | loss: 4.2138734Losses:  4.218888282775879 0.0895124077796936
CurrentTrain: epoch  5, batch    15 | loss: 4.3084006Losses:  4.133556365966797 0.1331421285867691
CurrentTrain: epoch  5, batch    16 | loss: 4.2666984Losses:  4.144472122192383 0.25640738010406494
CurrentTrain: epoch  5, batch    17 | loss: 4.4008794Losses:  4.097625732421875 0.1139640361070633
CurrentTrain: epoch  5, batch    18 | loss: 4.2115898Losses:  4.17193603515625 0.07878106832504272
CurrentTrain: epoch  5, batch    19 | loss: 4.2507172Losses:  4.140594482421875 0.08294343948364258
CurrentTrain: epoch  5, batch    20 | loss: 4.2235379Losses:  4.053638935089111 0.1381842941045761
CurrentTrain: epoch  5, batch    21 | loss: 4.1918230Losses:  4.137568950653076 0.09158474206924438
CurrentTrain: epoch  5, batch    22 | loss: 4.2291536Losses:  4.220425605773926 0.11103928834199905
CurrentTrain: epoch  5, batch    23 | loss: 4.3314648Losses:  4.233279228210449 0.17565284669399261
CurrentTrain: epoch  5, batch    24 | loss: 4.4089322Losses:  4.117389678955078 0.09876574575901031
CurrentTrain: epoch  5, batch    25 | loss: 4.2161555Losses:  4.2613935470581055 0.1872044801712036
CurrentTrain: epoch  5, batch    26 | loss: 4.4485979Losses:  4.113313674926758 0.17851895093917847
CurrentTrain: epoch  5, batch    27 | loss: 4.2918324Losses:  4.254946708679199 0.1396811455488205
CurrentTrain: epoch  5, batch    28 | loss: 4.3946280Losses:  4.145761489868164 0.110537089407444
CurrentTrain: epoch  5, batch    29 | loss: 4.2562985Losses:  4.178689956665039 0.16680876910686493
CurrentTrain: epoch  5, batch    30 | loss: 4.3454986Losses:  4.142115592956543 0.24083031713962555
CurrentTrain: epoch  5, batch    31 | loss: 4.3829460Losses:  4.221596717834473 0.21388810873031616
CurrentTrain: epoch  5, batch    32 | loss: 4.4354849Losses:  4.133260250091553 0.17521551251411438
CurrentTrain: epoch  5, batch    33 | loss: 4.3084760Losses:  4.150045394897461 0.1691206395626068
CurrentTrain: epoch  5, batch    34 | loss: 4.3191662Losses:  4.152632713317871 0.14549940824508667
CurrentTrain: epoch  5, batch    35 | loss: 4.2981319Losses:  4.115489482879639 0.16256938874721527
CurrentTrain: epoch  5, batch    36 | loss: 4.2780590Losses:  4.131203651428223 0.20483721792697906
CurrentTrain: epoch  5, batch    37 | loss: 4.3360410Losses:  4.192233562469482 0.12853287160396576
CurrentTrain: epoch  5, batch    38 | loss: 4.3207664Losses:  4.166807651519775 0.13980767130851746
CurrentTrain: epoch  5, batch    39 | loss: 4.3066154Losses:  4.175296783447266 0.1540776789188385
CurrentTrain: epoch  5, batch    40 | loss: 4.3293743Losses:  4.186154365539551 0.13425567746162415
CurrentTrain: epoch  5, batch    41 | loss: 4.3204103Losses:  4.116249084472656 0.19909116625785828
CurrentTrain: epoch  5, batch    42 | loss: 4.3153400Losses:  4.112310409545898 0.18209496140480042
CurrentTrain: epoch  5, batch    43 | loss: 4.2944055Losses:  4.116655349731445 0.12021476030349731
CurrentTrain: epoch  5, batch    44 | loss: 4.2368703Losses:  4.165940284729004 0.08939008414745331
CurrentTrain: epoch  5, batch    45 | loss: 4.2553306Losses:  4.080314636230469 0.14985615015029907
CurrentTrain: epoch  5, batch    46 | loss: 4.2301707Losses:  4.044403076171875 0.20933368802070618
CurrentTrain: epoch  5, batch    47 | loss: 4.2537370Losses:  4.165190696716309 0.05668157339096069
CurrentTrain: epoch  5, batch    48 | loss: 4.2218723Losses:  4.095093727111816 0.12774553894996643
CurrentTrain: epoch  5, batch    49 | loss: 4.2228394Losses:  4.146672248840332 0.16817538440227509
CurrentTrain: epoch  5, batch    50 | loss: 4.3148475Losses:  4.089748859405518 0.18647605180740356
CurrentTrain: epoch  5, batch    51 | loss: 4.2762251Losses:  4.18358850479126 0.19762317836284637
CurrentTrain: epoch  5, batch    52 | loss: 4.3812118Losses:  4.057534694671631 0.0754837840795517
CurrentTrain: epoch  5, batch    53 | loss: 4.1330185Losses:  4.155259132385254 0.22177471220493317
CurrentTrain: epoch  5, batch    54 | loss: 4.3770337Losses:  4.137912750244141 0.10989636927843094
CurrentTrain: epoch  5, batch    55 | loss: 4.2478089Losses:  3.9935355186462402 0.0805414617061615
CurrentTrain: epoch  5, batch    56 | loss: 4.0740771Losses:  3.998967170715332 0.1310759335756302
CurrentTrain: epoch  5, batch    57 | loss: 4.1300430Losses:  4.1309614181518555 0.09221693873405457
CurrentTrain: epoch  5, batch    58 | loss: 4.2231784Losses:  4.150600433349609 0.1833345741033554
CurrentTrain: epoch  5, batch    59 | loss: 4.3339348Losses:  4.070913314819336 0.09546580910682678
CurrentTrain: epoch  5, batch    60 | loss: 4.1663790Losses:  4.126273155212402 0.0887746587395668
CurrentTrain: epoch  5, batch    61 | loss: 4.2150478Losses:  4.065874099731445 0.19309478998184204
CurrentTrain: epoch  5, batch    62 | loss: 4.2589688Losses:  4.10642671585083 0.1730097532272339
CurrentTrain: epoch  5, batch    63 | loss: 4.2794366Losses:  4.129990577697754 0.17042814195156097
CurrentTrain: epoch  5, batch    64 | loss: 4.3004189Losses:  4.063993453979492 0.21631106734275818
CurrentTrain: epoch  5, batch    65 | loss: 4.2803044Losses:  4.105695724487305 0.11539778858423233
CurrentTrain: epoch  5, batch    66 | loss: 4.2210937Losses:  4.067486763000488 0.1683695912361145
CurrentTrain: epoch  5, batch    67 | loss: 4.2358565Losses:  4.162180423736572 0.11399431526660919
CurrentTrain: epoch  5, batch    68 | loss: 4.2761745Losses:  4.127904891967773 0.13965952396392822
CurrentTrain: epoch  5, batch    69 | loss: 4.2675643Losses:  4.082953929901123 0.05690297484397888
CurrentTrain: epoch  5, batch    70 | loss: 4.1398568Losses:  4.061327934265137 0.10585707426071167
CurrentTrain: epoch  5, batch    71 | loss: 4.1671848Losses:  4.067503929138184 0.10217176377773285
CurrentTrain: epoch  5, batch    72 | loss: 4.1696758Losses:  4.080594062805176 0.14618214964866638
CurrentTrain: epoch  5, batch    73 | loss: 4.2267761Losses:  4.1166276931762695 0.15790584683418274
CurrentTrain: epoch  5, batch    74 | loss: 4.2745337Losses:  4.089092254638672 0.08641321212053299
CurrentTrain: epoch  5, batch    75 | loss: 4.1755056Losses:  4.853113651275635 0.2339189648628235
CurrentTrain: epoch  5, batch    76 | loss: 5.0870328Losses:  4.174999713897705 0.11470472067594528
CurrentTrain: epoch  5, batch    77 | loss: 4.2897043Losses:  4.084010124206543 0.16052666306495667
CurrentTrain: epoch  5, batch    78 | loss: 4.2445369Losses:  4.097938537597656 0.2300863265991211
CurrentTrain: epoch  5, batch    79 | loss: 4.3280249Losses:  4.106586456298828 0.142825186252594
CurrentTrain: epoch  5, batch    80 | loss: 4.2494116Losses:  4.0934553146362305 0.12099391222000122
CurrentTrain: epoch  5, batch    81 | loss: 4.2144494Losses:  4.1127824783325195 0.08270294964313507
CurrentTrain: epoch  5, batch    82 | loss: 4.1954856Losses:  4.173445224761963 0.07284688949584961
CurrentTrain: epoch  5, batch    83 | loss: 4.2462921Losses:  4.108719348907471 0.10790953785181046
CurrentTrain: epoch  5, batch    84 | loss: 4.2166290Losses:  4.054311752319336 0.16190765798091888
CurrentTrain: epoch  5, batch    85 | loss: 4.2162194Losses:  4.056851387023926 0.06664396822452545
CurrentTrain: epoch  5, batch    86 | loss: 4.1234956Losses:  4.106912136077881 0.09539680182933807
CurrentTrain: epoch  5, batch    87 | loss: 4.2023091Losses:  4.10145902633667 0.13468432426452637
CurrentTrain: epoch  5, batch    88 | loss: 4.2361431Losses:  4.042978763580322 0.08356788009405136
CurrentTrain: epoch  5, batch    89 | loss: 4.1265469Losses:  4.046607494354248 0.17933477461338043
CurrentTrain: epoch  5, batch    90 | loss: 4.2259421Losses:  4.105196952819824 0.09211385995149612
CurrentTrain: epoch  5, batch    91 | loss: 4.1973109Losses:  4.072784900665283 0.15264905989170074
CurrentTrain: epoch  5, batch    92 | loss: 4.2254338Losses:  4.090457916259766 0.16054093837738037
CurrentTrain: epoch  5, batch    93 | loss: 4.2509990Losses:  4.015866279602051 0.07509865611791611
CurrentTrain: epoch  5, batch    94 | loss: 4.0909648Losses:  4.025460243225098 0.07127969712018967
CurrentTrain: epoch  5, batch    95 | loss: 4.0967398Losses:  4.063962459564209 0.12048982828855515
CurrentTrain: epoch  5, batch    96 | loss: 4.1844521Losses:  4.004035949707031 0.13014063239097595
CurrentTrain: epoch  5, batch    97 | loss: 4.1341767Losses:  4.102190017700195 0.12679503858089447
CurrentTrain: epoch  5, batch    98 | loss: 4.2289848Losses:  4.130362510681152 0.16328488290309906
CurrentTrain: epoch  5, batch    99 | loss: 4.2936473Losses:  4.05063533782959 0.10159788280725479
CurrentTrain: epoch  5, batch   100 | loss: 4.1522331Losses:  4.116446495056152 0.17176848649978638
CurrentTrain: epoch  5, batch   101 | loss: 4.2882152Losses:  4.075598239898682 0.14490783214569092
CurrentTrain: epoch  5, batch   102 | loss: 4.2205062Losses:  3.9587323665618896 0.06849212944507599
CurrentTrain: epoch  5, batch   103 | loss: 4.0272245Losses:  4.069340705871582 0.18330514430999756
CurrentTrain: epoch  5, batch   104 | loss: 4.2526460Losses:  4.122133255004883 0.13408519327640533
CurrentTrain: epoch  5, batch   105 | loss: 4.2562184Losses:  4.0096235275268555 0.11754366755485535
CurrentTrain: epoch  5, batch   106 | loss: 4.1271672Losses:  4.082590103149414 0.14752015471458435
CurrentTrain: epoch  5, batch   107 | loss: 4.2301102Losses:  4.0921430587768555 0.16681841015815735
CurrentTrain: epoch  5, batch   108 | loss: 4.2589617Losses:  4.029145240783691 0.10384368896484375
CurrentTrain: epoch  5, batch   109 | loss: 4.1329889Losses:  4.056506156921387 0.14725977182388306
CurrentTrain: epoch  5, batch   110 | loss: 4.2037659Losses:  4.073110103607178 0.18904387950897217
CurrentTrain: epoch  5, batch   111 | loss: 4.2621541Losses:  4.032449722290039 0.11527378857135773
CurrentTrain: epoch  5, batch   112 | loss: 4.1477237Losses:  4.114378452301025 0.09191287308931351
CurrentTrain: epoch  5, batch   113 | loss: 4.2062912Losses:  4.064295768737793 0.2191160023212433
CurrentTrain: epoch  5, batch   114 | loss: 4.2834120Losses:  4.07235860824585 0.16384834051132202
CurrentTrain: epoch  5, batch   115 | loss: 4.2362070Losses:  4.094974994659424 0.13733017444610596
CurrentTrain: epoch  5, batch   116 | loss: 4.2323050Losses:  4.048635959625244 0.21224480867385864
CurrentTrain: epoch  5, batch   117 | loss: 4.2608809Losses:  4.062346458435059 0.077762171626091
CurrentTrain: epoch  5, batch   118 | loss: 4.1401086Losses:  4.17916202545166 0.09030171483755112
CurrentTrain: epoch  5, batch   119 | loss: 4.2694635Losses:  4.110489845275879 0.09510486572980881
CurrentTrain: epoch  5, batch   120 | loss: 4.2055945Losses:  4.077912330627441 0.12417743355035782
CurrentTrain: epoch  5, batch   121 | loss: 4.2020898Losses:  4.073676109313965 0.1286608874797821
CurrentTrain: epoch  5, batch   122 | loss: 4.2023368Losses:  4.067077159881592 0.05990677326917648
CurrentTrain: epoch  5, batch   123 | loss: 4.1269841Losses:  4.066120147705078 0.13350364565849304
CurrentTrain: epoch  5, batch   124 | loss: 4.1996236Losses:  4.03452730178833 0.17995847761631012
CurrentTrain: epoch  6, batch     0 | loss: 4.2144856Losses:  4.099674224853516 0.18377161026000977
CurrentTrain: epoch  6, batch     1 | loss: 4.2834458Losses:  4.075137615203857 0.1468544900417328
CurrentTrain: epoch  6, batch     2 | loss: 4.2219920Losses:  4.094252109527588 0.15335172414779663
CurrentTrain: epoch  6, batch     3 | loss: 4.2476039Losses:  4.12503719329834 0.1053035706281662
CurrentTrain: epoch  6, batch     4 | loss: 4.2303410Losses:  4.050227642059326 0.08126459270715714
CurrentTrain: epoch  6, batch     5 | loss: 4.1314921Losses:  4.05178165435791 0.11504130065441132
CurrentTrain: epoch  6, batch     6 | loss: 4.1668229Losses:  4.079207420349121 0.1285620629787445
CurrentTrain: epoch  6, batch     7 | loss: 4.2077694Losses:  4.070012092590332 0.10167954862117767
CurrentTrain: epoch  6, batch     8 | loss: 4.1716914Losses:  4.061945915222168 0.1418881118297577
CurrentTrain: epoch  6, batch     9 | loss: 4.2038341Losses:  4.065361022949219 0.08789320290088654
CurrentTrain: epoch  6, batch    10 | loss: 4.1532540Losses:  4.067288398742676 0.08274523913860321
CurrentTrain: epoch  6, batch    11 | loss: 4.1500335Losses:  4.097576141357422 0.10764580965042114
CurrentTrain: epoch  6, batch    12 | loss: 4.2052221Losses:  4.086938381195068 0.12003563344478607
CurrentTrain: epoch  6, batch    13 | loss: 4.2069740Losses:  4.006718158721924 0.12802693247795105
CurrentTrain: epoch  6, batch    14 | loss: 4.1347451Losses:  4.0797319412231445 0.11721709370613098
CurrentTrain: epoch  6, batch    15 | loss: 4.1969490Losses:  4.037830352783203 0.10752527415752411
CurrentTrain: epoch  6, batch    16 | loss: 4.1453557Losses:  4.095203876495361 0.09956815093755722
CurrentTrain: epoch  6, batch    17 | loss: 4.1947722Losses:  4.058695316314697 0.02268681302666664
CurrentTrain: epoch  6, batch    18 | loss: 4.0813823Losses:  4.14634370803833 0.1540200114250183
CurrentTrain: epoch  6, batch    19 | loss: 4.3003635Losses:  4.074545860290527 0.11420788615942001
CurrentTrain: epoch  6, batch    20 | loss: 4.1887536Losses:  4.050086975097656 0.07877838611602783
CurrentTrain: epoch  6, batch    21 | loss: 4.1288652Losses:  4.0432281494140625 0.21739625930786133
CurrentTrain: epoch  6, batch    22 | loss: 4.2606244Losses:  4.12007999420166 0.0758778303861618
CurrentTrain: epoch  6, batch    23 | loss: 4.1959577Losses:  4.121438026428223 0.09881030023097992
CurrentTrain: epoch  6, batch    24 | loss: 4.2202482Losses:  4.093444347381592 0.06230878084897995
CurrentTrain: epoch  6, batch    25 | loss: 4.1557531Losses:  3.9717509746551514 0.1800554096698761
CurrentTrain: epoch  6, batch    26 | loss: 4.1518064Losses:  4.0653862953186035 0.09341363608837128
CurrentTrain: epoch  6, batch    27 | loss: 4.1588001Losses:  4.042718887329102 0.12637440860271454
CurrentTrain: epoch  6, batch    28 | loss: 4.1690931Losses:  4.054716110229492 0.15752212703227997
CurrentTrain: epoch  6, batch    29 | loss: 4.2122383Losses:  4.109494209289551 0.08301563560962677
CurrentTrain: epoch  6, batch    30 | loss: 4.1925097Losses:  4.1283416748046875 0.06268446147441864
CurrentTrain: epoch  6, batch    31 | loss: 4.1910262Losses:  4.045869827270508 0.18611600995063782
CurrentTrain: epoch  6, batch    32 | loss: 4.2319860Losses:  4.053934574127197 0.14983215928077698
CurrentTrain: epoch  6, batch    33 | loss: 4.2037668Losses:  4.053109645843506 0.1331752985715866
CurrentTrain: epoch  6, batch    34 | loss: 4.1862850Losses:  4.071816444396973 0.129014253616333
CurrentTrain: epoch  6, batch    35 | loss: 4.2008305Losses:  4.050291061401367 0.1139453798532486
CurrentTrain: epoch  6, batch    36 | loss: 4.1642365Losses:  4.065058708190918 0.12242718040943146
CurrentTrain: epoch  6, batch    37 | loss: 4.1874857Losses:  4.081221580505371 0.1158694475889206
CurrentTrain: epoch  6, batch    38 | loss: 4.1970911Losses:  4.039515495300293 0.1329033523797989
CurrentTrain: epoch  6, batch    39 | loss: 4.1724191Losses:  4.031397819519043 0.14515243470668793
CurrentTrain: epoch  6, batch    40 | loss: 4.1765504Losses:  4.067018032073975 0.06211753934621811
CurrentTrain: epoch  6, batch    41 | loss: 4.1291356Losses:  4.02258825302124 0.13788411021232605
CurrentTrain: epoch  6, batch    42 | loss: 4.1604724Losses:  4.052883148193359 0.1598978042602539
CurrentTrain: epoch  6, batch    43 | loss: 4.2127810Losses:  4.084543228149414 0.09098611772060394
CurrentTrain: epoch  6, batch    44 | loss: 4.1755295Losses:  4.062596797943115 0.12417305260896683
CurrentTrain: epoch  6, batch    45 | loss: 4.1867700Losses:  4.08483362197876 0.10931289196014404
CurrentTrain: epoch  6, batch    46 | loss: 4.1941466Losses:  4.109321594238281 0.12984812259674072
CurrentTrain: epoch  6, batch    47 | loss: 4.2391696Losses:  3.990499496459961 0.05901169031858444
CurrentTrain: epoch  6, batch    48 | loss: 4.0495110Losses:  4.03209114074707 0.11420176923274994
CurrentTrain: epoch  6, batch    49 | loss: 4.1462927Losses:  4.071496963500977 0.0945468470454216
CurrentTrain: epoch  6, batch    50 | loss: 4.1660438Losses:  4.0223894119262695 0.08650118112564087
CurrentTrain: epoch  6, batch    51 | loss: 4.1088905Losses:  4.076904296875 0.10252713412046432
CurrentTrain: epoch  6, batch    52 | loss: 4.1794314Losses:  4.121824264526367 0.0986020639538765
CurrentTrain: epoch  6, batch    53 | loss: 4.2204266Losses:  4.096063613891602 0.09418046474456787
CurrentTrain: epoch  6, batch    54 | loss: 4.1902442Losses:  4.0576324462890625 0.1036098524928093
CurrentTrain: epoch  6, batch    55 | loss: 4.1612425Losses:  4.033787727355957 0.094277024269104
CurrentTrain: epoch  6, batch    56 | loss: 4.1280646Losses:  4.104794025421143 0.15460436046123505
CurrentTrain: epoch  6, batch    57 | loss: 4.2593985Losses:  4.041894435882568 0.10065564513206482
CurrentTrain: epoch  6, batch    58 | loss: 4.1425500Losses:  4.069204807281494 0.10322852432727814
CurrentTrain: epoch  6, batch    59 | loss: 4.1724334Losses:  4.064121246337891 0.08765119314193726
CurrentTrain: epoch  6, batch    60 | loss: 4.1517725Losses:  4.0669755935668945 0.20263542234897614
CurrentTrain: epoch  6, batch    61 | loss: 4.2696109Losses:  3.935980796813965 0.04818993806838989
CurrentTrain: epoch  6, batch    62 | loss: 3.9841707Losses:  4.014771461486816 0.11305293440818787
CurrentTrain: epoch  6, batch    63 | loss: 4.1278243Losses:  4.0704240798950195 0.12701721489429474
CurrentTrain: epoch  6, batch    64 | loss: 4.1974411Losses:  3.950470447540283 0.05008525401353836
CurrentTrain: epoch  6, batch    65 | loss: 4.0005555Losses:  4.079209327697754 0.10470837354660034
CurrentTrain: epoch  6, batch    66 | loss: 4.1839175Losses:  4.045212268829346 0.0833759605884552
CurrentTrain: epoch  6, batch    67 | loss: 4.1285882Losses:  4.0657196044921875 0.061850737780332565
CurrentTrain: epoch  6, batch    68 | loss: 4.1275702Losses:  4.012076377868652 0.06440097093582153
CurrentTrain: epoch  6, batch    69 | loss: 4.0764775Losses:  3.9858264923095703 0.07368959486484528
CurrentTrain: epoch  6, batch    70 | loss: 4.0595160Losses:  4.014991760253906 0.15074680745601654
CurrentTrain: epoch  6, batch    71 | loss: 4.1657386Losses:  4.112825393676758 0.12026991695165634
CurrentTrain: epoch  6, batch    72 | loss: 4.2330952Losses:  4.1111955642700195 0.09564489126205444
CurrentTrain: epoch  6, batch    73 | loss: 4.2068405Losses:  4.02061128616333 0.16899099946022034
CurrentTrain: epoch  6, batch    74 | loss: 4.1896024Losses:  4.020057201385498 0.10090447962284088
CurrentTrain: epoch  6, batch    75 | loss: 4.1209617Losses:  4.016772270202637 0.1107226088643074
CurrentTrain: epoch  6, batch    76 | loss: 4.1274948Losses:  4.044020652770996 0.06367285549640656
CurrentTrain: epoch  6, batch    77 | loss: 4.1076937Losses:  4.061738014221191 0.14313149452209473
CurrentTrain: epoch  6, batch    78 | loss: 4.2048693Losses:  3.969790458679199 0.11190208047628403
CurrentTrain: epoch  6, batch    79 | loss: 4.0816927Losses:  4.078253746032715 0.10383059829473495
CurrentTrain: epoch  6, batch    80 | loss: 4.1820846Losses:  4.051529884338379 0.06912649422883987
CurrentTrain: epoch  6, batch    81 | loss: 4.1206565Losses:  4.044562816619873 0.1311194896697998
CurrentTrain: epoch  6, batch    82 | loss: 4.1756821Losses:  4.044125080108643 0.14986065030097961
CurrentTrain: epoch  6, batch    83 | loss: 4.1939859Losses:  4.003183364868164 0.07727042585611343
CurrentTrain: epoch  6, batch    84 | loss: 4.0804539Losses:  4.047124862670898 0.1702972650527954
CurrentTrain: epoch  6, batch    85 | loss: 4.2174220Losses:  4.071160793304443 0.08841642737388611
CurrentTrain: epoch  6, batch    86 | loss: 4.1595774Losses:  4.0063796043396 0.16500893235206604
CurrentTrain: epoch  6, batch    87 | loss: 4.1713886Losses:  4.032790660858154 0.06871400773525238
CurrentTrain: epoch  6, batch    88 | loss: 4.1015048Losses:  4.040675163269043 0.08937442302703857
CurrentTrain: epoch  6, batch    89 | loss: 4.1300497Losses:  4.037676811218262 0.09465046226978302
CurrentTrain: epoch  6, batch    90 | loss: 4.1323271Losses:  4.025521278381348 0.13280940055847168
CurrentTrain: epoch  6, batch    91 | loss: 4.1583309Losses:  4.059393882751465 0.04414293169975281
CurrentTrain: epoch  6, batch    92 | loss: 4.1035366Losses:  4.025590419769287 0.11707635223865509
CurrentTrain: epoch  6, batch    93 | loss: 4.1426668Losses:  4.058791160583496 0.18062296509742737
CurrentTrain: epoch  6, batch    94 | loss: 4.2394142Losses:  4.023031234741211 0.04801628366112709
CurrentTrain: epoch  6, batch    95 | loss: 4.0710473Losses:  4.036184310913086 0.09296154975891113
CurrentTrain: epoch  6, batch    96 | loss: 4.1291456Losses:  3.974778175354004 0.07799829542636871
CurrentTrain: epoch  6, batch    97 | loss: 4.0527763Losses:  3.9830408096313477 0.09430006146430969
CurrentTrain: epoch  6, batch    98 | loss: 4.0773411Losses:  3.982522964477539 0.14410951733589172
CurrentTrain: epoch  6, batch    99 | loss: 4.1266327Losses:  4.075163841247559 0.06687616556882858
CurrentTrain: epoch  6, batch   100 | loss: 4.1420398Losses:  4.008419036865234 0.15582431852817535
CurrentTrain: epoch  6, batch   101 | loss: 4.1642432Losses:  4.038506507873535 0.09590242058038712
CurrentTrain: epoch  6, batch   102 | loss: 4.1344090Losses:  4.127035140991211 0.05842011421918869
CurrentTrain: epoch  6, batch   103 | loss: 4.1854553Losses:  4.094437599182129 0.05077941715717316
CurrentTrain: epoch  6, batch   104 | loss: 4.1452169Losses:  4.038612365722656 0.07380189746618271
CurrentTrain: epoch  6, batch   105 | loss: 4.1124144Losses:  3.9969747066497803 0.12560583651065826
CurrentTrain: epoch  6, batch   106 | loss: 4.1225805Losses:  4.012455940246582 0.11167409271001816
CurrentTrain: epoch  6, batch   107 | loss: 4.1241302Losses:  4.007515907287598 0.07882317155599594
CurrentTrain: epoch  6, batch   108 | loss: 4.0863390Losses:  4.110805511474609 0.08802930265665054
CurrentTrain: epoch  6, batch   109 | loss: 4.1988349Losses:  4.0560736656188965 0.11382277309894562
CurrentTrain: epoch  6, batch   110 | loss: 4.1698966Losses:  4.063110828399658 0.11817141622304916
CurrentTrain: epoch  6, batch   111 | loss: 4.1812820Losses:  4.035333156585693 0.12755317986011505
CurrentTrain: epoch  6, batch   112 | loss: 4.1628861Losses:  4.011594772338867 0.09242570400238037
CurrentTrain: epoch  6, batch   113 | loss: 4.1040206Losses:  4.048365592956543 0.10123269259929657
CurrentTrain: epoch  6, batch   114 | loss: 4.1495981Losses:  3.9344308376312256 0.0643220841884613
CurrentTrain: epoch  6, batch   115 | loss: 3.9987528Losses:  3.988274335861206 0.13079975545406342
CurrentTrain: epoch  6, batch   116 | loss: 4.1190739Losses:  4.0126142501831055 0.11555816233158112
CurrentTrain: epoch  6, batch   117 | loss: 4.1281724Losses:  3.9998319149017334 0.11498246341943741
CurrentTrain: epoch  6, batch   118 | loss: 4.1148143Losses:  4.097868919372559 0.08787210285663605
CurrentTrain: epoch  6, batch   119 | loss: 4.1857409Losses:  4.047076225280762 0.06869477778673172
CurrentTrain: epoch  6, batch   120 | loss: 4.1157708Losses:  3.9480092525482178 0.0597030371427536
CurrentTrain: epoch  6, batch   121 | loss: 4.0077124Losses:  4.22256326675415 0.19720101356506348
CurrentTrain: epoch  6, batch   122 | loss: 4.4197645Losses:  3.9864399433135986 0.1581742912530899
CurrentTrain: epoch  6, batch   123 | loss: 4.1446142Losses:  4.024392127990723 0.09499241411685944
CurrentTrain: epoch  6, batch   124 | loss: 4.1193848Losses:  4.014168739318848 0.1822904795408249
CurrentTrain: epoch  7, batch     0 | loss: 4.1964593Losses:  4.103102684020996 0.1420518010854721
CurrentTrain: epoch  7, batch     1 | loss: 4.2451544Losses:  3.9936578273773193 0.11374042928218842
CurrentTrain: epoch  7, batch     2 | loss: 4.1073980Losses:  3.9785399436950684 0.0810677707195282
CurrentTrain: epoch  7, batch     3 | loss: 4.0596075Losses:  4.063078880310059 0.14511115849018097
CurrentTrain: epoch  7, batch     4 | loss: 4.2081900Losses:  4.00543212890625 0.08368787169456482
CurrentTrain: epoch  7, batch     5 | loss: 4.0891199Losses:  4.118897438049316 0.10218726098537445
CurrentTrain: epoch  7, batch     6 | loss: 4.2210846Losses:  3.9531452655792236 0.057748038321733475
CurrentTrain: epoch  7, batch     7 | loss: 4.0108933Losses:  4.047304630279541 0.09028192609548569
CurrentTrain: epoch  7, batch     8 | loss: 4.1375866Losses:  4.1075944900512695 0.08329804241657257
CurrentTrain: epoch  7, batch     9 | loss: 4.1908927Losses:  4.029107093811035 0.1287592500448227
CurrentTrain: epoch  7, batch    10 | loss: 4.1578665Losses:  4.0489301681518555 0.06159253045916557
CurrentTrain: epoch  7, batch    11 | loss: 4.1105227Losses:  4.093400955200195 0.06127328798174858
CurrentTrain: epoch  7, batch    12 | loss: 4.1546741Losses:  4.077642440795898 0.08772911876440048
CurrentTrain: epoch  7, batch    13 | loss: 4.1653714Losses:  4.062019348144531 0.07628224790096283
CurrentTrain: epoch  7, batch    14 | loss: 4.1383014Losses:  4.040058135986328 0.12432079017162323
CurrentTrain: epoch  7, batch    15 | loss: 4.1643791Losses:  4.060976028442383 0.09062979370355606
CurrentTrain: epoch  7, batch    16 | loss: 4.1516056Losses:  4.033665657043457 0.09922192990779877
CurrentTrain: epoch  7, batch    17 | loss: 4.1328874Losses:  4.926873683929443 0.2393503189086914
CurrentTrain: epoch  7, batch    18 | loss: 5.1662240Losses:  3.985927104949951 0.09482181072235107
CurrentTrain: epoch  7, batch    19 | loss: 4.0807490Losses:  4.023194313049316 0.14991717040538788
CurrentTrain: epoch  7, batch    20 | loss: 4.1731114Losses:  4.00766658782959 0.09524901211261749
CurrentTrain: epoch  7, batch    21 | loss: 4.1029158Losses:  3.9821319580078125 0.13963904976844788
CurrentTrain: epoch  7, batch    22 | loss: 4.1217709Losses:  3.928760528564453 0.07697119563817978
CurrentTrain: epoch  7, batch    23 | loss: 4.0057316Losses:  4.061342239379883 0.07932858914136887
CurrentTrain: epoch  7, batch    24 | loss: 4.1406708Losses:  4.025801181793213 0.06200650334358215
CurrentTrain: epoch  7, batch    25 | loss: 4.0878077Losses:  4.012761116027832 0.12920023500919342
CurrentTrain: epoch  7, batch    26 | loss: 4.1419616Losses:  4.09295654296875 0.11271677166223526
CurrentTrain: epoch  7, batch    27 | loss: 4.2056732Losses:  4.072286605834961 0.13372257351875305
CurrentTrain: epoch  7, batch    28 | loss: 4.2060094Losses:  4.087787628173828 0.09773796796798706
CurrentTrain: epoch  7, batch    29 | loss: 4.1855254Losses:  4.027344703674316 0.05712208151817322
CurrentTrain: epoch  7, batch    30 | loss: 4.0844669Losses:  3.9766271114349365 0.08035069704055786
CurrentTrain: epoch  7, batch    31 | loss: 4.0569777Losses:  4.067650318145752 0.08151523768901825
CurrentTrain: epoch  7, batch    32 | loss: 4.1491656Losses:  4.1285295486450195 0.1051713302731514
CurrentTrain: epoch  7, batch    33 | loss: 4.2337008Losses:  4.023138523101807 0.15466324985027313
CurrentTrain: epoch  7, batch    34 | loss: 4.1778016Losses:  4.0796098709106445 0.11203818768262863
CurrentTrain: epoch  7, batch    35 | loss: 4.1916480Losses:  4.067361354827881 0.05388578400015831
CurrentTrain: epoch  7, batch    36 | loss: 4.1212473Losses:  4.0934672355651855 0.029728610068559647
CurrentTrain: epoch  7, batch    37 | loss: 4.1231956Losses:  3.998959541320801 0.21036452054977417
CurrentTrain: epoch  7, batch    38 | loss: 4.2093239Losses:  3.9831557273864746 0.09065674245357513
CurrentTrain: epoch  7, batch    39 | loss: 4.0738125Losses:  4.026105880737305 0.018495935946702957
CurrentTrain: epoch  7, batch    40 | loss: 4.0446019Losses:  4.005626678466797 0.10959187150001526
CurrentTrain: epoch  7, batch    41 | loss: 4.1152186Losses:  3.988163471221924 0.06839105486869812
CurrentTrain: epoch  7, batch    42 | loss: 4.0565543Losses:  3.9253897666931152 0.14700999855995178
CurrentTrain: epoch  7, batch    43 | loss: 4.0723996Losses:  4.005914688110352 0.10474658012390137
CurrentTrain: epoch  7, batch    44 | loss: 4.1106615Losses:  4.078460216522217 0.1468820571899414
CurrentTrain: epoch  7, batch    45 | loss: 4.2253423Losses:  4.088501930236816 0.10844606161117554
CurrentTrain: epoch  7, batch    46 | loss: 4.1969481Losses:  4.063652515411377 0.08587704598903656
CurrentTrain: epoch  7, batch    47 | loss: 4.1495295Losses:  4.0490312576293945 0.09717585146427155
CurrentTrain: epoch  7, batch    48 | loss: 4.1462073Losses:  4.047025203704834 0.0750708281993866
CurrentTrain: epoch  7, batch    49 | loss: 4.1220961Losses:  4.003846168518066 0.1270763874053955
CurrentTrain: epoch  7, batch    50 | loss: 4.1309223Losses:  4.05332088470459 0.07598888874053955
CurrentTrain: epoch  7, batch    51 | loss: 4.1293097Losses:  4.054636478424072 0.15626558661460876
CurrentTrain: epoch  7, batch    52 | loss: 4.2109022Losses:  3.9529240131378174 0.09697221219539642
CurrentTrain: epoch  7, batch    53 | loss: 4.0498962Losses:  3.9550905227661133 0.08602141588926315
CurrentTrain: epoch  7, batch    54 | loss: 4.0411119Losses:  4.030050277709961 0.04740668833255768
CurrentTrain: epoch  7, batch    55 | loss: 4.0774570Losses:  4.9494452476501465 0.2796090543270111
CurrentTrain: epoch  7, batch    56 | loss: 5.2290545Losses:  4.031376838684082 0.1337161660194397
CurrentTrain: epoch  7, batch    57 | loss: 4.1650929Losses:  4.01529598236084 0.08668842911720276
CurrentTrain: epoch  7, batch    58 | loss: 4.1019845Losses:  4.053533554077148 0.06875413656234741
CurrentTrain: epoch  7, batch    59 | loss: 4.1222878Losses:  4.060961723327637 0.06907360255718231
CurrentTrain: epoch  7, batch    60 | loss: 4.1300354Losses:  4.025322914123535 0.08635245263576508
CurrentTrain: epoch  7, batch    61 | loss: 4.1116753Losses:  3.997019052505493 0.125054270029068
CurrentTrain: epoch  7, batch    62 | loss: 4.1220732Losses:  4.059925079345703 0.13843205571174622
CurrentTrain: epoch  7, batch    63 | loss: 4.1983571Losses:  4.014621257781982 0.15979832410812378
CurrentTrain: epoch  7, batch    64 | loss: 4.1744194Losses:  3.956526041030884 0.07523900270462036
CurrentTrain: epoch  7, batch    65 | loss: 4.0317650Losses:  4.23172664642334 0.1904243528842926
CurrentTrain: epoch  7, batch    66 | loss: 4.4221511Losses:  4.077764511108398 0.06538033485412598
CurrentTrain: epoch  7, batch    67 | loss: 4.1431446Losses:  4.391186714172363 0.12043914198875427
CurrentTrain: epoch  7, batch    68 | loss: 4.5116258Losses:  4.02796745300293 0.08659551292657852
CurrentTrain: epoch  7, batch    69 | loss: 4.1145630Losses:  4.038940906524658 0.13406869769096375
CurrentTrain: epoch  7, batch    70 | loss: 4.1730094Losses:  4.02505350112915 0.10177004337310791
CurrentTrain: epoch  7, batch    71 | loss: 4.1268234Losses:  4.0163655281066895 0.04302649572491646
CurrentTrain: epoch  7, batch    72 | loss: 4.0593920Losses:  4.089940547943115 0.09073704481124878
CurrentTrain: epoch  7, batch    73 | loss: 4.1806774Losses:  4.035358428955078 0.13735447824001312
CurrentTrain: epoch  7, batch    74 | loss: 4.1727128Losses:  4.018722057342529 0.12425492703914642
CurrentTrain: epoch  7, batch    75 | loss: 4.1429768Losses:  4.036185264587402 0.09971937537193298
CurrentTrain: epoch  7, batch    76 | loss: 4.1359048Losses:  4.043206214904785 0.1627684235572815
CurrentTrain: epoch  7, batch    77 | loss: 4.2059746Losses:  3.997897148132324 0.1139475479722023
CurrentTrain: epoch  7, batch    78 | loss: 4.1118445Losses:  3.961132287979126 0.08458799123764038
CurrentTrain: epoch  7, batch    79 | loss: 4.0457201Losses:  4.062887191772461 0.15046894550323486
CurrentTrain: epoch  7, batch    80 | loss: 4.2133560Losses:  4.071431636810303 0.1436222642660141
CurrentTrain: epoch  7, batch    81 | loss: 4.2150540Losses:  4.011458873748779 0.0952778309583664
CurrentTrain: epoch  7, batch    82 | loss: 4.1067367Losses:  4.003046035766602 0.12194201350212097
CurrentTrain: epoch  7, batch    83 | loss: 4.1249881Losses:  4.101284027099609 0.08560509979724884
CurrentTrain: epoch  7, batch    84 | loss: 4.1868892Losses:  3.9796600341796875 0.05143379047513008
CurrentTrain: epoch  7, batch    85 | loss: 4.0310936Losses:  4.003431797027588 0.06868673115968704
CurrentTrain: epoch  7, batch    86 | loss: 4.0721188Losses:  3.9785261154174805 0.1034056693315506
CurrentTrain: epoch  7, batch    87 | loss: 4.0819316Losses:  3.977562427520752 0.05691149830818176
CurrentTrain: epoch  7, batch    88 | loss: 4.0344739Losses:  4.123144149780273 0.12432198226451874
CurrentTrain: epoch  7, batch    89 | loss: 4.2474661Losses:  4.162889003753662 0.06142555922269821
CurrentTrain: epoch  7, batch    90 | loss: 4.2243147Losses:  4.119589805603027 0.08646829426288605
CurrentTrain: epoch  7, batch    91 | loss: 4.2060580Losses:  4.008871078491211 0.08083908259868622
CurrentTrain: epoch  7, batch    92 | loss: 4.0897102Losses:  3.946699857711792 0.08979619294404984
CurrentTrain: epoch  7, batch    93 | loss: 4.0364962Losses:  4.160813331604004 0.11512912809848785
CurrentTrain: epoch  7, batch    94 | loss: 4.2759423Losses:  4.0527777671813965 0.1141909509897232
CurrentTrain: epoch  7, batch    95 | loss: 4.1669688Losses:  4.018167972564697 0.07569609582424164
CurrentTrain: epoch  7, batch    96 | loss: 4.0938640Losses:  4.126992225646973 0.06876400858163834
CurrentTrain: epoch  7, batch    97 | loss: 4.1957564Losses:  4.040349006652832 0.08092307299375534
CurrentTrain: epoch  7, batch    98 | loss: 4.1212721Losses:  4.033152103424072 0.07715708762407303
CurrentTrain: epoch  7, batch    99 | loss: 4.1103091Losses:  4.043540954589844 0.07967441529035568
CurrentTrain: epoch  7, batch   100 | loss: 4.1232152Losses:  3.994475841522217 0.1057559922337532
CurrentTrain: epoch  7, batch   101 | loss: 4.1002316Losses:  4.001931667327881 0.08485473692417145
CurrentTrain: epoch  7, batch   102 | loss: 4.0867863Losses:  4.051421165466309 0.09432540833950043
CurrentTrain: epoch  7, batch   103 | loss: 4.1457467Losses:  4.136163234710693 0.04064440354704857
CurrentTrain: epoch  7, batch   104 | loss: 4.1768074Losses:  3.9808549880981445 0.11458621919155121
CurrentTrain: epoch  7, batch   105 | loss: 4.0954413Losses:  4.004016399383545 0.11428496241569519
CurrentTrain: epoch  7, batch   106 | loss: 4.1183014Losses:  4.158557891845703 0.061721816658973694
CurrentTrain: epoch  7, batch   107 | loss: 4.2202797Losses:  3.995821475982666 0.0731142908334732
CurrentTrain: epoch  7, batch   108 | loss: 4.0689359Losses:  3.9957098960876465 0.09381662309169769
CurrentTrain: epoch  7, batch   109 | loss: 4.0895267Losses:  4.0320563316345215 0.04624463617801666
CurrentTrain: epoch  7, batch   110 | loss: 4.0783010Losses:  4.06821346282959 0.13433212041854858
CurrentTrain: epoch  7, batch   111 | loss: 4.2025456Losses:  4.063177585601807 0.07536262273788452
CurrentTrain: epoch  7, batch   112 | loss: 4.1385403Losses:  4.0563154220581055 0.0838918536901474
CurrentTrain: epoch  7, batch   113 | loss: 4.1402073Losses:  4.015257835388184 0.08145633339881897
CurrentTrain: epoch  7, batch   114 | loss: 4.0967140Losses:  3.9532275199890137 0.05177821218967438
CurrentTrain: epoch  7, batch   115 | loss: 4.0050058Losses:  4.055654525756836 0.09080018848180771
CurrentTrain: epoch  7, batch   116 | loss: 4.1464548Losses:  3.9870548248291016 0.06537789851427078
CurrentTrain: epoch  7, batch   117 | loss: 4.0524325Losses:  4.092905044555664 0.09281647950410843
CurrentTrain: epoch  7, batch   118 | loss: 4.1857214Losses:  3.99847674369812 0.1214626282453537
CurrentTrain: epoch  7, batch   119 | loss: 4.1199393Losses:  4.195438385009766 0.09799942374229431
CurrentTrain: epoch  7, batch   120 | loss: 4.2934380Losses:  4.152965545654297 0.15519262850284576
CurrentTrain: epoch  7, batch   121 | loss: 4.3081584Losses:  4.073672771453857 0.12560424208641052
CurrentTrain: epoch  7, batch   122 | loss: 4.1992769Losses:  4.050749778747559 0.05339127033948898
CurrentTrain: epoch  7, batch   123 | loss: 4.1041412Losses:  3.995818614959717 0.07314739376306534
CurrentTrain: epoch  7, batch   124 | loss: 4.0689659Losses:  4.031939506530762 0.1174018606543541
CurrentTrain: epoch  8, batch     0 | loss: 4.1493416Losses:  4.036055088043213 0.08775756508111954
CurrentTrain: epoch  8, batch     1 | loss: 4.1238127Losses:  4.016794204711914 0.08038586378097534
CurrentTrain: epoch  8, batch     2 | loss: 4.0971799Losses:  4.043229103088379 0.049031421542167664
CurrentTrain: epoch  8, batch     3 | loss: 4.0922604Losses:  4.053539276123047 0.0626724436879158
CurrentTrain: epoch  8, batch     4 | loss: 4.1162119Losses:  4.030021667480469 0.10376350581645966
CurrentTrain: epoch  8, batch     5 | loss: 4.1337852Losses:  4.069486618041992 0.11183620244264603
CurrentTrain: epoch  8, batch     6 | loss: 4.1813231Losses:  4.025571346282959 0.12968581914901733
CurrentTrain: epoch  8, batch     7 | loss: 4.1552572Losses:  4.0151166915893555 0.08484061062335968
CurrentTrain: epoch  8, batch     8 | loss: 4.0999575Losses:  4.036235809326172 0.0715067982673645
CurrentTrain: epoch  8, batch     9 | loss: 4.1077428Losses:  3.9845504760742188 0.1356307715177536
CurrentTrain: epoch  8, batch    10 | loss: 4.1201811Losses:  3.9623355865478516 0.11587022989988327
CurrentTrain: epoch  8, batch    11 | loss: 4.0782056Losses:  4.0668792724609375 0.06559242308139801
CurrentTrain: epoch  8, batch    12 | loss: 4.1324716Losses:  4.036099433898926 0.14545151591300964
CurrentTrain: epoch  8, batch    13 | loss: 4.1815510Losses:  3.9995617866516113 0.09019820392131805
CurrentTrain: epoch  8, batch    14 | loss: 4.0897598Losses:  3.996817111968994 0.08972211927175522
CurrentTrain: epoch  8, batch    15 | loss: 4.0865393Losses:  4.905206680297852 0.16630476713180542
CurrentTrain: epoch  8, batch    16 | loss: 5.0715113Losses:  4.016079425811768 0.1207394003868103
CurrentTrain: epoch  8, batch    17 | loss: 4.1368189Losses:  4.065440654754639 0.0991438627243042
CurrentTrain: epoch  8, batch    18 | loss: 4.1645846Losses:  4.046286582946777 0.10935181379318237
CurrentTrain: epoch  8, batch    19 | loss: 4.1556382Losses:  4.053179740905762 0.08937215059995651
CurrentTrain: epoch  8, batch    20 | loss: 4.1425519Losses:  3.964271068572998 0.08588828146457672
CurrentTrain: epoch  8, batch    21 | loss: 4.0501595Losses:  4.092809677124023 0.10892249643802643
CurrentTrain: epoch  8, batch    22 | loss: 4.2017322Losses:  4.061506271362305 0.08481381833553314
CurrentTrain: epoch  8, batch    23 | loss: 4.1463199Losses:  4.0576276779174805 0.07105518877506256
CurrentTrain: epoch  8, batch    24 | loss: 4.1286831Losses:  3.9999608993530273 0.07456877827644348
CurrentTrain: epoch  8, batch    25 | loss: 4.0745296Losses:  4.013507843017578 0.06964141130447388
CurrentTrain: epoch  8, batch    26 | loss: 4.0831494Losses:  4.0298309326171875 0.10204192996025085
CurrentTrain: epoch  8, batch    27 | loss: 4.1318727Losses:  4.058224678039551 0.056987322866916656
CurrentTrain: epoch  8, batch    28 | loss: 4.1152120Losses:  4.004727363586426 0.08815626800060272
CurrentTrain: epoch  8, batch    29 | loss: 4.0928836Losses:  4.006839752197266 0.11867079138755798
CurrentTrain: epoch  8, batch    30 | loss: 4.1255107Losses:  4.001768112182617 0.07761909067630768
CurrentTrain: epoch  8, batch    31 | loss: 4.0793872Losses:  3.992642402648926 0.10125620663166046
CurrentTrain: epoch  8, batch    32 | loss: 4.0938988Losses:  4.005182266235352 0.0914103239774704
CurrentTrain: epoch  8, batch    33 | loss: 4.0965924Losses:  4.278255462646484 0.05710296332836151
CurrentTrain: epoch  8, batch    34 | loss: 4.3353586Losses:  4.013416767120361 0.10789493471384048
CurrentTrain: epoch  8, batch    35 | loss: 4.1213117Losses:  4.066941738128662 0.05527336522936821
CurrentTrain: epoch  8, batch    36 | loss: 4.1222153Losses:  4.0676069259643555 0.1246543675661087
CurrentTrain: epoch  8, batch    37 | loss: 4.1922612Losses:  4.008508682250977 0.08655370026826859
CurrentTrain: epoch  8, batch    38 | loss: 4.0950623Losses:  3.982377767562866 0.09671109914779663
CurrentTrain: epoch  8, batch    39 | loss: 4.0790887Losses:  4.028586387634277 0.09055258333683014
CurrentTrain: epoch  8, batch    40 | loss: 4.1191392Losses:  4.027338981628418 0.06889283657073975
CurrentTrain: epoch  8, batch    41 | loss: 4.0962319Losses:  3.998450756072998 0.06317253410816193
CurrentTrain: epoch  8, batch    42 | loss: 4.0616231Losses:  4.086977958679199 0.09867869317531586
CurrentTrain: epoch  8, batch    43 | loss: 4.1856565Losses:  4.00514554977417 0.07000002264976501
CurrentTrain: epoch  8, batch    44 | loss: 4.0751457Losses:  4.0453362464904785 0.06656059622764587
CurrentTrain: epoch  8, batch    45 | loss: 4.1118970Losses:  4.00479793548584 0.0810946375131607
CurrentTrain: epoch  8, batch    46 | loss: 4.0858927Losses:  4.085211753845215 0.06404100358486176
CurrentTrain: epoch  8, batch    47 | loss: 4.1492529Losses:  3.999035358428955 0.044512875378131866
CurrentTrain: epoch  8, batch    48 | loss: 4.0435481Losses:  4.027036666870117 0.0800820142030716
CurrentTrain: epoch  8, batch    49 | loss: 4.1071186Losses:  4.24532413482666 0.08903263509273529
CurrentTrain: epoch  8, batch    50 | loss: 4.3343568Losses:  4.02678108215332 0.06517636775970459
CurrentTrain: epoch  8, batch    51 | loss: 4.0919576Losses:  4.036169052124023 0.06793022900819778
CurrentTrain: epoch  8, batch    52 | loss: 4.1040993Losses:  3.991518974304199 0.05576460435986519
CurrentTrain: epoch  8, batch    53 | loss: 4.0472836Losses:  3.955996036529541 0.055428341031074524
CurrentTrain: epoch  8, batch    54 | loss: 4.0114245Losses:  3.967043399810791 0.05567718297243118
CurrentTrain: epoch  8, batch    55 | loss: 4.0227208Losses:  4.05158805847168 0.03661099821329117
CurrentTrain: epoch  8, batch    56 | loss: 4.0881991Losses:  4.138802528381348 0.07109630107879639
CurrentTrain: epoch  8, batch    57 | loss: 4.2098989Losses:  4.072370529174805 0.07496959716081619
CurrentTrain: epoch  8, batch    58 | loss: 4.1473403Losses:  3.9733633995056152 0.11126833409070969
CurrentTrain: epoch  8, batch    59 | loss: 4.0846319Losses:  4.031252384185791 0.09418930858373642
CurrentTrain: epoch  8, batch    60 | loss: 4.1254416Losses:  4.002735137939453 0.08019164204597473
CurrentTrain: epoch  8, batch    61 | loss: 4.0829268Losses:  3.9981932640075684 0.11161047220230103
CurrentTrain: epoch  8, batch    62 | loss: 4.1098037Losses:  4.045978546142578 0.07355952262878418
CurrentTrain: epoch  8, batch    63 | loss: 4.1195383Losses:  4.111424922943115 0.05143240839242935
CurrentTrain: epoch  8, batch    64 | loss: 4.1628575Losses:  4.052902698516846 0.16297493875026703
CurrentTrain: epoch  8, batch    65 | loss: 4.2158775Losses:  4.014723300933838 0.0761059895157814
CurrentTrain: epoch  8, batch    66 | loss: 4.0908294Losses:  4.025036811828613 0.04858287423849106
CurrentTrain: epoch  8, batch    67 | loss: 4.0736198Losses:  3.9609487056732178 0.09934431314468384
CurrentTrain: epoch  8, batch    68 | loss: 4.0602932Losses:  4.023601055145264 0.09605541080236435
CurrentTrain: epoch  8, batch    69 | loss: 4.1196566Losses:  4.089721202850342 0.055715449154376984
CurrentTrain: epoch  8, batch    70 | loss: 4.1454368Losses:  4.000110626220703 0.08308199793100357
CurrentTrain: epoch  8, batch    71 | loss: 4.0831928Losses:  4.022613525390625 0.10177335888147354
CurrentTrain: epoch  8, batch    72 | loss: 4.1243868Losses:  3.919189453125 0.15537261962890625
CurrentTrain: epoch  8, batch    73 | loss: 4.0745621Losses:  4.017923355102539 0.1336827427148819
CurrentTrain: epoch  8, batch    74 | loss: 4.1516061Losses:  3.9939324855804443 0.07841534912586212
CurrentTrain: epoch  8, batch    75 | loss: 4.0723476Losses:  4.010368824005127 0.08233816921710968
CurrentTrain: epoch  8, batch    76 | loss: 4.0927072Losses:  4.055664539337158 0.07926200330257416
CurrentTrain: epoch  8, batch    77 | loss: 4.1349263Losses:  3.952871799468994 0.029268957674503326
CurrentTrain: epoch  8, batch    78 | loss: 3.9821408Losses:  4.053881645202637 0.06700104475021362
CurrentTrain: epoch  8, batch    79 | loss: 4.1208825Losses:  4.016575336456299 0.08027848601341248
CurrentTrain: epoch  8, batch    80 | loss: 4.0968537Losses:  4.063671112060547 0.07029607892036438
CurrentTrain: epoch  8, batch    81 | loss: 4.1339674Losses:  4.0319504737854 0.05489777401089668
CurrentTrain: epoch  8, batch    82 | loss: 4.0868483Losses:  4.042213439941406 0.13664744794368744
CurrentTrain: epoch  8, batch    83 | loss: 4.1788607Losses:  3.984135389328003 0.06669788807630539
CurrentTrain: epoch  8, batch    84 | loss: 4.0508332Losses:  4.041593551635742 0.1229187548160553
CurrentTrain: epoch  8, batch    85 | loss: 4.1645122Losses:  4.031037330627441 0.09024578332901001
CurrentTrain: epoch  8, batch    86 | loss: 4.1212831Losses:  4.057750225067139 0.06922586262226105
CurrentTrain: epoch  8, batch    87 | loss: 4.1269760Losses:  3.8948049545288086 0.02843700349330902
CurrentTrain: epoch  8, batch    88 | loss: 3.9232419Losses:  4.005102157592773 0.07126480340957642
CurrentTrain: epoch  8, batch    89 | loss: 4.0763669Losses:  3.8579912185668945 0.055159635841846466
CurrentTrain: epoch  8, batch    90 | loss: 3.9131508Losses:  4.027766227722168 0.07766197621822357
CurrentTrain: epoch  8, batch    91 | loss: 4.1054282Losses:  4.032729148864746 0.11116695404052734
CurrentTrain: epoch  8, batch    92 | loss: 4.1438961Losses:  4.039189338684082 0.07419232279062271
CurrentTrain: epoch  8, batch    93 | loss: 4.1133819Losses:  4.029529571533203 0.08559772372245789
CurrentTrain: epoch  8, batch    94 | loss: 4.1151271Losses:  3.958282470703125 0.09571650624275208
CurrentTrain: epoch  8, batch    95 | loss: 4.0539989Losses:  3.9800870418548584 0.07726635038852692
CurrentTrain: epoch  8, batch    96 | loss: 4.0573535Losses:  3.96250319480896 0.08161696791648865
CurrentTrain: epoch  8, batch    97 | loss: 4.0441203Losses:  4.019434452056885 0.12033239752054214
CurrentTrain: epoch  8, batch    98 | loss: 4.1397667Losses:  4.028707981109619 0.12443146854639053
CurrentTrain: epoch  8, batch    99 | loss: 4.1531396Losses:  4.058280944824219 0.08307388424873352
CurrentTrain: epoch  8, batch   100 | loss: 4.1413550Losses:  3.9529919624328613 0.07837916165590286
CurrentTrain: epoch  8, batch   101 | loss: 4.0313711Losses:  3.993678092956543 0.059452638030052185
CurrentTrain: epoch  8, batch   102 | loss: 4.0531306Losses:  4.0208635330200195 0.11758491396903992
CurrentTrain: epoch  8, batch   103 | loss: 4.1384482Losses:  4.04700231552124 0.04466935992240906
CurrentTrain: epoch  8, batch   104 | loss: 4.0916715Losses:  4.049199104309082 0.0814877450466156
CurrentTrain: epoch  8, batch   105 | loss: 4.1306868Losses:  4.047689437866211 0.07367807626724243
CurrentTrain: epoch  8, batch   106 | loss: 4.1213675Losses:  4.006909370422363 0.05878567323088646
CurrentTrain: epoch  8, batch   107 | loss: 4.0656948Losses:  4.002651214599609 0.05629181116819382
CurrentTrain: epoch  8, batch   108 | loss: 4.0589428Losses:  4.034950256347656 0.054070256650447845
CurrentTrain: epoch  8, batch   109 | loss: 4.0890207Losses:  4.017291069030762 0.0650530681014061
CurrentTrain: epoch  8, batch   110 | loss: 4.0823441Losses:  4.014254093170166 0.07501430809497833
CurrentTrain: epoch  8, batch   111 | loss: 4.0892682Losses:  3.96012020111084 0.0435430109500885
CurrentTrain: epoch  8, batch   112 | loss: 4.0036631Losses:  3.9889745712280273 0.06151895970106125
CurrentTrain: epoch  8, batch   113 | loss: 4.0504937Losses:  4.0269856452941895 0.08999790996313095
CurrentTrain: epoch  8, batch   114 | loss: 4.1169834Losses:  3.9825000762939453 0.07352415472269058
CurrentTrain: epoch  8, batch   115 | loss: 4.0560241Losses:  3.9758787155151367 0.11248941719532013
CurrentTrain: epoch  8, batch   116 | loss: 4.0883679Losses:  3.9939684867858887 0.06678168475627899
CurrentTrain: epoch  8, batch   117 | loss: 4.0607500Losses:  3.994091272354126 0.074503093957901
CurrentTrain: epoch  8, batch   118 | loss: 4.0685945Losses:  3.9770798683166504 0.07012221962213516
CurrentTrain: epoch  8, batch   119 | loss: 4.0472021Losses:  3.975466012954712 0.12837576866149902
CurrentTrain: epoch  8, batch   120 | loss: 4.1038418Losses:  3.9189252853393555 0.055651936680078506
CurrentTrain: epoch  8, batch   121 | loss: 3.9745772Losses:  3.988598346710205 0.055700045078992844
CurrentTrain: epoch  8, batch   122 | loss: 4.0442982Losses:  4.000121593475342 0.12052243202924728
CurrentTrain: epoch  8, batch   123 | loss: 4.1206441Losses:  4.1172194480896 0.028052853420376778
CurrentTrain: epoch  8, batch   124 | loss: 4.1452723Losses:  4.0912394523620605 0.06607820093631744
CurrentTrain: epoch  9, batch     0 | loss: 4.1573176Losses:  3.951143980026245 0.06267675757408142
CurrentTrain: epoch  9, batch     1 | loss: 4.0138206Losses:  3.9515559673309326 0.08834274858236313
CurrentTrain: epoch  9, batch     2 | loss: 4.0398989Losses:  4.025911808013916 0.09438631683588028
CurrentTrain: epoch  9, batch     3 | loss: 4.1202979Losses:  4.028355121612549 0.06111298128962517
CurrentTrain: epoch  9, batch     4 | loss: 4.0894680Losses:  3.969088554382324 0.07829494774341583
CurrentTrain: epoch  9, batch     5 | loss: 4.0473833Losses:  3.9779746532440186 0.10662071406841278
CurrentTrain: epoch  9, batch     6 | loss: 4.0845952Losses:  3.872089385986328 0.033481329679489136
CurrentTrain: epoch  9, batch     7 | loss: 3.9055707Losses:  4.002028942108154 0.12243114411830902
CurrentTrain: epoch  9, batch     8 | loss: 4.1244602Losses:  3.911787986755371 0.06363535672426224
CurrentTrain: epoch  9, batch     9 | loss: 3.9754233Losses:  3.9948320388793945 0.0511169470846653
CurrentTrain: epoch  9, batch    10 | loss: 4.0459490Losses:  4.0101728439331055 0.08575259894132614
CurrentTrain: epoch  9, batch    11 | loss: 4.0959253Losses:  3.981355667114258 0.05556023493409157
CurrentTrain: epoch  9, batch    12 | loss: 4.0369158Losses:  3.978273868560791 0.07205758988857269
CurrentTrain: epoch  9, batch    13 | loss: 4.0503316Losses:  3.9970664978027344 0.08354730904102325
CurrentTrain: epoch  9, batch    14 | loss: 4.0806136Losses:  4.021849632263184 0.06629347056150436
CurrentTrain: epoch  9, batch    15 | loss: 4.0881429Losses:  3.998467445373535 0.07199964672327042
CurrentTrain: epoch  9, batch    16 | loss: 4.0704670Losses:  3.9624223709106445 0.12106439471244812
CurrentTrain: epoch  9, batch    17 | loss: 4.0834866Losses:  3.9536023139953613 0.09877021610736847
CurrentTrain: epoch  9, batch    18 | loss: 4.0523725Losses:  3.9797346591949463 0.12995044887065887
CurrentTrain: epoch  9, batch    19 | loss: 4.1096849Losses:  3.9832370281219482 0.16093966364860535
CurrentTrain: epoch  9, batch    20 | loss: 4.1441765Losses:  4.034751892089844 0.08262486755847931
CurrentTrain: epoch  9, batch    21 | loss: 4.1173768Losses:  4.026523590087891 0.04875596612691879
CurrentTrain: epoch  9, batch    22 | loss: 4.0752797Losses:  4.074252128601074 0.0772010013461113
CurrentTrain: epoch  9, batch    23 | loss: 4.1514530Losses:  4.021146297454834 0.03645177185535431
CurrentTrain: epoch  9, batch    24 | loss: 4.0575981Losses:  4.029314994812012 0.12377646565437317
CurrentTrain: epoch  9, batch    25 | loss: 4.1530914Losses:  4.011580467224121 0.10946689546108246
CurrentTrain: epoch  9, batch    26 | loss: 4.1210475Losses:  3.9682559967041016 0.04337264597415924
CurrentTrain: epoch  9, batch    27 | loss: 4.0116286Losses:  4.0283989906311035 0.06440757215023041
CurrentTrain: epoch  9, batch    28 | loss: 4.0928063Losses:  3.997638702392578 0.05099260061979294
CurrentTrain: epoch  9, batch    29 | loss: 4.0486312Losses:  3.9734623432159424 0.11086203902959824
CurrentTrain: epoch  9, batch    30 | loss: 4.0843244Losses:  4.038002967834473 0.058794762939214706
CurrentTrain: epoch  9, batch    31 | loss: 4.0967979Losses:  3.966207504272461 0.10457540303468704
CurrentTrain: epoch  9, batch    32 | loss: 4.0707831Losses:  4.038263320922852 0.07398615032434464
CurrentTrain: epoch  9, batch    33 | loss: 4.1122494Losses:  4.023613929748535 0.07555913925170898
CurrentTrain: epoch  9, batch    34 | loss: 4.0991731Losses:  3.971883773803711 0.05685104802250862
CurrentTrain: epoch  9, batch    35 | loss: 4.0287347Losses:  3.972442626953125 0.07263744622468948
CurrentTrain: epoch  9, batch    36 | loss: 4.0450802Losses:  4.0236053466796875 0.06713145971298218
CurrentTrain: epoch  9, batch    37 | loss: 4.0907369Losses:  3.958775043487549 0.10743555426597595
CurrentTrain: epoch  9, batch    38 | loss: 4.0662107Losses:  3.9869885444641113 0.09866160154342651
CurrentTrain: epoch  9, batch    39 | loss: 4.0856500Losses:  4.013269901275635 0.08255927264690399
CurrentTrain: epoch  9, batch    40 | loss: 4.0958290Losses:  4.020259857177734 0.06354827433824539
CurrentTrain: epoch  9, batch    41 | loss: 4.0838079Losses:  3.9802207946777344 0.04750918596982956
CurrentTrain: epoch  9, batch    42 | loss: 4.0277300Losses:  3.986553430557251 0.08276674151420593
CurrentTrain: epoch  9, batch    43 | loss: 4.0693202Losses:  3.9887609481811523 0.08909420669078827
CurrentTrain: epoch  9, batch    44 | loss: 4.0778551Losses:  3.991201639175415 0.04530448466539383
CurrentTrain: epoch  9, batch    45 | loss: 4.0365062Losses:  3.9783663749694824 0.05348501354455948
CurrentTrain: epoch  9, batch    46 | loss: 4.0318513Losses:  3.983196258544922 0.050290580838918686
CurrentTrain: epoch  9, batch    47 | loss: 4.0334868Losses:  4.004030704498291 0.12778547406196594
CurrentTrain: epoch  9, batch    48 | loss: 4.1318164Losses:  3.9869725704193115 0.06800536066293716
CurrentTrain: epoch  9, batch    49 | loss: 4.0549779Losses:  3.946180820465088 0.13626928627490997
CurrentTrain: epoch  9, batch    50 | loss: 4.0824499Losses:  3.9699506759643555 0.07095716893672943
CurrentTrain: epoch  9, batch    51 | loss: 4.0409079Losses:  3.9990220069885254 0.048735663294792175
CurrentTrain: epoch  9, batch    52 | loss: 4.0477576Losses:  4.030035972595215 0.06719153374433517
CurrentTrain: epoch  9, batch    53 | loss: 4.0972276Losses:  3.97000789642334 0.08719825744628906
CurrentTrain: epoch  9, batch    54 | loss: 4.0572062Losses:  3.9353256225585938 0.02470017969608307
CurrentTrain: epoch  9, batch    55 | loss: 3.9600258Losses:  3.966588258743286 0.06087523698806763
CurrentTrain: epoch  9, batch    56 | loss: 4.0274634Losses:  3.9670615196228027 0.08735187351703644
CurrentTrain: epoch  9, batch    57 | loss: 4.0544133Losses:  4.069519996643066 0.05202215909957886
CurrentTrain: epoch  9, batch    58 | loss: 4.1215420Losses:  3.9901015758514404 0.08682257682085037
CurrentTrain: epoch  9, batch    59 | loss: 4.0769243Losses:  3.986743450164795 0.06530898064374924
CurrentTrain: epoch  9, batch    60 | loss: 4.0520525Losses:  3.9730775356292725 0.1027250587940216
CurrentTrain: epoch  9, batch    61 | loss: 4.0758028Losses:  4.017255783081055 0.07429419457912445
CurrentTrain: epoch  9, batch    62 | loss: 4.0915499Losses:  3.994340658187866 0.0993097722530365
CurrentTrain: epoch  9, batch    63 | loss: 4.0936503Losses:  3.9385128021240234 0.1300356686115265
CurrentTrain: epoch  9, batch    64 | loss: 4.0685487Losses:  3.9822614192962646 0.10373155027627945
CurrentTrain: epoch  9, batch    65 | loss: 4.0859928Losses:  4.043697357177734 0.07213319838047028
CurrentTrain: epoch  9, batch    66 | loss: 4.1158304Losses:  4.035496711730957 0.06148938089609146
CurrentTrain: epoch  9, batch    67 | loss: 4.0969863Losses:  4.008950710296631 0.04588901996612549
CurrentTrain: epoch  9, batch    68 | loss: 4.0548396Losses:  3.9923129081726074 0.11867010593414307
CurrentTrain: epoch  9, batch    69 | loss: 4.1109829Losses:  3.9848923683166504 0.09790566563606262
CurrentTrain: epoch  9, batch    70 | loss: 4.0827980Losses:  3.9526846408843994 0.09686422348022461
CurrentTrain: epoch  9, batch    71 | loss: 4.0495491Losses:  3.9876227378845215 0.09368713200092316
CurrentTrain: epoch  9, batch    72 | loss: 4.0813098Losses:  3.986806869506836 0.04832034930586815
CurrentTrain: epoch  9, batch    73 | loss: 4.0351272Losses:  4.007205009460449 0.06988689303398132
CurrentTrain: epoch  9, batch    74 | loss: 4.0770917Losses:  4.030479431152344 0.0985800251364708
CurrentTrain: epoch  9, batch    75 | loss: 4.1290593Losses:  3.9733259677886963 0.11279730498790741
CurrentTrain: epoch  9, batch    76 | loss: 4.0861235Losses:  3.9488377571105957 0.09623059630393982
CurrentTrain: epoch  9, batch    77 | loss: 4.0450683Losses:  4.0057783126831055 0.08792231976985931
CurrentTrain: epoch  9, batch    78 | loss: 4.0937004Losses:  3.938551664352417 0.09741425514221191
CurrentTrain: epoch  9, batch    79 | loss: 4.0359659Losses:  4.005937576293945 0.09515439718961716
CurrentTrain: epoch  9, batch    80 | loss: 4.1010919Losses:  3.9872958660125732 0.07965745031833649
CurrentTrain: epoch  9, batch    81 | loss: 4.0669532Losses:  4.0221171379089355 0.0882454365491867
CurrentTrain: epoch  9, batch    82 | loss: 4.1103625Losses:  4.004070281982422 0.12747865915298462
CurrentTrain: epoch  9, batch    83 | loss: 4.1315489Losses:  3.9945640563964844 0.09096953272819519
CurrentTrain: epoch  9, batch    84 | loss: 4.0855336Losses:  3.995302200317383 0.08736196160316467
CurrentTrain: epoch  9, batch    85 | loss: 4.0826640Losses:  3.9386849403381348 0.08338480442762375
CurrentTrain: epoch  9, batch    86 | loss: 4.0220699Losses:  4.005509376525879 0.06343769282102585
CurrentTrain: epoch  9, batch    87 | loss: 4.0689468Losses:  3.863448143005371 0.06953788548707962
CurrentTrain: epoch  9, batch    88 | loss: 3.9329860Losses:  4.011446952819824 0.08432917296886444
CurrentTrain: epoch  9, batch    89 | loss: 4.0957761Losses:  3.9175639152526855 0.0904303789138794
CurrentTrain: epoch  9, batch    90 | loss: 4.0079942Losses:  3.9960312843322754 0.09119492769241333
CurrentTrain: epoch  9, batch    91 | loss: 4.0872264Losses:  3.967478036880493 0.08212713897228241
CurrentTrain: epoch  9, batch    92 | loss: 4.0496054Losses:  3.9306087493896484 0.06626377999782562
CurrentTrain: epoch  9, batch    93 | loss: 3.9968724Losses:  3.969315528869629 0.03784847632050514
CurrentTrain: epoch  9, batch    94 | loss: 4.0071640Losses:  4.014244079589844 0.06340149790048599
CurrentTrain: epoch  9, batch    95 | loss: 4.0776458Losses:  4.040715217590332 0.0494493767619133
CurrentTrain: epoch  9, batch    96 | loss: 4.0901647Losses:  3.993020534515381 0.07765363901853561
CurrentTrain: epoch  9, batch    97 | loss: 4.0706739Losses:  3.9746720790863037 0.0781058520078659
CurrentTrain: epoch  9, batch    98 | loss: 4.0527778Losses:  3.935062885284424 0.04162183776497841
CurrentTrain: epoch  9, batch    99 | loss: 3.9766848Losses:  4.01243782043457 0.06223657354712486
CurrentTrain: epoch  9, batch   100 | loss: 4.0746746Losses:  4.035076141357422 0.05281135067343712
CurrentTrain: epoch  9, batch   101 | loss: 4.0878873Losses:  3.9805479049682617 0.07852500677108765
CurrentTrain: epoch  9, batch   102 | loss: 4.0590730Losses:  3.9651525020599365 0.09991750121116638
CurrentTrain: epoch  9, batch   103 | loss: 4.0650702Losses:  4.004281997680664 0.05480087548494339
CurrentTrain: epoch  9, batch   104 | loss: 4.0590830Losses:  3.9726269245147705 0.14446121454238892
CurrentTrain: epoch  9, batch   105 | loss: 4.1170883Losses:  3.918947219848633 0.05005376785993576
CurrentTrain: epoch  9, batch   106 | loss: 3.9690011Losses:  4.0230278968811035 0.09389975666999817
CurrentTrain: epoch  9, batch   107 | loss: 4.1169276Losses:  3.963451623916626 0.061507001519203186
CurrentTrain: epoch  9, batch   108 | loss: 4.0249586Losses:  4.00330924987793 0.0842881053686142
CurrentTrain: epoch  9, batch   109 | loss: 4.0875974Losses:  3.9314262866973877 0.024551108479499817
CurrentTrain: epoch  9, batch   110 | loss: 3.9559774Losses:  4.033720016479492 0.06501012295484543
CurrentTrain: epoch  9, batch   111 | loss: 4.0987301Losses:  3.9884586334228516 0.023899804800748825
CurrentTrain: epoch  9, batch   112 | loss: 4.0123587Losses:  3.9537675380706787 0.08340410888195038
CurrentTrain: epoch  9, batch   113 | loss: 4.0371718Losses:  3.9562389850616455 0.09212937951087952
CurrentTrain: epoch  9, batch   114 | loss: 4.0483685Losses:  3.968783378601074 0.0790325254201889
CurrentTrain: epoch  9, batch   115 | loss: 4.0478158Losses:  3.9838221073150635 0.07171730697154999
CurrentTrain: epoch  9, batch   116 | loss: 4.0555396Losses:  3.970276355743408 0.024470794945955276
CurrentTrain: epoch  9, batch   117 | loss: 3.9947472Losses:  4.018223285675049 0.054543040692806244
CurrentTrain: epoch  9, batch   118 | loss: 4.0727663Losses:  4.010871887207031 0.11132179200649261
CurrentTrain: epoch  9, batch   119 | loss: 4.1221938Losses:  3.953458786010742 0.12334005534648895
CurrentTrain: epoch  9, batch   120 | loss: 4.0767989Losses:  3.958322525024414 0.09090698510408401
CurrentTrain: epoch  9, batch   121 | loss: 4.0492296Losses:  4.022159576416016 0.06429403275251389
CurrentTrain: epoch  9, batch   122 | loss: 4.0864534Losses:  4.005393028259277 0.05917099490761757
CurrentTrain: epoch  9, batch   123 | loss: 4.0645642Losses:  3.999600410461426 0.072206512093544
CurrentTrain: epoch  9, batch   124 | loss: 4.0718069
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  7.461540222167969 1.3970069885253906
CurrentTrain: epoch  0, batch     0 | loss: 8.8585472Losses:  8.147876739501953 1.330912470817566
CurrentTrain: epoch  0, batch     1 | loss: 9.4787893Losses:  8.300190925598145 1.880744457244873
CurrentTrain: epoch  0, batch     2 | loss: 10.1809349Losses:  8.193763732910156 1.8154363632202148
CurrentTrain: epoch  0, batch     3 | loss: 10.0092001Losses:  6.241034984588623 1.3602434396743774
CurrentTrain: epoch  0, batch     4 | loss: 7.6012783Losses:  6.765054702758789 1.21192467212677
CurrentTrain: epoch  0, batch     5 | loss: 7.9769793Losses:  5.642767906188965 0.5611807107925415
CurrentTrain: epoch  0, batch     6 | loss: 6.2039485Losses:  5.86712646484375 1.0748357772827148
CurrentTrain: epoch  1, batch     0 | loss: 6.9419622Losses:  5.997241497039795 1.4506094455718994
CurrentTrain: epoch  1, batch     1 | loss: 7.4478512Losses:  5.962523460388184 1.2333087921142578
CurrentTrain: epoch  1, batch     2 | loss: 7.1958323Losses:  8.589101791381836 1.677978754043579
CurrentTrain: epoch  1, batch     3 | loss: 10.2670803Losses:  7.772854804992676 1.097883701324463
CurrentTrain: epoch  1, batch     4 | loss: 8.8707390Losses:  5.541646480560303 1.1507289409637451
CurrentTrain: epoch  1, batch     5 | loss: 6.6923752Losses:  8.227598190307617 0.7453333735466003
CurrentTrain: epoch  1, batch     6 | loss: 8.9729319Losses:  5.356863021850586 1.4046943187713623
CurrentTrain: epoch  2, batch     0 | loss: 6.7615576Losses:  5.372968673706055 0.8700094819068909
CurrentTrain: epoch  2, batch     1 | loss: 6.2429781Losses:  5.770291328430176 1.391160249710083
CurrentTrain: epoch  2, batch     2 | loss: 7.1614513Losses:  5.331277370452881 1.423558235168457
CurrentTrain: epoch  2, batch     3 | loss: 6.7548356Losses:  7.184162139892578 1.0473988056182861
CurrentTrain: epoch  2, batch     4 | loss: 8.2315607Losses:  4.16856050491333 1.2367475032806396
CurrentTrain: epoch  2, batch     5 | loss: 5.4053078Losses:  5.232359409332275 0.31375476717948914
CurrentTrain: epoch  2, batch     6 | loss: 5.5461140Losses:  4.547842979431152 0.8517187833786011
CurrentTrain: epoch  3, batch     0 | loss: 5.3995619Losses:  5.24709415435791 1.3018152713775635
CurrentTrain: epoch  3, batch     1 | loss: 6.5489092Losses:  3.3939902782440186 0.8762208223342896
CurrentTrain: epoch  3, batch     2 | loss: 4.2702112Losses:  4.578180313110352 0.9617507457733154
CurrentTrain: epoch  3, batch     3 | loss: 5.5399313Losses:  4.7011003494262695 1.150394320487976
CurrentTrain: epoch  3, batch     4 | loss: 5.8514948Losses:  5.600704193115234 0.9976474642753601
CurrentTrain: epoch  3, batch     5 | loss: 6.5983515Losses:  5.883120536804199 0.43449753522872925
CurrentTrain: epoch  3, batch     6 | loss: 6.3176179Losses:  4.898832321166992 1.2019617557525635
CurrentTrain: epoch  4, batch     0 | loss: 6.1007938Losses:  3.9743001461029053 1.0238412618637085
CurrentTrain: epoch  4, batch     1 | loss: 4.9981413Losses:  5.054381370544434 1.181206464767456
CurrentTrain: epoch  4, batch     2 | loss: 6.2355881Losses:  3.552717924118042 1.1920324563980103
CurrentTrain: epoch  4, batch     3 | loss: 4.7447505Losses:  5.99519681930542 0.7360467314720154
CurrentTrain: epoch  4, batch     4 | loss: 6.7312436Losses:  3.918571949005127 1.1627719402313232
CurrentTrain: epoch  4, batch     5 | loss: 5.0813437Losses:  2.351762533187866 0.3569311797618866
CurrentTrain: epoch  4, batch     6 | loss: 2.7086937Losses:  4.377727508544922 0.959710955619812
CurrentTrain: epoch  5, batch     0 | loss: 5.3374386Losses:  4.5911712646484375 1.1530464887619019
CurrentTrain: epoch  5, batch     1 | loss: 5.7442179Losses:  4.327177047729492 1.057860255241394
CurrentTrain: epoch  5, batch     2 | loss: 5.3850374Losses:  4.952212333679199 0.9917231202125549
CurrentTrain: epoch  5, batch     3 | loss: 5.9439354Losses:  3.533811092376709 0.9283071160316467
CurrentTrain: epoch  5, batch     4 | loss: 4.4621181Losses:  4.364611625671387 1.119171142578125
CurrentTrain: epoch  5, batch     5 | loss: 5.4837828Losses:  1.9237691164016724 0.23764702677726746
CurrentTrain: epoch  5, batch     6 | loss: 2.1614161Losses:  4.398691177368164 1.0471701622009277
CurrentTrain: epoch  6, batch     0 | loss: 5.4458613Losses:  2.599987745285034 0.6690516471862793
CurrentTrain: epoch  6, batch     1 | loss: 3.2690394Losses:  4.843827247619629 1.000133752822876
CurrentTrain: epoch  6, batch     2 | loss: 5.8439608Losses:  3.0182790756225586 0.8832892775535583
CurrentTrain: epoch  6, batch     3 | loss: 3.9015684Losses:  3.869638442993164 1.0789146423339844
CurrentTrain: epoch  6, batch     4 | loss: 4.9485531Losses:  4.434100151062012 0.9766613841056824
CurrentTrain: epoch  6, batch     5 | loss: 5.4107614Losses:  4.271704196929932 0.11882899701595306
CurrentTrain: epoch  6, batch     6 | loss: 4.3905330Losses:  4.189640998840332 1.1813082695007324
CurrentTrain: epoch  7, batch     0 | loss: 5.3709493Losses:  3.2884414196014404 0.8307161331176758
CurrentTrain: epoch  7, batch     1 | loss: 4.1191578Losses:  4.356998443603516 1.0198701620101929
CurrentTrain: epoch  7, batch     2 | loss: 5.3768687Losses:  2.813026189804077 0.6851428151130676
CurrentTrain: epoch  7, batch     3 | loss: 3.4981689Losses:  2.6476633548736572 0.7005400657653809
CurrentTrain: epoch  7, batch     4 | loss: 3.3482034Losses:  3.3953685760498047 1.2105882167816162
CurrentTrain: epoch  7, batch     5 | loss: 4.6059570Losses:  2.8280510902404785 0.1648850291967392
CurrentTrain: epoch  7, batch     6 | loss: 2.9929361Losses:  2.8257689476013184 0.8557521104812622
CurrentTrain: epoch  8, batch     0 | loss: 3.6815209Losses:  3.979721784591675 0.943889856338501
CurrentTrain: epoch  8, batch     1 | loss: 4.9236116Losses:  2.7712841033935547 0.888569712638855
CurrentTrain: epoch  8, batch     2 | loss: 3.6598539Losses:  3.307648181915283 0.8057752847671509
CurrentTrain: epoch  8, batch     3 | loss: 4.1134233Losses:  2.8692774772644043 0.7229841947555542
CurrentTrain: epoch  8, batch     4 | loss: 3.5922618Losses:  3.1692395210266113 0.7685916423797607
CurrentTrain: epoch  8, batch     5 | loss: 3.9378312Losses:  1.6406173706054688 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.6406174Losses:  2.7878928184509277 0.6686935424804688
CurrentTrain: epoch  9, batch     0 | loss: 3.4565864Losses:  2.6625518798828125 0.7061645984649658
CurrentTrain: epoch  9, batch     1 | loss: 3.3687165Losses:  2.8726749420166016 1.1437735557556152
CurrentTrain: epoch  9, batch     2 | loss: 4.0164485Losses:  2.464832067489624 0.6489105224609375
CurrentTrain: epoch  9, batch     3 | loss: 3.1137426Losses:  3.0168116092681885 0.8781899809837341
CurrentTrain: epoch  9, batch     4 | loss: 3.8950016Losses:  2.30680513381958 0.640069842338562
CurrentTrain: epoch  9, batch     5 | loss: 2.9468751Losses:  3.419445037841797 0.5452996492385864
CurrentTrain: epoch  9, batch     6 | loss: 3.9647446
Losses:  1.1524096727371216 0.8508076667785645
MemoryTrain:  epoch  0, batch     0 | loss: 2.0032172Losses:  0.9523741006851196 0.40017420053482056
MemoryTrain:  epoch  0, batch     1 | loss: 1.3525484Losses:  0.5419784188270569 0.1987830400466919
MemoryTrain:  epoch  0, batch     2 | loss: 0.7407615Losses:  0.6250854730606079 0.7099323272705078
MemoryTrain:  epoch  1, batch     0 | loss: 1.3350178Losses:  2.0664567947387695 0.8808269500732422
MemoryTrain:  epoch  1, batch     1 | loss: 2.9472837Losses:  0.6060490608215332 0.1269347220659256
MemoryTrain:  epoch  1, batch     2 | loss: 0.7329838Losses:  0.8469281196594238 0.5256314277648926
MemoryTrain:  epoch  2, batch     0 | loss: 1.3725595Losses:  0.190423846244812 0.43899139761924744
MemoryTrain:  epoch  2, batch     1 | loss: 0.6294153Losses:  0.3678939938545227 0.3116919994354248
MemoryTrain:  epoch  2, batch     2 | loss: 0.6795860Losses:  0.16506311297416687 0.5858370661735535
MemoryTrain:  epoch  3, batch     0 | loss: 0.7509001Losses:  0.11562711000442505 0.314294695854187
MemoryTrain:  epoch  3, batch     1 | loss: 0.4299218Losses:  0.98658287525177 0.41627418994903564
MemoryTrain:  epoch  3, batch     2 | loss: 1.4028571Losses:  0.3403072953224182 0.7505654096603394
MemoryTrain:  epoch  4, batch     0 | loss: 1.0908728Losses:  0.10488978773355484 0.4246136546134949
MemoryTrain:  epoch  4, batch     1 | loss: 0.5295035Losses:  0.027964167296886444 0.24117682874202728
MemoryTrain:  epoch  4, batch     2 | loss: 0.2691410Losses:  0.07030527293682098 0.6455169320106506
MemoryTrain:  epoch  5, batch     0 | loss: 0.7158222Losses:  0.02793777361512184 0.4425635039806366
MemoryTrain:  epoch  5, batch     1 | loss: 0.4705013Losses:  0.016645418480038643 0.2945634722709656
MemoryTrain:  epoch  5, batch     2 | loss: 0.3112089Losses:  0.01869131810963154 0.5325833559036255
MemoryTrain:  epoch  6, batch     0 | loss: 0.5512747Losses:  0.05825333669781685 0.5490947961807251
MemoryTrain:  epoch  6, batch     1 | loss: 0.6073481Losses:  0.01190955750644207 0.2793721556663513
MemoryTrain:  epoch  6, batch     2 | loss: 0.2912817Losses:  0.022354144603013992 0.4231417179107666
MemoryTrain:  epoch  7, batch     0 | loss: 0.4454959Losses:  0.018609829246997833 0.479968786239624
MemoryTrain:  epoch  7, batch     1 | loss: 0.4985786Losses:  0.019897418096661568 0.2789546251296997
MemoryTrain:  epoch  7, batch     2 | loss: 0.2988521Losses:  0.008176342584192753 0.3919169306755066
MemoryTrain:  epoch  8, batch     0 | loss: 0.4000933Losses:  0.019465859979391098 0.4992724359035492
MemoryTrain:  epoch  8, batch     1 | loss: 0.5187383Losses:  0.03912919759750366 0.20736992359161377
MemoryTrain:  epoch  8, batch     2 | loss: 0.2464991Losses:  0.013517423532903194 0.3998352289199829
MemoryTrain:  epoch  9, batch     0 | loss: 0.4133526Losses:  0.021736498922109604 0.5647299289703369
MemoryTrain:  epoch  9, batch     1 | loss: 0.5864664Losses:  0.007285264320671558 0.08505509793758392
MemoryTrain:  epoch  9, batch     2 | loss: 0.0923404
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.77%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 74.56%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 74.14%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 73.54%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 92.12%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.79%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 94.49%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 94.36%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.75%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 90.87%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 89.49%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 88.15%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 87.04%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 86.05%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 85.85%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 85.79%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.74%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.47%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 85.49%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 85.54%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 85.44%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 85.47%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 85.63%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 85.72%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 86.37%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.06%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 86.89%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 86.78%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 86.79%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 86.23%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 85.95%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 85.62%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 85.47%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 85.21%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.38%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 84.79%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 84.50%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 83.99%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 83.62%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 83.45%   
cur_acc:  ['0.9464', '0.7192']
his_acc:  ['0.9464', '0.8345']
Clustering into  14  clusters
Clusters:  [ 2 11 13  2  2  2  0  2  9  7  6  2  8  2 10  0  4  2  2  2  0 12  6  2
  2  3  2  1  2  5]
Losses:  7.04038667678833 1.678576946258545
CurrentTrain: epoch  0, batch     0 | loss: 8.7189636Losses:  7.093901634216309 1.2932884693145752
CurrentTrain: epoch  0, batch     1 | loss: 8.3871899Losses:  5.4644927978515625 1.2932391166687012
CurrentTrain: epoch  0, batch     2 | loss: 6.7577319Losses:  6.535908222198486 1.4277701377868652
CurrentTrain: epoch  0, batch     3 | loss: 7.9636784Losses:  5.611336708068848 1.228121280670166
CurrentTrain: epoch  0, batch     4 | loss: 6.8394580Losses:  7.13671875 1.538878321647644
CurrentTrain: epoch  0, batch     5 | loss: 8.6755972Losses:  7.332379341125488 0.43081188201904297
CurrentTrain: epoch  0, batch     6 | loss: 7.7631912Losses:  6.090973854064941 1.3168604373931885
CurrentTrain: epoch  1, batch     0 | loss: 7.4078341Losses:  5.69105863571167 1.5006171464920044
CurrentTrain: epoch  1, batch     1 | loss: 7.1916757Losses:  6.9155964851379395 1.379725694656372
CurrentTrain: epoch  1, batch     2 | loss: 8.2953224Losses:  3.5526654720306396 0.9855764508247375
CurrentTrain: epoch  1, batch     3 | loss: 4.5382419Losses:  5.614596366882324 1.1522499322891235
CurrentTrain: epoch  1, batch     4 | loss: 6.7668462Losses:  4.808355331420898 1.3765532970428467
CurrentTrain: epoch  1, batch     5 | loss: 6.1849089Losses:  4.827884674072266 0.5289332866668701
CurrentTrain: epoch  1, batch     6 | loss: 5.3568182Losses:  5.216315269470215 1.3002357482910156
CurrentTrain: epoch  2, batch     0 | loss: 6.5165510Losses:  4.951729774475098 0.9832150936126709
CurrentTrain: epoch  2, batch     1 | loss: 5.9349451Losses:  3.8482401371002197 0.9721577167510986
CurrentTrain: epoch  2, batch     2 | loss: 4.8203979Losses:  5.515024662017822 1.341036081314087
CurrentTrain: epoch  2, batch     3 | loss: 6.8560610Losses:  4.599679470062256 1.314953088760376
CurrentTrain: epoch  2, batch     4 | loss: 5.9146328Losses:  5.520167350769043 1.0226244926452637
CurrentTrain: epoch  2, batch     5 | loss: 6.5427918Losses:  2.8566856384277344 0.20867526531219482
CurrentTrain: epoch  2, batch     6 | loss: 3.0653610Losses:  4.050853729248047 0.8863916397094727
CurrentTrain: epoch  3, batch     0 | loss: 4.9372454Losses:  2.957732915878296 0.7960715889930725
CurrentTrain: epoch  3, batch     1 | loss: 3.7538044Losses:  4.771768569946289 0.9124525189399719
CurrentTrain: epoch  3, batch     2 | loss: 5.6842213Losses:  4.3236613273620605 0.6338525414466858
CurrentTrain: epoch  3, batch     3 | loss: 4.9575138Losses:  5.202058792114258 1.5006176233291626
CurrentTrain: epoch  3, batch     4 | loss: 6.7026763Losses:  4.840628623962402 1.3742344379425049
CurrentTrain: epoch  3, batch     5 | loss: 6.2148628Losses:  4.956048011779785 0.23807647824287415
CurrentTrain: epoch  3, batch     6 | loss: 5.1941247Losses:  4.111789226531982 1.0287046432495117
CurrentTrain: epoch  4, batch     0 | loss: 5.1404939Losses:  4.79396915435791 1.2804970741271973
CurrentTrain: epoch  4, batch     1 | loss: 6.0744662Losses:  4.243916034698486 1.0748472213745117
CurrentTrain: epoch  4, batch     2 | loss: 5.3187633Losses:  3.639526844024658 0.8688478469848633
CurrentTrain: epoch  4, batch     3 | loss: 4.5083747Losses:  3.3803844451904297 0.8340387940406799
CurrentTrain: epoch  4, batch     4 | loss: 4.2144232Losses:  3.380215644836426 1.017024278640747
CurrentTrain: epoch  4, batch     5 | loss: 4.3972397Losses:  5.313173294067383 0.33332908153533936
CurrentTrain: epoch  4, batch     6 | loss: 5.6465025Losses:  2.9071967601776123 1.0351183414459229
CurrentTrain: epoch  5, batch     0 | loss: 3.9423151Losses:  4.0951409339904785 1.0285435914993286
CurrentTrain: epoch  5, batch     1 | loss: 5.1236844Losses:  4.117082595825195 1.0301257371902466
CurrentTrain: epoch  5, batch     2 | loss: 5.1472082Losses:  3.6940572261810303 0.8027896285057068
CurrentTrain: epoch  5, batch     3 | loss: 4.4968467Losses:  3.5756030082702637 0.7833027839660645
CurrentTrain: epoch  5, batch     4 | loss: 4.3589058Losses:  3.533134698867798 0.9951671361923218
CurrentTrain: epoch  5, batch     5 | loss: 4.5283017Losses:  3.885308265686035 0.2626558542251587
CurrentTrain: epoch  5, batch     6 | loss: 4.1479640Losses:  3.8579893112182617 0.9128229022026062
CurrentTrain: epoch  6, batch     0 | loss: 4.7708120Losses:  3.907609462738037 1.179147720336914
CurrentTrain: epoch  6, batch     1 | loss: 5.0867572Losses:  3.433690309524536 0.6352639198303223
CurrentTrain: epoch  6, batch     2 | loss: 4.0689545Losses:  3.0767478942871094 0.9186197519302368
CurrentTrain: epoch  6, batch     3 | loss: 3.9953675Losses:  2.9456725120544434 0.8973882794380188
CurrentTrain: epoch  6, batch     4 | loss: 3.8430607Losses:  3.272538185119629 0.9562384486198425
CurrentTrain: epoch  6, batch     5 | loss: 4.2287765Losses:  5.0462493896484375 0.49968892335891724
CurrentTrain: epoch  6, batch     6 | loss: 5.5459385Losses:  2.916937828063965 0.8616436719894409
CurrentTrain: epoch  7, batch     0 | loss: 3.7785816Losses:  3.148735761642456 0.6661839485168457
CurrentTrain: epoch  7, batch     1 | loss: 3.8149197Losses:  4.272486686706543 1.1843016147613525
CurrentTrain: epoch  7, batch     2 | loss: 5.4567881Losses:  2.5697901248931885 0.6072617769241333
CurrentTrain: epoch  7, batch     3 | loss: 3.1770520Losses:  3.1436104774475098 0.6289743185043335
CurrentTrain: epoch  7, batch     4 | loss: 3.7725849Losses:  3.1577346324920654 0.9938700199127197
CurrentTrain: epoch  7, batch     5 | loss: 4.1516047Losses:  1.964789628982544 0.22378958761692047
CurrentTrain: epoch  7, batch     6 | loss: 2.1885793Losses:  2.8312737941741943 0.743176281452179
CurrentTrain: epoch  8, batch     0 | loss: 3.5744500Losses:  2.484795093536377 0.6354506611824036
CurrentTrain: epoch  8, batch     1 | loss: 3.1202457Losses:  2.4376072883605957 0.6116346120834351
CurrentTrain: epoch  8, batch     2 | loss: 3.0492420Losses:  3.8482680320739746 0.7988300323486328
CurrentTrain: epoch  8, batch     3 | loss: 4.6470981Losses:  2.200291633605957 0.5970104336738586
CurrentTrain: epoch  8, batch     4 | loss: 2.7973020Losses:  3.0681703090667725 0.7683358788490295
CurrentTrain: epoch  8, batch     5 | loss: 3.8365061Losses:  4.496885299682617 0.2512698471546173
CurrentTrain: epoch  8, batch     6 | loss: 4.7481551Losses:  3.756612777709961 0.7422851324081421
CurrentTrain: epoch  9, batch     0 | loss: 4.4988980Losses:  2.7040648460388184 0.7697474956512451
CurrentTrain: epoch  9, batch     1 | loss: 3.4738123Losses:  2.8939619064331055 0.8095694780349731
CurrentTrain: epoch  9, batch     2 | loss: 3.7035313Losses:  2.0767667293548584 0.573544442653656
CurrentTrain: epoch  9, batch     3 | loss: 2.6503112Losses:  2.531341314315796 0.7651752233505249
CurrentTrain: epoch  9, batch     4 | loss: 3.2965164Losses:  2.703122854232788 0.8058954477310181
CurrentTrain: epoch  9, batch     5 | loss: 3.5090184Losses:  2.212397813796997 0.19044606387615204
CurrentTrain: epoch  9, batch     6 | loss: 2.4028440
Losses:  1.7099640369415283 0.6085644960403442
MemoryTrain:  epoch  0, batch     0 | loss: 2.3185287Losses:  0.4913380742073059 0.8226245641708374
MemoryTrain:  epoch  0, batch     1 | loss: 1.3139627Losses:  0.84096360206604 0.4396743178367615
MemoryTrain:  epoch  0, batch     2 | loss: 1.2806380Losses:  0.8049595355987549 0.5391337275505066
MemoryTrain:  epoch  0, batch     3 | loss: 1.3440933Losses:  0.4929477572441101 0.7341817617416382
MemoryTrain:  epoch  1, batch     0 | loss: 1.2271295Losses:  2.0091357231140137 0.6168831586837769
MemoryTrain:  epoch  1, batch     1 | loss: 2.6260190Losses:  0.3199635148048401 0.5509418845176697
MemoryTrain:  epoch  1, batch     2 | loss: 0.8709054Losses:  1.1733651161193848 0.4817281663417816
MemoryTrain:  epoch  1, batch     3 | loss: 1.6550933Losses:  0.6706796884536743 0.6733838319778442
MemoryTrain:  epoch  2, batch     0 | loss: 1.3440635Losses:  0.8442328572273254 0.6165416240692139
MemoryTrain:  epoch  2, batch     1 | loss: 1.4607744Losses:  0.21996542811393738 0.5127674341201782
MemoryTrain:  epoch  2, batch     2 | loss: 0.7327329Losses:  0.3841111361980438 0.5057480335235596
MemoryTrain:  epoch  2, batch     3 | loss: 0.8898592Losses:  0.11916495859622955 0.402230441570282
MemoryTrain:  epoch  3, batch     0 | loss: 0.5213954Losses:  0.20788736641407013 0.4667855501174927
MemoryTrain:  epoch  3, batch     1 | loss: 0.6746729Losses:  0.8728507161140442 0.57366544008255
MemoryTrain:  epoch  3, batch     2 | loss: 1.4465162Losses:  0.677984356880188 0.8595439791679382
MemoryTrain:  epoch  3, batch     3 | loss: 1.5375283Losses:  0.13884952664375305 0.7989387512207031
MemoryTrain:  epoch  4, batch     0 | loss: 0.9377882Losses:  0.6870638132095337 0.6068474650382996
MemoryTrain:  epoch  4, batch     1 | loss: 1.2939112Losses:  0.04211275279521942 0.4732951819896698
MemoryTrain:  epoch  4, batch     2 | loss: 0.5154079Losses:  0.24531051516532898 0.49647533893585205
MemoryTrain:  epoch  4, batch     3 | loss: 0.7417859Losses:  0.22939372062683105 0.7069571018218994
MemoryTrain:  epoch  5, batch     0 | loss: 0.9363508Losses:  0.3303017020225525 0.6036335825920105
MemoryTrain:  epoch  5, batch     1 | loss: 0.9339353Losses:  0.029893623664975166 0.37782275676727295
MemoryTrain:  epoch  5, batch     2 | loss: 0.4077164Losses:  0.10927706956863403 0.381879061460495
MemoryTrain:  epoch  5, batch     3 | loss: 0.4911561Losses:  0.06616297364234924 0.5862430334091187
MemoryTrain:  epoch  6, batch     0 | loss: 0.6524060Losses:  0.09000234305858612 0.49566665291786194
MemoryTrain:  epoch  6, batch     1 | loss: 0.5856690Losses:  0.049926452338695526 0.45471468567848206
MemoryTrain:  epoch  6, batch     2 | loss: 0.5046411Losses:  0.05427267774939537 0.6978185176849365
MemoryTrain:  epoch  6, batch     3 | loss: 0.7520912Losses:  0.02806738391518593 0.2499120831489563
MemoryTrain:  epoch  7, batch     0 | loss: 0.2779795Losses:  0.07617486268281937 0.8344720005989075
MemoryTrain:  epoch  7, batch     1 | loss: 0.9106469Losses:  0.02760646864771843 0.43651828169822693
MemoryTrain:  epoch  7, batch     2 | loss: 0.4641247Losses:  0.06721597164869308 0.4384440779685974
MemoryTrain:  epoch  7, batch     3 | loss: 0.5056601Losses:  0.040845341980457306 0.4825519025325775
MemoryTrain:  epoch  8, batch     0 | loss: 0.5233973Losses:  0.038750022649765015 0.6297987699508667
MemoryTrain:  epoch  8, batch     1 | loss: 0.6685488Losses:  0.07265074551105499 0.642005205154419
MemoryTrain:  epoch  8, batch     2 | loss: 0.7146559Losses:  0.028249604627490044 0.26223403215408325
MemoryTrain:  epoch  8, batch     3 | loss: 0.2904836Losses:  0.01759624108672142 0.32298606634140015
MemoryTrain:  epoch  9, batch     0 | loss: 0.3405823Losses:  0.03266055881977081 0.5851235389709473
MemoryTrain:  epoch  9, batch     1 | loss: 0.6177841Losses:  0.04796433448791504 0.5089648962020874
MemoryTrain:  epoch  9, batch     2 | loss: 0.5569292Losses:  0.05130523443222046 0.5706720948219299
MemoryTrain:  epoch  9, batch     3 | loss: 0.6219773
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 48.03%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 50.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.98%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 54.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 55.82%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 53.83%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 55.68%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.04%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 92.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 91.70%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 89.26%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 87.88%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 86.55%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 85.26%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 84.01%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 83.06%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 82.75%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 82.55%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 82.36%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 81.73%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.65%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 81.33%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 81.33%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 81.02%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 80.57%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 80.06%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 79.71%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 79.14%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 78.88%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 81.31%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 81.19%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 81.07%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.50%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 80.22%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 79.50%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 79.13%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 79.36%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 79.27%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 79.18%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 78.78%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 78.70%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 78.27%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 77.69%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 77.42%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 77.02%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 76.72%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 77.36%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 76.80%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 76.34%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 75.98%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 75.53%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 74.65%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 75.33%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 74.63%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 74.39%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 74.03%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 73.76%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 73.81%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  172 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 75.96%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 75.92%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 75.88%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 75.87%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 75.95%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 75.80%   
cur_acc:  ['0.9464', '0.7192', '0.7004']
his_acc:  ['0.9464', '0.8345', '0.7580']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15  2  1 12  1 10  0  4  1  1  1 17 16  2  1
  1  5  1 18  1  8  1 14  1  1  7  1  1  1  3  6]
Losses:  5.314727783203125 1.3093196153640747
CurrentTrain: epoch  0, batch     0 | loss: 6.6240473Losses:  5.760139465332031 1.129724144935608
CurrentTrain: epoch  0, batch     1 | loss: 6.8898635Losses:  6.648175239562988 1.3409423828125
CurrentTrain: epoch  0, batch     2 | loss: 7.9891176Losses:  5.85508918762207 1.2408528327941895
CurrentTrain: epoch  0, batch     3 | loss: 7.0959420Losses:  5.332492828369141 1.051344871520996
CurrentTrain: epoch  0, batch     4 | loss: 6.3838377Losses:  6.585216522216797 1.3026139736175537
CurrentTrain: epoch  0, batch     5 | loss: 7.8878307Losses:  7.141229629516602 0.15388764441013336
CurrentTrain: epoch  0, batch     6 | loss: 7.2951174Losses:  5.227159023284912 1.4200198650360107
CurrentTrain: epoch  1, batch     0 | loss: 6.6471786Losses:  6.256026268005371 1.2078475952148438
CurrentTrain: epoch  1, batch     1 | loss: 7.4638739Losses:  4.677770614624023 0.8777056932449341
CurrentTrain: epoch  1, batch     2 | loss: 5.5554762Losses:  4.5066022872924805 1.3914777040481567
CurrentTrain: epoch  1, batch     3 | loss: 5.8980799Losses:  3.7770419120788574 1.1222083568572998
CurrentTrain: epoch  1, batch     4 | loss: 4.8992500Losses:  4.1273393630981445 0.9456339478492737
CurrentTrain: epoch  1, batch     5 | loss: 5.0729733Losses:  6.067646026611328 0.42282214760780334
CurrentTrain: epoch  1, batch     6 | loss: 6.4904680Losses:  3.7028207778930664 1.161818027496338
CurrentTrain: epoch  2, batch     0 | loss: 4.8646388Losses:  4.834414005279541 0.948281466960907
CurrentTrain: epoch  2, batch     1 | loss: 5.7826953Losses:  4.326334476470947 1.1043152809143066
CurrentTrain: epoch  2, batch     2 | loss: 5.4306498Losses:  3.890162229537964 0.9363291263580322
CurrentTrain: epoch  2, batch     3 | loss: 4.8264914Losses:  3.7486348152160645 1.2984938621520996
CurrentTrain: epoch  2, batch     4 | loss: 5.0471287Losses:  4.942758083343506 1.3340489864349365
CurrentTrain: epoch  2, batch     5 | loss: 6.2768068Losses:  3.000095844268799 0.37122124433517456
CurrentTrain: epoch  2, batch     6 | loss: 3.3713171Losses:  4.504225730895996 1.432720422744751
CurrentTrain: epoch  3, batch     0 | loss: 5.9369459Losses:  3.4002771377563477 1.0351694822311401
CurrentTrain: epoch  3, batch     1 | loss: 4.4354467Losses:  3.6125597953796387 1.2286726236343384
CurrentTrain: epoch  3, batch     2 | loss: 4.8412323Losses:  3.1860194206237793 0.8607597351074219
CurrentTrain: epoch  3, batch     3 | loss: 4.0467792Losses:  4.507839202880859 1.0648472309112549
CurrentTrain: epoch  3, batch     4 | loss: 5.5726862Losses:  2.8049793243408203 0.8240785598754883
CurrentTrain: epoch  3, batch     5 | loss: 3.6290579Losses:  2.646855354309082 0.36616045236587524
CurrentTrain: epoch  3, batch     6 | loss: 3.0130157Losses:  2.9083943367004395 0.791604220867157
CurrentTrain: epoch  4, batch     0 | loss: 3.6999986Losses:  3.5947201251983643 1.129476547241211
CurrentTrain: epoch  4, batch     1 | loss: 4.7241964Losses:  3.7614333629608154 1.1999956369400024
CurrentTrain: epoch  4, batch     2 | loss: 4.9614291Losses:  3.4356143474578857 0.5664445161819458
CurrentTrain: epoch  4, batch     3 | loss: 4.0020590Losses:  2.564049482345581 0.7322103977203369
CurrentTrain: epoch  4, batch     4 | loss: 3.2962599Losses:  3.4920454025268555 1.1299490928649902
CurrentTrain: epoch  4, batch     5 | loss: 4.6219945Losses:  2.094924211502075 0.33147910237312317
CurrentTrain: epoch  4, batch     6 | loss: 2.4264033Losses:  3.3426132202148438 0.9624103903770447
CurrentTrain: epoch  5, batch     0 | loss: 4.3050237Losses:  3.467690944671631 0.8565512895584106
CurrentTrain: epoch  5, batch     1 | loss: 4.3242421Losses:  2.4626169204711914 1.014554500579834
CurrentTrain: epoch  5, batch     2 | loss: 3.4771714Losses:  2.293919086456299 0.5384207963943481
CurrentTrain: epoch  5, batch     3 | loss: 2.8323398Losses:  3.7170627117156982 0.6123411655426025
CurrentTrain: epoch  5, batch     4 | loss: 4.3294039Losses:  2.280780792236328 0.7408262491226196
CurrentTrain: epoch  5, batch     5 | loss: 3.0216069Losses:  3.0184597969055176 0.17813873291015625
CurrentTrain: epoch  5, batch     6 | loss: 3.1965985Losses:  2.175704002380371 0.8306176066398621
CurrentTrain: epoch  6, batch     0 | loss: 3.0063217Losses:  2.8644607067108154 0.8152285218238831
CurrentTrain: epoch  6, batch     1 | loss: 3.6796892Losses:  3.0600783824920654 0.7477884292602539
CurrentTrain: epoch  6, batch     2 | loss: 3.8078668Losses:  2.3392763137817383 0.9002014398574829
CurrentTrain: epoch  6, batch     3 | loss: 3.2394776Losses:  3.3119750022888184 1.0778260231018066
CurrentTrain: epoch  6, batch     4 | loss: 4.3898010Losses:  2.7432518005371094 0.7484054565429688
CurrentTrain: epoch  6, batch     5 | loss: 3.4916573Losses:  1.8690931797027588 0.06885109841823578
CurrentTrain: epoch  6, batch     6 | loss: 1.9379443Losses:  2.3328776359558105 0.6029239296913147
CurrentTrain: epoch  7, batch     0 | loss: 2.9358015Losses:  2.186218500137329 0.6173827648162842
CurrentTrain: epoch  7, batch     1 | loss: 2.8036013Losses:  2.132236957550049 0.42125657200813293
CurrentTrain: epoch  7, batch     2 | loss: 2.5534935Losses:  2.6052422523498535 0.37879592180252075
CurrentTrain: epoch  7, batch     3 | loss: 2.9840381Losses:  2.817823886871338 1.0425002574920654
CurrentTrain: epoch  7, batch     4 | loss: 3.8603241Losses:  2.3891215324401855 1.0414230823516846
CurrentTrain: epoch  7, batch     5 | loss: 3.4305446Losses:  2.842430591583252 0.3270862102508545
CurrentTrain: epoch  7, batch     6 | loss: 3.1695168Losses:  2.239138603210449 0.6625195741653442
CurrentTrain: epoch  8, batch     0 | loss: 2.9016581Losses:  3.1008520126342773 0.7282788753509521
CurrentTrain: epoch  8, batch     1 | loss: 3.8291309Losses:  1.9758236408233643 0.8813959360122681
CurrentTrain: epoch  8, batch     2 | loss: 2.8572197Losses:  2.289910316467285 0.7236802577972412
CurrentTrain: epoch  8, batch     3 | loss: 3.0135906Losses:  2.180570602416992 0.562244713306427
CurrentTrain: epoch  8, batch     4 | loss: 2.7428153Losses:  1.984057903289795 0.6698145270347595
CurrentTrain: epoch  8, batch     5 | loss: 2.6538725Losses:  1.8003312349319458 0.1770487129688263
CurrentTrain: epoch  8, batch     6 | loss: 1.9773799Losses:  1.9905914068222046 0.6304380893707275
CurrentTrain: epoch  9, batch     0 | loss: 2.6210294Losses:  2.5965423583984375 0.4454679489135742
CurrentTrain: epoch  9, batch     1 | loss: 3.0420103Losses:  2.037986993789673 0.7546361684799194
CurrentTrain: epoch  9, batch     2 | loss: 2.7926230Losses:  2.081878900527954 0.6995550394058228
CurrentTrain: epoch  9, batch     3 | loss: 2.7814341Losses:  2.3547754287719727 0.6140198707580566
CurrentTrain: epoch  9, batch     4 | loss: 2.9687953Losses:  2.0759119987487793 0.772736668586731
CurrentTrain: epoch  9, batch     5 | loss: 2.8486485Losses:  1.8691513538360596 0.03023824468255043
CurrentTrain: epoch  9, batch     6 | loss: 1.8993896
Losses:  1.431410789489746 0.6263779997825623
MemoryTrain:  epoch  0, batch     0 | loss: 2.0577888Losses:  0.7177440524101257 0.47793036699295044
MemoryTrain:  epoch  0, batch     1 | loss: 1.1956744Losses:  1.877663493156433 0.617426872253418
MemoryTrain:  epoch  0, batch     2 | loss: 2.4950905Losses:  0.5464153289794922 0.34601831436157227
MemoryTrain:  epoch  0, batch     3 | loss: 0.8924336Losses:  0.634177029132843 0.6232680678367615
MemoryTrain:  epoch  0, batch     4 | loss: 1.2574451Losses:  1.7259130477905273 0.6689150333404541
MemoryTrain:  epoch  1, batch     0 | loss: 2.3948281Losses:  0.8953102231025696 0.6440944075584412
MemoryTrain:  epoch  1, batch     1 | loss: 1.5394046Losses:  0.2619643807411194 0.601146936416626
MemoryTrain:  epoch  1, batch     2 | loss: 0.8631113Losses:  0.9115554094314575 0.4799853265285492
MemoryTrain:  epoch  1, batch     3 | loss: 1.3915408Losses:  1.6817114353179932 0.7320481538772583
MemoryTrain:  epoch  1, batch     4 | loss: 2.4137597Losses:  0.21326223015785217 0.42687827348709106
MemoryTrain:  epoch  2, batch     0 | loss: 0.6401405Losses:  1.2313517332077026 0.6207114458084106
MemoryTrain:  epoch  2, batch     1 | loss: 1.8520632Losses:  0.5615636706352234 0.8451105356216431
MemoryTrain:  epoch  2, batch     2 | loss: 1.4066741Losses:  0.4760230779647827 0.7472820281982422
MemoryTrain:  epoch  2, batch     3 | loss: 1.2233051Losses:  0.10325084626674652 0.5252164602279663
MemoryTrain:  epoch  2, batch     4 | loss: 0.6284673Losses:  0.16985830664634705 0.7570005655288696
MemoryTrain:  epoch  3, batch     0 | loss: 0.9268589Losses:  1.237962007522583 0.6115442514419556
MemoryTrain:  epoch  3, batch     1 | loss: 1.8495063Losses:  0.763595700263977 0.7685518264770508
MemoryTrain:  epoch  3, batch     2 | loss: 1.5321475Losses:  0.5719282627105713 0.520168662071228
MemoryTrain:  epoch  3, batch     3 | loss: 1.0920969Losses:  0.03822978585958481 0.4225528836250305
MemoryTrain:  epoch  3, batch     4 | loss: 0.4607827Losses:  0.08382198214530945 0.5646942257881165
MemoryTrain:  epoch  4, batch     0 | loss: 0.6485162Losses:  0.22484377026557922 0.6745295524597168
MemoryTrain:  epoch  4, batch     1 | loss: 0.8993733Losses:  0.33987095952033997 0.5598820447921753
MemoryTrain:  epoch  4, batch     2 | loss: 0.8997530Losses:  0.5497069954872131 0.5199190974235535
MemoryTrain:  epoch  4, batch     3 | loss: 1.0696261Losses:  0.6984032988548279 0.632027268409729
MemoryTrain:  epoch  4, batch     4 | loss: 1.3304305Losses:  0.1420256644487381 0.8222497701644897
MemoryTrain:  epoch  5, batch     0 | loss: 0.9642754Losses:  0.1696876734495163 0.4267248511314392
MemoryTrain:  epoch  5, batch     1 | loss: 0.5964125Losses:  0.40983226895332336 0.28265026211738586
MemoryTrain:  epoch  5, batch     2 | loss: 0.6924825Losses:  0.18283510208129883 0.3793647885322571
MemoryTrain:  epoch  5, batch     3 | loss: 0.5621999Losses:  0.23921459913253784 0.8358500003814697
MemoryTrain:  epoch  5, batch     4 | loss: 1.0750647Losses:  0.5080217123031616 0.7190005779266357
MemoryTrain:  epoch  6, batch     0 | loss: 1.2270223Losses:  0.07970822602510452 0.41281720995903015
MemoryTrain:  epoch  6, batch     1 | loss: 0.4925254Losses:  0.08322624117136002 0.6017378568649292
MemoryTrain:  epoch  6, batch     2 | loss: 0.6849641Losses:  0.10172969102859497 0.5736504197120667
MemoryTrain:  epoch  6, batch     3 | loss: 0.6753801Losses:  0.12321540713310242 0.42538216710090637
MemoryTrain:  epoch  6, batch     4 | loss: 0.5485976Losses:  0.3251607418060303 0.5650436878204346
MemoryTrain:  epoch  7, batch     0 | loss: 0.8902044Losses:  0.042211420834064484 0.4555833339691162
MemoryTrain:  epoch  7, batch     1 | loss: 0.4977947Losses:  0.03729218989610672 0.45674073696136475
MemoryTrain:  epoch  7, batch     2 | loss: 0.4940329Losses:  0.07901591062545776 0.6132559776306152
MemoryTrain:  epoch  7, batch     3 | loss: 0.6922719Losses:  0.06897592544555664 0.38761794567108154
MemoryTrain:  epoch  7, batch     4 | loss: 0.4565939Losses:  0.07764752954244614 0.49099820852279663
MemoryTrain:  epoch  8, batch     0 | loss: 0.5686457Losses:  0.045174770057201385 0.6472512483596802
MemoryTrain:  epoch  8, batch     1 | loss: 0.6924260Losses:  0.07384973019361496 0.7907081842422485
MemoryTrain:  epoch  8, batch     2 | loss: 0.8645579Losses:  0.01870403066277504 0.34788674116134644
MemoryTrain:  epoch  8, batch     3 | loss: 0.3665908Losses:  0.020520582795143127 0.5446891188621521
MemoryTrain:  epoch  8, batch     4 | loss: 0.5652097Losses:  0.03179539740085602 0.5031412243843079
MemoryTrain:  epoch  9, batch     0 | loss: 0.5349366Losses:  0.023878134787082672 0.32545769214630127
MemoryTrain:  epoch  9, batch     1 | loss: 0.3493358Losses:  0.07981432974338531 0.46819496154785156
MemoryTrain:  epoch  9, batch     2 | loss: 0.5480093Losses:  0.053193598985672 0.5548810958862305
MemoryTrain:  epoch  9, batch     3 | loss: 0.6080747Losses:  0.09388069063425064 0.6654362678527832
MemoryTrain:  epoch  9, batch     4 | loss: 0.7593170
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.02%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 74.89%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 74.25%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.94%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.61%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 91.78%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 90.73%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 89.83%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 89.38%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 88.83%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 88.41%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 87.60%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 86.23%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 84.90%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 83.62%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 82.37%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 81.16%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 80.25%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 80.28%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 80.12%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 79.97%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.98%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 78.96%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 78.52%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 77.64%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 76.16%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 75.79%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 78.59%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 78.62%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 78.64%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 78.61%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 78.77%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.50%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 77.75%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 77.14%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.21%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.44%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 77.52%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 77.08%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 76.60%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 76.13%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 75.66%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 75.15%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 74.65%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 74.21%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 73.72%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 73.49%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 73.26%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 72.88%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 72.61%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.37%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 72.84%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.37%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.94%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.43%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 70.94%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 70.62%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 71.48%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 71.05%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 70.67%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 71.45%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 71.29%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 71.10%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 71.01%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 70.79%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 70.51%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 70.36%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.37%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 70.60%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 70.49%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 70.62%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 70.39%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 70.10%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 69.85%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 69.60%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 69.48%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 70.33%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.14%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 71.40%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 71.31%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 71.18%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 70.91%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 71.65%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 71.67%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 71.66%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 71.56%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.64%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 71.67%   
cur_acc:  ['0.9464', '0.7192', '0.7004', '0.7361']
his_acc:  ['0.9464', '0.8345', '0.7580', '0.7167']
Clustering into  24  clusters
Clusters:  [ 0 23 13  0  0  0 21  0 12 15 20  0 14  0 22 17 11  0  0  0 19 18  1  0
  0  6  0  9  0 10  0 16  0  5  8  0  0  0  2  7  3  0  0  1  1  4  0  0
  0  0]
Losses:  6.847210884094238 1.4414491653442383
CurrentTrain: epoch  0, batch     0 | loss: 8.2886600Losses:  6.719235420227051 1.662628412246704
CurrentTrain: epoch  0, batch     1 | loss: 8.3818636Losses:  6.379426002502441 1.556650161743164
CurrentTrain: epoch  0, batch     2 | loss: 7.9360762Losses:  7.561391353607178 1.6504080295562744
CurrentTrain: epoch  0, batch     3 | loss: 9.2117996Losses:  5.715267658233643 1.3073370456695557
CurrentTrain: epoch  0, batch     4 | loss: 7.0226049Losses:  7.074151992797852 1.6604201793670654
CurrentTrain: epoch  0, batch     5 | loss: 8.7345724Losses:  6.107341766357422 0.5276142358779907
CurrentTrain: epoch  0, batch     6 | loss: 6.6349559Losses:  6.1144633293151855 1.4721803665161133
CurrentTrain: epoch  1, batch     0 | loss: 7.5866437Losses:  5.778202056884766 1.5669009685516357
CurrentTrain: epoch  1, batch     1 | loss: 7.3451033Losses:  6.169770240783691 1.376856803894043
CurrentTrain: epoch  1, batch     2 | loss: 7.5466270Losses:  4.689940929412842 1.3855695724487305
CurrentTrain: epoch  1, batch     3 | loss: 6.0755105Losses:  4.525982856750488 1.2210992574691772
CurrentTrain: epoch  1, batch     4 | loss: 5.7470822Losses:  6.059187412261963 1.452812910079956
CurrentTrain: epoch  1, batch     5 | loss: 7.5120001Losses:  4.105093002319336 0.495663046836853
CurrentTrain: epoch  1, batch     6 | loss: 4.6007562Losses:  3.6961889266967773 0.9910836815834045
CurrentTrain: epoch  2, batch     0 | loss: 4.6872725Losses:  4.785256862640381 1.0737555027008057
CurrentTrain: epoch  2, batch     1 | loss: 5.8590126Losses:  4.658652305603027 1.590184211730957
CurrentTrain: epoch  2, batch     2 | loss: 6.2488365Losses:  4.870609283447266 1.4237475395202637
CurrentTrain: epoch  2, batch     3 | loss: 6.2943568Losses:  3.8721117973327637 1.3729872703552246
CurrentTrain: epoch  2, batch     4 | loss: 5.2450991Losses:  5.277548789978027 1.3160343170166016
CurrentTrain: epoch  2, batch     5 | loss: 6.5935831Losses:  7.336985111236572 0.5045841336250305
CurrentTrain: epoch  2, batch     6 | loss: 7.8415694Losses:  5.15499210357666 1.336512565612793
CurrentTrain: epoch  3, batch     0 | loss: 6.4915047Losses:  3.4066648483276367 1.1644043922424316
CurrentTrain: epoch  3, batch     1 | loss: 4.5710692Losses:  4.887387275695801 0.9304290413856506
CurrentTrain: epoch  3, batch     2 | loss: 5.8178163Losses:  4.426694869995117 1.125251293182373
CurrentTrain: epoch  3, batch     3 | loss: 5.5519462Losses:  4.012810230255127 1.1336324214935303
CurrentTrain: epoch  3, batch     4 | loss: 5.1464424Losses:  2.892305612564087 0.9885490536689758
CurrentTrain: epoch  3, batch     5 | loss: 3.8808546Losses:  3.040910243988037 0.19223076105117798
CurrentTrain: epoch  3, batch     6 | loss: 3.2331409Losses:  4.251230716705322 1.1956995725631714
CurrentTrain: epoch  4, batch     0 | loss: 5.4469304Losses:  3.7562811374664307 1.0434268712997437
CurrentTrain: epoch  4, batch     1 | loss: 4.7997079Losses:  4.302150726318359 1.019507646560669
CurrentTrain: epoch  4, batch     2 | loss: 5.3216581Losses:  3.3114078044891357 1.2757271528244019
CurrentTrain: epoch  4, batch     3 | loss: 4.5871348Losses:  4.106189727783203 1.079944372177124
CurrentTrain: epoch  4, batch     4 | loss: 5.1861343Losses:  3.2485077381134033 1.2298035621643066
CurrentTrain: epoch  4, batch     5 | loss: 4.4783115Losses:  2.7574076652526855 5.960465188081798e-08
CurrentTrain: epoch  4, batch     6 | loss: 2.7574077Losses:  4.297898292541504 1.4045284986495972
CurrentTrain: epoch  5, batch     0 | loss: 5.7024269Losses:  2.6547162532806396 1.1620181798934937
CurrentTrain: epoch  5, batch     1 | loss: 3.8167343Losses:  2.7320199012756348 0.7970762252807617
CurrentTrain: epoch  5, batch     2 | loss: 3.5290961Losses:  2.9273242950439453 0.977042555809021
CurrentTrain: epoch  5, batch     3 | loss: 3.9043670Losses:  4.089716911315918 0.7539272308349609
CurrentTrain: epoch  5, batch     4 | loss: 4.8436441Losses:  3.84033203125 0.8017823100090027
CurrentTrain: epoch  5, batch     5 | loss: 4.6421142Losses:  6.743105888366699 0.40233200788497925
CurrentTrain: epoch  5, batch     6 | loss: 7.1454377Losses:  3.454416275024414 0.9042364954948425
CurrentTrain: epoch  6, batch     0 | loss: 4.3586526Losses:  3.276827812194824 1.0083423852920532
CurrentTrain: epoch  6, batch     1 | loss: 4.2851701Losses:  3.8313100337982178 1.0150153636932373
CurrentTrain: epoch  6, batch     2 | loss: 4.8463254Losses:  2.938321113586426 1.0225133895874023
CurrentTrain: epoch  6, batch     3 | loss: 3.9608345Losses:  2.899820566177368 0.7213262915611267
CurrentTrain: epoch  6, batch     4 | loss: 3.6211469Losses:  2.7126150131225586 1.227420449256897
CurrentTrain: epoch  6, batch     5 | loss: 3.9400353Losses:  3.2649314403533936 0.28319594264030457
CurrentTrain: epoch  6, batch     6 | loss: 3.5481274Losses:  3.4908626079559326 1.0744661092758179
CurrentTrain: epoch  7, batch     0 | loss: 4.5653286Losses:  3.4747209548950195 1.048473834991455
CurrentTrain: epoch  7, batch     1 | loss: 4.5231948Losses:  3.3977057933807373 0.8628264665603638
CurrentTrain: epoch  7, batch     2 | loss: 4.2605324Losses:  2.7095065116882324 1.0035637617111206
CurrentTrain: epoch  7, batch     3 | loss: 3.7130704Losses:  2.2635347843170166 0.7073588371276855
CurrentTrain: epoch  7, batch     4 | loss: 2.9708936Losses:  2.9445595741271973 0.8759137392044067
CurrentTrain: epoch  7, batch     5 | loss: 3.8204732Losses:  1.9705909490585327 0.19978299736976624
CurrentTrain: epoch  7, batch     6 | loss: 2.1703739Losses:  2.2444682121276855 0.6991606950759888
CurrentTrain: epoch  8, batch     0 | loss: 2.9436288Losses:  3.9082531929016113 0.8390250205993652
CurrentTrain: epoch  8, batch     1 | loss: 4.7472782Losses:  2.595560073852539 1.102695107460022
CurrentTrain: epoch  8, batch     2 | loss: 3.6982551Losses:  3.292710304260254 1.0372458696365356
CurrentTrain: epoch  8, batch     3 | loss: 4.3299561Losses:  2.4253952503204346 0.9965665936470032
CurrentTrain: epoch  8, batch     4 | loss: 3.4219618Losses:  2.790781259536743 0.6438761949539185
CurrentTrain: epoch  8, batch     5 | loss: 3.4346576Losses:  1.9799118041992188 0.3364991843700409
CurrentTrain: epoch  8, batch     6 | loss: 2.3164110Losses:  3.711493968963623 0.7199611663818359
CurrentTrain: epoch  9, batch     0 | loss: 4.4314551Losses:  2.221266746520996 0.7420841455459595
CurrentTrain: epoch  9, batch     1 | loss: 2.9633508Losses:  2.35115122795105 0.7942424416542053
CurrentTrain: epoch  9, batch     2 | loss: 3.1453936Losses:  2.6767876148223877 1.0025182962417603
CurrentTrain: epoch  9, batch     3 | loss: 3.6793060Losses:  2.018327236175537 0.5757577419281006
CurrentTrain: epoch  9, batch     4 | loss: 2.5940850Losses:  2.6168150901794434 0.9014211893081665
CurrentTrain: epoch  9, batch     5 | loss: 3.5182362Losses:  2.4612905979156494 0.3103879392147064
CurrentTrain: epoch  9, batch     6 | loss: 2.7716784
Losses:  1.272294044494629 0.4299636483192444
MemoryTrain:  epoch  0, batch     0 | loss: 1.7022576Losses:  0.5508886575698853 0.5856992602348328
MemoryTrain:  epoch  0, batch     1 | loss: 1.1365879Losses:  0.13213053345680237 0.661171555519104
MemoryTrain:  epoch  0, batch     2 | loss: 0.7933021Losses:  0.10775701701641083 0.6261246204376221
MemoryTrain:  epoch  0, batch     3 | loss: 0.7338817Losses:  0.44390496611595154 0.6232084631919861
MemoryTrain:  epoch  0, batch     4 | loss: 1.0671134Losses:  0.3594907820224762 0.7789212465286255
MemoryTrain:  epoch  0, batch     5 | loss: 1.1384120Losses:  0.06638951599597931 0.09286091476678848
MemoryTrain:  epoch  0, batch     6 | loss: 0.1592504Losses:  0.7292179465293884 0.6444495916366577
MemoryTrain:  epoch  1, batch     0 | loss: 1.3736675Losses:  1.121619701385498 0.7889427542686462
MemoryTrain:  epoch  1, batch     1 | loss: 1.9105625Losses:  0.733003556728363 0.5047248005867004
MemoryTrain:  epoch  1, batch     2 | loss: 1.2377284Losses:  0.27901288866996765 0.7456949949264526
MemoryTrain:  epoch  1, batch     3 | loss: 1.0247079Losses:  0.44194984436035156 0.4120715260505676
MemoryTrain:  epoch  1, batch     4 | loss: 0.8540214Losses:  0.5643512606620789 0.7579965591430664
MemoryTrain:  epoch  1, batch     5 | loss: 1.3223479Losses:  0.3744007349014282 0.051613204181194305
MemoryTrain:  epoch  1, batch     6 | loss: 0.4260139Losses:  0.17089301347732544 0.5858113765716553
MemoryTrain:  epoch  2, batch     0 | loss: 0.7567044Losses:  0.16308355331420898 0.6650004386901855
MemoryTrain:  epoch  2, batch     1 | loss: 0.8280840Losses:  0.3378628194332123 0.5861077904701233
MemoryTrain:  epoch  2, batch     2 | loss: 0.9239706Losses:  0.32206714153289795 0.5981169939041138
MemoryTrain:  epoch  2, batch     3 | loss: 0.9201841Losses:  0.08322906494140625 0.4191085696220398
MemoryTrain:  epoch  2, batch     4 | loss: 0.5023376Losses:  0.9769330024719238 0.7292588949203491
MemoryTrain:  epoch  2, batch     5 | loss: 1.7061919Losses:  0.10164378583431244 0.15376871824264526
MemoryTrain:  epoch  2, batch     6 | loss: 0.2554125Losses:  0.16643449664115906 0.4522596597671509
MemoryTrain:  epoch  3, batch     0 | loss: 0.6186942Losses:  0.07489213347434998 0.4879535436630249
MemoryTrain:  epoch  3, batch     1 | loss: 0.5628457Losses:  0.04799529165029526 0.5679193735122681
MemoryTrain:  epoch  3, batch     2 | loss: 0.6159146Losses:  0.29173436760902405 0.6396029591560364
MemoryTrain:  epoch  3, batch     3 | loss: 0.9313374Losses:  0.17015458643436432 0.7063259482383728
MemoryTrain:  epoch  3, batch     4 | loss: 0.8764805Losses:  0.08982165157794952 0.48791947960853577
MemoryTrain:  epoch  3, batch     5 | loss: 0.5777411Losses:  0.20844879746437073 0.07153463363647461
MemoryTrain:  epoch  3, batch     6 | loss: 0.2799834Losses:  0.16300509870052338 0.5596821904182434
MemoryTrain:  epoch  4, batch     0 | loss: 0.7226873Losses:  0.08561202883720398 0.5452147722244263
MemoryTrain:  epoch  4, batch     1 | loss: 0.6308268Losses:  0.0788501650094986 0.7444227933883667
MemoryTrain:  epoch  4, batch     2 | loss: 0.8232729Losses:  0.09246683865785599 0.4791615903377533
MemoryTrain:  epoch  4, batch     3 | loss: 0.5716285Losses:  0.1158943772315979 0.6229395270347595
MemoryTrain:  epoch  4, batch     4 | loss: 0.7388339Losses:  0.03338425233960152 0.3157989978790283
MemoryTrain:  epoch  4, batch     5 | loss: 0.3491833Losses:  0.0515088215470314 0.5203979015350342
MemoryTrain:  epoch  4, batch     6 | loss: 0.5719067Losses:  0.0694579929113388 0.7492789626121521
MemoryTrain:  epoch  5, batch     0 | loss: 0.8187370Losses:  0.08801763504743576 0.5215108394622803
MemoryTrain:  epoch  5, batch     1 | loss: 0.6095285Losses:  0.048616498708724976 0.3821689188480377
MemoryTrain:  epoch  5, batch     2 | loss: 0.4307854Losses:  0.05118395388126373 0.4165133237838745
MemoryTrain:  epoch  5, batch     3 | loss: 0.4676973Losses:  0.03559024631977081 0.4967416524887085
MemoryTrain:  epoch  5, batch     4 | loss: 0.5323319Losses:  0.19355693459510803 0.4847918152809143
MemoryTrain:  epoch  5, batch     5 | loss: 0.6783488Losses:  0.07444266229867935 0.10379187762737274
MemoryTrain:  epoch  5, batch     6 | loss: 0.1782345Losses:  0.11188692599534988 0.6101964712142944
MemoryTrain:  epoch  6, batch     0 | loss: 0.7220834Losses:  0.04376412183046341 0.5234915018081665
MemoryTrain:  epoch  6, batch     1 | loss: 0.5672556Losses:  0.08932545781135559 0.8075926899909973
MemoryTrain:  epoch  6, batch     2 | loss: 0.8969182Losses:  0.0627065896987915 0.37832459807395935
MemoryTrain:  epoch  6, batch     3 | loss: 0.4410312Losses:  0.04526948928833008 0.3868837058544159
MemoryTrain:  epoch  6, batch     4 | loss: 0.4321532Losses:  0.0666244626045227 0.5431703925132751
MemoryTrain:  epoch  6, batch     5 | loss: 0.6097949Losses:  0.05470052734017372 0.028694666922092438
MemoryTrain:  epoch  6, batch     6 | loss: 0.0833952Losses:  0.06989400088787079 0.49236637353897095
MemoryTrain:  epoch  7, batch     0 | loss: 0.5622604Losses:  0.14653010666370392 0.8235772848129272
MemoryTrain:  epoch  7, batch     1 | loss: 0.9701074Losses:  0.048287294805049896 0.37228837609291077
MemoryTrain:  epoch  7, batch     2 | loss: 0.4205757Losses:  0.04851970076560974 0.48178550601005554
MemoryTrain:  epoch  7, batch     3 | loss: 0.5303052Losses:  0.0405258983373642 0.8050356507301331
MemoryTrain:  epoch  7, batch     4 | loss: 0.8455616Losses:  0.06000003591179848 0.42468881607055664
MemoryTrain:  epoch  7, batch     5 | loss: 0.4846888Losses:  0.08770790696144104 0.1180284321308136
MemoryTrain:  epoch  7, batch     6 | loss: 0.2057363Losses:  0.03919563069939613 0.5661090612411499
MemoryTrain:  epoch  8, batch     0 | loss: 0.6053047Losses:  0.033152855932712555 0.5578914880752563
MemoryTrain:  epoch  8, batch     1 | loss: 0.5910444Losses:  0.0377972349524498 0.6145268082618713
MemoryTrain:  epoch  8, batch     2 | loss: 0.6523240Losses:  0.034889012575149536 0.48002055287361145
MemoryTrain:  epoch  8, batch     3 | loss: 0.5149096Losses:  0.08706504106521606 0.4293906092643738
MemoryTrain:  epoch  8, batch     4 | loss: 0.5164557Losses:  0.06791839003562927 0.34736037254333496
MemoryTrain:  epoch  8, batch     5 | loss: 0.4152788Losses:  0.018089620396494865 0.038707584142684937
MemoryTrain:  epoch  8, batch     6 | loss: 0.0567972Losses:  0.031190618872642517 0.7995640635490417
MemoryTrain:  epoch  9, batch     0 | loss: 0.8307547Losses:  0.04961279034614563 0.3971831798553467
MemoryTrain:  epoch  9, batch     1 | loss: 0.4467960Losses:  0.035380952060222626 0.4586222171783447
MemoryTrain:  epoch  9, batch     2 | loss: 0.4940032Losses:  0.030526841059327126 0.3543194830417633
MemoryTrain:  epoch  9, batch     3 | loss: 0.3848463Losses:  0.04086720943450928 0.5528808832168579
MemoryTrain:  epoch  9, batch     4 | loss: 0.5937481Losses:  0.026563797146081924 0.3133837580680847
MemoryTrain:  epoch  9, batch     5 | loss: 0.3399476Losses:  0.028849512338638306 0.07328860461711884
MemoryTrain:  epoch  9, batch     6 | loss: 0.1021381
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 55.09%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 51.29%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 49.79%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 48.19%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 48.63%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 54.93%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 57.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 58.78%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 59.10%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 58.59%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.16%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 58.09%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.65%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.20%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 59.64%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.90%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 60.08%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.42%   
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.26%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.28%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.95%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 92.32%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 90.47%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 88.29%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 86.91%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 84.28%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 83.02%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 81.80%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 80.89%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.89%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 80.81%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 80.57%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 80.66%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 80.14%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 79.84%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.57%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.42%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 78.01%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 77.40%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 77.01%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.06%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 79.61%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 79.57%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 79.72%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 79.38%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 78.99%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 78.73%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 78.30%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 78.04%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.68%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 78.15%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 77.66%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 77.27%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 76.90%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 76.42%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 75.91%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 75.55%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 75.05%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 74.56%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.27%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 73.93%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 73.56%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 73.23%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 73.25%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 73.38%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.90%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 72.47%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.96%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 71.46%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.14%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 71.98%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 71.67%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 71.28%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.73%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.68%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.13%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 70.81%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 70.49%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 70.14%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 69.84%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.57%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 69.41%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 69.18%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 68.96%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 68.79%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.08%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 69.08%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 68.49%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 68.33%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 68.11%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 68.50%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 68.26%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 67.93%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 69.63%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 68.86%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 69.63%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 69.20%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 68.95%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 68.68%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 68.14%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.85%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  264 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 68.64%   [EVAL] batch:  273 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 68.52%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 68.30%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 68.05%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 67.81%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 67.56%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 67.34%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 68.01%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 67.84%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 67.60%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 67.52%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 67.64%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 67.73%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 67.58%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 67.43%   
cur_acc:  ['0.9464', '0.7192', '0.7004', '0.7361', '0.5942']
his_acc:  ['0.9464', '0.8345', '0.7580', '0.7167', '0.6743']
Clustering into  29  clusters
Clusters:  [ 0 19 15  0  0  0 24  0 27 28 20  0 17  0 14  9 18  0  0  0 23 21  2  0
  0 16  0 22  0 25  0 11  0 13 26  0  0  0  6  8 10  0  0  2  2  7  0  0
  0  0  0  0  0  4  0  3  0 12  5  1]
Losses:  5.816216468811035 0.8309732675552368
CurrentTrain: epoch  0, batch     0 | loss: 6.6471896Losses:  6.019157409667969 0.7380799055099487
CurrentTrain: epoch  0, batch     1 | loss: 6.7572374Losses:  5.966267108917236 1.1491938829421997
CurrentTrain: epoch  0, batch     2 | loss: 7.1154609Losses:  6.109745979309082 1.0119941234588623
CurrentTrain: epoch  0, batch     3 | loss: 7.1217403Losses:  6.094825744628906 0.9764665961265564
CurrentTrain: epoch  0, batch     4 | loss: 7.0712924Losses:  7.620799541473389 0.9836224317550659
CurrentTrain: epoch  0, batch     5 | loss: 8.6044216Losses:  6.125862121582031 0.7422043681144714
CurrentTrain: epoch  0, batch     6 | loss: 6.8680663Losses:  4.618062496185303 0.5382545590400696
CurrentTrain: epoch  1, batch     0 | loss: 5.1563172Losses:  6.06673526763916 0.9665194749832153
CurrentTrain: epoch  1, batch     1 | loss: 7.0332546Losses:  5.018624782562256 0.6782476902008057
CurrentTrain: epoch  1, batch     2 | loss: 5.6968727Losses:  5.12740421295166 0.7415679097175598
CurrentTrain: epoch  1, batch     3 | loss: 5.8689723Losses:  4.760432243347168 0.7426464557647705
CurrentTrain: epoch  1, batch     4 | loss: 5.5030785Losses:  4.925403118133545 0.7283945083618164
CurrentTrain: epoch  1, batch     5 | loss: 5.6537976Losses:  3.5553359985351562 0.3949710726737976
CurrentTrain: epoch  1, batch     6 | loss: 3.9503071Losses:  3.948406457901001 0.6403051614761353
CurrentTrain: epoch  2, batch     0 | loss: 4.5887117Losses:  4.3658599853515625 0.5071147680282593
CurrentTrain: epoch  2, batch     1 | loss: 4.8729749Losses:  4.931032657623291 0.8324679136276245
CurrentTrain: epoch  2, batch     2 | loss: 5.7635007Losses:  4.5051774978637695 0.5427666902542114
CurrentTrain: epoch  2, batch     3 | loss: 5.0479441Losses:  3.4880013465881348 0.5044925212860107
CurrentTrain: epoch  2, batch     4 | loss: 3.9924939Losses:  2.7632908821105957 0.6036468148231506
CurrentTrain: epoch  2, batch     5 | loss: 3.3669376Losses:  5.118229389190674 0.374525249004364
CurrentTrain: epoch  2, batch     6 | loss: 5.4927545Losses:  3.541820764541626 0.595473051071167
CurrentTrain: epoch  3, batch     0 | loss: 4.1372938Losses:  3.246459484100342 0.5460374355316162
CurrentTrain: epoch  3, batch     1 | loss: 3.7924969Losses:  3.166569232940674 0.42660367488861084
CurrentTrain: epoch  3, batch     2 | loss: 3.5931730Losses:  3.6461191177368164 0.6698454022407532
CurrentTrain: epoch  3, batch     3 | loss: 4.3159647Losses:  2.803309440612793 0.6537947654724121
CurrentTrain: epoch  3, batch     4 | loss: 3.4571042Losses:  2.7705304622650146 0.507622241973877
CurrentTrain: epoch  3, batch     5 | loss: 3.2781527Losses:  3.5987465381622314 0.05938960611820221
CurrentTrain: epoch  3, batch     6 | loss: 3.6581361Losses:  3.1234793663024902 0.7228063344955444
CurrentTrain: epoch  4, batch     0 | loss: 3.8462858Losses:  3.4381253719329834 0.609828531742096
CurrentTrain: epoch  4, batch     1 | loss: 4.0479541Losses:  2.816866397857666 0.5090746283531189
CurrentTrain: epoch  4, batch     2 | loss: 3.3259411Losses:  2.9156947135925293 0.7427284717559814
CurrentTrain: epoch  4, batch     3 | loss: 3.6584232Losses:  2.96923828125 0.6878142356872559
CurrentTrain: epoch  4, batch     4 | loss: 3.6570525Losses:  2.4862427711486816 0.34447959065437317
CurrentTrain: epoch  4, batch     5 | loss: 2.8307223Losses:  2.8572745323181152 8.94069742685133e-08
CurrentTrain: epoch  4, batch     6 | loss: 2.8572745Losses:  2.5638227462768555 0.5781767964363098
CurrentTrain: epoch  5, batch     0 | loss: 3.1419995Losses:  2.3294849395751953 0.4162358343601227
CurrentTrain: epoch  5, batch     1 | loss: 2.7457209Losses:  2.808135509490967 0.5792363882064819
CurrentTrain: epoch  5, batch     2 | loss: 3.3873720Losses:  3.0557618141174316 0.6390427350997925
CurrentTrain: epoch  5, batch     3 | loss: 3.6948047Losses:  2.12275767326355 0.3727409839630127
CurrentTrain: epoch  5, batch     4 | loss: 2.4954987Losses:  2.461088180541992 0.5837335586547852
CurrentTrain: epoch  5, batch     5 | loss: 3.0448217Losses:  2.2564404010772705 0.0
CurrentTrain: epoch  5, batch     6 | loss: 2.2564404Losses:  2.5460166931152344 0.6039189696311951
CurrentTrain: epoch  6, batch     0 | loss: 3.1499357Losses:  1.9757399559020996 0.34708520770072937
CurrentTrain: epoch  6, batch     1 | loss: 2.3228252Losses:  2.640944242477417 0.39450278878211975
CurrentTrain: epoch  6, batch     2 | loss: 3.0354471Losses:  2.1723740100860596 0.3968829810619354
CurrentTrain: epoch  6, batch     3 | loss: 2.5692570Losses:  2.1049394607543945 0.509021520614624
CurrentTrain: epoch  6, batch     4 | loss: 2.6139610Losses:  2.869842052459717 0.5592764019966125
CurrentTrain: epoch  6, batch     5 | loss: 3.4291184Losses:  2.5918068885803223 0.4806872010231018
CurrentTrain: epoch  6, batch     6 | loss: 3.0724940Losses:  2.6436426639556885 0.4476054906845093
CurrentTrain: epoch  7, batch     0 | loss: 3.0912480Losses:  2.359952926635742 0.4729772210121155
CurrentTrain: epoch  7, batch     1 | loss: 2.8329301Losses:  2.0656776428222656 0.44720354676246643
CurrentTrain: epoch  7, batch     2 | loss: 2.5128813Losses:  2.7901391983032227 0.49671486020088196
CurrentTrain: epoch  7, batch     3 | loss: 3.2868540Losses:  2.1295738220214844 0.27465322613716125
CurrentTrain: epoch  7, batch     4 | loss: 2.4042270Losses:  2.3068928718566895 0.374657541513443
CurrentTrain: epoch  7, batch     5 | loss: 2.6815505Losses:  2.3321642875671387 0.15661996603012085
CurrentTrain: epoch  7, batch     6 | loss: 2.4887843Losses:  2.6286685466766357 0.24845512211322784
CurrentTrain: epoch  8, batch     0 | loss: 2.8771236Losses:  2.0941123962402344 0.4121355712413788
CurrentTrain: epoch  8, batch     1 | loss: 2.5062480Losses:  2.3416078090667725 0.37186747789382935
CurrentTrain: epoch  8, batch     2 | loss: 2.7134752Losses:  1.8327136039733887 0.25278741121292114
CurrentTrain: epoch  8, batch     3 | loss: 2.0855010Losses:  2.083439826965332 0.5313520431518555
CurrentTrain: epoch  8, batch     4 | loss: 2.6147919Losses:  2.073434591293335 0.3569295406341553
CurrentTrain: epoch  8, batch     5 | loss: 2.4303641Losses:  2.449434995651245 0.02134144864976406
CurrentTrain: epoch  8, batch     6 | loss: 2.4707766Losses:  1.9711260795593262 0.4047558307647705
CurrentTrain: epoch  9, batch     0 | loss: 2.3758819Losses:  2.117278575897217 0.49909472465515137
CurrentTrain: epoch  9, batch     1 | loss: 2.6163733Losses:  2.072376012802124 0.5298004150390625
CurrentTrain: epoch  9, batch     2 | loss: 2.6021764Losses:  1.9188182353973389 0.39870280027389526
CurrentTrain: epoch  9, batch     3 | loss: 2.3175211Losses:  2.0949723720550537 0.30019810795783997
CurrentTrain: epoch  9, batch     4 | loss: 2.3951705Losses:  2.329268455505371 0.33204376697540283
CurrentTrain: epoch  9, batch     5 | loss: 2.6613121Losses:  1.980894923210144 0.28451380133628845
CurrentTrain: epoch  9, batch     6 | loss: 2.2654088
Losses:  0.23941589891910553 0.42534416913986206
MemoryTrain:  epoch  0, batch     0 | loss: 0.6647601Losses:  1.2192933559417725 0.47535648941993713
MemoryTrain:  epoch  0, batch     1 | loss: 1.6946498Losses:  0.8159103989601135 0.6375970244407654
MemoryTrain:  epoch  0, batch     2 | loss: 1.4535074Losses:  1.0113874673843384 0.4154213070869446
MemoryTrain:  epoch  0, batch     3 | loss: 1.4268088Losses:  0.14140549302101135 0.7765470743179321
MemoryTrain:  epoch  0, batch     4 | loss: 0.9179525Losses:  0.8238048553466797 0.44950154423713684
MemoryTrain:  epoch  0, batch     5 | loss: 1.2733064Losses:  1.3999755382537842 0.46901339292526245
MemoryTrain:  epoch  0, batch     6 | loss: 1.8689890Losses:  0.2717110812664032 0.29966726899147034
MemoryTrain:  epoch  0, batch     7 | loss: 0.5713784Losses:  1.330960750579834 0.42947590351104736
MemoryTrain:  epoch  1, batch     0 | loss: 1.7604367Losses:  1.5796048641204834 0.4632686376571655
MemoryTrain:  epoch  1, batch     1 | loss: 2.0428734Losses:  0.8842015266418457 0.360365629196167
MemoryTrain:  epoch  1, batch     2 | loss: 1.2445672Losses:  0.32060331106185913 0.7060597538948059
MemoryTrain:  epoch  1, batch     3 | loss: 1.0266631Losses:  1.1567789316177368 0.5011647939682007
MemoryTrain:  epoch  1, batch     4 | loss: 1.6579437Losses:  0.6841598749160767 0.48489829897880554
MemoryTrain:  epoch  1, batch     5 | loss: 1.1690582Losses:  0.8306493163108826 0.35803472995758057
MemoryTrain:  epoch  1, batch     6 | loss: 1.1886840Losses:  0.14370465278625488 0.2792145013809204
MemoryTrain:  epoch  1, batch     7 | loss: 0.4229192Losses:  0.7820435762405396 0.6134579181671143
MemoryTrain:  epoch  2, batch     0 | loss: 1.3955015Losses:  0.55333411693573 0.4844518303871155
MemoryTrain:  epoch  2, batch     1 | loss: 1.0377860Losses:  0.45123350620269775 0.5331000685691833
MemoryTrain:  epoch  2, batch     2 | loss: 0.9843336Losses:  0.5808160305023193 0.6215727925300598
MemoryTrain:  epoch  2, batch     3 | loss: 1.2023888Losses:  0.3924192190170288 0.4928966760635376
MemoryTrain:  epoch  2, batch     4 | loss: 0.8853159Losses:  0.10303456336259842 0.3613157868385315
MemoryTrain:  epoch  2, batch     5 | loss: 0.4643503Losses:  0.36396968364715576 0.3683457374572754
MemoryTrain:  epoch  2, batch     6 | loss: 0.7323154Losses:  0.1398552507162094 0.16743341088294983
MemoryTrain:  epoch  2, batch     7 | loss: 0.3072886Losses:  0.32848313450813293 0.6289071440696716
MemoryTrain:  epoch  3, batch     0 | loss: 0.9573903Losses:  0.44975459575653076 0.4088565409183502
MemoryTrain:  epoch  3, batch     1 | loss: 0.8586111Losses:  0.3222203552722931 0.6035717725753784
MemoryTrain:  epoch  3, batch     2 | loss: 0.9257921Losses:  0.13045717775821686 0.5348978638648987
MemoryTrain:  epoch  3, batch     3 | loss: 0.6653550Losses:  0.15594124794006348 0.4799588620662689
MemoryTrain:  epoch  3, batch     4 | loss: 0.6359001Losses:  0.23417621850967407 0.5798932909965515
MemoryTrain:  epoch  3, batch     5 | loss: 0.8140695Losses:  0.1418958604335785 0.230424165725708
MemoryTrain:  epoch  3, batch     6 | loss: 0.3723200Losses:  0.7629166841506958 0.256523460149765
MemoryTrain:  epoch  3, batch     7 | loss: 1.0194402Losses:  0.06388363987207413 0.635905921459198
MemoryTrain:  epoch  4, batch     0 | loss: 0.6997896Losses:  0.13961580395698547 0.6383254528045654
MemoryTrain:  epoch  4, batch     1 | loss: 0.7779412Losses:  0.05574037507176399 0.6308637857437134
MemoryTrain:  epoch  4, batch     2 | loss: 0.6866041Losses:  0.36688196659088135 0.4894545078277588
MemoryTrain:  epoch  4, batch     3 | loss: 0.8563365Losses:  0.24337564408779144 0.519005298614502
MemoryTrain:  epoch  4, batch     4 | loss: 0.7623810Losses:  0.06229744851589203 0.3798391819000244
MemoryTrain:  epoch  4, batch     5 | loss: 0.4421366Losses:  0.08919516205787659 0.4327998757362366
MemoryTrain:  epoch  4, batch     6 | loss: 0.5219951Losses:  0.12384002655744553 0.09250359237194061
MemoryTrain:  epoch  4, batch     7 | loss: 0.2163436Losses:  0.23726683855056763 0.4910396933555603
MemoryTrain:  epoch  5, batch     0 | loss: 0.7283065Losses:  0.4946400225162506 0.49496641755104065
MemoryTrain:  epoch  5, batch     1 | loss: 0.9896064Losses:  0.12794867157936096 0.5447124242782593
MemoryTrain:  epoch  5, batch     2 | loss: 0.6726611Losses:  0.05389779806137085 0.4682362973690033
MemoryTrain:  epoch  5, batch     3 | loss: 0.5221341Losses:  0.16024941205978394 0.5851193070411682
MemoryTrain:  epoch  5, batch     4 | loss: 0.7453687Losses:  0.13798266649246216 0.35607483983039856
MemoryTrain:  epoch  5, batch     5 | loss: 0.4940575Losses:  0.09042014181613922 0.354655385017395
MemoryTrain:  epoch  5, batch     6 | loss: 0.4450755Losses:  0.1812632977962494 0.19803479313850403
MemoryTrain:  epoch  5, batch     7 | loss: 0.3792981Losses:  0.04455776512622833 0.5523151159286499
MemoryTrain:  epoch  6, batch     0 | loss: 0.5968729Losses:  0.18931162357330322 0.6071460247039795
MemoryTrain:  epoch  6, batch     1 | loss: 0.7964576Losses:  0.04972946643829346 0.3250947594642639
MemoryTrain:  epoch  6, batch     2 | loss: 0.3748242Losses:  0.21157270669937134 0.4505440592765808
MemoryTrain:  epoch  6, batch     3 | loss: 0.6621168Losses:  0.1642456203699112 0.3626571595668793
MemoryTrain:  epoch  6, batch     4 | loss: 0.5269028Losses:  0.09539084136486053 0.3108404874801636
MemoryTrain:  epoch  6, batch     5 | loss: 0.4062313Losses:  0.2261238843202591 0.6205787062644958
MemoryTrain:  epoch  6, batch     6 | loss: 0.8467026Losses:  0.02432658150792122 0.14875105023384094
MemoryTrain:  epoch  6, batch     7 | loss: 0.1730776Losses:  0.10597896575927734 0.5228643417358398
MemoryTrain:  epoch  7, batch     0 | loss: 0.6288433Losses:  0.2086188793182373 0.3783908784389496
MemoryTrain:  epoch  7, batch     1 | loss: 0.5870098Losses:  0.04032032936811447 0.534877359867096
MemoryTrain:  epoch  7, batch     2 | loss: 0.5751977Losses:  0.05052321404218674 0.5874632596969604
MemoryTrain:  epoch  7, batch     3 | loss: 0.6379865Losses:  0.11228154599666595 0.47126898169517517
MemoryTrain:  epoch  7, batch     4 | loss: 0.5835505Losses:  0.12214621156454086 0.4709343910217285
MemoryTrain:  epoch  7, batch     5 | loss: 0.5930806Losses:  0.05895756185054779 0.4520309567451477
MemoryTrain:  epoch  7, batch     6 | loss: 0.5109885Losses:  0.0358475036919117 0.15940502285957336
MemoryTrain:  epoch  7, batch     7 | loss: 0.1952525Losses:  0.43111419677734375 0.3092876672744751
MemoryTrain:  epoch  8, batch     0 | loss: 0.7404019Losses:  0.08137955516576767 0.6557793617248535
MemoryTrain:  epoch  8, batch     1 | loss: 0.7371589Losses:  0.17593815922737122 0.21176491677761078
MemoryTrain:  epoch  8, batch     2 | loss: 0.3877031Losses:  0.08746802061796188 0.41226470470428467
MemoryTrain:  epoch  8, batch     3 | loss: 0.4997327Losses:  0.07749990373849869 0.3348924219608307
MemoryTrain:  epoch  8, batch     4 | loss: 0.4123923Losses:  0.10865002125501633 0.5577756762504578
MemoryTrain:  epoch  8, batch     5 | loss: 0.6664257Losses:  0.08520534634590149 0.7891752123832703
MemoryTrain:  epoch  8, batch     6 | loss: 0.8743806Losses:  0.045788563787937164 0.20680557191371918
MemoryTrain:  epoch  8, batch     7 | loss: 0.2525941Losses:  0.052341289818286896 0.4202156662940979
MemoryTrain:  epoch  9, batch     0 | loss: 0.4725569Losses:  0.15759819746017456 0.5756435394287109
MemoryTrain:  epoch  9, batch     1 | loss: 0.7332417Losses:  0.11119238287210464 0.4047456383705139
MemoryTrain:  epoch  9, batch     2 | loss: 0.5159380Losses:  0.029734738171100616 0.5013073682785034
MemoryTrain:  epoch  9, batch     3 | loss: 0.5310421Losses:  0.12233825773000717 0.442381888628006
MemoryTrain:  epoch  9, batch     4 | loss: 0.5647202Losses:  0.0671122670173645 0.39039361476898193
MemoryTrain:  epoch  9, batch     5 | loss: 0.4575059Losses:  0.07868314534425735 0.47360390424728394
MemoryTrain:  epoch  9, batch     6 | loss: 0.5522870Losses:  0.04082424193620682 0.32888489961624146
MemoryTrain:  epoch  9, batch     7 | loss: 0.3697091
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 66.62%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 63.90%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.82%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.99%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.09%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.46%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 89.55%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 88.33%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.91%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.60%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 86.90%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 85.55%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 82.95%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 81.72%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 80.51%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 79.62%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 79.40%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 78.45%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 78.00%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 77.53%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 77.11%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 76.93%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.20%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 75.67%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 75.29%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 74.71%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 74.35%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 77.35%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 77.27%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 77.12%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 77.10%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 77.24%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.87%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.33%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 75.92%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 75.28%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 75.06%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 74.61%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.38%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.21%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 72.61%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 72.03%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 71.50%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 70.98%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 70.42%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 70.12%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.72%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 69.04%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 69.08%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.38%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 68.93%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.05%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 67.57%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 68.34%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 68.01%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 67.45%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 67.14%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 66.83%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 67.31%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 66.89%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.83%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 66.97%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 66.72%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 66.68%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 67.09%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 66.84%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 66.66%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 66.57%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 66.45%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 66.82%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 66.49%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 66.23%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 65.96%   [EVAL] batch:  204 | acc: 6.25%,  total acc: 65.67%   [EVAL] batch:  205 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 66.33%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  225 | acc: 0.00%,  total acc: 67.04%   [EVAL] batch:  226 | acc: 0.00%,  total acc: 66.74%   [EVAL] batch:  227 | acc: 0.00%,  total acc: 66.45%   [EVAL] batch:  228 | acc: 0.00%,  total acc: 66.16%   [EVAL] batch:  229 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  230 | acc: 0.00%,  total acc: 65.58%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 66.16%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 65.86%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 65.11%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 64.85%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 64.62%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 64.64%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  261 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 65.53%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 65.41%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 65.11%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 64.12%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 63.89%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 63.66%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.43%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 63.52%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 63.77%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 64.54%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 64.47%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 64.36%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 64.25%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 64.13%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 64.02%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 64.02%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 64.15%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 64.13%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 64.13%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 64.21%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  315 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  316 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 65.18%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 65.21%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 65.54%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  336 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  337 | acc: 43.75%,  total acc: 65.55%   [EVAL] batch:  338 | acc: 43.75%,  total acc: 65.49%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  342 | acc: 18.75%,  total acc: 65.22%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 65.03%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 64.95%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  356 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 64.73%   [EVAL] batch:  358 | acc: 0.00%,  total acc: 64.55%   [EVAL] batch:  359 | acc: 0.00%,  total acc: 64.38%   [EVAL] batch:  360 | acc: 0.00%,  total acc: 64.20%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 63.98%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 65.00%   
cur_acc:  ['0.9464', '0.7192', '0.7004', '0.7361', '0.5942', '0.6875']
his_acc:  ['0.9464', '0.8345', '0.7580', '0.7167', '0.6743', '0.6500']
Clustering into  34  clusters
Clusters:  [ 0  1 17  0  0  0 29  0 31 32 26  0 20  0 33 23 19  0  0  0 21 24  2  0
  0 18  0 25  0 30  0 12  0 16 14  0  0  0 11 22 27  0  0  2  2 15  0  0
  0  0  0  0  0 10  0  9  0  5 28 13  1  8  0  0  0  4  0  7  6  3]
Losses:  4.672656059265137 1.2055047750473022
CurrentTrain: epoch  0, batch     0 | loss: 5.8781610Losses:  6.162527084350586 1.2033965587615967
CurrentTrain: epoch  0, batch     1 | loss: 7.3659239Losses:  5.209216117858887 0.9109430313110352
CurrentTrain: epoch  0, batch     2 | loss: 6.1201591Losses:  5.2457194328308105 1.1486058235168457
CurrentTrain: epoch  0, batch     3 | loss: 6.3943253Losses:  5.030275344848633 0.8269641995429993
CurrentTrain: epoch  0, batch     4 | loss: 5.8572397Losses:  7.743840217590332 1.3973379135131836
CurrentTrain: epoch  0, batch     5 | loss: 9.1411781Losses:  5.111626148223877 0.15020132064819336
CurrentTrain: epoch  0, batch     6 | loss: 5.2618275Losses:  3.959958076477051 0.6558432579040527
CurrentTrain: epoch  1, batch     0 | loss: 4.6158013Losses:  3.846343994140625 0.9288030862808228
CurrentTrain: epoch  1, batch     1 | loss: 4.7751470Losses:  4.147603511810303 1.200873851776123
CurrentTrain: epoch  1, batch     2 | loss: 5.3484774Losses:  3.8983383178710938 1.0167821645736694
CurrentTrain: epoch  1, batch     3 | loss: 4.9151206Losses:  4.148370742797852 0.7514637112617493
CurrentTrain: epoch  1, batch     4 | loss: 4.8998346Losses:  5.328267574310303 1.0514099597930908
CurrentTrain: epoch  1, batch     5 | loss: 6.3796778Losses:  6.195110321044922 0.2669358551502228
CurrentTrain: epoch  1, batch     6 | loss: 6.4620461Losses:  3.809879779815674 0.814399242401123
CurrentTrain: epoch  2, batch     0 | loss: 4.6242790Losses:  3.63720965385437 0.918107271194458
CurrentTrain: epoch  2, batch     1 | loss: 4.5553169Losses:  4.102435111999512 0.7216312289237976
CurrentTrain: epoch  2, batch     2 | loss: 4.8240662Losses:  3.389228582382202 0.707438588142395
CurrentTrain: epoch  2, batch     3 | loss: 4.0966673Losses:  4.275228500366211 0.9234437942504883
CurrentTrain: epoch  2, batch     4 | loss: 5.1986723Losses:  3.650491952896118 0.9397881031036377
CurrentTrain: epoch  2, batch     5 | loss: 4.5902801Losses:  2.366360664367676 0.22854575514793396
CurrentTrain: epoch  2, batch     6 | loss: 2.5949063Losses:  3.9888625144958496 0.789968729019165
CurrentTrain: epoch  3, batch     0 | loss: 4.7788315Losses:  3.282094955444336 0.6246298551559448
CurrentTrain: epoch  3, batch     1 | loss: 3.9067249Losses:  4.063667297363281 0.8282901048660278
CurrentTrain: epoch  3, batch     2 | loss: 4.8919573Losses:  3.1629161834716797 0.8249819874763489
CurrentTrain: epoch  3, batch     3 | loss: 3.9878981Losses:  2.328181028366089 0.48786139488220215
CurrentTrain: epoch  3, batch     4 | loss: 2.8160424Losses:  2.7847561836242676 0.8812592029571533
CurrentTrain: epoch  3, batch     5 | loss: 3.6660154Losses:  2.81181263923645 0.10775430500507355
CurrentTrain: epoch  3, batch     6 | loss: 2.9195669Losses:  3.2189011573791504 0.9633103013038635
CurrentTrain: epoch  4, batch     0 | loss: 4.1822114Losses:  2.337151050567627 0.455411434173584
CurrentTrain: epoch  4, batch     1 | loss: 2.7925625Losses:  3.708601474761963 0.6229323744773865
CurrentTrain: epoch  4, batch     2 | loss: 4.3315339Losses:  3.4303951263427734 0.8099815249443054
CurrentTrain: epoch  4, batch     3 | loss: 4.2403765Losses:  3.0772390365600586 0.7707604169845581
CurrentTrain: epoch  4, batch     4 | loss: 3.8479996Losses:  2.118891477584839 0.4862338602542877
CurrentTrain: epoch  4, batch     5 | loss: 2.6051254Losses:  2.466108560562134 0.2315458208322525
CurrentTrain: epoch  4, batch     6 | loss: 2.6976545Losses:  2.347869873046875 0.5189483761787415
CurrentTrain: epoch  5, batch     0 | loss: 2.8668182Losses:  2.883538007736206 0.6810938119888306
CurrentTrain: epoch  5, batch     1 | loss: 3.5646319Losses:  3.074136257171631 0.49983957409858704
CurrentTrain: epoch  5, batch     2 | loss: 3.5739758Losses:  2.6887850761413574 0.7343575954437256
CurrentTrain: epoch  5, batch     3 | loss: 3.4231427Losses:  2.5604915618896484 0.4277939200401306
CurrentTrain: epoch  5, batch     4 | loss: 2.9882855Losses:  2.376117467880249 0.48506394028663635
CurrentTrain: epoch  5, batch     5 | loss: 2.8611815Losses:  2.527174472808838 0.05245567113161087
CurrentTrain: epoch  5, batch     6 | loss: 2.5796301Losses:  2.7244749069213867 0.737562894821167
CurrentTrain: epoch  6, batch     0 | loss: 3.4620378Losses:  2.489055871963501 0.6047146320343018
CurrentTrain: epoch  6, batch     1 | loss: 3.0937705Losses:  2.110652208328247 0.4358617663383484
CurrentTrain: epoch  6, batch     2 | loss: 2.5465140Losses:  2.471792697906494 0.5853062868118286
CurrentTrain: epoch  6, batch     3 | loss: 3.0570989Losses:  1.8884973526000977 0.4058602750301361
CurrentTrain: epoch  6, batch     4 | loss: 2.2943575Losses:  2.1461381912231445 0.45595699548721313
CurrentTrain: epoch  6, batch     5 | loss: 2.6020951Losses:  2.195995330810547 0.07748472690582275
CurrentTrain: epoch  6, batch     6 | loss: 2.2734799Losses:  2.1960349082946777 0.4536920487880707
CurrentTrain: epoch  7, batch     0 | loss: 2.6497269Losses:  2.273132801055908 0.6329542398452759
CurrentTrain: epoch  7, batch     1 | loss: 2.9060869Losses:  2.0114593505859375 0.5177433490753174
CurrentTrain: epoch  7, batch     2 | loss: 2.5292027Losses:  2.0880303382873535 0.6603512167930603
CurrentTrain: epoch  7, batch     3 | loss: 2.7483816Losses:  2.5446395874023438 0.5515892505645752
CurrentTrain: epoch  7, batch     4 | loss: 3.0962288Losses:  1.8845417499542236 0.40468624234199524
CurrentTrain: epoch  7, batch     5 | loss: 2.2892280Losses:  1.9945663213729858 0.06958607584238052
CurrentTrain: epoch  7, batch     6 | loss: 2.0641525Losses:  1.8404359817504883 0.4306526184082031
CurrentTrain: epoch  8, batch     0 | loss: 2.2710886Losses:  1.8914446830749512 0.4680914878845215
CurrentTrain: epoch  8, batch     1 | loss: 2.3595362Losses:  2.1880578994750977 0.7330740690231323
CurrentTrain: epoch  8, batch     2 | loss: 2.9211321Losses:  1.9491055011749268 0.38945549726486206
CurrentTrain: epoch  8, batch     3 | loss: 2.3385611Losses:  2.3662731647491455 0.4310126304626465
CurrentTrain: epoch  8, batch     4 | loss: 2.7972858Losses:  2.19999098777771 0.5334951877593994
CurrentTrain: epoch  8, batch     5 | loss: 2.7334862Losses:  1.7541184425354004 0.18748867511749268
CurrentTrain: epoch  8, batch     6 | loss: 1.9416071Losses:  1.9680317640304565 0.4531657099723816
CurrentTrain: epoch  9, batch     0 | loss: 2.4211974Losses:  2.0494656562805176 0.39822566509246826
CurrentTrain: epoch  9, batch     1 | loss: 2.4476914Losses:  1.939578652381897 0.36094558238983154
CurrentTrain: epoch  9, batch     2 | loss: 2.3005242Losses:  1.9991424083709717 0.38504523038864136
CurrentTrain: epoch  9, batch     3 | loss: 2.3841877Losses:  2.03582763671875 0.6908915042877197
CurrentTrain: epoch  9, batch     4 | loss: 2.7267191Losses:  1.9130806922912598 0.5361769199371338
CurrentTrain: epoch  9, batch     5 | loss: 2.4492576Losses:  1.8377728462219238 1.1920930376163597e-07
CurrentTrain: epoch  9, batch     6 | loss: 1.8377730
Losses:  0.356200248003006 0.39417725801467896
MemoryTrain:  epoch  0, batch     0 | loss: 0.7503775Losses:  1.4305245876312256 0.3954787254333496
MemoryTrain:  epoch  0, batch     1 | loss: 1.8260033Losses:  0.47527074813842773 0.5150789022445679
MemoryTrain:  epoch  0, batch     2 | loss: 0.9903497Losses:  0.5343459248542786 0.45755380392074585
MemoryTrain:  epoch  0, batch     3 | loss: 0.9918997Losses:  1.7754559516906738 0.43011999130249023
MemoryTrain:  epoch  0, batch     4 | loss: 2.2055759Losses:  0.24494066834449768 0.5800484418869019
MemoryTrain:  epoch  0, batch     5 | loss: 0.8249891Losses:  0.8975636959075928 0.6592295169830322
MemoryTrain:  epoch  0, batch     6 | loss: 1.5567932Losses:  0.8170438408851624 0.6263109445571899
MemoryTrain:  epoch  0, batch     7 | loss: 1.4433548Losses:  0.44259461760520935 0.44013509154319763
MemoryTrain:  epoch  0, batch     8 | loss: 0.8827297Losses:  1.3219897747039795 0.8330650925636292
MemoryTrain:  epoch  1, batch     0 | loss: 2.1550548Losses:  1.215556263923645 0.7139986753463745
MemoryTrain:  epoch  1, batch     1 | loss: 1.9295549Losses:  0.7050642371177673 0.5494810938835144
MemoryTrain:  epoch  1, batch     2 | loss: 1.2545453Losses:  0.6418368220329285 0.4953221082687378
MemoryTrain:  epoch  1, batch     3 | loss: 1.1371589Losses:  1.1343660354614258 0.2957301735877991
MemoryTrain:  epoch  1, batch     4 | loss: 1.4300961Losses:  0.2799382209777832 0.4911128878593445
MemoryTrain:  epoch  1, batch     5 | loss: 0.7710511Losses:  1.6960749626159668 0.33355072140693665
MemoryTrain:  epoch  1, batch     6 | loss: 2.0296257Losses:  0.4196879267692566 0.3000900447368622
MemoryTrain:  epoch  1, batch     7 | loss: 0.7197779Losses:  0.5941357016563416 0.38099366426467896
MemoryTrain:  epoch  1, batch     8 | loss: 0.9751294Losses:  0.3151303231716156 0.5253605842590332
MemoryTrain:  epoch  2, batch     0 | loss: 0.8404909Losses:  1.047114372253418 0.4042440950870514
MemoryTrain:  epoch  2, batch     1 | loss: 1.4513584Losses:  0.49102550745010376 0.44691202044487
MemoryTrain:  epoch  2, batch     2 | loss: 0.9379375Losses:  0.6453667879104614 0.3810429275035858
MemoryTrain:  epoch  2, batch     3 | loss: 1.0264097Losses:  0.19956238567829132 0.5578073263168335
MemoryTrain:  epoch  2, batch     4 | loss: 0.7573697Losses:  0.43059247732162476 0.51143479347229
MemoryTrain:  epoch  2, batch     5 | loss: 0.9420273Losses:  0.7986568212509155 0.7084077596664429
MemoryTrain:  epoch  2, batch     6 | loss: 1.5070646Losses:  0.18893414735794067 0.4759857654571533
MemoryTrain:  epoch  2, batch     7 | loss: 0.6649199Losses:  0.2915208339691162 0.2383139580488205
MemoryTrain:  epoch  2, batch     8 | loss: 0.5298348Losses:  0.8857923746109009 0.5124979019165039
MemoryTrain:  epoch  3, batch     0 | loss: 1.3982903Losses:  0.24557314813137054 0.6832035779953003
MemoryTrain:  epoch  3, batch     1 | loss: 0.9287767Losses:  0.06028465926647186 0.5375425815582275
MemoryTrain:  epoch  3, batch     2 | loss: 0.5978273Losses:  0.23603466153144836 0.28115755319595337
MemoryTrain:  epoch  3, batch     3 | loss: 0.5171922Losses:  0.2256963551044464 0.6231971979141235
MemoryTrain:  epoch  3, batch     4 | loss: 0.8488935Losses:  0.1465195268392563 0.4554414451122284
MemoryTrain:  epoch  3, batch     5 | loss: 0.6019610Losses:  0.08618047833442688 0.4792271852493286
MemoryTrain:  epoch  3, batch     6 | loss: 0.5654076Losses:  0.665239691734314 0.4934050440788269
MemoryTrain:  epoch  3, batch     7 | loss: 1.1586447Losses:  0.11426687240600586 0.27398616075515747
MemoryTrain:  epoch  3, batch     8 | loss: 0.3882530Losses:  0.047448351979255676 0.7433700561523438
MemoryTrain:  epoch  4, batch     0 | loss: 0.7908184Losses:  0.09404969215393066 0.47882014513015747
MemoryTrain:  epoch  4, batch     1 | loss: 0.5728698Losses:  0.15234583616256714 0.6959054470062256
MemoryTrain:  epoch  4, batch     2 | loss: 0.8482513Losses:  0.19219399988651276 0.5065696835517883
MemoryTrain:  epoch  4, batch     3 | loss: 0.6987637Losses:  0.16548866033554077 0.39583897590637207
MemoryTrain:  epoch  4, batch     4 | loss: 0.5613276Losses:  0.30795928835868835 0.4472413957118988
MemoryTrain:  epoch  4, batch     5 | loss: 0.7552007Losses:  0.15670806169509888 0.5007245540618896
MemoryTrain:  epoch  4, batch     6 | loss: 0.6574326Losses:  0.08339998126029968 0.3520902395248413
MemoryTrain:  epoch  4, batch     7 | loss: 0.4354902Losses:  0.02618667110800743 0.14537391066551208
MemoryTrain:  epoch  4, batch     8 | loss: 0.1715606Losses:  0.04261859506368637 0.49391353130340576
MemoryTrain:  epoch  5, batch     0 | loss: 0.5365321Losses:  0.03400760143995285 0.33046120405197144
MemoryTrain:  epoch  5, batch     1 | loss: 0.3644688Losses:  0.11842186748981476 0.5558726787567139
MemoryTrain:  epoch  5, batch     2 | loss: 0.6742945Losses:  0.09178753197193146 0.5169142484664917
MemoryTrain:  epoch  5, batch     3 | loss: 0.6087018Losses:  0.04977233707904816 0.5206879377365112
MemoryTrain:  epoch  5, batch     4 | loss: 0.5704603Losses:  0.1299596130847931 0.35755085945129395
MemoryTrain:  epoch  5, batch     5 | loss: 0.4875105Losses:  0.08434819430112839 0.41518092155456543
MemoryTrain:  epoch  5, batch     6 | loss: 0.4995291Losses:  0.11231556534767151 0.5996214151382446
MemoryTrain:  epoch  5, batch     7 | loss: 0.7119370Losses:  0.06259359419345856 0.43259868025779724
MemoryTrain:  epoch  5, batch     8 | loss: 0.4951923Losses:  0.04426020383834839 0.44586536288261414
MemoryTrain:  epoch  6, batch     0 | loss: 0.4901256Losses:  0.07590582966804504 0.5577422380447388
MemoryTrain:  epoch  6, batch     1 | loss: 0.6336480Losses:  0.05109992250800133 0.39341259002685547
MemoryTrain:  epoch  6, batch     2 | loss: 0.4445125Losses:  0.2843537926673889 0.619507908821106
MemoryTrain:  epoch  6, batch     3 | loss: 0.9038617Losses:  0.11023031175136566 0.5172216892242432
MemoryTrain:  epoch  6, batch     4 | loss: 0.6274520Losses:  0.03517121449112892 0.34427616000175476
MemoryTrain:  epoch  6, batch     5 | loss: 0.3794474Losses:  0.08252283930778503 0.40784144401550293
MemoryTrain:  epoch  6, batch     6 | loss: 0.4903643Losses:  0.07797490060329437 0.5075811147689819
MemoryTrain:  epoch  6, batch     7 | loss: 0.5855560Losses:  0.05807369947433472 0.47877177596092224
MemoryTrain:  epoch  6, batch     8 | loss: 0.5368454Losses:  0.12964126467704773 0.6105663776397705
MemoryTrain:  epoch  7, batch     0 | loss: 0.7402077Losses:  0.12375815212726593 0.47435253858566284
MemoryTrain:  epoch  7, batch     1 | loss: 0.5981107Losses:  0.09894603490829468 0.5024282932281494
MemoryTrain:  epoch  7, batch     2 | loss: 0.6013743Losses:  0.08645272254943848 0.5327078104019165
MemoryTrain:  epoch  7, batch     3 | loss: 0.6191605Losses:  0.03516850620508194 0.36395400762557983
MemoryTrain:  epoch  7, batch     4 | loss: 0.3991225Losses:  0.053903721272945404 0.29431915283203125
MemoryTrain:  epoch  7, batch     5 | loss: 0.3482229Losses:  0.10181663185358047 0.44897153973579407
MemoryTrain:  epoch  7, batch     6 | loss: 0.5507882Losses:  0.048319876194000244 0.5447642207145691
MemoryTrain:  epoch  7, batch     7 | loss: 0.5930841Losses:  0.13129466772079468 0.33258891105651855
MemoryTrain:  epoch  7, batch     8 | loss: 0.4638836Losses:  0.12305671721696854 0.511742353439331
MemoryTrain:  epoch  8, batch     0 | loss: 0.6347991Losses:  0.041091252118349075 0.4147970676422119
MemoryTrain:  epoch  8, batch     1 | loss: 0.4558883Losses:  0.07577525079250336 0.46043306589126587
MemoryTrain:  epoch  8, batch     2 | loss: 0.5362083Losses:  0.07912307232618332 0.312987744808197
MemoryTrain:  epoch  8, batch     3 | loss: 0.3921108Losses:  0.08675505965948105 0.3440174460411072
MemoryTrain:  epoch  8, batch     4 | loss: 0.4307725Losses:  0.03328458219766617 0.4892377257347107
MemoryTrain:  epoch  8, batch     5 | loss: 0.5225223Losses:  0.06035973131656647 0.6749427914619446
MemoryTrain:  epoch  8, batch     6 | loss: 0.7353025Losses:  0.052944980561733246 0.2511175274848938
MemoryTrain:  epoch  8, batch     7 | loss: 0.3040625Losses:  0.07768440991640091 0.4842963218688965
MemoryTrain:  epoch  8, batch     8 | loss: 0.5619807Losses:  0.03712163493037224 0.36600929498672485
MemoryTrain:  epoch  9, batch     0 | loss: 0.4031309Losses:  0.06112606078386307 0.5073264837265015
MemoryTrain:  epoch  9, batch     1 | loss: 0.5684525Losses:  0.07773926109075546 0.38281285762786865
MemoryTrain:  epoch  9, batch     2 | loss: 0.4605521Losses:  0.04143741726875305 0.544954776763916
MemoryTrain:  epoch  9, batch     3 | loss: 0.5863922Losses:  0.05968756973743439 0.43440061807632446
MemoryTrain:  epoch  9, batch     4 | loss: 0.4940882Losses:  0.037573762238025665 0.35700368881225586
MemoryTrain:  epoch  9, batch     5 | loss: 0.3945774Losses:  0.09312654286623001 0.49181848764419556
MemoryTrain:  epoch  9, batch     6 | loss: 0.5849450Losses:  0.03705409914255142 0.492125540971756
MemoryTrain:  epoch  9, batch     7 | loss: 0.5291796Losses:  0.04297412559390068 0.4388540983200073
MemoryTrain:  epoch  9, batch     8 | loss: 0.4818282
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 70.31%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 74.23%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.40%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.92%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 87.06%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 86.21%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 85.49%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 85.10%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 84.73%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.48%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 82.52%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 81.25%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 80.02%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 78.82%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 76.58%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 76.22%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 75.94%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 75.76%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 74.44%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 74.37%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 73.83%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 72.97%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 72.47%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 71.66%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 71.34%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.30%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 73.78%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 73.34%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.73%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 72.10%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 72.36%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 72.49%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 72.08%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.16%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.58%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.01%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 69.50%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 69.00%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 68.45%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 68.21%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 67.83%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 67.18%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 67.54%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 66.71%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 66.24%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 65.78%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 66.43%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 66.12%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 65.77%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 65.58%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 64.86%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 65.64%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 65.26%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 65.00%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 64.71%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 65.21%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 64.91%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 64.43%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 64.18%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 63.96%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  225 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:  226 | acc: 0.00%,  total acc: 65.47%   [EVAL] batch:  227 | acc: 0.00%,  total acc: 65.19%   [EVAL] batch:  228 | acc: 0.00%,  total acc: 64.90%   [EVAL] batch:  229 | acc: 0.00%,  total acc: 64.62%   [EVAL] batch:  230 | acc: 0.00%,  total acc: 64.34%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 65.03%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 64.99%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 64.82%   [EVAL] batch:  251 | acc: 0.00%,  total acc: 64.56%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 64.30%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 64.05%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 63.80%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 63.55%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 63.55%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 63.98%   [EVAL] batch:  261 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 64.19%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.20%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 64.17%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 64.02%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 63.86%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 63.71%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 63.57%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 63.41%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.20%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 62.97%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 62.52%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 62.30%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.08%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 62.21%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 63.27%   [EVAL] batch:  294 | acc: 6.25%,  total acc: 63.07%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 62.86%   [EVAL] batch:  296 | acc: 0.00%,  total acc: 62.65%   [EVAL] batch:  297 | acc: 0.00%,  total acc: 62.44%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 62.23%   [EVAL] batch:  299 | acc: 6.25%,  total acc: 62.04%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 62.06%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 62.17%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 62.30%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 62.30%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 62.28%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  315 | acc: 93.75%,  total acc: 62.60%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 62.68%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 62.76%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 62.79%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.29%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 63.36%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 63.43%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 63.55%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 63.69%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  336 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 63.79%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:  340 | acc: 50.00%,  total acc: 63.65%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 63.63%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 63.52%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 63.41%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 63.35%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 63.38%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 63.30%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 63.31%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 63.29%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:  356 | acc: 18.75%,  total acc: 63.20%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 63.02%   [EVAL] batch:  358 | acc: 0.00%,  total acc: 62.85%   [EVAL] batch:  359 | acc: 0.00%,  total acc: 62.67%   [EVAL] batch:  360 | acc: 6.25%,  total acc: 62.52%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 62.34%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 62.31%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 63.48%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 63.52%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.61%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 63.68%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 63.77%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 63.77%   [EVAL] batch:  390 | acc: 37.50%,  total acc: 63.70%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 63.70%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 64.26%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  402 | acc: 43.75%,  total acc: 64.21%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  405 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 64.16%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  408 | acc: 31.25%,  total acc: 64.07%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 64.04%   [EVAL] batch:  410 | acc: 31.25%,  total acc: 63.96%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 63.87%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 63.85%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 63.99%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 64.13%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 64.19%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 64.21%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 64.20%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 65.08%   
cur_acc:  ['0.9464', '0.7192', '0.7004', '0.7361', '0.5942', '0.6875', '0.7540']
his_acc:  ['0.9464', '0.8345', '0.7580', '0.7167', '0.6743', '0.6500', '0.6508']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0 32 27  1  0
  0 37  0 29  0 34  2 31  0  0 36  0  0  0 20 23 17  0  0  1  1  9  0  0
  0  0  0  0  0 12  0 16  0 11 15 30  5 14  0  0  0  2  0 18 28 10  8  4
  0  0 13  7  6  0  0  3]
Losses:  3.4490785598754883 1.4060639142990112
CurrentTrain: epoch  0, batch     0 | loss: 4.8551426Losses:  4.108901023864746 1.0630924701690674
CurrentTrain: epoch  0, batch     1 | loss: 5.1719933Losses:  5.323094367980957 1.110643982887268
CurrentTrain: epoch  0, batch     2 | loss: 6.4337382Losses:  4.101500034332275 1.2281553745269775
CurrentTrain: epoch  0, batch     3 | loss: 5.3296556Losses:  5.505951881408691 1.3991976976394653
CurrentTrain: epoch  0, batch     4 | loss: 6.9051495Losses:  4.897494316101074 1.2424957752227783
CurrentTrain: epoch  0, batch     5 | loss: 6.1399899Losses:  6.111310005187988 0.7374443411827087
CurrentTrain: epoch  0, batch     6 | loss: 6.8487544Losses:  5.5229902267456055 1.191935658454895
CurrentTrain: epoch  1, batch     0 | loss: 6.7149258Losses:  3.047114849090576 0.6936087608337402
CurrentTrain: epoch  1, batch     1 | loss: 3.7407236Losses:  3.653909683227539 0.9692932367324829
CurrentTrain: epoch  1, batch     2 | loss: 4.6232028Losses:  2.944977283477783 1.2021520137786865
CurrentTrain: epoch  1, batch     3 | loss: 4.1471291Losses:  2.6779820919036865 0.8642027974128723
CurrentTrain: epoch  1, batch     4 | loss: 3.5421848Losses:  2.8969931602478027 0.7613630890846252
CurrentTrain: epoch  1, batch     5 | loss: 3.6583562Losses:  3.803555488586426 0.17643465101718903
CurrentTrain: epoch  1, batch     6 | loss: 3.9799902Losses:  3.352112054824829 1.0818860530853271
CurrentTrain: epoch  2, batch     0 | loss: 4.4339981Losses:  2.640970230102539 0.7061896920204163
CurrentTrain: epoch  2, batch     1 | loss: 3.3471599Losses:  2.158115863800049 0.7360368967056274
CurrentTrain: epoch  2, batch     2 | loss: 2.8941526Losses:  2.998501777648926 0.7269565463066101
CurrentTrain: epoch  2, batch     3 | loss: 3.7254584Losses:  3.882856845855713 0.8390002250671387
CurrentTrain: epoch  2, batch     4 | loss: 4.7218571Losses:  2.8094863891601562 1.317690372467041
CurrentTrain: epoch  2, batch     5 | loss: 4.1271768Losses:  1.8688457012176514 0.02973032183945179
CurrentTrain: epoch  2, batch     6 | loss: 1.8985760Losses:  2.431797742843628 0.8158151507377625
CurrentTrain: epoch  3, batch     0 | loss: 3.2476130Losses:  2.7414050102233887 0.6689296960830688
CurrentTrain: epoch  3, batch     1 | loss: 3.4103346Losses:  3.0608460903167725 1.2672120332717896
CurrentTrain: epoch  3, batch     2 | loss: 4.3280582Losses:  2.8323676586151123 0.5623369216918945
CurrentTrain: epoch  3, batch     3 | loss: 3.3947046Losses:  2.035548210144043 0.6103984117507935
CurrentTrain: epoch  3, batch     4 | loss: 2.6459465Losses:  2.4213364124298096 0.8557429313659668
CurrentTrain: epoch  3, batch     5 | loss: 3.2770793Losses:  1.9682165384292603 1.1920930376163597e-07
CurrentTrain: epoch  3, batch     6 | loss: 1.9682167Losses:  2.356440544128418 0.7315212488174438
CurrentTrain: epoch  4, batch     0 | loss: 3.0879617Losses:  2.2103750705718994 0.49751707911491394
CurrentTrain: epoch  4, batch     1 | loss: 2.7078922Losses:  2.5070066452026367 0.5514335036277771
CurrentTrain: epoch  4, batch     2 | loss: 3.0584402Losses:  2.254166603088379 0.4871754050254822
CurrentTrain: epoch  4, batch     3 | loss: 2.7413421Losses:  2.353389263153076 0.6026688814163208
CurrentTrain: epoch  4, batch     4 | loss: 2.9560580Losses:  2.218778610229492 0.7322148084640503
CurrentTrain: epoch  4, batch     5 | loss: 2.9509935Losses:  2.244828701019287 0.17863106727600098
CurrentTrain: epoch  4, batch     6 | loss: 2.4234598Losses:  2.3872792720794678 0.6656924486160278
CurrentTrain: epoch  5, batch     0 | loss: 3.0529718Losses:  2.085082530975342 0.851336658000946
CurrentTrain: epoch  5, batch     1 | loss: 2.9364192Losses:  2.079843521118164 0.5470070242881775
CurrentTrain: epoch  5, batch     2 | loss: 2.6268506Losses:  1.8898565769195557 0.46351832151412964
CurrentTrain: epoch  5, batch     3 | loss: 2.3533750Losses:  2.0612406730651855 0.7098404169082642
CurrentTrain: epoch  5, batch     4 | loss: 2.7710810Losses:  1.894913673400879 0.5148112177848816
CurrentTrain: epoch  5, batch     5 | loss: 2.4097250Losses:  1.9333966970443726 0.026511527597904205
CurrentTrain: epoch  5, batch     6 | loss: 1.9599082Losses:  1.9428104162216187 0.46051567792892456
CurrentTrain: epoch  6, batch     0 | loss: 2.4033260Losses:  1.949113130569458 0.6099509000778198
CurrentTrain: epoch  6, batch     1 | loss: 2.5590639Losses:  2.0207631587982178 0.42655348777770996
CurrentTrain: epoch  6, batch     2 | loss: 2.4473166Losses:  1.8619325160980225 0.4389236569404602
CurrentTrain: epoch  6, batch     3 | loss: 2.3008561Losses:  1.8294438123703003 0.5534752011299133
CurrentTrain: epoch  6, batch     4 | loss: 2.3829191Losses:  1.9943196773529053 0.30548936128616333
CurrentTrain: epoch  6, batch     5 | loss: 2.2998090Losses:  1.781346082687378 0.15389513969421387
CurrentTrain: epoch  6, batch     6 | loss: 1.9352412Losses:  1.7802814245224 0.5660351514816284
CurrentTrain: epoch  7, batch     0 | loss: 2.3463166Losses:  1.8062783479690552 0.5319583415985107
CurrentTrain: epoch  7, batch     1 | loss: 2.3382368Losses:  1.8428252935409546 0.6478966474533081
CurrentTrain: epoch  7, batch     2 | loss: 2.4907219Losses:  1.808053970336914 0.612602949142456
CurrentTrain: epoch  7, batch     3 | loss: 2.4206569Losses:  1.7771767377853394 0.48065537214279175
CurrentTrain: epoch  7, batch     4 | loss: 2.2578321Losses:  1.8408210277557373 0.32660919427871704
CurrentTrain: epoch  7, batch     5 | loss: 2.1674302Losses:  1.730802297592163 0.08926808089017868
CurrentTrain: epoch  7, batch     6 | loss: 1.8200704Losses:  1.9186278581619263 0.5516833066940308
CurrentTrain: epoch  8, batch     0 | loss: 2.4703112Losses:  1.8459079265594482 0.29066693782806396
CurrentTrain: epoch  8, batch     1 | loss: 2.1365747Losses:  1.8359439373016357 0.5453822016716003
CurrentTrain: epoch  8, batch     2 | loss: 2.3813262Losses:  1.8300564289093018 0.22995586693286896
CurrentTrain: epoch  8, batch     3 | loss: 2.0600123Losses:  1.8654776811599731 0.499269962310791
CurrentTrain: epoch  8, batch     4 | loss: 2.3647475Losses:  1.8368556499481201 0.3980873227119446
CurrentTrain: epoch  8, batch     5 | loss: 2.2349429Losses:  1.7743133306503296 0.0893058031797409
CurrentTrain: epoch  8, batch     6 | loss: 1.8636191Losses:  1.7752079963684082 0.5285581350326538
CurrentTrain: epoch  9, batch     0 | loss: 2.3037663Losses:  1.8946077823638916 0.45743227005004883
CurrentTrain: epoch  9, batch     1 | loss: 2.3520401Losses:  1.7314155101776123 0.27317672967910767
CurrentTrain: epoch  9, batch     2 | loss: 2.0045922Losses:  1.760892391204834 0.27969178557395935
CurrentTrain: epoch  9, batch     3 | loss: 2.0405841Losses:  1.79107666015625 0.37222999334335327
CurrentTrain: epoch  9, batch     4 | loss: 2.1633067Losses:  1.8498029708862305 0.3540754020214081
CurrentTrain: epoch  9, batch     5 | loss: 2.2038784Losses:  1.7447710037231445 0.03197154402732849
CurrentTrain: epoch  9, batch     6 | loss: 1.7767426
Losses:  0.20621342957019806 0.4830341041088104
MemoryTrain:  epoch  0, batch     0 | loss: 0.6892475Losses:  0.1622944474220276 0.4747709631919861
MemoryTrain:  epoch  0, batch     1 | loss: 0.6370654Losses:  0.0577116385102272 0.4437517821788788
MemoryTrain:  epoch  0, batch     2 | loss: 0.5014634Losses:  0.42067813873291016 0.4718558192253113
MemoryTrain:  epoch  0, batch     3 | loss: 0.8925340Losses:  0.48775115609169006 0.3677546977996826
MemoryTrain:  epoch  0, batch     4 | loss: 0.8555058Losses:  0.4199245274066925 0.7387362718582153
MemoryTrain:  epoch  0, batch     5 | loss: 1.1586608Losses:  0.4389513432979584 0.5274523496627808
MemoryTrain:  epoch  0, batch     6 | loss: 0.9664037Losses:  0.017608385533094406 0.452168345451355
MemoryTrain:  epoch  0, batch     7 | loss: 0.4697767Losses:  0.5904542803764343 0.4974992275238037
MemoryTrain:  epoch  0, batch     8 | loss: 1.0879536Losses:  0.5284661650657654 0.4432488679885864
MemoryTrain:  epoch  0, batch     9 | loss: 0.9717150Losses:  0.4123036861419678 0.38443195819854736
MemoryTrain:  epoch  1, batch     0 | loss: 0.7967356Losses:  0.4118884205818176 0.30069440603256226
MemoryTrain:  epoch  1, batch     1 | loss: 0.7125828Losses:  0.3265664875507355 0.5012821555137634
MemoryTrain:  epoch  1, batch     2 | loss: 0.8278487Losses:  0.4516676366329193 0.5318137407302856
MemoryTrain:  epoch  1, batch     3 | loss: 0.9834814Losses:  0.621351957321167 0.5445168614387512
MemoryTrain:  epoch  1, batch     4 | loss: 1.1658688Losses:  0.40841639041900635 0.49046409130096436
MemoryTrain:  epoch  1, batch     5 | loss: 0.8988805Losses:  0.3247522711753845 0.6875628232955933
MemoryTrain:  epoch  1, batch     6 | loss: 1.0123150Losses:  0.6780731081962585 0.5271570682525635
MemoryTrain:  epoch  1, batch     7 | loss: 1.2052302Losses:  0.7396906018257141 0.49471932649612427
MemoryTrain:  epoch  1, batch     8 | loss: 1.2344099Losses:  0.12117527425289154 0.34703707695007324
MemoryTrain:  epoch  1, batch     9 | loss: 0.4682124Losses:  0.8523311614990234 0.4546760618686676
MemoryTrain:  epoch  2, batch     0 | loss: 1.3070072Losses:  0.10077907890081406 0.5002077221870422
MemoryTrain:  epoch  2, batch     1 | loss: 0.6009868Losses:  0.3340214490890503 0.5493079423904419
MemoryTrain:  epoch  2, batch     2 | loss: 0.8833294Losses:  0.03564251586794853 0.2564730644226074
MemoryTrain:  epoch  2, batch     3 | loss: 0.2921156Losses:  0.07976099103689194 0.40951573848724365
MemoryTrain:  epoch  2, batch     4 | loss: 0.4892767Losses:  0.2114352285861969 0.3503221869468689
MemoryTrain:  epoch  2, batch     5 | loss: 0.5617574Losses:  0.12155379354953766 0.43163254857063293
MemoryTrain:  epoch  2, batch     6 | loss: 0.5531864Losses:  0.20912951231002808 0.4726622998714447
MemoryTrain:  epoch  2, batch     7 | loss: 0.6817918Losses:  0.1731492280960083 0.5165657997131348
MemoryTrain:  epoch  2, batch     8 | loss: 0.6897150Losses:  0.17768895626068115 0.4876517653465271
MemoryTrain:  epoch  2, batch     9 | loss: 0.6653407Losses:  0.3387088477611542 0.5696027278900146
MemoryTrain:  epoch  3, batch     0 | loss: 0.9083116Losses:  0.03989507257938385 0.23559626936912537
MemoryTrain:  epoch  3, batch     1 | loss: 0.2754914Losses:  0.08124838024377823 0.5885794162750244
MemoryTrain:  epoch  3, batch     2 | loss: 0.6698278Losses:  0.06844456493854523 0.44446080923080444
MemoryTrain:  epoch  3, batch     3 | loss: 0.5129054Losses:  0.0681321993470192 0.44441238045692444
MemoryTrain:  epoch  3, batch     4 | loss: 0.5125446Losses:  0.1717960238456726 0.5480716228485107
MemoryTrain:  epoch  3, batch     5 | loss: 0.7198676Losses:  0.07978157699108124 0.4247007369995117
MemoryTrain:  epoch  3, batch     6 | loss: 0.5044823Losses:  0.055637236684560776 0.30089592933654785
MemoryTrain:  epoch  3, batch     7 | loss: 0.3565332Losses:  0.061238158494234085 0.4943000376224518
MemoryTrain:  epoch  3, batch     8 | loss: 0.5555382Losses:  0.0807291567325592 0.49908196926116943
MemoryTrain:  epoch  3, batch     9 | loss: 0.5798111Losses:  0.06890816241502762 0.4127597212791443
MemoryTrain:  epoch  4, batch     0 | loss: 0.4816679Losses:  0.05761167034506798 0.35642486810684204
MemoryTrain:  epoch  4, batch     1 | loss: 0.4140365Losses:  0.07786322385072708 0.5338625311851501
MemoryTrain:  epoch  4, batch     2 | loss: 0.6117257Losses:  0.09632062911987305 0.2905591130256653
MemoryTrain:  epoch  4, batch     3 | loss: 0.3868797Losses:  0.04300927370786667 0.3815639615058899
MemoryTrain:  epoch  4, batch     4 | loss: 0.4245732Losses:  0.03592396900057793 0.3829772174358368
MemoryTrain:  epoch  4, batch     5 | loss: 0.4189012Losses:  0.04483392834663391 0.4973934292793274
MemoryTrain:  epoch  4, batch     6 | loss: 0.5422274Losses:  0.06354985386133194 0.4594513177871704
MemoryTrain:  epoch  4, batch     7 | loss: 0.5230012Losses:  0.0810391828417778 0.36593398451805115
MemoryTrain:  epoch  4, batch     8 | loss: 0.4469732Losses:  0.20187577605247498 0.5931554436683655
MemoryTrain:  epoch  4, batch     9 | loss: 0.7950312Losses:  0.05889014154672623 0.3809766173362732
MemoryTrain:  epoch  5, batch     0 | loss: 0.4398668Losses:  0.04857756942510605 0.35083243250846863
MemoryTrain:  epoch  5, batch     1 | loss: 0.3994100Losses:  0.06791160255670547 0.5257750749588013
MemoryTrain:  epoch  5, batch     2 | loss: 0.5936867Losses:  0.02401583269238472 0.2520821988582611
MemoryTrain:  epoch  5, batch     3 | loss: 0.2760980Losses:  0.0584523007273674 0.3941783905029297
MemoryTrain:  epoch  5, batch     4 | loss: 0.4526307Losses:  0.056543782353401184 0.33180657029151917
MemoryTrain:  epoch  5, batch     5 | loss: 0.3883504Losses:  0.05944689363241196 0.6045584082603455
MemoryTrain:  epoch  5, batch     6 | loss: 0.6640053Losses:  0.08116217702627182 0.482305645942688
MemoryTrain:  epoch  5, batch     7 | loss: 0.5634678Losses:  0.08988607674837112 0.4818624258041382
MemoryTrain:  epoch  5, batch     8 | loss: 0.5717485Losses:  0.04324107617139816 0.32743844389915466
MemoryTrain:  epoch  5, batch     9 | loss: 0.3706795Losses:  0.10060271620750427 0.3579481840133667
MemoryTrain:  epoch  6, batch     0 | loss: 0.4585509Losses:  0.27972257137298584 0.5756775736808777
MemoryTrain:  epoch  6, batch     1 | loss: 0.8554001Losses:  0.02305334061384201 0.19867783784866333
MemoryTrain:  epoch  6, batch     2 | loss: 0.2217312Losses:  0.05540694668889046 0.42877793312072754
MemoryTrain:  epoch  6, batch     3 | loss: 0.4841849Losses:  0.038787923753261566 0.5441159009933472
MemoryTrain:  epoch  6, batch     4 | loss: 0.5829038Losses:  0.07238074392080307 0.3837222158908844
MemoryTrain:  epoch  6, batch     5 | loss: 0.4561030Losses:  0.07460081577301025 0.449107825756073
MemoryTrain:  epoch  6, batch     6 | loss: 0.5237086Losses:  0.03573669120669365 0.4144744277000427
MemoryTrain:  epoch  6, batch     7 | loss: 0.4502111Losses:  0.048268821090459824 0.38745734095573425
MemoryTrain:  epoch  6, batch     8 | loss: 0.4357262Losses:  0.03799811750650406 0.27609753608703613
MemoryTrain:  epoch  6, batch     9 | loss: 0.3140956Losses:  0.026546332985162735 0.4601639211177826
MemoryTrain:  epoch  7, batch     0 | loss: 0.4867103Losses:  0.13779622316360474 0.3638531565666199
MemoryTrain:  epoch  7, batch     1 | loss: 0.5016494Losses:  0.05733144283294678 0.3838554620742798
MemoryTrain:  epoch  7, batch     2 | loss: 0.4411869Losses:  0.04756174236536026 0.5744530558586121
MemoryTrain:  epoch  7, batch     3 | loss: 0.6220148Losses:  0.07612990587949753 0.3723355531692505
MemoryTrain:  epoch  7, batch     4 | loss: 0.4484655Losses:  0.05433567985892296 0.29331889748573303
MemoryTrain:  epoch  7, batch     5 | loss: 0.3476546Losses:  0.03394652158021927 0.3095611035823822
MemoryTrain:  epoch  7, batch     6 | loss: 0.3435076Losses:  0.22261208295822144 0.389382004737854
MemoryTrain:  epoch  7, batch     7 | loss: 0.6119941Losses:  0.041011374443769455 0.21955350041389465
MemoryTrain:  epoch  7, batch     8 | loss: 0.2605649Losses:  0.11000531911849976 0.6954044103622437
MemoryTrain:  epoch  7, batch     9 | loss: 0.8054097Losses:  0.07040756940841675 0.3252166509628296
MemoryTrain:  epoch  8, batch     0 | loss: 0.3956242Losses:  0.04301228001713753 0.30296629667282104
MemoryTrain:  epoch  8, batch     1 | loss: 0.3459786Losses:  0.057057660073041916 0.36977657675743103
MemoryTrain:  epoch  8, batch     2 | loss: 0.4268342Losses:  0.029377367347478867 0.2813645303249359
MemoryTrain:  epoch  8, batch     3 | loss: 0.3107419Losses:  0.06138285994529724 0.32922542095184326
MemoryTrain:  epoch  8, batch     4 | loss: 0.3906083Losses:  0.03411789610981941 0.4729698598384857
MemoryTrain:  epoch  8, batch     5 | loss: 0.5070878Losses:  0.048627741634845734 0.6193097233772278
MemoryTrain:  epoch  8, batch     6 | loss: 0.6679375Losses:  0.08906691521406174 0.5558295845985413
MemoryTrain:  epoch  8, batch     7 | loss: 0.6448965Losses:  0.040166083723306656 0.28348761796951294
MemoryTrain:  epoch  8, batch     8 | loss: 0.3236537Losses:  0.061236828565597534 0.4198845624923706
MemoryTrain:  epoch  8, batch     9 | loss: 0.4811214Losses:  0.036911964416503906 0.3223155438899994
MemoryTrain:  epoch  9, batch     0 | loss: 0.3592275Losses:  0.03903020918369293 0.3633057475090027
MemoryTrain:  epoch  9, batch     1 | loss: 0.4023359Losses:  0.03734029829502106 0.40817105770111084
MemoryTrain:  epoch  9, batch     2 | loss: 0.4455113Losses:  0.03153695538640022 0.1828455626964569
MemoryTrain:  epoch  9, batch     3 | loss: 0.2143825Losses:  0.06834880262613297 0.4374288320541382
MemoryTrain:  epoch  9, batch     4 | loss: 0.5057777Losses:  0.059372372925281525 0.34358835220336914
MemoryTrain:  epoch  9, batch     5 | loss: 0.4029607Losses:  0.049762047827243805 0.4026036262512207
MemoryTrain:  epoch  9, batch     6 | loss: 0.4523657Losses:  0.039841022342443466 0.28610432147979736
MemoryTrain:  epoch  9, batch     7 | loss: 0.3259453Losses:  0.06646271795034409 0.28832897543907166
MemoryTrain:  epoch  9, batch     8 | loss: 0.3547917Losses:  0.06672061234712601 0.45087817311286926
MemoryTrain:  epoch  9, batch     9 | loss: 0.5175988
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.03%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 86.77%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 86.41%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.72%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 82.67%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 82.06%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 81.48%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 80.06%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 79.10%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.37%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.15%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 75.96%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 74.81%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.69%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 72.61%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 71.83%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 70.56%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 69.53%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 69.36%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 68.98%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 68.45%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 67.73%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 67.53%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 67.76%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 71.38%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 70.89%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 70.06%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 70.04%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 70.33%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 69.79%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 69.42%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 68.85%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 68.29%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 67.74%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 67.25%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 66.87%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 66.44%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 66.26%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 65.94%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 65.46%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.44%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 65.96%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 65.58%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 65.20%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 64.74%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 64.02%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 65.02%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 64.64%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 64.26%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 63.67%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 63.38%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 64.18%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 63.63%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 63.36%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 63.17%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 63.05%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 63.29%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 63.14%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 62.92%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:  178 | acc: 18.75%,  total acc: 62.47%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 62.47%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 62.33%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 62.53%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 63.10%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 63.03%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 62.76%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 62.57%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 62.37%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 62.18%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 62.15%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 62.28%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 62.63%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  200 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 62.59%   [EVAL] batch:  202 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:  203 | acc: 0.00%,  total acc: 62.01%   [EVAL] batch:  204 | acc: 6.25%,  total acc: 61.74%   [EVAL] batch:  205 | acc: 6.25%,  total acc: 61.47%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 61.59%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 62.29%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 62.53%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 62.56%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 62.62%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 63.57%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:  225 | acc: 6.25%,  total acc: 63.61%   [EVAL] batch:  226 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  227 | acc: 0.00%,  total acc: 63.05%   [EVAL] batch:  228 | acc: 0.00%,  total acc: 62.77%   [EVAL] batch:  229 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:  230 | acc: 6.25%,  total acc: 62.26%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 62.31%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 62.45%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 62.58%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 62.86%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 62.84%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 62.81%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 62.65%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 62.53%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 62.42%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 62.30%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 62.25%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 62.15%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 61.90%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 61.68%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 61.44%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 61.20%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 60.96%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 60.74%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 60.75%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 60.85%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 60.98%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 61.08%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:  261 | acc: 93.75%,  total acc: 61.31%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 61.39%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 61.37%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 61.42%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 61.42%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 61.38%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 61.29%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 61.18%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 61.05%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 60.82%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 60.69%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 60.54%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 60.34%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 60.14%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 59.93%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 59.71%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 59.50%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 59.29%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 59.07%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 59.09%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 59.19%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 59.22%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 59.36%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 59.46%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 59.54%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 59.64%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 59.73%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 59.85%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 59.97%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 60.04%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 60.17%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 60.23%   [EVAL] batch:  294 | acc: 6.25%,  total acc: 60.04%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 59.84%   [EVAL] batch:  296 | acc: 6.25%,  total acc: 59.66%   [EVAL] batch:  297 | acc: 0.00%,  total acc: 59.46%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 59.26%   [EVAL] batch:  299 | acc: 0.00%,  total acc: 59.06%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 59.05%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 59.13%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 59.18%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 59.25%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 59.24%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 59.31%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 59.30%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 59.19%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 59.12%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 59.17%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 59.10%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 59.15%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 59.22%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 59.35%   [EVAL] batch:  315 | acc: 100.00%,  total acc: 59.47%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 59.56%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 59.67%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 59.74%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 59.99%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 60.05%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 60.26%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 60.35%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 60.37%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 60.36%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 60.47%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 60.49%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 60.54%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 60.60%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 60.72%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 60.75%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 60.83%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 60.87%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 60.89%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 60.88%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 60.85%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 60.83%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 60.82%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 60.77%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 60.70%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 60.71%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 60.68%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 60.73%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 60.72%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 60.76%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 60.72%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 60.72%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 60.73%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 60.73%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 60.76%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:  356 | acc: 12.50%,  total acc: 60.63%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 60.46%   [EVAL] batch:  358 | acc: 0.00%,  total acc: 60.29%   [EVAL] batch:  359 | acc: 0.00%,  total acc: 60.12%   [EVAL] batch:  360 | acc: 0.00%,  total acc: 59.95%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 59.79%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 59.76%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 59.87%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 59.98%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 60.09%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 60.20%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 60.52%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 60.65%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 60.74%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 60.83%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 60.87%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 60.89%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 61.00%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 61.06%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 61.10%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 61.12%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 61.10%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 61.06%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 61.09%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 61.18%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 61.17%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 61.13%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 61.13%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 61.13%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 61.22%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 61.28%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 61.75%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 61.75%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:  402 | acc: 43.75%,  total acc: 61.68%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 61.65%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 61.65%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 61.65%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 61.72%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 61.67%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 61.68%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 61.59%   [EVAL] batch:  411 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 61.53%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 61.56%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 61.67%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 61.77%   [EVAL] batch:  419 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 61.75%   [EVAL] batch:  424 | acc: 62.50%,  total acc: 61.75%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 62.36%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 62.41%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 62.47%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 62.54%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 62.84%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 63.04%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:  453 | acc: 50.00%,  total acc: 63.56%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:  455 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  457 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:  458 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  459 | acc: 68.75%,  total acc: 63.63%   [EVAL] batch:  460 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 63.62%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:  472 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:  474 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  499 | acc: 87.50%,  total acc: 65.79%   
cur_acc:  ['0.9464', '0.7192', '0.7004', '0.7361', '0.5942', '0.6875', '0.7540', '0.8641']
his_acc:  ['0.9464', '0.8345', '0.7580', '0.7167', '0.6743', '0.6500', '0.6508', '0.6579']
----------END
his_acc mean:  [0.9473 0.8367 0.7815 0.7421 0.7285 0.7013 0.6819 0.6634]
