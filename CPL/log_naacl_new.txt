#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  11.659886360168457 1.710938572883606
CurrentTrain: epoch  0, batch     0 | loss: 11.6598864Losses:  10.972888946533203 1.420559048652649
CurrentTrain: epoch  0, batch     1 | loss: 10.9728889Losses:  11.361885070800781 1.6243659257888794
CurrentTrain: epoch  0, batch     2 | loss: 11.3618851Losses:  10.482089042663574 1.4399960041046143
CurrentTrain: epoch  0, batch     3 | loss: 10.4820890Losses:  11.695279121398926 1.8623418807983398
CurrentTrain: epoch  0, batch     4 | loss: 11.6952791Losses:  10.115202903747559 1.7448278665542603
CurrentTrain: epoch  0, batch     5 | loss: 10.1152029Losses:  10.565838813781738 1.4712352752685547
CurrentTrain: epoch  0, batch     6 | loss: 10.5658388Losses:  11.49336051940918 1.8524136543273926
CurrentTrain: epoch  0, batch     7 | loss: 11.4933605Losses:  11.470062255859375 1.5662267208099365
CurrentTrain: epoch  0, batch     8 | loss: 11.4700623Losses:  10.420628547668457 1.9211609363555908
CurrentTrain: epoch  0, batch     9 | loss: 10.4206285Losses:  10.036314010620117 1.6500749588012695
CurrentTrain: epoch  0, batch    10 | loss: 10.0363140Losses:  11.420825004577637 1.7115910053253174
CurrentTrain: epoch  0, batch    11 | loss: 11.4208250Losses:  11.248079299926758 1.6191096305847168
CurrentTrain: epoch  0, batch    12 | loss: 11.2480793Losses:  11.463874816894531 1.9824349880218506
CurrentTrain: epoch  0, batch    13 | loss: 11.4638748Losses:  10.93499755859375 1.647944450378418
CurrentTrain: epoch  0, batch    14 | loss: 10.9349976Losses:  9.919023513793945 1.5344703197479248
CurrentTrain: epoch  0, batch    15 | loss: 9.9190235Losses:  9.799312591552734 1.2951087951660156
CurrentTrain: epoch  0, batch    16 | loss: 9.7993126Losses:  10.607182502746582 1.7222282886505127
CurrentTrain: epoch  0, batch    17 | loss: 10.6071825Losses:  10.66923713684082 1.6965298652648926
CurrentTrain: epoch  0, batch    18 | loss: 10.6692371Losses:  10.23411750793457 1.3887287378311157
CurrentTrain: epoch  0, batch    19 | loss: 10.2341175Losses:  8.965106964111328 1.3006565570831299
CurrentTrain: epoch  0, batch    20 | loss: 8.9651070Losses:  9.172229766845703 1.223607063293457
CurrentTrain: epoch  0, batch    21 | loss: 9.1722298Losses:  9.473307609558105 1.3705377578735352
CurrentTrain: epoch  0, batch    22 | loss: 9.4733076Losses:  10.092208862304688 1.458559274673462
CurrentTrain: epoch  0, batch    23 | loss: 10.0922089Losses:  10.077795028686523 1.440134048461914
CurrentTrain: epoch  0, batch    24 | loss: 10.0777950Losses:  10.433675765991211 1.7271225452423096
CurrentTrain: epoch  0, batch    25 | loss: 10.4336758Losses:  9.554000854492188 1.4754462242126465
CurrentTrain: epoch  0, batch    26 | loss: 9.5540009Losses:  9.994525909423828 1.5673363208770752
CurrentTrain: epoch  0, batch    27 | loss: 9.9945259Losses:  9.61744213104248 1.5902092456817627
CurrentTrain: epoch  0, batch    28 | loss: 9.6174421Losses:  10.350066184997559 1.5992835760116577
CurrentTrain: epoch  0, batch    29 | loss: 10.3500662Losses:  9.179032325744629 1.325040578842163
CurrentTrain: epoch  0, batch    30 | loss: 9.1790323Losses:  8.596212387084961 1.1961455345153809
CurrentTrain: epoch  0, batch    31 | loss: 8.5962124Losses:  9.531006813049316 1.5271961688995361
CurrentTrain: epoch  0, batch    32 | loss: 9.5310068Losses:  8.059170722961426 1.273144006729126
CurrentTrain: epoch  0, batch    33 | loss: 8.0591707Losses:  9.414546012878418 1.5723450183868408
CurrentTrain: epoch  0, batch    34 | loss: 9.4145460Losses:  9.872570991516113 1.3019770383834839
CurrentTrain: epoch  0, batch    35 | loss: 9.8725710Losses:  8.868850708007812 1.250732421875
CurrentTrain: epoch  0, batch    36 | loss: 8.8688507Losses:  8.69625473022461 1.2266852855682373
CurrentTrain: epoch  0, batch    37 | loss: 8.6962547Losses:  8.80325698852539 1.2990877628326416
CurrentTrain: epoch  0, batch    38 | loss: 8.8032570Losses:  9.953206062316895 1.4246070384979248
CurrentTrain: epoch  0, batch    39 | loss: 9.9532061Losses:  9.322044372558594 1.365416169166565
CurrentTrain: epoch  0, batch    40 | loss: 9.3220444Losses:  9.395101547241211 1.1825673580169678
CurrentTrain: epoch  0, batch    41 | loss: 9.3951015Losses:  9.762907028198242 1.3824537992477417
CurrentTrain: epoch  0, batch    42 | loss: 9.7629070Losses:  9.041288375854492 1.2679524421691895
CurrentTrain: epoch  0, batch    43 | loss: 9.0412884Losses:  8.961434364318848 1.534699559211731
CurrentTrain: epoch  0, batch    44 | loss: 8.9614344Losses:  9.027950286865234 1.2870184183120728
CurrentTrain: epoch  0, batch    45 | loss: 9.0279503Losses:  9.260890007019043 1.1405478715896606
CurrentTrain: epoch  0, batch    46 | loss: 9.2608900Losses:  9.335281372070312 1.192596197128296
CurrentTrain: epoch  0, batch    47 | loss: 9.3352814Losses:  8.958555221557617 1.2509163618087769
CurrentTrain: epoch  0, batch    48 | loss: 8.9585552Losses:  9.349847793579102 1.2784929275512695
CurrentTrain: epoch  0, batch    49 | loss: 9.3498478Losses:  8.93239688873291 0.9482957124710083
CurrentTrain: epoch  0, batch    50 | loss: 8.9323969Losses:  8.170989990234375 1.062972068786621
CurrentTrain: epoch  0, batch    51 | loss: 8.1709900Losses:  9.693309783935547 1.3527719974517822
CurrentTrain: epoch  0, batch    52 | loss: 9.6933098Losses:  8.473260879516602 1.0658996105194092
CurrentTrain: epoch  0, batch    53 | loss: 8.4732609Losses:  8.354528427124023 1.0764524936676025
CurrentTrain: epoch  0, batch    54 | loss: 8.3545284Losses:  8.386077880859375 1.233011245727539
CurrentTrain: epoch  0, batch    55 | loss: 8.3860779Losses:  8.917129516601562 1.126957893371582
CurrentTrain: epoch  0, batch    56 | loss: 8.9171295Losses:  8.005904197692871 1.050379991531372
CurrentTrain: epoch  0, batch    57 | loss: 8.0059042Losses:  7.720633506774902 0.850163996219635
CurrentTrain: epoch  0, batch    58 | loss: 7.7206335Losses:  8.52928352355957 1.034505844116211
CurrentTrain: epoch  0, batch    59 | loss: 8.5292835Losses:  8.162894248962402 0.923078179359436
CurrentTrain: epoch  0, batch    60 | loss: 8.1628942Losses:  7.549900531768799 0.8957523703575134
CurrentTrain: epoch  0, batch    61 | loss: 7.5499005Losses:  9.29092788696289 1.2250096797943115
CurrentTrain: epoch  0, batch    62 | loss: 9.2909279Losses:  8.126771926879883 1.0832312107086182
CurrentTrain: epoch  1, batch     0 | loss: 8.1267719Losses:  7.466096878051758 0.8967477679252625
CurrentTrain: epoch  1, batch     1 | loss: 7.4660969Losses:  7.372134208679199 0.9416630268096924
CurrentTrain: epoch  1, batch     2 | loss: 7.3721342Losses:  8.423428535461426 1.0592784881591797
CurrentTrain: epoch  1, batch     3 | loss: 8.4234285Losses:  8.130431175231934 1.0677798986434937
CurrentTrain: epoch  1, batch     4 | loss: 8.1304312Losses:  7.1841278076171875 0.8164910078048706
CurrentTrain: epoch  1, batch     5 | loss: 7.1841278Losses:  7.966658115386963 0.997115969657898
CurrentTrain: epoch  1, batch     6 | loss: 7.9666581Losses:  8.167903900146484 1.1144882440567017
CurrentTrain: epoch  1, batch     7 | loss: 8.1679039Losses:  7.983293533325195 1.0986244678497314
CurrentTrain: epoch  1, batch     8 | loss: 7.9832935Losses:  7.7012434005737305 1.1173245906829834
CurrentTrain: epoch  1, batch     9 | loss: 7.7012434Losses:  8.348795890808105 1.0934696197509766
CurrentTrain: epoch  1, batch    10 | loss: 8.3487959Losses:  7.888060092926025 1.2432705163955688
CurrentTrain: epoch  1, batch    11 | loss: 7.8880601Losses:  7.916165351867676 0.946948230266571
CurrentTrain: epoch  1, batch    12 | loss: 7.9161654Losses:  7.859926223754883 1.046565055847168
CurrentTrain: epoch  1, batch    13 | loss: 7.8599262Losses:  8.639876365661621 1.120610237121582
CurrentTrain: epoch  1, batch    14 | loss: 8.6398764Losses:  8.405874252319336 1.0364505052566528
CurrentTrain: epoch  1, batch    15 | loss: 8.4058743Losses:  8.37869644165039 0.8969517946243286
CurrentTrain: epoch  1, batch    16 | loss: 8.3786964Losses:  8.297757148742676 0.9025682210922241
CurrentTrain: epoch  1, batch    17 | loss: 8.2977571Losses:  7.1165995597839355 0.8084421157836914
CurrentTrain: epoch  1, batch    18 | loss: 7.1165996Losses:  7.721523284912109 1.1390154361724854
CurrentTrain: epoch  1, batch    19 | loss: 7.7215233Losses:  7.201112747192383 0.7873993515968323
CurrentTrain: epoch  1, batch    20 | loss: 7.2011127Losses:  8.268583297729492 1.1680842638015747
CurrentTrain: epoch  1, batch    21 | loss: 8.2685833Losses:  7.724944114685059 0.9756251573562622
CurrentTrain: epoch  1, batch    22 | loss: 7.7249441Losses:  7.498501300811768 1.0616873502731323
CurrentTrain: epoch  1, batch    23 | loss: 7.4985013Losses:  6.461741924285889 0.7776875495910645
CurrentTrain: epoch  1, batch    24 | loss: 6.4617419Losses:  6.360543727874756 0.6700099110603333
CurrentTrain: epoch  1, batch    25 | loss: 6.3605437Losses:  8.157637596130371 1.0799533128738403
CurrentTrain: epoch  1, batch    26 | loss: 8.1576376Losses:  7.32647705078125 0.9871814846992493
CurrentTrain: epoch  1, batch    27 | loss: 7.3264771Losses:  7.924238204956055 1.092482328414917
CurrentTrain: epoch  1, batch    28 | loss: 7.9242382Losses:  6.765564441680908 0.8025504350662231
CurrentTrain: epoch  1, batch    29 | loss: 6.7655644Losses:  7.777849197387695 0.9499484300613403
CurrentTrain: epoch  1, batch    30 | loss: 7.7778492Losses:  8.085277557373047 0.9544834494590759
CurrentTrain: epoch  1, batch    31 | loss: 8.0852776Losses:  7.732418060302734 1.067546010017395
CurrentTrain: epoch  1, batch    32 | loss: 7.7324181Losses:  6.886369705200195 0.7894225716590881
CurrentTrain: epoch  1, batch    33 | loss: 6.8863697Losses:  7.425012588500977 0.8498942852020264
CurrentTrain: epoch  1, batch    34 | loss: 7.4250126Losses:  7.799239635467529 0.8781565427780151
CurrentTrain: epoch  1, batch    35 | loss: 7.7992396Losses:  7.549215316772461 0.8944923877716064
CurrentTrain: epoch  1, batch    36 | loss: 7.5492153Losses:  6.625621795654297 0.6702959537506104
CurrentTrain: epoch  1, batch    37 | loss: 6.6256218Losses:  7.039035797119141 0.7444881200790405
CurrentTrain: epoch  1, batch    38 | loss: 7.0390358Losses:  6.375056743621826 0.7466821074485779
CurrentTrain: epoch  1, batch    39 | loss: 6.3750567Losses:  6.879405975341797 0.8225395679473877
CurrentTrain: epoch  1, batch    40 | loss: 6.8794060Losses:  6.715964317321777 0.8066518306732178
CurrentTrain: epoch  1, batch    41 | loss: 6.7159643Losses:  6.920467853546143 0.8768759369850159
CurrentTrain: epoch  1, batch    42 | loss: 6.9204679Losses:  7.617149353027344 0.9147346615791321
CurrentTrain: epoch  1, batch    43 | loss: 7.6171494Losses:  7.798620223999023 0.8746546506881714
CurrentTrain: epoch  1, batch    44 | loss: 7.7986202Losses:  7.663893699645996 0.8522073030471802
CurrentTrain: epoch  1, batch    45 | loss: 7.6638937Losses:  7.507852077484131 1.0917572975158691
CurrentTrain: epoch  1, batch    46 | loss: 7.5078521Losses:  7.6272382736206055 0.8927900195121765
CurrentTrain: epoch  1, batch    47 | loss: 7.6272383Losses:  7.244458198547363 0.5795468688011169
CurrentTrain: epoch  1, batch    48 | loss: 7.2444582Losses:  7.009355068206787 0.7444283366203308
CurrentTrain: epoch  1, batch    49 | loss: 7.0093551Losses:  7.463980197906494 0.7665423154830933
CurrentTrain: epoch  1, batch    50 | loss: 7.4639802Losses:  7.124890327453613 0.899433970451355
CurrentTrain: epoch  1, batch    51 | loss: 7.1248903Losses:  6.881237983703613 0.6201581954956055
CurrentTrain: epoch  1, batch    52 | loss: 6.8812380Losses:  6.152885437011719 0.6767387390136719
CurrentTrain: epoch  1, batch    53 | loss: 6.1528854Losses:  6.451004505157471 0.6792901754379272
CurrentTrain: epoch  1, batch    54 | loss: 6.4510045Losses:  6.351191520690918 0.6507440805435181
CurrentTrain: epoch  1, batch    55 | loss: 6.3511915Losses:  6.5138092041015625 0.7576944828033447
CurrentTrain: epoch  1, batch    56 | loss: 6.5138092Losses:  7.732420921325684 0.7322022914886475
CurrentTrain: epoch  1, batch    57 | loss: 7.7324209Losses:  6.225381374359131 0.672208309173584
CurrentTrain: epoch  1, batch    58 | loss: 6.2253814Losses:  7.394287109375 0.8324918150901794
CurrentTrain: epoch  1, batch    59 | loss: 7.3942871Losses:  6.490385055541992 0.7648016214370728
CurrentTrain: epoch  1, batch    60 | loss: 6.4903851Losses:  8.335650444030762 1.089666724205017
CurrentTrain: epoch  1, batch    61 | loss: 8.3356504Losses:  5.140726089477539 0.38852882385253906
CurrentTrain: epoch  1, batch    62 | loss: 5.1407261Losses:  5.854081153869629 0.6661226153373718
CurrentTrain: epoch  2, batch     0 | loss: 5.8540812Losses:  6.148200511932373 0.7139421701431274
CurrentTrain: epoch  2, batch     1 | loss: 6.1482005Losses:  5.686216354370117 0.6384278535842896
CurrentTrain: epoch  2, batch     2 | loss: 5.6862164Losses:  7.552926063537598 0.7211054563522339
CurrentTrain: epoch  2, batch     3 | loss: 7.5529261Losses:  8.0425443649292 0.7248491048812866
CurrentTrain: epoch  2, batch     4 | loss: 8.0425444Losses:  6.859673023223877 0.7419476509094238
CurrentTrain: epoch  2, batch     5 | loss: 6.8596730Losses:  6.663628578186035 0.7528502941131592
CurrentTrain: epoch  2, batch     6 | loss: 6.6636286Losses:  6.3021039962768555 0.6312524676322937
CurrentTrain: epoch  2, batch     7 | loss: 6.3021040Losses:  6.415246963500977 0.6895197629928589
CurrentTrain: epoch  2, batch     8 | loss: 6.4152470Losses:  6.840280055999756 0.8192151784896851
CurrentTrain: epoch  2, batch     9 | loss: 6.8402801Losses:  6.19864559173584 0.48924490809440613
CurrentTrain: epoch  2, batch    10 | loss: 6.1986456Losses:  5.677891731262207 0.5084784030914307
CurrentTrain: epoch  2, batch    11 | loss: 5.6778917Losses:  5.876149654388428 0.6430457234382629
CurrentTrain: epoch  2, batch    12 | loss: 5.8761497Losses:  6.015619277954102 0.5457649230957031
CurrentTrain: epoch  2, batch    13 | loss: 6.0156193Losses:  6.424589157104492 0.670076310634613
CurrentTrain: epoch  2, batch    14 | loss: 6.4245892Losses:  6.056757926940918 0.5897715091705322
CurrentTrain: epoch  2, batch    15 | loss: 6.0567579Losses:  6.441903591156006 0.6517987251281738
CurrentTrain: epoch  2, batch    16 | loss: 6.4419036Losses:  6.104837417602539 0.5576265454292297
CurrentTrain: epoch  2, batch    17 | loss: 6.1048374Losses:  5.332138538360596 0.32129722833633423
CurrentTrain: epoch  2, batch    18 | loss: 5.3321385Losses:  6.507681846618652 0.7411682605743408
CurrentTrain: epoch  2, batch    19 | loss: 6.5076818Losses:  5.890718460083008 0.6924550533294678
CurrentTrain: epoch  2, batch    20 | loss: 5.8907185Losses:  6.313446521759033 0.660954475402832
CurrentTrain: epoch  2, batch    21 | loss: 6.3134465Losses:  6.252720832824707 0.6216503381729126
CurrentTrain: epoch  2, batch    22 | loss: 6.2527208Losses:  6.22773551940918 0.5231938362121582
CurrentTrain: epoch  2, batch    23 | loss: 6.2277355Losses:  5.51594352722168 0.517073392868042
CurrentTrain: epoch  2, batch    24 | loss: 5.5159435Losses:  5.784165859222412 0.4502731263637543
CurrentTrain: epoch  2, batch    25 | loss: 5.7841659Losses:  6.214494705200195 0.5934456586837769
CurrentTrain: epoch  2, batch    26 | loss: 6.2144947Losses:  5.992427825927734 0.6432010531425476
CurrentTrain: epoch  2, batch    27 | loss: 5.9924278Losses:  5.564728260040283 0.5025529861450195
CurrentTrain: epoch  2, batch    28 | loss: 5.5647283Losses:  6.253773212432861 0.5570330619812012
CurrentTrain: epoch  2, batch    29 | loss: 6.2537732Losses:  5.874355792999268 0.631815493106842
CurrentTrain: epoch  2, batch    30 | loss: 5.8743558Losses:  5.582281112670898 0.5095794796943665
CurrentTrain: epoch  2, batch    31 | loss: 5.5822811Losses:  6.090355396270752 0.5448764562606812
CurrentTrain: epoch  2, batch    32 | loss: 6.0903554Losses:  6.58300256729126 0.6708952188491821
CurrentTrain: epoch  2, batch    33 | loss: 6.5830026Losses:  5.925968170166016 0.5236928462982178
CurrentTrain: epoch  2, batch    34 | loss: 5.9259682Losses:  6.830263614654541 0.7228072285652161
CurrentTrain: epoch  2, batch    35 | loss: 6.8302636Losses:  5.59820556640625 0.4020402431488037
CurrentTrain: epoch  2, batch    36 | loss: 5.5982056Losses:  6.557716369628906 0.5985221266746521
CurrentTrain: epoch  2, batch    37 | loss: 6.5577164Losses:  5.942734718322754 0.561279296875
CurrentTrain: epoch  2, batch    38 | loss: 5.9427347Losses:  5.604929447174072 0.5897029638290405
CurrentTrain: epoch  2, batch    39 | loss: 5.6049294Losses:  5.77515172958374 0.4974951148033142
CurrentTrain: epoch  2, batch    40 | loss: 5.7751517Losses:  6.92299222946167 0.5852372050285339
CurrentTrain: epoch  2, batch    41 | loss: 6.9229922Losses:  5.7205119132995605 0.46386951208114624
CurrentTrain: epoch  2, batch    42 | loss: 5.7205119Losses:  5.841375350952148 0.6304025650024414
CurrentTrain: epoch  2, batch    43 | loss: 5.8413754Losses:  5.740666389465332 0.4264810085296631
CurrentTrain: epoch  2, batch    44 | loss: 5.7406664Losses:  5.838081359863281 0.6950045824050903
CurrentTrain: epoch  2, batch    45 | loss: 5.8380814Losses:  5.359123229980469 0.4418466091156006
CurrentTrain: epoch  2, batch    46 | loss: 5.3591232Losses:  6.0793633460998535 0.5037866830825806
CurrentTrain: epoch  2, batch    47 | loss: 6.0793633Losses:  5.619547367095947 0.5441266894340515
CurrentTrain: epoch  2, batch    48 | loss: 5.6195474Losses:  6.111042499542236 0.6696611642837524
CurrentTrain: epoch  2, batch    49 | loss: 6.1110425Losses:  5.55112361907959 0.3666571080684662
CurrentTrain: epoch  2, batch    50 | loss: 5.5511236Losses:  5.242435932159424 0.4454660415649414
CurrentTrain: epoch  2, batch    51 | loss: 5.2424359Losses:  5.245832443237305 0.39057886600494385
CurrentTrain: epoch  2, batch    52 | loss: 5.2458324Losses:  5.338474750518799 0.5365234613418579
CurrentTrain: epoch  2, batch    53 | loss: 5.3384748Losses:  5.545101642608643 0.49712273478507996
CurrentTrain: epoch  2, batch    54 | loss: 5.5451016Losses:  5.227343559265137 0.49243125319480896
CurrentTrain: epoch  2, batch    55 | loss: 5.2273436Losses:  5.347570419311523 0.4447348117828369
CurrentTrain: epoch  2, batch    56 | loss: 5.3475704Losses:  6.365145206451416 0.6161133050918579
CurrentTrain: epoch  2, batch    57 | loss: 6.3651452Losses:  5.8138108253479 0.6095014810562134
CurrentTrain: epoch  2, batch    58 | loss: 5.8138108Losses:  5.390261650085449 0.4412386417388916
CurrentTrain: epoch  2, batch    59 | loss: 5.3902617Losses:  5.663802146911621 0.6070674657821655
CurrentTrain: epoch  2, batch    60 | loss: 5.6638021Losses:  6.057365894317627 0.4632316827774048
CurrentTrain: epoch  2, batch    61 | loss: 6.0573659Losses:  4.942676067352295 0.4282688498497009
CurrentTrain: epoch  2, batch    62 | loss: 4.9426761Losses:  5.1439008712768555 0.4422600269317627
CurrentTrain: epoch  3, batch     0 | loss: 5.1439009Losses:  5.2568817138671875 0.4827459454536438
CurrentTrain: epoch  3, batch     1 | loss: 5.2568817Losses:  5.614696502685547 0.5440748929977417
CurrentTrain: epoch  3, batch     2 | loss: 5.6146965Losses:  5.773218631744385 0.43358564376831055
CurrentTrain: epoch  3, batch     3 | loss: 5.7732186Losses:  5.031519412994385 0.3622029423713684
CurrentTrain: epoch  3, batch     4 | loss: 5.0315194Losses:  5.814365863800049 0.39587515592575073
CurrentTrain: epoch  3, batch     5 | loss: 5.8143659Losses:  5.06851053237915 0.40768328309059143
CurrentTrain: epoch  3, batch     6 | loss: 5.0685105Losses:  5.461173057556152 0.41029858589172363
CurrentTrain: epoch  3, batch     7 | loss: 5.4611731Losses:  5.914586067199707 0.4139692187309265
CurrentTrain: epoch  3, batch     8 | loss: 5.9145861Losses:  5.698314189910889 0.5361388921737671
CurrentTrain: epoch  3, batch     9 | loss: 5.6983142Losses:  5.4338250160217285 0.5265849828720093
CurrentTrain: epoch  3, batch    10 | loss: 5.4338250Losses:  5.221922397613525 0.43437445163726807
CurrentTrain: epoch  3, batch    11 | loss: 5.2219224Losses:  5.300400733947754 0.46813029050827026
CurrentTrain: epoch  3, batch    12 | loss: 5.3004007Losses:  5.752448081970215 0.5712911486625671
CurrentTrain: epoch  3, batch    13 | loss: 5.7524481Losses:  5.778468608856201 0.45784643292427063
CurrentTrain: epoch  3, batch    14 | loss: 5.7784686Losses:  5.531735897064209 0.5014216303825378
CurrentTrain: epoch  3, batch    15 | loss: 5.5317359Losses:  6.042949676513672 0.5465662479400635
CurrentTrain: epoch  3, batch    16 | loss: 6.0429497Losses:  5.260751247406006 0.42641228437423706
CurrentTrain: epoch  3, batch    17 | loss: 5.2607512Losses:  5.304389953613281 0.45373785495758057
CurrentTrain: epoch  3, batch    18 | loss: 5.3043900Losses:  4.713019371032715 0.26954367756843567
CurrentTrain: epoch  3, batch    19 | loss: 4.7130194Losses:  5.08981466293335 0.3738914728164673
CurrentTrain: epoch  3, batch    20 | loss: 5.0898147Losses:  5.109796047210693 0.4351058006286621
CurrentTrain: epoch  3, batch    21 | loss: 5.1097960Losses:  5.811213493347168 0.5837201476097107
CurrentTrain: epoch  3, batch    22 | loss: 5.8112135Losses:  5.408981800079346 0.35391098260879517
CurrentTrain: epoch  3, batch    23 | loss: 5.4089818Losses:  5.522214412689209 0.35076838731765747
CurrentTrain: epoch  3, batch    24 | loss: 5.5222144Losses:  5.845862865447998 0.4842853844165802
CurrentTrain: epoch  3, batch    25 | loss: 5.8458629Losses:  4.935579299926758 0.3585866689682007
CurrentTrain: epoch  3, batch    26 | loss: 4.9355793Losses:  4.957229137420654 0.3356221318244934
CurrentTrain: epoch  3, batch    27 | loss: 4.9572291Losses:  5.630646705627441 0.45311877131462097
CurrentTrain: epoch  3, batch    28 | loss: 5.6306467Losses:  5.198989391326904 0.4822582006454468
CurrentTrain: epoch  3, batch    29 | loss: 5.1989894Losses:  4.9730448722839355 0.40742623805999756
CurrentTrain: epoch  3, batch    30 | loss: 4.9730449Losses:  5.250960350036621 0.48572972416877747
CurrentTrain: epoch  3, batch    31 | loss: 5.2509604Losses:  5.257001876831055 0.3836632966995239
CurrentTrain: epoch  3, batch    32 | loss: 5.2570019Losses:  5.290794849395752 0.39389848709106445
CurrentTrain: epoch  3, batch    33 | loss: 5.2907948Losses:  5.3408050537109375 0.38983896374702454
CurrentTrain: epoch  3, batch    34 | loss: 5.3408051Losses:  5.104101181030273 0.4072713255882263
CurrentTrain: epoch  3, batch    35 | loss: 5.1041012Losses:  4.870065689086914 0.3364828824996948
CurrentTrain: epoch  3, batch    36 | loss: 4.8700657Losses:  5.198156356811523 0.446525514125824
CurrentTrain: epoch  3, batch    37 | loss: 5.1981564Losses:  4.872496128082275 0.3390902876853943
CurrentTrain: epoch  3, batch    38 | loss: 4.8724961Losses:  4.781749725341797 0.18308165669441223
CurrentTrain: epoch  3, batch    39 | loss: 4.7817497Losses:  5.808716297149658 0.4988112449645996
CurrentTrain: epoch  3, batch    40 | loss: 5.8087163Losses:  5.174056053161621 0.4321020245552063
CurrentTrain: epoch  3, batch    41 | loss: 5.1740561Losses:  4.771381378173828 0.370256632566452
CurrentTrain: epoch  3, batch    42 | loss: 4.7713814Losses:  5.0809478759765625 0.2719447612762451
CurrentTrain: epoch  3, batch    43 | loss: 5.0809479Losses:  5.348476409912109 0.3653830289840698
CurrentTrain: epoch  3, batch    44 | loss: 5.3484764Losses:  5.126251220703125 0.34573835134506226
CurrentTrain: epoch  3, batch    45 | loss: 5.1262512Losses:  5.170259475708008 0.5364658832550049
CurrentTrain: epoch  3, batch    46 | loss: 5.1702595Losses:  4.755002975463867 0.2409878522157669
CurrentTrain: epoch  3, batch    47 | loss: 4.7550030Losses:  4.999529838562012 0.41788017749786377
CurrentTrain: epoch  3, batch    48 | loss: 4.9995298Losses:  4.941304683685303 0.40942078828811646
CurrentTrain: epoch  3, batch    49 | loss: 4.9413047Losses:  5.601566791534424 0.5202862024307251
CurrentTrain: epoch  3, batch    50 | loss: 5.6015668Losses:  5.18162727355957 0.33677810430526733
CurrentTrain: epoch  3, batch    51 | loss: 5.1816273Losses:  4.613742351531982 0.23633544147014618
CurrentTrain: epoch  3, batch    52 | loss: 4.6137424Losses:  5.778566837310791 0.5002893209457397
CurrentTrain: epoch  3, batch    53 | loss: 5.7785668Losses:  4.778319835662842 0.25505489110946655
CurrentTrain: epoch  3, batch    54 | loss: 4.7783198Losses:  4.781693935394287 0.3255266845226288
CurrentTrain: epoch  3, batch    55 | loss: 4.7816939Losses:  4.911549091339111 0.32274967432022095
CurrentTrain: epoch  3, batch    56 | loss: 4.9115491Losses:  4.934225082397461 0.32521191239356995
CurrentTrain: epoch  3, batch    57 | loss: 4.9342251Losses:  4.6811981201171875 0.26093292236328125
CurrentTrain: epoch  3, batch    58 | loss: 4.6811981Losses:  5.895094394683838 0.4113345742225647
CurrentTrain: epoch  3, batch    59 | loss: 5.8950944Losses:  4.886208534240723 0.3999646008014679
CurrentTrain: epoch  3, batch    60 | loss: 4.8862085Losses:  5.185532093048096 0.3988831639289856
CurrentTrain: epoch  3, batch    61 | loss: 5.1855321Losses:  4.9086995124816895 0.28809595108032227
CurrentTrain: epoch  3, batch    62 | loss: 4.9086995Losses:  5.319982528686523 0.4275413453578949
CurrentTrain: epoch  4, batch     0 | loss: 5.3199825Losses:  4.757288932800293 0.22954940795898438
CurrentTrain: epoch  4, batch     1 | loss: 4.7572889Losses:  5.262115001678467 0.3294386565685272
CurrentTrain: epoch  4, batch     2 | loss: 5.2621150Losses:  4.6464009284973145 0.2867826819419861
CurrentTrain: epoch  4, batch     3 | loss: 4.6464009Losses:  4.742517948150635 0.39034461975097656
CurrentTrain: epoch  4, batch     4 | loss: 4.7425179Losses:  4.68297004699707 0.2598717212677002
CurrentTrain: epoch  4, batch     5 | loss: 4.6829700Losses:  5.3885884284973145 0.2809996008872986
CurrentTrain: epoch  4, batch     6 | loss: 5.3885884Losses:  5.003460884094238 0.2587308883666992
CurrentTrain: epoch  4, batch     7 | loss: 5.0034609Losses:  4.80389404296875 0.29343724250793457
CurrentTrain: epoch  4, batch     8 | loss: 4.8038940Losses:  4.96130895614624 0.34764590859413147
CurrentTrain: epoch  4, batch     9 | loss: 4.9613090Losses:  4.7779011726379395 0.3131527304649353
CurrentTrain: epoch  4, batch    10 | loss: 4.7779012Losses:  5.0673394203186035 0.22458800673484802
CurrentTrain: epoch  4, batch    11 | loss: 5.0673394Losses:  4.922349452972412 0.3746640086174011
CurrentTrain: epoch  4, batch    12 | loss: 4.9223495Losses:  4.719468116760254 0.2981550991535187
CurrentTrain: epoch  4, batch    13 | loss: 4.7194681Losses:  5.132099151611328 0.3786107003688812
CurrentTrain: epoch  4, batch    14 | loss: 5.1320992Losses:  5.042869567871094 0.3152904510498047
CurrentTrain: epoch  4, batch    15 | loss: 5.0428696Losses:  4.887622356414795 0.3844485878944397
CurrentTrain: epoch  4, batch    16 | loss: 4.8876224Losses:  4.806036472320557 0.32155734300613403
CurrentTrain: epoch  4, batch    17 | loss: 4.8060365Losses:  4.820199489593506 0.32372528314590454
CurrentTrain: epoch  4, batch    18 | loss: 4.8201995Losses:  4.658806800842285 0.28069305419921875
CurrentTrain: epoch  4, batch    19 | loss: 4.6588068Losses:  4.657465934753418 0.3222402334213257
CurrentTrain: epoch  4, batch    20 | loss: 4.6574659Losses:  4.899810791015625 0.2550530731678009
CurrentTrain: epoch  4, batch    21 | loss: 4.8998108Losses:  4.7143168449401855 0.338137149810791
CurrentTrain: epoch  4, batch    22 | loss: 4.7143168Losses:  4.823769569396973 0.2599892020225525
CurrentTrain: epoch  4, batch    23 | loss: 4.8237696Losses:  4.754755020141602 0.3310418128967285
CurrentTrain: epoch  4, batch    24 | loss: 4.7547550Losses:  4.831225395202637 0.42650678753852844
CurrentTrain: epoch  4, batch    25 | loss: 4.8312254Losses:  4.69599723815918 0.3788226246833801
CurrentTrain: epoch  4, batch    26 | loss: 4.6959972Losses:  4.908544063568115 0.3349760174751282
CurrentTrain: epoch  4, batch    27 | loss: 4.9085441Losses:  4.985215663909912 0.38168588280677795
CurrentTrain: epoch  4, batch    28 | loss: 4.9852157Losses:  4.878271579742432 0.3338848948478699
CurrentTrain: epoch  4, batch    29 | loss: 4.8782716Losses:  4.630447864532471 0.25232094526290894
CurrentTrain: epoch  4, batch    30 | loss: 4.6304479Losses:  4.814795970916748 0.33393967151641846
CurrentTrain: epoch  4, batch    31 | loss: 4.8147960Losses:  4.624416828155518 0.2947028577327728
CurrentTrain: epoch  4, batch    32 | loss: 4.6244168Losses:  4.601290225982666 0.18029354512691498
CurrentTrain: epoch  4, batch    33 | loss: 4.6012902Losses:  4.661564826965332 0.3224447965621948
CurrentTrain: epoch  4, batch    34 | loss: 4.6615648Losses:  5.01858377456665 0.38514262437820435
CurrentTrain: epoch  4, batch    35 | loss: 5.0185838Losses:  4.889547824859619 0.32786446809768677
CurrentTrain: epoch  4, batch    36 | loss: 4.8895478Losses:  4.674820423126221 0.2921507656574249
CurrentTrain: epoch  4, batch    37 | loss: 4.6748204Losses:  4.633350372314453 0.33775782585144043
CurrentTrain: epoch  4, batch    38 | loss: 4.6333504Losses:  4.687087535858154 0.3331772983074188
CurrentTrain: epoch  4, batch    39 | loss: 4.6870875Losses:  4.6575140953063965 0.2971016764640808
CurrentTrain: epoch  4, batch    40 | loss: 4.6575141Losses:  4.647528648376465 0.32094281911849976
CurrentTrain: epoch  4, batch    41 | loss: 4.6475286Losses:  4.661799907684326 0.26102155447006226
CurrentTrain: epoch  4, batch    42 | loss: 4.6617999Losses:  4.571371078491211 0.30514073371887207
CurrentTrain: epoch  4, batch    43 | loss: 4.5713711Losses:  4.599635601043701 0.301213800907135
CurrentTrain: epoch  4, batch    44 | loss: 4.5996356Losses:  4.741909980773926 0.32182127237319946
CurrentTrain: epoch  4, batch    45 | loss: 4.7419100Losses:  4.755385398864746 0.25237923860549927
CurrentTrain: epoch  4, batch    46 | loss: 4.7553854Losses:  4.603316307067871 0.2547098398208618
CurrentTrain: epoch  4, batch    47 | loss: 4.6033163Losses:  4.820873737335205 0.33922237157821655
CurrentTrain: epoch  4, batch    48 | loss: 4.8208737Losses:  4.633570671081543 0.23821261525154114
CurrentTrain: epoch  4, batch    49 | loss: 4.6335707Losses:  4.585082530975342 0.2676255404949188
CurrentTrain: epoch  4, batch    50 | loss: 4.5850825Losses:  4.705652236938477 0.24605654180049896
CurrentTrain: epoch  4, batch    51 | loss: 4.7056522Losses:  4.636256694793701 0.27798992395401
CurrentTrain: epoch  4, batch    52 | loss: 4.6362567Losses:  5.935027599334717 0.3508439064025879
CurrentTrain: epoch  4, batch    53 | loss: 5.9350276Losses:  4.643686294555664 0.25815004110336304
CurrentTrain: epoch  4, batch    54 | loss: 4.6436863Losses:  4.654439926147461 0.3237135410308838
CurrentTrain: epoch  4, batch    55 | loss: 4.6544399Losses:  4.654765605926514 0.3055300712585449
CurrentTrain: epoch  4, batch    56 | loss: 4.6547656Losses:  4.541546821594238 0.23517847061157227
CurrentTrain: epoch  4, batch    57 | loss: 4.5415468Losses:  4.979272365570068 0.3638240396976471
CurrentTrain: epoch  4, batch    58 | loss: 4.9792724Losses:  4.542700290679932 0.31469249725341797
CurrentTrain: epoch  4, batch    59 | loss: 4.5427003Losses:  4.772315502166748 0.26207199692726135
CurrentTrain: epoch  4, batch    60 | loss: 4.7723155Losses:  4.583229064941406 0.23002266883850098
CurrentTrain: epoch  4, batch    61 | loss: 4.5832291Losses:  4.290004730224609 0.13166415691375732
CurrentTrain: epoch  4, batch    62 | loss: 4.2900047Losses:  4.464813232421875 0.19140034914016724
CurrentTrain: epoch  5, batch     0 | loss: 4.4648132Losses:  4.502047061920166 0.24721859395503998
CurrentTrain: epoch  5, batch     1 | loss: 4.5020471Losses:  4.655493259429932 0.296970009803772
CurrentTrain: epoch  5, batch     2 | loss: 4.6554933Losses:  4.649383544921875 0.31490445137023926
CurrentTrain: epoch  5, batch     3 | loss: 4.6493835Losses:  4.9261040687561035 0.2915186583995819
CurrentTrain: epoch  5, batch     4 | loss: 4.9261041Losses:  4.6520233154296875 0.2716538906097412
CurrentTrain: epoch  5, batch     5 | loss: 4.6520233Losses:  5.069810390472412 0.39652693271636963
CurrentTrain: epoch  5, batch     6 | loss: 5.0698104Losses:  4.614284992218018 0.2756290137767792
CurrentTrain: epoch  5, batch     7 | loss: 4.6142850Losses:  4.673651695251465 0.336954802274704
CurrentTrain: epoch  5, batch     8 | loss: 4.6736517Losses:  4.8662004470825195 0.2939843237400055
CurrentTrain: epoch  5, batch     9 | loss: 4.8662004Losses:  4.752016544342041 0.3332919478416443
CurrentTrain: epoch  5, batch    10 | loss: 4.7520165Losses:  4.509913444519043 0.23800018429756165
CurrentTrain: epoch  5, batch    11 | loss: 4.5099134Losses:  4.586976528167725 0.311085045337677
CurrentTrain: epoch  5, batch    12 | loss: 4.5869765Losses:  4.488047122955322 0.29540008306503296
CurrentTrain: epoch  5, batch    13 | loss: 4.4880471Losses:  4.6025710105896 0.32348474860191345
CurrentTrain: epoch  5, batch    14 | loss: 4.6025710Losses:  4.825380802154541 0.23470428586006165
CurrentTrain: epoch  5, batch    15 | loss: 4.8253808Losses:  4.563674449920654 0.2223430573940277
CurrentTrain: epoch  5, batch    16 | loss: 4.5636744Losses:  4.412065505981445 0.2240183800458908
CurrentTrain: epoch  5, batch    17 | loss: 4.4120655Losses:  4.521369934082031 0.23120670020580292
CurrentTrain: epoch  5, batch    18 | loss: 4.5213699Losses:  4.5197577476501465 0.2708262503147125
CurrentTrain: epoch  5, batch    19 | loss: 4.5197577Losses:  4.4661478996276855 0.21434611082077026
CurrentTrain: epoch  5, batch    20 | loss: 4.4661479Losses:  4.365229606628418 0.2248842418193817
CurrentTrain: epoch  5, batch    21 | loss: 4.3652296Losses:  4.371096611022949 0.13700200617313385
CurrentTrain: epoch  5, batch    22 | loss: 4.3710966Losses:  4.669679164886475 0.23606976866722107
CurrentTrain: epoch  5, batch    23 | loss: 4.6696792Losses:  4.357662200927734 0.17712661623954773
CurrentTrain: epoch  5, batch    24 | loss: 4.3576622Losses:  5.189477920532227 0.3645990490913391
CurrentTrain: epoch  5, batch    25 | loss: 5.1894779Losses:  4.48818826675415 0.24914516508579254
CurrentTrain: epoch  5, batch    26 | loss: 4.4881883Losses:  4.567884922027588 0.28317517042160034
CurrentTrain: epoch  5, batch    27 | loss: 4.5678849Losses:  4.3498759269714355 0.15667571127414703
CurrentTrain: epoch  5, batch    28 | loss: 4.3498759Losses:  4.535358428955078 0.28301429748535156
CurrentTrain: epoch  5, batch    29 | loss: 4.5353584Losses:  4.603872299194336 0.2948184609413147
CurrentTrain: epoch  5, batch    30 | loss: 4.6038723Losses:  4.549717426300049 0.25935477018356323
CurrentTrain: epoch  5, batch    31 | loss: 4.5497174Losses:  4.562640190124512 0.27338263392448425
CurrentTrain: epoch  5, batch    32 | loss: 4.5626402Losses:  4.4566450119018555 0.1988683044910431
CurrentTrain: epoch  5, batch    33 | loss: 4.4566450Losses:  4.618939399719238 0.2734561562538147
CurrentTrain: epoch  5, batch    34 | loss: 4.6189394Losses:  4.413613796234131 0.2603870630264282
CurrentTrain: epoch  5, batch    35 | loss: 4.4136138Losses:  4.511982440948486 0.2673371434211731
CurrentTrain: epoch  5, batch    36 | loss: 4.5119824Losses:  4.327818393707275 0.21986931562423706
CurrentTrain: epoch  5, batch    37 | loss: 4.3278184Losses:  4.4640889167785645 0.30389559268951416
CurrentTrain: epoch  5, batch    38 | loss: 4.4640889Losses:  4.520452499389648 0.21169845759868622
CurrentTrain: epoch  5, batch    39 | loss: 4.5204525Losses:  4.347056865692139 0.20744194090366364
CurrentTrain: epoch  5, batch    40 | loss: 4.3470569Losses:  4.855810642242432 0.3000512719154358
CurrentTrain: epoch  5, batch    41 | loss: 4.8558106Losses:  5.190673351287842 0.21162256598472595
CurrentTrain: epoch  5, batch    42 | loss: 5.1906734Losses:  4.4288105964660645 0.1995013803243637
CurrentTrain: epoch  5, batch    43 | loss: 4.4288106Losses:  4.530525207519531 0.2644094228744507
CurrentTrain: epoch  5, batch    44 | loss: 4.5305252Losses:  4.420708656311035 0.2500039041042328
CurrentTrain: epoch  5, batch    45 | loss: 4.4207087Losses:  4.359099864959717 0.21880389750003815
CurrentTrain: epoch  5, batch    46 | loss: 4.3590999Losses:  4.749438762664795 0.27361828088760376
CurrentTrain: epoch  5, batch    47 | loss: 4.7494388Losses:  4.445235729217529 0.22786667943000793
CurrentTrain: epoch  5, batch    48 | loss: 4.4452357Losses:  4.875962734222412 0.2161373794078827
CurrentTrain: epoch  5, batch    49 | loss: 4.8759627Losses:  4.530788898468018 0.220873162150383
CurrentTrain: epoch  5, batch    50 | loss: 4.5307889Losses:  4.403820991516113 0.21774134039878845
CurrentTrain: epoch  5, batch    51 | loss: 4.4038210Losses:  4.37875509262085 0.1759837567806244
CurrentTrain: epoch  5, batch    52 | loss: 4.3787551Losses:  4.6268744468688965 0.287523090839386
CurrentTrain: epoch  5, batch    53 | loss: 4.6268744Losses:  4.430186748504639 0.2695774435997009
CurrentTrain: epoch  5, batch    54 | loss: 4.4301867Losses:  4.3755059242248535 0.14019940793514252
CurrentTrain: epoch  5, batch    55 | loss: 4.3755059Losses:  4.78496789932251 0.2175704538822174
CurrentTrain: epoch  5, batch    56 | loss: 4.7849679Losses:  4.357084274291992 0.1890108585357666
CurrentTrain: epoch  5, batch    57 | loss: 4.3570843Losses:  4.661622047424316 0.27138710021972656
CurrentTrain: epoch  5, batch    58 | loss: 4.6616220Losses:  4.519618988037109 0.23521722853183746
CurrentTrain: epoch  5, batch    59 | loss: 4.5196190Losses:  4.417490005493164 0.18837355077266693
CurrentTrain: epoch  5, batch    60 | loss: 4.4174900Losses:  4.500296115875244 0.2556701898574829
CurrentTrain: epoch  5, batch    61 | loss: 4.5002961Losses:  4.3951416015625 0.1508370190858841
CurrentTrain: epoch  5, batch    62 | loss: 4.3951416Losses:  4.473291397094727 0.19654062390327454
CurrentTrain: epoch  6, batch     0 | loss: 4.4732914Losses:  4.474442481994629 0.18636541068553925
CurrentTrain: epoch  6, batch     1 | loss: 4.4744425Losses:  4.469724178314209 0.16278252005577087
CurrentTrain: epoch  6, batch     2 | loss: 4.4697242Losses:  4.540556907653809 0.2951444685459137
CurrentTrain: epoch  6, batch     3 | loss: 4.5405569Losses:  4.416133403778076 0.2131124585866928
CurrentTrain: epoch  6, batch     4 | loss: 4.4161334Losses:  4.601609230041504 0.254984974861145
CurrentTrain: epoch  6, batch     5 | loss: 4.6016092Losses:  4.585568428039551 0.26302120089530945
CurrentTrain: epoch  6, batch     6 | loss: 4.5855684Losses:  4.441653728485107 0.2664180397987366
CurrentTrain: epoch  6, batch     7 | loss: 4.4416537Losses:  4.417961597442627 0.11022728681564331
CurrentTrain: epoch  6, batch     8 | loss: 4.4179616Losses:  4.364918231964111 0.22967678308486938
CurrentTrain: epoch  6, batch     9 | loss: 4.3649182Losses:  4.629316329956055 0.2722913920879364
CurrentTrain: epoch  6, batch    10 | loss: 4.6293163Losses:  4.307150840759277 0.1406480371952057
CurrentTrain: epoch  6, batch    11 | loss: 4.3071508Losses:  4.507521152496338 0.2379138171672821
CurrentTrain: epoch  6, batch    12 | loss: 4.5075212Losses:  4.3218607902526855 0.20078524947166443
CurrentTrain: epoch  6, batch    13 | loss: 4.3218608Losses:  4.355379581451416 0.21566730737686157
CurrentTrain: epoch  6, batch    14 | loss: 4.3553796Losses:  4.486761569976807 0.3145917057991028
CurrentTrain: epoch  6, batch    15 | loss: 4.4867616Losses:  4.3036699295043945 0.15978777408599854
CurrentTrain: epoch  6, batch    16 | loss: 4.3036699Losses:  4.472149848937988 0.21680155396461487
CurrentTrain: epoch  6, batch    17 | loss: 4.4721498Losses:  4.447312831878662 0.26391467452049255
CurrentTrain: epoch  6, batch    18 | loss: 4.4473128Losses:  4.375540733337402 0.1781986951828003
CurrentTrain: epoch  6, batch    19 | loss: 4.3755407Losses:  4.325238227844238 0.16829128563404083
CurrentTrain: epoch  6, batch    20 | loss: 4.3252382Losses:  4.431051254272461 0.2249690294265747
CurrentTrain: epoch  6, batch    21 | loss: 4.4310513Losses:  4.351372718811035 0.2210361510515213
CurrentTrain: epoch  6, batch    22 | loss: 4.3513727Losses:  4.410844802856445 0.18856292963027954
CurrentTrain: epoch  6, batch    23 | loss: 4.4108448Losses:  4.39790678024292 0.2420307993888855
CurrentTrain: epoch  6, batch    24 | loss: 4.3979068Losses:  4.3071465492248535 0.17516866326332092
CurrentTrain: epoch  6, batch    25 | loss: 4.3071465Losses:  4.348061561584473 0.2521875500679016
CurrentTrain: epoch  6, batch    26 | loss: 4.3480616Losses:  4.429852485656738 0.26176145672798157
CurrentTrain: epoch  6, batch    27 | loss: 4.4298525Losses:  4.249901294708252 0.15356123447418213
CurrentTrain: epoch  6, batch    28 | loss: 4.2499013Losses:  4.483388900756836 0.24966788291931152
CurrentTrain: epoch  6, batch    29 | loss: 4.4833889Losses:  4.242116928100586 0.16371890902519226
CurrentTrain: epoch  6, batch    30 | loss: 4.2421169Losses:  4.376932621002197 0.17040860652923584
CurrentTrain: epoch  6, batch    31 | loss: 4.3769326Losses:  4.367557048797607 0.15551242232322693
CurrentTrain: epoch  6, batch    32 | loss: 4.3675570Losses:  4.403324604034424 0.18665054440498352
CurrentTrain: epoch  6, batch    33 | loss: 4.4033246Losses:  4.42793083190918 0.16871534287929535
CurrentTrain: epoch  6, batch    34 | loss: 4.4279308Losses:  4.368719577789307 0.16918496787548065
CurrentTrain: epoch  6, batch    35 | loss: 4.3687196Losses:  4.325976371765137 0.18140262365341187
CurrentTrain: epoch  6, batch    36 | loss: 4.3259764Losses:  4.399837493896484 0.26216191053390503
CurrentTrain: epoch  6, batch    37 | loss: 4.3998375Losses:  4.343515872955322 0.11882129311561584
CurrentTrain: epoch  6, batch    38 | loss: 4.3435159Losses:  4.335420608520508 0.23658280074596405
CurrentTrain: epoch  6, batch    39 | loss: 4.3354206Losses:  4.3983588218688965 0.19709423184394836
CurrentTrain: epoch  6, batch    40 | loss: 4.3983588Losses:  4.362977504730225 0.2294069230556488
CurrentTrain: epoch  6, batch    41 | loss: 4.3629775Losses:  4.356235504150391 0.21478354930877686
CurrentTrain: epoch  6, batch    42 | loss: 4.3562355Losses:  4.290801048278809 0.20653796195983887
CurrentTrain: epoch  6, batch    43 | loss: 4.2908010Losses:  4.4045891761779785 0.21159985661506653
CurrentTrain: epoch  6, batch    44 | loss: 4.4045892Losses:  4.345933437347412 0.20803327858448029
CurrentTrain: epoch  6, batch    45 | loss: 4.3459334Losses:  4.358779430389404 0.21306286752223969
CurrentTrain: epoch  6, batch    46 | loss: 4.3587794Losses:  4.320939540863037 0.1902550607919693
CurrentTrain: epoch  6, batch    47 | loss: 4.3209395Losses:  4.343735694885254 0.21825231611728668
CurrentTrain: epoch  6, batch    48 | loss: 4.3437357Losses:  4.367829322814941 0.1400776207447052
CurrentTrain: epoch  6, batch    49 | loss: 4.3678293Losses:  4.325343132019043 0.20274779200553894
CurrentTrain: epoch  6, batch    50 | loss: 4.3253431Losses:  4.347511291503906 0.22188162803649902
CurrentTrain: epoch  6, batch    51 | loss: 4.3475113Losses:  4.358870029449463 0.22192254662513733
CurrentTrain: epoch  6, batch    52 | loss: 4.3588700Losses:  4.286023139953613 0.19435501098632812
CurrentTrain: epoch  6, batch    53 | loss: 4.2860231Losses:  4.270755767822266 0.18300923705101013
CurrentTrain: epoch  6, batch    54 | loss: 4.2707558Losses:  4.251893997192383 0.17642781138420105
CurrentTrain: epoch  6, batch    55 | loss: 4.2518940Losses:  4.254576683044434 0.18037594854831696
CurrentTrain: epoch  6, batch    56 | loss: 4.2545767Losses:  4.286606788635254 0.22603337466716766
CurrentTrain: epoch  6, batch    57 | loss: 4.2866068Losses:  4.369820594787598 0.24837437272071838
CurrentTrain: epoch  6, batch    58 | loss: 4.3698206Losses:  4.272543430328369 0.19290296733379364
CurrentTrain: epoch  6, batch    59 | loss: 4.2725434Losses:  4.270984172821045 0.19017231464385986
CurrentTrain: epoch  6, batch    60 | loss: 4.2709842Losses:  4.172334671020508 0.12317074835300446
CurrentTrain: epoch  6, batch    61 | loss: 4.1723347Losses:  4.14246940612793 0.10088267922401428
CurrentTrain: epoch  6, batch    62 | loss: 4.1424694Losses:  4.261858940124512 0.1877126842737198
CurrentTrain: epoch  7, batch     0 | loss: 4.2618589Losses:  4.335817337036133 0.20815905928611755
CurrentTrain: epoch  7, batch     1 | loss: 4.3358173Losses:  4.3001556396484375 0.14378182590007782
CurrentTrain: epoch  7, batch     2 | loss: 4.3001556Losses:  4.322722911834717 0.20024746656417847
CurrentTrain: epoch  7, batch     3 | loss: 4.3227229Losses:  4.179065227508545 0.09820935130119324
CurrentTrain: epoch  7, batch     4 | loss: 4.1790652Losses:  4.213253498077393 0.14714683592319489
CurrentTrain: epoch  7, batch     5 | loss: 4.2132535Losses:  4.2845282554626465 0.20362433791160583
CurrentTrain: epoch  7, batch     6 | loss: 4.2845283Losses:  4.208325386047363 0.16608625650405884
CurrentTrain: epoch  7, batch     7 | loss: 4.2083254Losses:  4.343505382537842 0.20751699805259705
CurrentTrain: epoch  7, batch     8 | loss: 4.3435054Losses:  4.216020107269287 0.18297477066516876
CurrentTrain: epoch  7, batch     9 | loss: 4.2160201Losses:  4.545851707458496 0.22572335600852966
CurrentTrain: epoch  7, batch    10 | loss: 4.5458517Losses:  4.319306373596191 0.2243848443031311
CurrentTrain: epoch  7, batch    11 | loss: 4.3193064Losses:  4.280610084533691 0.197519913315773
CurrentTrain: epoch  7, batch    12 | loss: 4.2806101Losses:  4.25903844833374 0.2131011188030243
CurrentTrain: epoch  7, batch    13 | loss: 4.2590384Losses:  4.343852519989014 0.20372799038887024
CurrentTrain: epoch  7, batch    14 | loss: 4.3438525Losses:  4.272617816925049 0.1651703119277954
CurrentTrain: epoch  7, batch    15 | loss: 4.2726178Losses:  4.253327369689941 0.2010725885629654
CurrentTrain: epoch  7, batch    16 | loss: 4.2533274Losses:  4.161350727081299 0.12699416279792786
CurrentTrain: epoch  7, batch    17 | loss: 4.1613507Losses:  4.347871780395508 0.23143017292022705
CurrentTrain: epoch  7, batch    18 | loss: 4.3478718Losses:  4.212744235992432 0.11503288894891739
CurrentTrain: epoch  7, batch    19 | loss: 4.2127442Losses:  4.28974723815918 0.1857147365808487
CurrentTrain: epoch  7, batch    20 | loss: 4.2897472Losses:  4.305844306945801 0.21827031672000885
CurrentTrain: epoch  7, batch    21 | loss: 4.3058443Losses:  4.281497001647949 0.18786141276359558
CurrentTrain: epoch  7, batch    22 | loss: 4.2814970Losses:  4.333199501037598 0.25519827008247375
CurrentTrain: epoch  7, batch    23 | loss: 4.3331995Losses:  4.254079341888428 0.15911072492599487
CurrentTrain: epoch  7, batch    24 | loss: 4.2540793Losses:  4.391103744506836 0.22339202463626862
CurrentTrain: epoch  7, batch    25 | loss: 4.3911037Losses:  4.232274055480957 0.15862448513507843
CurrentTrain: epoch  7, batch    26 | loss: 4.2322741Losses:  4.275111675262451 0.1936725229024887
CurrentTrain: epoch  7, batch    27 | loss: 4.2751117Losses:  4.270461559295654 0.15367253124713898
CurrentTrain: epoch  7, batch    28 | loss: 4.2704616Losses:  4.219850540161133 0.14228606224060059
CurrentTrain: epoch  7, batch    29 | loss: 4.2198505Losses:  4.213450908660889 0.14263808727264404
CurrentTrain: epoch  7, batch    30 | loss: 4.2134509Losses:  4.228513717651367 0.15782515704631805
CurrentTrain: epoch  7, batch    31 | loss: 4.2285137Losses:  4.190234661102295 0.14310862123966217
CurrentTrain: epoch  7, batch    32 | loss: 4.1902347Losses:  4.1754255294799805 0.17692619562149048
CurrentTrain: epoch  7, batch    33 | loss: 4.1754255Losses:  4.156692028045654 0.12847107648849487
CurrentTrain: epoch  7, batch    34 | loss: 4.1566920Losses:  4.216058731079102 0.1392078548669815
CurrentTrain: epoch  7, batch    35 | loss: 4.2160587Losses:  4.221097946166992 0.17347314953804016
CurrentTrain: epoch  7, batch    36 | loss: 4.2210979Losses:  4.321230888366699 0.218703031539917
CurrentTrain: epoch  7, batch    37 | loss: 4.3212309Losses:  4.209001064300537 0.14865431189537048
CurrentTrain: epoch  7, batch    38 | loss: 4.2090011Losses:  4.172845363616943 0.13916489481925964
CurrentTrain: epoch  7, batch    39 | loss: 4.1728454Losses:  4.203446865081787 0.14876487851142883
CurrentTrain: epoch  7, batch    40 | loss: 4.2034469Losses:  4.208391189575195 0.14664292335510254
CurrentTrain: epoch  7, batch    41 | loss: 4.2083912Losses:  4.195313453674316 0.1668044477701187
CurrentTrain: epoch  7, batch    42 | loss: 4.1953135Losses:  4.250056743621826 0.19003233313560486
CurrentTrain: epoch  7, batch    43 | loss: 4.2500567Losses:  4.203962326049805 0.1357099413871765
CurrentTrain: epoch  7, batch    44 | loss: 4.2039623Losses:  4.254854202270508 0.16909393668174744
CurrentTrain: epoch  7, batch    45 | loss: 4.2548542Losses:  4.2454609870910645 0.16137605905532837
CurrentTrain: epoch  7, batch    46 | loss: 4.2454610Losses:  4.194103717803955 0.13447614014148712
CurrentTrain: epoch  7, batch    47 | loss: 4.1941037Losses:  4.280978679656982 0.18751996755599976
CurrentTrain: epoch  7, batch    48 | loss: 4.2809787Losses:  4.231058597564697 0.17367586493492126
CurrentTrain: epoch  7, batch    49 | loss: 4.2310586Losses:  4.300364971160889 0.19550073146820068
CurrentTrain: epoch  7, batch    50 | loss: 4.3003650Losses:  4.293154716491699 0.17273235321044922
CurrentTrain: epoch  7, batch    51 | loss: 4.2931547Losses:  4.212637424468994 0.17269480228424072
CurrentTrain: epoch  7, batch    52 | loss: 4.2126374Losses:  4.182976245880127 0.12991726398468018
CurrentTrain: epoch  7, batch    53 | loss: 4.1829762Losses:  4.230932712554932 0.17742271721363068
CurrentTrain: epoch  7, batch    54 | loss: 4.2309327Losses:  4.326167583465576 0.2215019166469574
CurrentTrain: epoch  7, batch    55 | loss: 4.3261676Losses:  4.219717502593994 0.16072887182235718
CurrentTrain: epoch  7, batch    56 | loss: 4.2197175Losses:  4.331058025360107 0.19541099667549133
CurrentTrain: epoch  7, batch    57 | loss: 4.3310580Losses:  4.212986469268799 0.15520423650741577
CurrentTrain: epoch  7, batch    58 | loss: 4.2129865Losses:  4.24473237991333 0.1754729002714157
CurrentTrain: epoch  7, batch    59 | loss: 4.2447324Losses:  4.264087677001953 0.2017805278301239
CurrentTrain: epoch  7, batch    60 | loss: 4.2640877Losses:  4.18524169921875 0.18953058123588562
CurrentTrain: epoch  7, batch    61 | loss: 4.1852417Losses:  4.169858932495117 0.0831584706902504
CurrentTrain: epoch  7, batch    62 | loss: 4.1698589Losses:  4.218068599700928 0.1321280598640442
CurrentTrain: epoch  8, batch     0 | loss: 4.2180686Losses:  4.188766002655029 0.1433677077293396
CurrentTrain: epoch  8, batch     1 | loss: 4.1887660Losses:  4.219823360443115 0.1452326476573944
CurrentTrain: epoch  8, batch     2 | loss: 4.2198234Losses:  4.160455226898193 0.13247676193714142
CurrentTrain: epoch  8, batch     3 | loss: 4.1604552Losses:  4.31804895401001 0.21488375961780548
CurrentTrain: epoch  8, batch     4 | loss: 4.3180490Losses:  4.23930549621582 0.17535923421382904
CurrentTrain: epoch  8, batch     5 | loss: 4.2393055Losses:  4.193964004516602 0.15272793173789978
CurrentTrain: epoch  8, batch     6 | loss: 4.1939640Losses:  4.19044828414917 0.15311750769615173
CurrentTrain: epoch  8, batch     7 | loss: 4.1904483Losses:  4.2084784507751465 0.12613266706466675
CurrentTrain: epoch  8, batch     8 | loss: 4.2084785Losses:  4.102642059326172 0.09452830255031586
CurrentTrain: epoch  8, batch     9 | loss: 4.1026421Losses:  4.265262126922607 0.21024499833583832
CurrentTrain: epoch  8, batch    10 | loss: 4.2652621Losses:  4.2334442138671875 0.17365095019340515
CurrentTrain: epoch  8, batch    11 | loss: 4.2334442Losses:  4.199824333190918 0.17462053894996643
CurrentTrain: epoch  8, batch    12 | loss: 4.1998243Losses:  4.223373889923096 0.16186951100826263
CurrentTrain: epoch  8, batch    13 | loss: 4.2233739Losses:  4.195916652679443 0.13958582282066345
CurrentTrain: epoch  8, batch    14 | loss: 4.1959167Losses:  4.168359756469727 0.11403612792491913
CurrentTrain: epoch  8, batch    15 | loss: 4.1683598Losses:  4.280011177062988 0.18885546922683716
CurrentTrain: epoch  8, batch    16 | loss: 4.2800112Losses:  4.236361980438232 0.18069395422935486
CurrentTrain: epoch  8, batch    17 | loss: 4.2363620Losses:  4.244908332824707 0.14473259449005127
CurrentTrain: epoch  8, batch    18 | loss: 4.2449083Losses:  4.247925281524658 0.17932748794555664
CurrentTrain: epoch  8, batch    19 | loss: 4.2479253Losses:  4.169057846069336 0.14400291442871094
CurrentTrain: epoch  8, batch    20 | loss: 4.1690578Losses:  4.221340656280518 0.1499142199754715
CurrentTrain: epoch  8, batch    21 | loss: 4.2213407Losses:  4.1862006187438965 0.1674947738647461
CurrentTrain: epoch  8, batch    22 | loss: 4.1862006Losses:  4.1734209060668945 0.1553436666727066
CurrentTrain: epoch  8, batch    23 | loss: 4.1734209Losses:  4.189230918884277 0.15421022474765778
CurrentTrain: epoch  8, batch    24 | loss: 4.1892309Losses:  4.126559257507324 0.12086112052202225
CurrentTrain: epoch  8, batch    25 | loss: 4.1265593Losses:  4.219730854034424 0.16143864393234253
CurrentTrain: epoch  8, batch    26 | loss: 4.2197309Losses:  4.127283573150635 0.09644483774900436
CurrentTrain: epoch  8, batch    27 | loss: 4.1272836Losses:  4.149112224578857 0.12568935751914978
CurrentTrain: epoch  8, batch    28 | loss: 4.1491122Losses:  4.127504348754883 0.10716128349304199
CurrentTrain: epoch  8, batch    29 | loss: 4.1275043Losses:  4.149157524108887 0.1654987633228302
CurrentTrain: epoch  8, batch    30 | loss: 4.1491575Losses:  4.065166473388672 0.08899899572134018
CurrentTrain: epoch  8, batch    31 | loss: 4.0651665Losses:  4.217979431152344 0.1482703685760498
CurrentTrain: epoch  8, batch    32 | loss: 4.2179794Losses:  4.242519378662109 0.1566201150417328
CurrentTrain: epoch  8, batch    33 | loss: 4.2425194Losses:  4.180057525634766 0.17093753814697266
CurrentTrain: epoch  8, batch    34 | loss: 4.1800575Losses:  4.184059143066406 0.17337724566459656
CurrentTrain: epoch  8, batch    35 | loss: 4.1840591Losses:  4.19789981842041 0.17633585631847382
CurrentTrain: epoch  8, batch    36 | loss: 4.1978998Losses:  4.187229633331299 0.13403499126434326
CurrentTrain: epoch  8, batch    37 | loss: 4.1872296Losses:  4.2389116287231445 0.1634170114994049
CurrentTrain: epoch  8, batch    38 | loss: 4.2389116Losses:  4.1584248542785645 0.12038376927375793
CurrentTrain: epoch  8, batch    39 | loss: 4.1584249Losses:  4.176095962524414 0.1297861486673355
CurrentTrain: epoch  8, batch    40 | loss: 4.1760960Losses:  4.229708671569824 0.1710660457611084
CurrentTrain: epoch  8, batch    41 | loss: 4.2297087Losses:  4.147572040557861 0.1262456178665161
CurrentTrain: epoch  8, batch    42 | loss: 4.1475720Losses:  4.102271556854248 0.09649457037448883
CurrentTrain: epoch  8, batch    43 | loss: 4.1022716Losses:  4.156938552856445 0.157208651304245
CurrentTrain: epoch  8, batch    44 | loss: 4.1569386Losses:  4.1557464599609375 0.12493397295475006
CurrentTrain: epoch  8, batch    45 | loss: 4.1557465Losses:  4.110826015472412 0.10516639053821564
CurrentTrain: epoch  8, batch    46 | loss: 4.1108260Losses:  4.174870491027832 0.15379348397254944
CurrentTrain: epoch  8, batch    47 | loss: 4.1748705Losses:  4.164541721343994 0.15192502737045288
CurrentTrain: epoch  8, batch    48 | loss: 4.1645417Losses:  4.196450233459473 0.16298489272594452
CurrentTrain: epoch  8, batch    49 | loss: 4.1964502Losses:  4.163439750671387 0.15641364455223083
CurrentTrain: epoch  8, batch    50 | loss: 4.1634398Losses:  4.218715190887451 0.17973467707633972
CurrentTrain: epoch  8, batch    51 | loss: 4.2187152Losses:  4.252891540527344 0.17225559055805206
CurrentTrain: epoch  8, batch    52 | loss: 4.2528915Losses:  4.196159362792969 0.1666087657213211
CurrentTrain: epoch  8, batch    53 | loss: 4.1961594Losses:  4.224046230316162 0.14369621872901917
CurrentTrain: epoch  8, batch    54 | loss: 4.2240462Losses:  4.204168796539307 0.16113504767417908
CurrentTrain: epoch  8, batch    55 | loss: 4.2041688Losses:  4.15917444229126 0.13303010165691376
CurrentTrain: epoch  8, batch    56 | loss: 4.1591744Losses:  4.149951934814453 0.132654070854187
CurrentTrain: epoch  8, batch    57 | loss: 4.1499519Losses:  4.21158504486084 0.18455606698989868
CurrentTrain: epoch  8, batch    58 | loss: 4.2115850Losses:  4.134162902832031 0.16825655102729797
CurrentTrain: epoch  8, batch    59 | loss: 4.1341629Losses:  4.139078617095947 0.12605060636997223
CurrentTrain: epoch  8, batch    60 | loss: 4.1390786Losses:  4.20904016494751 0.17941416800022125
CurrentTrain: epoch  8, batch    61 | loss: 4.2090402Losses:  4.0787553787231445 0.050277628004550934
CurrentTrain: epoch  8, batch    62 | loss: 4.0787554Losses:  4.104933261871338 0.12389121949672699
CurrentTrain: epoch  9, batch     0 | loss: 4.1049333Losses:  4.15855073928833 0.13447916507720947
CurrentTrain: epoch  9, batch     1 | loss: 4.1585507Losses:  4.203864574432373 0.17729729413986206
CurrentTrain: epoch  9, batch     2 | loss: 4.2038646Losses:  4.162557601928711 0.14682704210281372
CurrentTrain: epoch  9, batch     3 | loss: 4.1625576Losses:  4.120514869689941 0.13709697127342224
CurrentTrain: epoch  9, batch     4 | loss: 4.1205149Losses:  4.155228614807129 0.13241121172904968
CurrentTrain: epoch  9, batch     5 | loss: 4.1552286Losses:  4.169162273406982 0.14993730187416077
CurrentTrain: epoch  9, batch     6 | loss: 4.1691623Losses:  4.191048622131348 0.1705988347530365
CurrentTrain: epoch  9, batch     7 | loss: 4.1910486Losses:  4.089299201965332 0.09304551780223846
CurrentTrain: epoch  9, batch     8 | loss: 4.0892992Losses:  4.133020401000977 0.12228802591562271
CurrentTrain: epoch  9, batch     9 | loss: 4.1330204Losses:  4.163774013519287 0.13460147380828857
CurrentTrain: epoch  9, batch    10 | loss: 4.1637740Losses:  4.029822826385498 0.10939812660217285
CurrentTrain: epoch  9, batch    11 | loss: 4.0298228Losses:  4.0847296714782715 0.07941880077123642
CurrentTrain: epoch  9, batch    12 | loss: 4.0847297Losses:  4.154387474060059 0.13593122363090515
CurrentTrain: epoch  9, batch    13 | loss: 4.1543875Losses:  4.1577911376953125 0.14896656572818756
CurrentTrain: epoch  9, batch    14 | loss: 4.1577911Losses:  4.1643171310424805 0.1304416060447693
CurrentTrain: epoch  9, batch    15 | loss: 4.1643171Losses:  4.187103748321533 0.1549096405506134
CurrentTrain: epoch  9, batch    16 | loss: 4.1871037Losses:  4.175576210021973 0.16326802968978882
CurrentTrain: epoch  9, batch    17 | loss: 4.1755762Losses:  4.169602870941162 0.1389557123184204
CurrentTrain: epoch  9, batch    18 | loss: 4.1696029Losses:  4.1377177238464355 0.10084463655948639
CurrentTrain: epoch  9, batch    19 | loss: 4.1377177Losses:  4.127800464630127 0.11174049973487854
CurrentTrain: epoch  9, batch    20 | loss: 4.1278005Losses:  4.15224552154541 0.107313372194767
CurrentTrain: epoch  9, batch    21 | loss: 4.1522455Losses:  4.158720016479492 0.13595342636108398
CurrentTrain: epoch  9, batch    22 | loss: 4.1587200Losses:  4.151884078979492 0.11990535259246826
CurrentTrain: epoch  9, batch    23 | loss: 4.1518841Losses:  4.1946940422058105 0.19487282633781433
CurrentTrain: epoch  9, batch    24 | loss: 4.1946940Losses:  4.195371627807617 0.1443130075931549
CurrentTrain: epoch  9, batch    25 | loss: 4.1953716Losses:  4.106828212738037 0.14275017380714417
CurrentTrain: epoch  9, batch    26 | loss: 4.1068282Losses:  4.13771915435791 0.13908499479293823
CurrentTrain: epoch  9, batch    27 | loss: 4.1377192Losses:  4.132661819458008 0.14880812168121338
CurrentTrain: epoch  9, batch    28 | loss: 4.1326618Losses:  4.090885639190674 0.12046276032924652
CurrentTrain: epoch  9, batch    29 | loss: 4.0908856Losses:  4.107526779174805 0.11560525000095367
CurrentTrain: epoch  9, batch    30 | loss: 4.1075268Losses:  4.171308994293213 0.1559205949306488
CurrentTrain: epoch  9, batch    31 | loss: 4.1713090Losses:  4.180276870727539 0.14514197409152985
CurrentTrain: epoch  9, batch    32 | loss: 4.1802769Losses:  4.131137371063232 0.13085781037807465
CurrentTrain: epoch  9, batch    33 | loss: 4.1311374Losses:  4.158396244049072 0.14094388484954834
CurrentTrain: epoch  9, batch    34 | loss: 4.1583962Losses:  4.133580684661865 0.1409037709236145
CurrentTrain: epoch  9, batch    35 | loss: 4.1335807Losses:  4.1639299392700195 0.0995662659406662
CurrentTrain: epoch  9, batch    36 | loss: 4.1639299Losses:  4.185674667358398 0.16715645790100098
CurrentTrain: epoch  9, batch    37 | loss: 4.1856747Losses:  4.120992660522461 0.1295291930437088
CurrentTrain: epoch  9, batch    38 | loss: 4.1209927Losses:  4.144084930419922 0.12507519125938416
CurrentTrain: epoch  9, batch    39 | loss: 4.1440849Losses:  4.138545989990234 0.1482122838497162
CurrentTrain: epoch  9, batch    40 | loss: 4.1385460Losses:  4.150883197784424 0.1394786238670349
CurrentTrain: epoch  9, batch    41 | loss: 4.1508832Losses:  4.249117851257324 0.16852301359176636
CurrentTrain: epoch  9, batch    42 | loss: 4.2491179Losses:  4.134673118591309 0.11568467319011688
CurrentTrain: epoch  9, batch    43 | loss: 4.1346731Losses:  4.137291431427002 0.12522746622562408
CurrentTrain: epoch  9, batch    44 | loss: 4.1372914Losses:  4.1120924949646 0.11994943022727966
CurrentTrain: epoch  9, batch    45 | loss: 4.1120925Losses:  4.174272537231445 0.1686360239982605
CurrentTrain: epoch  9, batch    46 | loss: 4.1742725Losses:  4.124974727630615 0.13068866729736328
CurrentTrain: epoch  9, batch    47 | loss: 4.1249747Losses:  4.140017509460449 0.1512046903371811
CurrentTrain: epoch  9, batch    48 | loss: 4.1400175Losses:  4.1508660316467285 0.14588703215122223
CurrentTrain: epoch  9, batch    49 | loss: 4.1508660Losses:  4.173305034637451 0.105536088347435
CurrentTrain: epoch  9, batch    50 | loss: 4.1733050Losses:  4.117884635925293 0.1198756992816925
CurrentTrain: epoch  9, batch    51 | loss: 4.1178846Losses:  4.160765647888184 0.1434699296951294
CurrentTrain: epoch  9, batch    52 | loss: 4.1607656Losses:  4.151410102844238 0.12267683446407318
CurrentTrain: epoch  9, batch    53 | loss: 4.1514101Losses:  4.079935550689697 0.08144642412662506
CurrentTrain: epoch  9, batch    54 | loss: 4.0799356Losses:  4.1756181716918945 0.1290900707244873
CurrentTrain: epoch  9, batch    55 | loss: 4.1756182Losses:  4.147318363189697 0.13771292567253113
CurrentTrain: epoch  9, batch    56 | loss: 4.1473184Losses:  4.085630893707275 0.1003158837556839
CurrentTrain: epoch  9, batch    57 | loss: 4.0856309Losses:  4.150745391845703 0.1337868571281433
CurrentTrain: epoch  9, batch    58 | loss: 4.1507454Losses:  4.186452865600586 0.10248111188411713
CurrentTrain: epoch  9, batch    59 | loss: 4.1864529Losses:  4.184600353240967 0.1288323700428009
CurrentTrain: epoch  9, batch    60 | loss: 4.1846004Losses:  4.112438678741455 0.12344549596309662
CurrentTrain: epoch  9, batch    61 | loss: 4.1124387Losses:  4.058237075805664 0.054201602935791016
CurrentTrain: epoch  9, batch    62 | loss: 4.0582371
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  8.547813415527344 1.682019591331482
CurrentTrain: epoch  0, batch     0 | loss: 8.5478134Losses:  7.700161457061768 1.3009228706359863
CurrentTrain: epoch  0, batch     1 | loss: 7.7001615Losses:  7.917000770568848 1.4636991024017334
CurrentTrain: epoch  0, batch     2 | loss: 7.9170008Losses:  8.206446647644043 0.637735903263092
CurrentTrain: epoch  0, batch     3 | loss: 8.2064466Losses:  8.527141571044922 1.7901138067245483
CurrentTrain: epoch  1, batch     0 | loss: 8.5271416Losses:  7.025538921356201 1.486092448234558
CurrentTrain: epoch  1, batch     1 | loss: 7.0255389Losses:  6.0736002922058105 1.5837253332138062
CurrentTrain: epoch  1, batch     2 | loss: 6.0736003Losses:  6.499075889587402 0.44388794898986816
CurrentTrain: epoch  1, batch     3 | loss: 6.4990759Losses:  6.16290283203125 1.4617319107055664
CurrentTrain: epoch  2, batch     0 | loss: 6.1629028Losses:  6.07546329498291 1.4606903791427612
CurrentTrain: epoch  2, batch     1 | loss: 6.0754633Losses:  6.161786079406738 1.5241913795471191
CurrentTrain: epoch  2, batch     2 | loss: 6.1617861Losses:  3.002133846282959 0.6262480020523071
CurrentTrain: epoch  2, batch     3 | loss: 3.0021338Losses:  6.1375298500061035 1.401486873626709
CurrentTrain: epoch  3, batch     0 | loss: 6.1375299Losses:  5.212461948394775 1.3051358461380005
CurrentTrain: epoch  3, batch     1 | loss: 5.2124619Losses:  4.59464168548584 1.2795984745025635
CurrentTrain: epoch  3, batch     2 | loss: 4.5946417Losses:  4.4424638748168945 0.34324124455451965
CurrentTrain: epoch  3, batch     3 | loss: 4.4424639Losses:  4.314548015594482 1.3326226472854614
CurrentTrain: epoch  4, batch     0 | loss: 4.3145480Losses:  5.136375427246094 1.5091588497161865
CurrentTrain: epoch  4, batch     1 | loss: 5.1363754Losses:  4.800193786621094 1.1982537508010864
CurrentTrain: epoch  4, batch     2 | loss: 4.8001938Losses:  4.098855018615723 0.5199515223503113
CurrentTrain: epoch  4, batch     3 | loss: 4.0988550Losses:  4.579079627990723 1.272949457168579
CurrentTrain: epoch  5, batch     0 | loss: 4.5790796Losses:  3.9731030464172363 1.2153533697128296
CurrentTrain: epoch  5, batch     1 | loss: 3.9731030Losses:  4.468906402587891 1.3240289688110352
CurrentTrain: epoch  5, batch     2 | loss: 4.4689064Losses:  2.968179225921631 0.25750279426574707
CurrentTrain: epoch  5, batch     3 | loss: 2.9681792Losses:  4.078949928283691 1.3777520656585693
CurrentTrain: epoch  6, batch     0 | loss: 4.0789499Losses:  3.8364768028259277 1.219441294670105
CurrentTrain: epoch  6, batch     1 | loss: 3.8364768Losses:  4.276376724243164 1.1888561248779297
CurrentTrain: epoch  6, batch     2 | loss: 4.2763767Losses:  3.2453269958496094 0.1834409385919571
CurrentTrain: epoch  6, batch     3 | loss: 3.2453270Losses:  3.8190155029296875 1.2387280464172363
CurrentTrain: epoch  7, batch     0 | loss: 3.8190155Losses:  3.9507737159729004 1.0538558959960938
CurrentTrain: epoch  7, batch     1 | loss: 3.9507737Losses:  3.5156359672546387 0.9806762933731079
CurrentTrain: epoch  7, batch     2 | loss: 3.5156360Losses:  2.664736270904541 0.41534602642059326
CurrentTrain: epoch  7, batch     3 | loss: 2.6647363Losses:  3.918022632598877 1.3012504577636719
CurrentTrain: epoch  8, batch     0 | loss: 3.9180226Losses:  3.820861339569092 1.1910299062728882
CurrentTrain: epoch  8, batch     1 | loss: 3.8208613Losses:  3.220150947570801 0.9823881387710571
CurrentTrain: epoch  8, batch     2 | loss: 3.2201509Losses:  2.305241584777832 0.047986049205064774
CurrentTrain: epoch  8, batch     3 | loss: 2.3052416Losses:  3.5278546810150146 0.988694429397583
CurrentTrain: epoch  9, batch     0 | loss: 3.5278547Losses:  3.3550209999084473 1.1652361154556274
CurrentTrain: epoch  9, batch     1 | loss: 3.3550210Losses:  3.8121886253356934 1.2439812421798706
CurrentTrain: epoch  9, batch     2 | loss: 3.8121886Losses:  2.4379231929779053 0.25412705540657043
CurrentTrain: epoch  9, batch     3 | loss: 2.4379232
Losses:  2.9622232913970947 0.9856412410736084
MemoryTrain:  epoch  0, batch     0 | loss: 2.9622233Losses:  0.754455029964447 0.4307466149330139
MemoryTrain:  epoch  0, batch     1 | loss: 0.7544550Losses:  2.6804721355438232 1.017029047012329
MemoryTrain:  epoch  1, batch     0 | loss: 2.6804721Losses:  0.340488463640213 0.21709701418876648
MemoryTrain:  epoch  1, batch     1 | loss: 0.3404885Losses:  1.9053304195404053 1.0000441074371338
MemoryTrain:  epoch  2, batch     0 | loss: 1.9053304Losses:  2.070413589477539 0.08719296753406525
MemoryTrain:  epoch  2, batch     1 | loss: 2.0704136Losses:  1.7000309228897095 0.7844982147216797
MemoryTrain:  epoch  3, batch     0 | loss: 1.7000309Losses:  1.1320712566375732 0.33080315589904785
MemoryTrain:  epoch  3, batch     1 | loss: 1.1320713Losses:  1.6830121278762817 0.9474884271621704
MemoryTrain:  epoch  4, batch     0 | loss: 1.6830121Losses:  0.5283306837081909 0.24112090468406677
MemoryTrain:  epoch  4, batch     1 | loss: 0.5283307Losses:  1.266744613647461 0.8058272004127502
MemoryTrain:  epoch  5, batch     0 | loss: 1.2667446Losses:  1.3283424377441406 0.3370242714881897
MemoryTrain:  epoch  5, batch     1 | loss: 1.3283424Losses:  1.300130844116211 0.806611180305481
MemoryTrain:  epoch  6, batch     0 | loss: 1.3001308Losses:  0.6953353881835938 0.5319148302078247
MemoryTrain:  epoch  6, batch     1 | loss: 0.6953354Losses:  1.3371707201004028 0.9507189989089966
MemoryTrain:  epoch  7, batch     0 | loss: 1.3371707Losses:  0.1493862271308899 0.060462743043899536
MemoryTrain:  epoch  7, batch     1 | loss: 0.1493862Losses:  1.067447304725647 0.8544484972953796
MemoryTrain:  epoch  8, batch     0 | loss: 1.0674473Losses:  0.5081385374069214 0.46126124262809753
MemoryTrain:  epoch  8, batch     1 | loss: 0.5081385Losses:  1.075194239616394 0.8500359058380127
MemoryTrain:  epoch  9, batch     0 | loss: 1.0751942Losses:  0.4232838451862335 0.3489474654197693
MemoryTrain:  epoch  9, batch     1 | loss: 0.4232838
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 66.84%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.84%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.92%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 93.92%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.83%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 93.59%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 93.27%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 93.04%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.73%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 92.53%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 92.17%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 91.89%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 91.69%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 91.50%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 91.09%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.98%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.80%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 90.42%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 90.32%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.15%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 89.58%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.03%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 88.55%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.09%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.76%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.24%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.93%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 86.01%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.72%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 85.19%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.70%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.05%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 83.56%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.03%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 82.56%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.09%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.20%   
cur_acc:  ['0.9484', '0.7212']
his_acc:  ['0.9484', '0.8320']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  7.175018310546875 1.3736865520477295
CurrentTrain: epoch  0, batch     0 | loss: 7.1750183Losses:  8.636763572692871 1.4386131763458252
CurrentTrain: epoch  0, batch     1 | loss: 8.6367636Losses:  7.482740879058838 1.475430965423584
CurrentTrain: epoch  0, batch     2 | loss: 7.4827409Losses:  9.052133560180664 0.6096633672714233
CurrentTrain: epoch  0, batch     3 | loss: 9.0521336Losses:  8.028536796569824 1.687813639640808
CurrentTrain: epoch  1, batch     0 | loss: 8.0285368Losses:  6.190436840057373 1.3646926879882812
CurrentTrain: epoch  1, batch     1 | loss: 6.1904368Losses:  7.951556205749512 1.5131378173828125
CurrentTrain: epoch  1, batch     2 | loss: 7.9515562Losses:  6.778902053833008 0.41427984833717346
CurrentTrain: epoch  1, batch     3 | loss: 6.7789021Losses:  6.181257247924805 1.0719069242477417
CurrentTrain: epoch  2, batch     0 | loss: 6.1812572Losses:  6.611659526824951 1.3187003135681152
CurrentTrain: epoch  2, batch     1 | loss: 6.6116595Losses:  6.648287296295166 1.4721899032592773
CurrentTrain: epoch  2, batch     2 | loss: 6.6482873Losses:  6.326396465301514 0.5489392876625061
CurrentTrain: epoch  2, batch     3 | loss: 6.3263965Losses:  5.712215423583984 1.263843297958374
CurrentTrain: epoch  3, batch     0 | loss: 5.7122154Losses:  5.632116317749023 1.247787594795227
CurrentTrain: epoch  3, batch     1 | loss: 5.6321163Losses:  6.777054309844971 1.4155155420303345
CurrentTrain: epoch  3, batch     2 | loss: 6.7770543Losses:  5.732586860656738 0.27600571513175964
CurrentTrain: epoch  3, batch     3 | loss: 5.7325869Losses:  5.758131980895996 1.3790473937988281
CurrentTrain: epoch  4, batch     0 | loss: 5.7581320Losses:  5.357241630554199 1.3322336673736572
CurrentTrain: epoch  4, batch     1 | loss: 5.3572416Losses:  6.546644687652588 1.4441561698913574
CurrentTrain: epoch  4, batch     2 | loss: 6.5466447Losses:  2.926689386367798 0.2780163586139679
CurrentTrain: epoch  4, batch     3 | loss: 2.9266894Losses:  5.677116394042969 1.3606579303741455
CurrentTrain: epoch  5, batch     0 | loss: 5.6771164Losses:  4.845891952514648 1.2420976161956787
CurrentTrain: epoch  5, batch     1 | loss: 4.8458920Losses:  5.752013206481934 1.3667200803756714
CurrentTrain: epoch  5, batch     2 | loss: 5.7520132Losses:  4.337472915649414 0.1830560564994812
CurrentTrain: epoch  5, batch     3 | loss: 4.3374729Losses:  4.9552202224731445 1.3508679866790771
CurrentTrain: epoch  6, batch     0 | loss: 4.9552202Losses:  5.590577125549316 1.242774248123169
CurrentTrain: epoch  6, batch     1 | loss: 5.5905771Losses:  4.849518775939941 1.1157546043395996
CurrentTrain: epoch  6, batch     2 | loss: 4.8495188Losses:  4.949337482452393 0.34221750497817993
CurrentTrain: epoch  6, batch     3 | loss: 4.9493375Losses:  4.25814962387085 1.126877784729004
CurrentTrain: epoch  7, batch     0 | loss: 4.2581496Losses:  5.593587398529053 1.3299168348312378
CurrentTrain: epoch  7, batch     1 | loss: 5.5935874Losses:  5.022134304046631 1.233799695968628
CurrentTrain: epoch  7, batch     2 | loss: 5.0221343Losses:  4.724396705627441 0.09438011795282364
CurrentTrain: epoch  7, batch     3 | loss: 4.7243967Losses:  4.232565879821777 1.1033574342727661
CurrentTrain: epoch  8, batch     0 | loss: 4.2325659Losses:  5.352015495300293 1.2741079330444336
CurrentTrain: epoch  8, batch     1 | loss: 5.3520155Losses:  3.744133472442627 0.7853513956069946
CurrentTrain: epoch  8, batch     2 | loss: 3.7441335Losses:  4.399091720581055 0.41456663608551025
CurrentTrain: epoch  8, batch     3 | loss: 4.3990917Losses:  4.256439685821533 1.136560082435608
CurrentTrain: epoch  9, batch     0 | loss: 4.2564397Losses:  4.186890602111816 1.1466293334960938
CurrentTrain: epoch  9, batch     1 | loss: 4.1868906Losses:  5.142430305480957 1.2700304985046387
CurrentTrain: epoch  9, batch     2 | loss: 5.1424303Losses:  2.1940813064575195 0.1482817828655243
CurrentTrain: epoch  9, batch     3 | loss: 2.1940813
Losses:  1.7225706577301025 1.126110553741455
MemoryTrain:  epoch  0, batch     0 | loss: 1.7225707Losses:  2.6811325550079346 1.2350802421569824
MemoryTrain:  epoch  0, batch     1 | loss: 2.6811326Losses:  1.9163874387741089 0.9563243389129639
MemoryTrain:  epoch  1, batch     0 | loss: 1.9163874Losses:  2.34256649017334 1.1764072179794312
MemoryTrain:  epoch  1, batch     1 | loss: 2.3425665Losses:  2.055757522583008 1.0641064643859863
MemoryTrain:  epoch  2, batch     0 | loss: 2.0557575Losses:  1.2993550300598145 1.0299638509750366
MemoryTrain:  epoch  2, batch     1 | loss: 1.2993550Losses:  1.4139536619186401 1.0447313785552979
MemoryTrain:  epoch  3, batch     0 | loss: 1.4139537Losses:  1.349663257598877 0.9640755653381348
MemoryTrain:  epoch  3, batch     1 | loss: 1.3496633Losses:  1.316479206085205 1.0670647621154785
MemoryTrain:  epoch  4, batch     0 | loss: 1.3164792Losses:  1.075173258781433 0.9056072235107422
MemoryTrain:  epoch  4, batch     1 | loss: 1.0751733Losses:  1.191701054573059 1.10057532787323
MemoryTrain:  epoch  5, batch     0 | loss: 1.1917011Losses:  0.9449688792228699 0.8028806447982788
MemoryTrain:  epoch  5, batch     1 | loss: 0.9449689Losses:  1.0732659101486206 0.9820520281791687
MemoryTrain:  epoch  6, batch     0 | loss: 1.0732659Losses:  0.9221382141113281 0.8496817946434021
MemoryTrain:  epoch  6, batch     1 | loss: 0.9221382Losses:  1.201716423034668 1.121647834777832
MemoryTrain:  epoch  7, batch     0 | loss: 1.2017164Losses:  0.7440943121910095 0.6926062107086182
MemoryTrain:  epoch  7, batch     1 | loss: 0.7440943Losses:  0.8776413202285767 0.8248249292373657
MemoryTrain:  epoch  8, batch     0 | loss: 0.8776413Losses:  1.0430328845977783 0.9829760193824768
MemoryTrain:  epoch  8, batch     1 | loss: 1.0430329Losses:  0.9390271306037903 0.8968177437782288
MemoryTrain:  epoch  9, batch     0 | loss: 0.9390271Losses:  0.9859955906867981 0.9206247925758362
MemoryTrain:  epoch  9, batch     1 | loss: 0.9859956
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 59.88%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 61.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 64.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 70.46%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.42%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.44%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 92.28%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.47%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.43%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 92.21%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 91.91%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.69%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 91.16%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 90.59%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 89.96%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 89.24%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 88.43%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 88.07%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 87.78%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 87.43%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 87.29%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 87.16%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 86.49%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.97%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 85.46%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 85.03%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 84.73%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 84.25%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 83.96%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 83.11%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 82.84%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 82.52%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 82.27%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.96%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.54%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.96%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.39%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 79.28%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.91%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.71%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.34%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 80.26%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 79.97%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 79.74%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 79.66%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.43%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 80.04%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.69%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 79.26%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 78.87%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 78.39%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.07%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 78.64%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 78.17%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.70%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 77.27%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 76.77%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 76.32%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 77.62%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 77.57%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 77.51%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 77.49%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 77.37%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 77.13%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 77.05%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.05%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.13%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 76.89%   
cur_acc:  ['0.9484', '0.7212', '0.6994']
his_acc:  ['0.9484', '0.8320', '0.7689']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  6.852117538452148 1.5039513111114502
CurrentTrain: epoch  0, batch     0 | loss: 6.8521175Losses:  6.525766372680664 1.4497723579406738
CurrentTrain: epoch  0, batch     1 | loss: 6.5257664Losses:  5.597897529602051 1.2826709747314453
CurrentTrain: epoch  0, batch     2 | loss: 5.5978975Losses:  3.942474842071533 0.2980656027793884
CurrentTrain: epoch  0, batch     3 | loss: 3.9424748Losses:  5.695521354675293 1.3712208271026611
CurrentTrain: epoch  1, batch     0 | loss: 5.6955214Losses:  5.270450592041016 1.1182546615600586
CurrentTrain: epoch  1, batch     1 | loss: 5.2704506Losses:  4.409605026245117 1.3223004341125488
CurrentTrain: epoch  1, batch     2 | loss: 4.4096050Losses:  3.8116841316223145 0.3850194215774536
CurrentTrain: epoch  1, batch     3 | loss: 3.8116841Losses:  5.2817182540893555 1.3686103820800781
CurrentTrain: epoch  2, batch     0 | loss: 5.2817183Losses:  4.677116394042969 1.1630938053131104
CurrentTrain: epoch  2, batch     1 | loss: 4.6771164Losses:  3.8030667304992676 1.1616344451904297
CurrentTrain: epoch  2, batch     2 | loss: 3.8030667Losses:  1.734633445739746 0.0
CurrentTrain: epoch  2, batch     3 | loss: 1.7346334Losses:  4.752889633178711 1.2289820909500122
CurrentTrain: epoch  3, batch     0 | loss: 4.7528896Losses:  4.281060695648193 1.1298270225524902
CurrentTrain: epoch  3, batch     1 | loss: 4.2810607Losses:  3.572613000869751 0.8658692836761475
CurrentTrain: epoch  3, batch     2 | loss: 3.5726130Losses:  2.348694324493408 0.1801805943250656
CurrentTrain: epoch  3, batch     3 | loss: 2.3486943Losses:  3.901953935623169 1.03147292137146
CurrentTrain: epoch  4, batch     0 | loss: 3.9019539Losses:  4.214547634124756 1.1614928245544434
CurrentTrain: epoch  4, batch     1 | loss: 4.2145476Losses:  4.288885593414307 1.2316269874572754
CurrentTrain: epoch  4, batch     2 | loss: 4.2888856Losses:  2.03521466255188 0.11405767500400543
CurrentTrain: epoch  4, batch     3 | loss: 2.0352147Losses:  3.3417184352874756 1.016526460647583
CurrentTrain: epoch  5, batch     0 | loss: 3.3417184Losses:  3.8628194332122803 1.0715715885162354
CurrentTrain: epoch  5, batch     1 | loss: 3.8628194Losses:  4.214678764343262 1.037235140800476
CurrentTrain: epoch  5, batch     2 | loss: 4.2146788Losses:  2.5367372035980225 0.19143980741500854
CurrentTrain: epoch  5, batch     3 | loss: 2.5367372Losses:  3.6001644134521484 1.1094441413879395
CurrentTrain: epoch  6, batch     0 | loss: 3.6001644Losses:  3.579108238220215 0.9624204039573669
CurrentTrain: epoch  6, batch     1 | loss: 3.5791082Losses:  3.709932327270508 0.9855773448944092
CurrentTrain: epoch  6, batch     2 | loss: 3.7099323Losses:  1.8098760843276978 0.08483884483575821
CurrentTrain: epoch  6, batch     3 | loss: 1.8098761Losses:  3.407923936843872 0.9476725459098816
CurrentTrain: epoch  7, batch     0 | loss: 3.4079239Losses:  3.659715175628662 1.0794535875320435
CurrentTrain: epoch  7, batch     1 | loss: 3.6597152Losses:  3.1078171730041504 0.8949733376502991
CurrentTrain: epoch  7, batch     2 | loss: 3.1078172Losses:  1.9069628715515137 0.11485534906387329
CurrentTrain: epoch  7, batch     3 | loss: 1.9069629Losses:  3.3397274017333984 0.8582558631896973
CurrentTrain: epoch  8, batch     0 | loss: 3.3397274Losses:  3.467780590057373 1.0328444242477417
CurrentTrain: epoch  8, batch     1 | loss: 3.4677806Losses:  2.9884586334228516 0.9016246795654297
CurrentTrain: epoch  8, batch     2 | loss: 2.9884586Losses:  2.1808176040649414 0.13185730576515198
CurrentTrain: epoch  8, batch     3 | loss: 2.1808176Losses:  3.130704641342163 0.9074665904045105
CurrentTrain: epoch  9, batch     0 | loss: 3.1307046Losses:  2.9678385257720947 0.885615348815918
CurrentTrain: epoch  9, batch     1 | loss: 2.9678385Losses:  2.8559467792510986 0.8434770703315735
CurrentTrain: epoch  9, batch     2 | loss: 2.8559468Losses:  1.7850626707077026 0.07265456765890121
CurrentTrain: epoch  9, batch     3 | loss: 1.7850627
Losses:  1.681627869606018 0.9755958318710327
MemoryTrain:  epoch  0, batch     0 | loss: 1.6816279Losses:  1.5734305381774902 1.0896623134613037
MemoryTrain:  epoch  0, batch     1 | loss: 1.5734305Losses:  2.12253475189209 0.824458658695221
MemoryTrain:  epoch  0, batch     2 | loss: 2.1225348Losses:  1.6196948289871216 1.1004271507263184
MemoryTrain:  epoch  1, batch     0 | loss: 1.6196948Losses:  2.13057017326355 1.0695115327835083
MemoryTrain:  epoch  1, batch     1 | loss: 2.1305702Losses:  0.9952726364135742 0.40948665142059326
MemoryTrain:  epoch  1, batch     2 | loss: 0.9952726Losses:  1.4453164339065552 0.9337819218635559
MemoryTrain:  epoch  2, batch     0 | loss: 1.4453164Losses:  1.2596752643585205 0.8793407678604126
MemoryTrain:  epoch  2, batch     1 | loss: 1.2596753Losses:  1.4757448434829712 0.7768915295600891
MemoryTrain:  epoch  2, batch     2 | loss: 1.4757448Losses:  0.8011102080345154 0.7095372080802917
MemoryTrain:  epoch  3, batch     0 | loss: 0.8011102Losses:  1.1927273273468018 1.084643840789795
MemoryTrain:  epoch  3, batch     1 | loss: 1.1927273Losses:  2.097860336303711 0.8409458994865417
MemoryTrain:  epoch  3, batch     2 | loss: 2.0978603Losses:  1.3346487283706665 1.09199059009552
MemoryTrain:  epoch  4, batch     0 | loss: 1.3346487Losses:  0.9397233724594116 0.7965763807296753
MemoryTrain:  epoch  4, batch     1 | loss: 0.9397234Losses:  0.6771336793899536 0.6435767412185669
MemoryTrain:  epoch  4, batch     2 | loss: 0.6771337Losses:  1.0381790399551392 0.9621003270149231
MemoryTrain:  epoch  5, batch     0 | loss: 1.0381790Losses:  1.2469840049743652 1.0051451921463013
MemoryTrain:  epoch  5, batch     1 | loss: 1.2469840Losses:  0.5216382145881653 0.48243388533592224
MemoryTrain:  epoch  5, batch     2 | loss: 0.5216382Losses:  1.0194401741027832 0.9494462013244629
MemoryTrain:  epoch  6, batch     0 | loss: 1.0194402Losses:  1.1366559267044067 1.0339088439941406
MemoryTrain:  epoch  6, batch     1 | loss: 1.1366559Losses:  0.3833981454372406 0.3559163808822632
MemoryTrain:  epoch  6, batch     2 | loss: 0.3833981Losses:  0.8672139644622803 0.823121964931488
MemoryTrain:  epoch  7, batch     0 | loss: 0.8672140Losses:  0.9256094694137573 0.8516156673431396
MemoryTrain:  epoch  7, batch     1 | loss: 0.9256095Losses:  0.9452529549598694 0.8688716292381287
MemoryTrain:  epoch  7, batch     2 | loss: 0.9452530Losses:  0.7525755763053894 0.7158856391906738
MemoryTrain:  epoch  8, batch     0 | loss: 0.7525756Losses:  1.0907124280929565 1.0216789245605469
MemoryTrain:  epoch  8, batch     1 | loss: 1.0907124Losses:  0.5960792303085327 0.5498980283737183
MemoryTrain:  epoch  8, batch     2 | loss: 0.5960792Losses:  0.8770521879196167 0.8330124020576477
MemoryTrain:  epoch  9, batch     0 | loss: 0.8770522Losses:  0.9502399563789368 0.9206361770629883
MemoryTrain:  epoch  9, batch     1 | loss: 0.9502400Losses:  0.4369223117828369 0.41126155853271484
MemoryTrain:  epoch  9, batch     2 | loss: 0.4369223
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.25%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.87%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.26%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.12%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 90.78%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.68%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.58%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 90.43%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.58%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 90.53%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 90.49%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 90.35%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.71%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 90.34%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.03%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 89.48%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 88.48%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 87.43%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 86.69%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 85.76%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 84.30%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 83.99%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 83.61%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 83.24%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.88%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.19%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 81.72%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 81.12%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 80.53%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 79.59%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 79.23%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 78.69%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 78.34%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 77.94%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 77.61%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 77.20%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 77.00%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.16%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 75.57%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 74.94%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 74.49%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 74.16%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 74.00%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 76.15%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 75.60%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 73.85%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 73.63%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 74.19%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 73.93%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.54%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 73.15%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 72.99%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 72.79%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.82%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 72.34%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 71.96%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.49%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 71.72%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 71.73%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 71.51%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 71.42%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 71.46%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 72.58%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 72.49%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.41%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 72.19%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 71.83%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 71.64%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.48%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 71.99%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 72.04%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 72.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.42%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 73.43%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 73.56%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.64%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.17%   
cur_acc:  ['0.9484', '0.7212', '0.6994', '0.8026']
his_acc:  ['0.9484', '0.8320', '0.7689', '0.7418']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  7.498282432556152 1.3248472213745117
CurrentTrain: epoch  0, batch     0 | loss: 7.4982824Losses:  7.512566089630127 1.4293885231018066
CurrentTrain: epoch  0, batch     1 | loss: 7.5125661Losses:  7.370110988616943 1.395140290260315
CurrentTrain: epoch  0, batch     2 | loss: 7.3701110Losses:  6.276055335998535 0.07686883211135864
CurrentTrain: epoch  0, batch     3 | loss: 6.2760553Losses:  5.936135292053223 1.0913629531860352
CurrentTrain: epoch  1, batch     0 | loss: 5.9361353Losses:  6.601964473724365 1.4872404336929321
CurrentTrain: epoch  1, batch     1 | loss: 6.6019645Losses:  6.732372283935547 1.2245080471038818
CurrentTrain: epoch  1, batch     2 | loss: 6.7323723Losses:  3.663017749786377 0.29358676075935364
CurrentTrain: epoch  1, batch     3 | loss: 3.6630177Losses:  6.8259477615356445 1.271965742111206
CurrentTrain: epoch  2, batch     0 | loss: 6.8259478Losses:  5.508125305175781 1.2816047668457031
CurrentTrain: epoch  2, batch     1 | loss: 5.5081253Losses:  4.623405933380127 1.1083601713180542
CurrentTrain: epoch  2, batch     2 | loss: 4.6234059Losses:  4.7378716468811035 0.20780549943447113
CurrentTrain: epoch  2, batch     3 | loss: 4.7378716Losses:  6.029974937438965 1.1215211153030396
CurrentTrain: epoch  3, batch     0 | loss: 6.0299749Losses:  4.798362731933594 1.237226128578186
CurrentTrain: epoch  3, batch     1 | loss: 4.7983627Losses:  5.493661880493164 1.2928588390350342
CurrentTrain: epoch  3, batch     2 | loss: 5.4936619Losses:  2.9289445877075195 0.2954829931259155
CurrentTrain: epoch  3, batch     3 | loss: 2.9289446Losses:  5.4979023933410645 0.9651256799697876
CurrentTrain: epoch  4, batch     0 | loss: 5.4979024Losses:  5.136496543884277 1.2571995258331299
CurrentTrain: epoch  4, batch     1 | loss: 5.1364965Losses:  4.688480377197266 1.0876579284667969
CurrentTrain: epoch  4, batch     2 | loss: 4.6884804Losses:  3.4466850757598877 0.09183569252490997
CurrentTrain: epoch  4, batch     3 | loss: 3.4466851Losses:  5.785871505737305 1.1628057956695557
CurrentTrain: epoch  5, batch     0 | loss: 5.7858715Losses:  4.410638332366943 1.028885841369629
CurrentTrain: epoch  5, batch     1 | loss: 4.4106383Losses:  4.826006889343262 1.1840684413909912
CurrentTrain: epoch  5, batch     2 | loss: 4.8260069Losses:  2.4725801944732666 0.15332818031311035
CurrentTrain: epoch  5, batch     3 | loss: 2.4725802Losses:  5.232939720153809 1.0588667392730713
CurrentTrain: epoch  6, batch     0 | loss: 5.2329397Losses:  3.8143794536590576 0.9268760681152344
CurrentTrain: epoch  6, batch     1 | loss: 3.8143795Losses:  4.611250400543213 1.0610474348068237
CurrentTrain: epoch  6, batch     2 | loss: 4.6112504Losses:  2.1838345527648926 0.18087945878505707
CurrentTrain: epoch  6, batch     3 | loss: 2.1838346Losses:  4.19868278503418 1.0784714221954346
CurrentTrain: epoch  7, batch     0 | loss: 4.1986828Losses:  4.353846549987793 0.8739960193634033
CurrentTrain: epoch  7, batch     1 | loss: 4.3538465Losses:  4.332399368286133 1.1712820529937744
CurrentTrain: epoch  7, batch     2 | loss: 4.3323994Losses:  4.6512627601623535 0.42787420749664307
CurrentTrain: epoch  7, batch     3 | loss: 4.6512628Losses:  4.380468845367432 0.9611252546310425
CurrentTrain: epoch  8, batch     0 | loss: 4.3804688Losses:  4.0346832275390625 1.0351028442382812
CurrentTrain: epoch  8, batch     1 | loss: 4.0346832Losses:  3.945373058319092 0.8879176378250122
CurrentTrain: epoch  8, batch     2 | loss: 3.9453731Losses:  2.5761520862579346 0.19460958242416382
CurrentTrain: epoch  8, batch     3 | loss: 2.5761521Losses:  3.792884588241577 1.0288302898406982
CurrentTrain: epoch  9, batch     0 | loss: 3.7928846Losses:  4.274397850036621 0.9203650951385498
CurrentTrain: epoch  9, batch     1 | loss: 4.2743979Losses:  4.052125930786133 1.0642738342285156
CurrentTrain: epoch  9, batch     2 | loss: 4.0521259Losses:  2.6215579509735107 0.44962793588638306
CurrentTrain: epoch  9, batch     3 | loss: 2.6215580
Losses:  2.2427797317504883 1.0518999099731445
MemoryTrain:  epoch  0, batch     0 | loss: 2.2427797Losses:  1.3653504848480225 1.0077534914016724
MemoryTrain:  epoch  0, batch     1 | loss: 1.3653505Losses:  1.2647165060043335 0.9859395027160645
MemoryTrain:  epoch  0, batch     2 | loss: 1.2647165Losses:  1.813063621520996 0.10436229407787323
MemoryTrain:  epoch  0, batch     3 | loss: 1.8130636Losses:  1.828449010848999 1.0721523761749268
MemoryTrain:  epoch  1, batch     0 | loss: 1.8284490Losses:  1.5002580881118774 0.7898414134979248
MemoryTrain:  epoch  1, batch     1 | loss: 1.5002581Losses:  1.6727144718170166 1.1023318767547607
MemoryTrain:  epoch  1, batch     2 | loss: 1.6727145Losses:  1.4808050394058228 0.1217593103647232
MemoryTrain:  epoch  1, batch     3 | loss: 1.4808050Losses:  1.4083702564239502 1.0039933919906616
MemoryTrain:  epoch  2, batch     0 | loss: 1.4083703Losses:  1.3674652576446533 1.018069863319397
MemoryTrain:  epoch  2, batch     1 | loss: 1.3674653Losses:  1.1974997520446777 0.8247029781341553
MemoryTrain:  epoch  2, batch     2 | loss: 1.1974998Losses:  0.4545850455760956 0.2801629602909088
MemoryTrain:  epoch  2, batch     3 | loss: 0.4545850Losses:  1.453932285308838 1.0657448768615723
MemoryTrain:  epoch  3, batch     0 | loss: 1.4539323Losses:  1.0485641956329346 0.7924085855484009
MemoryTrain:  epoch  3, batch     1 | loss: 1.0485642Losses:  1.167344093322754 1.0614038705825806
MemoryTrain:  epoch  3, batch     2 | loss: 1.1673441Losses:  0.11356698721647263 0.09325249493122101
MemoryTrain:  epoch  3, batch     3 | loss: 0.1135670Losses:  1.3821688890457153 1.1168253421783447
MemoryTrain:  epoch  4, batch     0 | loss: 1.3821689Losses:  0.6466147899627686 0.610171377658844
MemoryTrain:  epoch  4, batch     1 | loss: 0.6466148Losses:  1.2798875570297241 1.0522792339324951
MemoryTrain:  epoch  4, batch     2 | loss: 1.2798876Losses:  0.20773032307624817 0.17261236906051636
MemoryTrain:  epoch  4, batch     3 | loss: 0.2077303Losses:  1.087769865989685 0.9800696969032288
MemoryTrain:  epoch  5, batch     0 | loss: 1.0877699Losses:  0.9842616319656372 0.8800023794174194
MemoryTrain:  epoch  5, batch     1 | loss: 0.9842616Losses:  0.8705670833587646 0.7982757091522217
MemoryTrain:  epoch  5, batch     2 | loss: 0.8705671Losses:  0.12040739506483078 0.052304111421108246
MemoryTrain:  epoch  5, batch     3 | loss: 0.1204074Losses:  0.9712254405021667 0.8923132419586182
MemoryTrain:  epoch  6, batch     0 | loss: 0.9712254Losses:  0.8391438722610474 0.7673301696777344
MemoryTrain:  epoch  6, batch     1 | loss: 0.8391439Losses:  0.9987738728523254 0.9089483022689819
MemoryTrain:  epoch  6, batch     2 | loss: 0.9987739Losses:  0.13154509663581848 0.0828818678855896
MemoryTrain:  epoch  6, batch     3 | loss: 0.1315451Losses:  0.8721882104873657 0.8211830854415894
MemoryTrain:  epoch  7, batch     0 | loss: 0.8721882Losses:  1.0270267724990845 0.9680237770080566
MemoryTrain:  epoch  7, batch     1 | loss: 1.0270268Losses:  1.097150206565857 1.0251047611236572
MemoryTrain:  epoch  7, batch     2 | loss: 1.0971502Losses:  0.03762261942028999 0.007365661673247814
MemoryTrain:  epoch  7, batch     3 | loss: 0.0376226Losses:  0.9361490607261658 0.8970483541488647
MemoryTrain:  epoch  8, batch     0 | loss: 0.9361491Losses:  0.9688972234725952 0.9233850240707397
MemoryTrain:  epoch  8, batch     1 | loss: 0.9688972Losses:  0.8161152601242065 0.7747328877449036
MemoryTrain:  epoch  8, batch     2 | loss: 0.8161153Losses:  0.0808395966887474 0.03236379474401474
MemoryTrain:  epoch  8, batch     3 | loss: 0.0808396Losses:  0.8860752582550049 0.8326019048690796
MemoryTrain:  epoch  9, batch     0 | loss: 0.8860753Losses:  0.8254327774047852 0.7909973859786987
MemoryTrain:  epoch  9, batch     1 | loss: 0.8254328Losses:  0.9001904129981995 0.8636826276779175
MemoryTrain:  epoch  9, batch     2 | loss: 0.9001904Losses:  0.23947873711585999 0.18990401923656464
MemoryTrain:  epoch  9, batch     3 | loss: 0.2394787
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 61.04%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 60.28%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 58.82%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 58.21%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 55.59%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 55.93%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 56.41%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 57.85%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 58.38%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 58.19%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 58.02%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 58.51%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 59.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 66.87%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.12%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.88%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.45%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 88.43%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.57%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 88.37%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.36%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.34%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 87.58%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 87.42%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 87.03%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 86.28%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 85.24%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 84.23%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 83.24%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 82.27%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 81.39%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 80.89%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 80.55%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 80.07%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 79.67%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 79.35%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 78.70%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 78.19%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 77.57%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 76.28%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 76.01%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 75.50%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 75.06%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 74.69%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 74.33%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 74.10%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 73.93%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 73.70%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.42%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.92%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 72.36%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.82%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 71.40%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.96%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 72.82%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 72.24%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 71.66%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 71.20%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 70.94%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 70.98%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 71.42%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.04%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 70.76%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 70.07%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 69.75%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 70.36%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.90%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.44%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.99%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.55%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.11%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 69.08%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 68.93%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 68.71%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 68.72%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.91%   #############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  11.4970121383667 1.5480644702911377
CurrentTrain: epoch  0, batch     0 | loss: 11.4970121Losses:  10.592986106872559 1.042342185974121
CurrentTrain: epoch  0, batch     1 | loss: 10.5929861Losses:  11.08990478515625 1.3515405654907227
CurrentTrain: epoch  0, batch     2 | loss: 11.0899048Losses:  10.14352035522461 1.1056256294250488
CurrentTrain: epoch  0, batch     3 | loss: 10.1435204Losses:  11.330062866210938 1.4982435703277588
CurrentTrain: epoch  0, batch     4 | loss: 11.3300629Losses:  9.745765686035156 1.3925495147705078
CurrentTrain: epoch  0, batch     5 | loss: 9.7457657Losses:  10.214280128479004 1.1385623216629028
CurrentTrain: epoch  0, batch     6 | loss: 10.2142801Losses:  11.15280532836914 1.5136313438415527
CurrentTrain: epoch  0, batch     7 | loss: 11.1528053Losses:  11.095279693603516 1.197519063949585
CurrentTrain: epoch  0, batch     8 | loss: 11.0952797Losses:  9.880900382995605 1.4063373804092407
CurrentTrain: epoch  0, batch     9 | loss: 9.8809004Losses:  9.612981796264648 1.2502363920211792
CurrentTrain: epoch  0, batch    10 | loss: 9.6129818Losses:  11.165337562561035 1.4798228740692139
CurrentTrain: epoch  0, batch    11 | loss: 11.1653376Losses:  10.974129676818848 1.3687909841537476
CurrentTrain: epoch  0, batch    12 | loss: 10.9741297Losses:  11.070302963256836 1.6165127754211426
CurrentTrain: epoch  0, batch    13 | loss: 11.0703030Losses:  10.564462661743164 1.3110613822937012
CurrentTrain: epoch  0, batch    14 | loss: 10.5644627Losses:  9.427331924438477 1.0584628582000732
CurrentTrain: epoch  0, batch    15 | loss: 9.4273319Losses:  9.478541374206543 1.0145483016967773
CurrentTrain: epoch  0, batch    16 | loss: 9.4785414Losses:  10.266489028930664 1.371293544769287
CurrentTrain: epoch  0, batch    17 | loss: 10.2664890Losses:  10.35614013671875 1.4195005893707275
CurrentTrain: epoch  0, batch    18 | loss: 10.3561401Losses:  10.040178298950195 1.2169437408447266
CurrentTrain: epoch  0, batch    19 | loss: 10.0401783Losses:  8.63301944732666 0.9376957416534424
CurrentTrain: epoch  0, batch    20 | loss: 8.6330194Losses:  8.88477611541748 0.8939952850341797
CurrentTrain: epoch  0, batch    21 | loss: 8.8847761Losses:  9.119125366210938 1.0454519987106323
CurrentTrain: epoch  0, batch    22 | loss: 9.1191254Losses:  9.759127616882324 1.1356971263885498
CurrentTrain: epoch  0, batch    23 | loss: 9.7591276Losses:  9.655691146850586 1.011099100112915
CurrentTrain: epoch  0, batch    24 | loss: 9.6556911Losses:  10.189722061157227 1.4141578674316406
CurrentTrain: epoch  0, batch    25 | loss: 10.1897221Losses:  9.185192108154297 1.141857624053955
CurrentTrain: epoch  0, batch    26 | loss: 9.1851921Losses:  9.68136215209961 1.2534117698669434
CurrentTrain: epoch  0, batch    27 | loss: 9.6813622Losses:  9.278762817382812 1.2582662105560303
CurrentTrain: epoch  0, batch    28 | loss: 9.2787628Losses:  10.120654106140137 1.3801778554916382
CurrentTrain: epoch  0, batch    29 | loss: 10.1206541Losses:  8.960844039916992 1.0988929271697998
CurrentTrain: epoch  0, batch    30 | loss: 8.9608440Losses:  8.174278259277344 0.8065468072891235
CurrentTrain: epoch  0, batch    31 | loss: 8.1742783Losses:  9.14998722076416 1.149553656578064
CurrentTrain: epoch  0, batch    32 | loss: 9.1499872Losses:  7.654618263244629 0.8839601278305054
CurrentTrain: epoch  0, batch    33 | loss: 7.6546183Losses:  8.98906421661377 1.1484644412994385
CurrentTrain: epoch  0, batch    34 | loss: 8.9890642Losses:  9.595468521118164 1.0240806341171265
CurrentTrain: epoch  0, batch    35 | loss: 9.5954685Losses:  8.56608772277832 0.9714951515197754
CurrentTrain: epoch  0, batch    36 | loss: 8.5660877Losses:  8.393576622009277 0.9198788404464722
CurrentTrain: epoch  0, batch    37 | loss: 8.3935766Losses:  8.514309883117676 0.9634024500846863
CurrentTrain: epoch  0, batch    38 | loss: 8.5143099Losses:  9.629746437072754 1.1116998195648193
CurrentTrain: epoch  0, batch    39 | loss: 9.6297464Losses:  9.055073738098145 1.0935804843902588
CurrentTrain: epoch  0, batch    40 | loss: 9.0550737Losses:  9.229692459106445 0.9354239702224731
CurrentTrain: epoch  0, batch    41 | loss: 9.2296925Losses:  9.442092895507812 1.0744633674621582
CurrentTrain: epoch  0, batch    42 | loss: 9.4420929Losses:  8.902498245239258 1.1118402481079102
CurrentTrain: epoch  0, batch    43 | loss: 8.9024982Losses:  8.563194274902344 1.095123529434204
CurrentTrain: epoch  0, batch    44 | loss: 8.5631943Losses:  8.862129211425781 1.1013693809509277
CurrentTrain: epoch  0, batch    45 | loss: 8.8621292Losses:  8.968608856201172 0.905282735824585
CurrentTrain: epoch  0, batch    46 | loss: 8.9686089Losses:  9.125615119934082 1.0054073333740234
CurrentTrain: epoch  0, batch    47 | loss: 9.1256151Losses:  8.775832176208496 1.0815494060516357
CurrentTrain: epoch  0, batch    48 | loss: 8.7758322Losses:  9.222686767578125 1.1264197826385498
CurrentTrain: epoch  0, batch    49 | loss: 9.2226868Losses:  8.822052955627441 0.8610997796058655
CurrentTrain: epoch  0, batch    50 | loss: 8.8220530Losses:  8.005400657653809 0.7872965931892395
CurrentTrain: epoch  0, batch    51 | loss: 8.0054007Losses:  9.485156059265137 1.1770637035369873
CurrentTrain: epoch  0, batch    52 | loss: 9.4851561Losses:  8.249079704284668 0.8898197412490845
CurrentTrain: epoch  0, batch    53 | loss: 8.2490797Losses:  8.152501106262207 0.8533110022544861
CurrentTrain: epoch  0, batch    54 | loss: 8.1525011Losses:  8.200372695922852 1.052383303642273
CurrentTrain: epoch  0, batch    55 | loss: 8.2003727Losses:  8.740761756896973 0.9513251185417175
CurrentTrain: epoch  0, batch    56 | loss: 8.7407618Losses:  7.6978936195373535 0.8016878366470337
CurrentTrain: epoch  0, batch    57 | loss: 7.6978936Losses:  7.602931022644043 0.729225218296051
CurrentTrain: epoch  0, batch    58 | loss: 7.6029310Losses:  8.360559463500977 0.8387414216995239
CurrentTrain: epoch  0, batch    59 | loss: 8.3605595Losses:  7.992137908935547 0.794780969619751
CurrentTrain: epoch  0, batch    60 | loss: 7.9921379Losses:  7.311120986938477 0.7759455442428589
CurrentTrain: epoch  0, batch    61 | loss: 7.3111210Losses:  9.018265724182129 1.0632150173187256
CurrentTrain: epoch  0, batch    62 | loss: 9.0182657Losses:  7.913012504577637 0.9073501825332642
CurrentTrain: epoch  1, batch     0 | loss: 7.9130125Losses:  7.237400054931641 0.6777564883232117
CurrentTrain: epoch  1, batch     1 | loss: 7.2374001Losses:  7.191174507141113 0.849674642086029
CurrentTrain: epoch  1, batch     2 | loss: 7.1911745Losses:  8.449771881103516 1.061577558517456
CurrentTrain: epoch  1, batch     3 | loss: 8.4497719Losses:  7.989487648010254 0.8931667804718018
CurrentTrain: epoch  1, batch     4 | loss: 7.9894876Losses:  6.990357398986816 0.6778966188430786
CurrentTrain: epoch  1, batch     5 | loss: 6.9903574Losses:  7.7603912353515625 0.8184632062911987
CurrentTrain: epoch  1, batch     6 | loss: 7.7603912Losses:  7.964129447937012 0.9417484998703003
CurrentTrain: epoch  1, batch     7 | loss: 7.9641294Losses:  7.812635898590088 0.9737071990966797
CurrentTrain: epoch  1, batch     8 | loss: 7.8126359Losses:  7.5019731521606445 1.0033977031707764
CurrentTrain: epoch  1, batch     9 | loss: 7.5019732Losses:  8.033401489257812 0.8384084105491638
CurrentTrain: epoch  1, batch    10 | loss: 8.0334015Losses:  7.701339244842529 1.0430747270584106
CurrentTrain: epoch  1, batch    11 | loss: 7.7013392Losses:  7.731141567230225 0.8037109375
CurrentTrain: epoch  1, batch    12 | loss: 7.7311416Losses:  7.773772716522217 0.9893669486045837
CurrentTrain: epoch  1, batch    13 | loss: 7.7737727Losses:  8.358362197875977 0.9289867281913757
CurrentTrain: epoch  1, batch    14 | loss: 8.3583622Losses:  8.267295837402344 0.9607416391372681
CurrentTrain: epoch  1, batch    15 | loss: 8.2672958Losses:  8.265901565551758 0.8847088813781738
CurrentTrain: epoch  1, batch    16 | loss: 8.2659016Losses:  8.24997329711914 0.925556480884552
CurrentTrain: epoch  1, batch    17 | loss: 8.2499733Losses:  6.954829692840576 0.7309142351150513
CurrentTrain: epoch  1, batch    18 | loss: 6.9548297Losses:  7.485569953918457 0.9708765149116516
CurrentTrain: epoch  1, batch    19 | loss: 7.4855700Losses:  7.040289878845215 0.7480504512786865
CurrentTrain: epoch  1, batch    20 | loss: 7.0402899Losses:  8.014195442199707 0.9847050309181213
CurrentTrain: epoch  1, batch    21 | loss: 8.0141954Losses:  7.64227819442749 0.9440509080886841
CurrentTrain: epoch  1, batch    22 | loss: 7.6422782Losses:  7.2589287757873535 0.8572102785110474
CurrentTrain: epoch  1, batch    23 | loss: 7.2589288Losses:  6.216219902038574 0.6082886457443237
CurrentTrain: epoch  1, batch    24 | loss: 6.2162199Losses:  6.250831127166748 0.5577276349067688
CurrentTrain: epoch  1, batch    25 | loss: 6.2508311Losses:  7.970601558685303 0.9771587252616882
CurrentTrain: epoch  1, batch    26 | loss: 7.9706016Losses:  7.198911666870117 0.9064309000968933
CurrentTrain: epoch  1, batch    27 | loss: 7.1989117Losses:  7.660717010498047 0.8742144107818604
CurrentTrain: epoch  1, batch    28 | loss: 7.6607170Losses:  6.7751359939575195 0.7841923236846924
CurrentTrain: epoch  1, batch    29 | loss: 6.7751360Losses:  7.553650379180908 0.8587222099304199
CurrentTrain: epoch  1, batch    30 | loss: 7.5536504Losses:  7.891468048095703 0.8758682012557983
CurrentTrain: epoch  1, batch    31 | loss: 7.8914680Losses:  7.5504279136657715 0.8963566422462463
CurrentTrain: epoch  1, batch    32 | loss: 7.5504279Losses:  6.852849006652832 0.7756289839744568
CurrentTrain: epoch  1, batch    33 | loss: 6.8528490Losses:  7.309185028076172 0.8292049765586853
CurrentTrain: epoch  1, batch    34 | loss: 7.3091850Losses:  7.719222068786621 0.8922598361968994
CurrentTrain: epoch  1, batch    35 | loss: 7.7192221Losses:  7.348287582397461 0.7138997912406921
CurrentTrain: epoch  1, batch    36 | loss: 7.3482876Losses:  6.518690586090088 0.6082448959350586
CurrentTrain: epoch  1, batch    37 | loss: 6.5186906Losses:  6.8795976638793945 0.571040153503418
CurrentTrain: epoch  1, batch    38 | loss: 6.8795977Losses:  6.228698253631592 0.6572661399841309
CurrentTrain: epoch  1, batch    39 | loss: 6.2286983Losses:  6.74171781539917 0.767208456993103
CurrentTrain: epoch  1, batch    40 | loss: 6.7417178Losses:  6.589020729064941 0.8038151264190674
CurrentTrain: epoch  1, batch    41 | loss: 6.5890207Losses:  6.734243392944336 0.7359506487846375
CurrentTrain: epoch  1, batch    42 | loss: 6.7342434Losses:  7.4916157722473145 0.8241745829582214
CurrentTrain: epoch  1, batch    43 | loss: 7.4916158Losses:  7.705107688903809 0.8417304754257202
CurrentTrain: epoch  1, batch    44 | loss: 7.7051077Losses:  7.724852561950684 0.8920332193374634
CurrentTrain: epoch  1, batch    45 | loss: 7.7248526Losses:  7.272152900695801 0.9346526861190796
CurrentTrain: epoch  1, batch    46 | loss: 7.2721529Losses:  7.528193473815918 0.8396624326705933
CurrentTrain: epoch  1, batch    47 | loss: 7.5281935Losses:  7.360819339752197 0.6289058327674866
CurrentTrain: epoch  1, batch    48 | loss: 7.3608193Losses:  6.941124439239502 0.7332004308700562
CurrentTrain: epoch  1, batch    49 | loss: 6.9411244Losses:  7.431596755981445 0.7632424831390381
CurrentTrain: epoch  1, batch    50 | loss: 7.4315968Losses:  7.166815280914307 0.8815377354621887
CurrentTrain: epoch  1, batch    51 | loss: 7.1668153Losses:  6.739573001861572 0.5064024925231934
CurrentTrain: epoch  1, batch    52 | loss: 6.7395730Losses:  6.0602617263793945 0.615580677986145
CurrentTrain: epoch  1, batch    53 | loss: 6.0602617Losses:  6.494353294372559 0.6576923727989197
CurrentTrain: epoch  1, batch    54 | loss: 6.4943533Losses:  6.317110061645508 0.6746283173561096
CurrentTrain: epoch  1, batch    55 | loss: 6.3171101Losses:  6.384747505187988 0.6414971351623535
CurrentTrain: epoch  1, batch    56 | loss: 6.3847475Losses:  7.941830635070801 0.79034423828125
CurrentTrain: epoch  1, batch    57 | loss: 7.9418306Losses:  6.087218761444092 0.5922727584838867
CurrentTrain: epoch  1, batch    58 | loss: 6.0872188Losses:  7.375728130340576 0.8267693519592285
CurrentTrain: epoch  1, batch    59 | loss: 7.3757281Losses:  6.472781658172607 0.772628664970398
CurrentTrain: epoch  1, batch    60 | loss: 6.4727817Losses:  8.145263671875 1.0092787742614746
CurrentTrain: epoch  1, batch    61 | loss: 8.1452637Losses:  5.148318290710449 0.3553113639354706
CurrentTrain: epoch  1, batch    62 | loss: 5.1483183Losses:  5.8909196853637695 0.6861477494239807
CurrentTrain: epoch  2, batch     0 | loss: 5.8909197Losses:  6.256834983825684 0.7456371784210205
CurrentTrain: epoch  2, batch     1 | loss: 6.2568350Losses:  5.557401180267334 0.5325498580932617
CurrentTrain: epoch  2, batch     2 | loss: 5.5574012Losses:  7.828078269958496 0.7524079084396362
CurrentTrain: epoch  2, batch     3 | loss: 7.8280783Losses:  8.09582233428955 0.8182277083396912
CurrentTrain: epoch  2, batch     4 | loss: 8.0958223Losses:  6.918881416320801 0.7560141086578369
CurrentTrain: epoch  2, batch     5 | loss: 6.9188814Losses:  6.827269077301025 0.8034048080444336
CurrentTrain: epoch  2, batch     6 | loss: 6.8272691Losses:  6.333489418029785 0.6629642844200134
CurrentTrain: epoch  2, batch     7 | loss: 6.3334894Losses:  6.307950973510742 0.6258670091629028
CurrentTrain: epoch  2, batch     8 | loss: 6.3079510Losses:  7.007002830505371 0.8665133714675903
CurrentTrain: epoch  2, batch     9 | loss: 7.0070028Losses:  6.133852958679199 0.46961116790771484
CurrentTrain: epoch  2, batch    10 | loss: 6.1338530Losses:  5.6295342445373535 0.4804481267929077
CurrentTrain: epoch  2, batch    11 | loss: 5.6295342Losses:  5.687621116638184 0.6150864362716675
CurrentTrain: epoch  2, batch    12 | loss: 5.6876211Losses:  6.100225448608398 0.5867260694503784
CurrentTrain: epoch  2, batch    13 | loss: 6.1002254Losses:  6.402113437652588 0.5796298980712891
CurrentTrain: epoch  2, batch    14 | loss: 6.4021134Losses:  6.008840084075928 0.5175080299377441
CurrentTrain: epoch  2, batch    15 | loss: 6.0088401Losses:  6.443181991577148 0.6820461750030518
CurrentTrain: epoch  2, batch    16 | loss: 6.4431820Losses:  6.138288974761963 0.5763168931007385
CurrentTrain: epoch  2, batch    17 | loss: 6.1382890Losses:  5.363389015197754 0.26530495285987854
CurrentTrain: epoch  2, batch    18 | loss: 5.3633890Losses:  6.614645957946777 0.857903242111206
CurrentTrain: epoch  2, batch    19 | loss: 6.6146460Losses:  5.911590576171875 0.7054007053375244
CurrentTrain: epoch  2, batch    20 | loss: 5.9115906Losses:  6.2867279052734375 0.5908185243606567
CurrentTrain: epoch  2, batch    21 | loss: 6.2867279Losses:  6.214636325836182 0.6573740243911743
CurrentTrain: epoch  2, batch    22 | loss: 6.2146363Losses:  6.274471282958984 0.5548333525657654
CurrentTrain: epoch  2, batch    23 | loss: 6.2744713Losses:  5.543088912963867 0.5567411184310913
CurrentTrain: epoch  2, batch    24 | loss: 5.5430889Losses:  5.814116954803467 0.4513692557811737
CurrentTrain: epoch  2, batch    25 | loss: 5.8141170Losses:  6.298195838928223 0.6414887309074402
CurrentTrain: epoch  2, batch    26 | loss: 6.2981958Losses:  5.993854522705078 0.618786633014679
CurrentTrain: epoch  2, batch    27 | loss: 5.9938545Losses:  5.5312042236328125 0.4795866012573242
CurrentTrain: epoch  2, batch    28 | loss: 5.5312042Losses:  6.268487453460693 0.6331669092178345
CurrentTrain: epoch  2, batch    29 | loss: 6.2684875Losses:  5.877930641174316 0.6677557229995728
CurrentTrain: epoch  2, batch    30 | loss: 5.8779306Losses:  5.506267070770264 0.46604853868484497
CurrentTrain: epoch  2, batch    31 | loss: 5.5062671Losses:  6.063689708709717 0.4972626864910126
CurrentTrain: epoch  2, batch    32 | loss: 6.0636897Losses:  6.667008876800537 0.7038111090660095
CurrentTrain: epoch  2, batch    33 | loss: 6.6670089Losses:  5.801303863525391 0.5002049207687378
CurrentTrain: epoch  2, batch    34 | loss: 5.8013039Losses:  6.744561195373535 0.7017858028411865
CurrentTrain: epoch  2, batch    35 | loss: 6.7445612Losses:  5.652393341064453 0.4728025794029236
CurrentTrain: epoch  2, batch    36 | loss: 5.6523933Losses:  6.578572750091553 0.616696834564209
CurrentTrain: epoch  2, batch    37 | loss: 6.5785728Losses:  5.855157852172852 0.497295618057251
CurrentTrain: epoch  2, batch    38 | loss: 5.8551579Losses:  5.655609130859375 0.6680687069892883
CurrentTrain: epoch  2, batch    39 | loss: 5.6556091Losses:  5.712587833404541 0.45908915996551514
CurrentTrain: epoch  2, batch    40 | loss: 5.7125878Losses:  6.841158866882324 0.5976073741912842
CurrentTrain: epoch  2, batch    41 | loss: 6.8411589Losses:  5.719686508178711 0.4429522156715393
CurrentTrain: epoch  2, batch    42 | loss: 5.7196865Losses:  5.9672040939331055 0.6701294183731079
CurrentTrain: epoch  2, batch    43 | loss: 5.9672041Losses:  5.849239826202393 0.4677262306213379
CurrentTrain: epoch  2, batch    44 | loss: 5.8492398Losses:  5.889614105224609 0.7272402048110962
CurrentTrain: epoch  2, batch    45 | loss: 5.8896141Losses:  5.33879280090332 0.45616158843040466
CurrentTrain: epoch  2, batch    46 | loss: 5.3387928Losses:  6.149971008300781 0.5456554293632507
CurrentTrain: epoch  2, batch    47 | loss: 6.1499710Losses:  5.600755214691162 0.5314207673072815
CurrentTrain: epoch  2, batch    48 | loss: 5.6007552Losses:  6.258889198303223 0.7781431078910828
CurrentTrain: epoch  2, batch    49 | loss: 6.2588892Losses:  5.5084919929504395 0.3855481743812561
CurrentTrain: epoch  2, batch    50 | loss: 5.5084920Losses:  5.2458624839782715 0.45097607374191284
CurrentTrain: epoch  2, batch    51 | loss: 5.2458625Losses:  5.328853130340576 0.39938095211982727
CurrentTrain: epoch  2, batch    52 | loss: 5.3288531Losses:  5.407375335693359 0.5448141098022461
CurrentTrain: epoch  2, batch    53 | loss: 5.4073753Losses:  5.628528594970703 0.47807222604751587
CurrentTrain: epoch  2, batch    54 | loss: 5.6285286Losses:  5.264723300933838 0.4873107671737671
CurrentTrain: epoch  2, batch    55 | loss: 5.2647233Losses:  5.390952110290527 0.49800729751586914
CurrentTrain: epoch  2, batch    56 | loss: 5.3909521Losses:  6.396154403686523 0.6435949802398682
CurrentTrain: epoch  2, batch    57 | loss: 6.3961544Losses:  5.871964931488037 0.6315178871154785
CurrentTrain: epoch  2, batch    58 | loss: 5.8719649Losses:  5.321340560913086 0.4191514253616333
CurrentTrain: epoch  2, batch    59 | loss: 5.3213406Losses:  5.603604316711426 0.5426450371742249
CurrentTrain: epoch  2, batch    60 | loss: 5.6036043Losses:  6.2861247062683105 0.601191520690918
CurrentTrain: epoch  2, batch    61 | loss: 6.2861247Losses:  4.940016746520996 0.4411762058734894
CurrentTrain: epoch  2, batch    62 | loss: 4.9400167Losses:  5.25490665435791 0.48407453298568726
CurrentTrain: epoch  3, batch     0 | loss: 5.2549067Losses:  5.338848114013672 0.565563440322876
CurrentTrain: epoch  3, batch     1 | loss: 5.3388481Losses:  5.544902801513672 0.5448276996612549
CurrentTrain: epoch  3, batch     2 | loss: 5.5449028Losses:  5.70501708984375 0.5073370337486267
CurrentTrain: epoch  3, batch     3 | loss: 5.7050171Losses:  5.071159362792969 0.39204898476600647
CurrentTrain: epoch  3, batch     4 | loss: 5.0711594Losses:  5.68122673034668 0.44716954231262207
CurrentTrain: epoch  3, batch     5 | loss: 5.6812267Losses:  5.256333351135254 0.5661132335662842
CurrentTrain: epoch  3, batch     6 | loss: 5.2563334Losses:  5.647220611572266 0.4943321645259857
CurrentTrain: epoch  3, batch     7 | loss: 5.6472206Losses:  5.874546527862549 0.43486541509628296
CurrentTrain: epoch  3, batch     8 | loss: 5.8745465Losses:  5.940845966339111 0.6304192543029785
CurrentTrain: epoch  3, batch     9 | loss: 5.9408460Losses:  5.464955806732178 0.5768791437149048
CurrentTrain: epoch  3, batch    10 | loss: 5.4649558Losses:  5.1978936195373535 0.4952329993247986
CurrentTrain: epoch  3, batch    11 | loss: 5.1978936Losses:  5.390667915344238 0.5558784008026123
CurrentTrain: epoch  3, batch    12 | loss: 5.3906679Losses:  5.7954254150390625 0.5424882173538208
CurrentTrain: epoch  3, batch    13 | loss: 5.7954254Losses:  5.967645645141602 0.5422793030738831
CurrentTrain: epoch  3, batch    14 | loss: 5.9676456Losses:  5.562536239624023 0.481950581073761
CurrentTrain: epoch  3, batch    15 | loss: 5.5625362Losses:  6.121124744415283 0.56111079454422
CurrentTrain: epoch  3, batch    16 | loss: 6.1211247Losses:  5.375548362731934 0.4651753902435303
CurrentTrain: epoch  3, batch    17 | loss: 5.3755484Losses:  5.253544330596924 0.45033058524131775
CurrentTrain: epoch  3, batch    18 | loss: 5.2535443Losses:  4.774077892303467 0.31084203720092773
CurrentTrain: epoch  3, batch    19 | loss: 4.7740779Losses:  5.069032669067383 0.4147114157676697
CurrentTrain: epoch  3, batch    20 | loss: 5.0690327Losses:  5.015744686126709 0.3720411956310272
CurrentTrain: epoch  3, batch    21 | loss: 5.0157447Losses:  5.8504533767700195 0.6724014282226562
CurrentTrain: epoch  3, batch    22 | loss: 5.8504534Losses:  5.3906354904174805 0.39282798767089844
CurrentTrain: epoch  3, batch    23 | loss: 5.3906355Losses:  5.690298557281494 0.44170698523521423
CurrentTrain: epoch  3, batch    24 | loss: 5.6902986Losses:  5.889305114746094 0.58512282371521
CurrentTrain: epoch  3, batch    25 | loss: 5.8893051Losses:  4.868211269378662 0.3382076025009155
CurrentTrain: epoch  3, batch    26 | loss: 4.8682113Losses:  5.137887001037598 0.43647268414497375
CurrentTrain: epoch  3, batch    27 | loss: 5.1378870Losses:  5.874294281005859 0.5501234531402588
CurrentTrain: epoch  3, batch    28 | loss: 5.8742943Losses:  5.2530107498168945 0.5151250958442688
CurrentTrain: epoch  3, batch    29 | loss: 5.2530107Losses:  5.060490608215332 0.4416286051273346
CurrentTrain: epoch  3, batch    30 | loss: 5.0604906Losses:  5.360939025878906 0.5487918257713318
CurrentTrain: epoch  3, batch    31 | loss: 5.3609390Losses:  5.330023765563965 0.4386330544948578
CurrentTrain: epoch  3, batch    32 | loss: 5.3300238Losses:  5.616358757019043 0.46430498361587524
CurrentTrain: epoch  3, batch    33 | loss: 5.6163588Losses:  5.463548183441162 0.4565258324146271
CurrentTrain: epoch  3, batch    34 | loss: 5.4635482Losses:  5.34577751159668 0.5174136757850647
CurrentTrain: epoch  3, batch    35 | loss: 5.3457775Losses:  4.875326156616211 0.3882598876953125
CurrentTrain: epoch  3, batch    36 | loss: 4.8753262Losses:  5.255047798156738 0.49804678559303284
CurrentTrain: epoch  3, batch    37 | loss: 5.2550478Losses:  5.018628120422363 0.42565345764160156
CurrentTrain: epoch  3, batch    38 | loss: 5.0186281Losses:  4.895467281341553 0.2112487554550171
CurrentTrain: epoch  3, batch    39 | loss: 4.8954673Losses:  5.916641712188721 0.563805103302002
CurrentTrain: epoch  3, batch    40 | loss: 5.9166417Losses:  5.31594181060791 0.41861602663993835
CurrentTrain: epoch  3, batch    41 | loss: 5.3159418Losses:  4.800167560577393 0.4065125584602356
CurrentTrain: epoch  3, batch    42 | loss: 4.8001676Losses:  5.175673961639404 0.3168865442276001
CurrentTrain: epoch  3, batch    43 | loss: 5.1756740Losses:  5.319777965545654 0.3477821350097656
CurrentTrain: epoch  3, batch    44 | loss: 5.3197780Losses:  5.2932939529418945 0.44938376545906067
CurrentTrain: epoch  3, batch    45 | loss: 5.2932940Losses:  5.270143032073975 0.5770648717880249
CurrentTrain: epoch  3, batch    46 | loss: 5.2701430Losses:  4.902958393096924 0.31860285997390747
CurrentTrain: epoch  3, batch    47 | loss: 4.9029584Losses:  5.052802085876465 0.4771431088447571
CurrentTrain: epoch  3, batch    48 | loss: 5.0528021Losses:  4.857897758483887 0.3759946823120117
CurrentTrain: epoch  3, batch    49 | loss: 4.8578978Losses:  5.567366123199463 0.5588801503181458
CurrentTrain: epoch  3, batch    50 | loss: 5.5673661Losses:  5.642490386962891 0.5237576365470886
CurrentTrain: epoch  3, batch    51 | loss: 5.6424904Losses:  4.806371688842773 0.3719702959060669
CurrentTrain: epoch  3, batch    52 | loss: 4.8063717Losses:  5.805976867675781 0.5414849519729614
CurrentTrain: epoch  3, batch    53 | loss: 5.8059769Losses:  4.809326171875 0.27535682916641235
CurrentTrain: epoch  3, batch    54 | loss: 4.8093262Losses:  4.814703464508057 0.32534945011138916
CurrentTrain: epoch  3, batch    55 | loss: 4.8147035Losses:  5.026278972625732 0.3952263593673706
CurrentTrain: epoch  3, batch    56 | loss: 5.0262790Losses:  5.14811897277832 0.3891470432281494
CurrentTrain: epoch  3, batch    57 | loss: 5.1481190Losses:  4.795996189117432 0.3208674192428589
CurrentTrain: epoch  3, batch    58 | loss: 4.7959962Losses:  5.930741310119629 0.42387962341308594
CurrentTrain: epoch  3, batch    59 | loss: 5.9307413Losses:  4.824905872344971 0.3818792700767517
CurrentTrain: epoch  3, batch    60 | loss: 4.8249059Losses:  5.091531276702881 0.3729245364665985
CurrentTrain: epoch  3, batch    61 | loss: 5.0915313Losses:  4.908921241760254 0.3258907198905945
CurrentTrain: epoch  3, batch    62 | loss: 4.9089212Losses:  5.446678638458252 0.4929022192955017
CurrentTrain: epoch  4, batch     0 | loss: 5.4466786Losses:  4.857767581939697 0.25480201840400696
CurrentTrain: epoch  4, batch     1 | loss: 4.8577676Losses:  5.319520473480225 0.3758467435836792
CurrentTrain: epoch  4, batch     2 | loss: 5.3195205Losses:  4.823703765869141 0.3902779221534729
CurrentTrain: epoch  4, batch     3 | loss: 4.8237038Losses:  4.76370906829834 0.38126635551452637
CurrentTrain: epoch  4, batch     4 | loss: 4.7637091Losses:  4.834175109863281 0.31412482261657715
CurrentTrain: epoch  4, batch     5 | loss: 4.8341751Losses:  5.680565357208252 0.40181514620780945
CurrentTrain: epoch  4, batch     6 | loss: 5.6805654Losses:  5.2364654541015625 0.3068305253982544
CurrentTrain: epoch  4, batch     7 | loss: 5.2364655Losses:  5.003513813018799 0.43185001611709595
CurrentTrain: epoch  4, batch     8 | loss: 5.0035138Losses:  5.108572483062744 0.46823927760124207
CurrentTrain: epoch  4, batch     9 | loss: 5.1085725Losses:  4.779236316680908 0.3493528962135315
CurrentTrain: epoch  4, batch    10 | loss: 4.7792363Losses:  5.390384674072266 0.27832484245300293
CurrentTrain: epoch  4, batch    11 | loss: 5.3903847Losses:  5.036858558654785 0.42261242866516113
CurrentTrain: epoch  4, batch    12 | loss: 5.0368586Losses:  4.745115280151367 0.30758601427078247
CurrentTrain: epoch  4, batch    13 | loss: 4.7451153Losses:  5.119378089904785 0.45027729868888855
CurrentTrain: epoch  4, batch    14 | loss: 5.1193781Losses:  5.294645309448242 0.3700428605079651
CurrentTrain: epoch  4, batch    15 | loss: 5.2946453Losses:  4.980762481689453 0.4123391807079315
CurrentTrain: epoch  4, batch    16 | loss: 4.9807625Losses:  4.865179538726807 0.30668872594833374
CurrentTrain: epoch  4, batch    17 | loss: 4.8651795Losses:  4.855966567993164 0.3657863140106201
CurrentTrain: epoch  4, batch    18 | loss: 4.8559666Losses:  4.683485507965088 0.2805887460708618
CurrentTrain: epoch  4, batch    19 | loss: 4.6834855Losses:  4.808549880981445 0.43510890007019043
CurrentTrain: epoch  4, batch    20 | loss: 4.8085499Losses:  5.1992692947387695 0.36444729566574097
CurrentTrain: epoch  4, batch    21 | loss: 5.1992693Losses:  4.670895099639893 0.28869712352752686
CurrentTrain: epoch  4, batch    22 | loss: 4.6708951Losses:  4.974926948547363 0.32443124055862427
CurrentTrain: epoch  4, batch    23 | loss: 4.9749269Losses:  4.821672439575195 0.3756096363067627
CurrentTrain: epoch  4, batch    24 | loss: 4.8216724Losses:  4.915840148925781 0.5046234130859375
CurrentTrain: epoch  4, batch    25 | loss: 4.9158401Losses:  4.722308158874512 0.4349949359893799
CurrentTrain: epoch  4, batch    26 | loss: 4.7223082Losses:  4.934678554534912 0.32749050855636597
CurrentTrain: epoch  4, batch    27 | loss: 4.9346786Losses:  4.963854789733887 0.4058358669281006
CurrentTrain: epoch  4, batch    28 | loss: 4.9638548Losses:  4.979297637939453 0.45851826667785645
CurrentTrain: epoch  4, batch    29 | loss: 4.9792976Losses:  4.753429889678955 0.32315853238105774
CurrentTrain: epoch  4, batch    30 | loss: 4.7534299Losses:  4.836616039276123 0.3361048996448517
CurrentTrain: epoch  4, batch    31 | loss: 4.8366160Losses:  4.686467170715332 0.35096633434295654
CurrentTrain: epoch  4, batch    32 | loss: 4.6864672Losses:  4.6843743324279785 0.3237568438053131
CurrentTrain: epoch  4, batch    33 | loss: 4.6843743Losses:  4.723499774932861 0.3663944602012634
CurrentTrain: epoch  4, batch    34 | loss: 4.7234998Losses:  4.967250823974609 0.4219229221343994
CurrentTrain: epoch  4, batch    35 | loss: 4.9672508Losses:  4.791486740112305 0.3161712884902954
CurrentTrain: epoch  4, batch    36 | loss: 4.7914867Losses:  4.771218776702881 0.4040110409259796
CurrentTrain: epoch  4, batch    37 | loss: 4.7712188Losses:  4.6387410163879395 0.37053245306015015
CurrentTrain: epoch  4, batch    38 | loss: 4.6387410Losses:  4.841869354248047 0.46423912048339844
CurrentTrain: epoch  4, batch    39 | loss: 4.8418694Losses:  4.839544773101807 0.3950800597667694
CurrentTrain: epoch  4, batch    40 | loss: 4.8395448Losses:  4.6714324951171875 0.2996117174625397
CurrentTrain: epoch  4, batch    41 | loss: 4.6714325Losses:  4.665692329406738 0.2671809792518616
CurrentTrain: epoch  4, batch    42 | loss: 4.6656923Losses:  4.673357009887695 0.3771142065525055
CurrentTrain: epoch  4, batch    43 | loss: 4.6733570Losses:  4.560462474822998 0.2914736866950989
CurrentTrain: epoch  4, batch    44 | loss: 4.5604625Losses:  4.834365367889404 0.41560879349708557
CurrentTrain: epoch  4, batch    45 | loss: 4.8343654Losses:  4.90478515625 0.3199805021286011
CurrentTrain: epoch  4, batch    46 | loss: 4.9047852Losses:  4.618732452392578 0.2781450152397156
CurrentTrain: epoch  4, batch    47 | loss: 4.6187325Losses:  4.954557418823242 0.4030415713787079
CurrentTrain: epoch  4, batch    48 | loss: 4.9545574Losses:  4.526010990142822 0.22572851181030273
CurrentTrain: epoch  4, batch    49 | loss: 4.5260110Losses:  4.630315780639648 0.2624112367630005
CurrentTrain: epoch  4, batch    50 | loss: 4.6303158Losses:  4.7647833824157715 0.31587550044059753
CurrentTrain: epoch  4, batch    51 | loss: 4.7647834Losses:  4.659857749938965 0.3152885437011719
CurrentTrain: epoch  4, batch    52 | loss: 4.6598577Losses:  6.331474304199219 0.440565288066864
CurrentTrain: epoch  4, batch    53 | loss: 6.3314743Losses:  4.7777252197265625 0.3197682499885559
CurrentTrain: epoch  4, batch    54 | loss: 4.7777252Losses:  4.793436050415039 0.42328935861587524
CurrentTrain: epoch  4, batch    55 | loss: 4.7934361Losses:  4.730199813842773 0.38141000270843506
CurrentTrain: epoch  4, batch    56 | loss: 4.7301998Losses:  5.089803695678711 0.3284987211227417
CurrentTrain: epoch  4, batch    57 | loss: 5.0898037Losses:  4.667367935180664 0.37632930278778076
CurrentTrain: epoch  4, batch    58 | loss: 4.6673679Losses:  4.784003257751465 0.41128620505332947
CurrentTrain: epoch  4, batch    59 | loss: 4.7840033Losses:  4.777624607086182 0.32501405477523804
CurrentTrain: epoch  4, batch    60 | loss: 4.7776246Losses:  4.630557060241699 0.2720670700073242
CurrentTrain: epoch  4, batch    61 | loss: 4.6305571Losses:  4.559851169586182 0.14903667569160461
CurrentTrain: epoch  4, batch    62 | loss: 4.5598512Losses:  4.551688194274902 0.26483798027038574
CurrentTrain: epoch  5, batch     0 | loss: 4.5516882Losses:  4.621396064758301 0.34379667043685913
CurrentTrain: epoch  5, batch     1 | loss: 4.6213961Losses:  4.6463212966918945 0.33794140815734863
CurrentTrain: epoch  5, batch     2 | loss: 4.6463213Losses:  4.71021032333374 0.3978079855442047
CurrentTrain: epoch  5, batch     3 | loss: 4.7102103Losses:  4.59584379196167 0.3579852283000946
CurrentTrain: epoch  5, batch     4 | loss: 4.5958438Losses:  5.029284477233887 0.33787500858306885
CurrentTrain: epoch  5, batch     5 | loss: 5.0292845Losses:  5.020273685455322 0.38664010167121887
CurrentTrain: epoch  5, batch     6 | loss: 5.0202737Losses:  4.6697187423706055 0.32049480080604553
CurrentTrain: epoch  5, batch     7 | loss: 4.6697187Losses:  4.618224143981934 0.38593462109565735
CurrentTrain: epoch  5, batch     8 | loss: 4.6182241Losses:  4.791559219360352 0.41291114687919617
CurrentTrain: epoch  5, batch     9 | loss: 4.7915592Losses:  4.673730373382568 0.3362200856208801
CurrentTrain: epoch  5, batch    10 | loss: 4.6737304Losses:  4.599592685699463 0.3003355860710144
CurrentTrain: epoch  5, batch    11 | loss: 4.5995927Losses:  4.817915916442871 0.390048086643219
CurrentTrain: epoch  5, batch    12 | loss: 4.8179159Losses:  4.530962944030762 0.3555234670639038
CurrentTrain: epoch  5, batch    13 | loss: 4.5309629Losses:  4.609787464141846 0.3956558108329773
CurrentTrain: epoch  5, batch    14 | loss: 4.6097875Losses:  4.7242960929870605 0.28058236837387085
CurrentTrain: epoch  5, batch    15 | loss: 4.7242961Losses:  4.771039009094238 0.3142135441303253
CurrentTrain: epoch  5, batch    16 | loss: 4.7710390Losses:  4.4821600914001465 0.29146623611450195
CurrentTrain: epoch  5, batch    17 | loss: 4.4821601Losses:  4.680039405822754 0.2980029582977295
CurrentTrain: epoch  5, batch    18 | loss: 4.6800394Losses:  4.725386619567871 0.38255706429481506
CurrentTrain: epoch  5, batch    19 | loss: 4.7253866Losses:  4.58966588973999 0.28503185510635376
CurrentTrain: epoch  5, batch    20 | loss: 4.5896659Losses:  4.491475582122803 0.27662894129753113
CurrentTrain: epoch  5, batch    21 | loss: 4.4914756Losses:  4.291894912719727 0.1917070597410202
CurrentTrain: epoch  5, batch    22 | loss: 4.2918949Losses:  4.594003200531006 0.30061671137809753
CurrentTrain: epoch  5, batch    23 | loss: 4.5940032Losses:  4.382665634155273 0.22512753307819366
CurrentTrain: epoch  5, batch    24 | loss: 4.3826656Losses:  4.979247570037842 0.34489706158638
CurrentTrain: epoch  5, batch    25 | loss: 4.9792476Losses:  4.541640281677246 0.3080245852470398
CurrentTrain: epoch  5, batch    26 | loss: 4.5416403Losses:  4.5512542724609375 0.3522989749908447
CurrentTrain: epoch  5, batch    27 | loss: 4.5512543Losses:  4.427755355834961 0.23271393775939941
CurrentTrain: epoch  5, batch    28 | loss: 4.4277554Losses:  4.614977836608887 0.4058827757835388
CurrentTrain: epoch  5, batch    29 | loss: 4.6149778Losses:  4.682285308837891 0.31279122829437256
CurrentTrain: epoch  5, batch    30 | loss: 4.6822853Losses:  4.532232761383057 0.2817310094833374
CurrentTrain: epoch  5, batch    31 | loss: 4.5322328Losses:  4.63559103012085 0.354138046503067
CurrentTrain: epoch  5, batch    32 | loss: 4.6355910Losses:  4.6562724113464355 0.24518530070781708
CurrentTrain: epoch  5, batch    33 | loss: 4.6562724Losses:  4.614288806915283 0.30768144130706787
CurrentTrain: epoch  5, batch    34 | loss: 4.6142888Losses:  4.450453758239746 0.3147411346435547
CurrentTrain: epoch  5, batch    35 | loss: 4.4504538Losses:  4.487811088562012 0.28738778829574585
CurrentTrain: epoch  5, batch    36 | loss: 4.4878111Losses:  4.3618903160095215 0.24425898492336273
CurrentTrain: epoch  5, batch    37 | loss: 4.3618903Losses:  4.422591209411621 0.3030889332294464
CurrentTrain: epoch  5, batch    38 | loss: 4.4225912Losses:  4.503742218017578 0.29333704710006714
CurrentTrain: epoch  5, batch    39 | loss: 4.5037422Losses:  4.5080695152282715 0.3052150011062622
CurrentTrain: epoch  5, batch    40 | loss: 4.5080695Losses:  4.57042121887207 0.2888174057006836
CurrentTrain: epoch  5, batch    41 | loss: 4.5704212Losses:  4.476926326751709 0.23554083704948425
CurrentTrain: epoch  5, batch    42 | loss: 4.4769263Losses:  4.492766857147217 0.26361167430877686
CurrentTrain: epoch  5, batch    43 | loss: 4.4927669Losses:  4.659414291381836 0.37225425243377686
CurrentTrain: epoch  5, batch    44 | loss: 4.6594143Losses:  4.494481086730957 0.33145642280578613
CurrentTrain: epoch  5, batch    45 | loss: 4.4944811Losses:  4.3760786056518555 0.24314261972904205
CurrentTrain: epoch  5, batch    46 | loss: 4.3760786Losses:  4.535261154174805 0.2887077331542969
CurrentTrain: epoch  5, batch    47 | loss: 4.5352612Losses:  4.4907660484313965 0.28188833594322205
CurrentTrain: epoch  5, batch    48 | loss: 4.4907660Losses:  4.51870059967041 0.23576389253139496
CurrentTrain: epoch  5, batch    49 | loss: 4.5187006Losses:  4.523561000823975 0.2708956003189087
CurrentTrain: epoch  5, batch    50 | loss: 4.5235610Losses:  4.459253787994385 0.2377423346042633
CurrentTrain: epoch  5, batch    51 | loss: 4.4592538Losses:  4.3839240074157715 0.20877408981323242
CurrentTrain: epoch  5, batch    52 | loss: 4.3839240Losses:  4.5270771980285645 0.35630014538764954
CurrentTrain: epoch  5, batch    53 | loss: 4.5270772Losses:  4.485422611236572 0.3573991656303406
CurrentTrain: epoch  5, batch    54 | loss: 4.4854226Losses:  4.519392013549805 0.2672593593597412
CurrentTrain: epoch  5, batch    55 | loss: 4.5193920Losses:  4.540170192718506 0.28311794996261597
CurrentTrain: epoch  5, batch    56 | loss: 4.5401702Losses:  4.346721172332764 0.2020285576581955
CurrentTrain: epoch  5, batch    57 | loss: 4.3467212Losses:  4.647472381591797 0.34699907898902893
CurrentTrain: epoch  5, batch    58 | loss: 4.6474724Losses:  5.1176910400390625 0.379489541053772
CurrentTrain: epoch  5, batch    59 | loss: 5.1176910Losses:  4.375459671020508 0.21650028228759766
CurrentTrain: epoch  5, batch    60 | loss: 4.3754597Losses:  4.475569248199463 0.2953695058822632
CurrentTrain: epoch  5, batch    61 | loss: 4.4755692Losses:  4.423783302307129 0.20122551918029785
CurrentTrain: epoch  5, batch    62 | loss: 4.4237833Losses:  4.458408355712891 0.23226246237754822
CurrentTrain: epoch  6, batch     0 | loss: 4.4584084Losses:  4.414458274841309 0.23736023902893066
CurrentTrain: epoch  6, batch     1 | loss: 4.4144583Losses:  4.621673107147217 0.2584260106086731
CurrentTrain: epoch  6, batch     2 | loss: 4.6216731Losses:  4.677199363708496 0.3804132342338562
CurrentTrain: epoch  6, batch     3 | loss: 4.6771994Losses:  4.406269073486328 0.27672410011291504
CurrentTrain: epoch  6, batch     4 | loss: 4.4062691Losses:  4.620133399963379 0.2992732524871826
CurrentTrain: epoch  6, batch     5 | loss: 4.6201334Losses:  4.510988712310791 0.35127225518226624
CurrentTrain: epoch  6, batch     6 | loss: 4.5109887Losses:  4.51627254486084 0.3332977294921875
CurrentTrain: epoch  6, batch     7 | loss: 4.5162725Losses:  4.9263691902160645 0.16879451274871826
CurrentTrain: epoch  6, batch     8 | loss: 4.9263692Losses:  4.3738837242126465 0.268610417842865
CurrentTrain: epoch  6, batch     9 | loss: 4.3738837Losses:  4.654184341430664 0.38509202003479004
CurrentTrain: epoch  6, batch    10 | loss: 4.6541843Losses:  4.30906867980957 0.15638673305511475
CurrentTrain: epoch  6, batch    11 | loss: 4.3090687Losses:  4.493338108062744 0.2965964674949646
CurrentTrain: epoch  6, batch    12 | loss: 4.4933381Losses:  4.3812689781188965 0.2838054895401001
CurrentTrain: epoch  6, batch    13 | loss: 4.3812690Losses:  4.406663417816162 0.28792423009872437
CurrentTrain: epoch  6, batch    14 | loss: 4.4066634Losses:  4.476485729217529 0.36250922083854675
CurrentTrain: epoch  6, batch    15 | loss: 4.4764857Losses:  4.312182903289795 0.20239773392677307
CurrentTrain: epoch  6, batch    16 | loss: 4.3121829Losses:  4.505277633666992 0.3229619264602661
CurrentTrain: epoch  6, batch    17 | loss: 4.5052776Losses:  4.410063743591309 0.3173733949661255
CurrentTrain: epoch  6, batch    18 | loss: 4.4100637Losses:  4.40195894241333 0.22889508306980133
CurrentTrain: epoch  6, batch    19 | loss: 4.4019589Losses:  4.319451808929443 0.2246822565793991
CurrentTrain: epoch  6, batch    20 | loss: 4.3194518Losses:  4.440539360046387 0.2715751528739929
CurrentTrain: epoch  6, batch    21 | loss: 4.4405394Losses:  4.367656230926514 0.2463509440422058
CurrentTrain: epoch  6, batch    22 | loss: 4.3676562Losses:  4.563251972198486 0.3022341728210449
CurrentTrain: epoch  6, batch    23 | loss: 4.5632520Losses:  4.408383846282959 0.2939000427722931
CurrentTrain: epoch  6, batch    24 | loss: 4.4083838Losses:  4.393232345581055 0.24083136022090912
CurrentTrain: epoch  6, batch    25 | loss: 4.3932323Losses:  4.369524002075195 0.282792866230011
CurrentTrain: epoch  6, batch    26 | loss: 4.3695240Losses:  4.464224338531494 0.31625044345855713
CurrentTrain: epoch  6, batch    27 | loss: 4.4642243Losses:  4.29041862487793 0.21048077940940857
CurrentTrain: epoch  6, batch    28 | loss: 4.2904186Losses:  4.385665416717529 0.2747071087360382
CurrentTrain: epoch  6, batch    29 | loss: 4.3856654Losses:  4.265405654907227 0.18191036581993103
CurrentTrain: epoch  6, batch    30 | loss: 4.2654057Losses:  4.448741436004639 0.21717196702957153
CurrentTrain: epoch  6, batch    31 | loss: 4.4487414Losses:  4.38901948928833 0.20322000980377197
CurrentTrain: epoch  6, batch    32 | loss: 4.3890195Losses:  4.40641450881958 0.2377633899450302
CurrentTrain: epoch  6, batch    33 | loss: 4.4064145Losses:  4.4203925132751465 0.22514915466308594
CurrentTrain: epoch  6, batch    34 | loss: 4.4203925Losses:  4.580565452575684 0.27314257621765137
CurrentTrain: epoch  6, batch    35 | loss: 4.5805655Losses:  4.414149761199951 0.28884071111679077
CurrentTrain: epoch  6, batch    36 | loss: 4.4141498Losses:  4.442922592163086 0.3261757493019104
CurrentTrain: epoch  6, batch    37 | loss: 4.4429226Losses:  4.4792375564575195 0.1936171054840088
CurrentTrain: epoch  6, batch    38 | loss: 4.4792376Losses:  4.440533638000488 0.2932335138320923
CurrentTrain: epoch  6, batch    39 | loss: 4.4405336Losses:  4.435189247131348 0.23783990740776062
CurrentTrain: epoch  6, batch    40 | loss: 4.4351892Losses:  4.405694961547852 0.2741432785987854
CurrentTrain: epoch  6, batch    41 | loss: 4.4056950Losses:  4.55556583404541 0.3084552586078644
CurrentTrain: epoch  6, batch    42 | loss: 4.5555658Losses:  4.382565021514893 0.28149428963661194
CurrentTrain: epoch  6, batch    43 | loss: 4.3825650Losses:  4.446718215942383 0.2671506404876709
CurrentTrain: epoch  6, batch    44 | loss: 4.4467182Losses:  4.416251182556152 0.2745751738548279
CurrentTrain: epoch  6, batch    45 | loss: 4.4162512Losses:  4.454360008239746 0.26777034997940063
CurrentTrain: epoch  6, batch    46 | loss: 4.4543600Losses:  4.352575302124023 0.2536119222640991
CurrentTrain: epoch  6, batch    47 | loss: 4.3525753Losses:  4.389706134796143 0.29940617084503174
CurrentTrain: epoch  6, batch    48 | loss: 4.3897061Losses:  4.379984378814697 0.1769433319568634
CurrentTrain: epoch  6, batch    49 | loss: 4.3799844Losses:  4.404337406158447 0.2913561463356018
CurrentTrain: epoch  6, batch    50 | loss: 4.4043374Losses:  4.365732192993164 0.26449066400527954
CurrentTrain: epoch  6, batch    51 | loss: 4.3657322Losses:  4.569675445556641 0.3205064833164215
CurrentTrain: epoch  6, batch    52 | loss: 4.5696754Losses:  4.387217998504639 0.27407127618789673
CurrentTrain: epoch  6, batch    53 | loss: 4.3872180Losses:  4.2801594734191895 0.19165357947349548
CurrentTrain: epoch  6, batch    54 | loss: 4.2801595Losses:  4.286838531494141 0.2102382928133011
CurrentTrain: epoch  6, batch    55 | loss: 4.2868385Losses:  4.383020401000977 0.27397194504737854
CurrentTrain: epoch  6, batch    56 | loss: 4.3830204Losses:  4.313746452331543 0.24868810176849365
CurrentTrain: epoch  6, batch    57 | loss: 4.3137465Losses:  4.477468490600586 0.3287994861602783
CurrentTrain: epoch  6, batch    58 | loss: 4.4774685Losses:  4.3816022872924805 0.27686619758605957
CurrentTrain: epoch  6, batch    59 | loss: 4.3816023Losses:  4.3313069343566895 0.2563740313053131
CurrentTrain: epoch  6, batch    60 | loss: 4.3313069Losses:  4.241469860076904 0.20068007707595825
CurrentTrain: epoch  6, batch    61 | loss: 4.2414699Losses:  4.187504768371582 0.15862751007080078
CurrentTrain: epoch  6, batch    62 | loss: 4.1875048Losses:  4.381158828735352 0.282758504152298
CurrentTrain: epoch  7, batch     0 | loss: 4.3811588Losses:  4.429385185241699 0.30499860644340515
CurrentTrain: epoch  7, batch     1 | loss: 4.4293852Losses:  4.299785137176514 0.21863329410552979
CurrentTrain: epoch  7, batch     2 | loss: 4.2997851Losses:  4.425644874572754 0.2795153260231018
CurrentTrain: epoch  7, batch     3 | loss: 4.4256449Losses:  4.22855281829834 0.15255746245384216
CurrentTrain: epoch  7, batch     4 | loss: 4.2285528Losses:  4.32451868057251 0.25616884231567383
CurrentTrain: epoch  7, batch     5 | loss: 4.3245187Losses:  4.420928001403809 0.2718846797943115
CurrentTrain: epoch  7, batch     6 | loss: 4.4209280Losses:  4.284601211547852 0.2360537350177765
CurrentTrain: epoch  7, batch     7 | loss: 4.2846012Losses:  4.420222759246826 0.2898879945278168
CurrentTrain: epoch  7, batch     8 | loss: 4.4202228Losses:  4.273899555206299 0.21971645951271057
CurrentTrain: epoch  7, batch     9 | loss: 4.2738996Losses:  4.456498622894287 0.3199145495891571
CurrentTrain: epoch  7, batch    10 | loss: 4.4564986Losses:  4.381982803344727 0.2889745235443115
CurrentTrain: epoch  7, batch    11 | loss: 4.3819828Losses:  4.347555160522461 0.24887481331825256
CurrentTrain: epoch  7, batch    12 | loss: 4.3475552Losses:  4.34152889251709 0.28904619812965393
CurrentTrain: epoch  7, batch    13 | loss: 4.3415289Losses:  4.348220348358154 0.24498870968818665
CurrentTrain: epoch  7, batch    14 | loss: 4.3482203Losses:  4.325706481933594 0.22980159521102905
CurrentTrain: epoch  7, batch    15 | loss: 4.3257065Losses:  4.27620792388916 0.2609652280807495
CurrentTrain: epoch  7, batch    16 | loss: 4.2762079Losses:  4.219672679901123 0.190351665019989
CurrentTrain: epoch  7, batch    17 | loss: 4.2196727Losses:  4.415682792663574 0.311731219291687
CurrentTrain: epoch  7, batch    18 | loss: 4.4156828Losses:  4.243808269500732 0.1448175311088562
CurrentTrain: epoch  7, batch    19 | loss: 4.2438083Losses:  4.3650641441345215 0.2695784568786621
CurrentTrain: epoch  7, batch    20 | loss: 4.3650641Losses:  4.342981815338135 0.2677881121635437
CurrentTrain: epoch  7, batch    21 | loss: 4.3429818Losses:  4.329799175262451 0.24145647883415222
CurrentTrain: epoch  7, batch    22 | loss: 4.3297992Losses:  4.299413204193115 0.25757455825805664
CurrentTrain: epoch  7, batch    23 | loss: 4.2994132Losses:  4.314054489135742 0.23558901250362396
CurrentTrain: epoch  7, batch    24 | loss: 4.3140545Losses:  4.399487018585205 0.2796608507633209
CurrentTrain: epoch  7, batch    25 | loss: 4.3994870Losses:  4.22521448135376 0.16130578517913818
CurrentTrain: epoch  7, batch    26 | loss: 4.2252145Losses:  4.290489673614502 0.22708100080490112
CurrentTrain: epoch  7, batch    27 | loss: 4.2904897Losses:  4.314289569854736 0.22260735929012299
CurrentTrain: epoch  7, batch    28 | loss: 4.3142896Losses:  4.273920059204102 0.184370219707489
CurrentTrain: epoch  7, batch    29 | loss: 4.2739201Losses:  4.244905471801758 0.1887647807598114
CurrentTrain: epoch  7, batch    30 | loss: 4.2449055Losses:  4.225636005401611 0.17050990462303162
CurrentTrain: epoch  7, batch    31 | loss: 4.2256360Losses:  4.226333141326904 0.19971314072608948
CurrentTrain: epoch  7, batch    32 | loss: 4.2263331Losses:  4.205729007720947 0.2060307413339615
CurrentTrain: epoch  7, batch    33 | loss: 4.2057290Losses:  4.225624084472656 0.20257019996643066
CurrentTrain: epoch  7, batch    34 | loss: 4.2256241Losses:  4.292412281036377 0.2285022735595703
CurrentTrain: epoch  7, batch    35 | loss: 4.2924123Losses:  4.250457286834717 0.22790460288524628
CurrentTrain: epoch  7, batch    36 | loss: 4.2504573Losses:  4.269906044006348 0.20731228590011597
CurrentTrain: epoch  7, batch    37 | loss: 4.2699060Losses:  4.318192481994629 0.2411009967327118
CurrentTrain: epoch  7, batch    38 | loss: 4.3181925Losses:  4.191910266876221 0.1914677917957306
CurrentTrain: epoch  7, batch    39 | loss: 4.1919103Losses:  4.213584899902344 0.1599481701850891
CurrentTrain: epoch  7, batch    40 | loss: 4.2135849Losses:  4.211944580078125 0.1675434410572052
CurrentTrain: epoch  7, batch    41 | loss: 4.2119446Losses:  4.198000907897949 0.179645374417305
CurrentTrain: epoch  7, batch    42 | loss: 4.1980009Losses:  4.318104267120361 0.26711446046829224
CurrentTrain: epoch  7, batch    43 | loss: 4.3181043Losses:  4.2878828048706055 0.21384023129940033
CurrentTrain: epoch  7, batch    44 | loss: 4.2878828Losses:  4.302567481994629 0.24441926181316376
CurrentTrain: epoch  7, batch    45 | loss: 4.3025675Losses:  4.28883171081543 0.2055595964193344
CurrentTrain: epoch  7, batch    46 | loss: 4.2888317Losses:  4.264741897583008 0.20112121105194092
CurrentTrain: epoch  7, batch    47 | loss: 4.2647419Losses:  4.334411144256592 0.25837242603302
CurrentTrain: epoch  7, batch    48 | loss: 4.3344111Losses:  4.281455039978027 0.2088795006275177
CurrentTrain: epoch  7, batch    49 | loss: 4.2814550Losses:  4.322575569152832 0.2585636079311371
CurrentTrain: epoch  7, batch    50 | loss: 4.3225756Losses:  4.334475994110107 0.22773656249046326
CurrentTrain: epoch  7, batch    51 | loss: 4.3344760Losses:  4.229959011077881 0.19027331471443176
CurrentTrain: epoch  7, batch    52 | loss: 4.2299590Losses:  4.217257499694824 0.17999675869941711
CurrentTrain: epoch  7, batch    53 | loss: 4.2172575Losses:  4.285637855529785 0.23658166825771332
CurrentTrain: epoch  7, batch    54 | loss: 4.2856379Losses:  4.354963779449463 0.2730461359024048
CurrentTrain: epoch  7, batch    55 | loss: 4.3549638Losses:  4.214953422546387 0.17290490865707397
CurrentTrain: epoch  7, batch    56 | loss: 4.2149534Losses:  4.335677623748779 0.22747202217578888
CurrentTrain: epoch  7, batch    57 | loss: 4.3356776Losses:  4.274145603179932 0.22394153475761414
CurrentTrain: epoch  7, batch    58 | loss: 4.2741456Losses:  4.313823223114014 0.25171518325805664
CurrentTrain: epoch  7, batch    59 | loss: 4.3138232Losses:  4.312813758850098 0.2598264813423157
CurrentTrain: epoch  7, batch    60 | loss: 4.3128138Losses:  4.157656192779541 0.17341789603233337
CurrentTrain: epoch  7, batch    61 | loss: 4.1576562Losses:  4.207128047943115 0.14171063899993896
CurrentTrain: epoch  7, batch    62 | loss: 4.2071280Losses:  4.2086591720581055 0.1540365219116211
CurrentTrain: epoch  8, batch     0 | loss: 4.2086592Losses:  4.243359565734863 0.19979265332221985
CurrentTrain: epoch  8, batch     1 | loss: 4.2433596Losses:  4.210935592651367 0.14715981483459473
CurrentTrain: epoch  8, batch     2 | loss: 4.2109356Losses:  4.1969451904296875 0.18097764253616333
CurrentTrain: epoch  8, batch     3 | loss: 4.1969452Losses:  4.362009048461914 0.2705885171890259
CurrentTrain: epoch  8, batch     4 | loss: 4.3620090Losses:  4.2449164390563965 0.1892620027065277
CurrentTrain: epoch  8, batch     5 | loss: 4.2449164Losses:  4.260314464569092 0.21226747334003448
CurrentTrain: epoch  8, batch     6 | loss: 4.2603145Losses:  4.225952625274658 0.20498663187026978
CurrentTrain: epoch  8, batch     7 | loss: 4.2259526Losses:  4.235051155090332 0.17530924081802368
CurrentTrain: epoch  8, batch     8 | loss: 4.2350512Losses:  4.120811462402344 0.12754280865192413
CurrentTrain: epoch  8, batch     9 | loss: 4.1208115Losses:  4.273119926452637 0.2290250062942505
CurrentTrain: epoch  8, batch    10 | loss: 4.2731199Losses:  4.269373416900635 0.24066752195358276
CurrentTrain: epoch  8, batch    11 | loss: 4.2693734Losses:  4.243921756744385 0.2453708052635193
CurrentTrain: epoch  8, batch    12 | loss: 4.2439218Losses:  4.261284828186035 0.2122829556465149
CurrentTrain: epoch  8, batch    13 | loss: 4.2612848Losses:  4.216018199920654 0.1772380769252777
CurrentTrain: epoch  8, batch    14 | loss: 4.2160182Losses:  4.18641996383667 0.14249464869499207
CurrentTrain: epoch  8, batch    15 | loss: 4.1864200Losses:  4.314520835876465 0.21994608640670776
CurrentTrain: epoch  8, batch    16 | loss: 4.3145208Losses:  4.237170696258545 0.20383867621421814
CurrentTrain: epoch  8, batch    17 | loss: 4.2371707Losses:  4.311293601989746 0.2381732165813446
CurrentTrain: epoch  8, batch    18 | loss: 4.3112936Losses:  4.283405780792236 0.24874812364578247
CurrentTrain: epoch  8, batch    19 | loss: 4.2834058Losses:  4.220416069030762 0.2078058272600174
CurrentTrain: epoch  8, batch    20 | loss: 4.2204161Losses:  4.273659706115723 0.2131730020046234
CurrentTrain: epoch  8, batch    21 | loss: 4.2736597Losses:  4.22343111038208 0.21641579270362854
CurrentTrain: epoch  8, batch    22 | loss: 4.2234311Losses:  4.228160858154297 0.21441811323165894
CurrentTrain: epoch  8, batch    23 | loss: 4.2281609Losses:  4.199482440948486 0.1675306260585785
CurrentTrain: epoch  8, batch    24 | loss: 4.1994824Losses:  4.140495777130127 0.15493032336235046
CurrentTrain: epoch  8, batch    25 | loss: 4.1404958Losses:  4.283700942993164 0.2411937117576599
CurrentTrain: epoch  8, batch    26 | loss: 4.2837009Losses:  4.101436138153076 0.10534946620464325
CurrentTrain: epoch  8, batch    27 | loss: 4.1014361Losses:  4.209832191467285 0.18899399042129517
CurrentTrain: epoch  8, batch    28 | loss: 4.2098322Losses:  4.146172523498535 0.15847991406917572
CurrentTrain: epoch  8, batch    29 | loss: 4.1461725Losses:  4.155076503753662 0.18078891932964325
CurrentTrain: epoch  8, batch    30 | loss: 4.1550765Losses:  4.113533020019531 0.16712573170661926
CurrentTrain: epoch  8, batch    31 | loss: 4.1135330Losses:  4.259643077850342 0.1850476711988449
CurrentTrain: epoch  8, batch    32 | loss: 4.2596431Losses:  4.294480800628662 0.22017717361450195
CurrentTrain: epoch  8, batch    33 | loss: 4.2944808Losses:  4.211606025695801 0.21636053919792175
CurrentTrain: epoch  8, batch    34 | loss: 4.2116060Losses:  4.240123748779297 0.23890963196754456
CurrentTrain: epoch  8, batch    35 | loss: 4.2401237Losses:  4.238796234130859 0.22591900825500488
CurrentTrain: epoch  8, batch    36 | loss: 4.2387962Losses:  4.249435901641846 0.20783287286758423
CurrentTrain: epoch  8, batch    37 | loss: 4.2494359Losses:  4.261063098907471 0.19319477677345276
CurrentTrain: epoch  8, batch    38 | loss: 4.2610631Losses:  4.192888259887695 0.17142309248447418
CurrentTrain: epoch  8, batch    39 | loss: 4.1928883Losses:  4.189637184143066 0.16276592016220093
CurrentTrain: epoch  8, batch    40 | loss: 4.1896372Losses:  4.326569557189941 0.2618970274925232
CurrentTrain: epoch  8, batch    41 | loss: 4.3265696Losses:  4.138912200927734 0.12881216406822205
CurrentTrain: epoch  8, batch    42 | loss: 4.1389122Losses:  4.1374030113220215 0.13228853046894073
CurrentTrain: epoch  8, batch    43 | loss: 4.1374030Losses:  4.207932949066162 0.2183440923690796
CurrentTrain: epoch  8, batch    44 | loss: 4.2079329Losses:  4.20835018157959 0.1819063425064087
CurrentTrain: epoch  8, batch    45 | loss: 4.2083502Losses:  4.172974586486816 0.1781734675168991
CurrentTrain: epoch  8, batch    46 | loss: 4.1729746Losses:  4.207523345947266 0.2053157091140747
CurrentTrain: epoch  8, batch    47 | loss: 4.2075233Losses:  4.220180034637451 0.20887216925621033
CurrentTrain: epoch  8, batch    48 | loss: 4.2201800Losses:  4.2621378898620605 0.2430129498243332
CurrentTrain: epoch  8, batch    49 | loss: 4.2621379Losses:  4.305767059326172 0.22417908906936646
CurrentTrain: epoch  8, batch    50 | loss: 4.3057671Losses:  4.262631893157959 0.23947401344776154
CurrentTrain: epoch  8, batch    51 | loss: 4.2626319Losses:  4.287290096282959 0.22850191593170166
CurrentTrain: epoch  8, batch    52 | loss: 4.2872901Losses:  4.263959884643555 0.2260417640209198
CurrentTrain: epoch  8, batch    53 | loss: 4.2639599Losses:  4.176361083984375 0.17121011018753052
CurrentTrain: epoch  8, batch    54 | loss: 4.1763611Losses:  4.2575178146362305 0.22779566049575806
CurrentTrain: epoch  8, batch    55 | loss: 4.2575178Losses:  4.176552772521973 0.16702567040920258
CurrentTrain: epoch  8, batch    56 | loss: 4.1765528Losses:  4.19191312789917 0.1874116063117981
CurrentTrain: epoch  8, batch    57 | loss: 4.1919131Losses:  4.276096820831299 0.2527678906917572
CurrentTrain: epoch  8, batch    58 | loss: 4.2760968Losses:  4.18112850189209 0.21018573641777039
CurrentTrain: epoch  8, batch    59 | loss: 4.1811285Losses:  4.193270206451416 0.19503000378608704
CurrentTrain: epoch  8, batch    60 | loss: 4.1932702Losses:  4.214121341705322 0.18659459054470062
CurrentTrain: epoch  8, batch    61 | loss: 4.2141213Losses:  4.1122260093688965 0.09696386754512787
CurrentTrain: epoch  8, batch    62 | loss: 4.1122260Losses:  4.1226701736450195 0.1558493673801422
CurrentTrain: epoch  9, batch     0 | loss: 4.1226702Losses:  4.175076484680176 0.17259228229522705
CurrentTrain: epoch  9, batch     1 | loss: 4.1750765Losses:  4.258222579956055 0.2295246571302414
CurrentTrain: epoch  9, batch     2 | loss: 4.2582226Losses:  4.165704250335693 0.17136895656585693
CurrentTrain: epoch  9, batch     3 | loss: 4.1657043Losses:  4.142510890960693 0.15343691408634186
CurrentTrain: epoch  9, batch     4 | loss: 4.1425109Losses:  4.197732448577881 0.1786995530128479
CurrentTrain: epoch  9, batch     5 | loss: 4.1977324Losses:  4.178277015686035 0.17266477644443512
CurrentTrain: epoch  9, batch     6 | loss: 4.1782770Losses:  4.220522403717041 0.1964823305606842
CurrentTrain: epoch  9, batch     7 | loss: 4.2205224Losses:  4.117592811584473 0.11591315269470215
CurrentTrain: epoch  9, batch     8 | loss: 4.1175928Losses:  4.1631011962890625 0.16362929344177246
CurrentTrain: epoch  9, batch     9 | loss: 4.1631012Losses:  4.230854511260986 0.20790550112724304
CurrentTrain: epoch  9, batch    10 | loss: 4.2308545Losses:  4.0839080810546875 0.1603577882051468
CurrentTrain: epoch  9, batch    11 | loss: 4.0839081Losses:  4.1006293296813965 0.11442790925502777
CurrentTrain: epoch  9, batch    12 | loss: 4.1006293Losses:  4.19948673248291 0.18089888989925385
CurrentTrain: epoch  9, batch    13 | loss: 4.1994867Losses:  4.197906017303467 0.19331806898117065
CurrentTrain: epoch  9, batch    14 | loss: 4.1979060Losses:  4.170493125915527 0.14507485926151276
CurrentTrain: epoch  9, batch    15 | loss: 4.1704931Losses:  4.231081008911133 0.20171532034873962
CurrentTrain: epoch  9, batch    16 | loss: 4.2310810Losses:  4.220874786376953 0.22363051772117615
CurrentTrain: epoch  9, batch    17 | loss: 4.2208748Losses:  4.190312385559082 0.16439303755760193
CurrentTrain: epoch  9, batch    18 | loss: 4.1903124Losses:  4.143836975097656 0.1287049800157547
CurrentTrain: epoch  9, batch    19 | loss: 4.1438370Losses:  4.172358989715576 0.1453072428703308
CurrentTrain: epoch  9, batch    20 | loss: 4.1723590Losses:  4.209420680999756 0.1768907904624939
CurrentTrain: epoch  9, batch    21 | loss: 4.2094207Losses:  4.219357967376709 0.19312092661857605
CurrentTrain: epoch  9, batch    22 | loss: 4.2193580Losses:  4.183277606964111 0.15232151746749878
CurrentTrain: epoch  9, batch    23 | loss: 4.1832776Losses:  4.237888813018799 0.24107609689235687
CurrentTrain: epoch  9, batch    24 | loss: 4.2378888Losses:  4.196411609649658 0.16755670309066772
CurrentTrain: epoch  9, batch    25 | loss: 4.1964116Losses:  4.154511451721191 0.2050723135471344
CurrentTrain: epoch  9, batch    26 | loss: 4.1545115Losses:  4.180750370025635 0.18405795097351074
CurrentTrain: epoch  9, batch    27 | loss: 4.1807504Losses:  4.170823574066162 0.18637463450431824
CurrentTrain: epoch  9, batch    28 | loss: 4.1708236Losses:  4.143716335296631 0.17656400799751282
CurrentTrain: epoch  9, batch    29 | loss: 4.1437163Losses:  4.167829990386963 0.16709201037883759
CurrentTrain: epoch  9, batch    30 | loss: 4.1678300Losses:  4.213295936584473 0.20219430327415466
CurrentTrain: epoch  9, batch    31 | loss: 4.2132959Losses:  4.2399163246154785 0.20901747047901154
CurrentTrain: epoch  9, batch    32 | loss: 4.2399163Losses:  4.132905006408691 0.13746438920497894
CurrentTrain: epoch  9, batch    33 | loss: 4.1329050Losses:  4.197041034698486 0.18066106736660004
CurrentTrain: epoch  9, batch    34 | loss: 4.1970410Losses:  4.1842570304870605 0.19081386923789978
CurrentTrain: epoch  9, batch    35 | loss: 4.1842570Losses:  4.193398475646973 0.14821402728557587
CurrentTrain: epoch  9, batch    36 | loss: 4.1933985Losses:  4.219015598297119 0.2056804746389389
CurrentTrain: epoch  9, batch    37 | loss: 4.2190156Losses:  4.191410541534424 0.19424206018447876
CurrentTrain: epoch  9, batch    38 | loss: 4.1914105Losses:  4.194779396057129 0.1872633546590805
CurrentTrain: epoch  9, batch    39 | loss: 4.1947794Losses:  4.174100399017334 0.20364578068256378
CurrentTrain: epoch  9, batch    40 | loss: 4.1741004Losses:  4.177502155303955 0.1720162332057953
CurrentTrain: epoch  9, batch    41 | loss: 4.1775022Losses:  4.311258316040039 0.22499622404575348
CurrentTrain: epoch  9, batch    42 | loss: 4.3112583Losses:  4.184454917907715 0.18121759593486786
CurrentTrain: epoch  9, batch    43 | loss: 4.1844549Losses:  4.180960178375244 0.17603042721748352
CurrentTrain: epoch  9, batch    44 | loss: 4.1809602Losses:  4.097351551055908 0.1265876293182373
CurrentTrain: epoch  9, batch    45 | loss: 4.0973516Losses:  4.22178316116333 0.21910762786865234
CurrentTrain: epoch  9, batch    46 | loss: 4.2217832Losses:  4.18684720993042 0.19509829580783844
CurrentTrain: epoch  9, batch    47 | loss: 4.1868472Losses:  4.183150768280029 0.2018505334854126
CurrentTrain: epoch  9, batch    48 | loss: 4.1831508Losses:  4.204328536987305 0.20360757410526276
CurrentTrain: epoch  9, batch    49 | loss: 4.2043285Losses:  4.186014175415039 0.13166628777980804
CurrentTrain: epoch  9, batch    50 | loss: 4.1860142Losses:  4.15031623840332 0.15953797101974487
CurrentTrain: epoch  9, batch    51 | loss: 4.1503162Losses:  4.182159423828125 0.17610761523246765
CurrentTrain: epoch  9, batch    52 | loss: 4.1821594Losses:  4.123169898986816 0.12522202730178833
CurrentTrain: epoch  9, batch    53 | loss: 4.1231699Losses:  4.098138809204102 0.11299960315227509
CurrentTrain: epoch  9, batch    54 | loss: 4.0981388Losses:  4.226480960845947 0.1956111192703247
CurrentTrain: epoch  9, batch    55 | loss: 4.2264810Losses:  4.16680908203125 0.17182457447052002
CurrentTrain: epoch  9, batch    56 | loss: 4.1668091Losses:  4.12274169921875 0.13951222598552704
CurrentTrain: epoch  9, batch    57 | loss: 4.1227417Losses:  4.169195652008057 0.1641393005847931
CurrentTrain: epoch  9, batch    58 | loss: 4.1691957Losses:  4.1883649826049805 0.13231003284454346
CurrentTrain: epoch  9, batch    59 | loss: 4.1883650Losses:  4.201320648193359 0.16333851218223572
CurrentTrain: epoch  9, batch    60 | loss: 4.2013206Losses:  4.158518314361572 0.17099252343177795
CurrentTrain: epoch  9, batch    61 | loss: 4.1585183Losses:  4.14242696762085 0.11928130686283112
CurrentTrain: epoch  9, batch    62 | loss: 4.1424270
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  7.78499698638916 1.079986333847046
CurrentTrain: epoch  0, batch     0 | loss: 7.7849970Losses:  7.313563823699951 1.1511402130126953
CurrentTrain: epoch  0, batch     1 | loss: 7.3135638Losses:  7.217039108276367 0.879524290561676
CurrentTrain: epoch  0, batch     2 | loss: 7.2170391Losses:  7.755195617675781 0.15062707662582397
CurrentTrain: epoch  0, batch     3 | loss: 7.7551956Losses:  7.756561279296875 1.2790063619613647
CurrentTrain: epoch  1, batch     0 | loss: 7.7565613Losses:  6.099781036376953 0.9469099640846252
CurrentTrain: epoch  1, batch     1 | loss: 6.0997810Losses:  5.311919689178467 0.9544539451599121
CurrentTrain: epoch  1, batch     2 | loss: 5.3119197Losses:  6.868320465087891 0.2327386885881424
CurrentTrain: epoch  1, batch     3 | loss: 6.8683205Losses:  5.4959306716918945 0.8178920745849609
CurrentTrain: epoch  2, batch     0 | loss: 5.4959307Losses:  5.851909637451172 1.0300788879394531
CurrentTrain: epoch  2, batch     1 | loss: 5.8519096Losses:  5.1481404304504395 0.9840182662010193
CurrentTrain: epoch  2, batch     2 | loss: 5.1481404Losses:  2.734597682952881 0.2418096363544464
CurrentTrain: epoch  2, batch     3 | loss: 2.7345977Losses:  5.476271152496338 0.9479125738143921
CurrentTrain: epoch  3, batch     0 | loss: 5.4762712Losses:  4.777297496795654 0.8193912506103516
CurrentTrain: epoch  3, batch     1 | loss: 4.7772975Losses:  4.408319473266602 0.6978282928466797
CurrentTrain: epoch  3, batch     2 | loss: 4.4083195Losses:  4.592593193054199 0.07788854092359543
CurrentTrain: epoch  3, batch     3 | loss: 4.5925932Losses:  4.0793304443359375 0.8676468729972839
CurrentTrain: epoch  4, batch     0 | loss: 4.0793304Losses:  4.916314125061035 0.9035462737083435
CurrentTrain: epoch  4, batch     1 | loss: 4.9163141Losses:  4.495597839355469 0.8273313641548157
CurrentTrain: epoch  4, batch     2 | loss: 4.4955978Losses:  4.847135066986084 0.4117101728916168
CurrentTrain: epoch  4, batch     3 | loss: 4.8471351Losses:  4.309391498565674 0.7907785177230835
CurrentTrain: epoch  5, batch     0 | loss: 4.3093915Losses:  3.7321643829345703 0.7887778282165527
CurrentTrain: epoch  5, batch     1 | loss: 3.7321644Losses:  4.251225471496582 0.8581076264381409
CurrentTrain: epoch  5, batch     2 | loss: 4.2512255Losses:  2.914921283721924 0.13163475692272186
CurrentTrain: epoch  5, batch     3 | loss: 2.9149213Losses:  3.616253614425659 0.8342649340629578
CurrentTrain: epoch  6, batch     0 | loss: 3.6162536Losses:  3.4017603397369385 0.6836097240447998
CurrentTrain: epoch  6, batch     1 | loss: 3.4017603Losses:  3.8889174461364746 0.7997413873672485
CurrentTrain: epoch  6, batch     2 | loss: 3.8889174Losses:  2.9286203384399414 0.0934959277510643
CurrentTrain: epoch  6, batch     3 | loss: 2.9286203Losses:  3.4103050231933594 0.6848428845405579
CurrentTrain: epoch  7, batch     0 | loss: 3.4103050Losses:  3.3519179821014404 0.6040084362030029
CurrentTrain: epoch  7, batch     1 | loss: 3.3519180Losses:  3.251913070678711 0.7142413854598999
CurrentTrain: epoch  7, batch     2 | loss: 3.2519131Losses:  2.5249388217926025 0.2959084212779999
CurrentTrain: epoch  7, batch     3 | loss: 2.5249388Losses:  3.2281241416931152 0.7770593166351318
CurrentTrain: epoch  8, batch     0 | loss: 3.2281241Losses:  3.4634556770324707 0.8566458225250244
CurrentTrain: epoch  8, batch     1 | loss: 3.4634557Losses:  2.795391082763672 0.5709084272384644
CurrentTrain: epoch  8, batch     2 | loss: 2.7953911Losses:  2.1494107246398926 0.04818759858608246
CurrentTrain: epoch  8, batch     3 | loss: 2.1494107Losses:  3.1072583198547363 0.7192445397377014
CurrentTrain: epoch  9, batch     0 | loss: 3.1072583Losses:  2.8725454807281494 0.6709039211273193
CurrentTrain: epoch  9, batch     1 | loss: 2.8725455Losses:  3.2329938411712646 0.7699124813079834
CurrentTrain: epoch  9, batch     2 | loss: 3.2329938Losses:  2.343776226043701 0.11456453800201416
CurrentTrain: epoch  9, batch     3 | loss: 2.3437762
Losses:  3.277170181274414 0.8213052153587341
MemoryTrain:  epoch  0, batch     0 | loss: 3.2771702Losses:  0.28782451152801514 0.23145395517349243
MemoryTrain:  epoch  0, batch     1 | loss: 0.2878245Losses:  3.101367473602295 0.8841065764427185
MemoryTrain:  epoch  1, batch     0 | loss: 3.1013675Losses:  0.15291866660118103 0.07320847362279892
MemoryTrain:  epoch  1, batch     1 | loss: 0.1529187Losses:  1.76869797706604 0.7912344336509705
MemoryTrain:  epoch  2, batch     0 | loss: 1.7686980Losses:  2.1261653900146484 0.1021386981010437
MemoryTrain:  epoch  2, batch     1 | loss: 2.1261654Losses:  2.101594924926758 0.7617241144180298
MemoryTrain:  epoch  3, batch     0 | loss: 2.1015949Losses:  0.34366583824157715 0.14954425394535065
MemoryTrain:  epoch  3, batch     1 | loss: 0.3436658Losses:  1.8374271392822266 0.7501515746116638
MemoryTrain:  epoch  4, batch     0 | loss: 1.8374271Losses:  0.1573132574558258 0.11629988253116608
MemoryTrain:  epoch  4, batch     1 | loss: 0.1573133Losses:  1.1264739036560059 0.6475921869277954
MemoryTrain:  epoch  5, batch     0 | loss: 1.1264739Losses:  1.8885430097579956 0.1467031091451645
MemoryTrain:  epoch  5, batch     1 | loss: 1.8885430Losses:  1.5381355285644531 0.6711012125015259
MemoryTrain:  epoch  6, batch     0 | loss: 1.5381355Losses:  0.6995902061462402 0.5299015641212463
MemoryTrain:  epoch  6, batch     1 | loss: 0.6995902Losses:  1.559652328491211 0.6975813508033752
MemoryTrain:  epoch  7, batch     0 | loss: 1.5596523Losses:  0.1731213629245758 0.137099489569664
MemoryTrain:  epoch  7, batch     1 | loss: 0.1731214Losses:  1.3468403816223145 0.6407145261764526
MemoryTrain:  epoch  8, batch     0 | loss: 1.3468404Losses:  0.38123175501823425 0.3161100149154663
MemoryTrain:  epoch  8, batch     1 | loss: 0.3812318Losses:  1.3205909729003906 0.6909874677658081
MemoryTrain:  epoch  9, batch     0 | loss: 1.3205910Losses:  0.2154327929019928 0.1690666675567627
MemoryTrain:  epoch  9, batch     1 | loss: 0.2154328
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.59%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 76.41%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.21%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 94.54%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 94.67%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 94.70%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 94.61%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 94.51%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 94.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 94.33%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 94.16%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 93.99%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 93.83%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 93.52%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 93.45%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 93.22%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 93.08%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 92.94%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 92.88%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 92.53%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 92.35%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 91.94%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 91.90%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 91.71%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 91.40%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 90.89%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 90.46%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 89.97%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 89.56%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 88.97%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 88.64%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 88.12%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.75%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 87.20%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.96%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 86.79%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 86.21%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 85.53%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 84.92%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 84.26%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 83.73%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 83.31%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 83.08%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.74%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.14%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.40%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.53%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 84.65%   
cur_acc:  ['0.9464', '0.7421']
his_acc:  ['0.9464', '0.8465']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  6.588973522186279 0.8238115906715393
CurrentTrain: epoch  0, batch     0 | loss: 6.5889735Losses:  8.47752857208252 1.089361310005188
CurrentTrain: epoch  0, batch     1 | loss: 8.4775286Losses:  7.0917863845825195 1.0115389823913574
CurrentTrain: epoch  0, batch     2 | loss: 7.0917864Losses:  8.949739456176758 0.336230993270874
CurrentTrain: epoch  0, batch     3 | loss: 8.9497395Losses:  7.39744758605957 1.2237763404846191
CurrentTrain: epoch  1, batch     0 | loss: 7.3974476Losses:  5.685296058654785 0.8634388446807861
CurrentTrain: epoch  1, batch     1 | loss: 5.6852961Losses:  7.651430606842041 1.1277546882629395
CurrentTrain: epoch  1, batch     2 | loss: 7.6514306Losses:  6.537590980529785 0.2147091031074524
CurrentTrain: epoch  1, batch     3 | loss: 6.5375910Losses:  6.005673408508301 0.7371442317962646
CurrentTrain: epoch  2, batch     0 | loss: 6.0056734Losses:  6.150445461273193 0.8410084843635559
CurrentTrain: epoch  2, batch     1 | loss: 6.1504455Losses:  6.293562889099121 1.08048415184021
CurrentTrain: epoch  2, batch     2 | loss: 6.2935629Losses:  6.167157173156738 0.34239667654037476
CurrentTrain: epoch  2, batch     3 | loss: 6.1671572Losses:  5.437617778778076 0.8812346458435059
CurrentTrain: epoch  3, batch     0 | loss: 5.4376178Losses:  5.482260227203369 0.8351740837097168
CurrentTrain: epoch  3, batch     1 | loss: 5.4822602Losses:  6.597228527069092 0.9999948740005493
CurrentTrain: epoch  3, batch     2 | loss: 6.5972285Losses:  6.323826313018799 0.3134419322013855
CurrentTrain: epoch  3, batch     3 | loss: 6.3238263Losses:  5.609204292297363 0.9811955094337463
CurrentTrain: epoch  4, batch     0 | loss: 5.6092043Losses:  5.126003742218018 0.991732120513916
CurrentTrain: epoch  4, batch     1 | loss: 5.1260037Losses:  6.32612419128418 1.0827608108520508
CurrentTrain: epoch  4, batch     2 | loss: 6.3261242Losses:  2.700286865234375 0.0762069895863533
CurrentTrain: epoch  4, batch     3 | loss: 2.7002869Losses:  5.428732395172119 1.011186957359314
CurrentTrain: epoch  5, batch     0 | loss: 5.4287324Losses:  4.481983661651611 0.8453978300094604
CurrentTrain: epoch  5, batch     1 | loss: 4.4819837Losses:  5.572988033294678 1.0073057413101196
CurrentTrain: epoch  5, batch     2 | loss: 5.5729880Losses:  5.302338600158691 0.14199921488761902
CurrentTrain: epoch  5, batch     3 | loss: 5.3023386Losses:  4.483802795410156 0.8601268529891968
CurrentTrain: epoch  6, batch     0 | loss: 4.4838028Losses:  5.453779697418213 0.9024063348770142
CurrentTrain: epoch  6, batch     1 | loss: 5.4537797Losses:  4.729424953460693 0.9166048765182495
CurrentTrain: epoch  6, batch     2 | loss: 4.7294250Losses:  4.7756547927856445 0.2339087873697281
CurrentTrain: epoch  6, batch     3 | loss: 4.7756548Losses:  3.843226909637451 0.7923403978347778
CurrentTrain: epoch  7, batch     0 | loss: 3.8432269Losses:  5.156599998474121 0.9039523601531982
CurrentTrain: epoch  7, batch     1 | loss: 5.1566000Losses:  4.7725653648376465 0.8788047432899475
CurrentTrain: epoch  7, batch     2 | loss: 4.7725654Losses:  4.958301544189453 0.11730499565601349
CurrentTrain: epoch  7, batch     3 | loss: 4.9583015Losses:  4.072846412658691 0.8608299493789673
CurrentTrain: epoch  8, batch     0 | loss: 4.0728464Losses:  5.215007781982422 0.9197988510131836
CurrentTrain: epoch  8, batch     1 | loss: 5.2150078Losses:  3.521212100982666 0.5655492544174194
CurrentTrain: epoch  8, batch     2 | loss: 3.5212121Losses:  4.456335067749023 0.3146551847457886
CurrentTrain: epoch  8, batch     3 | loss: 4.4563351Losses:  4.009735107421875 0.8134682178497314
CurrentTrain: epoch  9, batch     0 | loss: 4.0097351Losses:  3.8468027114868164 0.7790245413780212
CurrentTrain: epoch  9, batch     1 | loss: 3.8468027Losses:  4.908874988555908 1.0638787746429443
CurrentTrain: epoch  9, batch     2 | loss: 4.9088750Losses:  2.0092978477478027 0.09802456200122833
CurrentTrain: epoch  9, batch     3 | loss: 2.0092978
Losses:  1.4062080383300781 0.8483612537384033
MemoryTrain:  epoch  0, batch     0 | loss: 1.4062080Losses:  2.190728187561035 0.9216232895851135
MemoryTrain:  epoch  0, batch     1 | loss: 2.1907282Losses:  1.6159684658050537 0.9303905963897705
MemoryTrain:  epoch  1, batch     0 | loss: 1.6159685Losses:  2.034226179122925 0.7856059670448303
MemoryTrain:  epoch  1, batch     1 | loss: 2.0342262Losses:  1.4973498582839966 0.77338045835495
MemoryTrain:  epoch  2, batch     0 | loss: 1.4973499Losses:  1.1336781978607178 0.8401525020599365
MemoryTrain:  epoch  2, batch     1 | loss: 1.1336782Losses:  1.1022164821624756 0.7964311838150024
MemoryTrain:  epoch  3, batch     0 | loss: 1.1022165Losses:  1.3359431028366089 0.8450754284858704
MemoryTrain:  epoch  3, batch     1 | loss: 1.3359431Losses:  1.2461414337158203 0.9628564119338989
MemoryTrain:  epoch  4, batch     0 | loss: 1.2461414Losses:  0.7529110908508301 0.5703723430633545
MemoryTrain:  epoch  4, batch     1 | loss: 0.7529111Losses:  1.0727667808532715 0.9212018847465515
MemoryTrain:  epoch  5, batch     0 | loss: 1.0727668Losses:  0.6678195595741272 0.5844910144805908
MemoryTrain:  epoch  5, batch     1 | loss: 0.6678196Losses:  0.9437243938446045 0.7753621339797974
MemoryTrain:  epoch  6, batch     0 | loss: 0.9437244Losses:  0.7745046019554138 0.7382968664169312
MemoryTrain:  epoch  6, batch     1 | loss: 0.7745046Losses:  0.9024397730827332 0.8611986637115479
MemoryTrain:  epoch  7, batch     0 | loss: 0.9024398Losses:  0.6780104041099548 0.5794751048088074
MemoryTrain:  epoch  7, batch     1 | loss: 0.6780104Losses:  0.7995192408561707 0.7576096057891846
MemoryTrain:  epoch  8, batch     0 | loss: 0.7995192Losses:  0.6993617415428162 0.656182050704956
MemoryTrain:  epoch  8, batch     1 | loss: 0.6993617Losses:  0.7767255306243896 0.7372830510139465
MemoryTrain:  epoch  9, batch     0 | loss: 0.7767255Losses:  0.7460084557533264 0.6964586973190308
MemoryTrain:  epoch  9, batch     1 | loss: 0.7460085
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 74.76%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 74.87%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 74.76%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.50%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.91%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.98%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.78%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.58%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.26%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.23%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 92.07%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.12%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.94%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.72%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.59%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.38%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 91.17%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.13%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 91.01%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.66%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 90.48%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 90.29%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 90.12%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 89.73%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 89.54%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 89.31%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 88.98%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 88.63%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 88.16%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 87.63%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 87.24%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.73%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 86.30%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 85.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.40%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 85.17%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.89%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.68%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 84.52%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.32%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.88%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 83.16%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.57%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 81.93%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.42%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.03%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.75%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.72%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 81.71%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.81%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 81.96%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.95%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.84%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 81.69%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 82.52%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 82.06%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 81.65%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 81.21%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 80.94%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 80.38%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.52%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.74%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 80.71%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 80.35%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 79.98%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 79.55%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 79.11%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 78.69%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 78.62%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.92%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.90%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 79.96%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 79.94%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 79.87%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 79.81%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 79.71%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 79.59%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 79.57%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 79.63%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 79.68%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 79.45%   
cur_acc:  ['0.9464', '0.7421', '0.7450']
his_acc:  ['0.9464', '0.8465', '0.7945']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  6.066640853881836 0.809952974319458
CurrentTrain: epoch  0, batch     0 | loss: 6.0666409Losses:  5.449679851531982 0.7035824656486511
CurrentTrain: epoch  0, batch     1 | loss: 5.4496799Losses:  4.737313747406006 0.6874128580093384
CurrentTrain: epoch  0, batch     2 | loss: 4.7373137Losses:  3.8081915378570557 0.05336277186870575
CurrentTrain: epoch  0, batch     3 | loss: 3.8081915Losses:  5.460174083709717 0.7664428949356079
CurrentTrain: epoch  1, batch     0 | loss: 5.4601741Losses:  4.558547019958496 0.6054779291152954
CurrentTrain: epoch  1, batch     1 | loss: 4.5585470Losses:  3.72230863571167 0.6422410011291504
CurrentTrain: epoch  1, batch     2 | loss: 3.7223086Losses:  3.094129800796509 0.249955415725708
CurrentTrain: epoch  1, batch     3 | loss: 3.0941298Losses:  4.377918243408203 0.6989259719848633
CurrentTrain: epoch  2, batch     0 | loss: 4.3779182Losses:  4.107435703277588 0.7426778674125671
CurrentTrain: epoch  2, batch     1 | loss: 4.1074357Losses:  3.237366199493408 0.621017336845398
CurrentTrain: epoch  2, batch     2 | loss: 3.2373662Losses:  1.7926280498504639 0.0
CurrentTrain: epoch  2, batch     3 | loss: 1.7926280Losses:  3.893566131591797 0.7028188705444336
CurrentTrain: epoch  3, batch     0 | loss: 3.8935661Losses:  3.5517148971557617 0.579940676689148
CurrentTrain: epoch  3, batch     1 | loss: 3.5517149Losses:  3.069669723510742 0.4926995038986206
CurrentTrain: epoch  3, batch     2 | loss: 3.0696697Losses:  2.3370723724365234 0.032116882503032684
CurrentTrain: epoch  3, batch     3 | loss: 2.3370724Losses:  3.2214653491973877 0.5439075827598572
CurrentTrain: epoch  4, batch     0 | loss: 3.2214653Losses:  3.4406511783599854 0.6051661968231201
CurrentTrain: epoch  4, batch     1 | loss: 3.4406512Losses:  3.6163649559020996 0.6792465448379517
CurrentTrain: epoch  4, batch     2 | loss: 3.6163650Losses:  2.068190097808838 0.10836998373270035
CurrentTrain: epoch  4, batch     3 | loss: 2.0681901Losses:  2.77982759475708 0.5689169764518738
CurrentTrain: epoch  5, batch     0 | loss: 2.7798276Losses:  3.328629970550537 0.6393181085586548
CurrentTrain: epoch  5, batch     1 | loss: 3.3286300Losses:  3.5111100673675537 0.4807870388031006
CurrentTrain: epoch  5, batch     2 | loss: 3.5111101Losses:  2.1959307193756104 0.05433350056409836
CurrentTrain: epoch  5, batch     3 | loss: 2.1959307Losses:  2.935206651687622 0.5276567935943604
CurrentTrain: epoch  6, batch     0 | loss: 2.9352067Losses:  2.9621071815490723 0.47381120920181274
CurrentTrain: epoch  6, batch     1 | loss: 2.9621072Losses:  3.312283515930176 0.6797993183135986
CurrentTrain: epoch  6, batch     2 | loss: 3.3122835Losses:  1.7633088827133179 0.009955504909157753
CurrentTrain: epoch  6, batch     3 | loss: 1.7633089Losses:  2.883281707763672 0.5563993453979492
CurrentTrain: epoch  7, batch     0 | loss: 2.8832817Losses:  3.2543365955352783 0.6212461590766907
CurrentTrain: epoch  7, batch     1 | loss: 3.2543366Losses:  2.639726161956787 0.4562431275844574
CurrentTrain: epoch  7, batch     2 | loss: 2.6397262Losses:  1.8165595531463623 0.03108559548854828
CurrentTrain: epoch  7, batch     3 | loss: 1.8165596Losses:  2.8654673099517822 0.44464612007141113
CurrentTrain: epoch  8, batch     0 | loss: 2.8654673Losses:  2.8620495796203613 0.5953915119171143
CurrentTrain: epoch  8, batch     1 | loss: 2.8620496Losses:  2.52341628074646 0.4772021472454071
CurrentTrain: epoch  8, batch     2 | loss: 2.5234163Losses:  2.152466297149658 0.09196746349334717
CurrentTrain: epoch  8, batch     3 | loss: 2.1524663Losses:  2.758486032485962 0.5128905773162842
CurrentTrain: epoch  9, batch     0 | loss: 2.7584860Losses:  2.559124231338501 0.4629320502281189
CurrentTrain: epoch  9, batch     1 | loss: 2.5591242Losses:  2.4215407371520996 0.5050697326660156
CurrentTrain: epoch  9, batch     2 | loss: 2.4215407Losses:  1.756260633468628 0.030825169757008553
CurrentTrain: epoch  9, batch     3 | loss: 1.7562606
Losses:  1.6699669361114502 0.8890145421028137
MemoryTrain:  epoch  0, batch     0 | loss: 1.6699669Losses:  1.2286046743392944 0.7590094208717346
MemoryTrain:  epoch  0, batch     1 | loss: 1.2286047Losses:  1.6134746074676514 0.37735599279403687
MemoryTrain:  epoch  0, batch     2 | loss: 1.6134746Losses:  1.0512176752090454 0.7623733878135681
MemoryTrain:  epoch  1, batch     0 | loss: 1.0512177Losses:  1.7998729944229126 0.7880967855453491
MemoryTrain:  epoch  1, batch     1 | loss: 1.7998730Losses:  0.9337592124938965 0.36726218461990356
MemoryTrain:  epoch  1, batch     2 | loss: 0.9337592Losses:  1.244222640991211 0.7547165155410767
MemoryTrain:  epoch  2, batch     0 | loss: 1.2442226Losses:  0.8827043771743774 0.617374062538147
MemoryTrain:  epoch  2, batch     1 | loss: 0.8827044Losses:  1.0732442140579224 0.4591016471385956
MemoryTrain:  epoch  2, batch     2 | loss: 1.0732442Losses:  0.6198922991752625 0.552353024482727
MemoryTrain:  epoch  3, batch     0 | loss: 0.6198923Losses:  1.025515079498291 0.800954282283783
MemoryTrain:  epoch  3, batch     1 | loss: 1.0255151Losses:  1.487882375717163 0.6067787408828735
MemoryTrain:  epoch  3, batch     2 | loss: 1.4878824Losses:  0.9054907560348511 0.6718887090682983
MemoryTrain:  epoch  4, batch     0 | loss: 0.9054908Losses:  0.7249560356140137 0.62192702293396
MemoryTrain:  epoch  4, batch     1 | loss: 0.7249560Losses:  0.5177581310272217 0.46082645654678345
MemoryTrain:  epoch  4, batch     2 | loss: 0.5177581Losses:  0.7841655611991882 0.7362507581710815
MemoryTrain:  epoch  5, batch     0 | loss: 0.7841656Losses:  0.8208240866661072 0.6410611271858215
MemoryTrain:  epoch  5, batch     1 | loss: 0.8208241Losses:  0.491117000579834 0.43783038854599
MemoryTrain:  epoch  5, batch     2 | loss: 0.4911170Losses:  0.7665758728981018 0.7239605188369751
MemoryTrain:  epoch  6, batch     0 | loss: 0.7665759Losses:  0.6920439004898071 0.643028974533081
MemoryTrain:  epoch  6, batch     1 | loss: 0.6920439Losses:  0.37453049421310425 0.34854456782341003
MemoryTrain:  epoch  6, batch     2 | loss: 0.3745305Losses:  0.7154020071029663 0.6899181604385376
MemoryTrain:  epoch  7, batch     0 | loss: 0.7154020Losses:  0.6741364002227783 0.5994848012924194
MemoryTrain:  epoch  7, batch     1 | loss: 0.6741364Losses:  0.37246906757354736 0.33631405234336853
MemoryTrain:  epoch  7, batch     2 | loss: 0.3724691Losses:  0.7713289260864258 0.7425636053085327
MemoryTrain:  epoch  8, batch     0 | loss: 0.7713289Losses:  0.602386474609375 0.569796085357666
MemoryTrain:  epoch  8, batch     1 | loss: 0.6023865Losses:  0.3284986913204193 0.2918435335159302
MemoryTrain:  epoch  8, batch     2 | loss: 0.3284987Losses:  0.8409703373908997 0.8076634407043457
MemoryTrain:  epoch  9, batch     0 | loss: 0.8409703Losses:  0.6152921319007874 0.5733767747879028
MemoryTrain:  epoch  9, batch     1 | loss: 0.6152921Losses:  0.25211358070373535 0.23542529344558716
MemoryTrain:  epoch  9, batch     2 | loss: 0.2521136
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 56.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.91%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.48%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.68%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.69%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 90.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 90.58%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 90.37%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 90.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.13%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.94%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.82%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.64%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 89.41%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 89.01%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.76%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.53%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 88.37%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 88.00%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 87.93%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.71%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 87.57%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 87.36%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 87.09%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 86.69%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 86.30%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 85.09%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 84.73%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 84.18%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 83.65%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 83.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.80%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 82.66%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.40%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.08%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.54%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.90%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.33%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 79.28%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.91%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.65%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.79%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 79.80%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 79.87%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.09%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.20%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.17%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 80.13%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 80.04%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.89%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 80.44%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 80.13%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 79.70%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 79.36%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 79.15%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.82%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 79.18%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 78.74%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 78.27%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 77.84%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 77.42%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 77.00%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 78.16%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.18%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 78.20%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.00%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 77.84%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 77.72%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 77.64%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 77.52%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 77.37%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.39%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 77.45%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 77.35%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 77.24%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 77.23%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 77.28%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.54%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.64%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 77.40%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 77.14%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 76.94%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 76.67%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 76.41%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 76.22%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 76.01%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.91%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 75.66%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 75.51%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 75.30%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 75.09%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 74.85%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.82%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 75.45%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.40%   
cur_acc:  ['0.9464', '0.7421', '0.7450', '0.7748']
his_acc:  ['0.9464', '0.8465', '0.7945', '0.7740']
Clustering into  24  clusters
Clusters:  [ 2  8  1  0 14 23  1 22  3  3  1  3 21 18 11  4 10 18 12  4  2  4  0  7
  0  8  9  0  6 15  1  9  2  0  5  2 19 16 22  6 20 17  7  1 22  0  2  1
 13 10]
Losses:  7.5546135902404785 1.0610260963439941
CurrentTrain: epoch  0, batch     0 | loss: 7.5546136Losses:  7.341825485229492 1.0973236560821533
CurrentTrain: epoch  0, batch     1 | loss: 7.3418255Losses:  7.401348114013672 1.031340479850769
CurrentTrain: epoch  0, batch     2 | loss: 7.4013481Losses:  6.6540207862854 0.10108041763305664
CurrentTrain: epoch  0, batch     3 | loss: 6.6540208Losses:  6.1887898445129395 0.8329142332077026
CurrentTrain: epoch  1, batch     0 | loss: 6.1887898Losses:  6.744751930236816 1.124380111694336
CurrentTrain: epoch  1, batch     1 | loss: 6.7447519Losses:  6.767275810241699 0.9664878249168396
CurrentTrain: epoch  1, batch     2 | loss: 6.7672758Losses:  4.357736587524414 0.12228564918041229
CurrentTrain: epoch  1, batch     3 | loss: 4.3577366Losses:  6.854912281036377 1.077050805091858
CurrentTrain: epoch  2, batch     0 | loss: 6.8549123Losses:  5.525596618652344 0.975335955619812
CurrentTrain: epoch  2, batch     1 | loss: 5.5255966Losses:  4.664904594421387 0.8616420030593872
CurrentTrain: epoch  2, batch     2 | loss: 4.6649046Losses:  4.831346035003662 0.1132420152425766
CurrentTrain: epoch  2, batch     3 | loss: 4.8313460Losses:  5.759876728057861 0.8791036009788513
CurrentTrain: epoch  3, batch     0 | loss: 5.7598767Losses:  4.683698654174805 0.9473921060562134
CurrentTrain: epoch  3, batch     1 | loss: 4.6836987Losses:  5.360232830047607 0.9064353704452515
CurrentTrain: epoch  3, batch     2 | loss: 5.3602328Losses:  2.921576976776123 0.14653818309307098
CurrentTrain: epoch  3, batch     3 | loss: 2.9215770Losses:  5.226015090942383 0.7216131687164307
CurrentTrain: epoch  4, batch     0 | loss: 5.2260151Losses:  4.776708602905273 0.9105799198150635
CurrentTrain: epoch  4, batch     1 | loss: 4.7767086Losses:  4.5806989669799805 0.8375126123428345
CurrentTrain: epoch  4, batch     2 | loss: 4.5806990Losses:  3.759875774383545 0.08830589056015015
CurrentTrain: epoch  4, batch     3 | loss: 3.7598758Losses:  5.570304870605469 0.987015962600708
CurrentTrain: epoch  5, batch     0 | loss: 5.5703049Losses:  4.0485663414001465 0.8074706792831421
CurrentTrain: epoch  5, batch     1 | loss: 4.0485663Losses:  4.591463088989258 0.8958275318145752
CurrentTrain: epoch  5, batch     2 | loss: 4.5914631Losses:  2.420788288116455 0.11632964760065079
CurrentTrain: epoch  5, batch     3 | loss: 2.4207883Losses:  5.005623817443848 0.9372614026069641
CurrentTrain: epoch  6, batch     0 | loss: 5.0056238Losses:  3.7813355922698975 0.7079112529754639
CurrentTrain: epoch  6, batch     1 | loss: 3.7813356Losses:  4.358480453491211 0.7810219526290894
CurrentTrain: epoch  6, batch     2 | loss: 4.3584805Losses:  2.051457405090332 0.0969306007027626
CurrentTrain: epoch  6, batch     3 | loss: 2.0514574Losses:  4.224752902984619 0.8791495561599731
CurrentTrain: epoch  7, batch     0 | loss: 4.2247529Losses:  4.067052364349365 0.6676442623138428
CurrentTrain: epoch  7, batch     1 | loss: 4.0670524Losses:  4.143980503082275 0.9204474687576294
CurrentTrain: epoch  7, batch     2 | loss: 4.1439805Losses:  4.782995223999023 0.32952365279197693
CurrentTrain: epoch  7, batch     3 | loss: 4.7829952Losses:  4.32689905166626 0.8419734239578247
CurrentTrain: epoch  8, batch     0 | loss: 4.3268991Losses:  3.7878079414367676 0.7807350158691406
CurrentTrain: epoch  8, batch     1 | loss: 3.7878079Losses:  3.809258460998535 0.7142128944396973
CurrentTrain: epoch  8, batch     2 | loss: 3.8092585Losses:  2.590909481048584 0.15576469898223877
CurrentTrain: epoch  8, batch     3 | loss: 2.5909095Losses:  3.5559871196746826 0.7433291077613831
CurrentTrain: epoch  9, batch     0 | loss: 3.5559871Losses:  4.05558967590332 0.7805171012878418
CurrentTrain: epoch  9, batch     1 | loss: 4.0555897Losses:  4.019664287567139 0.9391937851905823
CurrentTrain: epoch  9, batch     2 | loss: 4.0196643Losses:  2.594094753265381 0.20478829741477966
CurrentTrain: epoch  9, batch     3 | loss: 2.5940948
Losses:  1.6818594932556152 0.9622939229011536
MemoryTrain:  epoch  0, batch     0 | loss: 1.6818595Losses:  1.2843668460845947 0.7760022282600403
MemoryTrain:  epoch  0, batch     1 | loss: 1.2843668Losses:  0.9534978866577148 0.7003496885299683
MemoryTrain:  epoch  0, batch     2 | loss: 0.9534979Losses:  1.440435528755188 0.09894473105669022
MemoryTrain:  epoch  0, batch     3 | loss: 1.4404355Losses:  1.4681260585784912 0.8142175078392029
MemoryTrain:  epoch  1, batch     0 | loss: 1.4681261Losses:  1.2417384386062622 0.7239158153533936
MemoryTrain:  epoch  1, batch     1 | loss: 1.2417384Losses:  1.2711191177368164 0.7514635920524597
MemoryTrain:  epoch  1, batch     2 | loss: 1.2711191Losses:  0.5170044898986816 0.06998705863952637
MemoryTrain:  epoch  1, batch     3 | loss: 0.5170045Losses:  1.0028082132339478 0.7227299213409424
MemoryTrain:  epoch  2, batch     0 | loss: 1.0028082Losses:  1.2897228002548218 0.8323339223861694
MemoryTrain:  epoch  2, batch     1 | loss: 1.2897228Losses:  0.7579268217086792 0.6142623424530029
MemoryTrain:  epoch  2, batch     2 | loss: 0.7579268Losses:  0.15274569392204285 0.06975755095481873
MemoryTrain:  epoch  2, batch     3 | loss: 0.1527457Losses:  1.1467690467834473 0.8652396202087402
MemoryTrain:  epoch  3, batch     0 | loss: 1.1467690Losses:  0.8125623464584351 0.7502120137214661
MemoryTrain:  epoch  3, batch     1 | loss: 0.8125623Losses:  0.7082544565200806 0.6295559406280518
MemoryTrain:  epoch  3, batch     2 | loss: 0.7082545Losses:  0.06915967166423798 0.04835778474807739
MemoryTrain:  epoch  3, batch     3 | loss: 0.0691597Losses:  0.7978923916816711 0.6887483596801758
MemoryTrain:  epoch  4, batch     0 | loss: 0.7978924Losses:  0.5997791290283203 0.5562108755111694
MemoryTrain:  epoch  4, batch     1 | loss: 0.5997791Losses:  0.7717716693878174 0.7167230844497681
MemoryTrain:  epoch  4, batch     2 | loss: 0.7717717Losses:  0.25779253244400024 0.20025420188903809
MemoryTrain:  epoch  4, batch     3 | loss: 0.2577925Losses:  0.7276005148887634 0.6337466239929199
MemoryTrain:  epoch  5, batch     0 | loss: 0.7276005Losses:  0.6974594593048096 0.6354754567146301
MemoryTrain:  epoch  5, batch     1 | loss: 0.6974595Losses:  0.7909871339797974 0.7475991249084473
MemoryTrain:  epoch  5, batch     2 | loss: 0.7909871Losses:  0.036931946873664856 0.02017155848443508
MemoryTrain:  epoch  5, batch     3 | loss: 0.0369319Losses:  0.8066344857215881 0.757425844669342
MemoryTrain:  epoch  6, batch     0 | loss: 0.8066345Losses:  0.634758710861206 0.5677236318588257
MemoryTrain:  epoch  6, batch     1 | loss: 0.6347587Losses:  0.6723439693450928 0.618394136428833
MemoryTrain:  epoch  6, batch     2 | loss: 0.6723440Losses:  0.29310160875320435 0.1359950304031372
MemoryTrain:  epoch  6, batch     3 | loss: 0.2931016Losses:  0.7058938145637512 0.6481328010559082
MemoryTrain:  epoch  7, batch     0 | loss: 0.7058938Losses:  0.8142825365066528 0.7546131610870361
MemoryTrain:  epoch  7, batch     1 | loss: 0.8142825Losses:  0.5864723920822144 0.5527364015579224
MemoryTrain:  epoch  7, batch     2 | loss: 0.5864724Losses:  0.09874890744686127 0.07627756893634796
MemoryTrain:  epoch  7, batch     3 | loss: 0.0987489Losses:  0.6683202981948853 0.6346367597579956
MemoryTrain:  epoch  8, batch     0 | loss: 0.6683203Losses:  0.6544969081878662 0.6202060580253601
MemoryTrain:  epoch  8, batch     1 | loss: 0.6544969Losses:  0.7324965596199036 0.6945099234580994
MemoryTrain:  epoch  8, batch     2 | loss: 0.7324966Losses:  0.05581073462963104 0.014944706112146378
MemoryTrain:  epoch  8, batch     3 | loss: 0.0558107Losses:  0.6572874784469604 0.6253103017807007
MemoryTrain:  epoch  9, batch     0 | loss: 0.6572875Losses:  0.5577434301376343 0.5367687940597534
MemoryTrain:  epoch  9, batch     1 | loss: 0.5577434Losses:  0.7262770533561707 0.702426552772522
MemoryTrain:  epoch  9, batch     2 | loss: 0.7262771Losses:  0.10397666692733765 0.04150298237800598
MemoryTrain:  epoch  9, batch     3 | loss: 0.1039767
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 38.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 42.86%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 44.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 44.92%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 46.32%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 47.57%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.50%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 56.05%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 56.05%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 56.06%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 54.60%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 53.93%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 52.78%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 51.35%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 51.32%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 51.76%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 52.44%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 53.27%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 53.49%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 53.69%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 53.80%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 54.39%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 54.82%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 55.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 56.62%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 58.25%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.99%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.68%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.03%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.35%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.94%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 89.65%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.52%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 89.37%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.34%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.55%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 89.52%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 89.24%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.10%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 88.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 88.47%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 88.22%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.05%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.89%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 87.73%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.42%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 86.90%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 86.53%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 86.03%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 85.68%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 84.99%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 84.73%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 84.34%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 84.03%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 83.72%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 83.36%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 82.86%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.38%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 81.91%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 81.32%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 80.99%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 80.48%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 79.99%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 79.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.21%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 78.88%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.63%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.10%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.49%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.95%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.36%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 75.84%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 75.50%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.28%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 76.59%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 76.76%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 76.85%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 76.77%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 76.71%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 77.45%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.34%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.01%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 76.55%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 76.14%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.92%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.08%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 75.66%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.16%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 74.80%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.31%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.88%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.96%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 75.07%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.19%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 75.43%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.25%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 75.07%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 74.79%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 74.93%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 74.90%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 74.87%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 74.80%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 74.83%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 74.90%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 75.13%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 74.81%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 74.47%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 74.14%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 73.94%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 73.71%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.56%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 73.33%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 73.22%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 73.00%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.58%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.57%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 73.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 74.95%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 75.65%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 75.45%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 75.20%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 74.95%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 74.75%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 74.54%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 74.42%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 74.37%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.35%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 74.25%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 74.10%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 73.97%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 74.17%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 74.01%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 73.79%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 73.67%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 73.60%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 73.54%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 73.31%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 73.16%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 72.95%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 72.69%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 72.42%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 72.46%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.46%   
cur_acc:  ['0.9464', '0.7421', '0.7450', '0.7748', '0.6399']
his_acc:  ['0.9464', '0.8465', '0.7945', '0.7740', '0.7346']
Clustering into  29  clusters
Clusters:  [11  4  0  5 14 28  0  5 22 22  0 22 24  9 15  8 10  9  1  8 21  8 25 16
  5 18  3  5 26 27  0  3  3  4 23 11 19 17  5 26  4  7  6  0  5 12 21 10
 13 10 12  8  2 20  6  1  2 10  4  3]
Losses:  7.947210788726807 0.9138686060905457
CurrentTrain: epoch  0, batch     0 | loss: 7.9472108Losses:  7.645237445831299 0.9515599608421326
CurrentTrain: epoch  0, batch     1 | loss: 7.6452374Losses:  8.079808235168457 0.858899712562561
CurrentTrain: epoch  0, batch     2 | loss: 8.0798082Losses:  8.365564346313477 0.666246235370636
CurrentTrain: epoch  0, batch     3 | loss: 8.3655643Losses:  6.882524490356445 0.8415906429290771
CurrentTrain: epoch  1, batch     0 | loss: 6.8825245Losses:  7.320022106170654 1.0127354860305786
CurrentTrain: epoch  1, batch     1 | loss: 7.3200221Losses:  5.884944915771484 0.8484232425689697
CurrentTrain: epoch  1, batch     2 | loss: 5.8849449Losses:  5.930311679840088 0.15494653582572937
CurrentTrain: epoch  1, batch     3 | loss: 5.9303117Losses:  5.9894232749938965 0.8079733848571777
CurrentTrain: epoch  2, batch     0 | loss: 5.9894233Losses:  5.928288459777832 0.8669350147247314
CurrentTrain: epoch  2, batch     1 | loss: 5.9282885Losses:  5.821495056152344 0.9201725721359253
CurrentTrain: epoch  2, batch     2 | loss: 5.8214951Losses:  3.0625410079956055 0.0
CurrentTrain: epoch  2, batch     3 | loss: 3.0625410Losses:  5.457037925720215 0.9122951030731201
CurrentTrain: epoch  3, batch     0 | loss: 5.4570379Losses:  4.783536434173584 0.8265470862388611
CurrentTrain: epoch  3, batch     1 | loss: 4.7835364Losses:  5.309021472930908 0.7029925584793091
CurrentTrain: epoch  3, batch     2 | loss: 5.3090215Losses:  6.014611721038818 0.4868292510509491
CurrentTrain: epoch  3, batch     3 | loss: 6.0146117Losses:  4.6670823097229 0.811119019985199
CurrentTrain: epoch  4, batch     0 | loss: 4.6670823Losses:  5.382961273193359 0.8750572204589844
CurrentTrain: epoch  4, batch     1 | loss: 5.3829613Losses:  4.949275970458984 0.874006986618042
CurrentTrain: epoch  4, batch     2 | loss: 4.9492760Losses:  2.591740846633911 0.1807626187801361
CurrentTrain: epoch  4, batch     3 | loss: 2.5917408Losses:  3.8060972690582275 0.6948636174201965
CurrentTrain: epoch  5, batch     0 | loss: 3.8060973Losses:  4.688499450683594 0.864063560962677
CurrentTrain: epoch  5, batch     1 | loss: 4.6884995Losses:  5.767022609710693 0.8860896229743958
CurrentTrain: epoch  5, batch     2 | loss: 5.7670226Losses:  2.3018226623535156 0.11481805890798569
CurrentTrain: epoch  5, batch     3 | loss: 2.3018227Losses:  4.255720615386963 0.8266543745994568
CurrentTrain: epoch  6, batch     0 | loss: 4.2557206Losses:  4.647058486938477 0.7468796372413635
CurrentTrain: epoch  6, batch     1 | loss: 4.6470585Losses:  4.08872127532959 0.6676089763641357
CurrentTrain: epoch  6, batch     2 | loss: 4.0887213Losses:  3.8135364055633545 0.14963656663894653
CurrentTrain: epoch  6, batch     3 | loss: 3.8135364Losses:  4.8028717041015625 0.8125193119049072
CurrentTrain: epoch  7, batch     0 | loss: 4.8028717Losses:  4.027673244476318 0.8141829967498779
CurrentTrain: epoch  7, batch     1 | loss: 4.0276732Losses:  3.6249351501464844 0.7378782033920288
CurrentTrain: epoch  7, batch     2 | loss: 3.6249352Losses:  1.9525419473648071 0.046691060066223145
CurrentTrain: epoch  7, batch     3 | loss: 1.9525419Losses:  4.093597412109375 0.6604775786399841
CurrentTrain: epoch  8, batch     0 | loss: 4.0935974Losses:  3.843379259109497 0.7105119824409485
CurrentTrain: epoch  8, batch     1 | loss: 3.8433793Losses:  3.9082517623901367 0.8862797021865845
CurrentTrain: epoch  8, batch     2 | loss: 3.9082518Losses:  1.8344939947128296 0.11909900605678558
CurrentTrain: epoch  8, batch     3 | loss: 1.8344940Losses:  3.4901862144470215 0.728239119052887
CurrentTrain: epoch  9, batch     0 | loss: 3.4901862Losses:  3.599432945251465 0.8579577207565308
CurrentTrain: epoch  9, batch     1 | loss: 3.5994329Losses:  4.083177089691162 0.8039669990539551
CurrentTrain: epoch  9, batch     2 | loss: 4.0831771Losses:  1.8443225622177124 0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.8443226
Losses:  1.6057255268096924 0.8483023643493652
MemoryTrain:  epoch  0, batch     0 | loss: 1.6057255Losses:  1.0190680027008057 0.7566404342651367
MemoryTrain:  epoch  0, batch     1 | loss: 1.0190680Losses:  1.419440507888794 0.7650734186172485
MemoryTrain:  epoch  0, batch     2 | loss: 1.4194405Losses:  1.1016318798065186 0.6157375574111938
MemoryTrain:  epoch  0, batch     3 | loss: 1.1016319Losses:  1.4651110172271729 0.7769670486450195
MemoryTrain:  epoch  1, batch     0 | loss: 1.4651110Losses:  1.8749217987060547 0.8340246677398682
MemoryTrain:  epoch  1, batch     1 | loss: 1.8749218Losses:  0.6968804001808167 0.6063604950904846
MemoryTrain:  epoch  1, batch     2 | loss: 0.6968804Losses:  1.2439289093017578 0.7800248861312866
MemoryTrain:  epoch  1, batch     3 | loss: 1.2439289Losses:  1.0010825395584106 0.803037166595459
MemoryTrain:  epoch  2, batch     0 | loss: 1.0010825Losses:  0.9527384638786316 0.7185429334640503
MemoryTrain:  epoch  2, batch     1 | loss: 0.9527385Losses:  0.7901697754859924 0.6568853259086609
MemoryTrain:  epoch  2, batch     2 | loss: 0.7901698Losses:  0.9883997440338135 0.5199293494224548
MemoryTrain:  epoch  2, batch     3 | loss: 0.9883997Losses:  0.7862551808357239 0.6424239873886108
MemoryTrain:  epoch  3, batch     0 | loss: 0.7862552Losses:  1.1035284996032715 0.8107123374938965
MemoryTrain:  epoch  3, batch     1 | loss: 1.1035285Losses:  0.8997828960418701 0.7861603498458862
MemoryTrain:  epoch  3, batch     2 | loss: 0.8997829Losses:  0.5264328718185425 0.4476590156555176
MemoryTrain:  epoch  3, batch     3 | loss: 0.5264329Losses:  0.717129647731781 0.5835902094841003
MemoryTrain:  epoch  4, batch     0 | loss: 0.7171296Losses:  0.9963903427124023 0.7201821804046631
MemoryTrain:  epoch  4, batch     1 | loss: 0.9963903Losses:  0.8666555285453796 0.7898814082145691
MemoryTrain:  epoch  4, batch     2 | loss: 0.8666555Losses:  0.5038331747055054 0.4373261630535126
MemoryTrain:  epoch  4, batch     3 | loss: 0.5038332Losses:  0.9296814799308777 0.8284822106361389
MemoryTrain:  epoch  5, batch     0 | loss: 0.9296815Losses:  0.8169208765029907 0.7436697483062744
MemoryTrain:  epoch  5, batch     1 | loss: 0.8169209Losses:  0.6379398107528687 0.5975929498672485
MemoryTrain:  epoch  5, batch     2 | loss: 0.6379398Losses:  0.5275754928588867 0.4718460440635681
MemoryTrain:  epoch  5, batch     3 | loss: 0.5275755Losses:  0.6839863657951355 0.6256781220436096
MemoryTrain:  epoch  6, batch     0 | loss: 0.6839864Losses:  0.8719778060913086 0.8170548677444458
MemoryTrain:  epoch  6, batch     1 | loss: 0.8719778Losses:  0.5187768340110779 0.477215975522995
MemoryTrain:  epoch  6, batch     2 | loss: 0.5187768Losses:  0.6975470781326294 0.6128402352333069
MemoryTrain:  epoch  6, batch     3 | loss: 0.6975471Losses:  0.7323150634765625 0.6799293756484985
MemoryTrain:  epoch  7, batch     0 | loss: 0.7323151Losses:  0.6882598996162415 0.6502456665039062
MemoryTrain:  epoch  7, batch     1 | loss: 0.6882599Losses:  0.6413153409957886 0.6178959608078003
MemoryTrain:  epoch  7, batch     2 | loss: 0.6413153Losses:  0.687412440776825 0.6443251371383667
MemoryTrain:  epoch  7, batch     3 | loss: 0.6874124Losses:  0.7473447918891907 0.701365053653717
MemoryTrain:  epoch  8, batch     0 | loss: 0.7473448Losses:  0.5695153474807739 0.5455853939056396
MemoryTrain:  epoch  8, batch     1 | loss: 0.5695153Losses:  0.5846753120422363 0.554572582244873
MemoryTrain:  epoch  8, batch     2 | loss: 0.5846753Losses:  0.552349865436554 0.5120114088058472
MemoryTrain:  epoch  8, batch     3 | loss: 0.5523499Losses:  0.5882151126861572 0.5646970868110657
MemoryTrain:  epoch  9, batch     0 | loss: 0.5882151Losses:  0.5747451186180115 0.5419652462005615
MemoryTrain:  epoch  9, batch     1 | loss: 0.5747451Losses:  0.6615902781486511 0.6380659341812134
MemoryTrain:  epoch  9, batch     2 | loss: 0.6615903Losses:  0.6134669184684753 0.5753289461135864
MemoryTrain:  epoch  9, batch     3 | loss: 0.6134669
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 66.07%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.01%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.00%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 87.12%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 86.66%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 86.31%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 86.14%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 86.20%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.13%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.15%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 85.39%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 85.26%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.13%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 84.88%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 84.83%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 84.34%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 84.15%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 83.82%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 83.72%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 83.26%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 82.79%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 82.21%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 81.93%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 81.59%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 81.05%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 80.46%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 79.88%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 79.51%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 78.89%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 78.35%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 77.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.48%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 77.39%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.18%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.04%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.96%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.83%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 76.40%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 75.75%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 75.23%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.66%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 74.21%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.67%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 74.89%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.90%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 74.79%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 74.60%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 74.36%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 74.17%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 73.94%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 73.84%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.36%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 72.93%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 72.55%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.27%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.81%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.37%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.90%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 71.51%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.05%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 71.47%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 71.30%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 71.10%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 71.03%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 71.21%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 71.03%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 70.98%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 70.97%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 70.93%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 70.84%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 70.99%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 71.47%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 71.24%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 71.07%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 70.86%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 70.60%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 70.44%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 70.34%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 70.24%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 69.89%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 69.70%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 69.52%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 69.28%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 70.13%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 72.51%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 72.27%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 72.01%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 71.78%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 71.52%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 71.26%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 71.16%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 71.12%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 71.01%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 70.82%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 70.81%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 70.71%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 71.03%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 70.60%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 70.20%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 70.02%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 69.82%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 69.58%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  290 | acc: 43.75%,  total acc: 69.37%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 69.15%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 68.94%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 69.79%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 69.40%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 69.22%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 69.06%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 68.95%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 69.22%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 69.15%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 69.71%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 70.17%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 70.05%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 69.95%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 69.74%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 69.61%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  362 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 70.11%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 70.03%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.98%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 69.89%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.84%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 69.57%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 69.52%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 69.48%   
cur_acc:  ['0.9464', '0.7421', '0.7450', '0.7748', '0.6399', '0.6607']
his_acc:  ['0.9464', '0.8465', '0.7945', '0.7740', '0.7346', '0.6948']
Clustering into  34  clusters
Clusters:  [ 6 30  0  1 27 25  0 12  2  2  0  2 26 13 33  5 16 13  3  5 14  5 15 19
  1 30 22  1 21 23 28 22 14  1 24  6 20 31 12 21 17 18  8  0 12  7  4 10
 29 16  7  5 32 11  8  3  9 10  1 22 30  5 22 33 15  3  4  1  4  1]
Losses:  7.97452449798584 1.0114200115203857
CurrentTrain: epoch  0, batch     0 | loss: 7.9745245Losses:  7.715744972229004 0.9386109113693237
CurrentTrain: epoch  0, batch     1 | loss: 7.7157450Losses:  7.170099258422852 0.8555994033813477
CurrentTrain: epoch  0, batch     2 | loss: 7.1700993Losses:  9.60236644744873 0.561152458190918
CurrentTrain: epoch  0, batch     3 | loss: 9.6023664Losses:  6.612886428833008 0.833710789680481
CurrentTrain: epoch  1, batch     0 | loss: 6.6128864Losses:  6.976291179656982 0.9395781755447388
CurrentTrain: epoch  1, batch     1 | loss: 6.9762912Losses:  6.645931720733643 0.8996576070785522
CurrentTrain: epoch  1, batch     2 | loss: 6.6459317Losses:  5.4256720542907715 0.12989549338817596
CurrentTrain: epoch  1, batch     3 | loss: 5.4256721Losses:  5.684026718139648 0.7541779279708862
CurrentTrain: epoch  2, batch     0 | loss: 5.6840267Losses:  6.392070770263672 0.9494674801826477
CurrentTrain: epoch  2, batch     1 | loss: 6.3920708Losses:  6.628242015838623 0.8222033381462097
CurrentTrain: epoch  2, batch     2 | loss: 6.6282420Losses:  6.119869709014893 0.21749404072761536
CurrentTrain: epoch  2, batch     3 | loss: 6.1198697Losses:  5.658720970153809 0.7952689528465271
CurrentTrain: epoch  3, batch     0 | loss: 5.6587210Losses:  6.383079528808594 0.8236567974090576
CurrentTrain: epoch  3, batch     1 | loss: 6.3830795Losses:  5.1438398361206055 0.8410309553146362
CurrentTrain: epoch  3, batch     2 | loss: 5.1438398Losses:  7.5608229637146 0.2900356352329254
CurrentTrain: epoch  3, batch     3 | loss: 7.5608230Losses:  5.732935905456543 0.7602112293243408
CurrentTrain: epoch  4, batch     0 | loss: 5.7329359Losses:  4.749614238739014 0.7124060988426208
CurrentTrain: epoch  4, batch     1 | loss: 4.7496142Losses:  5.294899940490723 0.6518474817276001
CurrentTrain: epoch  4, batch     2 | loss: 5.2948999Losses:  3.141181707382202 0.1323835402727127
CurrentTrain: epoch  4, batch     3 | loss: 3.1411817Losses:  5.26165771484375 0.8065541982650757
CurrentTrain: epoch  5, batch     0 | loss: 5.2616577Losses:  5.306667327880859 0.8436039686203003
CurrentTrain: epoch  5, batch     1 | loss: 5.3066673Losses:  4.2869873046875 0.7571811676025391
CurrentTrain: epoch  5, batch     2 | loss: 4.2869873Losses:  3.7394323348999023 0.14524509012699127
CurrentTrain: epoch  5, batch     3 | loss: 3.7394323Losses:  4.884555339813232 0.8217893838882446
CurrentTrain: epoch  6, batch     0 | loss: 4.8845553Losses:  5.251956462860107 0.8296691179275513
CurrentTrain: epoch  6, batch     1 | loss: 5.2519565Losses:  3.9684078693389893 0.6112067699432373
CurrentTrain: epoch  6, batch     2 | loss: 3.9684079Losses:  2.220120668411255 0.10757432878017426
CurrentTrain: epoch  6, batch     3 | loss: 2.2201207Losses:  4.386526584625244 0.7701817750930786
CurrentTrain: epoch  7, batch     0 | loss: 4.3865266Losses:  4.756331920623779 0.8201333284378052
CurrentTrain: epoch  7, batch     1 | loss: 4.7563319Losses:  4.017695903778076 0.6655872464179993
CurrentTrain: epoch  7, batch     2 | loss: 4.0176959Losses:  3.203866720199585 0.13780611753463745
CurrentTrain: epoch  7, batch     3 | loss: 3.2038667Losses:  4.4626383781433105 0.6921336054801941
CurrentTrain: epoch  8, batch     0 | loss: 4.4626384Losses:  3.783818244934082 0.6789594888687134
CurrentTrain: epoch  8, batch     1 | loss: 3.7838182Losses:  4.306504726409912 0.6946633458137512
CurrentTrain: epoch  8, batch     2 | loss: 4.3065047Losses:  2.0936954021453857 0.0511607900261879
CurrentTrain: epoch  8, batch     3 | loss: 2.0936954Losses:  4.296656608581543 0.6904826164245605
CurrentTrain: epoch  9, batch     0 | loss: 4.2966566Losses:  3.5422115325927734 0.7323273420333862
CurrentTrain: epoch  9, batch     1 | loss: 3.5422115Losses:  4.3158111572265625 0.6815940141677856
CurrentTrain: epoch  9, batch     2 | loss: 4.3158112Losses:  2.662628173828125 0.03942021727561951
CurrentTrain: epoch  9, batch     3 | loss: 2.6626282
Losses:  1.1201703548431396 0.7859522104263306
MemoryTrain:  epoch  0, batch     0 | loss: 1.1201704Losses:  1.2785042524337769 0.8637346029281616
MemoryTrain:  epoch  0, batch     1 | loss: 1.2785043Losses:  0.8583480715751648 0.7408015727996826
MemoryTrain:  epoch  0, batch     2 | loss: 0.8583481Losses:  1.392857313156128 0.5864179134368896
MemoryTrain:  epoch  0, batch     3 | loss: 1.3928573Losses:  0.7515327334403992 0.26915329694747925
MemoryTrain:  epoch  0, batch     4 | loss: 0.7515327Losses:  1.2837438583374023 0.7605197429656982
MemoryTrain:  epoch  1, batch     0 | loss: 1.2837439Losses:  1.4705369472503662 0.6588453054428101
MemoryTrain:  epoch  1, batch     1 | loss: 1.4705369Losses:  0.775024950504303 0.5636134743690491
MemoryTrain:  epoch  1, batch     2 | loss: 0.7750250Losses:  0.8768342137336731 0.7208632230758667
MemoryTrain:  epoch  1, batch     3 | loss: 0.8768342Losses:  0.767810046672821 0.45911550521850586
MemoryTrain:  epoch  1, batch     4 | loss: 0.7678100Losses:  0.9845855236053467 0.6509273648262024
MemoryTrain:  epoch  2, batch     0 | loss: 0.9845855Losses:  0.7728574275970459 0.6607620716094971
MemoryTrain:  epoch  2, batch     1 | loss: 0.7728574Losses:  0.8861767649650574 0.7485564351081848
MemoryTrain:  epoch  2, batch     2 | loss: 0.8861768Losses:  1.0698182582855225 0.6532174348831177
MemoryTrain:  epoch  2, batch     3 | loss: 1.0698183Losses:  0.2085002064704895 0.17266401648521423
MemoryTrain:  epoch  2, batch     4 | loss: 0.2085002Losses:  0.8831770420074463 0.7081886529922485
MemoryTrain:  epoch  3, batch     0 | loss: 0.8831770Losses:  0.6496331691741943 0.5940213203430176
MemoryTrain:  epoch  3, batch     1 | loss: 0.6496332Losses:  0.7708931565284729 0.6368563771247864
MemoryTrain:  epoch  3, batch     2 | loss: 0.7708932Losses:  0.7483720779418945 0.6784664988517761
MemoryTrain:  epoch  3, batch     3 | loss: 0.7483721Losses:  0.2962751090526581 0.2405013144016266
MemoryTrain:  epoch  3, batch     4 | loss: 0.2962751Losses:  0.7281453609466553 0.6640183329582214
MemoryTrain:  epoch  4, batch     0 | loss: 0.7281454Losses:  0.6762553453445435 0.597786545753479
MemoryTrain:  epoch  4, batch     1 | loss: 0.6762553Losses:  0.8261983394622803 0.7613491415977478
MemoryTrain:  epoch  4, batch     2 | loss: 0.8261983Losses:  0.8142803311347961 0.5648766160011292
MemoryTrain:  epoch  4, batch     3 | loss: 0.8142803Losses:  0.46128079295158386 0.40961381793022156
MemoryTrain:  epoch  4, batch     4 | loss: 0.4612808Losses:  0.6683286428451538 0.5761027336120605
MemoryTrain:  epoch  5, batch     0 | loss: 0.6683286Losses:  0.849717915058136 0.7834473252296448
MemoryTrain:  epoch  5, batch     1 | loss: 0.8497179Losses:  0.5320164561271667 0.45521342754364014
MemoryTrain:  epoch  5, batch     2 | loss: 0.5320165Losses:  0.8153395652770996 0.7717389464378357
MemoryTrain:  epoch  5, batch     3 | loss: 0.8153396Losses:  0.3502082824707031 0.27927225828170776
MemoryTrain:  epoch  5, batch     4 | loss: 0.3502083Losses:  0.7183424234390259 0.6791063547134399
MemoryTrain:  epoch  6, batch     0 | loss: 0.7183424Losses:  0.6154350638389587 0.5693632364273071
MemoryTrain:  epoch  6, batch     1 | loss: 0.6154351Losses:  0.7248094081878662 0.6481499671936035
MemoryTrain:  epoch  6, batch     2 | loss: 0.7248094Losses:  0.579963743686676 0.5286007523536682
MemoryTrain:  epoch  6, batch     3 | loss: 0.5799637Losses:  0.3357214331626892 0.29969653487205505
MemoryTrain:  epoch  6, batch     4 | loss: 0.3357214Losses:  0.672481894493103 0.6325128078460693
MemoryTrain:  epoch  7, batch     0 | loss: 0.6724819Losses:  0.6484860777854919 0.615023136138916
MemoryTrain:  epoch  7, batch     1 | loss: 0.6484861Losses:  0.649613618850708 0.6214269995689392
MemoryTrain:  epoch  7, batch     2 | loss: 0.6496136Losses:  0.7091115713119507 0.6702955961227417
MemoryTrain:  epoch  7, batch     3 | loss: 0.7091116Losses:  0.19042164087295532 0.16333144903182983
MemoryTrain:  epoch  7, batch     4 | loss: 0.1904216Losses:  0.6255277395248413 0.5993995666503906
MemoryTrain:  epoch  8, batch     0 | loss: 0.6255277Losses:  0.5906346440315247 0.562378466129303
MemoryTrain:  epoch  8, batch     1 | loss: 0.5906346Losses:  0.7320941090583801 0.6638551950454712
MemoryTrain:  epoch  8, batch     2 | loss: 0.7320941Losses:  0.6559656262397766 0.627536416053772
MemoryTrain:  epoch  8, batch     3 | loss: 0.6559656Losses:  0.21557503938674927 0.19003063440322876
MemoryTrain:  epoch  8, batch     4 | loss: 0.2155750Losses:  0.6556302309036255 0.622237503528595
MemoryTrain:  epoch  9, batch     0 | loss: 0.6556302Losses:  0.597269594669342 0.5670608878135681
MemoryTrain:  epoch  9, batch     1 | loss: 0.5972696Losses:  0.46621936559677124 0.4295122027397156
MemoryTrain:  epoch  9, batch     2 | loss: 0.4662194Losses:  0.6534484624862671 0.6265339851379395
MemoryTrain:  epoch  9, batch     3 | loss: 0.6534485Losses:  0.23256252706050873 0.21865233778953552
MemoryTrain:  epoch  9, batch     4 | loss: 0.2325625
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 5.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 25.69%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 38.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 41.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 49.73%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 48.44%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 47.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 45.67%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 43.98%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 42.41%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 40.95%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 39.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 38.31%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 39.06%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 40.34%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 41.36%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 43.04%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 43.92%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 44.59%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 45.56%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 46.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 47.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 49.09%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 49.85%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 51.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 51.85%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 52.72%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 53.19%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 53.65%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 54.34%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 54.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 54.66%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 55.29%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 55.54%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 56.02%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 56.36%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 56.81%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 56.58%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 56.03%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 55.72%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 55.73%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 55.64%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 55.75%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 55.06%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 88.86%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 88.49%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 88.25%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 87.60%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.40%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.15%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 85.70%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 85.17%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 84.56%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.46%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.42%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.13%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 83.69%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 83.49%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.28%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 83.18%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 82.61%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 82.44%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 82.13%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.05%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 81.47%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.32%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 80.97%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 80.49%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 80.22%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 79.96%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 79.50%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 78.92%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 78.36%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 77.67%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 77.26%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 76.14%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 75.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.25%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.94%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 74.36%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 73.73%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 73.11%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.50%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 72.02%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 71.54%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 72.63%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.62%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 72.53%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 72.87%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 72.39%   [EVAL] batch:  139 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 71.41%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.91%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 70.50%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 70.23%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 70.82%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 70.35%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.89%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 69.48%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 69.03%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 68.98%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 68.67%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 68.30%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 67.93%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 67.60%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 67.83%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 67.74%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 67.74%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 68.15%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 68.49%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 68.02%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 67.80%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 67.53%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 67.34%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 67.18%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 67.03%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 66.86%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 66.35%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 66.21%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 66.01%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 65.85%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.66%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 66.64%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 69.45%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 69.30%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 69.05%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 68.82%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 68.46%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 68.34%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 67.89%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 68.46%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.30%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:  278 | acc: 31.25%,  total acc: 68.08%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 67.79%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 67.67%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.45%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.07%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 66.83%   [EVAL] batch:  287 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:  288 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 66.34%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 66.17%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:  292 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 65.72%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 65.55%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 65.46%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 65.41%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 66.40%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 66.02%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 65.83%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.68%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 66.06%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 66.46%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 66.46%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 67.04%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 66.89%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 66.74%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 66.60%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 66.49%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 66.87%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:  366 | acc: 12.50%,  total acc: 66.66%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 66.58%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 66.45%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 66.06%   [EVAL] batch:  377 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  378 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 65.40%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 65.41%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  388 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 65.67%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 65.51%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 65.29%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.02%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 64.86%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 64.70%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 64.54%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 64.38%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.22%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.22%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 64.40%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 64.41%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 64.96%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 65.03%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 65.12%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 64.92%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 64.74%   
cur_acc:  ['0.9464', '0.7421', '0.7450', '0.7748', '0.6399', '0.6607', '0.5506']
his_acc:  ['0.9464', '0.8465', '0.7945', '0.7740', '0.7346', '0.6948', '0.6474']
Clustering into  38  clusters
Clusters:  [ 7 34  5  0 33 27  5  0 26 26  2 26 28  9 30 14 18  9 12 14 15 14  3 21
  0 34 17  0 10 29  5 17 15  8 31  7 13 19  0 35 20 37 11 36  0  6  1  4
 32 18  6 14 16 24 11 12  2  4  8 17 34 14 17 30  3 12  1  5  1  8  5 26
 10 14 23 22 25 16 36  2]
Losses:  6.9389543533325195 0.7473436594009399
CurrentTrain: epoch  0, batch     0 | loss: 6.9389544Losses:  5.285758018493652 0.5441762804985046
CurrentTrain: epoch  0, batch     1 | loss: 5.2857580Losses:  8.630586624145508 0.8590518832206726
CurrentTrain: epoch  0, batch     2 | loss: 8.6305866Losses:  6.436331748962402 0.0
CurrentTrain: epoch  0, batch     3 | loss: 6.4363317Losses:  6.079433441162109 0.6376064419746399
CurrentTrain: epoch  1, batch     0 | loss: 6.0794334Losses:  6.393909454345703 0.823296308517456
CurrentTrain: epoch  1, batch     1 | loss: 6.3939095Losses:  5.7189621925354 0.6732221841812134
CurrentTrain: epoch  1, batch     2 | loss: 5.7189622Losses:  6.539980888366699 0.17186923325061798
CurrentTrain: epoch  1, batch     3 | loss: 6.5399809Losses:  6.0157647132873535 0.782717227935791
CurrentTrain: epoch  2, batch     0 | loss: 6.0157647Losses:  4.587832450866699 0.5460152626037598
CurrentTrain: epoch  2, batch     1 | loss: 4.5878325Losses:  6.782763957977295 0.7737407684326172
CurrentTrain: epoch  2, batch     2 | loss: 6.7827640Losses:  2.5547451972961426 0.03341483324766159
CurrentTrain: epoch  2, batch     3 | loss: 2.5547452Losses:  5.637797832489014 0.615458071231842
CurrentTrain: epoch  3, batch     0 | loss: 5.6377978Losses:  4.6405930519104 0.5024788975715637
CurrentTrain: epoch  3, batch     1 | loss: 4.6405931Losses:  5.472025394439697 0.6509438157081604
CurrentTrain: epoch  3, batch     2 | loss: 5.4720254Losses:  7.43973970413208 0.12390690296888351
CurrentTrain: epoch  3, batch     3 | loss: 7.4397397Losses:  5.859934329986572 0.6441540122032166
CurrentTrain: epoch  4, batch     0 | loss: 5.8599343Losses:  4.621270179748535 0.5267418622970581
CurrentTrain: epoch  4, batch     1 | loss: 4.6212702Losses:  4.394351959228516 0.6004346013069153
CurrentTrain: epoch  4, batch     2 | loss: 4.3943520Losses:  7.81779146194458 0.3960307240486145
CurrentTrain: epoch  4, batch     3 | loss: 7.8177915Losses:  4.500854015350342 0.5845850706100464
CurrentTrain: epoch  5, batch     0 | loss: 4.5008540Losses:  4.139540195465088 0.5257365703582764
CurrentTrain: epoch  5, batch     1 | loss: 4.1395402Losses:  5.36674690246582 0.5903608798980713
CurrentTrain: epoch  5, batch     2 | loss: 5.3667469Losses:  4.1067023277282715 0.13724133372306824
CurrentTrain: epoch  5, batch     3 | loss: 4.1067023Losses:  4.541192531585693 0.6279321908950806
CurrentTrain: epoch  6, batch     0 | loss: 4.5411925Losses:  4.407649517059326 0.6447386741638184
CurrentTrain: epoch  6, batch     1 | loss: 4.4076495Losses:  4.416285037994385 0.5794087648391724
CurrentTrain: epoch  6, batch     2 | loss: 4.4162850Losses:  3.5971758365631104 0.15836688876152039
CurrentTrain: epoch  6, batch     3 | loss: 3.5971758Losses:  3.7185218334198 0.5641549229621887
CurrentTrain: epoch  7, batch     0 | loss: 3.7185218Losses:  4.804050445556641 0.6360383033752441
CurrentTrain: epoch  7, batch     1 | loss: 4.8040504Losses:  3.9566822052001953 0.49695271253585815
CurrentTrain: epoch  7, batch     2 | loss: 3.9566822Losses:  1.8649040460586548 0.015211492776870728
CurrentTrain: epoch  7, batch     3 | loss: 1.8649040Losses:  4.085308074951172 0.5742037892341614
CurrentTrain: epoch  8, batch     0 | loss: 4.0853081Losses:  3.7743260860443115 0.4860699772834778
CurrentTrain: epoch  8, batch     1 | loss: 3.7743261Losses:  3.5475473403930664 0.43786200881004333
CurrentTrain: epoch  8, batch     2 | loss: 3.5475473Losses:  5.264782905578613 0.20104148983955383
CurrentTrain: epoch  8, batch     3 | loss: 5.2647829Losses:  4.174985408782959 0.5622872710227966
CurrentTrain: epoch  9, batch     0 | loss: 4.1749854Losses:  3.1162054538726807 0.3785993456840515
CurrentTrain: epoch  9, batch     1 | loss: 3.1162055Losses:  3.6877593994140625 0.6246769428253174
CurrentTrain: epoch  9, batch     2 | loss: 3.6877594Losses:  4.533132076263428 0.2582867741584778
CurrentTrain: epoch  9, batch     3 | loss: 4.5331321
Losses:  0.950594961643219 0.704578161239624
MemoryTrain:  epoch  0, batch     0 | loss: 0.9505950Losses:  1.1303138732910156 0.6662524938583374
MemoryTrain:  epoch  0, batch     1 | loss: 1.1303139Losses:  0.9745648503303528 0.7339029312133789
MemoryTrain:  epoch  0, batch     2 | loss: 0.9745649Losses:  0.8848937749862671 0.6152374148368835
MemoryTrain:  epoch  0, batch     3 | loss: 0.8848938Losses:  1.486769199371338 0.6930162310600281
MemoryTrain:  epoch  0, batch     4 | loss: 1.4867692Losses:  1.0818408727645874 0.7806960344314575
MemoryTrain:  epoch  1, batch     0 | loss: 1.0818409Losses:  1.1958073377609253 0.5895021557807922
MemoryTrain:  epoch  1, batch     1 | loss: 1.1958073Losses:  1.197557806968689 0.5856322050094604
MemoryTrain:  epoch  1, batch     2 | loss: 1.1975578Losses:  1.0069674253463745 0.6686229705810547
MemoryTrain:  epoch  1, batch     3 | loss: 1.0069674Losses:  0.7253066897392273 0.6373603343963623
MemoryTrain:  epoch  1, batch     4 | loss: 0.7253067Losses:  0.8410907983779907 0.6532758474349976
MemoryTrain:  epoch  2, batch     0 | loss: 0.8410908Losses:  0.830734372138977 0.6289685964584351
MemoryTrain:  epoch  2, batch     1 | loss: 0.8307344Losses:  0.7350660562515259 0.5875683426856995
MemoryTrain:  epoch  2, batch     2 | loss: 0.7350661Losses:  0.7384945750236511 0.6133894324302673
MemoryTrain:  epoch  2, batch     3 | loss: 0.7384946Losses:  0.7109367847442627 0.6154177784919739
MemoryTrain:  epoch  2, batch     4 | loss: 0.7109368Losses:  0.8184454441070557 0.6610041260719299
MemoryTrain:  epoch  3, batch     0 | loss: 0.8184454Losses:  0.5951717495918274 0.536428689956665
MemoryTrain:  epoch  3, batch     1 | loss: 0.5951717Losses:  0.5944207906723022 0.5407485961914062
MemoryTrain:  epoch  3, batch     2 | loss: 0.5944208Losses:  0.888696551322937 0.7457533478736877
MemoryTrain:  epoch  3, batch     3 | loss: 0.8886966Losses:  0.5881449580192566 0.5376275777816772
MemoryTrain:  epoch  3, batch     4 | loss: 0.5881450Losses:  0.9227035045623779 0.8163816928863525
MemoryTrain:  epoch  4, batch     0 | loss: 0.9227035Losses:  0.661083459854126 0.6150960922241211
MemoryTrain:  epoch  4, batch     1 | loss: 0.6610835Losses:  0.5620781183242798 0.5131932497024536
MemoryTrain:  epoch  4, batch     2 | loss: 0.5620781Losses:  0.5245506167411804 0.47635945677757263
MemoryTrain:  epoch  4, batch     3 | loss: 0.5245506Losses:  0.5554911494255066 0.5158300399780273
MemoryTrain:  epoch  4, batch     4 | loss: 0.5554911Losses:  0.6217982769012451 0.5600091218948364
MemoryTrain:  epoch  5, batch     0 | loss: 0.6217983Losses:  0.5649662017822266 0.5277067422866821
MemoryTrain:  epoch  5, batch     1 | loss: 0.5649662Losses:  0.6759127378463745 0.5651339292526245
MemoryTrain:  epoch  5, batch     2 | loss: 0.6759127Losses:  0.599639356136322 0.5456426739692688
MemoryTrain:  epoch  5, batch     3 | loss: 0.5996394Losses:  0.715796947479248 0.6745880246162415
MemoryTrain:  epoch  5, batch     4 | loss: 0.7157969Losses:  0.5650492906570435 0.5280354619026184
MemoryTrain:  epoch  6, batch     0 | loss: 0.5650493Losses:  0.5103031992912292 0.47612231969833374
MemoryTrain:  epoch  6, batch     1 | loss: 0.5103032Losses:  0.7046204805374146 0.6565387845039368
MemoryTrain:  epoch  6, batch     2 | loss: 0.7046205Losses:  0.7003508806228638 0.6511307954788208
MemoryTrain:  epoch  6, batch     3 | loss: 0.7003509Losses:  0.5458921194076538 0.521105170249939
MemoryTrain:  epoch  6, batch     4 | loss: 0.5458921Losses:  0.6211084723472595 0.6005526781082153
MemoryTrain:  epoch  7, batch     0 | loss: 0.6211085Losses:  0.5625231266021729 0.5277685523033142
MemoryTrain:  epoch  7, batch     1 | loss: 0.5625231Losses:  0.49603602290153503 0.4570831060409546
MemoryTrain:  epoch  7, batch     2 | loss: 0.4960360Losses:  0.6342912912368774 0.5742042064666748
MemoryTrain:  epoch  7, batch     3 | loss: 0.6342913Losses:  0.6381160020828247 0.6058205366134644
MemoryTrain:  epoch  7, batch     4 | loss: 0.6381160Losses:  0.6487627625465393 0.6238521337509155
MemoryTrain:  epoch  8, batch     0 | loss: 0.6487628Losses:  0.48969295620918274 0.4662472605705261
MemoryTrain:  epoch  8, batch     1 | loss: 0.4896930Losses:  0.6820496916770935 0.6220208406448364
MemoryTrain:  epoch  8, batch     2 | loss: 0.6820497Losses:  0.6887970566749573 0.6508349180221558
MemoryTrain:  epoch  8, batch     3 | loss: 0.6887971Losses:  0.5270549058914185 0.4958053231239319
MemoryTrain:  epoch  8, batch     4 | loss: 0.5270549Losses:  0.5671666860580444 0.5298234820365906
MemoryTrain:  epoch  9, batch     0 | loss: 0.5671667Losses:  0.5990598201751709 0.5712839365005493
MemoryTrain:  epoch  9, batch     1 | loss: 0.5990598Losses:  0.5367019176483154 0.519489586353302
MemoryTrain:  epoch  9, batch     2 | loss: 0.5367019Losses:  0.6932880282402039 0.6535336971282959
MemoryTrain:  epoch  9, batch     3 | loss: 0.6932880Losses:  0.5649279356002808 0.5387299656867981
MemoryTrain:  epoch  9, batch     4 | loss: 0.5649279
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 0.00%,  total acc: 55.99%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 68.91%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 64.11%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 63.59%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 85.24%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 84.96%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.79%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.63%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.48%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 83.73%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 82.52%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 81.35%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 80.21%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 79.10%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 78.03%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 77.45%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 77.86%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 78.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 77.52%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.32%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 77.13%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 76.73%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 76.54%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 75.70%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 75.42%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 75.07%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 74.40%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 73.36%   [EVAL] batch:   95 | acc: 6.25%,  total acc: 72.66%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 72.29%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 71.75%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 70.62%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 70.53%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 70.22%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 69.86%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 69.27%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 68.69%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 68.12%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 67.30%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 67.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 69.38%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.88%   [EVAL] batch:  139 | acc: 0.00%,  total acc: 68.39%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 67.95%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 67.09%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 67.55%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 67.11%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 66.27%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 65.85%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.42%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 66.33%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 66.01%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 65.68%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 64.97%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 64.56%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 64.99%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 64.87%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 64.75%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 64.41%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 64.70%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 65.67%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 65.47%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 65.15%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 64.98%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 64.61%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 64.48%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 64.38%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 64.25%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 63.85%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 63.66%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 63.51%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 63.30%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 63.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 64.32%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.43%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 67.23%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 67.09%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 66.90%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 66.73%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 66.32%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.27%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 66.19%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  274 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 66.39%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.25%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:  278 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 65.79%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 65.49%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 65.31%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.12%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.90%   [EVAL] batch:  287 | acc: 18.75%,  total acc: 64.74%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 64.55%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 64.22%   [EVAL] batch:  291 | acc: 31.25%,  total acc: 64.11%   [EVAL] batch:  292 | acc: 25.00%,  total acc: 63.97%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 63.86%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 63.79%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 63.72%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:  297 | acc: 31.25%,  total acc: 63.53%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 64.02%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 63.84%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 63.71%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 64.10%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 64.13%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 63.97%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 63.85%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 63.56%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 63.61%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 63.70%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 64.03%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 64.05%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 64.30%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 64.52%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 64.23%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 64.07%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 63.94%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 63.99%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.07%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 64.46%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 64.37%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 64.30%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 64.22%   [EVAL] batch:  366 | acc: 12.50%,  total acc: 64.08%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 64.01%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 63.85%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 63.83%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 63.77%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 63.73%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 63.61%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 63.48%   [EVAL] batch:  377 | acc: 0.00%,  total acc: 63.31%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 63.18%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 63.01%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 62.86%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 62.84%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 62.88%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 62.87%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 62.92%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 62.94%   [EVAL] batch:  386 | acc: 50.00%,  total acc: 62.90%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:  389 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 63.36%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 63.28%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 63.15%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 62.99%   [EVAL] batch:  397 | acc: 18.75%,  total acc: 62.88%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 62.77%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 62.64%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 62.34%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.19%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 61.88%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 61.73%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 61.73%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 61.78%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 61.80%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 61.92%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 61.94%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 62.03%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 62.23%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 62.49%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 62.51%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 62.59%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 62.51%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 62.54%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 62.57%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 62.56%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  431 | acc: 37.50%,  total acc: 62.54%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 62.46%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 62.40%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 62.39%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 62.34%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 62.33%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 62.29%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 62.30%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 62.33%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 62.33%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 62.32%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 62.33%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 62.35%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 62.35%   [EVAL] batch:  446 | acc: 43.75%,  total acc: 62.30%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 62.32%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 62.31%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  450 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 62.46%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 62.51%   [EVAL] batch:  454 | acc: 62.50%,  total acc: 62.51%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:  456 | acc: 43.75%,  total acc: 62.53%   [EVAL] batch:  457 | acc: 6.25%,  total acc: 62.40%   [EVAL] batch:  458 | acc: 0.00%,  total acc: 62.27%   [EVAL] batch:  459 | acc: 6.25%,  total acc: 62.15%   [EVAL] batch:  460 | acc: 6.25%,  total acc: 62.03%   [EVAL] batch:  461 | acc: 6.25%,  total acc: 61.90%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 61.89%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 62.46%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 62.88%   [EVAL] batch:  478 | acc: 75.00%,  total acc: 62.90%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 62.93%   [EVAL] batch:  482 | acc: 18.75%,  total acc: 62.84%   [EVAL] batch:  483 | acc: 12.50%,  total acc: 62.73%   [EVAL] batch:  484 | acc: 18.75%,  total acc: 62.64%   [EVAL] batch:  485 | acc: 31.25%,  total acc: 62.58%   [EVAL] batch:  486 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 62.53%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 62.58%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 62.68%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 62.73%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 62.82%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 62.77%   [EVAL] batch:  495 | acc: 37.50%,  total acc: 62.71%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 62.66%   [EVAL] batch:  497 | acc: 31.25%,  total acc: 62.60%   [EVAL] batch:  498 | acc: 43.75%,  total acc: 62.56%   [EVAL] batch:  499 | acc: 37.50%,  total acc: 62.51%   
cur_acc:  ['0.9464', '0.7421', '0.7450', '0.7748', '0.6399', '0.6607', '0.5506', '0.6359']
his_acc:  ['0.9464', '0.8465', '0.7945', '0.7740', '0.7346', '0.6948', '0.6474', '0.6251']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  11.480340003967285 1.5006341934204102
CurrentTrain: epoch  0, batch     0 | loss: 11.4803400Losses:  11.204778671264648 1.5281791687011719
CurrentTrain: epoch  0, batch     1 | loss: 11.2047787Losses:  10.912738800048828 1.3280441761016846
CurrentTrain: epoch  0, batch     2 | loss: 10.9127388Losses:  11.418795585632324 1.4722224473953247
CurrentTrain: epoch  0, batch     3 | loss: 11.4187956Losses:  10.549209594726562 1.2681713104248047
CurrentTrain: epoch  0, batch     4 | loss: 10.5492096Losses:  10.404483795166016 1.3013877868652344
CurrentTrain: epoch  0, batch     5 | loss: 10.4044838Losses:  10.388015747070312 1.2808917760849
CurrentTrain: epoch  0, batch     6 | loss: 10.3880157Losses:  10.732222557067871 1.2768030166625977
CurrentTrain: epoch  0, batch     7 | loss: 10.7322226Losses:  10.721409797668457 1.3149385452270508
CurrentTrain: epoch  0, batch     8 | loss: 10.7214098Losses:  10.51937198638916 1.2429760694503784
CurrentTrain: epoch  0, batch     9 | loss: 10.5193720Losses:  9.605721473693848 1.1656863689422607
CurrentTrain: epoch  0, batch    10 | loss: 9.6057215Losses:  10.792001724243164 1.3918254375457764
CurrentTrain: epoch  0, batch    11 | loss: 10.7920017Losses:  9.594405174255371 1.0452998876571655
CurrentTrain: epoch  0, batch    12 | loss: 9.5944052Losses:  10.05826187133789 1.1998121738433838
CurrentTrain: epoch  0, batch    13 | loss: 10.0582619Losses:  10.06931209564209 1.291707992553711
CurrentTrain: epoch  0, batch    14 | loss: 10.0693121Losses:  10.24627685546875 1.0089597702026367
CurrentTrain: epoch  0, batch    15 | loss: 10.2462769Losses:  9.1000337600708 1.1022603511810303
CurrentTrain: epoch  0, batch    16 | loss: 9.1000338Losses:  9.611675262451172 1.1267726421356201
CurrentTrain: epoch  0, batch    17 | loss: 9.6116753Losses:  9.851391792297363 1.4269877672195435
CurrentTrain: epoch  0, batch    18 | loss: 9.8513918Losses:  8.808494567871094 1.0120209455490112
CurrentTrain: epoch  0, batch    19 | loss: 8.8084946Losses:  9.075109481811523 1.0877790451049805
CurrentTrain: epoch  0, batch    20 | loss: 9.0751095Losses:  8.92733383178711 0.873064398765564
CurrentTrain: epoch  0, batch    21 | loss: 8.9273338Losses:  8.947208404541016 1.1059155464172363
CurrentTrain: epoch  0, batch    22 | loss: 8.9472084Losses:  10.30527114868164 1.4402029514312744
CurrentTrain: epoch  0, batch    23 | loss: 10.3052711Losses:  10.568717956542969 1.287513256072998
CurrentTrain: epoch  0, batch    24 | loss: 10.5687180Losses:  8.651330947875977 1.0174460411071777
CurrentTrain: epoch  0, batch    25 | loss: 8.6513309Losses:  9.805314064025879 1.1062108278274536
CurrentTrain: epoch  0, batch    26 | loss: 9.8053141Losses:  8.435256958007812 0.872445821762085
CurrentTrain: epoch  0, batch    27 | loss: 8.4352570Losses:  9.367633819580078 1.227002739906311
CurrentTrain: epoch  0, batch    28 | loss: 9.3676338Losses:  8.527860641479492 1.0760446786880493
CurrentTrain: epoch  0, batch    29 | loss: 8.5278606Losses:  9.199979782104492 0.8976768851280212
CurrentTrain: epoch  0, batch    30 | loss: 9.1999798Losses:  9.294038772583008 1.04672372341156
CurrentTrain: epoch  0, batch    31 | loss: 9.2940388Losses:  9.306232452392578 1.1991692781448364
CurrentTrain: epoch  0, batch    32 | loss: 9.3062325Losses:  9.974981307983398 1.0911157131195068
CurrentTrain: epoch  0, batch    33 | loss: 9.9749813Losses:  9.132365226745605 1.0471283197402954
CurrentTrain: epoch  0, batch    34 | loss: 9.1323652Losses:  10.077510833740234 1.4262850284576416
CurrentTrain: epoch  0, batch    35 | loss: 10.0775108Losses:  8.180537223815918 1.0741766691207886
CurrentTrain: epoch  0, batch    36 | loss: 8.1805372Losses:  8.298737525939941 1.1226608753204346
CurrentTrain: epoch  0, batch    37 | loss: 8.2987375Losses:  9.262551307678223 1.1418771743774414
CurrentTrain: epoch  0, batch    38 | loss: 9.2625513Losses:  8.443254470825195 1.1025447845458984
CurrentTrain: epoch  0, batch    39 | loss: 8.4432545Losses:  8.54513931274414 0.8292907476425171
CurrentTrain: epoch  0, batch    40 | loss: 8.5451393Losses:  7.394844055175781 0.6680322885513306
CurrentTrain: epoch  0, batch    41 | loss: 7.3948441Losses:  8.229263305664062 1.0619993209838867
CurrentTrain: epoch  0, batch    42 | loss: 8.2292633Losses:  9.19650650024414 1.113657832145691
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  10.722980499267578 1.5480644702911377
CurrentTrain: epoch  0, batch     0 | loss: 10.7229805Losses:  10.070488929748535 1.04404878616333
CurrentTrain: epoch  0, batch     1 | loss: 10.0704889Losses:  10.409707069396973 1.3534172773361206
CurrentTrain: epoch  0, batch     2 | loss: 10.4097071Losses:  9.58221435546875 1.1102304458618164
CurrentTrain: epoch  0, batch     3 | loss: 9.5822144Losses:  10.57669448852539 1.507495403289795
CurrentTrain: epoch  0, batch     4 | loss: 10.5766945Losses:  9.042466163635254 1.4005720615386963
CurrentTrain: epoch  0, batch     5 | loss: 9.0424662Losses:  9.63620662689209 1.1519606113433838
CurrentTrain: epoch  0, batch     6 | loss: 9.6362066Losses:  10.383849143981934 1.5244643688201904
CurrentTrain: epoch  0, batch     7 | loss: 10.3838491Losses:  10.471334457397461 1.209905982017517
CurrentTrain: epoch  0, batch     8 | loss: 10.4713345Losses:  9.164994239807129 1.4264845848083496
CurrentTrain: epoch  0, batch     9 | loss: 9.1649942Losses:  8.978153228759766 1.2731858491897583
CurrentTrain: epoch  0, batch    10 | loss: 8.9781532Losses:  10.408731460571289 1.500751256942749
CurrentTrain: epoch  0, batch    11 | loss: 10.4087315Losses:  10.261919021606445 1.391587734222412
CurrentTrain: epoch  0, batch    12 | loss: 10.2619190Losses:  10.242263793945312 1.6440367698669434
CurrentTrain: epoch  0, batch    13 | loss: 10.2422638Losses:  9.884214401245117 1.3401484489440918
CurrentTrain: epoch  0, batch    14 | loss: 9.8842144Losses:  8.861774444580078 1.0844085216522217
CurrentTrain: epoch  0, batch    15 | loss: 8.8617744Losses:  8.97780990600586 1.0585345029830933
CurrentTrain: epoch  0, batch    16 | loss: 8.9778099Losses:  9.57280158996582 1.4104535579681396
CurrentTrain: epoch  0, batch    17 | loss: 9.5728016Losses:  9.643485069274902 1.4638159275054932
CurrentTrain: epoch  0, batch    18 | loss: 9.6434851Losses:  9.417558670043945 1.2573323249816895
CurrentTrain: epoch  0, batch    19 | loss: 9.4175587Losses:  8.190071105957031 0.9844292402267456
CurrentTrain: epoch  0, batch    20 | loss: 8.1900711Losses:  8.444146156311035 0.9333497285842896
CurrentTrain: epoch  0, batch    21 | loss: 8.4441462Losses:  8.579381942749023 1.0893769264221191
CurrentTrain: epoch  0, batch    22 | loss: 8.5793819Losses:  9.191344261169434 1.2017490863800049
CurrentTrain: epoch  0, batch    23 | loss: 9.1913443Losses:  9.196331024169922 1.0621211528778076
CurrentTrain: epoch  0, batch    24 | loss: 9.1963310Losses:  9.53122615814209 1.4682633876800537
CurrentTrain: epoch  0, batch    25 | loss: 9.5312262Losses:  8.634815216064453 1.1937429904937744
CurrentTrain: epoch  0, batch    26 | loss: 8.6348152Losses:  9.031065940856934 1.3105432987213135
CurrentTrain: epoch  0, batch    27 | loss: 9.0310659Losses:  8.670027732849121 1.320882797241211
CurrentTrain: epoch  0, batch    28 | loss: 8.6700277Losses:  9.438484191894531 1.4504311084747314
CurrentTrain: epoch  0, batch    29 | loss: 9.4384842Losses:  8.385740280151367 1.1469298601150513
CurrentTrain: epoch  0, batch    30 | loss: 8.3857403Losses:  7.781571388244629 0.8677812814712524
CurrentTrain: epoch  0, batch    31 | loss: 7.7815714Losses:  8.550745010375977 1.1988897323608398
CurrentTrain: epoch  0, batch    32 | loss: 8.5507450Losses:  7.153085708618164 0.9406542778015137
CurrentTrain: epoch  0, batch    33 | loss: 7.1530857Losses:  8.387107849121094 1.217664122581482
CurrentTrain: epoch  0, batch    34 | loss: 8.3871078Losses:  9.073463439941406 1.0945708751678467
CurrentTrain: epoch  0, batch    35 | loss: 9.0734634Losses:  8.085371971130371 1.0430901050567627
CurrentTrain: epoch  0, batch    36 | loss: 8.0853720Losses:  7.936525821685791 0.991719126701355
CurrentTrain: epoch  0, batch    37 | loss: 7.9365258Losses:  8.030014991760254 1.0397017002105713
CurrentTrain: epoch  0, batch    38 | loss: 8.0300150Losses:  9.082942008972168 1.1902835369110107
CurrentTrain: epoch  0, batch    39 | loss: 9.0829420Losses:  8.50158977508545 1.1573901176452637
CurrentTrain: epoch  0, batch    40 | loss: 8.5015898Losses:  8.740446090698242 1.0073362588882446
CurrentTrain: epoch  0, batch    41 | loss: 8.7404461Losses:  8.881275177001953 1.142831563949585
CurrentTrain: epoch  0, batch    42 | loss: 8.8812752Losses:  8.340335845947266 1.1866767406463623
CurrentTrain: epoch  0, batch    43 | loss: 8.3403358Losses:  7.990662097930908 1.1705769300460815
CurrentTrain: epoch  0, batch    44 | loss: 7.9906621Losses:  8.29380989074707 1.189770221710205
CurrentTrain: epoch  0, batch    45 | loss: 8.2938099Losses:  8.533737182617188 0.999431848526001
CurrentTrain: epoch  0, batch    46 | loss: 8.5337372Losses:  8.616573333740234 1.0841753482818604
CurrentTrain: epoch  0, batch    47 | loss: 8.6165733Losses:  8.215646743774414 1.169586420059204
CurrentTrain: epoch  0, batch    48 | loss: 8.2156467Losses:  8.637628555297852 1.2029242515563965
CurrentTrain: epoch  0, batch    49 | loss: 8.6376286Losses:  8.391034126281738 0.964340329170227
CurrentTrain: epoch  0, batch    50 | loss: 8.3910341Losses:  7.566482067108154 0.8811542391777039
CurrentTrain: epoch  0, batch    51 | loss: 7.5664821Losses:  8.887386322021484 1.262035846710205
CurrentTrain: epoch  0, batch    52 | loss: 8.8873863Losses:  7.815330982208252 0.9822400212287903
CurrentTrain: epoch  0, batch    53 | loss: 7.8153310Losses:  7.7128777503967285 0.9489390254020691
CurrentTrain: epoch  0, batch    54 | loss: 7.7128778Losses:  7.6851654052734375 1.151092290878296
CurrentTrain: epoch  0, batch    55 | loss: 7.6851654Losses:  8.248018264770508 1.0481902360916138
CurrentTrain: epoch  0, batch    56 | loss: 8.2480183Losses:  7.287583827972412 0.8839728832244873
CurrentTrain: epoch  0, batch    57 | loss: 7.2875838Losses:  7.245944976806641 0.801389217376709
CurrentTrain: epoch  0, batch    58 | loss: 7.2459450Losses:  7.9052934646606445 0.9312434196472168
CurrentTrain: epoch  0, batch    59 | loss: 7.9052935Losses:  7.607489585876465 0.8905453085899353
CurrentTrain: epoch  0, batch    60 | loss: 7.6074896Losses:  6.927579402923584 0.8713701963424683
CurrentTrain: epoch  0, batch    61 | loss: 6.9275794Losses:  8.45732307434082 1.1642154455184937
CurrentTrain: epoch  0, batch    62 | loss: 8.4573231Losses:  7.488893508911133 1.0145853757858276
CurrentTrain: epoch  1, batch     0 | loss: 7.4888935Losses:  6.881978988647461 0.7730902433395386
CurrentTrain: epoch  1, batch     1 | loss: 6.8819790Losses:  6.788061141967773 0.9498984813690186
CurrentTrain: epoch  1, batch     2 | loss: 6.7880611Losses:  7.929020404815674 1.171401023864746
CurrentTrain: epoch  1, batch     3 | loss: 7.9290204Losses:  7.5565009117126465 0.9934900403022766
CurrentTrain: epoch  1, batch     4 | loss: 7.5565009Losses:  6.628859996795654 0.7799402475357056
CurrentTrain: epoch  1, batch     5 | loss: 6.6288600Losses:  7.3525567054748535 0.9250383973121643
CurrentTrain: epoch  1, batch     6 | loss: 7.3525567Losses:  7.503864288330078 1.0552629232406616
CurrentTrain: epoch  1, batch     7 | loss: 7.5038643Losses:  7.379017353057861 1.1166058778762817
CurrentTrain: epoch  1, batch     8 | loss: 7.3790174Losses:  7.045160293579102 1.123144507408142
CurrentTrain: epoch  1, batch     9 | loss: 7.0451603Losses:  7.612140655517578 0.9512343406677246
CurrentTrain: epoch  1, batch    10 | loss: 7.6121407Losses:  7.222041130065918 1.163931131362915
CurrentTrain: epoch  1, batch    11 | loss: 7.2220411Losses:  7.341743469238281 0.9035626649856567
CurrentTrain: epoch  1, batch    12 | loss: 7.3417435Losses:  7.348844528198242 1.1141343116760254
CurrentTrain: epoch  1, batch    13 | loss: 7.3488445Losses:  7.912604808807373 1.0606937408447266
CurrentTrain: epoch  1, batch    14 | loss: 7.9126048Losses:  7.798483371734619 1.0941393375396729
CurrentTrain: epoch  1, batch    15 | loss: 7.7984834Losses:  7.828611373901367 1.0096204280853271
CurrentTrain: epoch  1, batch    16 | loss: 7.8286114Losses:  7.829734802246094 1.0442585945129395
CurrentTrain: epoch  1, batch    17 | loss: 7.8297348Losses:  6.619174003601074 0.8696115016937256
CurrentTrain: epoch  1, batch    18 | loss: 6.6191740Losses:  7.04201078414917 1.0991147756576538
CurrentTrain: epoch  1, batch    19 | loss: 7.0420108Losses:  6.717226505279541 0.8869392275810242
CurrentTrain: epoch  1, batch    20 | loss: 6.7172265Losses:  7.560704231262207 1.1285245418548584
CurrentTrain: epoch  1, batch    21 | loss: 7.5607042Losses:  7.252962589263916 1.0698368549346924
CurrentTrain: epoch  1, batch    22 | loss: 7.2529626Losses:  6.850853443145752 0.986021876335144
CurrentTrain: epoch  1, batch    23 | loss: 6.8508534Losses:  5.941919326782227 0.7218433022499084
CurrentTrain: epoch  1, batch    24 | loss: 5.9419193Losses:  5.984241962432861 0.6509112119674683
CurrentTrain: epoch  1, batch    25 | loss: 5.9842420Losses:  7.507261276245117 1.1290990114212036
CurrentTrain: epoch  1, batch    26 | loss: 7.5072613Losses:  6.808750152587891 1.0339622497558594
CurrentTrain: epoch  1, batch    27 | loss: 6.8087502Losses:  7.25816535949707 1.0252459049224854
CurrentTrain: epoch  1, batch    28 | loss: 7.2581654Losses:  6.453068256378174 0.9412720203399658
CurrentTrain: epoch  1, batch    29 | loss: 6.4530683Losses:  7.172384262084961 1.0068159103393555
CurrentTrain: epoch  1, batch    30 | loss: 7.1723843Losses:  7.491950035095215 0.9876542091369629
CurrentTrain: epoch  1, batch    31 | loss: 7.4919500Losses:  7.146862983703613 1.025933861732483
CurrentTrain: epoch  1, batch    32 | loss: 7.1468630Losses:  6.4852776527404785 0.9219335913658142
CurrentTrain: epoch  1, batch    33 | loss: 6.4852777Losses:  6.961653232574463 0.9647724628448486
CurrentTrain: epoch  1, batch    34 | loss: 6.9616532Losses:  7.328350067138672 1.0355843305587769
CurrentTrain: epoch  1, batch    35 | loss: 7.3283501Losses:  7.032778263092041 0.8767185211181641
CurrentTrain: epoch  1, batch    36 | loss: 7.0327783Losses:  6.2158708572387695 0.721554160118103
CurrentTrain: epoch  1, batch    37 | loss: 6.2158709Losses:  6.646748065948486 0.686724066734314
CurrentTrain: epoch  1, batch    38 | loss: 6.6467481Losses:  5.944854259490967 0.8028405904769897
CurrentTrain: epoch  1, batch    39 | loss: 5.9448543Losses:  6.392493724822998 0.9187605381011963
CurrentTrain: epoch  1, batch    40 | loss: 6.3924937Losses:  6.217972278594971 0.9616729021072388
CurrentTrain: epoch  1, batch    41 | loss: 6.2179723Losses:  6.439988613128662 0.8870989680290222
CurrentTrain: epoch  1, batch    42 | loss: 6.4399886Losses:  7.092545986175537 0.977988064289093
CurrentTrain: epoch  1, batch    43 | loss: 7.0925460Losses:  7.320504665374756 0.9916003346443176
CurrentTrain: epoch  1, batch    44 | loss: 7.3205047Losses:  7.2111053466796875 1.0411429405212402
CurrentTrain: epoch  1, batch    45 | loss: 7.2111053Losses:  6.792932510375977 1.0803390741348267
CurrentTrain: epoch  1, batch    46 | loss: 6.7929325Losses:  7.13444709777832 1.0038797855377197
CurrentTrain: epoch  1, batch    47 | loss: 7.1344471Losses:  7.016650676727295 0.7729624509811401
CurrentTrain: epoch  1, batch    48 | loss: 7.0166507Losses:  6.588600158691406 0.883086621761322
CurrentTrain: epoch  1, batch    49 | loss: 6.5886002Losses:  7.062201023101807 0.9415371417999268
CurrentTrain: epoch  1, batch    50 | loss: 7.0622010Losses:  6.786438941955566 1.0665316581726074
CurrentTrain: epoch  1, batch    51 | loss: 6.7864389Losses:  6.495351314544678 0.6243524551391602
CurrentTrain: epoch  1, batch    52 | loss: 6.4953513Losses:  5.766244411468506 0.7656510472297668
CurrentTrain: epoch  1, batch    53 | loss: 5.7662444Losses:  6.237533092498779 0.8214216232299805
CurrentTrain: epoch  1, batch    54 | loss: 6.2375331Losses:  6.012411117553711 0.8219413757324219
CurrentTrain: epoch  1, batch    55 | loss: 6.0124111Losses:  6.099595069885254 0.8129422664642334
CurrentTrain: epoch  1, batch    56 | loss: 6.0995951Losses:  7.642082214355469 0.9877017736434937
CurrentTrain: epoch  1, batch    57 | loss: 7.6420822Losses:  5.937872886657715 0.7715332508087158
CurrentTrain: epoch  1, batch    58 | loss: 5.9378729Losses:  6.95384407043457 0.9831298589706421
CurrentTrain: epoch  1, batch    59 | loss: 6.9538441Losses:  6.099367618560791 0.9216393232345581
CurrentTrain: epoch  1, batch    60 | loss: 6.0993676Losses:  7.679906368255615 1.1912397146224976
CurrentTrain: epoch  1, batch    61 | loss: 7.6799064Losses:  4.97963285446167 0.4637657403945923
CurrentTrain: epoch  1, batch    62 | loss: 4.9796329Losses:  5.540060043334961 0.8267284631729126
CurrentTrain: epoch  2, batch     0 | loss: 5.5400600Losses:  5.935025215148926 0.9219703674316406
CurrentTrain: epoch  2, batch     1 | loss: 5.9350252Losses:  5.3795857429504395 0.6730550527572632
CurrentTrain: epoch  2, batch     2 | loss: 5.3795857Losses:  7.2236199378967285 0.9103447198867798
CurrentTrain: epoch  2, batch     3 | loss: 7.2236199Losses:  7.580750942230225 0.9836776256561279
CurrentTrain: epoch  2, batch     4 | loss: 7.5807509Losses:  6.620813369750977 0.9449670910835266
CurrentTrain: epoch  2, batch     5 | loss: 6.6208134Losses:  6.382210731506348 0.9772463440895081
CurrentTrain: epoch  2, batch     6 | loss: 6.3822107Losses:  6.022136211395264 0.835234522819519
CurrentTrain: epoch  2, batch     7 | loss: 6.0221362Losses:  6.0331315994262695 0.7991060018539429
CurrentTrain: epoch  2, batch     8 | loss: 6.0331316Losses:  6.595470905303955 1.0675671100616455
CurrentTrain: epoch  2, batch     9 | loss: 6.5954709Losses:  5.948683738708496 0.6208606958389282
CurrentTrain: epoch  2, batch    10 | loss: 5.9486837Losses:  5.471550464630127 0.6417238712310791
CurrentTrain: epoch  2, batch    11 | loss: 5.4715505Losses:  5.499929428100586 0.7547578811645508
CurrentTrain: epoch  2, batch    12 | loss: 5.4999294Losses:  5.932502269744873 0.7630698680877686
CurrentTrain: epoch  2, batch    13 | loss: 5.9325023Losses:  6.205006122589111 0.7149391174316406
CurrentTrain: epoch  2, batch    14 | loss: 6.2050061Losses:  5.818278789520264 0.6584618091583252
CurrentTrain: epoch  2, batch    15 | loss: 5.8182788Losses:  6.157511234283447 0.8851597309112549
CurrentTrain: epoch  2, batch    16 | loss: 6.1575112Losses:  5.8650383949279785 0.7473536133766174
CurrentTrain: epoch  2, batch    17 | loss: 5.8650384Losses:  5.272648811340332 0.3687855005264282
CurrentTrain: epoch  2, batch    18 | loss: 5.2726488Losses:  6.2009501457214355 1.0617904663085938
CurrentTrain: epoch  2, batch    19 | loss: 6.2009501Losses:  5.637707233428955 0.9102750420570374
CurrentTrain: epoch  2, batch    20 | loss: 5.6377072Losses:  6.018433570861816 0.7504980564117432
CurrentTrain: epoch  2, batch    21 | loss: 6.0184336Losses:  5.956108570098877 0.8495670557022095
CurrentTrain: epoch  2, batch    22 | loss: 5.9561086Losses:  6.066195011138916 0.725405752658844
CurrentTrain: epoch  2, batch    23 | loss: 6.0661950Losses:  5.311973571777344 0.7374816536903381
CurrentTrain: epoch  2, batch    24 | loss: 5.3119736Losses:  5.5940961837768555 0.5995231866836548
CurrentTrain: epoch  2, batch    25 | loss: 5.5940962Losses:  5.992940902709961 0.8117785453796387
CurrentTrain: epoch  2, batch    26 | loss: 5.9929409Losses:  5.74322509765625 0.7829685807228088
CurrentTrain: epoch  2, batch    27 | loss: 5.7432251Losses:  5.390773773193359 0.6466902494430542
CurrentTrain: epoch  2, batch    28 | loss: 5.3907738Losses:  6.022489070892334 0.8202177286148071
CurrentTrain: epoch  2, batch    29 | loss: 6.0224891Losses:  5.63088321685791 0.8514138460159302
CurrentTrain: epoch  2, batch    30 | loss: 5.6308832Losses:  5.349694728851318 0.6047325730323792
CurrentTrain: epoch  2, batch    31 | loss: 5.3496947Losses:  5.866064071655273 0.6341610550880432
CurrentTrain: epoch  2, batch    32 | loss: 5.8660641Losses:  6.31920862197876 0.877095103263855
CurrentTrain: epoch  2, batch    33 | loss: 6.3192086Losses:  5.617514133453369 0.6423359513282776
CurrentTrain: epoch  2, batch    34 | loss: 5.6175141Losses:  6.531073570251465 0.9165401458740234
CurrentTrain: epoch  2, batch    35 | loss: 6.5310736Losses:  5.473630428314209 0.6339502930641174
CurrentTrain: epoch  2, batch    36 | loss: 5.4736304Losses:  6.164007186889648 0.7951600551605225
CurrentTrain: epoch  2, batch    37 | loss: 6.1640072Losses:  5.648886680603027 0.6538540720939636
CurrentTrain: epoch  2, batch    38 | loss: 5.6488867Losses:  5.439549922943115 0.8755081295967102
CurrentTrain: epoch  2, batch    39 | loss: 5.4395499Losses:  5.5514817237854 0.5923555493354797
CurrentTrain: epoch  2, batch    40 | loss: 5.5514817Losses:  6.6086649894714355 0.7865087389945984
CurrentTrain: epoch  2, batch    41 | loss: 6.6086650Losses:  5.5657830238342285 0.6130093932151794
CurrentTrain: epoch  2, batch    42 | loss: 5.5657830Losses:  5.676515102386475 0.8742724657058716
CurrentTrain: epoch  2, batch    43 | loss: 5.6765151Losses:  5.617114067077637 0.6277907490730286
CurrentTrain: epoch  2, batch    44 | loss: 5.6171141Losses:  5.687407493591309 0.9380872249603271
CurrentTrain: epoch  2, batch    45 | loss: 5.6874075Losses:  5.170283794403076 0.6198186874389648
CurrentTrain: epoch  2, batch    46 | loss: 5.1702838Losses:  5.961315155029297 0.7152844667434692
CurrentTrain: epoch  2, batch    47 | loss: 5.9613152Losses:  5.394172668457031 0.7238451242446899
CurrentTrain: epoch  2, batch    48 | loss: 5.3941727Losses:  5.979040145874023 0.9777948260307312
CurrentTrain: epoch  2, batch    49 | loss: 5.9790401Losses:  5.36438512802124 0.5435647964477539
CurrentTrain: epoch  2, batch    50 | loss: 5.3643851Losses:  5.110264301300049 0.6071997880935669
CurrentTrain: epoch  2, batch    51 | loss: 5.1102643Losses:  5.162822246551514 0.541902482509613
CurrentTrain: epoch  2, batch    52 | loss: 5.1628222Losses:  5.221942901611328 0.724794328212738
CurrentTrain: epoch  2, batch    53 | loss: 5.2219429Losses:  5.50954008102417 0.6624040007591248
CurrentTrain: epoch  2, batch    54 | loss: 5.5095401Losses:  5.089792251586914 0.6567419171333313
CurrentTrain: epoch  2, batch    55 | loss: 5.0897923Losses:  5.260839939117432 0.6631771326065063
CurrentTrain: epoch  2, batch    56 | loss: 5.2608399Losses:  6.108449935913086 0.8518538475036621
CurrentTrain: epoch  2, batch    57 | loss: 6.1084499Losses:  5.619909286499023 0.8265575766563416
CurrentTrain: epoch  2, batch    58 | loss: 5.6199093Losses:  5.1694769859313965 0.5656226277351379
CurrentTrain: epoch  2, batch    59 | loss: 5.1694770Losses:  5.443495750427246 0.7146056890487671
CurrentTrain: epoch  2, batch    60 | loss: 5.4434958Losses:  5.950887680053711 0.7878953218460083
CurrentTrain: epoch  2, batch    61 | loss: 5.9508877Losses:  4.8148193359375 0.6222742795944214
CurrentTrain: epoch  2, batch    62 | loss: 4.8148193Losses:  5.086909770965576 0.6685837507247925
CurrentTrain: epoch  3, batch     0 | loss: 5.0869098Losses:  5.136532306671143 0.7721565961837769
CurrentTrain: epoch  3, batch     1 | loss: 5.1365323Losses:  5.437955856323242 0.7262404561042786
CurrentTrain: epoch  3, batch     2 | loss: 5.4379559Losses:  5.539805889129639 0.6834825277328491
CurrentTrain: epoch  3, batch     3 | loss: 5.5398059Losses:  4.95928430557251 0.5606732964515686
CurrentTrain: epoch  3, batch     4 | loss: 4.9592843Losses:  5.588519096374512 0.6091580390930176
CurrentTrain: epoch  3, batch     5 | loss: 5.5885191Losses:  5.063849449157715 0.7704809308052063
CurrentTrain: epoch  3, batch     6 | loss: 5.0638494Losses:  5.432209491729736 0.6817352771759033
CurrentTrain: epoch  3, batch     7 | loss: 5.4322095Losses:  5.716547966003418 0.6075557470321655
CurrentTrain: epoch  3, batch     8 | loss: 5.7165480Losses:  5.651555061340332 0.8586299419403076
CurrentTrain: epoch  3, batch     9 | loss: 5.6515551Losses:  5.3466339111328125 0.7893744111061096
CurrentTrain: epoch  3, batch    10 | loss: 5.3466339Losses:  5.032760143280029 0.6644645929336548
CurrentTrain: epoch  3, batch    11 | loss: 5.0327601Losses:  5.232757091522217 0.7416013479232788
CurrentTrain: epoch  3, batch    12 | loss: 5.2327571Losses:  5.623255252838135 0.7234461903572083
CurrentTrain: epoch  3, batch    13 | loss: 5.6232553Losses:  5.662167549133301 0.7234843373298645
CurrentTrain: epoch  3, batch    14 | loss: 5.6621675Losses:  5.397305965423584 0.6619712114334106
CurrentTrain: epoch  3, batch    15 | loss: 5.3973060Losses:  5.8642473220825195 0.7310376167297363
CurrentTrain: epoch  3, batch    16 | loss: 5.8642473Losses:  5.200263500213623 0.6490041017532349
CurrentTrain: epoch  3, batch    17 | loss: 5.2002635Losses:  5.079222202301025 0.619968831539154
CurrentTrain: epoch  3, batch    18 | loss: 5.0792222Losses:  4.682032585144043 0.43877795338630676
CurrentTrain: epoch  3, batch    19 | loss: 4.6820326Losses:  4.962759971618652 0.5770108103752136
CurrentTrain: epoch  3, batch    20 | loss: 4.9627600Losses:  4.909053802490234 0.5072810053825378
CurrentTrain: epoch  3, batch    21 | loss: 4.9090538Losses:  5.577779769897461 0.8964447975158691
CurrentTrain: epoch  3, batch    22 | loss: 5.5777798Losses:  5.241122245788574 0.5594521760940552
CurrentTrain: epoch  3, batch    23 | loss: 5.2411222Losses:  5.468106269836426 0.6146216988563538
CurrentTrain: epoch  3, batch    24 | loss: 5.4681063Losses:  5.601535320281982 0.8018798232078552
CurrentTrain: epoch  3, batch    25 | loss: 5.6015353Losses:  4.794116497039795 0.48478633165359497
CurrentTrain: epoch  3, batch    26 | loss: 4.7941165Losses:  4.966383457183838 0.609557569026947
CurrentTrain: epoch  3, batch    27 | loss: 4.9663835Losses:  5.618642807006836 0.7480407953262329
CurrentTrain: epoch  3, batch    28 | loss: 5.6186428Losses:  5.089046955108643 0.7148622274398804
CurrentTrain: epoch  3, batch    29 | loss: 5.0890470Losses:  4.918581485748291 0.6185712218284607
CurrentTrain: epoch  3, batch    30 | loss: 4.9185815Losses:  5.1432414054870605 0.7675802707672119
CurrentTrain: epoch  3, batch    31 | loss: 5.1432414Losses:  5.138943195343018 0.6238343715667725
CurrentTrain: epoch  3, batch    32 | loss: 5.1389432Losses:  5.244666576385498 0.6220815181732178
CurrentTrain: epoch  3, batch    33 | loss: 5.2446666Losses:  5.3925604820251465 0.6302732229232788
CurrentTrain: epoch  3, batch    34 | loss: 5.3925605Losses:  5.041472434997559 0.7259732484817505
CurrentTrain: epoch  3, batch    35 | loss: 5.0414724Losses:  4.7507171630859375 0.5548005104064941
CurrentTrain: epoch  3, batch    36 | loss: 4.7507172Losses:  5.066868305206299 0.6979339122772217
CurrentTrain: epoch  3, batch    37 | loss: 5.0668683Losses:  4.879657745361328 0.6000189781188965
CurrentTrain: epoch  3, batch    38 | loss: 4.8796577Losses:  4.855466365814209 0.32022762298583984
CurrentTrain: epoch  3, batch    39 | loss: 4.8554664Losses:  5.612118244171143 0.7728861570358276
CurrentTrain: epoch  3, batch    40 | loss: 5.6121182Losses:  5.105706214904785 0.5806079506874084
CurrentTrain: epoch  3, batch    41 | loss: 5.1057062Losses:  4.695562839508057 0.5739574432373047
CurrentTrain: epoch  3, batch    42 | loss: 4.6955628Losses:  5.021159648895264 0.44647216796875
CurrentTrain: epoch  3, batch    43 | loss: 5.0211596Losses:  5.184293746948242 0.5033400654792786
CurrentTrain: epoch  3, batch    44 | loss: 5.1842937Losses:  5.107970714569092 0.6543417572975159
CurrentTrain: epoch  3, batch    45 | loss: 5.1079707Losses:  5.0637969970703125 0.800087034702301
CurrentTrain: epoch  3, batch    46 | loss: 5.0637970Losses:  4.743622779846191 0.44844794273376465
CurrentTrain: epoch  3, batch    47 | loss: 4.7436228Losses:  4.910167694091797 0.6481834650039673
CurrentTrain: epoch  3, batch    48 | loss: 4.9101677Losses:  4.7457404136657715 0.5204713940620422
CurrentTrain: epoch  3, batch    49 | loss: 4.7457404Losses:  5.35029411315918 0.7218681573867798
CurrentTrain: epoch  3, batch    50 | loss: 5.3502941Losses:  5.331323623657227 0.717180609703064
CurrentTrain: epoch  3, batch    51 | loss: 5.3313236Losses:  4.658806800842285 0.5404850840568542
CurrentTrain: epoch  3, batch    52 | loss: 4.6588068Losses:  5.655658721923828 0.74625563621521
CurrentTrain: epoch  3, batch    53 | loss: 5.6556587Losses:  4.720447063446045 0.40301889181137085
CurrentTrain: epoch  3, batch    54 | loss: 4.7204471Losses:  4.702859401702881 0.457854300737381
CurrentTrain: epoch  3, batch    55 | loss: 4.7028594Losses:  4.935668468475342 0.575445830821991
CurrentTrain: epoch  3, batch    56 | loss: 4.9356685Losses:  5.025354385375977 0.5530680418014526
CurrentTrain: epoch  3, batch    57 | loss: 5.0253544Losses:  4.689664840698242 0.47654685378074646
CurrentTrain: epoch  3, batch    58 | loss: 4.6896648Losses:  5.684686660766602 0.5802738666534424
CurrentTrain: epoch  3, batch    59 | loss: 5.6846867Losses:  4.708500862121582 0.5398249626159668
CurrentTrain: epoch  3, batch    60 | loss: 4.7085009Losses:  4.976432800292969 0.5204589366912842
CurrentTrain: epoch  3, batch    61 | loss: 4.9764328Losses:  4.839766025543213 0.48111042380332947
CurrentTrain: epoch  3, batch    62 | loss: 4.8397660Losses:  5.216753005981445 0.6862680912017822
CurrentTrain: epoch  4, batch     0 | loss: 5.2167530Losses:  4.786044597625732 0.3739374279975891
CurrentTrain: epoch  4, batch     1 | loss: 4.7860446Losses:  5.219369888305664 0.5704365372657776
CurrentTrain: epoch  4, batch     2 | loss: 5.2193699Losses:  4.620558738708496 0.5508061051368713
CurrentTrain: epoch  4, batch     3 | loss: 4.6205587Losses:  4.633374214172363 0.5399238467216492
CurrentTrain: epoch  4, batch     4 | loss: 4.6333742Losses:  4.7511491775512695 0.46730974316596985
CurrentTrain: epoch  4, batch     5 | loss: 4.7511492Losses:  5.323209762573242 0.5823418498039246
CurrentTrain: epoch  4, batch     6 | loss: 5.3232098Losses:  4.918238162994385 0.44177377223968506
CurrentTrain: epoch  4, batch     7 | loss: 4.9182382Losses:  4.842644214630127 0.6164361238479614
CurrentTrain: epoch  4, batch     8 | loss: 4.8426442Losses:  4.963778018951416 0.6793137788772583
CurrentTrain: epoch  4, batch     9 | loss: 4.9637780Losses:  4.693158149719238 0.5002273917198181
CurrentTrain: epoch  4, batch    10 | loss: 4.6931581Losses:  5.083569049835205 0.4272390305995941
CurrentTrain: epoch  4, batch    11 | loss: 5.0835690Losses:  4.828205585479736 0.6212716698646545
CurrentTrain: epoch  4, batch    12 | loss: 4.8282056Losses:  4.633420944213867 0.44238439202308655
CurrentTrain: epoch  4, batch    13 | loss: 4.6334209Losses:  5.036084175109863 0.6302528381347656
CurrentTrain: epoch  4, batch    14 | loss: 5.0360842Losses:  4.948873043060303 0.5395363569259644
CurrentTrain: epoch  4, batch    15 | loss: 4.9488730Losses:  4.938934326171875 0.5962716341018677
CurrentTrain: epoch  4, batch    16 | loss: 4.9389343Losses:  4.70656681060791 0.45119887590408325
CurrentTrain: epoch  4, batch    17 | loss: 4.7065668Losses:  4.7935686111450195 0.5433996319770813
CurrentTrain: epoch  4, batch    18 | loss: 4.7935686Losses:  4.62289571762085 0.419511079788208
CurrentTrain: epoch  4, batch    19 | loss: 4.6228957Losses:  4.748210430145264 0.6320789456367493
CurrentTrain: epoch  4, batch    20 | loss: 4.7482104Losses:  4.929753303527832 0.5422098636627197
CurrentTrain: epoch  4, batch    21 | loss: 4.9297533Losses:  4.606665134429932 0.40555644035339355
CurrentTrain: epoch  4, batch    22 | loss: 4.6066651Losses:  4.80307674407959 0.48525500297546387
CurrentTrain: epoch  4, batch    23 | loss: 4.8030767Losses:  4.734409809112549 0.5445698499679565
CurrentTrain: epoch  4, batch    24 | loss: 4.7344098Losses:  4.750054836273193 0.6773520112037659
CurrentTrain: epoch  4, batch    25 | loss: 4.7500548Losses:  4.630825042724609 0.6191009283065796
CurrentTrain: epoch  4, batch    26 | loss: 4.6308250Losses:  4.842411994934082 0.4619806110858917
CurrentTrain: epoch  4, batch    27 | loss: 4.8424120Losses:  4.876868724822998 0.5699014663696289
CurrentTrain: epoch  4, batch    28 | loss: 4.8768687Losses:  4.849030494689941 0.6481955051422119
CurrentTrain: epoch  4, batch    29 | loss: 4.8490305Losses:  4.657890796661377 0.4617115259170532
CurrentTrain: epoch  4, batch    30 | loss: 4.6578908Losses:  4.710317611694336 0.49550876021385193
CurrentTrain: epoch  4, batch    31 | loss: 4.7103176Losses:  4.600671291351318 0.5111690759658813
CurrentTrain: epoch  4, batch    32 | loss: 4.6006713Losses:  4.564160346984863 0.47075724601745605
CurrentTrain: epoch  4, batch    33 | loss: 4.5641603Losses:  4.630578517913818 0.5249811410903931
CurrentTrain: epoch  4, batch    34 | loss: 4.6305785Losses:  4.862210750579834 0.5896573662757874
CurrentTrain: epoch  4, batch    35 | loss: 4.8622108Losses:  4.721800327301025 0.4612264633178711
CurrentTrain: epoch  4, batch    36 | loss: 4.7218003Losses:  4.668949604034424 0.5853204727172852
CurrentTrain: epoch  4, batch    37 | loss: 4.6689496Losses:  4.556075096130371 0.5472514033317566
CurrentTrain: epoch  4, batch    38 | loss: 4.5560751Losses:  4.7233967781066895 0.664291262626648
CurrentTrain: epoch  4, batch    39 | loss: 4.7233968Losses:  4.692817687988281 0.5747201442718506
CurrentTrain: epoch  4, batch    40 | loss: 4.6928177Losses:  4.62278938293457 0.4385634660720825
CurrentTrain: epoch  4, batch    41 | loss: 4.6227894Losses:  4.61442232131958 0.3941868841648102
CurrentTrain: epoch  4, batch    42 | loss: 4.6144223Losses:  4.561445713043213 0.5549257397651672
CurrentTrain: epoch  4, batch    43 | loss: 4.5614457Losses:  4.51568603515625 0.44061365723609924
CurrentTrain: epoch  4, batch    44 | loss: 4.5156860Losses:  4.7445454597473145 0.6053255200386047
CurrentTrain: epoch  4, batch    45 | loss: 4.7445455Losses:  4.750363826751709 0.48107028007507324
CurrentTrain: epoch  4, batch    46 | loss: 4.7503638Losses:  4.550732135772705 0.4099411368370056
CurrentTrain: epoch  4, batch    47 | loss: 4.5507321Losses:  4.880838871002197 0.5892709493637085
CurrentTrain: epoch  4, batch    48 | loss: 4.8808389Losses:  4.4538164138793945 0.3222638964653015
CurrentTrain: epoch  4, batch    49 | loss: 4.4538164Losses:  4.539569854736328 0.38678765296936035
CurrentTrain: epoch  4, batch    50 | loss: 4.5395699Losses:  4.656619548797607 0.4646822214126587
CurrentTrain: epoch  4, batch    51 | loss: 4.6566195Losses:  4.593650817871094 0.4765755534172058
CurrentTrain: epoch  4, batch    52 | loss: 4.5936508Losses:  5.209956169128418 0.5785554647445679
CurrentTrain: epoch  4, batch    53 | loss: 5.2099562Losses:  4.566356658935547 0.4495621919631958
CurrentTrain: epoch  4, batch    54 | loss: 4.5663567Losses:  4.67969274520874 0.6147204637527466
CurrentTrain: epoch  4, batch    55 | loss: 4.6796927Losses:  4.613528251647949 0.5487403273582458
CurrentTrain: epoch  4, batch    56 | loss: 4.6135283Losses:  4.450504302978516 0.4956648051738739
CurrentTrain: epoch  4, batch    57 | loss: 4.4505043Losses:  4.588130474090576 0.5403451919555664
CurrentTrain: epoch  4, batch    58 | loss: 4.5881305Losses:  4.500465393066406 0.5881331562995911
CurrentTrain: epoch  4, batch    59 | loss: 4.5004654Losses:  4.865486145019531 0.47955822944641113
CurrentTrain: epoch  4, batch    60 | loss: 4.8654861Losses:  4.5388383865356445 0.43319547176361084
CurrentTrain: epoch  4, batch    61 | loss: 4.5388384Losses:  4.258882522583008 0.23156222701072693
CurrentTrain: epoch  4, batch    62 | loss: 4.2588825Losses:  4.557845115661621 0.40030306577682495
CurrentTrain: epoch  5, batch     0 | loss: 4.5578451Losses:  4.534585952758789 0.5161930322647095
CurrentTrain: epoch  5, batch     1 | loss: 4.5345860Losses:  4.571826457977295 0.5094045996665955
CurrentTrain: epoch  5, batch     2 | loss: 4.5718265Losses:  4.587223529815674 0.5651247501373291
CurrentTrain: epoch  5, batch     3 | loss: 4.5872235Losses:  4.521746635437012 0.5210824608802795
CurrentTrain: epoch  5, batch     4 | loss: 4.5217466Losses:  4.492032527923584 0.5025068521499634
CurrentTrain: epoch  5, batch     5 | loss: 4.4920325Losses:  4.759799957275391 0.5351920127868652
CurrentTrain: epoch  5, batch     6 | loss: 4.7598000Losses:  4.594376564025879 0.485195130109787
CurrentTrain: epoch  5, batch     7 | loss: 4.5943766Losses:  4.517543315887451 0.5391706228256226
CurrentTrain: epoch  5, batch     8 | loss: 4.5175433Losses:  4.713564872741699 0.5836160182952881
CurrentTrain: epoch  5, batch     9 | loss: 4.7135649Losses:  4.595321178436279 0.49818599224090576
CurrentTrain: epoch  5, batch    10 | loss: 4.5953212Losses:  4.513901233673096 0.4507574737071991
CurrentTrain: epoch  5, batch    11 | loss: 4.5139012Losses:  4.527165412902832 0.5625739693641663
CurrentTrain: epoch  5, batch    12 | loss: 4.5271654Losses:  4.43236780166626 0.5071744918823242
CurrentTrain: epoch  5, batch    13 | loss: 4.4323678Losses:  4.504312038421631 0.5825463533401489
CurrentTrain: epoch  5, batch    14 | loss: 4.5043120Losses:  4.433621406555176 0.4318881630897522
CurrentTrain: epoch  5, batch    15 | loss: 4.4336214Losses:  4.61928129196167 0.4504649043083191
CurrentTrain: epoch  5, batch    16 | loss: 4.6192813Losses:  4.3971028327941895 0.41158396005630493
CurrentTrain: epoch  5, batch    17 | loss: 4.3971028Losses:  4.427550315856934 0.41947141289711
CurrentTrain: epoch  5, batch    18 | loss: 4.4275503Losses:  4.5188307762146 0.5598981380462646
CurrentTrain: epoch  5, batch    19 | loss: 4.5188308Losses:  4.423142910003662 0.4340277314186096
CurrentTrain: epoch  5, batch    20 | loss: 4.4231429Losses:  4.330478668212891 0.40156227350234985
CurrentTrain: epoch  5, batch    21 | loss: 4.3304787Losses:  4.252173900604248 0.29609352350234985
CurrentTrain: epoch  5, batch    22 | loss: 4.2521739Losses:  4.539095878601074 0.44773465394973755
CurrentTrain: epoch  5, batch    23 | loss: 4.5390959Losses:  4.333106517791748 0.33790379762649536
CurrentTrain: epoch  5, batch    24 | loss: 4.3331065Losses:  4.550197601318359 0.47209930419921875
CurrentTrain: epoch  5, batch    25 | loss: 4.5501976Losses:  4.471010208129883 0.4526183605194092
CurrentTrain: epoch  5, batch    26 | loss: 4.4710102Losses:  4.455631732940674 0.5139705538749695
CurrentTrain: epoch  5, batch    27 | loss: 4.4556317Losses:  4.374820232391357 0.35318347811698914
CurrentTrain: epoch  5, batch    28 | loss: 4.3748202Losses:  4.516170501708984 0.5948070287704468
CurrentTrain: epoch  5, batch    29 | loss: 4.5161705Losses:  4.579193115234375 0.4656487703323364
CurrentTrain: epoch  5, batch    30 | loss: 4.5791931Losses:  4.475438117980957 0.4113289415836334
CurrentTrain: epoch  5, batch    31 | loss: 4.4754381Losses:  4.482851028442383 0.5167989730834961
CurrentTrain: epoch  5, batch    32 | loss: 4.4828510Losses:  4.26652717590332 0.34967657923698425
CurrentTrain: epoch  5, batch    33 | loss: 4.2665272Losses:  4.4311394691467285 0.45125165581703186
CurrentTrain: epoch  5, batch    34 | loss: 4.4311395Losses:  4.382744789123535 0.4724352955818176
CurrentTrain: epoch  5, batch    35 | loss: 4.3827448Losses:  4.418854713439941 0.4177829623222351
CurrentTrain: epoch  5, batch    36 | loss: 4.4188547Losses:  4.263721466064453 0.35004955530166626
CurrentTrain: epoch  5, batch    37 | loss: 4.2637215Losses:  4.335226535797119 0.4373876452445984
CurrentTrain: epoch  5, batch    38 | loss: 4.3352265Losses:  4.432053565979004 0.4266532361507416
CurrentTrain: epoch  5, batch    39 | loss: 4.4320536Losses:  4.354502201080322 0.45073065161705017
CurrentTrain: epoch  5, batch    40 | loss: 4.3545022Losses:  4.464522838592529 0.4278116226196289
CurrentTrain: epoch  5, batch    41 | loss: 4.4645228Losses:  4.412496566772461 0.349621057510376
CurrentTrain: epoch  5, batch    42 | loss: 4.4124966Losses:  4.433953285217285 0.40255212783813477
CurrentTrain: epoch  5, batch    43 | loss: 4.4339533Losses:  4.578911304473877 0.5312182903289795
CurrentTrain: epoch  5, batch    44 | loss: 4.5789113Losses:  4.400822639465332 0.479203999042511
CurrentTrain: epoch  5, batch    45 | loss: 4.4008226Losses:  4.321370601654053 0.36125490069389343
CurrentTrain: epoch  5, batch    46 | loss: 4.3213706Losses:  4.429750442504883 0.4438106417655945
CurrentTrain: epoch  5, batch    47 | loss: 4.4297504Losses:  4.476265907287598 0.4160313010215759
CurrentTrain: epoch  5, batch    48 | loss: 4.4762659Losses:  4.635275840759277 0.3457568883895874
CurrentTrain: epoch  5, batch    49 | loss: 4.6352758Losses:  4.372391223907471 0.39910709857940674
CurrentTrain: epoch  5, batch    50 | loss: 4.3723912Losses:  4.299434661865234 0.34822890162467957
CurrentTrain: epoch  5, batch    51 | loss: 4.2994347Losses:  4.327354431152344 0.3048059940338135
CurrentTrain: epoch  5, batch    52 | loss: 4.3273544Losses:  4.427941799163818 0.5157994031906128
CurrentTrain: epoch  5, batch    53 | loss: 4.4279418Losses:  4.399396896362305 0.5307461023330688
CurrentTrain: epoch  5, batch    54 | loss: 4.3993969Losses:  4.389545917510986 0.40007147192955017
CurrentTrain: epoch  5, batch    55 | loss: 4.3895459Losses:  4.447516441345215 0.41149452328681946
CurrentTrain: epoch  5, batch    56 | loss: 4.4475164Losses:  4.288597106933594 0.2993776798248291
CurrentTrain: epoch  5, batch    57 | loss: 4.2885971Losses:  4.529204845428467 0.4906068444252014
CurrentTrain: epoch  5, batch    58 | loss: 4.5292048Losses:  4.40739107131958 0.48868444561958313
CurrentTrain: epoch  5, batch    59 | loss: 4.4073911Losses:  4.310861110687256 0.3240671455860138
CurrentTrain: epoch  5, batch    60 | loss: 4.3108611Losses:  4.4157280921936035 0.4528685212135315
CurrentTrain: epoch  5, batch    61 | loss: 4.4157281Losses:  4.303579807281494 0.2813658118247986
CurrentTrain: epoch  5, batch    62 | loss: 4.3035798Losses:  4.373826026916504 0.3400874137878418
CurrentTrain: epoch  6, batch     0 | loss: 4.3738260Losses:  4.317552089691162 0.3515724539756775
CurrentTrain: epoch  6, batch     1 | loss: 4.3175521Losses:  4.364317417144775 0.3831014335155487
CurrentTrain: epoch  6, batch     2 | loss: 4.3643174Losses:  4.501349449157715 0.5365030169487
CurrentTrain: epoch  6, batch     3 | loss: 4.5013494Losses:  4.323148727416992 0.40561360120773315
CurrentTrain: epoch  6, batch     4 | loss: 4.3231487Losses:  4.612795352935791 0.42248743772506714
CurrentTrain: epoch  6, batch     5 | loss: 4.6127954Losses:  4.411812782287598 0.511214017868042
CurrentTrain: epoch  6, batch     6 | loss: 4.4118128Losses:  4.364948272705078 0.48548489809036255
CurrentTrain: epoch  6, batch     7 | loss: 4.3649483Losses:  4.308048725128174 0.23289762437343597
CurrentTrain: epoch  6, batch     8 | loss: 4.3080487Losses:  4.317187786102295 0.4011320471763611
CurrentTrain: epoch  6, batch     9 | loss: 4.3171878Losses:  4.568101406097412 0.5637937784194946
CurrentTrain: epoch  6, batch    10 | loss: 4.5681014Losses:  4.214200973510742 0.24512436985969543
CurrentTrain: epoch  6, batch    11 | loss: 4.2142010Losses:  4.4080376625061035 0.45404356718063354
CurrentTrain: epoch  6, batch    12 | loss: 4.4080377Losses:  4.291116714477539 0.42633914947509766
CurrentTrain: epoch  6, batch    13 | loss: 4.2911167Losses:  4.334597587585449 0.4421895146369934
CurrentTrain: epoch  6, batch    14 | loss: 4.3345976Losses:  4.396820545196533 0.5466670989990234
CurrentTrain: epoch  6, batch    15 | loss: 4.3968205Losses:  4.2283735275268555 0.3143366873264313
CurrentTrain: epoch  6, batch    16 | loss: 4.2283735Losses:  4.408777236938477 0.48265835642814636
CurrentTrain: epoch  6, batch    17 | loss: 4.4087772Losses:  4.34350061416626 0.48737797141075134
CurrentTrain: epoch  6, batch    18 | loss: 4.3435006Losses:  4.261425495147705 0.3296319246292114
CurrentTrain: epoch  6, batch    19 | loss: 4.2614255Losses:  4.24062442779541 0.33943265676498413
CurrentTrain: epoch  6, batch    20 | loss: 4.2406244Losses:  4.352978706359863 0.40527939796447754
CurrentTrain: epoch  6, batch    21 | loss: 4.3529787Losses:  4.300792217254639 0.35733115673065186
CurrentTrain: epoch  6, batch    22 | loss: 4.3007922Losses:  4.378571033477783 0.4323231279850006
CurrentTrain: epoch  6, batch    23 | loss: 4.3785710Losses:  4.313271999359131 0.43008866906166077
CurrentTrain: epoch  6, batch    24 | loss: 4.3132720Losses:  4.3688554763793945 0.3603840470314026
CurrentTrain: epoch  6, batch    25 | loss: 4.3688555Losses:  4.301326274871826 0.42747628688812256
CurrentTrain: epoch  6, batch    26 | loss: 4.3013263Losses:  4.373868942260742 0.4690631031990051
CurrentTrain: epoch  6, batch    27 | loss: 4.3738689Losses:  4.240591049194336 0.33516520261764526
CurrentTrain: epoch  6, batch    28 | loss: 4.2405910Losses:  4.312016010284424 0.4031204581260681
CurrentTrain: epoch  6, batch    29 | loss: 4.3120160Losses:  4.199822425842285 0.2664722204208374
CurrentTrain: epoch  6, batch    30 | loss: 4.1998224Losses:  4.354806900024414 0.32392922043800354
CurrentTrain: epoch  6, batch    31 | loss: 4.3548069Losses:  4.313969135284424 0.307653546333313
CurrentTrain: epoch  6, batch    32 | loss: 4.3139691Losses:  4.300527095794678 0.3458637595176697
CurrentTrain: epoch  6, batch    33 | loss: 4.3005271Losses:  4.301665306091309 0.33139508962631226
CurrentTrain: epoch  6, batch    34 | loss: 4.3016653Losses:  4.438342571258545 0.4152318239212036
CurrentTrain: epoch  6, batch    35 | loss: 4.4383426Losses:  4.337008953094482 0.4338405728340149
CurrentTrain: epoch  6, batch    36 | loss: 4.3370090Losses:  4.3723320960998535 0.48718827962875366
CurrentTrain: epoch  6, batch    37 | loss: 4.3723321Losses:  4.346747398376465 0.29214322566986084
CurrentTrain: epoch  6, batch    38 | loss: 4.3467474Losses:  4.39980411529541 0.45247089862823486
CurrentTrain: epoch  6, batch    39 | loss: 4.3998041Losses:  4.361852169036865 0.35897332429885864
CurrentTrain: epoch  6, batch    40 | loss: 4.3618522Losses:  4.317028522491455 0.4116228520870209
CurrentTrain: epoch  6, batch    41 | loss: 4.3170285Losses:  4.383652210235596 0.45738276839256287
CurrentTrain: epoch  6, batch    42 | loss: 4.3836522Losses:  4.294503211975098 0.4177626669406891
CurrentTrain: epoch  6, batch    43 | loss: 4.2945032Losses:  4.386712074279785 0.40578997135162354
CurrentTrain: epoch  6, batch    44 | loss: 4.3867121Losses:  4.341697692871094 0.41658368706703186
CurrentTrain: epoch  6, batch    45 | loss: 4.3416977Losses:  4.320456504821777 0.4024736285209656
CurrentTrain: epoch  6, batch    46 | loss: 4.3204565Losses:  4.254683971405029 0.3822387754917145
CurrentTrain: epoch  6, batch    47 | loss: 4.2546840Losses:  4.328286170959473 0.45601606369018555
CurrentTrain: epoch  6, batch    48 | loss: 4.3282862Losses:  4.386106014251709 0.2804574966430664
CurrentTrain: epoch  6, batch    49 | loss: 4.3861060Losses:  4.328010559082031 0.4352145195007324
CurrentTrain: epoch  6, batch    50 | loss: 4.3280106Losses:  4.299032211303711 0.4030492901802063
CurrentTrain: epoch  6, batch    51 | loss: 4.2990322Losses:  4.441984176635742 0.46373850107192993
CurrentTrain: epoch  6, batch    52 | loss: 4.4419842Losses:  4.334458827972412 0.42498648166656494
CurrentTrain: epoch  6, batch    53 | loss: 4.3344588Losses:  4.235543727874756 0.29006868600845337
CurrentTrain: epoch  6, batch    54 | loss: 4.2355437Losses:  4.28067684173584 0.3281322121620178
CurrentTrain: epoch  6, batch    55 | loss: 4.2806768Losses:  4.300631523132324 0.431954562664032
CurrentTrain: epoch  6, batch    56 | loss: 4.3006315Losses:  4.331053256988525 0.39111602306365967
CurrentTrain: epoch  6, batch    57 | loss: 4.3310533Losses:  4.441115856170654 0.49651816487312317
CurrentTrain: epoch  6, batch    58 | loss: 4.4411159Losses:  4.291947364807129 0.40648290514945984
CurrentTrain: epoch  6, batch    59 | loss: 4.2919474Losses:  4.292191028594971 0.38789528608322144
CurrentTrain: epoch  6, batch    60 | loss: 4.2921910Losses:  4.202263355255127 0.3110033869743347
CurrentTrain: epoch  6, batch    61 | loss: 4.2022634Losses:  4.163005352020264 0.26827481389045715
CurrentTrain: epoch  6, batch    62 | loss: 4.1630054Losses:  4.295440196990967 0.4302447736263275
CurrentTrain: epoch  7, batch     0 | loss: 4.2954402Losses:  4.355297565460205 0.4520684778690338
CurrentTrain: epoch  7, batch     1 | loss: 4.3552976Losses:  4.272698402404785 0.3345223069190979
CurrentTrain: epoch  7, batch     2 | loss: 4.2726984Losses:  4.327823162078857 0.39361485838890076
CurrentTrain: epoch  7, batch     3 | loss: 4.3278232Losses:  4.211860179901123 0.24710746109485626
CurrentTrain: epoch  7, batch     4 | loss: 4.2118602Losses:  4.265673637390137 0.3713831901550293
CurrentTrain: epoch  7, batch     5 | loss: 4.2656736Losses:  4.334168434143066 0.43062669038772583
CurrentTrain: epoch  7, batch     6 | loss: 4.3341684Losses:  4.241147518157959 0.3640323877334595
CurrentTrain: epoch  7, batch     7 | loss: 4.2411475Losses:  4.365340709686279 0.4325733184814453
CurrentTrain: epoch  7, batch     8 | loss: 4.3653407Losses:  4.232811450958252 0.33700791001319885
CurrentTrain: epoch  7, batch     9 | loss: 4.2328115Losses:  4.361110687255859 0.46268078684806824
CurrentTrain: epoch  7, batch    10 | loss: 4.3611107Losses:  4.321118354797363 0.43889665603637695
CurrentTrain: epoch  7, batch    11 | loss: 4.3211184Losses:  4.306798934936523 0.38466864824295044
CurrentTrain: epoch  7, batch    12 | loss: 4.3067989Losses:  4.280086517333984 0.434889018535614
CurrentTrain: epoch  7, batch    13 | loss: 4.2800865Losses:  4.311363697052002 0.3885790705680847
CurrentTrain: epoch  7, batch    14 | loss: 4.3113637Losses:  4.280853271484375 0.3537135124206543
CurrentTrain: epoch  7, batch    15 | loss: 4.2808533Losses:  4.211113929748535 0.372772216796875
CurrentTrain: epoch  7, batch    16 | loss: 4.2111139Losses:  4.210320949554443 0.3087065517902374
CurrentTrain: epoch  7, batch    17 | loss: 4.2103209Losses:  4.338342666625977 0.4642425775527954
CurrentTrain: epoch  7, batch    18 | loss: 4.3383427Losses:  4.240686416625977 0.23401041328907013
CurrentTrain: epoch  7, batch    19 | loss: 4.2406864Losses:  4.294887065887451 0.4032834768295288
CurrentTrain: epoch  7, batch    20 | loss: 4.2948871Losses:  4.293736457824707 0.40936172008514404
CurrentTrain: epoch  7, batch    21 | loss: 4.2937365Losses:  4.269071578979492 0.361055850982666
CurrentTrain: epoch  7, batch    22 | loss: 4.2690716Losses:  4.240141868591309 0.39423003792762756
CurrentTrain: epoch  7, batch    23 | loss: 4.2401419Losses:  4.283884525299072 0.3510288596153259
CurrentTrain: epoch  7, batch    24 | loss: 4.2838845Losses:  4.300847053527832 0.390810489654541
CurrentTrain: epoch  7, batch    25 | loss: 4.3008471Losses:  4.201447486877441 0.251172810792923
CurrentTrain: epoch  7, batch    26 | loss: 4.2014475Losses:  4.228071689605713 0.3374188542366028
CurrentTrain: epoch  7, batch    27 | loss: 4.2280717Losses:  4.25626277923584 0.3397725224494934
CurrentTrain: epoch  7, batch    28 | loss: 4.2562628Losses:  4.224149227142334 0.28645840287208557
CurrentTrain: epoch  7, batch    29 | loss: 4.2241492Losses:  4.2039008140563965 0.29476630687713623
CurrentTrain: epoch  7, batch    30 | loss: 4.2039008Losses:  4.1899495124816895 0.258341521024704
CurrentTrain: epoch  7, batch    31 | loss: 4.1899495Losses:  4.177855491638184 0.3094242215156555
CurrentTrain: epoch  7, batch    32 | loss: 4.1778555Losses:  4.157134056091309 0.3174936771392822
CurrentTrain: epoch  7, batch    33 | loss: 4.1571341Losses:  4.173196315765381 0.3128453493118286
CurrentTrain: epoch  7, batch    34 | loss: 4.1731963Losses:  4.239566802978516 0.35425230860710144
CurrentTrain: epoch  7, batch    35 | loss: 4.2395668Losses:  4.22212028503418 0.35886743664741516
CurrentTrain: epoch  7, batch    36 | loss: 4.2221203Losses:  4.2723188400268555 0.32858091592788696
CurrentTrain: epoch  7, batch    37 | loss: 4.2723188Losses:  4.237791061401367 0.35793644189834595
CurrentTrain: epoch  7, batch    38 | loss: 4.2377911Losses:  4.160816192626953 0.30231818556785583
CurrentTrain: epoch  7, batch    39 | loss: 4.1608162Losses:  4.167754650115967 0.2412227988243103
CurrentTrain: epoch  7, batch    40 | loss: 4.1677547Losses:  4.186280727386475 0.26912760734558105
CurrentTrain: epoch  7, batch    41 | loss: 4.1862807Losses:  4.176291465759277 0.28819501399993896
CurrentTrain: epoch  7, batch    42 | loss: 4.1762915Losses:  4.242058277130127 0.40209949016571045
CurrentTrain: epoch  7, batch    43 | loss: 4.2420583Losses:  4.23571252822876 0.33050090074539185
CurrentTrain: epoch  7, batch    44 | loss: 4.2357125Losses:  4.249432563781738 0.3775820732116699
CurrentTrain: epoch  7, batch    45 | loss: 4.2494326Losses:  4.219668388366699 0.3102542459964752
CurrentTrain: epoch  7, batch    46 | loss: 4.2196684Losses:  4.214426517486572 0.31158703565597534
CurrentTrain: epoch  7, batch    47 | loss: 4.2144265Losses:  4.270193099975586 0.38134056329727173
CurrentTrain: epoch  7, batch    48 | loss: 4.2701931Losses:  4.249752998352051 0.31862279772758484
CurrentTrain: epoch  7, batch    49 | loss: 4.2497530Losses:  4.311947345733643 0.4189785122871399
CurrentTrain: epoch  7, batch    50 | loss: 4.3119473Losses:  4.286261558532715 0.3489775061607361
CurrentTrain: epoch  7, batch    51 | loss: 4.2862616Losses:  4.192371845245361 0.28409701585769653
CurrentTrain: epoch  7, batch    52 | loss: 4.1923718Losses:  4.185397624969482 0.27451878786087036
CurrentTrain: epoch  7, batch    53 | loss: 4.1853976Losses:  4.236523151397705 0.35670948028564453
CurrentTrain: epoch  7, batch    54 | loss: 4.2365232Losses:  4.297580242156982 0.4061111807823181
CurrentTrain: epoch  7, batch    55 | loss: 4.2975802Losses:  4.189554214477539 0.26671233773231506
CurrentTrain: epoch  7, batch    56 | loss: 4.1895542Losses:  4.28060245513916 0.34007740020751953
CurrentTrain: epoch  7, batch    57 | loss: 4.2806025Losses:  4.2343549728393555 0.3533642292022705
CurrentTrain: epoch  7, batch    58 | loss: 4.2343550Losses:  4.250606060028076 0.3936721682548523
CurrentTrain: epoch  7, batch    59 | loss: 4.2506061Losses:  4.260204792022705 0.39754581451416016
CurrentTrain: epoch  7, batch    60 | loss: 4.2602048Losses:  4.125251770019531 0.26739180088043213
CurrentTrain: epoch  7, batch    61 | loss: 4.1252518Losses:  4.171566009521484 0.23379695415496826
CurrentTrain: epoch  7, batch    62 | loss: 4.1715660Losses:  4.188717842102051 0.2469680905342102
CurrentTrain: epoch  8, batch     0 | loss: 4.1887178Losses:  4.213548183441162 0.3117930293083191
CurrentTrain: epoch  8, batch     1 | loss: 4.2135482Losses:  4.185365676879883 0.22849798202514648
CurrentTrain: epoch  8, batch     2 | loss: 4.1853657Losses:  4.146847724914551 0.2794477641582489
CurrentTrain: epoch  8, batch     3 | loss: 4.1468477Losses:  4.297740936279297 0.41981321573257446
CurrentTrain: epoch  8, batch     4 | loss: 4.2977409Losses:  4.204283237457275 0.28486257791519165
CurrentTrain: epoch  8, batch     5 | loss: 4.2042832Losses:  4.218986511230469 0.32172197103500366
CurrentTrain: epoch  8, batch     6 | loss: 4.2189865Losses:  4.175937652587891 0.314827024936676
CurrentTrain: epoch  8, batch     7 | loss: 4.1759377Losses:  4.2045578956604 0.2661830186843872
CurrentTrain: epoch  8, batch     8 | loss: 4.2045579Losses:  4.102195739746094 0.20056673884391785
CurrentTrain: epoch  8, batch     9 | loss: 4.1021957Losses:  4.248539924621582 0.35575157403945923
CurrentTrain: epoch  8, batch    10 | loss: 4.2485399Losses:  4.224908351898193 0.37222781777381897
CurrentTrain: epoch  8, batch    11 | loss: 4.2249084Losses:  4.197394847869873 0.3684139847755432
CurrentTrain: epoch  8, batch    12 | loss: 4.1973948Losses:  4.2145771980285645 0.32198649644851685
CurrentTrain: epoch  8, batch    13 | loss: 4.2145772Losses:  4.188113689422607 0.27360033988952637
CurrentTrain: epoch  8, batch    14 | loss: 4.1881137Losses:  4.165924549102783 0.21973994374275208
CurrentTrain: epoch  8, batch    15 | loss: 4.1659245Losses:  4.261674880981445 0.3144301176071167
CurrentTrain: epoch  8, batch    16 | loss: 4.2616749Losses:  4.196995258331299 0.30857980251312256
CurrentTrain: epoch  8, batch    17 | loss: 4.1969953Losses:  4.256038665771484 0.3609752357006073
CurrentTrain: epoch  8, batch    18 | loss: 4.2560387Losses:  4.230489730834961 0.38080763816833496
CurrentTrain: epoch  8, batch    19 | loss: 4.2304897Losses:  4.179722785949707 0.31123146414756775
CurrentTrain: epoch  8, batch    20 | loss: 4.1797228Losses:  4.23450231552124 0.32550814747810364
CurrentTrain: epoch  8, batch    21 | loss: 4.2345023Losses:  4.190670967102051 0.3340606689453125
CurrentTrain: epoch  8, batch    22 | loss: 4.1906710Losses:  4.184614181518555 0.32572197914123535
CurrentTrain: epoch  8, batch    23 | loss: 4.1846142Losses:  4.164681434631348 0.26150891184806824
CurrentTrain: epoch  8, batch    24 | loss: 4.1646814Losses:  4.117761135101318 0.24248507618904114
CurrentTrain: epoch  8, batch    25 | loss: 4.1177611Losses:  4.24600076675415 0.37336409091949463
CurrentTrain: epoch  8, batch    26 | loss: 4.2460008Losses:  4.090791702270508 0.15696151554584503
CurrentTrain: epoch  8, batch    27 | loss: 4.0907917Losses:  4.1758036613464355 0.3023228943347931
CurrentTrain: epoch  8, batch    28 | loss: 4.1758037Losses:  4.107737064361572 0.23585045337677002
CurrentTrain: epoch  8, batch    29 | loss: 4.1077371Losses:  4.130682945251465 0.28416091203689575
CurrentTrain: epoch  8, batch    30 | loss: 4.1306829Losses:  4.082586288452148 0.26914918422698975
CurrentTrain: epoch  8, batch    31 | loss: 4.0825863Losses:  4.211039066314697 0.2619333267211914
CurrentTrain: epoch  8, batch    32 | loss: 4.2110391Losses:  4.242520809173584 0.3187362849712372
CurrentTrain: epoch  8, batch    33 | loss: 4.2425208Losses:  4.174877643585205 0.33323806524276733
CurrentTrain: epoch  8, batch    34 | loss: 4.1748776Losses:  4.184072971343994 0.36057740449905396
CurrentTrain: epoch  8, batch    35 | loss: 4.1840730Losses:  4.187901020050049 0.33134302496910095
CurrentTrain: epoch  8, batch    36 | loss: 4.1879010Losses:  4.205141544342041 0.32401376962661743
CurrentTrain: epoch  8, batch    37 | loss: 4.2051415Losses:  4.229416847229004 0.2947564423084259
CurrentTrain: epoch  8, batch    38 | loss: 4.2294168Losses:  4.15595006942749 0.26179826259613037
CurrentTrain: epoch  8, batch    39 | loss: 4.1559501Losses:  4.164729595184326 0.2536497116088867
CurrentTrain: epoch  8, batch    40 | loss: 4.1647296Losses:  4.306454181671143 0.39919358491897583
CurrentTrain: epoch  8, batch    41 | loss: 4.3064542Losses:  4.121702194213867 0.19363364577293396
CurrentTrain: epoch  8, batch    42 | loss: 4.1217022Losses:  4.104750156402588 0.2011486291885376
CurrentTrain: epoch  8, batch    43 | loss: 4.1047502Losses:  4.167420387268066 0.33118048310279846
CurrentTrain: epoch  8, batch    44 | loss: 4.1674204Losses:  4.189198017120361 0.2921922504901886
CurrentTrain: epoch  8, batch    45 | loss: 4.1891980Losses:  4.140933990478516 0.28081682324409485
CurrentTrain: epoch  8, batch    46 | loss: 4.1409340Losses:  4.160292148590088 0.3097883462905884
CurrentTrain: epoch  8, batch    47 | loss: 4.1602921Losses:  4.200930118560791 0.32659968733787537
CurrentTrain: epoch  8, batch    48 | loss: 4.2009301Losses:  4.222318649291992 0.3696451187133789
CurrentTrain: epoch  8, batch    49 | loss: 4.2223186Losses:  4.2794413566589355 0.3224179148674011
CurrentTrain: epoch  8, batch    50 | loss: 4.2794414Losses:  4.216134548187256 0.3606671094894409
CurrentTrain: epoch  8, batch    51 | loss: 4.2161345Losses:  4.242096424102783 0.3518756330013275
CurrentTrain: epoch  8, batch    52 | loss: 4.2420964Losses:  4.249300956726074 0.3473591208457947
CurrentTrain: epoch  8, batch    53 | loss: 4.2493010Losses:  4.1553521156311035 0.27038848400115967
CurrentTrain: epoch  8, batch    54 | loss: 4.1553521Losses:  4.202871799468994 0.3363138437271118
CurrentTrain: epoch  8, batch    55 | loss: 4.2028718Losses:  4.14874267578125 0.2618613839149475
CurrentTrain: epoch  8, batch    56 | loss: 4.1487427Losses:  4.167247772216797 0.2962000072002411
CurrentTrain: epoch  8, batch    57 | loss: 4.1672478Losses:  4.2312445640563965 0.3853268027305603
CurrentTrain: epoch  8, batch    58 | loss: 4.2312446Losses:  4.14822244644165 0.32107093930244446
CurrentTrain: epoch  8, batch    59 | loss: 4.1482224Losses:  4.158975601196289 0.3036060929298401
CurrentTrain: epoch  8, batch    60 | loss: 4.1589756Losses:  4.177635192871094 0.2716842293739319
CurrentTrain: epoch  8, batch    61 | loss: 4.1776352Losses:  4.099930763244629 0.15360628068447113
CurrentTrain: epoch  8, batch    62 | loss: 4.0999308Losses:  4.102873802185059 0.23704208433628082
CurrentTrain: epoch  9, batch     0 | loss: 4.1028738Losses:  4.148333549499512 0.2633788585662842
CurrentTrain: epoch  9, batch     1 | loss: 4.1483335Losses:  4.220206260681152 0.35443708300590515
CurrentTrain: epoch  9, batch     2 | loss: 4.2202063Losses:  4.1507086753845215 0.2755592465400696
CurrentTrain: epoch  9, batch     3 | loss: 4.1507087Losses:  4.105783462524414 0.233232319355011
CurrentTrain: epoch  9, batch     4 | loss: 4.1057835Losses:  4.1702961921691895 0.2787638008594513
CurrentTrain: epoch  9, batch     5 | loss: 4.1702962Losses:  4.153805732727051 0.2651670575141907
CurrentTrain: epoch  9, batch     6 | loss: 4.1538057Losses:  4.202855110168457 0.2976326048374176
CurrentTrain: epoch  9, batch     7 | loss: 4.2028551Losses:  4.101287364959717 0.1844506859779358
CurrentTrain: epoch  9, batch     8 | loss: 4.1012874Losses:  4.134047031402588 0.2565804719924927
CurrentTrain: epoch  9, batch     9 | loss: 4.1340470Losses:  4.186861991882324 0.3177185654640198
CurrentTrain: epoch  9, batch    10 | loss: 4.1868620Losses:  4.05755615234375 0.24271464347839355
CurrentTrain: epoch  9, batch    11 | loss: 4.0575562Losses:  4.075960159301758 0.16935989260673523
CurrentTrain: epoch  9, batch    12 | loss: 4.0759602Losses:  4.167865753173828 0.26655900478363037
CurrentTrain: epoch  9, batch    13 | loss: 4.1678658Losses:  4.1693854331970215 0.3034065365791321
CurrentTrain: epoch  9, batch    14 | loss: 4.1693854Losses:  4.151913642883301 0.23233076930046082
CurrentTrain: epoch  9, batch    15 | loss: 4.1519136Losses:  4.205682277679443 0.30883464217185974
CurrentTrain: epoch  9, batch    16 | loss: 4.2056823Losses:  4.179452896118164 0.3399694561958313
CurrentTrain: epoch  9, batch    17 | loss: 4.1794529Losses:  4.16902494430542 0.25876671075820923
CurrentTrain: epoch  9, batch    18 | loss: 4.1690249Losses:  4.123085975646973 0.20338040590286255
CurrentTrain: epoch  9, batch    19 | loss: 4.1230860Losses:  4.14372444152832 0.22371646761894226
CurrentTrain: epoch  9, batch    20 | loss: 4.1437244Losses:  4.175960540771484 0.27218228578567505
CurrentTrain: epoch  9, batch    21 | loss: 4.1759605Losses:  4.174788951873779 0.28234365582466125
CurrentTrain: epoch  9, batch    22 | loss: 4.1747890Losses:  4.143774032592773 0.22494663298130035
CurrentTrain: epoch  9, batch    23 | loss: 4.1437740Losses:  4.198079586029053 0.3545239269733429
CurrentTrain: epoch  9, batch    24 | loss: 4.1980796Losses:  4.170652389526367 0.25937318801879883
CurrentTrain: epoch  9, batch    25 | loss: 4.1706524Losses:  4.118500709533691 0.32531213760375977
CurrentTrain: epoch  9, batch    26 | loss: 4.1185007Losses:  4.166134357452393 0.2946467995643616
CurrentTrain: epoch  9, batch    27 | loss: 4.1661344Losses:  4.128736972808838 0.28881192207336426
CurrentTrain: epoch  9, batch    28 | loss: 4.1287370Losses:  4.1025543212890625 0.2748093605041504
CurrentTrain: epoch  9, batch    29 | loss: 4.1025543Losses:  4.144508361816406 0.26882827281951904
CurrentTrain: epoch  9, batch    30 | loss: 4.1445084Losses:  4.181329727172852 0.3162902295589447
CurrentTrain: epoch  9, batch    31 | loss: 4.1813297Losses:  4.203658103942871 0.32014530897140503
CurrentTrain: epoch  9, batch    32 | loss: 4.2036581Losses:  4.100076675415039 0.2099340707063675
CurrentTrain: epoch  9, batch    33 | loss: 4.1000767Losses:  4.171552658081055 0.26821285486221313
CurrentTrain: epoch  9, batch    34 | loss: 4.1715527Losses:  4.147695541381836 0.2896103858947754
CurrentTrain: epoch  9, batch    35 | loss: 4.1476955Losses:  4.167874336242676 0.22908896207809448
CurrentTrain: epoch  9, batch    36 | loss: 4.1678743Losses:  4.183666706085205 0.318102091550827
CurrentTrain: epoch  9, batch    37 | loss: 4.1836667Losses:  4.14796257019043 0.3042200207710266
CurrentTrain: epoch  9, batch    38 | loss: 4.1479626Losses:  4.165501117706299 0.2917155921459198
CurrentTrain: epoch  9, batch    39 | loss: 4.1655011Losses:  4.133551120758057 0.31001245975494385
CurrentTrain: epoch  9, batch    40 | loss: 4.1335511Losses:  4.142327308654785 0.268104612827301
CurrentTrain: epoch  9, batch    41 | loss: 4.1423273Losses:  4.257411003112793 0.3270829916000366
CurrentTrain: epoch  9, batch    42 | loss: 4.2574110Losses:  4.1561174392700195 0.2852458953857422
CurrentTrain: epoch  9, batch    43 | loss: 4.1561174Losses:  4.142980098724365 0.2669951915740967
CurrentTrain: epoch  9, batch    44 | loss: 4.1429801Losses:  4.071003437042236 0.18976087868213654
CurrentTrain: epoch  9, batch    45 | loss: 4.0710034Losses:  4.179754257202148 0.34125322103500366
CurrentTrain: epoch  9, batch    46 | loss: 4.1797543Losses:  4.145280838012695 0.29371964931488037
CurrentTrain: epoch  9, batch    47 | loss: 4.1452808Losses:  4.141427040100098 0.30776339769363403
CurrentTrain: epoch  9, batch    48 | loss: 4.1414270Losses:  4.1613664627075195 0.3212798833847046
CurrentTrain: epoch  9, batch    49 | loss: 4.1613665Losses:  4.1779069900512695 0.2154695987701416
CurrentTrain: epoch  9, batch    50 | loss: 4.1779070Losses:  4.107343673706055 0.23065371811389923
CurrentTrain: epoch  9, batch    51 | loss: 4.1073437Losses:  4.150909900665283 0.2738310396671295
CurrentTrain: epoch  9, batch    52 | loss: 4.1509099Losses:  4.1149516105651855 0.19835373759269714
CurrentTrain: epoch  9, batch    53 | loss: 4.1149516Losses:  4.076222896575928 0.17727597057819366
CurrentTrain: epoch  9, batch    54 | loss: 4.0762229Losses:  4.188299655914307 0.2961565852165222
CurrentTrain: epoch  9, batch    55 | loss: 4.1882997Losses:  4.131186008453369 0.26986339688301086
CurrentTrain: epoch  9, batch    56 | loss: 4.1311860Losses:  4.093569278717041 0.20892487466335297
CurrentTrain: epoch  9, batch    57 | loss: 4.0935693Losses:  4.137347221374512 0.2618696689605713
CurrentTrain: epoch  9, batch    58 | loss: 4.1373472Losses:  4.169545650482178 0.20983056724071503
CurrentTrain: epoch  9, batch    59 | loss: 4.1695457Losses:  4.1740336418151855 0.24727147817611694
CurrentTrain: epoch  9, batch    60 | loss: 4.1740336Losses:  4.119899749755859 0.26311972737312317
CurrentTrain: epoch  9, batch    61 | loss: 4.1198997Losses:  4.128549098968506 0.1952923834323883
CurrentTrain: epoch  9, batch    62 | loss: 4.1285491
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  7.331826210021973 1.123380422592163
CurrentTrain: epoch  0, batch     0 | loss: 7.3318262Losses:  6.803910255432129 1.2011253833770752
CurrentTrain: epoch  0, batch     1 | loss: 6.8039103Losses:  6.68790864944458 0.925922691822052
CurrentTrain: epoch  0, batch     2 | loss: 6.6879086Losses:  7.804160118103027 0.1798667311668396
CurrentTrain: epoch  0, batch     3 | loss: 7.8041601Losses:  7.318970680236816 1.3596464395523071
CurrentTrain: epoch  1, batch     0 | loss: 7.3189707Losses:  5.553310394287109 0.9980754852294922
CurrentTrain: epoch  1, batch     1 | loss: 5.5533104Losses:  4.934884071350098 1.0289888381958008
CurrentTrain: epoch  1, batch     2 | loss: 4.9348841Losses:  6.4369635581970215 0.2885464131832123
CurrentTrain: epoch  1, batch     3 | loss: 6.4369636Losses:  5.03280782699585 0.8671824932098389
CurrentTrain: epoch  2, batch     0 | loss: 5.0328078Losses:  5.503250598907471 1.120697021484375
CurrentTrain: epoch  2, batch     1 | loss: 5.5032506Losses:  4.844403266906738 1.0649698972702026
CurrentTrain: epoch  2, batch     2 | loss: 4.8444033Losses:  2.4874284267425537 0.2704910933971405
CurrentTrain: epoch  2, batch     3 | loss: 2.4874284Losses:  5.173405170440674 1.0226472616195679
CurrentTrain: epoch  3, batch     0 | loss: 5.1734052Losses:  4.339703559875488 0.8859889507293701
CurrentTrain: epoch  3, batch     1 | loss: 4.3397036Losses:  4.118496894836426 0.7674403190612793
CurrentTrain: epoch  3, batch     2 | loss: 4.1184969Losses:  4.483660697937012 0.10357868671417236
CurrentTrain: epoch  3, batch     3 | loss: 4.4836607Losses:  3.503244638442993 0.9438652396202087
CurrentTrain: epoch  4, batch     0 | loss: 3.5032446Losses:  4.3618083000183105 0.9997671842575073
CurrentTrain: epoch  4, batch     1 | loss: 4.3618083Losses:  4.067769527435303 0.8882726430892944
CurrentTrain: epoch  4, batch     2 | loss: 4.0677695Losses:  4.214198589324951 0.3966411054134369
CurrentTrain: epoch  4, batch     3 | loss: 4.2141986Losses:  3.849228620529175 0.8914551734924316
CurrentTrain: epoch  5, batch     0 | loss: 3.8492286Losses:  3.2093448638916016 0.8837460279464722
CurrentTrain: epoch  5, batch     1 | loss: 3.2093449Losses:  3.89976167678833 0.9777497053146362
CurrentTrain: epoch  5, batch     2 | loss: 3.8997617Losses:  2.9611332416534424 0.15276466310024261
CurrentTrain: epoch  5, batch     3 | loss: 2.9611332Losses:  3.2960028648376465 0.9561588764190674
CurrentTrain: epoch  6, batch     0 | loss: 3.2960029Losses:  3.062709331512451 0.7916362285614014
CurrentTrain: epoch  6, batch     1 | loss: 3.0627093Losses:  3.5755317211151123 0.9060919880867004
CurrentTrain: epoch  6, batch     2 | loss: 3.5755317Losses:  2.803598642349243 0.11280669271945953
CurrentTrain: epoch  6, batch     3 | loss: 2.8035986Losses:  3.2154245376586914 0.800039529800415
CurrentTrain: epoch  7, batch     0 | loss: 3.2154245Losses:  3.2474875450134277 0.725165843963623
CurrentTrain: epoch  7, batch     1 | loss: 3.2474875Losses:  2.9634406566619873 0.8387266397476196
CurrentTrain: epoch  7, batch     2 | loss: 2.9634407Losses:  2.181757926940918 0.2977153956890106
CurrentTrain: epoch  7, batch     3 | loss: 2.1817579Losses:  2.983774185180664 0.9113377332687378
CurrentTrain: epoch  8, batch     0 | loss: 2.9837742Losses:  3.1705925464630127 0.9779467582702637
CurrentTrain: epoch  8, batch     1 | loss: 3.1705925Losses:  2.555436134338379 0.6764662265777588
CurrentTrain: epoch  8, batch     2 | loss: 2.5554361Losses:  2.114600658416748 0.053914912045001984
CurrentTrain: epoch  8, batch     3 | loss: 2.1146007Losses:  2.8571529388427734 0.8295289278030396
CurrentTrain: epoch  9, batch     0 | loss: 2.8571529Losses:  2.6818060874938965 0.8008549213409424
CurrentTrain: epoch  9, batch     1 | loss: 2.6818061Losses:  2.9490652084350586 0.9081394076347351
CurrentTrain: epoch  9, batch     2 | loss: 2.9490652Losses:  2.3431358337402344 0.13762705028057098
CurrentTrain: epoch  9, batch     3 | loss: 2.3431358
Losses:  2.917475461959839 0.9498021006584167
MemoryTrain:  epoch  0, batch     0 | loss: 2.9174755Losses:  0.1895827353000641 0.32561221718788147
MemoryTrain:  epoch  0, batch     1 | loss: 0.1895827Losses:  2.5403449535369873 1.0481983423233032
MemoryTrain:  epoch  1, batch     0 | loss: 2.5403450Losses:  0.13733941316604614 0.1139926016330719
MemoryTrain:  epoch  1, batch     1 | loss: 0.1373394Losses:  1.6472077369689941 1.011786937713623
MemoryTrain:  epoch  2, batch     0 | loss: 1.6472077Losses:  1.6727524995803833 0.14677730202674866
MemoryTrain:  epoch  2, batch     1 | loss: 1.6727525Losses:  1.4159154891967773 0.8986438512802124
MemoryTrain:  epoch  3, batch     0 | loss: 1.4159155Losses:  0.9848451614379883 0.21659868955612183
MemoryTrain:  epoch  3, batch     1 | loss: 0.9848452Losses:  1.3379307985305786 0.9116148948669434
MemoryTrain:  epoch  4, batch     0 | loss: 1.3379308Losses:  0.2544395327568054 0.18095755577087402
MemoryTrain:  epoch  4, batch     1 | loss: 0.2544395Losses:  0.8393548727035522 0.8351643681526184
MemoryTrain:  epoch  5, batch     0 | loss: 0.8393549Losses:  0.9816931486129761 0.21913379430770874
MemoryTrain:  epoch  5, batch     1 | loss: 0.9816931Losses:  0.9205369353294373 0.8148378133773804
MemoryTrain:  epoch  6, batch     0 | loss: 0.9205369Losses:  0.6445057392120361 0.6535419225692749
MemoryTrain:  epoch  6, batch     1 | loss: 0.6445057Losses:  0.8416624665260315 0.8802307844161987
MemoryTrain:  epoch  7, batch     0 | loss: 0.8416625Losses:  0.14857053756713867 0.17561468482017517
MemoryTrain:  epoch  7, batch     1 | loss: 0.1485705Losses:  0.6648892164230347 0.8129628896713257
MemoryTrain:  epoch  8, batch     0 | loss: 0.6648892Losses:  0.27887940406799316 0.46075156331062317
MemoryTrain:  epoch  8, batch     1 | loss: 0.2788794Losses:  0.625396192073822 0.8754971027374268
MemoryTrain:  epoch  9, batch     0 | loss: 0.6253962Losses:  0.24456806480884552 0.3326665461063385
MemoryTrain:  epoch  9, batch     1 | loss: 0.2445681
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 76.64%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 76.45%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.99%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 73.64%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 69.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.90%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 93.65%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 94.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 94.01%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 93.84%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.67%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 93.51%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 93.35%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 92.97%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.98%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.91%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 92.62%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 92.28%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 92.15%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.88%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 91.69%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 91.57%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 91.18%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 91.14%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.96%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 90.46%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.89%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 89.67%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 89.39%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 89.05%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 88.58%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 88.32%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 88.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 87.69%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 87.14%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.90%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 86.50%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 86.04%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 85.36%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 84.81%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 84.20%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 83.67%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 83.26%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 83.02%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.32%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.60%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.69%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 84.55%   
cur_acc:  ['0.9474', '0.7490']
his_acc:  ['0.9474', '0.8455']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  6.125850677490234 0.9032173156738281
CurrentTrain: epoch  0, batch     0 | loss: 6.1258507Losses:  7.915087699890137 1.135568618774414
CurrentTrain: epoch  0, batch     1 | loss: 7.9150877Losses:  6.543498992919922 1.1109338998794556
CurrentTrain: epoch  0, batch     2 | loss: 6.5434990Losses:  8.808846473693848 0.39305758476257324
CurrentTrain: epoch  0, batch     3 | loss: 8.8088465Losses:  6.718993186950684 1.3509905338287354
CurrentTrain: epoch  1, batch     0 | loss: 6.7189932Losses:  5.123783588409424 0.9667807221412659
CurrentTrain: epoch  1, batch     1 | loss: 5.1237836Losses:  6.968506813049316 1.236618161201477
CurrentTrain: epoch  1, batch     2 | loss: 6.9685068Losses:  6.7653961181640625 0.3124006688594818
CurrentTrain: epoch  1, batch     3 | loss: 6.7653961Losses:  5.531843662261963 0.8295155167579651
CurrentTrain: epoch  2, batch     0 | loss: 5.5318437Losses:  5.668673038482666 0.9448060989379883
CurrentTrain: epoch  2, batch     1 | loss: 5.6686730Losses:  5.781341552734375 1.1799005270004272
CurrentTrain: epoch  2, batch     2 | loss: 5.7813416Losses:  6.311875820159912 0.40350669622421265
CurrentTrain: epoch  2, batch     3 | loss: 6.3118758Losses:  5.071314334869385 1.0155017375946045
CurrentTrain: epoch  3, batch     0 | loss: 5.0713143Losses:  5.064746856689453 0.9417134523391724
CurrentTrain: epoch  3, batch     1 | loss: 5.0647469Losses:  6.031349182128906 1.1080464124679565
CurrentTrain: epoch  3, batch     2 | loss: 6.0313492Losses:  6.137998104095459 0.490029513835907
CurrentTrain: epoch  3, batch     3 | loss: 6.1379981Losses:  5.060643672943115 1.091155767440796
CurrentTrain: epoch  4, batch     0 | loss: 5.0606437Losses:  4.562251091003418 1.1238030195236206
CurrentTrain: epoch  4, batch     1 | loss: 4.5622511Losses:  5.780580997467041 1.2322301864624023
CurrentTrain: epoch  4, batch     2 | loss: 5.7805810Losses:  2.4773597717285156 0.14068076014518738
CurrentTrain: epoch  4, batch     3 | loss: 2.4773598Losses:  4.886551380157471 1.1838024854660034
CurrentTrain: epoch  5, batch     0 | loss: 4.8865514Losses:  4.094728946685791 1.010509729385376
CurrentTrain: epoch  5, batch     1 | loss: 4.0947289Losses:  4.992851257324219 1.1785988807678223
CurrentTrain: epoch  5, batch     2 | loss: 4.9928513Losses:  4.798034191131592 0.1688128113746643
CurrentTrain: epoch  5, batch     3 | loss: 4.7980342Losses:  4.098703384399414 1.0426273345947266
CurrentTrain: epoch  6, batch     0 | loss: 4.0987034Losses:  4.712040901184082 1.0316988229751587
CurrentTrain: epoch  6, batch     1 | loss: 4.7120409Losses:  4.276902675628662 1.1018339395523071
CurrentTrain: epoch  6, batch     2 | loss: 4.2769027Losses:  4.712978839874268 0.2329036295413971
CurrentTrain: epoch  6, batch     3 | loss: 4.7129788Losses:  3.5144503116607666 0.9803611040115356
CurrentTrain: epoch  7, batch     0 | loss: 3.5144503Losses:  4.733249664306641 1.105025053024292
CurrentTrain: epoch  7, batch     1 | loss: 4.7332497Losses:  4.300378322601318 1.0789953470230103
CurrentTrain: epoch  7, batch     2 | loss: 4.3003783Losses:  4.871386528015137 0.1397804319858551
CurrentTrain: epoch  7, batch     3 | loss: 4.8713865Losses:  3.725497007369995 1.0492334365844727
CurrentTrain: epoch  8, batch     0 | loss: 3.7254970Losses:  4.6546854972839355 1.1318271160125732
CurrentTrain: epoch  8, batch     1 | loss: 4.6546855Losses:  3.3232927322387695 0.6833150386810303
CurrentTrain: epoch  8, batch     2 | loss: 3.3232927Losses:  4.415766716003418 0.43301254510879517
CurrentTrain: epoch  8, batch     3 | loss: 4.4157667Losses:  3.580366373062134 1.0326627492904663
CurrentTrain: epoch  9, batch     0 | loss: 3.5803664Losses:  3.5119760036468506 0.9851736426353455
CurrentTrain: epoch  9, batch     1 | loss: 3.5119760Losses:  4.4200592041015625 1.2821128368377686
CurrentTrain: epoch  9, batch     2 | loss: 4.4200592Losses:  1.9921327829360962 0.16406488418579102
CurrentTrain: epoch  9, batch     3 | loss: 1.9921328
Losses:  1.0790610313415527 1.0281364917755127
MemoryTrain:  epoch  0, batch     0 | loss: 1.0790610Losses:  1.688453197479248 1.1837654113769531
MemoryTrain:  epoch  0, batch     1 | loss: 1.6884532Losses:  1.612267255783081 1.141070008277893
MemoryTrain:  epoch  1, batch     0 | loss: 1.6122673Losses:  1.5695075988769531 1.0247516632080078
MemoryTrain:  epoch  1, batch     1 | loss: 1.5695076Losses:  1.1805548667907715 1.016660213470459
MemoryTrain:  epoch  2, batch     0 | loss: 1.1805549Losses:  0.8222042322158813 1.0802414417266846
MemoryTrain:  epoch  2, batch     1 | loss: 0.8222042Losses:  0.7064763903617859 1.018312931060791
MemoryTrain:  epoch  3, batch     0 | loss: 0.7064764Losses:  0.783782958984375 1.0293359756469727
MemoryTrain:  epoch  3, batch     1 | loss: 0.7837830Losses:  0.7747538089752197 1.2166131734848022
MemoryTrain:  epoch  4, batch     0 | loss: 0.7747538Losses:  0.5105531811714172 0.7910572290420532
MemoryTrain:  epoch  4, batch     1 | loss: 0.5105532Losses:  0.6761692762374878 1.1418640613555908
MemoryTrain:  epoch  5, batch     0 | loss: 0.6761693Losses:  0.4760676622390747 0.8379649519920349
MemoryTrain:  epoch  5, batch     1 | loss: 0.4760677Losses:  0.6283221244812012 0.9550268650054932
MemoryTrain:  epoch  6, batch     0 | loss: 0.6283221Losses:  0.5616316199302673 0.9955913424491882
MemoryTrain:  epoch  6, batch     1 | loss: 0.5616316Losses:  0.6246438026428223 1.100935935974121
MemoryTrain:  epoch  7, batch     0 | loss: 0.6246438Losses:  0.5239037871360779 0.7862193584442139
MemoryTrain:  epoch  7, batch     1 | loss: 0.5239038Losses:  0.5258693099021912 0.9625703692436218
MemoryTrain:  epoch  8, batch     0 | loss: 0.5258693Losses:  0.5123200416564941 0.8962895274162292
MemoryTrain:  epoch  8, batch     1 | loss: 0.5123200Losses:  0.5039591193199158 0.9361192584037781
MemoryTrain:  epoch  9, batch     0 | loss: 0.5039591Losses:  0.5073953866958618 0.9226347804069519
MemoryTrain:  epoch  9, batch     1 | loss: 0.5073954
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 75.72%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 75.23%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 75.34%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 75.32%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.72%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.03%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.44%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 92.16%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 92.10%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.21%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.23%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.27%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 92.05%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.91%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 91.77%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.41%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 91.31%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.96%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 90.55%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 90.15%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 89.83%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 89.15%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 88.90%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 88.61%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 88.45%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 88.17%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 87.70%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 87.30%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 86.85%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.53%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 86.10%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 85.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.09%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.59%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.02%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.93%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.34%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.31%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.92%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.87%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 82.11%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 82.15%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 82.09%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 81.98%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 82.16%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.38%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 82.42%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 82.10%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 81.69%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 81.38%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 81.16%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 80.82%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 81.21%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 80.80%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 80.39%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 80.07%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 79.64%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 79.21%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 79.14%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.23%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 80.20%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 80.38%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 80.35%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 80.35%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 80.39%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 80.33%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 80.16%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 80.06%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 80.07%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 80.01%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 79.98%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 79.99%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 79.98%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 79.75%   
cur_acc:  ['0.9474', '0.7490', '0.7500']
his_acc:  ['0.9474', '0.8455', '0.7975']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  5.815688133239746 0.9142692685127258
CurrentTrain: epoch  0, batch     0 | loss: 5.8156881Losses:  5.145389080047607 0.795110821723938
CurrentTrain: epoch  0, batch     1 | loss: 5.1453891Losses:  4.465554714202881 0.790534257888794
CurrentTrain: epoch  0, batch     2 | loss: 4.4655547Losses:  3.7347209453582764 0.07826323807239532
CurrentTrain: epoch  0, batch     3 | loss: 3.7347209Losses:  5.040153980255127 0.9060183763504028
CurrentTrain: epoch  1, batch     0 | loss: 5.0401540Losses:  4.252273082733154 0.6871201395988464
CurrentTrain: epoch  1, batch     1 | loss: 4.2522731Losses:  3.493960380554199 0.7532328367233276
CurrentTrain: epoch  1, batch     2 | loss: 3.4939604Losses:  2.9886999130249023 0.31209757924079895
CurrentTrain: epoch  1, batch     3 | loss: 2.9886999Losses:  4.073830604553223 0.8035376071929932
CurrentTrain: epoch  2, batch     0 | loss: 4.0738306Losses:  3.713550090789795 0.835552453994751
CurrentTrain: epoch  2, batch     1 | loss: 3.7135501Losses:  3.0737881660461426 0.7576904296875
CurrentTrain: epoch  2, batch     2 | loss: 3.0737882Losses:  1.7409374713897705 0.0
CurrentTrain: epoch  2, batch     3 | loss: 1.7409375Losses:  3.7165920734405518 0.8560957312583923
CurrentTrain: epoch  3, batch     0 | loss: 3.7165921Losses:  3.2401912212371826 0.6801401376724243
CurrentTrain: epoch  3, batch     1 | loss: 3.2401912Losses:  2.9056155681610107 0.5894789099693298
CurrentTrain: epoch  3, batch     2 | loss: 2.9056156Losses:  2.426506280899048 0.052242159843444824
CurrentTrain: epoch  3, batch     3 | loss: 2.4265063Losses:  3.091323137283325 0.6719003915786743
CurrentTrain: epoch  4, batch     0 | loss: 3.0913231Losses:  3.1775238513946533 0.7547636032104492
CurrentTrain: epoch  4, batch     1 | loss: 3.1775239Losses:  3.361368417739868 0.8240842223167419
CurrentTrain: epoch  4, batch     2 | loss: 3.3613684Losses:  2.082728385925293 0.17180247604846954
CurrentTrain: epoch  4, batch     3 | loss: 2.0827284Losses:  2.6138532161712646 0.7066377997398376
CurrentTrain: epoch  5, batch     0 | loss: 2.6138532Losses:  3.0189061164855957 0.7836512327194214
CurrentTrain: epoch  5, batch     1 | loss: 3.0189061Losses:  3.3042407035827637 0.5826730728149414
CurrentTrain: epoch  5, batch     2 | loss: 3.3042407Losses:  2.2030656337738037 0.07494458556175232
CurrentTrain: epoch  5, batch     3 | loss: 2.2030656Losses:  2.7804384231567383 0.649066686630249
CurrentTrain: epoch  6, batch     0 | loss: 2.7804384Losses:  2.821044921875 0.5905768871307373
CurrentTrain: epoch  6, batch     1 | loss: 2.8210449Losses:  3.0671212673187256 0.80203777551651
CurrentTrain: epoch  6, batch     2 | loss: 3.0671213Losses:  1.7356555461883545 0.01570880226790905
CurrentTrain: epoch  6, batch     3 | loss: 1.7356555Losses:  2.843796968460083 0.694362998008728
CurrentTrain: epoch  7, batch     0 | loss: 2.8437970Losses:  2.894080638885498 0.7671835422515869
CurrentTrain: epoch  7, batch     1 | loss: 2.8940806Losses:  2.4747684001922607 0.5793882012367249
CurrentTrain: epoch  7, batch     2 | loss: 2.4747684Losses:  1.8525230884552002 0.04343554750084877
CurrentTrain: epoch  7, batch     3 | loss: 1.8525231Losses:  2.711836099624634 0.5788084268569946
CurrentTrain: epoch  8, batch     0 | loss: 2.7118361Losses:  2.5755438804626465 0.765350341796875
CurrentTrain: epoch  8, batch     1 | loss: 2.5755439Losses:  2.3045265674591064 0.6311677098274231
CurrentTrain: epoch  8, batch     2 | loss: 2.3045266Losses:  2.055192708969116 0.14637193083763123
CurrentTrain: epoch  8, batch     3 | loss: 2.0551927Losses:  2.563427448272705 0.6625549793243408
CurrentTrain: epoch  9, batch     0 | loss: 2.5634274Losses:  2.3813676834106445 0.6184344291687012
CurrentTrain: epoch  9, batch     1 | loss: 2.3813677Losses:  2.2829782962799072 0.6618036031723022
CurrentTrain: epoch  9, batch     2 | loss: 2.2829783Losses:  1.7792950868606567 0.0557304285466671
CurrentTrain: epoch  9, batch     3 | loss: 1.7792951
Losses:  1.557658076286316 1.1340261697769165
MemoryTrain:  epoch  0, batch     0 | loss: 1.5576581Losses:  1.157539963722229 0.9836269021034241
MemoryTrain:  epoch  0, batch     1 | loss: 1.1575400Losses:  1.237653374671936 0.4960429072380066
MemoryTrain:  epoch  0, batch     2 | loss: 1.2376534Losses:  1.1109118461608887 0.9892482161521912
MemoryTrain:  epoch  1, batch     0 | loss: 1.1109118Losses:  1.3539891242980957 0.9729526042938232
MemoryTrain:  epoch  1, batch     1 | loss: 1.3539891Losses:  1.1545079946517944 0.5574212670326233
MemoryTrain:  epoch  1, batch     2 | loss: 1.1545080Losses:  1.0003299713134766 1.0023193359375
MemoryTrain:  epoch  2, batch     0 | loss: 1.0003300Losses:  0.6655797958374023 0.820725679397583
MemoryTrain:  epoch  2, batch     1 | loss: 0.6655798Losses:  1.0197583436965942 0.6309210062026978
MemoryTrain:  epoch  2, batch     2 | loss: 1.0197583Losses:  0.44716137647628784 0.6825966835021973
MemoryTrain:  epoch  3, batch     0 | loss: 0.4471614Losses:  0.7172384858131409 1.0276753902435303
MemoryTrain:  epoch  3, batch     1 | loss: 0.7172385Losses:  1.4165335893630981 0.8236048221588135
MemoryTrain:  epoch  3, batch     2 | loss: 1.4165336Losses:  0.8252153992652893 0.870911717414856
MemoryTrain:  epoch  4, batch     0 | loss: 0.8252154Losses:  0.6162511110305786 0.8466615080833435
MemoryTrain:  epoch  4, batch     1 | loss: 0.6162511Losses:  0.3830238878726959 0.6304882168769836
MemoryTrain:  epoch  4, batch     2 | loss: 0.3830239Losses:  0.5763798952102661 0.9323872923851013
MemoryTrain:  epoch  5, batch     0 | loss: 0.5763799Losses:  0.7133683562278748 0.8669033050537109
MemoryTrain:  epoch  5, batch     1 | loss: 0.7133684Losses:  0.33776208758354187 0.5940151214599609
MemoryTrain:  epoch  5, batch     2 | loss: 0.3377621Losses:  0.5205020308494568 0.9220613241195679
MemoryTrain:  epoch  6, batch     0 | loss: 0.5205020Losses:  0.4839060306549072 0.8551344275474548
MemoryTrain:  epoch  6, batch     1 | loss: 0.4839060Losses:  0.3151262104511261 0.5229371190071106
MemoryTrain:  epoch  6, batch     2 | loss: 0.3151262Losses:  0.48127132654190063 0.9067355394363403
MemoryTrain:  epoch  7, batch     0 | loss: 0.4812713Losses:  0.47120630741119385 0.7797702550888062
MemoryTrain:  epoch  7, batch     1 | loss: 0.4712063Losses:  0.2965165674686432 0.46441757678985596
MemoryTrain:  epoch  7, batch     2 | loss: 0.2965166Losses:  0.5116560459136963 0.9520768523216248
MemoryTrain:  epoch  8, batch     0 | loss: 0.5116560Losses:  0.44460105895996094 0.7688559293746948
MemoryTrain:  epoch  8, batch     1 | loss: 0.4446011Losses:  0.287926584482193 0.4666309356689453
MemoryTrain:  epoch  8, batch     2 | loss: 0.2879266Losses:  0.5887971520423889 1.083960771560669
MemoryTrain:  epoch  9, batch     0 | loss: 0.5887972Losses:  0.4308245778083801 0.7830580472946167
MemoryTrain:  epoch  9, batch     1 | loss: 0.4308246Losses:  0.19074885547161102 0.3267213702201843
MemoryTrain:  epoch  9, batch     2 | loss: 0.1907489
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 56.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.91%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.48%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.83%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 90.82%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.04%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 90.83%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.71%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.59%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.23%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 90.17%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 89.76%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 89.29%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.90%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.66%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 88.07%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 87.93%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.71%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 87.43%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 87.23%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 86.89%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 86.36%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.84%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 85.53%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 85.16%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 84.86%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 84.57%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 84.28%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 83.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.07%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.87%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.74%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.13%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 81.54%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.96%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 80.28%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 79.84%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.26%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.84%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 81.05%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 80.86%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 81.11%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 80.94%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 80.58%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 80.19%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 79.89%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 79.68%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 79.38%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 80.12%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 79.68%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 79.24%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 78.76%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 78.41%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 77.94%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 77.52%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.14%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 78.07%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 78.24%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 78.22%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 78.24%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 78.23%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 78.32%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 78.16%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 77.77%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 77.58%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 77.35%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 77.39%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 77.35%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 77.52%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.54%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.64%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 77.44%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 77.17%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 76.94%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 76.70%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 76.48%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 76.28%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 76.21%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 76.27%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 76.16%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 76.13%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 75.97%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 75.75%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 75.54%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 75.33%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 75.12%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 74.91%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 74.85%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 75.51%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.54%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 75.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 76.73%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.18%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.50%   
cur_acc:  ['0.9474', '0.7490', '0.7500', '0.7748']
his_acc:  ['0.9474', '0.8455', '0.7975', '0.7750']
Clustering into  24  clusters
Clusters:  [ 2  8  1  0 14 23  1 22  3  3  1  3 21 18 11  4 10 18 12  4  2  4  0  7
  0  8  9  0  6 15  1  9  2  0  5  2 19 16 22  6 20 17  7  1 22  0  2  1
 13 10]
Losses:  6.743390083312988 1.1448135375976562
CurrentTrain: epoch  0, batch     0 | loss: 6.7433901Losses:  6.61051082611084 1.1778523921966553
CurrentTrain: epoch  0, batch     1 | loss: 6.6105108Losses:  6.582079887390137 1.1326026916503906
CurrentTrain: epoch  0, batch     2 | loss: 6.5820799Losses:  6.372162818908691 0.1269284039735794
CurrentTrain: epoch  0, batch     3 | loss: 6.3721628Losses:  5.394158840179443 0.9415079951286316
CurrentTrain: epoch  1, batch     0 | loss: 5.3941588Losses:  5.858963966369629 1.2653799057006836
CurrentTrain: epoch  1, batch     1 | loss: 5.8589640Losses:  6.0238165855407715 1.023287057876587
CurrentTrain: epoch  1, batch     2 | loss: 6.0238166Losses:  3.787686824798584 0.13396812975406647
CurrentTrain: epoch  1, batch     3 | loss: 3.7876868Losses:  6.043658256530762 1.193650722503662
CurrentTrain: epoch  2, batch     0 | loss: 6.0436583Losses:  4.971775054931641 1.1119818687438965
CurrentTrain: epoch  2, batch     1 | loss: 4.9717751Losses:  4.014893531799316 0.9852157831192017
CurrentTrain: epoch  2, batch     2 | loss: 4.0148935Losses:  4.66156530380249 0.14615851640701294
CurrentTrain: epoch  2, batch     3 | loss: 4.6615653Losses:  5.295064926147461 1.0255703926086426
CurrentTrain: epoch  3, batch     0 | loss: 5.2950649Losses:  4.1056060791015625 1.0821558237075806
CurrentTrain: epoch  3, batch     1 | loss: 4.1056061Losses:  4.746481895446777 1.0939176082611084
CurrentTrain: epoch  3, batch     2 | loss: 4.7464819Losses:  3.3791000843048096 0.19896571338176727
CurrentTrain: epoch  3, batch     3 | loss: 3.3791001Losses:  4.9226531982421875 0.894157886505127
CurrentTrain: epoch  4, batch     0 | loss: 4.9226532Losses:  4.279726505279541 1.0509727001190186
CurrentTrain: epoch  4, batch     1 | loss: 4.2797265Losses:  4.11934757232666 0.9770718812942505
CurrentTrain: epoch  4, batch     2 | loss: 4.1193476Losses:  3.127636194229126 0.122227244079113
CurrentTrain: epoch  4, batch     3 | loss: 3.1276362Losses:  5.047187805175781 1.1432766914367676
CurrentTrain: epoch  5, batch     0 | loss: 5.0471878Losses:  3.897425651550293 1.0242507457733154
CurrentTrain: epoch  5, batch     1 | loss: 3.8974257Losses:  4.082322597503662 1.0602465867996216
CurrentTrain: epoch  5, batch     2 | loss: 4.0823226Losses:  2.24169659614563 0.15766295790672302
CurrentTrain: epoch  5, batch     3 | loss: 2.2416966Losses:  4.578744888305664 1.10800302028656
CurrentTrain: epoch  6, batch     0 | loss: 4.5787449Losses:  3.3193907737731934 0.8630083799362183
CurrentTrain: epoch  6, batch     1 | loss: 3.3193908Losses:  3.9251952171325684 0.9597099423408508
CurrentTrain: epoch  6, batch     2 | loss: 3.9251952Losses:  2.054053544998169 0.13369402289390564
CurrentTrain: epoch  6, batch     3 | loss: 2.0540535Losses:  3.6776390075683594 1.0300943851470947
CurrentTrain: epoch  7, batch     0 | loss: 3.6776390Losses:  3.8590919971466064 0.8486846089363098
CurrentTrain: epoch  7, batch     1 | loss: 3.8590920Losses:  3.683326244354248 1.1427252292633057
CurrentTrain: epoch  7, batch     2 | loss: 3.6833262Losses:  4.595285892486572 0.37144848704338074
CurrentTrain: epoch  7, batch     3 | loss: 4.5952859Losses:  4.064392566680908 1.0395219326019287
CurrentTrain: epoch  8, batch     0 | loss: 4.0643926Losses:  3.521773338317871 1.0195132493972778
CurrentTrain: epoch  8, batch     1 | loss: 3.5217733Losses:  3.4415409564971924 0.8724226951599121
CurrentTrain: epoch  8, batch     2 | loss: 3.4415410Losses:  2.426816701889038 0.2379574179649353
CurrentTrain: epoch  8, batch     3 | loss: 2.4268167Losses:  3.2020366191864014 0.9580610990524292
CurrentTrain: epoch  9, batch     0 | loss: 3.2020366Losses:  3.8365659713745117 0.98732990026474
CurrentTrain: epoch  9, batch     1 | loss: 3.8365660Losses:  3.4762861728668213 1.1059139966964722
CurrentTrain: epoch  9, batch     2 | loss: 3.4762862Losses:  2.2544374465942383 0.31673383712768555
CurrentTrain: epoch  9, batch     3 | loss: 2.2544374
Losses:  1.5412278175354004 1.217116117477417
MemoryTrain:  epoch  0, batch     0 | loss: 1.5412278Losses:  1.0480085611343384 1.027636170387268
MemoryTrain:  epoch  0, batch     1 | loss: 1.0480086Losses:  0.5745234489440918 0.9083073735237122
MemoryTrain:  epoch  0, batch     2 | loss: 0.5745234Losses:  1.3049167394638062 0.13229362666606903
MemoryTrain:  epoch  0, batch     3 | loss: 1.3049167Losses:  1.2910382747650146 1.0194941759109497
MemoryTrain:  epoch  1, batch     0 | loss: 1.2910383Losses:  0.9072468280792236 0.91411292552948
MemoryTrain:  epoch  1, batch     1 | loss: 0.9072468Losses:  1.3075536489486694 0.9872791171073914
MemoryTrain:  epoch  1, batch     2 | loss: 1.3075536Losses:  0.6304141879081726 0.10311843454837799
MemoryTrain:  epoch  1, batch     3 | loss: 0.6304142Losses:  0.810691237449646 0.9304262399673462
MemoryTrain:  epoch  2, batch     0 | loss: 0.8106912Losses:  0.7895209789276123 1.0658118724822998
MemoryTrain:  epoch  2, batch     1 | loss: 0.7895210Losses:  0.7282748818397522 0.8119726181030273
MemoryTrain:  epoch  2, batch     2 | loss: 0.7282749Losses:  0.14357995986938477 0.10090702027082443
MemoryTrain:  epoch  2, batch     3 | loss: 0.1435800Losses:  0.8694260120391846 1.099367380142212
MemoryTrain:  epoch  3, batch     0 | loss: 0.8694260Losses:  0.5441719889640808 0.9648053646087646
MemoryTrain:  epoch  3, batch     1 | loss: 0.5441720Losses:  0.4920664429664612 0.8322252035140991
MemoryTrain:  epoch  3, batch     2 | loss: 0.4920664Losses:  0.1274813711643219 0.06788145005702972
MemoryTrain:  epoch  3, batch     3 | loss: 0.1274814Losses:  0.593224287033081 0.9100861549377441
MemoryTrain:  epoch  4, batch     0 | loss: 0.5932243Losses:  0.4427393078804016 0.7466744184494019
MemoryTrain:  epoch  4, batch     1 | loss: 0.4427393Losses:  0.5597437024116516 0.9693160057067871
MemoryTrain:  epoch  4, batch     2 | loss: 0.5597437Losses:  0.16498130559921265 0.23932945728302002
MemoryTrain:  epoch  4, batch     3 | loss: 0.1649813Losses:  0.5260183215141296 0.8250447511672974
MemoryTrain:  epoch  5, batch     0 | loss: 0.5260183Losses:  0.515651524066925 0.8659201860427856
MemoryTrain:  epoch  5, batch     1 | loss: 0.5156515Losses:  0.5437989234924316 1.001847267150879
MemoryTrain:  epoch  5, batch     2 | loss: 0.5437989Losses:  0.03873802348971367 0.02929849922657013
MemoryTrain:  epoch  5, batch     3 | loss: 0.0387380Losses:  0.5654153227806091 0.9989437460899353
MemoryTrain:  epoch  6, batch     0 | loss: 0.5654153Losses:  0.4794500768184662 0.7951077222824097
MemoryTrain:  epoch  6, batch     1 | loss: 0.4794501Losses:  0.4647064805030823 0.8068192005157471
MemoryTrain:  epoch  6, batch     2 | loss: 0.4647065Losses:  0.3245008587837219 0.2315831333398819
MemoryTrain:  epoch  6, batch     3 | loss: 0.3245009Losses:  0.47148263454437256 0.8448454141616821
MemoryTrain:  epoch  7, batch     0 | loss: 0.4714826Losses:  0.590129017829895 1.0132765769958496
MemoryTrain:  epoch  7, batch     1 | loss: 0.5901290Losses:  0.40578946471214294 0.7364963889122009
MemoryTrain:  epoch  7, batch     2 | loss: 0.4057895Losses:  0.09116020053625107 0.13070668280124664
MemoryTrain:  epoch  7, batch     3 | loss: 0.0911602Losses:  0.47374528646469116 0.890213131904602
MemoryTrain:  epoch  8, batch     0 | loss: 0.4737453Losses:  0.46017149090766907 0.8453828692436218
MemoryTrain:  epoch  8, batch     1 | loss: 0.4601715Losses:  0.49025875329971313 0.9143638610839844
MemoryTrain:  epoch  8, batch     2 | loss: 0.4902588Losses:  0.0725715160369873 0.023118827491998672
MemoryTrain:  epoch  8, batch     3 | loss: 0.0725715Losses:  0.47989529371261597 0.8830597400665283
MemoryTrain:  epoch  9, batch     0 | loss: 0.4798953Losses:  0.3974706828594208 0.7352319359779358
MemoryTrain:  epoch  9, batch     1 | loss: 0.3974707Losses:  0.4835505485534668 0.907480001449585
MemoryTrain:  epoch  9, batch     2 | loss: 0.4835505Losses:  0.08942463994026184 0.06771121919155121
MemoryTrain:  epoch  9, batch     3 | loss: 0.0894246
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 47.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 51.04%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 58.84%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 56.85%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 56.45%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 56.63%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 55.15%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 54.46%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 53.47%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 52.20%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 52.14%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 52.56%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 53.35%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 54.51%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 54.97%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 55.14%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 55.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 55.85%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 56.38%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 57.02%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 57.63%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 58.46%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.25%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 60.02%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 65.16%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 65.48%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.76%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.87%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.62%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.24%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.01%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 88.96%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.13%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.11%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 88.99%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 89.26%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.96%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 88.65%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 88.23%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.90%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.74%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 87.42%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.12%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 86.52%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 86.01%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 85.29%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 84.74%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 83.84%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 83.59%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 83.22%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 82.85%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 82.35%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 81.45%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 80.59%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 80.14%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 79.59%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 79.23%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 78.81%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 78.53%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.37%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 78.16%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 78.00%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 77.83%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.51%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.97%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.43%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 75.91%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 75.51%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 75.17%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.35%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 76.66%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 76.74%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 76.50%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 76.54%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 77.04%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 76.75%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.47%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 76.11%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 75.84%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 75.66%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.39%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 75.83%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 75.41%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 74.96%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 74.59%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.11%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.64%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.65%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 74.55%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 74.93%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 74.96%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.04%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.14%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 74.96%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 74.75%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 74.58%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 74.94%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 74.78%   [EVAL] batch:  195 | acc: 37.50%,  total acc: 74.59%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 74.40%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 74.24%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 73.99%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 73.81%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 73.79%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 73.71%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 73.67%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 73.55%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 73.32%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 72.95%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 72.78%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 72.55%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.51%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.15%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 73.14%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.09%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 74.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.10%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 75.22%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 75.05%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 74.80%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 74.58%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 74.41%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 74.17%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 74.03%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 73.92%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.80%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 73.84%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 73.81%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 74.21%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 74.03%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 73.85%   [EVAL] batch:  278 | acc: 37.50%,  total acc: 73.72%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 73.53%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 73.40%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 73.29%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 73.02%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 72.87%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 72.68%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 72.45%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 72.34%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 72.39%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 72.35%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.43%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.44%   
cur_acc:  ['0.9474', '0.7490', '0.7500', '0.7748', '0.6548']
his_acc:  ['0.9474', '0.8455', '0.7975', '0.7750', '0.7344']
Clustering into  29  clusters
Clusters:  [11  4  0  5 14 28  0  5 22 22  0 22 24  9 15  8 10  9  1  8 21  8 25 16
  5 18  3  5 26 27  0  3  3  4 23 11 19 17  5 26  4  7  6  0  5 12 21 10
 13 10 12  8  2 20  6  1  2 10  4  3]
Losses:  7.406307697296143 1.0470705032348633
CurrentTrain: epoch  0, batch     0 | loss: 7.4063077Losses:  7.033029079437256 1.0469353199005127
CurrentTrain: epoch  0, batch     1 | loss: 7.0330291Losses:  7.44959020614624 1.0106284618377686
CurrentTrain: epoch  0, batch     2 | loss: 7.4495902Losses:  7.788985252380371 0.6941450834274292
CurrentTrain: epoch  0, batch     3 | loss: 7.7889853Losses:  6.162800312042236 0.9894307255744934
CurrentTrain: epoch  1, batch     0 | loss: 6.1628003Losses:  6.6143798828125 1.1662602424621582
CurrentTrain: epoch  1, batch     1 | loss: 6.6143799Losses:  5.271432876586914 1.0548434257507324
CurrentTrain: epoch  1, batch     2 | loss: 5.2714329Losses:  5.403929233551025 0.1715216189622879
CurrentTrain: epoch  1, batch     3 | loss: 5.4039292Losses:  5.559338092803955 0.9888170957565308
CurrentTrain: epoch  2, batch     0 | loss: 5.5593381Losses:  5.405905246734619 1.036757469177246
CurrentTrain: epoch  2, batch     1 | loss: 5.4059052Losses:  5.259810924530029 1.116768479347229
CurrentTrain: epoch  2, batch     2 | loss: 5.2598109Losses:  2.7537002563476562 0.0
CurrentTrain: epoch  2, batch     3 | loss: 2.7537003Losses:  4.892484664916992 1.1206716299057007
CurrentTrain: epoch  3, batch     0 | loss: 4.8924847Losses:  4.356465816497803 0.9988116025924683
CurrentTrain: epoch  3, batch     1 | loss: 4.3564658Losses:  4.95788049697876 0.8732806444168091
CurrentTrain: epoch  3, batch     2 | loss: 4.9578805Losses:  5.485841274261475 0.5721612572669983
CurrentTrain: epoch  3, batch     3 | loss: 5.4858413Losses:  4.268799781799316 1.0190675258636475
CurrentTrain: epoch  4, batch     0 | loss: 4.2687998Losses:  4.876974105834961 1.0751911401748657
CurrentTrain: epoch  4, batch     1 | loss: 4.8769741Losses:  4.408369541168213 1.080828309059143
CurrentTrain: epoch  4, batch     2 | loss: 4.4083695Losses:  2.4592020511627197 0.20486894249916077
CurrentTrain: epoch  4, batch     3 | loss: 2.4592021Losses:  3.4032702445983887 0.8983842730522156
CurrentTrain: epoch  5, batch     0 | loss: 3.4032702Losses:  4.287589073181152 1.0997915267944336
CurrentTrain: epoch  5, batch     1 | loss: 4.2875891Losses:  5.191225051879883 1.0863364934921265
CurrentTrain: epoch  5, batch     2 | loss: 5.1912251Losses:  2.264716625213623 0.19234314560890198
CurrentTrain: epoch  5, batch     3 | loss: 2.2647166Losses:  3.7515482902526855 1.001089096069336
CurrentTrain: epoch  6, batch     0 | loss: 3.7515483Losses:  4.33547830581665 0.9651936292648315
CurrentTrain: epoch  6, batch     1 | loss: 4.3354783Losses:  3.833467483520508 0.8602924346923828
CurrentTrain: epoch  6, batch     2 | loss: 3.8334675Losses:  3.3937737941741943 0.22448748350143433
CurrentTrain: epoch  6, batch     3 | loss: 3.3937738Losses:  4.30471658706665 0.990311324596405
CurrentTrain: epoch  7, batch     0 | loss: 4.3047166Losses:  3.59957218170166 1.0468685626983643
CurrentTrain: epoch  7, batch     1 | loss: 3.5995722Losses:  3.262296438217163 0.8895155191421509
CurrentTrain: epoch  7, batch     2 | loss: 3.2622964Losses:  1.9406286478042603 0.0765259712934494
CurrentTrain: epoch  7, batch     3 | loss: 1.9406286Losses:  3.8159568309783936 0.8958963751792908
CurrentTrain: epoch  8, batch     0 | loss: 3.8159568Losses:  3.4112091064453125 0.9370543956756592
CurrentTrain: epoch  8, batch     1 | loss: 3.4112091Losses:  3.5542352199554443 1.0863919258117676
CurrentTrain: epoch  8, batch     2 | loss: 3.5542352Losses:  1.7985278367996216 0.17414632439613342
CurrentTrain: epoch  8, batch     3 | loss: 1.7985278Losses:  3.1886088848114014 0.939256489276886
CurrentTrain: epoch  9, batch     0 | loss: 3.1886089Losses:  3.223843812942505 1.0535744428634644
CurrentTrain: epoch  9, batch     1 | loss: 3.2238438Losses:  3.6859383583068848 0.9791554808616638
CurrentTrain: epoch  9, batch     2 | loss: 3.6859384Losses:  1.8522759675979614 0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.8522760
Losses:  1.3914971351623535 1.0784207582473755
MemoryTrain:  epoch  0, batch     0 | loss: 1.3914971Losses:  0.8066436052322388 0.9678855538368225
MemoryTrain:  epoch  0, batch     1 | loss: 0.8066436Losses:  1.3375844955444336 1.0372862815856934
MemoryTrain:  epoch  0, batch     2 | loss: 1.3375845Losses:  0.8487064838409424 0.8324917554855347
MemoryTrain:  epoch  0, batch     3 | loss: 0.8487065Losses:  1.2978432178497314 0.943873941898346
MemoryTrain:  epoch  1, batch     0 | loss: 1.2978432Losses:  1.6400604248046875 1.0647536516189575
MemoryTrain:  epoch  1, batch     1 | loss: 1.6400604Losses:  0.6622985601425171 0.8481031656265259
MemoryTrain:  epoch  1, batch     2 | loss: 0.6622986Losses:  1.1812200546264648 1.0214004516601562
MemoryTrain:  epoch  1, batch     3 | loss: 1.1812201Losses:  0.8620088696479797 1.0518765449523926
MemoryTrain:  epoch  2, batch     0 | loss: 0.8620089Losses:  0.7523416876792908 0.9935629963874817
MemoryTrain:  epoch  2, batch     1 | loss: 0.7523417Losses:  0.6348103880882263 0.8525842428207397
MemoryTrain:  epoch  2, batch     2 | loss: 0.6348104Losses:  0.7998817563056946 0.7593590617179871
MemoryTrain:  epoch  2, batch     3 | loss: 0.7998818Losses:  0.6465151309967041 0.8601726293563843
MemoryTrain:  epoch  3, batch     0 | loss: 0.6465151Losses:  0.6963269710540771 1.0853843688964844
MemoryTrain:  epoch  3, batch     1 | loss: 0.6963270Losses:  0.6593308448791504 1.0496249198913574
MemoryTrain:  epoch  3, batch     2 | loss: 0.6593308Losses:  0.372363805770874 0.6020591259002686
MemoryTrain:  epoch  3, batch     3 | loss: 0.3723638Losses:  0.5360134840011597 0.7844148278236389
MemoryTrain:  epoch  4, batch     0 | loss: 0.5360135Losses:  0.6316666603088379 0.9524855017662048
MemoryTrain:  epoch  4, batch     1 | loss: 0.6316667Losses:  0.6134165525436401 1.0549938678741455
MemoryTrain:  epoch  4, batch     2 | loss: 0.6134166Losses:  0.41716986894607544 0.629687488079071
MemoryTrain:  epoch  4, batch     3 | loss: 0.4171699Losses:  0.7197508811950684 1.06272554397583
MemoryTrain:  epoch  5, batch     0 | loss: 0.7197509Losses:  0.5853739380836487 0.999634861946106
MemoryTrain:  epoch  5, batch     1 | loss: 0.5853739Losses:  0.44017818570137024 0.799274206161499
MemoryTrain:  epoch  5, batch     2 | loss: 0.4401782Losses:  0.5416860580444336 0.6922789812088013
MemoryTrain:  epoch  5, batch     3 | loss: 0.5416861Losses:  0.4922912120819092 0.8415691256523132
MemoryTrain:  epoch  6, batch     0 | loss: 0.4922912Losses:  0.6255402565002441 1.0364177227020264
MemoryTrain:  epoch  6, batch     1 | loss: 0.6255403Losses:  0.3725665211677551 0.6524550914764404
MemoryTrain:  epoch  6, batch     2 | loss: 0.3725665Losses:  0.4817543029785156 0.8343684077262878
MemoryTrain:  epoch  6, batch     3 | loss: 0.4817543Losses:  0.5175924301147461 0.8919404745101929
MemoryTrain:  epoch  7, batch     0 | loss: 0.5175924Losses:  0.4753788709640503 0.8510711193084717
MemoryTrain:  epoch  7, batch     1 | loss: 0.4753789Losses:  0.45017102360725403 0.8344693183898926
MemoryTrain:  epoch  7, batch     2 | loss: 0.4501710Losses:  0.5239109396934509 0.8958739042282104
MemoryTrain:  epoch  7, batch     3 | loss: 0.5239109Losses:  0.5330780744552612 0.9409836530685425
MemoryTrain:  epoch  8, batch     0 | loss: 0.5330781Losses:  0.40811797976493835 0.7558434009552002
MemoryTrain:  epoch  8, batch     1 | loss: 0.4081180Losses:  0.4073374271392822 0.7284031510353088
MemoryTrain:  epoch  8, batch     2 | loss: 0.4073374Losses:  0.3943517506122589 0.7231935262680054
MemoryTrain:  epoch  8, batch     3 | loss: 0.3943518Losses:  0.4295564293861389 0.7969745397567749
MemoryTrain:  epoch  9, batch     0 | loss: 0.4295564Losses:  0.4295033812522888 0.7684649229049683
MemoryTrain:  epoch  9, batch     1 | loss: 0.4295034Losses:  0.44845908880233765 0.8366183042526245
MemoryTrain:  epoch  9, batch     2 | loss: 0.4484591Losses:  0.40494441986083984 0.727109432220459
MemoryTrain:  epoch  9, batch     3 | loss: 0.4049444
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 64.87%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 66.92%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 67.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 67.36%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.81%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.02%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 89.19%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.04%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 88.09%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 87.88%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 87.41%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 87.22%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 86.87%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.07%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.16%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 87.01%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 86.44%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 86.02%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.52%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 84.94%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 84.52%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 84.19%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 83.87%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 83.12%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 82.65%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 82.36%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 82.07%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 81.79%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 81.45%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 80.59%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 80.14%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 79.34%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 78.91%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 78.50%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.28%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.91%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 77.82%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 77.74%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.59%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.28%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 75.62%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 75.23%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 74.89%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.67%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 75.44%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 75.29%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 75.34%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 75.29%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.92%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 75.63%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 75.31%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 74.96%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 74.65%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.18%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.63%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 74.18%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.69%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.25%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 72.82%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.36%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 72.90%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 72.36%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 71.99%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.91%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 71.82%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 71.70%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 71.55%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 71.72%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 71.84%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 71.89%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 71.68%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 71.51%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 70.98%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 70.78%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.74%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 70.82%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 70.55%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 70.51%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 70.41%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 70.07%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 69.88%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 69.67%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 69.46%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 70.20%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 70.26%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 72.68%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 72.45%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 72.21%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 71.97%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 71.69%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 71.41%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 71.28%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 71.15%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 71.07%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 71.00%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 71.01%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 71.51%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 71.34%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 71.15%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 70.86%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 70.47%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 70.26%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 70.06%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 69.82%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 69.70%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 69.68%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  290 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 69.47%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 69.96%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 69.76%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 69.40%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 69.42%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.80%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 69.84%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 70.46%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 70.33%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 70.19%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 70.06%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.86%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 69.73%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 70.07%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 70.01%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 69.96%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 69.93%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.02%   
cur_acc:  ['0.9474', '0.7490', '0.7500', '0.7748', '0.6548', '0.6736']
his_acc:  ['0.9474', '0.8455', '0.7975', '0.7750', '0.7344', '0.7002']
Clustering into  34  clusters
Clusters:  [ 6 30  0  1 27 25  0 12  2  2  0  2 26 13 33  5 16 13  3  5 14  5 15 19
  1 30 22  1 21 23 28 22 14  1 24  6 20 31 12 21 17 18  8  0 12  7  4 10
 29 16  7  5 32 11  8  3  9 10  1 22 30  5 22 33 15  3  4  1  4  1]
Losses:  7.328729152679443 1.0918105840682983
CurrentTrain: epoch  0, batch     0 | loss: 7.3287292Losses:  7.162616729736328 1.0103026628494263
CurrentTrain: epoch  0, batch     1 | loss: 7.1626167Losses:  6.803174018859863 0.9630056619644165
CurrentTrain: epoch  0, batch     2 | loss: 6.8031740Losses:  9.047832489013672 0.5516473650932312
CurrentTrain: epoch  0, batch     3 | loss: 9.0478325Losses:  6.449677467346191 0.9447284936904907
CurrentTrain: epoch  1, batch     0 | loss: 6.4496775Losses:  6.1980791091918945 1.0342119932174683
CurrentTrain: epoch  1, batch     1 | loss: 6.1980791Losses:  6.217141151428223 1.0564484596252441
CurrentTrain: epoch  1, batch     2 | loss: 6.2171412Losses:  5.0389180183410645 0.16695281863212585
CurrentTrain: epoch  1, batch     3 | loss: 5.0389180Losses:  5.4558000564575195 0.8802793025970459
CurrentTrain: epoch  2, batch     0 | loss: 5.4558001Losses:  5.947238922119141 1.0945398807525635
CurrentTrain: epoch  2, batch     1 | loss: 5.9472389Losses:  6.4060235023498535 0.9984989166259766
CurrentTrain: epoch  2, batch     2 | loss: 6.4060235Losses:  5.924502372741699 0.2851027548313141
CurrentTrain: epoch  2, batch     3 | loss: 5.9245024Losses:  5.361647129058838 0.9290097951889038
CurrentTrain: epoch  3, batch     0 | loss: 5.3616471Losses:  5.9873785972595215 0.9227321147918701
CurrentTrain: epoch  3, batch     1 | loss: 5.9873786Losses:  4.714480876922607 0.9937707781791687
CurrentTrain: epoch  3, batch     2 | loss: 4.7144809Losses:  7.46423864364624 0.3970382511615753
CurrentTrain: epoch  3, batch     3 | loss: 7.4642386Losses:  5.334859371185303 0.8781144022941589
CurrentTrain: epoch  4, batch     0 | loss: 5.3348594Losses:  4.488745212554932 0.8476995229721069
CurrentTrain: epoch  4, batch     1 | loss: 4.4887452Losses:  5.312211513519287 0.7989349961280823
CurrentTrain: epoch  4, batch     2 | loss: 5.3122115Losses:  3.055569648742676 0.16250282526016235
CurrentTrain: epoch  4, batch     3 | loss: 3.0555696Losses:  5.043124198913574 0.9642434120178223
CurrentTrain: epoch  5, batch     0 | loss: 5.0431242Losses:  4.751288414001465 0.972231388092041
CurrentTrain: epoch  5, batch     1 | loss: 4.7512884Losses:  4.104182720184326 0.8877192139625549
CurrentTrain: epoch  5, batch     2 | loss: 4.1041827Losses:  4.260354995727539 0.2070179283618927
CurrentTrain: epoch  5, batch     3 | loss: 4.2603550Losses:  4.240240097045898 0.9677157998085022
CurrentTrain: epoch  6, batch     0 | loss: 4.2402401Losses:  4.80787467956543 0.9806391000747681
CurrentTrain: epoch  6, batch     1 | loss: 4.8078747Losses:  3.847132682800293 0.7528855800628662
CurrentTrain: epoch  6, batch     2 | loss: 3.8471327Losses:  2.3641881942749023 0.14165949821472168
CurrentTrain: epoch  6, batch     3 | loss: 2.3641882Losses:  4.024630546569824 0.8872694969177246
CurrentTrain: epoch  7, batch     0 | loss: 4.0246305Losses:  4.454405784606934 1.0120983123779297
CurrentTrain: epoch  7, batch     1 | loss: 4.4544058Losses:  3.7797892093658447 0.8131460547447205
CurrentTrain: epoch  7, batch     2 | loss: 3.7797892Losses:  3.682084560394287 0.17848502099514008
CurrentTrain: epoch  7, batch     3 | loss: 3.6820846Losses:  4.29748010635376 0.8259713649749756
CurrentTrain: epoch  8, batch     0 | loss: 4.2974801Losses:  3.710977554321289 0.8247120380401611
CurrentTrain: epoch  8, batch     1 | loss: 3.7109776Losses:  3.747615098953247 0.8483855128288269
CurrentTrain: epoch  8, batch     2 | loss: 3.7476151Losses:  2.3764045238494873 0.08775105327367783
CurrentTrain: epoch  8, batch     3 | loss: 2.3764045Losses:  3.959139585494995 0.7987047433853149
CurrentTrain: epoch  9, batch     0 | loss: 3.9591396Losses:  3.308928966522217 0.9031630158424377
CurrentTrain: epoch  9, batch     1 | loss: 3.3089290Losses:  4.05104923248291 0.8625558614730835
CurrentTrain: epoch  9, batch     2 | loss: 4.0510492Losses:  2.741529703140259 0.04955894500017166
CurrentTrain: epoch  9, batch     3 | loss: 2.7415297
Losses:  0.7774745225906372 0.9871864318847656
MemoryTrain:  epoch  0, batch     0 | loss: 0.7774745Losses:  0.988599419593811 1.1082717180252075
MemoryTrain:  epoch  0, batch     1 | loss: 0.9885994Losses:  0.5086954236030579 0.9226481914520264
MemoryTrain:  epoch  0, batch     2 | loss: 0.5086954Losses:  1.101286768913269 0.7623481154441833
MemoryTrain:  epoch  0, batch     3 | loss: 1.1012868Losses:  0.57806396484375 0.3876273036003113
MemoryTrain:  epoch  0, batch     4 | loss: 0.5780640Losses:  1.0438932180404663 0.9859631657600403
MemoryTrain:  epoch  1, batch     0 | loss: 1.0438932Losses:  1.0942814350128174 0.8283390998840332
MemoryTrain:  epoch  1, batch     1 | loss: 1.0942814Losses:  0.5322409868240356 0.7617255449295044
MemoryTrain:  epoch  1, batch     2 | loss: 0.5322410Losses:  0.8487778902053833 0.936417818069458
MemoryTrain:  epoch  1, batch     3 | loss: 0.8487779Losses:  0.5935080647468567 0.5666946172714233
MemoryTrain:  epoch  1, batch     4 | loss: 0.5935081Losses:  0.7984042167663574 0.895767092704773
MemoryTrain:  epoch  2, batch     0 | loss: 0.7984042Losses:  0.568899393081665 0.8586184978485107
MemoryTrain:  epoch  2, batch     1 | loss: 0.5688994Losses:  0.6122087836265564 0.9385973811149597
MemoryTrain:  epoch  2, batch     2 | loss: 0.6122088Losses:  0.7068302035331726 0.847407341003418
MemoryTrain:  epoch  2, batch     3 | loss: 0.7068302Losses:  0.18020351231098175 0.2689237892627716
MemoryTrain:  epoch  2, batch     4 | loss: 0.1802035Losses:  0.5871834754943848 0.9323310852050781
MemoryTrain:  epoch  3, batch     0 | loss: 0.5871835Losses:  0.5037084221839905 0.7766722440719604
MemoryTrain:  epoch  3, batch     1 | loss: 0.5037084Losses:  0.5791854858398438 0.8555378913879395
MemoryTrain:  epoch  3, batch     2 | loss: 0.5791855Losses:  0.5057856440544128 0.8530659079551697
MemoryTrain:  epoch  3, batch     3 | loss: 0.5057856Losses:  0.25899723172187805 0.36789795756340027
MemoryTrain:  epoch  3, batch     4 | loss: 0.2589972Losses:  0.5100120306015015 0.8948414325714111
MemoryTrain:  epoch  4, batch     0 | loss: 0.5100120Losses:  0.45611077547073364 0.748947024345398
MemoryTrain:  epoch  4, batch     1 | loss: 0.4561108Losses:  0.5563755631446838 0.970160961151123
MemoryTrain:  epoch  4, batch     2 | loss: 0.5563756Losses:  0.5047575235366821 0.7919171452522278
MemoryTrain:  epoch  4, batch     3 | loss: 0.5047575Losses:  0.35972222685813904 0.4965962767601013
MemoryTrain:  epoch  4, batch     4 | loss: 0.3597222Losses:  0.42860308289527893 0.7598363161087036
MemoryTrain:  epoch  5, batch     0 | loss: 0.4286031Losses:  0.6037416458129883 1.0684802532196045
MemoryTrain:  epoch  5, batch     1 | loss: 0.6037416Losses:  0.35648661851882935 0.6168187856674194
MemoryTrain:  epoch  5, batch     2 | loss: 0.3564866Losses:  0.5497100353240967 0.9983164072036743
MemoryTrain:  epoch  5, batch     3 | loss: 0.5497100Losses:  0.27107295393943787 0.3509162366390228
MemoryTrain:  epoch  5, batch     4 | loss: 0.2710730Losses:  0.4921693801879883 0.8875049352645874
MemoryTrain:  epoch  6, batch     0 | loss: 0.4921694Losses:  0.40263378620147705 0.7340142130851746
MemoryTrain:  epoch  6, batch     1 | loss: 0.4026338Losses:  0.5767601728439331 0.9044535160064697
MemoryTrain:  epoch  6, batch     2 | loss: 0.5767602Losses:  0.42875659465789795 0.7257538437843323
MemoryTrain:  epoch  6, batch     3 | loss: 0.4287566Losses:  0.23589858412742615 0.40127652883529663
MemoryTrain:  epoch  6, batch     4 | loss: 0.2358986Losses:  0.46500301361083984 0.7848531007766724
MemoryTrain:  epoch  7, batch     0 | loss: 0.4650030Losses:  0.47480180859565735 0.8740829229354858
MemoryTrain:  epoch  7, batch     1 | loss: 0.4748018Losses:  0.44724157452583313 0.8235397338867188
MemoryTrain:  epoch  7, batch     2 | loss: 0.4472416Losses:  0.5257537961006165 0.9152616858482361
MemoryTrain:  epoch  7, batch     3 | loss: 0.5257538Losses:  0.13597162067890167 0.21860913932323456
MemoryTrain:  epoch  7, batch     4 | loss: 0.1359716Losses:  0.42360055446624756 0.7847287654876709
MemoryTrain:  epoch  8, batch     0 | loss: 0.4236006Losses:  0.4252670407295227 0.7984297275543213
MemoryTrain:  epoch  8, batch     1 | loss: 0.4252670Losses:  0.46680158376693726 0.8528397083282471
MemoryTrain:  epoch  8, batch     2 | loss: 0.4668016Losses:  0.4553338885307312 0.8090776205062866
MemoryTrain:  epoch  8, batch     3 | loss: 0.4553339Losses:  0.18341881036758423 0.3055572509765625
MemoryTrain:  epoch  8, batch     4 | loss: 0.1834188Losses:  0.4493843913078308 0.8285531997680664
MemoryTrain:  epoch  9, batch     0 | loss: 0.4493844Losses:  0.43216046690940857 0.7763638496398926
MemoryTrain:  epoch  9, batch     1 | loss: 0.4321605Losses:  0.34076642990112305 0.5921355485916138
MemoryTrain:  epoch  9, batch     2 | loss: 0.3407664Losses:  0.4404161870479584 0.8197017312049866
MemoryTrain:  epoch  9, batch     3 | loss: 0.4404162Losses:  0.18463954329490662 0.3277907967567444
MemoryTrain:  epoch  9, batch     4 | loss: 0.1846395
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 55.71%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 52.40%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 50.69%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 48.88%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 47.41%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 44.35%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 44.92%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 46.02%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 46.88%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 48.39%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 49.13%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 49.83%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 50.99%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 51.76%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 52.81%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 53.81%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 54.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.67%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 56.53%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 57.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 57.61%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 57.85%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 58.20%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 58.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 58.82%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 59.79%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 60.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 60.45%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 60.83%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 60.34%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 59.96%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 60.10%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.62%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.14%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.00%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 87.40%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 87.03%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 86.66%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 86.21%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 85.87%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 86.02%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 85.96%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 85.39%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 85.18%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 84.92%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 84.60%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 84.26%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 83.53%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 83.28%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 82.69%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 82.39%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 81.88%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 81.32%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 79.82%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 79.17%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 78.59%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 78.22%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 77.67%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 76.79%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 76.39%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 75.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.24%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.06%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 74.77%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 74.25%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 73.74%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.18%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 72.80%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 72.49%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 72.35%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 73.67%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 73.65%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 73.57%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 73.49%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 73.40%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 73.88%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.57%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 73.23%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 72.89%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.68%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.44%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.97%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.53%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 71.63%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.75%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 71.15%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 70.83%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 70.44%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 70.10%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 69.79%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 69.56%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 69.67%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  194 | acc: 18.75%,  total acc: 70.03%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 69.80%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 69.57%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 69.16%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 69.09%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 69.03%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 68.78%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 68.66%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 68.48%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 67.68%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 68.36%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 68.39%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 70.33%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 70.84%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 70.59%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 70.33%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 70.08%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 69.80%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 69.53%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 69.41%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  275 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 69.56%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 69.42%   [EVAL] batch:  278 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  280 | acc: 25.00%,  total acc: 68.99%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 68.91%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 68.07%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 67.88%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 67.69%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 67.33%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:  292 | acc: 18.75%,  total acc: 67.02%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.69%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 66.66%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 67.60%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 67.40%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 67.23%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 67.03%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 66.86%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 66.77%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 67.14%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 67.04%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 66.97%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 67.99%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 67.61%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 67.91%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 67.81%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 67.78%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 67.60%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.36%   [EVAL] batch:  377 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  378 | acc: 6.25%,  total acc: 67.02%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 66.84%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 66.68%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 67.20%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  395 | acc: 37.50%,  total acc: 67.06%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 66.87%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 66.81%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.66%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 66.51%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.35%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 66.20%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 66.03%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 66.60%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 66.57%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 66.50%   
cur_acc:  ['0.9474', '0.7490', '0.7500', '0.7748', '0.6548', '0.6736', '0.5962']
his_acc:  ['0.9474', '0.8455', '0.7975', '0.7750', '0.7344', '0.7002', '0.6650']
Clustering into  38  clusters
Clusters:  [ 7 34  5  0 33 27  5  0 26 26  2 26 28  9 30 14 18  9 12 14 15 14  3 21
  0 34 17  0 10 29  5 17 15  8 31  7 13 19  0 35 20 37 11 36  0  6  1  4
 32 18  6 14 16 24 11 12  2  4  8 17 34 14 17 30  3 12  1  5  1  8  5 26
 10 14 23 22 25 16 36  2]
Losses:  6.551723480224609 0.8393905758857727
CurrentTrain: epoch  0, batch     0 | loss: 6.5517235Losses:  5.130241870880127 0.618235170841217
CurrentTrain: epoch  0, batch     1 | loss: 5.1302419Losses:  8.142653465270996 0.9393224716186523
CurrentTrain: epoch  0, batch     2 | loss: 8.1426535Losses:  5.924768447875977 0.0
CurrentTrain: epoch  0, batch     3 | loss: 5.9247684Losses:  5.830618381500244 0.7267424464225769
CurrentTrain: epoch  1, batch     0 | loss: 5.8306184Losses:  6.143996238708496 0.9109411239624023
CurrentTrain: epoch  1, batch     1 | loss: 6.1439962Losses:  5.371699810028076 0.7468674778938293
CurrentTrain: epoch  1, batch     2 | loss: 5.3716998Losses:  6.963908672332764 0.17560121417045593
CurrentTrain: epoch  1, batch     3 | loss: 6.9639087Losses:  5.611738204956055 0.9122119545936584
CurrentTrain: epoch  2, batch     0 | loss: 5.6117382Losses:  4.33113956451416 0.6178592443466187
CurrentTrain: epoch  2, batch     1 | loss: 4.3311396Losses:  6.309820652008057 0.8720619678497314
CurrentTrain: epoch  2, batch     2 | loss: 6.3098207Losses:  2.5602962970733643 0.04591424763202667
CurrentTrain: epoch  2, batch     3 | loss: 2.5602963Losses:  5.455595970153809 0.7060169577598572
CurrentTrain: epoch  3, batch     0 | loss: 5.4555960Losses:  4.327824592590332 0.5988672971725464
CurrentTrain: epoch  3, batch     1 | loss: 4.3278246Losses:  5.169351577758789 0.6925389170646667
CurrentTrain: epoch  3, batch     2 | loss: 5.1693516Losses:  7.387977123260498 0.13642868399620056
CurrentTrain: epoch  3, batch     3 | loss: 7.3879771Losses:  5.586833477020264 0.7397488951683044
CurrentTrain: epoch  4, batch     0 | loss: 5.5868335Losses:  4.497941493988037 0.6325960159301758
CurrentTrain: epoch  4, batch     1 | loss: 4.4979415Losses:  3.9147181510925293 0.6929596662521362
CurrentTrain: epoch  4, batch     2 | loss: 3.9147182Losses:  7.371284484863281 0.4517384171485901
CurrentTrain: epoch  4, batch     3 | loss: 7.3712845Losses:  4.317403793334961 0.7197103500366211
CurrentTrain: epoch  5, batch     0 | loss: 4.3174038Losses:  3.8669776916503906 0.6492920517921448
CurrentTrain: epoch  5, batch     1 | loss: 3.8669777Losses:  5.048935413360596 0.759253740310669
CurrentTrain: epoch  5, batch     2 | loss: 5.0489354Losses:  3.802802085876465 0.2103099822998047
CurrentTrain: epoch  5, batch     3 | loss: 3.8028021Losses:  4.134648323059082 0.7645469307899475
CurrentTrain: epoch  6, batch     0 | loss: 4.1346483Losses:  4.296478271484375 0.8193756341934204
CurrentTrain: epoch  6, batch     1 | loss: 4.2964783Losses:  4.157561302185059 0.6821688413619995
CurrentTrain: epoch  6, batch     2 | loss: 4.1575613Losses:  3.384448528289795 0.2250170260667801
CurrentTrain: epoch  6, batch     3 | loss: 3.3844485Losses:  3.420926332473755 0.6946672201156616
CurrentTrain: epoch  7, batch     0 | loss: 3.4209263Losses:  4.463512420654297 0.7516734600067139
CurrentTrain: epoch  7, batch     1 | loss: 4.4635124Losses:  3.743197441101074 0.6363896131515503
CurrentTrain: epoch  7, batch     2 | loss: 3.7431974Losses:  2.015517473220825 0.021973785012960434
CurrentTrain: epoch  7, batch     3 | loss: 2.0155175Losses:  3.6244986057281494 0.7185307741165161
CurrentTrain: epoch  8, batch     0 | loss: 3.6244986Losses:  3.619159698486328 0.6320746541023254
CurrentTrain: epoch  8, batch     1 | loss: 3.6191597Losses:  3.3634653091430664 0.5737168788909912
CurrentTrain: epoch  8, batch     2 | loss: 3.3634653Losses:  4.747949600219727 0.23096323013305664
CurrentTrain: epoch  8, batch     3 | loss: 4.7479496Losses:  3.7179558277130127 0.7335435152053833
CurrentTrain: epoch  9, batch     0 | loss: 3.7179558Losses:  3.07511568069458 0.5060785412788391
CurrentTrain: epoch  9, batch     1 | loss: 3.0751157Losses:  3.269378423690796 0.8111041188240051
CurrentTrain: epoch  9, batch     2 | loss: 3.2693784Losses:  3.7709243297576904 0.2956386208534241
CurrentTrain: epoch  9, batch     3 | loss: 3.7709243
Losses:  0.9708820581436157 0.9251636862754822
MemoryTrain:  epoch  0, batch     0 | loss: 0.9708821Losses:  1.0284771919250488 0.8759470582008362
MemoryTrain:  epoch  0, batch     1 | loss: 1.0284772Losses:  0.5654751062393188 0.9339800477027893
MemoryTrain:  epoch  0, batch     2 | loss: 0.5654751Losses:  0.65465247631073 0.8061206340789795
MemoryTrain:  epoch  0, batch     3 | loss: 0.6546525Losses:  0.997571587562561 0.8621580600738525
MemoryTrain:  epoch  0, batch     4 | loss: 0.9975716Losses:  1.2023899555206299 1.0046133995056152
MemoryTrain:  epoch  1, batch     0 | loss: 1.2023900Losses:  1.0682324171066284 0.7866612076759338
MemoryTrain:  epoch  1, batch     1 | loss: 1.0682324Losses:  0.9932946562767029 0.8228511810302734
MemoryTrain:  epoch  1, batch     2 | loss: 0.9932947Losses:  1.0669798851013184 0.8817282915115356
MemoryTrain:  epoch  1, batch     3 | loss: 1.0669799Losses:  0.4608829617500305 0.7831724882125854
MemoryTrain:  epoch  1, batch     4 | loss: 0.4608830Losses:  0.7061741352081299 0.9069007635116577
MemoryTrain:  epoch  2, batch     0 | loss: 0.7061741Losses:  0.7977595925331116 0.8935174345970154
MemoryTrain:  epoch  2, batch     1 | loss: 0.7977596Losses:  0.5472211241722107 0.7918131947517395
MemoryTrain:  epoch  2, batch     2 | loss: 0.5472211Losses:  0.5360609292984009 0.7508715391159058
MemoryTrain:  epoch  2, batch     3 | loss: 0.5360609Losses:  0.5207171440124512 0.8157227039337158
MemoryTrain:  epoch  2, batch     4 | loss: 0.5207171Losses:  0.6365546584129333 0.8277666568756104
MemoryTrain:  epoch  3, batch     0 | loss: 0.6365547Losses:  0.4847370684146881 0.7265337109565735
MemoryTrain:  epoch  3, batch     1 | loss: 0.4847371Losses:  0.4725647270679474 0.7866961359977722
MemoryTrain:  epoch  3, batch     2 | loss: 0.4725647Losses:  0.6993550062179565 0.9925674200057983
MemoryTrain:  epoch  3, batch     3 | loss: 0.6993550Losses:  0.4287354350090027 0.7433815598487854
MemoryTrain:  epoch  3, batch     4 | loss: 0.4287354Losses:  0.6266972422599792 1.04062819480896
MemoryTrain:  epoch  4, batch     0 | loss: 0.6266972Losses:  0.48817551136016846 0.8304481506347656
MemoryTrain:  epoch  4, batch     1 | loss: 0.4881755Losses:  0.43129608035087585 0.7239501476287842
MemoryTrain:  epoch  4, batch     2 | loss: 0.4312961Losses:  0.4941692352294922 0.6740536093711853
MemoryTrain:  epoch  4, batch     3 | loss: 0.4941692Losses:  0.4080585539340973 0.7230112552642822
MemoryTrain:  epoch  4, batch     4 | loss: 0.4080586Losses:  0.4478725790977478 0.768653154373169
MemoryTrain:  epoch  5, batch     0 | loss: 0.4478726Losses:  0.3974381685256958 0.6853727698326111
MemoryTrain:  epoch  5, batch     1 | loss: 0.3974382Losses:  0.43513619899749756 0.7836557030677795
MemoryTrain:  epoch  5, batch     2 | loss: 0.4351362Losses:  0.5135987997055054 0.7923752069473267
MemoryTrain:  epoch  5, batch     3 | loss: 0.5135988Losses:  0.4983953833580017 0.8901309967041016
MemoryTrain:  epoch  5, batch     4 | loss: 0.4983954Losses:  0.4064326286315918 0.7401660084724426
MemoryTrain:  epoch  6, batch     0 | loss: 0.4064326Losses:  0.37338152527809143 0.6646462678909302
MemoryTrain:  epoch  6, batch     1 | loss: 0.3733815Losses:  0.5467183589935303 0.8930166363716125
MemoryTrain:  epoch  6, batch     2 | loss: 0.5467184Losses:  0.4932405352592468 0.8744721412658691
MemoryTrain:  epoch  6, batch     3 | loss: 0.4932405Losses:  0.39569222927093506 0.716456413269043
MemoryTrain:  epoch  6, batch     4 | loss: 0.3956922Losses:  0.4754412770271301 0.8618655204772949
MemoryTrain:  epoch  7, batch     0 | loss: 0.4754413Losses:  0.4519513249397278 0.7311496734619141
MemoryTrain:  epoch  7, batch     1 | loss: 0.4519513Losses:  0.3628484904766083 0.6137748956680298
MemoryTrain:  epoch  7, batch     2 | loss: 0.3628485Losses:  0.4015686511993408 0.7036084532737732
MemoryTrain:  epoch  7, batch     3 | loss: 0.4015687Losses:  0.45380857586860657 0.847794771194458
MemoryTrain:  epoch  7, batch     4 | loss: 0.4538086Losses:  0.47386303544044495 0.8768373131752014
MemoryTrain:  epoch  8, batch     0 | loss: 0.4738630Losses:  0.3472243845462799 0.6324886083602905
MemoryTrain:  epoch  8, batch     1 | loss: 0.3472244Losses:  0.46528688073158264 0.8293012976646423
MemoryTrain:  epoch  8, batch     2 | loss: 0.4652869Losses:  0.4888591170310974 0.8680787086486816
MemoryTrain:  epoch  8, batch     3 | loss: 0.4888591Losses:  0.38229936361312866 0.6898126602172852
MemoryTrain:  epoch  8, batch     4 | loss: 0.3822994Losses:  0.394680380821228 0.7069240212440491
MemoryTrain:  epoch  9, batch     0 | loss: 0.3946804Losses:  0.4141818583011627 0.7678887248039246
MemoryTrain:  epoch  9, batch     1 | loss: 0.4141819Losses:  0.3690875172615051 0.6912403702735901
MemoryTrain:  epoch  9, batch     2 | loss: 0.3690875Losses:  0.49645763635635376 0.8941982984542847
MemoryTrain:  epoch  9, batch     3 | loss: 0.4964576Losses:  0.38961300253868103 0.7051805257797241
MemoryTrain:  epoch  9, batch     4 | loss: 0.3896130
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 69.70%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 67.60%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 67.73%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 67.04%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.47%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.37%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 87.28%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 86.96%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 85.96%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 85.79%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.22%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 84.08%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 82.98%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 81.91%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 80.69%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 79.50%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 78.80%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 79.23%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 79.20%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.19%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.81%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 78.43%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 78.16%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 77.90%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 77.65%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 76.78%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 76.40%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 75.55%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 74.07%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 73.68%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 73.18%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 72.94%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 72.39%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 71.97%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 71.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 71.26%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 71.12%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 71.03%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 70.93%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 70.68%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 70.20%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 69.72%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 69.20%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 68.81%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 70.09%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 70.28%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 70.35%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.37%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 70.95%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 70.54%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 70.12%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.67%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.70%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.83%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 68.47%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.02%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 68.14%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 67.84%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 67.47%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 67.14%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 66.89%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 66.72%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 67.21%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 66.55%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 67.20%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 67.44%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 67.13%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 66.95%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 66.74%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 66.30%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.08%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 65.94%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 65.55%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 65.01%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.01%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 66.10%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 66.03%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 68.48%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 67.98%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 67.74%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 67.48%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 67.10%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 67.10%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.95%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 66.59%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 66.37%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  275 | acc: 31.25%,  total acc: 66.85%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.70%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:  278 | acc: 37.50%,  total acc: 66.49%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 66.34%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 66.21%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 66.13%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.89%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 65.60%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 65.19%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 65.01%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 64.83%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 64.67%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 64.53%   [EVAL] batch:  292 | acc: 12.50%,  total acc: 64.36%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 64.24%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 64.03%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 63.98%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 63.96%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 65.01%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 64.68%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 64.47%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 64.29%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 64.17%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 64.59%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 64.41%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 64.27%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 64.11%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 63.88%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 64.95%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 64.77%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 64.60%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 64.42%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 64.26%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 64.73%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 64.66%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 64.60%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 64.54%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 64.49%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 64.48%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 64.33%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 64.19%   [EVAL] batch:  377 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 63.89%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 63.72%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 63.57%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.59%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  388 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  389 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 64.18%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 64.07%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 63.97%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 63.89%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 63.82%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 63.74%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.53%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 63.37%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 63.21%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 63.07%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.92%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.76%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 62.96%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 63.03%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.42%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 63.53%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 63.53%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.52%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 63.50%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 63.41%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 63.35%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 63.35%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 63.30%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 63.29%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 63.27%   [EVAL] batch:  441 | acc: 43.75%,  total acc: 63.22%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 63.18%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.19%   [EVAL] batch:  445 | acc: 50.00%,  total acc: 63.16%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 63.10%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  448 | acc: 50.00%,  total acc: 63.08%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 63.17%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 63.19%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.23%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 63.32%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 63.29%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 63.21%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 63.17%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 63.08%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 63.04%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  475 | acc: 50.00%,  total acc: 63.97%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 63.95%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.03%   [EVAL] batch:  481 | acc: 43.75%,  total acc: 63.99%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  483 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  484 | acc: 37.50%,  total acc: 63.76%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 63.72%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 63.72%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:  489 | acc: 87.50%,  total acc: 63.81%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.88%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 63.98%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 63.93%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 63.85%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:  498 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  499 | acc: 43.75%,  total acc: 63.71%   
cur_acc:  ['0.9474', '0.7490', '0.7500', '0.7748', '0.6548', '0.6736', '0.5962', '0.6647']
his_acc:  ['0.9474', '0.8455', '0.7975', '0.7750', '0.7344', '0.7002', '0.6650', '0.6371']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  10.730022430419922 1.5006341934204102
CurrentTrain: epoch  0, batch     0 | loss: 10.7300224Losses:  10.43830680847168 1.5301116704940796
CurrentTrain: epoch  0, batch     1 | loss: 10.4383068Losses:  10.245424270629883 1.3312036991119385
CurrentTrain: epoch  0, batch     2 | loss: 10.2454243Losses:  10.68059253692627 1.478010892868042
CurrentTrain: epoch  0, batch     3 | loss: 10.6805925Losses:  9.909902572631836 1.2735501527786255
CurrentTrain: epoch  0, batch     4 | loss: 9.9099026Losses:  9.745901107788086 1.310589075088501
CurrentTrain: epoch  0, batch     5 | loss: 9.7459011Losses:  9.729439735412598 1.2909168004989624
CurrentTrain: epoch  0, batch     6 | loss: 9.7294397Losses:  10.080253601074219 1.2905598878860474
CurrentTrain: epoch  0, batch     7 | loss: 10.0802536Losses:  10.042160987854004 1.3275854587554932
CurrentTrain: epoch  0, batch     8 | loss: 10.0421610Losses:  9.875136375427246 1.2599639892578125
CurrentTrain: epoch  0, batch     9 | loss: 9.8751364Losses:  9.005773544311523 1.1794242858886719
CurrentTrain: epoch  0, batch    10 | loss: 9.0057735Losses:  10.081621170043945 1.417356014251709
CurrentTrain: epoch  0, batch    11 | loss: 10.0816212Losses:  9.0331392288208 1.0707447528839111
CurrentTrain: epoch  0, batch    12 | loss: 9.0331392Losses:  9.4344482421875 1.2247662544250488
CurrentTrain: epoch  0, batch    13 | loss: 9.4344482Losses:  9.418785095214844 1.3265085220336914
CurrentTrain: epoch  0, batch    14 | loss: 9.4187851Losses:  9.716943740844727 1.0309476852416992
CurrentTrain: epoch  0, batch    15 | loss: 9.7169437Losses:  8.55456256866455 1.1545078754425049
CurrentTrain: epoch  0, batch    16 | loss: 8.5545626Losses:  9.058792114257812 1.1682835817337036
CurrentTrain: epoch  0, batch    17 | loss: 9.0587921Losses:  9.158863067626953 1.4740424156188965
CurrentTrain: epoch  0, batch    18 | loss: 9.1588631Losses:  8.315436363220215 1.0627570152282715
CurrentTrain: epoch  0, batch    19 | loss: 8.3154364Losses:  8.566120147705078 1.1422200202941895
CurrentTrain: epoch  0, batch    20 | loss: 8.5661201Losses:  8.499666213989258 0.9209749102592468
CurrentTrain: epoch  0, batch    21 | loss: 8.4996662Losses:  8.430938720703125 1.1606063842773438
CurrentTrain: epoch  0, batch    22 | loss: 8.4309387Losses:  9.598564147949219 1.493984580039978
CurrentTrain: epoch  0, batch    23 | loss: 9.5985641Losses:  9.910367965698242 1.344529151916504
CurrentTrain: epoch  0, batch    24 | loss: 9.9103680Losses:  8.121567726135254 1.0711320638656616
CurrentTrain: epoch  0, batch    25 | loss: 8.1215677Losses:  9.218761444091797 1.1506267786026
CurrentTrain: epoch  0, batch    26 | loss: 9.2187614Losses:  7.96937370300293 0.9285049438476562
CurrentTrain: epoch  0, batch    27 | loss: 7.9693737Losses:  8.735766410827637 1.2954691648483276
CurrentTrain: epoch  0, batch    28 | loss: 8.7357664Losses:  7.956780910491943 1.133998990058899
CurrentTrain: epoch  0, batch    29 | loss: 7.9567809Losses:  8.715455055236816 0.9398118853569031
CurrentTrain: epoch  0, batch    30 | loss: 8.7154551Losses:  8.738882064819336 1.11281418800354
CurrentTrain: epoch  0, batch    31 | loss: 8.7388821Losses:  8.68323802947998 1.2635892629623413
CurrentTrain: epoch  0, batch    32 | loss: 8.6832380Losses:  9.434792518615723 1.1643589735031128
CurrentTrain: epoch  0, batch    33 | loss: 9.4347925Losses:  8.5775146484375 1.1158032417297363
CurrentTrain: epoch  0, batch    34 | loss: 8.5775146Losses:  9.357522964477539 1.4755592346191406
CurrentTrain: epoch  0, batch    35 | loss: 9.3575230Losses:  7.615963935852051 1.144408941268921
CurrentTrain: epoch  0, batch    36 | loss: 7.6159639Losses:  7.74244499206543 1.20222806930542
CurrentTrain: epoch  0, batch    37 | loss: 7.7424450Losses:  8.72260570526123 1.2136900424957275
CurrentTrain: epoch  0, batch    38 | loss: 8.7226057Losses:  7.877929210662842 1.177842378616333
CurrentTrain: epoch  0, batch    39 | loss: 7.8779292Losses:  8.110260963439941 0.9025857448577881
CurrentTrain: epoch  0, batch    40 | loss: 8.1102610Losses:  7.0528740882873535 0.7361952662467957
CurrentTrain: epoch  0, batch    41 | loss: 7.0528741Losses:  7.681492805480957 1.1434483528137207
CurrentTrain: epoch  0, batch    42 | loss: 7.6814928Losses:  8.695045471191406 1.1822365522384644
CurrentTrain: epoch  0, batch    43 | loss: 8.6950455Losses:  8.896419525146484 1.122065782546997
CurrentTrain: epoch  0, batch    44 | loss: 8.8964195Losses:  9.055663108825684 1.267522931098938
CurrentTrain: epoch  0, batch    45 | loss: 9.0556631Losses:  8.431523323059082 1.016455888748169
CurrentTrain: epoch  0, batch    46 | loss: 8.4315233Losses:  7.504150390625 0.7463690638542175
CurrentTrain: epoch  0, batch    47 | loss: 7.5041504Losses:  8.261186599731445 0.9956274628639221
CurrentTrain: epoch  0, batch    48 | loss: 8.2611866Losses:  7.359450340270996 0.9962869882583618
CurrentTrain: epoch  0, batch    49 | loss: 7.3594503Losses:  8.857403755187988 1.4210866689682007
CurrentTrain: epoch  0, batch    50 | loss: 8.8574038Losses:  7.602156162261963 0.8877479434013367
CurrentTrain: epoch  0, batch    51 | loss: 7.6021562Losses:  7.2473554611206055 1.0245959758758545
CurrentTrain: epoch  0, batch    52 | loss: 7.2473555Losses:  7.313732624053955 0.9242280721664429
CurrentTrain: epoch  0, batch    53 | loss: 7.3137326Losses:  8.177303314208984 1.1045352220535278
CurrentTrain: epoch  0, batch    54 | loss: 8.1773033Losses:  9.277641296386719 1.2271595001220703
CurrentTrain: epoch  0, batch    55 | loss: 9.2776413Losses:  9.01129150390625 1.1470942497253418
CurrentTrain: epoch  0, batch    56 | loss: 9.0112915Losses:  6.9704999923706055 0.9542409181594849
CurrentTrain: epoch  0, batch    57 | loss: 6.9705000Losses:  8.788945198059082 1.3008437156677246
CurrentTrain: epoch  0, batch    58 | loss: 8.7889452Losses:  8.199751853942871 0.9142993092536926
CurrentTrain: epoch  0, batch    59 | loss: 8.1997519Losses:  8.099852561950684 0.9861293435096741
CurrentTrain: epoch  0, batch    60 | loss: 8.0998526Losses:  7.798494815826416 1.0076687335968018
CurrentTrain: epoch  0, batch    61 | loss: 7.7984948Losses:  7.613855361938477 1.0232014656066895
CurrentTrain: epoch  0, batch    62 | loss: 7.6138554Losses:  7.1734819412231445 0.9298118948936462
CurrentTrain: epoch  1, batch     0 | loss: 7.1734819Losses:  8.467639923095703 1.3565641641616821
CurrentTrain: epoch  1, batch     1 | loss: 8.4676399Losses:  7.123598575592041 0.93930584192276
CurrentTrain: epoch  1, batch     2 | loss: 7.1235986Losses:  7.361292362213135 0.7891942262649536
CurrentTrain: epoch  1, batch     3 | loss: 7.3612924Losses:  6.9225287437438965 0.8032386302947998
CurrentTrain: epoch  1, batch     4 | loss: 6.9225287Losses:  7.546040058135986 1.0483242273330688
CurrentTrain: epoch  1, batch     5 | loss: 7.5460401Losses:  8.051896095275879 0.8817782402038574
CurrentTrain: epoch  1, batch     6 | loss: 8.0518961Losses:  6.490650177001953 0.8075678944587708
CurrentTrain: epoch  1, batch     7 | loss: 6.4906502Losses:  7.135472774505615 1.0472004413604736
CurrentTrain: epoch  1, batch     8 | loss: 7.1354728Losses:  7.853265285491943 0.8611354827880859
CurrentTrain: epoch  1, batch     9 | loss: 7.8532653Losses:  6.943395614624023 0.8941843509674072
CurrentTrain: epoch  1, batch    10 | loss: 6.9433956Losses:  7.20753812789917 1.1160801649093628
CurrentTrain: epoch  1, batch    11 | loss: 7.2075381Losses:  7.616236686706543 0.8058662414550781
CurrentTrain: epoch  1, batch    12 | loss: 7.6162367Losses:  7.258674144744873 0.9598019123077393
CurrentTrain: epoch  1, batch    13 | loss: 7.2586741Losses:  7.099667072296143 1.1078579425811768
CurrentTrain: epoch  1, batch    14 | loss: 7.0996671Losses:  6.835961818695068 0.9249361753463745
CurrentTrain: epoch  1, batch    15 | loss: 6.8359618Losses:  7.346839904785156 0.7637079954147339
CurrentTrain: epoch  1, batch    16 | loss: 7.3468399Losses:  6.523528099060059 0.9574108123779297
CurrentTrain: epoch  1, batch    17 | loss: 6.5235281Losses:  7.5118584632873535 1.1784826517105103
CurrentTrain: epoch  1, batch    18 | loss: 7.5118585Losses:  8.058496475219727 1.2756847143173218
CurrentTrain: epoch  1, batch    19 | loss: 8.0584965Losses:  6.5092010498046875 0.7146592140197754
CurrentTrain: epoch  1, batch    20 | loss: 6.5092010Losses:  6.335226058959961 0.855197548866272
CurrentTrain: epoch  1, batch    21 | loss: 6.3352261Losses:  7.100684642791748 0.9977277517318726
CurrentTrain: epoch  1, batch    22 | loss: 7.1006846Losses:  6.7303266525268555 0.9553655982017517
CurrentTrain: epoch  1, batch    23 | loss: 6.7303267Losses:  7.361024379730225 0.5694646835327148
CurrentTrain: epoch  1, batch    24 | loss: 7.3610244Losses:  7.038374900817871 0.853850245475769
CurrentTrain: epoch  1, batch    25 | loss: 7.0383749Losses:  6.154751777648926 0.7121874690055847
CurrentTrain: epoch  1, batch    26 | loss: 6.1547518Losses:  6.671814918518066 0.9714862108230591
CurrentTrain: epoch  1, batch    27 | loss: 6.6718149Losses:  7.2887725830078125 0.9554467797279358
CurrentTrain: epoch  1, batch    28 | loss: 7.2887726Losses:  7.8150434494018555 1.0861010551452637
CurrentTrain: epoch  1, batch    29 | loss: 7.8150434Losses:  6.580903053283691 0.9277195334434509
CurrentTrain: epoch  1, batch    30 | loss: 6.5809031Losses:  7.670023441314697 1.0863736867904663
CurrentTrain: epoch  1, batch    31 | loss: 7.6700234Losses:  7.4532470703125 1.0558202266693115
CurrentTrain: epoch  1, batch    32 | loss: 7.4532471Losses:  7.301355838775635 0.9733136892318726
CurrentTrain: epoch  1, batch    33 | loss: 7.3013558Losses:  7.091216564178467 1.0066897869110107
CurrentTrain: epoch  1, batch    34 | loss: 7.0912166Losses:  6.643774032592773 1.0318125486373901
CurrentTrain: epoch  1, batch    35 | loss: 6.6437740Losses:  6.470065593719482 0.7936987280845642
CurrentTrain: epoch  1, batch    36 | loss: 6.4700656Losses:  7.633560657501221 1.1355791091918945
CurrentTrain: epoch  1, batch    37 | loss: 7.6335607Losses:  7.641849040985107 0.8506999611854553
CurrentTrain: epoch  1, batch    38 | loss: 7.6418490Losses:  7.067241668701172 0.7415463328361511
CurrentTrain: epoch  1, batch    39 | loss: 7.0672417Losses:  6.629736423492432 0.9130851626396179
CurrentTrain: epoch  1, batch    40 | loss: 6.6297364Losses:  6.993825912475586 0.777215838432312
CurrentTrain: epoch  1, batch    41 | loss: 6.9938259Losses:  6.385815620422363 0.9296382665634155
CurrentTrain: epoch  1, batch    42 | loss: 6.3858156Losses:  6.854333877563477 0.9876765012741089
CurrentTrain: epoch  1, batch    43 | loss: 6.8543339Losses:  7.149134635925293 1.1397509574890137
CurrentTrain: epoch  1, batch    44 | loss: 7.1491346Losses:  6.466608047485352 0.8846335411071777
CurrentTrain: epoch  1, batch    45 | loss: 6.4666080Losses:  6.305510997772217 0.8631166815757751
CurrentTrain: epoch  1, batch    46 | loss: 6.3055110Losses:  6.830816745758057 1.0085649490356445
CurrentTrain: epoch  1, batch    47 | loss: 6.8308167Losses:  6.776118278503418 0.8541717529296875
CurrentTrain: epoch  1, batch    48 | loss: 6.7761183Losses:  6.892282485961914 1.018136978149414
CurrentTrain: epoch  1, batch    49 | loss: 6.8922825Losses:  6.228948593139648 0.8464869260787964
CurrentTrain: epoch  1, batch    50 | loss: 6.2289486Losses:  6.58294153213501 0.9640928506851196
CurrentTrain: epoch  1, batch    51 | loss: 6.5829415Losses:  5.758736610412598 0.7197260856628418
CurrentTrain: epoch  1, batch    52 | loss: 5.7587366Losses:  5.821670055389404 0.8752428293228149
CurrentTrain: epoch  1, batch    53 | loss: 5.8216701Losses:  6.3326873779296875 0.8422322273254395
CurrentTrain: epoch  1, batch    54 | loss: 6.3326874Losses:  6.388766765594482 0.8237539529800415
CurrentTrain: epoch  1, batch    55 | loss: 6.3887668Losses:  5.849350929260254 0.8769622445106506
CurrentTrain: epoch  1, batch    56 | loss: 5.8493509Losses:  6.312028408050537 1.0382308959960938
CurrentTrain: epoch  1, batch    57 | loss: 6.3120284Losses:  7.381998062133789 0.9740501046180725
CurrentTrain: epoch  1, batch    58 | loss: 7.3819981Losses:  5.778791904449463 0.7232717871665955
CurrentTrain: epoch  1, batch    59 | loss: 5.7787919Losses:  6.568847179412842 0.8032767176628113
CurrentTrain: epoch  1, batch    60 | loss: 6.5688472Losses:  6.276644706726074 0.9060044884681702
CurrentTrain: epoch  1, batch    61 | loss: 6.2766447Losses:  8.85557746887207 1.0937738418579102
CurrentTrain: epoch  1, batch    62 | loss: 8.8555775Losses:  5.581965446472168 0.8038216829299927
CurrentTrain: epoch  2, batch     0 | loss: 5.5819654Losses:  6.829931259155273 0.9332888126373291
CurrentTrain: epoch  2, batch     1 | loss: 6.8299313Losses:  5.676418304443359 0.858640193939209
CurrentTrain: epoch  2, batch     2 | loss: 5.6764183Losses:  5.759762287139893 0.6717089414596558
CurrentTrain: epoch  2, batch     3 | loss: 5.7597623Losses:  5.375960826873779 0.7322701811790466
CurrentTrain: epoch  2, batch     4 | loss: 5.3759608Losses:  5.55383825302124 0.6236532926559448
CurrentTrain: epoch  2, batch     5 | loss: 5.5538383Losses:  7.083369255065918 1.0436186790466309
CurrentTrain: epoch  2, batch     6 | loss: 7.0833693Losses:  6.316350936889648 0.8326992988586426
CurrentTrain: epoch  2, batch     7 | loss: 6.3163509Losses:  5.9998602867126465 0.7704362273216248
CurrentTrain: epoch  2, batch     8 | loss: 5.9998603Losses:  5.482909202575684 0.8630498051643372
CurrentTrain: epoch  2, batch     9 | loss: 5.4829092Losses:  5.918973445892334 0.8804689645767212
CurrentTrain: epoch  2, batch    10 | loss: 5.9189734Losses:  6.532133102416992 0.8129945993423462
CurrentTrain: epoch  2, batch    11 | loss: 6.5321331Losses:  5.669107437133789 0.7121297121047974
CurrentTrain: epoch  2, batch    12 | loss: 5.6691074Losses:  5.728717803955078 0.6073839068412781
CurrentTrain: epoch  2, batch    13 | loss: 5.7287178Losses:  5.551210880279541 0.7101837396621704
CurrentTrain: epoch  2, batch    14 | loss: 5.5512109Losses:  6.2838850021362305 0.8685304522514343
CurrentTrain: epoch  2, batch    15 | loss: 6.2838850Losses:  6.397982597351074 0.901277482509613
CurrentTrain: epoch  2, batch    16 | loss: 6.3979826Losses:  6.053867340087891 0.7147034406661987
CurrentTrain: epoch  2, batch    17 | loss: 6.0538673Losses:  6.078094959259033 0.852637529373169
CurrentTrain: epoch  2, batch    18 | loss: 6.0780950Losses:  6.368168830871582 0.759544849395752
CurrentTrain: epoch  2, batch    19 | loss: 6.3681688Losses:  5.9540910720825195 0.7593151330947876
CurrentTrain: epoch  2, batch    20 | loss: 5.9540911Losses:  5.671180248260498 0.7166181802749634
CurrentTrain: epoch  2, batch    21 | loss: 5.6711802Losses:  5.815088748931885 0.9822419881820679
CurrentTrain: epoch  2, batch    22 | loss: 5.8150887Losses:  5.689966678619385 0.7257088422775269
CurrentTrain: epoch  2, batch    23 | loss: 5.6899667Losses:  6.355170726776123 0.9880014657974243
CurrentTrain: epoch  2, batch    24 | loss: 6.3551707Losses:  5.725705623626709 0.8328666687011719
CurrentTrain: epoch  2, batch    25 | loss: 5.7257056Losses:  6.406655788421631 1.008265733718872
CurrentTrain: epoch  2, batch    26 | loss: 6.4066558Losses:  5.6974406242370605 0.7354761362075806
CurrentTrain: epoch  2, batch    27 | loss: 5.6974406Losses:  5.312900543212891 0.6697314977645874
CurrentTrain: epoch  2, batch    28 | loss: 5.3129005Losses:  5.3786725997924805 0.7719380855560303
CurrentTrain: epoch  2, batch    29 | loss: 5.3786726Losses:  5.985087871551514 0.8413782715797424
CurrentTrain: epoch  2, batch    30 | loss: 5.9850879Losses:  5.935621738433838 0.788280725479126
CurrentTrain: epoch  2, batch    31 | loss: 5.9356217Losses:  6.1409993171691895 0.9270848035812378
CurrentTrain: epoch  2, batch    32 | loss: 6.1409993Losses:  5.830477714538574 0.7522234916687012
CurrentTrain: epoch  2, batch    33 | loss: 5.8304777Losses:  5.686687469482422 0.935289204120636
CurrentTrain: epoch  2, batch    34 | loss: 5.6866875Losses:  4.967336177825928 0.4960971176624298
CurrentTrain: epoch  2, batch    35 | loss: 4.9673362Losses:  6.79232120513916 0.8931971788406372
CurrentTrain: epoch  2, batch    36 | loss: 6.7923212Losses:  6.443633079528809 0.7674775123596191
CurrentTrain: epoch  2, batch    37 | loss: 6.4436331Losses:  5.5748796463012695 0.8820788860321045
CurrentTrain: epoch  2, batch    38 | loss: 5.5748796Losses:  6.273261547088623 0.8415759801864624
CurrentTrain: epoch  2, batch    39 | loss: 6.2732615Losses:  6.530751705169678 0.8471459746360779
CurrentTrain: epoch  2, batch    40 | loss: 6.5307517Losses:  6.155292987823486 0.8041442632675171
CurrentTrain: epoch  2, batch    41 | loss: 6.1552930Losses:  6.1693291664123535 0.693122148513794
CurrentTrain: epoch  2, batch    42 | loss: 6.1693292Losses:  5.683260440826416 0.6206009387969971
CurrentTrain: epoch  2, batch    43 | loss: 5.6832604Losses:  5.838599681854248 0.7330996990203857
CurrentTrain: epoch  2, batch    44 | loss: 5.8385997Losses:  5.875918865203857 0.8787203431129456
CurrentTrain: epoch  2, batch    45 | loss: 5.8759189Losses:  5.466872215270996 0.633802056312561
CurrentTrain: epoch  2, batch    46 | loss: 5.4668722Losses:  5.505608081817627 0.7118409276008606
CurrentTrain: epoch  2, batch    47 | loss: 5.5056081Losses:  5.207739353179932 0.5614238381385803
CurrentTrain: epoch  2, batch    48 | loss: 5.2077394Losses:  6.222073554992676 0.867368757724762
CurrentTrain: epoch  2, batch    49 | loss: 6.2220736Losses:  5.639320373535156 0.8607968688011169
CurrentTrain: epoch  2, batch    50 | loss: 5.6393204Losses:  5.265971660614014 0.56386399269104
CurrentTrain: epoch  2, batch    51 | loss: 5.2659717Losses:  6.00248908996582 0.7579609155654907
CurrentTrain: epoch  2, batch    52 | loss: 6.0024891Losses:  5.1232829093933105 0.7737151384353638
CurrentTrain: epoch  2, batch    53 | loss: 5.1232829Losses:  5.343257904052734 0.5869821310043335
CurrentTrain: epoch  2, batch    54 | loss: 5.3432579Losses:  5.347806930541992 0.7366492748260498
CurrentTrain: epoch  2, batch    55 | loss: 5.3478069Losses:  5.3815789222717285 0.7958247065544128
CurrentTrain: epoch  2, batch    56 | loss: 5.3815789Losses:  5.301381587982178 0.6173316240310669
CurrentTrain: epoch  2, batch    57 | loss: 5.3013816Losses:  5.786015510559082 0.7632768750190735
CurrentTrain: epoch  2, batch    58 | loss: 5.7860155Losses:  5.634253025054932 0.7329875230789185
CurrentTrain: epoch  2, batch    59 | loss: 5.6342530Losses:  5.585074424743652 0.9523486495018005
CurrentTrain: epoch  2, batch    60 | loss: 5.5850744Losses:  5.670024394989014 0.7521279454231262
CurrentTrain: epoch  2, batch    61 | loss: 5.6700244Losses:  5.149801254272461 0.5022071599960327
CurrentTrain: epoch  2, batch    62 | loss: 5.1498013Losses:  5.491824626922607 0.855491042137146
CurrentTrain: epoch  3, batch     0 | loss: 5.4918246Losses:  6.190832614898682 0.7040585279464722
CurrentTrain: epoch  3, batch     1 | loss: 6.1908326Losses:  5.296772003173828 0.760974645614624
CurrentTrain: epoch  3, batch     2 | loss: 5.2967720Losses:  5.681832313537598 0.666746199131012
CurrentTrain: epoch  3, batch     3 | loss: 5.6818323Losses:  5.609999656677246 0.8650320768356323
CurrentTrain: epoch  3, batch     4 | loss: 5.6099997Losses:  5.552659034729004 0.8762434720993042
CurrentTrain: epoch  3, batch     5 | loss: 5.5526590Losses:  5.126337051391602 0.6741615533828735
CurrentTrain: epoch  3, batch     6 | loss: 5.1263371Losses:  4.99959135055542 0.6246234178543091
CurrentTrain: epoch  3, batch     7 | loss: 4.9995914Losses:  5.260016918182373 0.6091039180755615
CurrentTrain: epoch  3, batch     8 | loss: 5.2600169Losses:  5.189845561981201 0.6442590951919556
CurrentTrain: epoch  3, batch     9 | loss: 5.1898456Losses:  5.375027656555176 0.5682035684585571
CurrentTrain: epoch  3, batch    10 | loss: 5.3750277Losses:  5.010563850402832 0.5559625625610352
CurrentTrain: epoch  3, batch    11 | loss: 5.0105639Losses:  5.165836811065674 0.7219340205192566
CurrentTrain: epoch  3, batch    12 | loss: 5.1658368Losses:  5.253965854644775 0.6766126155853271
CurrentTrain: epoch  3, batch    13 | loss: 5.2539659Losses:  5.713648796081543 0.591059148311615
CurrentTrain: epoch  3, batch    14 | loss: 5.7136488Losses:  5.104640007019043 0.6411950588226318
CurrentTrain: epoch  3, batch    15 | loss: 5.1046400Losses:  5.507495880126953 0.799547553062439
CurrentTrain: epoch  3, batch    16 | loss: 5.5074959Losses:  4.953100681304932 0.5495330095291138
CurrentTrain: epoch  3, batch    17 | loss: 4.9531007Losses:  5.102957248687744 0.6440834403038025
CurrentTrain: epoch  3, batch    18 | loss: 5.1029572Losses:  5.250516891479492 0.668039858341217
CurrentTrain: epoch  3, batch    19 | loss: 5.2505169Losses:  4.892716407775879 0.6819440722465515
CurrentTrain: epoch  3, batch    20 | loss: 4.8927164Losses:  5.04127836227417 0.6612141132354736
CurrentTrain: epoch  3, batch    21 | loss: 5.0412784Losses:  4.909511089324951 0.6380741596221924
CurrentTrain: epoch  3, batch    22 | loss: 4.9095111Losses:  5.392594814300537 0.7990716099739075
CurrentTrain: epoch  3, batch    23 | loss: 5.3925948Losses:  5.457295894622803 0.6753366589546204
CurrentTrain: epoch  3, batch    24 | loss: 5.4572959Losses:  5.217245578765869 0.8241477608680725
CurrentTrain: epoch  3, batch    25 | loss: 5.2172456Losses:  5.195966720581055 0.5406284332275391
CurrentTrain: epoch  3, batch    26 | loss: 5.1959667Losses:  4.857513427734375 0.5920438766479492
CurrentTrain: epoch  3, batch    27 | loss: 4.8575134Losses:  5.413382053375244 0.6466003656387329
CurrentTrain: epoch  3, batch    28 | loss: 5.4133821Losses:  4.7231831550598145 0.4863632023334503
CurrentTrain: epoch  3, batch    29 | loss: 4.7231832Losses:  5.363545894622803 0.6851575374603271
CurrentTrain: epoch  3, batch    30 | loss: 5.3635459Losses:  5.084292411804199 0.767876923084259
CurrentTrain: epoch  3, batch    31 | loss: 5.0842924Losses:  5.015836715698242 0.5452555418014526
CurrentTrain: epoch  3, batch    32 | loss: 5.0158367Losses:  4.832812786102295 0.725597620010376
CurrentTrain: epoch  3, batch    33 | loss: 4.8328128Losses:  5.153193473815918 0.6838957071304321
CurrentTrain: epoch  3, batch    34 | loss: 5.1531935Losses:  5.121837615966797 0.6163643598556519
CurrentTrain: epoch  3, batch    35 | loss: 5.1218376Losses:  5.470005512237549 0.6904163956642151
CurrentTrain: epoch  3, batch    36 | loss: 5.4700055Losses:  4.862580299377441 0.6591265201568604
CurrentTrain: epoch  3, batch    37 | loss: 4.8625803Losses:  4.985375881195068 0.6836780309677124
CurrentTrain: epoch  3, batch    38 | loss: 4.9853759Losses:  5.059369087219238 0.5437523722648621
CurrentTrain: epoch  3, batch    39 | loss: 5.0593691Losses:  4.882083415985107 0.49782437086105347
CurrentTrain: epoch  3, batch    40 | loss: 4.8820834Losses:  5.224061012268066 0.5652446746826172
CurrentTrain: epoch  3, batch    41 | loss: 5.2240610Losses:  5.725902557373047 0.8233513832092285
CurrentTrain: epoch  3, batch    42 | loss: 5.7259026Losses:  4.813138961791992 0.6695194840431213
CurrentTrain: epoch  3, batch    43 | loss: 4.8131390Losses:  5.168210506439209 0.5504202246665955
CurrentTrain: epoch  3, batch    44 | loss: 5.1682105Losses:  5.05825138092041 0.5660921931266785
CurrentTrain: epoch  3, batch    45 | loss: 5.0582514Losses:  4.809319972991943 0.5800533294677734
CurrentTrain: epoch  3, batch    46 | loss: 4.8093200Losses:  4.903872013092041 0.6532251834869385
CurrentTrain: epoch  3, batch    47 | loss: 4.9038720Losses:  4.825273036956787 0.45566433668136597
CurrentTrain: epoch  3, batch    48 | loss: 4.8252730Losses:  4.886451721191406 0.5946409702301025
CurrentTrain: epoch  3, batch    49 | loss: 4.8864517Losses:  4.72855281829834 0.4620010256767273
CurrentTrain: epoch  3, batch    50 | loss: 4.7285528Losses:  5.0555572509765625 0.7123969197273254
CurrentTrain: epoch  3, batch    51 | loss: 5.0555573Losses:  4.857865333557129 0.578269898891449
CurrentTrain: epoch  3, batch    52 | loss: 4.8578653Losses:  5.03181266784668 0.5439788699150085
CurrentTrain: epoch  3, batch    53 | loss: 5.0318127Losses:  4.832497596740723 0.602302610874176
CurrentTrain: epoch  3, batch    54 | loss: 4.8324976Losses:  4.97625732421875 0.5565398335456848
CurrentTrain: epoch  3, batch    55 | loss: 4.9762573Losses:  4.913082122802734 0.6701867580413818
CurrentTrain: epoch  3, batch    56 | loss: 4.9130821Losses:  5.79470157623291 0.7914519906044006
CurrentTrain: epoch  3, batch    57 | loss: 5.7947016Losses:  4.585264205932617 0.4364716708660126
CurrentTrain: epoch  3, batch    58 | loss: 4.5852642Losses:  4.950218677520752 0.5278351902961731
CurrentTrain: epoch  3, batch    59 | loss: 4.9502187Losses:  4.853558540344238 0.6757982969284058
CurrentTrain: epoch  3, batch    60 | loss: 4.8535585Losses:  5.655571460723877 0.8032733798027039
CurrentTrain: epoch  3, batch    61 | loss: 5.6555715Losses:  4.7264251708984375 0.37468379735946655
CurrentTrain: epoch  3, batch    62 | loss: 4.7264252Losses:  4.711119174957275 0.6671532392501831
CurrentTrain: epoch  4, batch     0 | loss: 4.7111192Losses:  4.86082124710083 0.5885440111160278
CurrentTrain: epoch  4, batch     1 | loss: 4.8608212Losses:  4.697551727294922 0.6047570705413818
CurrentTrain: epoch  4, batch     2 | loss: 4.6975517Losses:  4.712317943572998 0.5757661461830139
CurrentTrain: epoch  4, batch     3 | loss: 4.7123179Losses:  4.705216407775879 0.49194177985191345
CurrentTrain: epoch  4, batch     4 | loss: 4.7052164Losses:  5.0647873878479 0.5100637674331665
CurrentTrain: epoch  4, batch     5 | loss: 5.0647874Losses:  5.433180809020996 0.6283591985702515
CurrentTrain: epoch  4, batch     6 | loss: 5.4331808Losses:  4.990358352661133 0.5519408583641052
CurrentTrain: epoch  4, batch     7 | loss: 4.9903584Losses:  5.486528396606445 0.6341836452484131
CurrentTrain: epoch  4, batch     8 | loss: 5.4865284Losses:  4.7305216789245605 0.5438220500946045
CurrentTrain: epoch  4, batch     9 | loss: 4.7305217Losses:  4.730985164642334 0.617292582988739
CurrentTrain: epoch  4, batch    10 | loss: 4.7309852Losses:  4.903804302215576 0.5136312246322632
CurrentTrain: epoch  4, batch    11 | loss: 4.9038043Losses:  4.765127182006836 0.46535724401474
CurrentTrain: epoch  4, batch    12 | loss: 4.7651272Losses:  5.620265960693359 0.5724185705184937
CurrentTrain: epoch  4, batch    13 | loss: 5.6202660Losses:  4.8447651863098145 0.7188583016395569
CurrentTrain: epoch  4, batch    14 | loss: 4.8447652Losses:  5.201905727386475 0.6661233901977539
CurrentTrain: epoch  4, batch    15 | loss: 5.2019057Losses:  4.661203384399414 0.5724419355392456
CurrentTrain: epoch  4, batch    16 | loss: 4.6612034Losses:  5.072334289550781 0.5913299918174744
CurrentTrain: epoch  4, batch    17 | loss: 5.0723343Losses:  4.8425068855285645 0.5983152389526367
CurrentTrain: epoch  4, batch    18 | loss: 4.8425069Losses:  4.482208728790283 0.41483810544013977
CurrentTrain: epoch  4, batch    19 | loss: 4.4822087Losses:  5.053527355194092 0.6390466690063477
CurrentTrain: epoch  4, batch    20 | loss: 5.0535274Losses:  4.834756374359131 0.564305305480957
CurrentTrain: epoch  4, batch    21 | loss: 4.8347564Losses:  4.703012466430664 0.5994699001312256
CurrentTrain: epoch  4, batch    22 | loss: 4.7030125Losses:  4.981626987457275 0.745642900466919
CurrentTrain: epoch  4, batch    23 | loss: 4.9816270Losses:  4.623035430908203 0.48651376366615295
CurrentTrain: epoch  4, batch    24 | loss: 4.6230354Losses:  4.668871879577637 0.5582799911499023
CurrentTrain: epoch  4, batch    25 | loss: 4.6688719Losses:  4.695614814758301 0.5522369146347046
CurrentTrain: epoch  4, batch    26 | loss: 4.6956148Losses:  5.149941921234131 0.5906935930252075
CurrentTrain: epoch  4, batch    27 | loss: 5.1499419Losses:  4.836006164550781 0.3862035870552063
CurrentTrain: epoch  4, batch    28 | loss: 4.8360062Losses:  4.783421516418457 0.5756104588508606
CurrentTrain: epoch  4, batch    29 | loss: 4.7834215Losses:  4.6497392654418945 0.3892514407634735
CurrentTrain: epoch  4, batch    30 | loss: 4.6497393Losses:  4.75827693939209 0.6050454378128052
CurrentTrain: epoch  4, batch    31 | loss: 4.7582769Losses:  4.730872631072998 0.4888346195220947
CurrentTrain: epoch  4, batch    32 | loss: 4.7308726Losses:  4.668554782867432 0.4918932616710663
CurrentTrain: epoch  4, batch    33 | loss: 4.6685548Losses:  4.708627700805664 0.5520208477973938
CurrentTrain: epoch  4, batch    34 | loss: 4.7086277Losses:  4.509698390960693 0.46158725023269653
CurrentTrain: epoch  4, batch    35 | loss: 4.5096984Losses:  4.581507205963135 0.4207376539707184
CurrentTrain: epoch  4, batch    36 | loss: 4.5815072Losses:  4.821100234985352 0.6362321376800537
CurrentTrain: epoch  4, batch    37 | loss: 4.8211002Losses:  4.803342819213867 0.6014028787612915
CurrentTrain: epoch  4, batch    38 | loss: 4.8033428Losses:  4.703428268432617 0.5324889421463013
CurrentTrain: epoch  4, batch    39 | loss: 4.7034283Losses:  4.459068298339844 0.5083749890327454
CurrentTrain: epoch  4, batch    40 | loss: 4.4590683Losses:  4.540156364440918 0.3668326735496521
CurrentTrain: epoch  4, batch    41 | loss: 4.5401564Losses:  4.875570774078369 0.5511854290962219
CurrentTrain: epoch  4, batch    42 | loss: 4.8755708Losses:  5.018156051635742 0.6063135266304016
CurrentTrain: epoch  4, batch    43 | loss: 5.0181561Losses:  4.550566673278809 0.4674777090549469
CurrentTrain: epoch  4, batch    44 | loss: 4.5505667Losses:  4.511658191680908 0.5564061403274536
CurrentTrain: epoch  4, batch    45 | loss: 4.5116582Losses:  4.664474010467529 0.5167291760444641
CurrentTrain: epoch  4, batch    46 | loss: 4.6644740Losses:  4.719293594360352 0.6289690732955933
CurrentTrain: epoch  4, batch    47 | loss: 4.7192936Losses:  4.83539342880249 0.549574613571167
CurrentTrain: epoch  4, batch    48 | loss: 4.8353934Losses:  4.953901767730713 0.69481360912323
CurrentTrain: epoch  4, batch    49 | loss: 4.9539018Losses:  4.671459197998047 0.5574770569801331
CurrentTrain: epoch  4, batch    50 | loss: 4.6714592Losses:  4.8397440910339355 0.48440462350845337
CurrentTrain: epoch  4, batch    51 | loss: 4.8397441Losses:  4.730612754821777 0.572643518447876
CurrentTrain: epoch  4, batch    52 | loss: 4.7306128Losses:  4.668705463409424 0.5706408023834229
CurrentTrain: epoch  4, batch    53 | loss: 4.6687055Losses:  4.6019511222839355 0.5283013582229614
CurrentTrain: epoch  4, batch    54 | loss: 4.6019511Losses:  4.764366149902344 0.5889562368392944
CurrentTrain: epoch  4, batch    55 | loss: 4.7643661Losses:  4.815940856933594 0.43522486090660095
CurrentTrain: epoch  4, batch    56 | loss: 4.8159409Losses:  5.559934139251709 0.43421679735183716
CurrentTrain: epoch  4, batch    57 | loss: 5.5599341Losses:  4.490570545196533 0.5120575428009033
CurrentTrain: epoch  4, batch    58 | loss: 4.4905705Losses:  4.6781840324401855 0.5072103142738342
CurrentTrain: epoch  4, batch    59 | loss: 4.6781840Losses:  4.418878078460693 0.331332266330719
CurrentTrain: epoch  4, batch    60 | loss: 4.4188781Losses:  4.827643394470215 0.5785850286483765
CurrentTrain: epoch  4, batch    61 | loss: 4.8276434Losses:  4.786288261413574 0.45385318994522095
CurrentTrain: epoch  4, batch    62 | loss: 4.7862883Losses:  4.683574199676514 0.37210121750831604
CurrentTrain: epoch  5, batch     0 | loss: 4.6835742Losses:  4.503667831420898 0.41180598735809326
CurrentTrain: epoch  5, batch     1 | loss: 4.5036678Losses:  4.960890769958496 0.5312180519104004
CurrentTrain: epoch  5, batch     2 | loss: 4.9608908Losses:  4.560216903686523 0.5094513893127441
CurrentTrain: epoch  5, batch     3 | loss: 4.5602169Losses:  4.439757823944092 0.47319307923316956
CurrentTrain: epoch  5, batch     4 | loss: 4.4397578Losses:  4.554018497467041 0.4663018584251404
CurrentTrain: epoch  5, batch     5 | loss: 4.5540185Losses:  4.451119422912598 0.3959764242172241
CurrentTrain: epoch  5, batch     6 | loss: 4.4511194Losses:  4.77388858795166 0.4326767921447754
CurrentTrain: epoch  5, batch     7 | loss: 4.7738886Losses:  4.495948791503906 0.4696083962917328
CurrentTrain: epoch  5, batch     8 | loss: 4.4959488Losses:  4.956146717071533 0.5257138013839722
CurrentTrain: epoch  5, batch     9 | loss: 4.9561467Losses:  4.463547706604004 0.5100585222244263
CurrentTrain: epoch  5, batch    10 | loss: 4.4635477Losses:  4.480138778686523 0.49689584970474243
CurrentTrain: epoch  5, batch    11 | loss: 4.4801388Losses:  4.5003275871276855 0.48997247219085693
CurrentTrain: epoch  5, batch    12 | loss: 4.5003276Losses:  4.455663204193115 0.48048490285873413
CurrentTrain: epoch  5, batch    13 | loss: 4.4556632Losses:  4.576352596282959 0.5914608836174011
CurrentTrain: epoch  5, batch    14 | loss: 4.5763526Losses:  4.5756916999816895 0.4718310236930847
CurrentTrain: epoch  5, batch    15 | loss: 4.5756917Losses:  4.550226211547852 0.4565700888633728
CurrentTrain: epoch  5, batch    16 | loss: 4.5502262Losses:  4.862256050109863 0.47560837864875793
CurrentTrain: epoch  5, batch    17 | loss: 4.8622561Losses:  4.4859161376953125 0.47540563344955444
CurrentTrain: epoch  5, batch    18 | loss: 4.4859161Losses:  4.73154878616333 0.4562835693359375
CurrentTrain: epoch  5, batch    19 | loss: 4.7315488Losses:  4.252610683441162 0.2687162756919861
CurrentTrain: epoch  5, batch    20 | loss: 4.2526107Losses:  4.984776020050049 0.5791881084442139
CurrentTrain: epoch  5, batch    21 | loss: 4.9847760Losses:  5.0582075119018555 0.4856858253479004
CurrentTrain: epoch  5, batch    22 | loss: 5.0582075Losses:  5.0650224685668945 0.6606435179710388
CurrentTrain: epoch  5, batch    23 | loss: 5.0650225Losses:  4.550113201141357 0.46038538217544556
CurrentTrain: epoch  5, batch    24 | loss: 4.5501132Losses:  4.9090657234191895 0.31402915716171265
CurrentTrain: epoch  5, batch    25 | loss: 4.9090657Losses:  4.445065975189209 0.47384941577911377
CurrentTrain: epoch  5, batch    26 | loss: 4.4450660Losses:  4.522782325744629 0.5678857564926147
CurrentTrain: epoch  5, batch    27 | loss: 4.5227823Losses:  4.347239971160889 0.4384208917617798
CurrentTrain: epoch  5, batch    28 | loss: 4.3472400Losses:  4.53438663482666 0.516525149345398
CurrentTrain: epoch  5, batch    29 | loss: 4.5343866Losses:  4.748729228973389 0.42930328845977783
CurrentTrain: epoch  5, batch    30 | loss: 4.7487292Losses:  4.468185901641846 0.5215880870819092
CurrentTrain: epoch  5, batch    31 | loss: 4.4681859Losses:  5.427565097808838 0.44622743129730225
CurrentTrain: epoch  5, batch    32 | loss: 5.4275651Losses:  4.393359184265137 0.4620378017425537
CurrentTrain: epoch  5, batch    33 | loss: 4.3933592Losses:  4.381242275238037 0.4568309783935547
CurrentTrain: epoch  5, batch    34 | loss: 4.3812423Losses:  4.846844673156738 0.48344993591308594
CurrentTrain: epoch  5, batch    35 | loss: 4.8468447Losses:  4.495424270629883 0.5129624605178833
CurrentTrain: epoch  5, batch    36 | loss: 4.4954243Losses:  4.397218704223633 0.4759206473827362
CurrentTrain: epoch  5, batch    37 | loss: 4.3972187Losses:  4.787074089050293 0.4344467520713806
CurrentTrain: epoch  5, batch    38 | loss: 4.7870741Losses:  4.415107250213623 0.39919865131378174
CurrentTrain: epoch  5, batch    39 | loss: 4.4151073Losses:  4.407766819000244 0.3489767909049988
CurrentTrain: epoch  5, batch    40 | loss: 4.4077668Losses:  4.421701431274414 0.45156699419021606
CurrentTrain: epoch  5, batch    41 | loss: 4.4217014Losses:  4.453495025634766 0.4359365999698639
CurrentTrain: epoch  5, batch    42 | loss: 4.4534950Losses:  4.4356160163879395 0.4884057939052582
CurrentTrain: epoch  5, batch    43 | loss: 4.4356160Losses:  4.748358726501465 0.5001296997070312
CurrentTrain: epoch  5, batch    44 | loss: 4.7483587Losses:  4.597867012023926 0.5123224258422852
CurrentTrain: epoch  5, batch    45 | loss: 4.5978670Losses:  4.588601589202881 0.4234721064567566
CurrentTrain: epoch  5, batch    46 | loss: 4.5886016Losses:  4.555473804473877 0.4860086739063263
CurrentTrain: epoch  5, batch    47 | loss: 4.5554738Losses:  4.397988796234131 0.41252514719963074
CurrentTrain: epoch  5, batch    48 | loss: 4.3979888Losses:  4.450023174285889 0.5200660824775696
CurrentTrain: epoch  5, batch    49 | loss: 4.4500232Losses:  4.9099578857421875 0.5628800392150879
CurrentTrain: epoch  5, batch    50 | loss: 4.9099579Losses:  4.334941864013672 0.24166089296340942
CurrentTrain: epoch  5, batch    51 | loss: 4.3349419Losses:  4.4387617111206055 0.3821580410003662
CurrentTrain: epoch  5, batch    52 | loss: 4.4387617Losses:  4.6492695808410645 0.5669991374015808
CurrentTrain: epoch  5, batch    53 | loss: 4.6492696Losses:  4.4143805503845215 0.4884130358695984
CurrentTrain: epoch  5, batch    54 | loss: 4.4143806Losses:  4.558987140655518 0.4920988976955414
CurrentTrain: epoch  5, batch    55 | loss: 4.5589871Losses:  4.4556074142456055 0.4630332589149475
CurrentTrain: epoch  5, batch    56 | loss: 4.4556074Losses:  4.967913627624512 0.46581435203552246
CurrentTrain: epoch  5, batch    57 | loss: 4.9679136Losses:  4.401432991027832 0.36449354887008667
CurrentTrain: epoch  5, batch    58 | loss: 4.4014330Losses:  4.343905448913574 0.34058964252471924
CurrentTrain: epoch  5, batch    59 | loss: 4.3439054Losses:  4.65140438079834 0.43020549416542053
CurrentTrain: epoch  5, batch    60 | loss: 4.6514044Losses:  4.382449626922607 0.31256213784217834
CurrentTrain: epoch  5, batch    61 | loss: 4.3824496Losses:  4.385310649871826 0.259653776884079
CurrentTrain: epoch  5, batch    62 | loss: 4.3853106Losses:  4.360874652862549 0.43638116121292114
CurrentTrain: epoch  6, batch     0 | loss: 4.3608747Losses:  4.447719573974609 0.4288477897644043
CurrentTrain: epoch  6, batch     1 | loss: 4.4477196Losses:  4.418280601501465 0.533953070640564
CurrentTrain: epoch  6, batch     2 | loss: 4.4182806Losses:  4.3697333335876465 0.40067219734191895
CurrentTrain: epoch  6, batch     3 | loss: 4.3697333Losses:  4.510629177093506 0.461441308259964
CurrentTrain: epoch  6, batch     4 | loss: 4.5106292Losses:  4.39987850189209 0.4364311099052429
CurrentTrain: epoch  6, batch     5 | loss: 4.3998785Losses:  4.527433395385742 0.509130597114563
CurrentTrain: epoch  6, batch     6 | loss: 4.5274334Losses:  4.454960346221924 0.4883004426956177
CurrentTrain: epoch  6, batch     7 | loss: 4.4549603Losses:  4.3721771240234375 0.343229204416275
CurrentTrain: epoch  6, batch     8 | loss: 4.3721771Losses:  4.667559623718262 0.37050867080688477
CurrentTrain: epoch  6, batch     9 | loss: 4.6675596Losses:  4.267305374145508 0.36169329285621643
CurrentTrain: epoch  6, batch    10 | loss: 4.2673054Losses:  4.348879814147949 0.4442843198776245
CurrentTrain: epoch  6, batch    11 | loss: 4.3488798Losses:  4.4648051261901855 0.5180282592773438
CurrentTrain: epoch  6, batch    12 | loss: 4.4648051Losses:  4.457602500915527 0.5278488993644714
CurrentTrain: epoch  6, batch    13 | loss: 4.4576025Losses:  4.3668975830078125 0.41027238965034485
CurrentTrain: epoch  6, batch    14 | loss: 4.3668976Losses:  4.322501182556152 0.38588178157806396
CurrentTrain: epoch  6, batch    15 | loss: 4.3225012Losses:  4.45886754989624 0.47749394178390503
CurrentTrain: epoch  6, batch    16 | loss: 4.4588675Losses:  4.285271167755127 0.3923303484916687
CurrentTrain: epoch  6, batch    17 | loss: 4.2852712Losses:  4.303555488586426 0.46628400683403015
CurrentTrain: epoch  6, batch    18 | loss: 4.3035555Losses:  4.492842197418213 0.4793083667755127
CurrentTrain: epoch  6, batch    19 | loss: 4.4928422Losses:  4.387824535369873 0.40837007761001587
CurrentTrain: epoch  6, batch    20 | loss: 4.3878245Losses:  4.318491458892822 0.3102058470249176
CurrentTrain: epoch  6, batch    21 | loss: 4.3184915Losses:  4.424574375152588 0.412502259016037
CurrentTrain: epoch  6, batch    22 | loss: 4.4245744Losses:  4.299449443817139 0.3899686634540558
CurrentTrain: epoch  6, batch    23 | loss: 4.2994494Losses:  4.308333396911621 0.39053964614868164
CurrentTrain: epoch  6, batch    24 | loss: 4.3083334Losses:  4.207734107971191 0.29065823554992676
CurrentTrain: epoch  6, batch    25 | loss: 4.2077341Losses:  4.346996307373047 0.39243292808532715
CurrentTrain: epoch  6, batch    26 | loss: 4.3469963Losses:  4.969093322753906 0.4567297697067261
CurrentTrain: epoch  6, batch    27 | loss: 4.9690933Losses:  4.3164238929748535 0.30727070569992065
CurrentTrain: epoch  6, batch    28 | loss: 4.3164239Losses:  4.373039722442627 0.3721427917480469
CurrentTrain: epoch  6, batch    29 | loss: 4.3730397Losses:  4.429827690124512 0.5215549468994141
CurrentTrain: epoch  6, batch    30 | loss: 4.4298277Losses:  4.353309631347656 0.25007155537605286
CurrentTrain: epoch  6, batch    31 | loss: 4.3533096Losses:  4.289194583892822 0.4144275486469269
CurrentTrain: epoch  6, batch    32 | loss: 4.2891946Losses:  4.425379753112793 0.437884658575058
CurrentTrain: epoch  6, batch    33 | loss: 4.4253798Losses:  4.443948745727539 0.38655567169189453
CurrentTrain: epoch  6, batch    34 | loss: 4.4439487Losses:  4.264561653137207 0.3565906882286072
CurrentTrain: epoch  6, batch    35 | loss: 4.2645617Losses:  4.432854652404785 0.4355069398880005
CurrentTrain: epoch  6, batch    36 | loss: 4.4328547Losses:  4.434506416320801 0.47293829917907715
CurrentTrain: epoch  6, batch    37 | loss: 4.4345064Losses:  4.31234884262085 0.25649160146713257
CurrentTrain: epoch  6, batch    38 | loss: 4.3123488Losses:  4.308897972106934 0.3652347922325134
CurrentTrain: epoch  6, batch    39 | loss: 4.3088980Losses:  4.40939474105835 0.3691226840019226
CurrentTrain: epoch  6, batch    40 | loss: 4.4093947Losses:  4.358217239379883 0.43918609619140625
CurrentTrain: epoch  6, batch    41 | loss: 4.3582172Losses:  4.399469375610352 0.47995203733444214
CurrentTrain: epoch  6, batch    42 | loss: 4.3994694Losses:  4.378077983856201 0.3814062476158142
CurrentTrain: epoch  6, batch    43 | loss: 4.3780780Losses:  4.224186897277832 0.20348966121673584
CurrentTrain: epoch  6, batch    44 | loss: 4.2241869Losses:  4.395310878753662 0.42327386140823364
CurrentTrain: epoch  6, batch    45 | loss: 4.3953109Losses:  4.466975212097168 0.4202219843864441
CurrentTrain: epoch  6, batch    46 | loss: 4.4669752Losses:  4.421544075012207 0.38047417998313904
CurrentTrain: epoch  6, batch    47 | loss: 4.4215441Losses:  4.447942733764648 0.4668881595134735
CurrentTrain: epoch  6, batch    48 | loss: 4.4479427Losses:  4.294371604919434 0.2819649875164032
CurrentTrain: epoch  6, batch    49 | loss: 4.2943716Losses:  4.436417579650879 0.42717745900154114
CurrentTrain: epoch  6, batch    50 | loss: 4.4364176Losses:  4.221386909484863 0.34995612502098083
CurrentTrain: epoch  6, batch    51 | loss: 4.2213869Losses:  4.426372528076172 0.4608241319656372
CurrentTrain: epoch  6, batch    52 | loss: 4.4263725Losses:  4.28991174697876 0.4167432188987732
CurrentTrain: epoch  6, batch    53 | loss: 4.2899117Losses:  4.332529067993164 0.4088347554206848
CurrentTrain: epoch  6, batch    54 | loss: 4.3325291Losses:  4.39341402053833 0.37545087933540344
CurrentTrain: epoch  6, batch    55 | loss: 4.3934140Losses:  4.290618896484375 0.3698606491088867
CurrentTrain: epoch  6, batch    56 | loss: 4.2906189Losses:  4.367299556732178 0.3475329279899597
CurrentTrain: epoch  6, batch    57 | loss: 4.3672996Losses:  4.227673053741455 0.35277071595191956
CurrentTrain: epoch  6, batch    58 | loss: 4.2276731Losses:  4.315985202789307 0.38975435495376587
CurrentTrain: epoch  6, batch    59 | loss: 4.3159852Losses:  4.2566375732421875 0.349600613117218
CurrentTrain: epoch  6, batch    60 | loss: 4.2566376Losses:  4.367366313934326 0.4572974443435669
CurrentTrain: epoch  6, batch    61 | loss: 4.3673663Losses:  4.323204040527344 0.34614163637161255
CurrentTrain: epoch  6, batch    62 | loss: 4.3232040Losses:  4.243475914001465 0.367485374212265
CurrentTrain: epoch  7, batch     0 | loss: 4.2434759Losses:  4.267829418182373 0.3131020963191986
CurrentTrain: epoch  7, batch     1 | loss: 4.2678294Losses:  4.217988014221191 0.2575310468673706
CurrentTrain: epoch  7, batch     2 | loss: 4.2179880Losses:  4.305877208709717 0.3161191940307617
CurrentTrain: epoch  7, batch     3 | loss: 4.3058772Losses:  4.346841335296631 0.3845217823982239
CurrentTrain: epoch  7, batch     4 | loss: 4.3468413Losses:  4.223531723022461 0.2884039878845215
CurrentTrain: epoch  7, batch     5 | loss: 4.2235317Losses:  4.313439846038818 0.41257578134536743
CurrentTrain: epoch  7, batch     6 | loss: 4.3134398Losses:  4.338170528411865 0.375630259513855
CurrentTrain: epoch  7, batch     7 | loss: 4.3381705Losses:  4.36582612991333 0.39419057965278625
CurrentTrain: epoch  7, batch     8 | loss: 4.3658261Losses:  4.312080383300781 0.3097584843635559
CurrentTrain: epoch  7, batch     9 | loss: 4.3120804Losses:  4.223604679107666 0.29000478982925415
CurrentTrain: epoch  7, batch    10 | loss: 4.2236047Losses:  4.304287433624268 0.4579915404319763
CurrentTrain: epoch  7, batch    11 | loss: 4.3042874Losses:  4.396636009216309 0.2817554473876953
CurrentTrain: epoch  7, batch    12 | loss: 4.3966360Losses:  4.355465888977051 0.3892571032047272
CurrentTrain: epoch  7, batch    13 | loss: 4.3554659Losses:  4.2793707847595215 0.40169277787208557
CurrentTrain: epoch  7, batch    14 | loss: 4.2793708Losses:  4.273338317871094 0.3446410298347473
CurrentTrain: epoch  7, batch    15 | loss: 4.2733383Losses:  4.173494815826416 0.2508833110332489
CurrentTrain: epoch  7, batch    16 | loss: 4.1734948Losses:  4.262154579162598 0.3414687514305115
CurrentTrain: epoch  7, batch    17 | loss: 4.2621546Losses:  4.338573455810547 0.38500818610191345
CurrentTrain: epoch  7, batch    18 | loss: 4.3385735Losses:  4.306502342224121 0.41962504386901855
CurrentTrain: epoch  7, batch    19 | loss: 4.3065023Losses:  4.272398471832275 0.3562593460083008
CurrentTrain: epoch  7, batch    20 | loss: 4.2723985Losses:  4.267940044403076 0.3519294261932373
CurrentTrain: epoch  7, batch    21 | loss: 4.2679400Losses:  4.30006217956543 0.373712956905365
CurrentTrain: epoch  7, batch    22 | loss: 4.3000622Losses:  4.170936107635498 0.2669324278831482
CurrentTrain: epoch  7, batch    23 | loss: 4.1709361Losses:  4.284640312194824 0.3622591495513916
CurrentTrain: epoch  7, batch    24 | loss: 4.2846403Losses:  4.269654273986816 0.3300156593322754
CurrentTrain: epoch  7, batch    25 | loss: 4.2696543Losses:  4.331289768218994 0.418257474899292
CurrentTrain: epoch  7, batch    26 | loss: 4.3312898Losses:  4.316102027893066 0.2902100086212158
CurrentTrain: epoch  7, batch    27 | loss: 4.3161020Losses:  4.294169902801514 0.3531058430671692
CurrentTrain: epoch  7, batch    28 | loss: 4.2941699Losses:  4.237745761871338 0.27482154965400696
CurrentTrain: epoch  7, batch    29 | loss: 4.2377458Losses:  4.401256561279297 0.38346242904663086
CurrentTrain: epoch  7, batch    30 | loss: 4.4012566Losses:  4.279018878936768 0.3337666094303131
CurrentTrain: epoch  7, batch    31 | loss: 4.2790189Losses:  4.285029888153076 0.32046762108802795
CurrentTrain: epoch  7, batch    32 | loss: 4.2850299Losses:  4.2657880783081055 0.403339147567749
CurrentTrain: epoch  7, batch    33 | loss: 4.2657881Losses:  4.289053440093994 0.39977288246154785
CurrentTrain: epoch  7, batch    34 | loss: 4.2890534Losses:  4.294897556304932 0.309120774269104
CurrentTrain: epoch  7, batch    35 | loss: 4.2948976Losses:  4.242997646331787 0.3152626156806946
CurrentTrain: epoch  7, batch    36 | loss: 4.2429976Losses:  4.275069236755371 0.3330093026161194
CurrentTrain: epoch  7, batch    37 | loss: 4.2750692Losses:  4.184213161468506 0.30064547061920166
CurrentTrain: epoch  7, batch    38 | loss: 4.1842132Losses:  4.253943920135498 0.36354079842567444
CurrentTrain: epoch  7, batch    39 | loss: 4.2539439Losses:  4.315099716186523 0.37285763025283813
CurrentTrain: epoch  7, batch    40 | loss: 4.3150997Losses:  4.255614280700684 0.312287837266922
CurrentTrain: epoch  7, batch    41 | loss: 4.2556143Losses:  4.231301307678223 0.33027034997940063
CurrentTrain: epoch  7, batch    42 | loss: 4.2313013Losses:  4.286812782287598 0.3934086859226227
CurrentTrain: epoch  7, batch    43 | loss: 4.2868128Losses:  4.248106956481934 0.37321144342422485
CurrentTrain: epoch  7, batch    44 | loss: 4.2481070Losses:  4.245980262756348 0.3537536561489105
CurrentTrain: epoch  7, batch    45 | loss: 4.2459803Losses:  4.21734094619751 0.28474926948547363
CurrentTrain: epoch  7, batch    46 | loss: 4.2173409Losses:  4.228555679321289 0.31753167510032654
CurrentTrain: epoch  7, batch    47 | loss: 4.2285557Losses:  4.241559982299805 0.35835540294647217
CurrentTrain: epoch  7, batch    48 | loss: 4.2415600Losses:  4.344985485076904 0.39097023010253906
CurrentTrain: epoch  7, batch    49 | loss: 4.3449855Losses:  4.255640029907227 0.3801731467247009
CurrentTrain: epoch  7, batch    50 | loss: 4.2556400Losses:  4.18057918548584 0.3064500093460083
CurrentTrain: epoch  7, batch    51 | loss: 4.1805792Losses:  4.263643741607666 0.3922593593597412
CurrentTrain: epoch  7, batch    52 | loss: 4.2636437Losses:  4.217159271240234 0.306634783744812
CurrentTrain: epoch  7, batch    53 | loss: 4.2171593Losses:  4.264265060424805 0.3743099272251129
CurrentTrain: epoch  7, batch    54 | loss: 4.2642651Losses:  4.269680023193359 0.4248943328857422
CurrentTrain: epoch  7, batch    55 | loss: 4.2696800Losses:  4.240499973297119 0.3331957459449768
CurrentTrain: epoch  7, batch    56 | loss: 4.2405000Losses:  4.2143168449401855 0.31800514459609985
CurrentTrain: epoch  7, batch    57 | loss: 4.2143168Losses:  4.254140853881836 0.3717856705188751
CurrentTrain: epoch  7, batch    58 | loss: 4.2541409Losses:  4.225369453430176 0.33940649032592773
CurrentTrain: epoch  7, batch    59 | loss: 4.2253695Losses:  4.198986530303955 0.3250223398208618
CurrentTrain: epoch  7, batch    60 | loss: 4.1989865Losses:  4.269110202789307 0.3518858551979065
CurrentTrain: epoch  7, batch    61 | loss: 4.2691102Losses:  4.0857462882995605 0.15574786067008972
CurrentTrain: epoch  7, batch    62 | loss: 4.0857463Losses:  4.231522083282471 0.33947861194610596
CurrentTrain: epoch  8, batch     0 | loss: 4.2315221Losses:  4.2080302238464355 0.3177731931209564
CurrentTrain: epoch  8, batch     1 | loss: 4.2080302Losses:  4.172399044036865 0.3046112060546875
CurrentTrain: epoch  8, batch     2 | loss: 4.1723990Losses:  4.287540912628174 0.3974292278289795
CurrentTrain: epoch  8, batch     3 | loss: 4.2875409Losses:  4.258473873138428 0.32483384013175964
CurrentTrain: epoch  8, batch     4 | loss: 4.2584739Losses:  4.208001613616943 0.3333290219306946
CurrentTrain: epoch  8, batch     5 | loss: 4.2080016Losses:  4.1797404289245605 0.26025789976119995
CurrentTrain: epoch  8, batch     6 | loss: 4.1797404Losses:  4.200437545776367 0.3442322015762329
CurrentTrain: epoch  8, batch     7 | loss: 4.2004375Losses:  4.295461654663086 0.44646745920181274
CurrentTrain: epoch  8, batch     8 | loss: 4.2954617Losses:  4.214843273162842 0.3245031237602234
CurrentTrain: epoch  8, batch     9 | loss: 4.2148433Losses:  4.166861057281494 0.25923484563827515
CurrentTrain: epoch  8, batch    10 | loss: 4.1668611Losses:  4.2465667724609375 0.3097860813140869
CurrentTrain: epoch  8, batch    11 | loss: 4.2465668Losses:  4.226011276245117 0.32802292704582214
CurrentTrain: epoch  8, batch    12 | loss: 4.2260113Losses:  4.265177249908447 0.418429970741272
CurrentTrain: epoch  8, batch    13 | loss: 4.2651772Losses:  4.164292335510254 0.2970714271068573
CurrentTrain: epoch  8, batch    14 | loss: 4.1642923Losses:  4.155458450317383 0.23167622089385986
CurrentTrain: epoch  8, batch    15 | loss: 4.1554585Losses:  4.109394550323486 0.2365494817495346
CurrentTrain: epoch  8, batch    16 | loss: 4.1093946Losses:  4.152721405029297 0.3581830561161041
CurrentTrain: epoch  8, batch    17 | loss: 4.1527214Losses:  4.157095909118652 0.29084569215774536
CurrentTrain: epoch  8, batch    18 | loss: 4.1570959Losses:  4.175220489501953 0.3308807611465454
CurrentTrain: epoch  8, batch    19 | loss: 4.1752205Losses:  4.211791515350342 0.31366389989852905
CurrentTrain: epoch  8, batch    20 | loss: 4.2117915Losses:  4.193610191345215 0.3330724835395813
CurrentTrain: epoch  8, batch    21 | loss: 4.1936102Losses:  4.231724262237549 0.3381290137767792
CurrentTrain: epoch  8, batch    22 | loss: 4.2317243Losses:  4.174515247344971 0.30597174167633057
CurrentTrain: epoch  8, batch    23 | loss: 4.1745152Losses:  4.230336666107178 0.36490875482559204
CurrentTrain: epoch  8, batch    24 | loss: 4.2303367Losses:  4.2101731300354 0.30026188492774963
CurrentTrain: epoch  8, batch    25 | loss: 4.2101731Losses:  4.149242877960205 0.31195271015167236
CurrentTrain: epoch  8, batch    26 | loss: 4.1492429Losses:  4.1731276512146 0.3151366412639618
CurrentTrain: epoch  8, batch    27 | loss: 4.1731277Losses:  4.21488618850708 0.3925696611404419
CurrentTrain: epoch  8, batch    28 | loss: 4.2148862Losses:  4.095770359039307 0.2459379881620407
CurrentTrain: epoch  8, batch    29 | loss: 4.0957704Losses:  4.211225986480713 0.36237281560897827
CurrentTrain: epoch  8, batch    30 | loss: 4.2112260Losses:  4.163321018218994 0.24854335188865662
CurrentTrain: epoch  8, batch    31 | loss: 4.1633210Losses:  4.150694370269775 0.2656562328338623
CurrentTrain: epoch  8, batch    32 | loss: 4.1506944Losses:  4.2202630043029785 0.317942351102829
CurrentTrain: epoch  8, batch    33 | loss: 4.2202630Losses:  4.177057266235352 0.2773668169975281
CurrentTrain: epoch  8, batch    34 | loss: 4.1770573Losses:  4.1784515380859375 0.30901971459388733
CurrentTrain: epoch  8, batch    35 | loss: 4.1784515Losses:  4.207211494445801 0.35012078285217285
CurrentTrain: epoch  8, batch    36 | loss: 4.2072115Losses:  4.165065288543701 0.2974957227706909
CurrentTrain: epoch  8, batch    37 | loss: 4.1650653Losses:  4.147924423217773 0.2992613911628723
CurrentTrain: epoch  8, batch    38 | loss: 4.1479244Losses:  4.148212432861328 0.31448671221733093
CurrentTrain: epoch  8, batch    39 | loss: 4.1482124Losses:  4.15568733215332 0.3140701353549957
CurrentTrain: epoch  8, batch    40 | loss: 4.1556873Losses:  4.142351150512695 0.2525159418582916
CurrentTrain: epoch  8, batch    41 | loss: 4.1423512Losses:  4.213702201843262 0.3138885796070099
CurrentTrain: epoch  8, batch    42 | loss: 4.2137022Losses:  4.201958656311035 0.32967132329940796
CurrentTrain: epoch  8, batch    43 | loss: 4.2019587Losses:  4.149367809295654 0.28631430864334106
CurrentTrain: epoch  8, batch    44 | loss: 4.1493678Losses:  4.243724346160889 0.42035746574401855
CurrentTrain: epoch  8, batch    45 | loss: 4.2437243Losses:  4.144692420959473 0.3049471974372864
CurrentTrain: epoch  8, batch    46 | loss: 4.1446924Losses:  4.173560619354248 0.34122467041015625
CurrentTrain: epoch  8, batch    47 | loss: 4.1735606Losses:  4.196888446807861 0.3756808638572693
CurrentTrain: epoch  8, batch    48 | loss: 4.1968884Losses:  4.17681884765625 0.34614166617393494
CurrentTrain: epoch  8, batch    49 | loss: 4.1768188Losses:  4.159274101257324 0.29261842370033264
CurrentTrain: epoch  8, batch    50 | loss: 4.1592741Losses:  4.181413650512695 0.24783508479595184
CurrentTrain: epoch  8, batch    51 | loss: 4.1814137Losses:  4.132071495056152 0.27386474609375
CurrentTrain: epoch  8, batch    52 | loss: 4.1320715Losses:  4.191995620727539 0.3042469024658203
CurrentTrain: epoch  8, batch    53 | loss: 4.1919956Losses:  4.185758113861084 0.30614352226257324
CurrentTrain: epoch  8, batch    54 | loss: 4.1857581Losses:  4.158139228820801 0.2657756805419922
CurrentTrain: epoch  8, batch    55 | loss: 4.1581392Losses:  4.179264545440674 0.28251904249191284
CurrentTrain: epoch  8, batch    56 | loss: 4.1792645Losses:  4.1948323249816895 0.281829297542572
CurrentTrain: epoch  8, batch    57 | loss: 4.1948323Losses:  4.13210391998291 0.31697559356689453
CurrentTrain: epoch  8, batch    58 | loss: 4.1321039Losses:  4.126116752624512 0.23862534761428833
CurrentTrain: epoch  8, batch    59 | loss: 4.1261168Losses:  4.222684860229492 0.37498706579208374
CurrentTrain: epoch  8, batch    60 | loss: 4.2226849Losses:  4.1845383644104 0.36105406284332275
CurrentTrain: epoch  8, batch    61 | loss: 4.1845384Losses:  4.1442060470581055 0.2979280352592468
CurrentTrain: epoch  8, batch    62 | loss: 4.1442060Losses:  4.242580890655518 0.3298274576663971
CurrentTrain: epoch  9, batch     0 | loss: 4.2425809Losses:  4.218853950500488 0.36172857880592346
CurrentTrain: epoch  9, batch     1 | loss: 4.2188540Losses:  4.184338569641113 0.36126869916915894
CurrentTrain: epoch  9, batch     2 | loss: 4.1843386Losses:  4.1834282875061035 0.32263869047164917
CurrentTrain: epoch  9, batch     3 | loss: 4.1834283Losses:  4.262116432189941 0.3533252477645874
CurrentTrain: epoch  9, batch     4 | loss: 4.2621164Losses:  4.1732587814331055 0.2514725923538208
CurrentTrain: epoch  9, batch     5 | loss: 4.1732588Losses:  4.194725036621094 0.31103748083114624
CurrentTrain: epoch  9, batch     6 | loss: 4.1947250Losses:  4.334063529968262 0.3334823250770569
CurrentTrain: epoch  9, batch     7 | loss: 4.3340635Losses:  4.194812774658203 0.31405144929885864
CurrentTrain: epoch  9, batch     8 | loss: 4.1948128Losses:  4.142883777618408 0.3262002170085907
CurrentTrain: epoch  9, batch     9 | loss: 4.1428838Losses:  4.130833625793457 0.2662357687950134
CurrentTrain: epoch  9, batch    10 | loss: 4.1308336Losses:  4.185874938964844 0.25126829743385315
CurrentTrain: epoch  9, batch    11 | loss: 4.1858749Losses:  4.124621391296387 0.19235485792160034
CurrentTrain: epoch  9, batch    12 | loss: 4.1246214Losses:  4.192196369171143 0.2860996425151825
CurrentTrain: epoch  9, batch    13 | loss: 4.1921964Losses:  4.143822193145752 0.2968863248825073
CurrentTrain: epoch  9, batch    14 | loss: 4.1438222Losses:  4.078138828277588 0.21537260711193085
CurrentTrain: epoch  9, batch    15 | loss: 4.0781388Losses:  4.175987243652344 0.30376195907592773
CurrentTrain: epoch  9, batch    16 | loss: 4.1759872Losses:  4.154707908630371 0.27740478515625
CurrentTrain: epoch  9, batch    17 | loss: 4.1547079Losses:  4.179662227630615 0.3087962865829468
CurrentTrain: epoch  9, batch    18 | loss: 4.1796622Losses:  4.1867570877075195 0.2586122751235962
CurrentTrain: epoch  9, batch    19 | loss: 4.1867571Losses:  4.155478000640869 0.2633149027824402
CurrentTrain: epoch  9, batch    20 | loss: 4.1554780Losses:  4.157823085784912 0.28901931643486023
CurrentTrain: epoch  9, batch    21 | loss: 4.1578231Losses:  4.112151145935059 0.24993747472763062
CurrentTrain: epoch  9, batch    22 | loss: 4.1121511Losses:  4.1416497230529785 0.2670045495033264
CurrentTrain: epoch  9, batch    23 | loss: 4.1416497Losses:  4.131645679473877 0.3006725013256073
CurrentTrain: epoch  9, batch    24 | loss: 4.1316457Losses:  4.20719051361084 0.3409169912338257
CurrentTrain: epoch  9, batch    25 | loss: 4.2071905Losses:  4.154184341430664 0.2668434977531433
CurrentTrain: epoch  9, batch    26 | loss: 4.1541843Losses:  4.153517723083496 0.2703695595264435
CurrentTrain: epoch  9, batch    27 | loss: 4.1535177Losses:  4.149521827697754 0.2747761607170105
CurrentTrain: epoch  9, batch    28 | loss: 4.1495218Losses:  4.161929130554199 0.28496891260147095
CurrentTrain: epoch  9, batch    29 | loss: 4.1619291Losses:  4.162845611572266 0.28428080677986145
CurrentTrain: epoch  9, batch    30 | loss: 4.1628456Losses:  4.168970584869385 0.25950145721435547
CurrentTrain: epoch  9, batch    31 | loss: 4.1689706Losses:  4.185984134674072 0.34669721126556396
CurrentTrain: epoch  9, batch    32 | loss: 4.1859841Losses:  4.150331497192383 0.2612873911857605
CurrentTrain: epoch  9, batch    33 | loss: 4.1503315Losses:  4.195387840270996 0.3168654441833496
CurrentTrain: epoch  9, batch    34 | loss: 4.1953878Losses:  4.137655258178711 0.22595404088497162
CurrentTrain: epoch  9, batch    35 | loss: 4.1376553Losses:  4.086129665374756 0.23388028144836426
CurrentTrain: epoch  9, batch    36 | loss: 4.0861297Losses:  4.161804676055908 0.3413470387458801
CurrentTrain: epoch  9, batch    37 | loss: 4.1618047Losses:  4.135197162628174 0.290292888879776
CurrentTrain: epoch  9, batch    38 | loss: 4.1351972Losses:  4.190854549407959 0.3277689814567566
CurrentTrain: epoch  9, batch    39 | loss: 4.1908545Losses:  4.160457134246826 0.2855742275714874
CurrentTrain: epoch  9, batch    40 | loss: 4.1604571Losses:  4.096340179443359 0.23924806714057922
CurrentTrain: epoch  9, batch    41 | loss: 4.0963402Losses:  4.124731063842773 0.2617800235748291
CurrentTrain: epoch  9, batch    42 | loss: 4.1247311Losses:  4.125237941741943 0.21987465023994446
CurrentTrain: epoch  9, batch    43 | loss: 4.1252379Losses:  4.170284748077393 0.3028944730758667
CurrentTrain: epoch  9, batch    44 | loss: 4.1702847Losses:  4.23403263092041 0.3172945976257324
CurrentTrain: epoch  9, batch    45 | loss: 4.2340326Losses:  4.246272563934326 0.31946390867233276
CurrentTrain: epoch  9, batch    46 | loss: 4.2462726Losses:  4.152842998504639 0.2626199424266815
CurrentTrain: epoch  9, batch    47 | loss: 4.1528430Losses:  4.154178619384766 0.2570320665836334
CurrentTrain: epoch  9, batch    48 | loss: 4.1541786Losses:  4.110491752624512 0.24713373184204102
CurrentTrain: epoch  9, batch    49 | loss: 4.1104918Losses:  4.127640247344971 0.2446925938129425
CurrentTrain: epoch  9, batch    50 | loss: 4.1276402Losses:  4.202165126800537 0.22561869025230408
CurrentTrain: epoch  9, batch    51 | loss: 4.2021651Losses:  4.159901142120361 0.2805945873260498
CurrentTrain: epoch  9, batch    52 | loss: 4.1599011Losses:  4.107117176055908 0.18587204813957214
CurrentTrain: epoch  9, batch    53 | loss: 4.1071172Losses:  4.2353644371032715 0.2683449685573578
CurrentTrain: epoch  9, batch    54 | loss: 4.2353644Losses:  4.128628730773926 0.21766197681427002
CurrentTrain: epoch  9, batch    55 | loss: 4.1286287Losses:  4.15080451965332 0.25980648398399353
CurrentTrain: epoch  9, batch    56 | loss: 4.1508045Losses:  4.141200542449951 0.28764379024505615
CurrentTrain: epoch  9, batch    57 | loss: 4.1412005Losses:  4.148709774017334 0.2503511309623718
CurrentTrain: epoch  9, batch    58 | loss: 4.1487098Losses:  4.18374490737915 0.33459025621414185
CurrentTrain: epoch  9, batch    59 | loss: 4.1837449Losses:  4.13278341293335 0.21489262580871582
CurrentTrain: epoch  9, batch    60 | loss: 4.1327834Losses:  4.171919822692871 0.28996264934539795
CurrentTrain: epoch  9, batch    61 | loss: 4.1719198Losses:  4.024816989898682 0.15144649147987366
CurrentTrain: epoch  9, batch    62 | loss: 4.0248170
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.41%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.61%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.41%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.61%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
cur_acc:  ['0.9405']
his_acc:  ['0.9405']
Clustering into  9  clusters
Clusters:  [3 2 1 0 7 4 1 0 0 0 2 1 8 5 1 6 3 1 3 1]
Losses:  8.149275779724121 1.3248015642166138
CurrentTrain: epoch  0, batch     0 | loss: 8.1492758Losses:  8.763401985168457 1.4699536561965942
CurrentTrain: epoch  0, batch     1 | loss: 8.7634020Losses:  7.71018648147583 1.1670715808868408
CurrentTrain: epoch  0, batch     2 | loss: 7.7101865Losses:  6.09219217300415 0.27729982137680054
CurrentTrain: epoch  0, batch     3 | loss: 6.0921922Losses:  7.4608025550842285 1.2256958484649658
CurrentTrain: epoch  1, batch     0 | loss: 7.4608026Losses:  7.300437927246094 1.319509506225586
CurrentTrain: epoch  1, batch     1 | loss: 7.3004379Losses:  7.115114688873291 1.356518030166626
CurrentTrain: epoch  1, batch     2 | loss: 7.1151147Losses:  6.212983131408691 0.3015636205673218
CurrentTrain: epoch  1, batch     3 | loss: 6.2129831Losses:  6.341904163360596 1.1119533777236938
CurrentTrain: epoch  2, batch     0 | loss: 6.3419042Losses:  6.104823589324951 1.080153465270996
CurrentTrain: epoch  2, batch     1 | loss: 6.1048236Losses:  6.0371575355529785 1.2919238805770874
CurrentTrain: epoch  2, batch     2 | loss: 6.0371575Losses:  4.653217315673828 0.278896689414978
CurrentTrain: epoch  2, batch     3 | loss: 4.6532173Losses:  5.6700286865234375 1.3101999759674072
CurrentTrain: epoch  3, batch     0 | loss: 5.6700287Losses:  5.634741306304932 1.242643117904663
CurrentTrain: epoch  3, batch     1 | loss: 5.6347413Losses:  5.603804111480713 1.2921335697174072
CurrentTrain: epoch  3, batch     2 | loss: 5.6038041Losses:  3.640251398086548 0.1265343874692917
CurrentTrain: epoch  3, batch     3 | loss: 3.6402514Losses:  5.745949745178223 1.2217936515808105
CurrentTrain: epoch  4, batch     0 | loss: 5.7459497Losses:  5.028169631958008 1.342979907989502
CurrentTrain: epoch  4, batch     1 | loss: 5.0281696Losses:  4.9282331466674805 1.1310315132141113
CurrentTrain: epoch  4, batch     2 | loss: 4.9282331Losses:  3.1621267795562744 0.2119537889957428
CurrentTrain: epoch  4, batch     3 | loss: 3.1621268Losses:  4.971055507659912 1.1136362552642822
CurrentTrain: epoch  5, batch     0 | loss: 4.9710555Losses:  4.106154441833496 1.0103673934936523
CurrentTrain: epoch  5, batch     1 | loss: 4.1061544Losses:  5.257378578186035 1.1173367500305176
CurrentTrain: epoch  5, batch     2 | loss: 5.2573786Losses:  5.2102885246276855 0.2175268530845642
CurrentTrain: epoch  5, batch     3 | loss: 5.2102885Losses:  4.802149772644043 1.1307387351989746
CurrentTrain: epoch  6, batch     0 | loss: 4.8021498Losses:  4.701997756958008 1.0608011484146118
CurrentTrain: epoch  6, batch     1 | loss: 4.7019978Losses:  4.157832622528076 1.168642520904541
CurrentTrain: epoch  6, batch     2 | loss: 4.1578326Losses:  4.4089131355285645 0.2299950122833252
CurrentTrain: epoch  6, batch     3 | loss: 4.4089131Losses:  3.99576735496521 1.0108861923217773
CurrentTrain: epoch  7, batch     0 | loss: 3.9957674Losses:  4.518864154815674 1.2385483980178833
CurrentTrain: epoch  7, batch     1 | loss: 4.5188642Losses:  4.149971961975098 0.8985893726348877
CurrentTrain: epoch  7, batch     2 | loss: 4.1499720Losses:  3.6679253578186035 0.22244387865066528
CurrentTrain: epoch  7, batch     3 | loss: 3.6679254Losses:  3.6014151573181152 1.1020219326019287
CurrentTrain: epoch  8, batch     0 | loss: 3.6014152Losses:  4.413516044616699 1.006105899810791
CurrentTrain: epoch  8, batch     1 | loss: 4.4135160Losses:  4.286544322967529 0.8923448920249939
CurrentTrain: epoch  8, batch     2 | loss: 4.2865443Losses:  2.4802374839782715 0.0
CurrentTrain: epoch  8, batch     3 | loss: 2.4802375Losses:  3.906341791152954 1.1054071187973022
CurrentTrain: epoch  9, batch     0 | loss: 3.9063418Losses:  3.8090643882751465 0.9657962322235107
CurrentTrain: epoch  9, batch     1 | loss: 3.8090644Losses:  3.765481948852539 1.0670123100280762
CurrentTrain: epoch  9, batch     2 | loss: 3.7654819Losses:  4.302858352661133 0.19187375903129578
CurrentTrain: epoch  9, batch     3 | loss: 4.3028584
Losses:  1.3610117435455322 1.0779640674591064
MemoryTrain:  epoch  0, batch     0 | loss: 1.3610117Losses:  0.8439024090766907 0.17936620116233826
MemoryTrain:  epoch  0, batch     1 | loss: 0.8439024Losses:  1.1127099990844727 1.0223320722579956
MemoryTrain:  epoch  1, batch     0 | loss: 1.1127100Losses:  0.31898927688598633 0.2061559557914734
MemoryTrain:  epoch  1, batch     1 | loss: 0.3189893Losses:  0.7764338254928589 1.0619899034500122
MemoryTrain:  epoch  2, batch     0 | loss: 0.7764338Losses:  0.37895113229751587 0.18763576447963715
MemoryTrain:  epoch  2, batch     1 | loss: 0.3789511Losses:  0.47854697704315186 0.8481135964393616
MemoryTrain:  epoch  3, batch     0 | loss: 0.4785470Losses:  1.4736353158950806 0.3917614817619324
MemoryTrain:  epoch  3, batch     1 | loss: 1.4736353Losses:  0.6049957275390625 0.909110426902771
MemoryTrain:  epoch  4, batch     0 | loss: 0.6049957Losses:  0.17489930987358093 0.2922177016735077
MemoryTrain:  epoch  4, batch     1 | loss: 0.1748993Losses:  0.520368754863739 0.9085785150527954
MemoryTrain:  epoch  5, batch     0 | loss: 0.5203688Losses:  0.13734546303749084 0.24387897551059723
MemoryTrain:  epoch  5, batch     1 | loss: 0.1373455Losses:  0.4357081353664398 0.7938402891159058
MemoryTrain:  epoch  6, batch     0 | loss: 0.4357081Losses:  0.270902156829834 0.4474013149738312
MemoryTrain:  epoch  6, batch     1 | loss: 0.2709022Losses:  0.47073307633399963 0.865217924118042
MemoryTrain:  epoch  7, batch     0 | loss: 0.4707331Losses:  0.1677681803703308 0.2791815996170044
MemoryTrain:  epoch  7, batch     1 | loss: 0.1677682Losses:  0.45155006647109985 0.8435659408569336
MemoryTrain:  epoch  8, batch     0 | loss: 0.4515501Losses:  0.15487346053123474 0.2769313454627991
MemoryTrain:  epoch  8, batch     1 | loss: 0.1548735Losses:  0.39036107063293457 0.7480234503746033
MemoryTrain:  epoch  9, batch     0 | loss: 0.3903611Losses:  0.5888485312461853 0.37801629304885864
MemoryTrain:  epoch  9, batch     1 | loss: 0.5888485
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 56.03%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 54.37%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.62%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 55.71%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 56.42%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 57.07%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 57.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 61.81%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 62.75%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 63.22%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 64.69%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.11%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.60%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.77%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 88.81%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 87.87%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 87.23%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.41%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 87.59%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 87.65%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.35%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 86.90%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 86.84%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 86.41%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 85.16%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.20%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.26%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 82.35%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 81.52%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 80.65%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 79.92%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 79.94%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 80.01%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.15%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.28%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 80.49%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 80.39%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 80.17%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 80.12%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 79.98%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.88%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.09%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 79.64%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 79.44%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 79.05%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 78.81%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 78.50%   
cur_acc:  ['0.9405', '0.6260']
his_acc:  ['0.9405', '0.7850']
Clustering into  14  clusters
Clusters:  [ 9  7  5  0 13  4  5  0  2  2  7 10  8 12  2  3  9  5  6 10  5  1 11  7
 12 10  1  4  3 10]
Losses:  6.347282409667969 0.8193950653076172
CurrentTrain: epoch  0, batch     0 | loss: 6.3472824Losses:  6.388985633850098 0.8651251792907715
CurrentTrain: epoch  0, batch     1 | loss: 6.3889856Losses:  6.345363140106201 0.8974336981773376
CurrentTrain: epoch  0, batch     2 | loss: 6.3453631Losses:  5.535667896270752 0.13504156470298767
CurrentTrain: epoch  0, batch     3 | loss: 5.5356679Losses:  5.966019630432129 0.8382039070129395
CurrentTrain: epoch  1, batch     0 | loss: 5.9660196Losses:  5.343033790588379 1.0141067504882812
CurrentTrain: epoch  1, batch     1 | loss: 5.3430338Losses:  4.814742088317871 0.7657545804977417
CurrentTrain: epoch  1, batch     2 | loss: 4.8147421Losses:  2.98586106300354 0.04711636155843735
CurrentTrain: epoch  1, batch     3 | loss: 2.9858611Losses:  4.502994060516357 0.8563077449798584
CurrentTrain: epoch  2, batch     0 | loss: 4.5029941Losses:  4.373844146728516 0.8797287940979004
CurrentTrain: epoch  2, batch     1 | loss: 4.3738441Losses:  4.057179927825928 0.9089120626449585
CurrentTrain: epoch  2, batch     2 | loss: 4.0571799Losses:  3.724106788635254 0.16723167896270752
CurrentTrain: epoch  2, batch     3 | loss: 3.7241068Losses:  4.105140686035156 0.9518131017684937
CurrentTrain: epoch  3, batch     0 | loss: 4.1051407Losses:  4.082499980926514 0.869681715965271
CurrentTrain: epoch  3, batch     1 | loss: 4.0825000Losses:  3.7841084003448486 0.8702607750892639
CurrentTrain: epoch  3, batch     2 | loss: 3.7841084Losses:  2.147359848022461 0.04117412120103836
CurrentTrain: epoch  3, batch     3 | loss: 2.1473598Losses:  3.767631769180298 0.928423285484314
CurrentTrain: epoch  4, batch     0 | loss: 3.7676318Losses:  3.8055779933929443 0.8787620663642883
CurrentTrain: epoch  4, batch     1 | loss: 3.8055780Losses:  3.549203395843506 0.7681087255477905
CurrentTrain: epoch  4, batch     2 | loss: 3.5492034Losses:  2.2096364498138428 0.19504469633102417
CurrentTrain: epoch  4, batch     3 | loss: 2.2096364Losses:  3.365144729614258 0.8782252669334412
CurrentTrain: epoch  5, batch     0 | loss: 3.3651447Losses:  3.241307258605957 0.8228874206542969
CurrentTrain: epoch  5, batch     1 | loss: 3.2413073Losses:  3.5022025108337402 0.8695113658905029
CurrentTrain: epoch  5, batch     2 | loss: 3.5022025Losses:  2.3508148193359375 0.21674460172653198
CurrentTrain: epoch  5, batch     3 | loss: 2.3508148Losses:  3.1386358737945557 0.721126914024353
CurrentTrain: epoch  6, batch     0 | loss: 3.1386359Losses:  2.580866813659668 0.7599422931671143
CurrentTrain: epoch  6, batch     1 | loss: 2.5808668Losses:  3.587357521057129 0.8892812728881836
CurrentTrain: epoch  6, batch     2 | loss: 3.5873575Losses:  2.5340967178344727 0.06617185473442078
CurrentTrain: epoch  6, batch     3 | loss: 2.5340967Losses:  2.965898036956787 0.9331629872322083
CurrentTrain: epoch  7, batch     0 | loss: 2.9658980Losses:  2.854097366333008 0.7383644580841064
CurrentTrain: epoch  7, batch     1 | loss: 2.8540974Losses:  2.737259864807129 0.8494703769683838
CurrentTrain: epoch  7, batch     2 | loss: 2.7372599Losses:  2.90818452835083 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.9081845Losses:  3.0184502601623535 0.8844316005706787
CurrentTrain: epoch  8, batch     0 | loss: 3.0184503Losses:  2.437119483947754 0.8183342814445496
CurrentTrain: epoch  8, batch     1 | loss: 2.4371195Losses:  2.551504135131836 0.6581156253814697
CurrentTrain: epoch  8, batch     2 | loss: 2.5515041Losses:  2.690535306930542 0.12718211114406586
CurrentTrain: epoch  8, batch     3 | loss: 2.6905353Losses:  2.523796558380127 0.8521788120269775
CurrentTrain: epoch  9, batch     0 | loss: 2.5237966Losses:  2.6435623168945312 0.8830384016036987
CurrentTrain: epoch  9, batch     1 | loss: 2.6435623Losses:  2.6473803520202637 0.8974581360816956
CurrentTrain: epoch  9, batch     2 | loss: 2.6473804Losses:  1.9130089282989502 0.19519942998886108
CurrentTrain: epoch  9, batch     3 | loss: 1.9130089
Losses:  1.5557340383529663 1.211244821548462
MemoryTrain:  epoch  0, batch     0 | loss: 1.5557340Losses:  2.0113985538482666 0.7765299081802368
MemoryTrain:  epoch  0, batch     1 | loss: 2.0113986Losses:  2.21726655960083 1.0746426582336426
MemoryTrain:  epoch  1, batch     0 | loss: 2.2172666Losses:  1.4375187158584595 0.9895372986793518
MemoryTrain:  epoch  1, batch     1 | loss: 1.4375187Losses:  1.4539575576782227 1.067948341369629
MemoryTrain:  epoch  2, batch     0 | loss: 1.4539576Losses:  1.9873915910720825 0.9681960940361023
MemoryTrain:  epoch  2, batch     1 | loss: 1.9873916Losses:  1.045819640159607 1.1528241634368896
MemoryTrain:  epoch  3, batch     0 | loss: 1.0458196Losses:  1.8136848211288452 0.9073578715324402
MemoryTrain:  epoch  3, batch     1 | loss: 1.8136848Losses:  1.6573927402496338 1.0181207656860352
MemoryTrain:  epoch  4, batch     0 | loss: 1.6573927Losses:  0.7424251437187195 1.045407772064209
MemoryTrain:  epoch  4, batch     1 | loss: 0.7424251Losses:  0.6960659623146057 1.1191281080245972
MemoryTrain:  epoch  5, batch     0 | loss: 0.6960660Losses:  1.5498842000961304 0.9093239903450012
MemoryTrain:  epoch  5, batch     1 | loss: 1.5498842Losses:  0.5866053104400635 0.8567259311676025
MemoryTrain:  epoch  6, batch     0 | loss: 0.5866053Losses:  1.61820650100708 1.1239829063415527
MemoryTrain:  epoch  6, batch     1 | loss: 1.6182065Losses:  1.0750370025634766 1.0134572982788086
MemoryTrain:  epoch  7, batch     0 | loss: 1.0750370Losses:  0.8679680824279785 0.7981608510017395
MemoryTrain:  epoch  7, batch     1 | loss: 0.8679681Losses:  0.974020779132843 1.110788345336914
MemoryTrain:  epoch  8, batch     0 | loss: 0.9740208Losses:  0.7837378978729248 0.7023875117301941
MemoryTrain:  epoch  8, batch     1 | loss: 0.7837379Losses:  0.868882417678833 1.0067299604415894
MemoryTrain:  epoch  9, batch     0 | loss: 0.8688824Losses:  0.7784719467163086 0.7486116886138916
MemoryTrain:  epoch  9, batch     1 | loss: 0.7784719
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 77.36%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 76.22%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.59%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.55%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 87.59%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 86.57%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 85.48%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 84.96%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.02%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 84.88%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.98%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 85.18%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.21%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 85.39%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 84.91%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 83.89%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 82.96%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 82.06%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 81.10%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 80.17%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 79.40%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 78.51%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 77.64%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 76.79%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 76.02%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 75.20%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 75.90%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 76.20%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 76.19%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 76.24%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 76.23%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 76.15%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.48%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 76.20%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 75.72%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 75.20%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 77.05%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 77.21%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 77.20%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 77.35%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 77.38%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 77.35%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 77.18%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 76.91%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 76.78%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 76.69%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 76.51%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 76.43%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 76.17%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 75.48%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.15%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 74.89%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 74.57%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 74.36%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 75.80%   
cur_acc:  ['0.9405', '0.6260', '0.7698']
his_acc:  ['0.9405', '0.7850', '0.7580']
Clustering into  19  clusters
Clusters:  [ 4  5  7  0  9  3  7  6  1  1  5 14  0 12 18  2  4  7 17 14  8  1 13  5
 12 14  1  3  2 14  7  1  6 14 15 10 11 16 18  8]
Losses:  6.7862043380737305 1.136554479598999
CurrentTrain: epoch  0, batch     0 | loss: 6.7862043Losses:  6.976057052612305 0.9082068204879761
CurrentTrain: epoch  0, batch     1 | loss: 6.9760571Losses:  7.411526679992676 1.0969098806381226
CurrentTrain: epoch  0, batch     2 | loss: 7.4115267Losses:  8.010272979736328 0.20385336875915527
CurrentTrain: epoch  0, batch     3 | loss: 8.0102730Losses:  6.210546970367432 0.9921901822090149
CurrentTrain: epoch  1, batch     0 | loss: 6.2105470Losses:  6.490513324737549 0.9515247344970703
CurrentTrain: epoch  1, batch     1 | loss: 6.4905133Losses:  5.368137836456299 0.8467268347740173
CurrentTrain: epoch  1, batch     2 | loss: 5.3681378Losses:  5.5318121910095215 0.36554160714149475
CurrentTrain: epoch  1, batch     3 | loss: 5.5318122Losses:  5.769564628601074 1.0933380126953125
CurrentTrain: epoch  2, batch     0 | loss: 5.7695646Losses:  5.597310543060303 0.7708271145820618
CurrentTrain: epoch  2, batch     1 | loss: 5.5973105Losses:  4.997879981994629 0.9962562918663025
CurrentTrain: epoch  2, batch     2 | loss: 4.9978800Losses:  7.251661777496338 0.22159484028816223
CurrentTrain: epoch  2, batch     3 | loss: 7.2516618Losses:  4.698518753051758 0.9238916039466858
CurrentTrain: epoch  3, batch     0 | loss: 4.6985188Losses:  6.030700206756592 0.870591402053833
CurrentTrain: epoch  3, batch     1 | loss: 6.0307002Losses:  5.0442657470703125 0.8955649733543396
CurrentTrain: epoch  3, batch     2 | loss: 5.0442657Losses:  4.318143844604492 0.38237354159355164
CurrentTrain: epoch  3, batch     3 | loss: 4.3181438Losses:  4.562354564666748 1.0112606287002563
CurrentTrain: epoch  4, batch     0 | loss: 4.5623546Losses:  5.214115619659424 0.6848232746124268
CurrentTrain: epoch  4, batch     1 | loss: 5.2141156Losses:  5.41424560546875 0.986923098564148
CurrentTrain: epoch  4, batch     2 | loss: 5.4142456Losses:  3.5376226902008057 0.144502192735672
CurrentTrain: epoch  4, batch     3 | loss: 3.5376227Losses:  5.1138916015625 0.8681449294090271
CurrentTrain: epoch  5, batch     0 | loss: 5.1138916Losses:  5.020442008972168 0.8774887919425964
CurrentTrain: epoch  5, batch     1 | loss: 5.0204420Losses:  4.023164749145508 0.7346124649047852
CurrentTrain: epoch  5, batch     2 | loss: 4.0231647Losses:  4.920987606048584 0.2599993944168091
CurrentTrain: epoch  5, batch     3 | loss: 4.9209876Losses:  4.988600730895996 0.7817815542221069
CurrentTrain: epoch  6, batch     0 | loss: 4.9886007Losses:  4.918544769287109 0.9077016115188599
CurrentTrain: epoch  6, batch     1 | loss: 4.9185448Losses:  4.305525302886963 0.8261933326721191
CurrentTrain: epoch  6, batch     2 | loss: 4.3055253Losses:  2.369401693344116 0.185760036110878
CurrentTrain: epoch  6, batch     3 | loss: 2.3694017Losses:  5.893261909484863 1.0494650602340698
CurrentTrain: epoch  7, batch     0 | loss: 5.8932619Losses:  3.899273157119751 0.8482268452644348
CurrentTrain: epoch  7, batch     1 | loss: 3.8992732Losses:  4.161608695983887 0.730846107006073
CurrentTrain: epoch  7, batch     2 | loss: 4.1616087Losses:  1.898452877998352 0.05999719351530075
CurrentTrain: epoch  7, batch     3 | loss: 1.8984529Losses:  4.705242156982422 1.0322463512420654
CurrentTrain: epoch  8, batch     0 | loss: 4.7052422Losses:  4.061304569244385 0.7112022638320923
CurrentTrain: epoch  8, batch     1 | loss: 4.0613046Losses:  4.972494602203369 1.1004819869995117
CurrentTrain: epoch  8, batch     2 | loss: 4.9724946Losses:  1.8678257465362549 0.0
CurrentTrain: epoch  8, batch     3 | loss: 1.8678257Losses:  3.858246326446533 0.8575119972229004
CurrentTrain: epoch  9, batch     0 | loss: 3.8582463Losses:  5.169371128082275 0.8916328549385071
CurrentTrain: epoch  9, batch     1 | loss: 5.1693711Losses:  3.7944304943084717 0.8557246327400208
CurrentTrain: epoch  9, batch     2 | loss: 3.7944305Losses:  2.1015572547912598 0.10359874367713928
CurrentTrain: epoch  9, batch     3 | loss: 2.1015573
Losses:  2.325986623764038 1.0037510395050049
MemoryTrain:  epoch  0, batch     0 | loss: 2.3259866Losses:  1.2377376556396484 1.1668479442596436
MemoryTrain:  epoch  0, batch     1 | loss: 1.2377377Losses:  1.7387490272521973 0.9480246305465698
MemoryTrain:  epoch  0, batch     2 | loss: 1.7387490Losses:  1.776627540588379 1.055632472038269
MemoryTrain:  epoch  1, batch     0 | loss: 1.7766275Losses:  1.9375869035720825 1.1253893375396729
MemoryTrain:  epoch  1, batch     1 | loss: 1.9375869Losses:  1.4725278615951538 0.4616214334964752
MemoryTrain:  epoch  1, batch     2 | loss: 1.4725279Losses:  1.4437220096588135 0.9430001974105835
MemoryTrain:  epoch  2, batch     0 | loss: 1.4437220Losses:  1.469162940979004 1.1206166744232178
MemoryTrain:  epoch  2, batch     1 | loss: 1.4691629Losses:  0.4648425579071045 0.6170350313186646
MemoryTrain:  epoch  2, batch     2 | loss: 0.4648426Losses:  0.8702298402786255 1.0681240558624268
MemoryTrain:  epoch  3, batch     0 | loss: 0.8702298Losses:  1.0812780857086182 1.0053553581237793
MemoryTrain:  epoch  3, batch     1 | loss: 1.0812781Losses:  0.6448783874511719 0.5550516843795776
MemoryTrain:  epoch  3, batch     2 | loss: 0.6448784Losses:  0.6910256147384644 1.1913549900054932
MemoryTrain:  epoch  4, batch     0 | loss: 0.6910256Losses:  0.9414949417114258 1.064137578010559
MemoryTrain:  epoch  4, batch     1 | loss: 0.9414949Losses:  0.4294338822364807 0.5073341727256775
MemoryTrain:  epoch  4, batch     2 | loss: 0.4294339Losses:  0.7186768054962158 1.1971867084503174
MemoryTrain:  epoch  5, batch     0 | loss: 0.7186768Losses:  0.6834931969642639 0.8464559316635132
MemoryTrain:  epoch  5, batch     1 | loss: 0.6834932Losses:  0.4973655939102173 0.5777574181556702
MemoryTrain:  epoch  5, batch     2 | loss: 0.4973656Losses:  0.598831057548523 0.928749144077301
MemoryTrain:  epoch  6, batch     0 | loss: 0.5988311Losses:  0.6243608593940735 0.9702963829040527
MemoryTrain:  epoch  6, batch     1 | loss: 0.6243609Losses:  0.47264519333839417 0.7631006240844727
MemoryTrain:  epoch  6, batch     2 | loss: 0.4726452Losses:  0.5404234528541565 0.972449541091919
MemoryTrain:  epoch  7, batch     0 | loss: 0.5404235Losses:  0.5894818902015686 1.0446629524230957
MemoryTrain:  epoch  7, batch     1 | loss: 0.5894819Losses:  0.35543715953826904 0.4955580234527588
MemoryTrain:  epoch  7, batch     2 | loss: 0.3554372Losses:  0.5919135808944702 1.0729584693908691
MemoryTrain:  epoch  8, batch     0 | loss: 0.5919136Losses:  0.4775201082229614 0.889456570148468
MemoryTrain:  epoch  8, batch     1 | loss: 0.4775201Losses:  0.4005747139453888 0.6516201496124268
MemoryTrain:  epoch  8, batch     2 | loss: 0.4005747Losses:  0.43676823377609253 0.7981865406036377
MemoryTrain:  epoch  9, batch     0 | loss: 0.4367682Losses:  0.6104686260223389 1.090163230895996
MemoryTrain:  epoch  9, batch     1 | loss: 0.6104686Losses:  0.31570520997047424 0.5435309410095215
MemoryTrain:  epoch  9, batch     2 | loss: 0.3157052
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.47%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 65.68%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 64.82%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 63.99%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.78%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.28%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 87.18%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 87.40%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.19%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 85.25%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 83.94%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 81.90%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 80.88%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 80.43%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 80.27%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 80.28%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 80.38%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 80.48%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 80.15%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 80.37%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 80.46%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 80.26%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 79.44%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 78.57%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 77.79%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 76.96%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 76.08%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 75.36%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 74.51%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 73.68%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 72.87%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 72.08%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 71.30%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 70.74%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 73.01%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 72.67%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 72.48%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 72.19%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 71.95%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 71.39%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 71.15%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 70.73%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 70.42%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 70.02%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.62%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 70.93%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 71.14%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 70.97%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 70.55%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 70.22%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 70.13%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 69.98%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 69.56%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 69.23%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 68.86%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 68.25%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 69.74%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 69.51%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 69.08%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  208 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 69.08%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 69.02%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 70.35%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 70.34%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 70.21%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 69.98%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 69.79%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 69.51%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 69.80%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 69.54%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 69.36%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 69.10%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 68.73%   
cur_acc:  ['0.9405', '0.6260', '0.7698', '0.6399']
his_acc:  ['0.9405', '0.7850', '0.7580', '0.6873']
Clustering into  24  clusters
Clusters:  [18 10  7  1 23  5  7  1  9  9 10  4  0  6  4  3  2  7  2  4  8  9 16 10
  6  4 22  5  3  4  7  9  1  4 15 12 19 21 11  8 14 13  0 11  1 20  2  0
 17 22]
Losses:  6.565433502197266 1.1217728853225708
CurrentTrain: epoch  0, batch     0 | loss: 6.5654335Losses:  6.593252658843994 1.0793342590332031
CurrentTrain: epoch  0, batch     1 | loss: 6.5932527Losses:  7.333832740783691 1.059901237487793
CurrentTrain: epoch  0, batch     2 | loss: 7.3338327Losses:  3.451702117919922 0.2309015840291977
CurrentTrain: epoch  0, batch     3 | loss: 3.4517021Losses:  5.3781046867370605 1.0564625263214111
CurrentTrain: epoch  1, batch     0 | loss: 5.3781047Losses:  5.009987831115723 1.116901159286499
CurrentTrain: epoch  1, batch     1 | loss: 5.0099878Losses:  6.025641441345215 1.1532889604568481
CurrentTrain: epoch  1, batch     2 | loss: 6.0256414Losses:  3.368860960006714 0.24660724401474
CurrentTrain: epoch  1, batch     3 | loss: 3.3688610Losses:  5.784222602844238 1.1491129398345947
CurrentTrain: epoch  2, batch     0 | loss: 5.7842226Losses:  4.111653804779053 0.9390252828598022
CurrentTrain: epoch  2, batch     1 | loss: 4.1116538Losses:  3.971776008605957 1.014111042022705
CurrentTrain: epoch  2, batch     2 | loss: 3.9717760Losses:  6.717679500579834 0.5009669661521912
CurrentTrain: epoch  2, batch     3 | loss: 6.7176795Losses:  4.55610990524292 1.0780954360961914
CurrentTrain: epoch  3, batch     0 | loss: 4.5561099Losses:  4.338261127471924 1.2684296369552612
CurrentTrain: epoch  3, batch     1 | loss: 4.3382611Losses:  4.128754138946533 1.0976057052612305
CurrentTrain: epoch  3, batch     2 | loss: 4.1287541Losses:  3.21099853515625 0.08202194422483444
CurrentTrain: epoch  3, batch     3 | loss: 3.2109985Losses:  4.370811462402344 0.9431815147399902
CurrentTrain: epoch  4, batch     0 | loss: 4.3708115Losses:  3.570827007293701 0.9716750383377075
CurrentTrain: epoch  4, batch     1 | loss: 3.5708270Losses:  4.248702526092529 1.1317702531814575
CurrentTrain: epoch  4, batch     2 | loss: 4.2487025Losses:  2.6339056491851807 0.17434117197990417
CurrentTrain: epoch  4, batch     3 | loss: 2.6339056Losses:  4.01239013671875 0.9578157067298889
CurrentTrain: epoch  5, batch     0 | loss: 4.0123901Losses:  3.80592942237854 0.9528278112411499
CurrentTrain: epoch  5, batch     1 | loss: 3.8059294Losses:  3.836958408355713 0.8759653568267822
CurrentTrain: epoch  5, batch     2 | loss: 3.8369584Losses:  3.831977128982544 0.22992610931396484
CurrentTrain: epoch  5, batch     3 | loss: 3.8319771Losses:  3.859424114227295 0.9100148677825928
CurrentTrain: epoch  6, batch     0 | loss: 3.8594241Losses:  4.133740425109863 0.9097002744674683
CurrentTrain: epoch  6, batch     1 | loss: 4.1337404Losses:  3.017655611038208 0.9524216055870056
CurrentTrain: epoch  6, batch     2 | loss: 3.0176556Losses:  4.974311351776123 0.22850999236106873
CurrentTrain: epoch  6, batch     3 | loss: 4.9743114Losses:  3.735931873321533 0.8873260021209717
CurrentTrain: epoch  7, batch     0 | loss: 3.7359319Losses:  3.6676831245422363 0.9158999919891357
CurrentTrain: epoch  7, batch     1 | loss: 3.6676831Losses:  3.0859122276306152 0.9237990975379944
CurrentTrain: epoch  7, batch     2 | loss: 3.0859122Losses:  4.355222702026367 0.4223335385322571
CurrentTrain: epoch  7, batch     3 | loss: 4.3552227Losses:  3.0993785858154297 0.9397246837615967
CurrentTrain: epoch  8, batch     0 | loss: 3.0993786Losses:  3.8110756874084473 0.9954937100410461
CurrentTrain: epoch  8, batch     1 | loss: 3.8110757Losses:  3.3151745796203613 1.0298089981079102
CurrentTrain: epoch  8, batch     2 | loss: 3.3151746Losses:  3.8417584896087646 0.17724186182022095
CurrentTrain: epoch  8, batch     3 | loss: 3.8417585Losses:  3.0931811332702637 0.9940318465232849
CurrentTrain: epoch  9, batch     0 | loss: 3.0931811Losses:  3.110440731048584 1.0002381801605225
CurrentTrain: epoch  9, batch     1 | loss: 3.1104407Losses:  3.863917589187622 0.9927676916122437
CurrentTrain: epoch  9, batch     2 | loss: 3.8639176Losses:  2.0789237022399902 0.23231227695941925
CurrentTrain: epoch  9, batch     3 | loss: 2.0789237
Losses:  0.8957968950271606 0.9671903252601624
MemoryTrain:  epoch  0, batch     0 | loss: 0.8957969Losses:  0.8144172430038452 1.106787919998169
MemoryTrain:  epoch  0, batch     1 | loss: 0.8144172Losses:  1.4151079654693604 1.4816890954971313
MemoryTrain:  epoch  0, batch     2 | loss: 1.4151080Losses:  0.22201868891716003 0.11599026620388031
MemoryTrain:  epoch  0, batch     3 | loss: 0.2220187Losses:  1.629499077796936 1.1441020965576172
MemoryTrain:  epoch  1, batch     0 | loss: 1.6294991Losses:  1.1624693870544434 1.1482782363891602
MemoryTrain:  epoch  1, batch     1 | loss: 1.1624694Losses:  0.9593168497085571 1.0678436756134033
MemoryTrain:  epoch  1, batch     2 | loss: 0.9593168Losses:  0.6221944689750671 0.09020725637674332
MemoryTrain:  epoch  1, batch     3 | loss: 0.6221945Losses:  0.8741664886474609 1.1209988594055176
MemoryTrain:  epoch  2, batch     0 | loss: 0.8741665Losses:  1.2683906555175781 1.1610273122787476
MemoryTrain:  epoch  2, batch     1 | loss: 1.2683907Losses:  0.5857890844345093 0.9290919899940491
MemoryTrain:  epoch  2, batch     2 | loss: 0.5857891Losses:  0.45326530933380127 0.16211697459220886
MemoryTrain:  epoch  2, batch     3 | loss: 0.4532653Losses:  0.8493953943252563 1.1210060119628906
MemoryTrain:  epoch  3, batch     0 | loss: 0.8493954Losses:  0.5362398624420166 0.8615595102310181
MemoryTrain:  epoch  3, batch     1 | loss: 0.5362399Losses:  0.6489738821983337 1.0816922187805176
MemoryTrain:  epoch  3, batch     2 | loss: 0.6489739Losses:  0.248124897480011 0.24324946105480194
MemoryTrain:  epoch  3, batch     3 | loss: 0.2481249Losses:  0.5325515866279602 0.9611688852310181
MemoryTrain:  epoch  4, batch     0 | loss: 0.5325516Losses:  0.6358513236045837 1.0193692445755005
MemoryTrain:  epoch  4, batch     1 | loss: 0.6358513Losses:  0.7380901575088501 1.169110894203186
MemoryTrain:  epoch  4, batch     2 | loss: 0.7380902Losses:  0.16473209857940674 0.06539563834667206
MemoryTrain:  epoch  4, batch     3 | loss: 0.1647321Losses:  0.5671404600143433 0.9675314426422119
MemoryTrain:  epoch  5, batch     0 | loss: 0.5671405Losses:  0.6393408179283142 1.1362204551696777
MemoryTrain:  epoch  5, batch     1 | loss: 0.6393408Losses:  0.4970483183860779 0.8797801733016968
MemoryTrain:  epoch  5, batch     2 | loss: 0.4970483Losses:  0.14960530400276184 0.23133069276809692
MemoryTrain:  epoch  5, batch     3 | loss: 0.1496053Losses:  0.572232723236084 1.017680287361145
MemoryTrain:  epoch  6, batch     0 | loss: 0.5722327Losses:  0.6767841577529907 1.0008811950683594
MemoryTrain:  epoch  6, batch     1 | loss: 0.6767842Losses:  0.6075451374053955 1.012758731842041
MemoryTrain:  epoch  6, batch     2 | loss: 0.6075451Losses:  0.09650629758834839 0.12864971160888672
MemoryTrain:  epoch  6, batch     3 | loss: 0.0965063Losses:  0.5902143716812134 0.971916139125824
MemoryTrain:  epoch  7, batch     0 | loss: 0.5902144Losses:  0.575478732585907 1.068580150604248
MemoryTrain:  epoch  7, batch     1 | loss: 0.5754787Losses:  0.5590474009513855 0.978260338306427
MemoryTrain:  epoch  7, batch     2 | loss: 0.5590474Losses:  0.0476088747382164 0.04666781425476074
MemoryTrain:  epoch  7, batch     3 | loss: 0.0476089Losses:  0.5647329092025757 1.0572097301483154
MemoryTrain:  epoch  8, batch     0 | loss: 0.5647329Losses:  0.43601080775260925 0.8241422176361084
MemoryTrain:  epoch  8, batch     1 | loss: 0.4360108Losses:  0.5306168794631958 0.9747658967971802
MemoryTrain:  epoch  8, batch     2 | loss: 0.5306169Losses:  0.1122361570596695 0.1479911506175995
MemoryTrain:  epoch  8, batch     3 | loss: 0.1122362Losses:  0.613840639591217 1.1581670045852661
MemoryTrain:  epoch  9, batch     0 | loss: 0.6138406Losses:  0.5232507586479187 0.9821656942367554
MemoryTrain:  epoch  9, batch     1 | loss: 0.5232508Losses:  0.4820210933685303 0.8099727034568787
MemoryTrain:  epoch  9, batch     2 | loss: 0.4820211Losses:  0.0292314812541008 0.026564946398139
MemoryTrain:  epoch  9, batch     3 | loss: 0.0292315
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 70.10%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 68.29%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.40%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.55%   