#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=5
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  25.125850677490234 11.09357738494873 0.6415597200393677
CurrentTrain: epoch  0, batch     0 | loss: 25.1258507Losses:  23.431838989257812 9.646484375 0.645329475402832
CurrentTrain: epoch  0, batch     1 | loss: 23.4318390Losses:  22.776517868041992 9.043648719787598 0.6303292512893677
CurrentTrain: epoch  0, batch     2 | loss: 22.7765179Losses:  23.847043991088867 10.20238208770752 0.6439913511276245
CurrentTrain: epoch  0, batch     3 | loss: 23.8470440Losses:  23.505054473876953 9.971214294433594 0.6052734851837158
CurrentTrain: epoch  0, batch     4 | loss: 23.5050545Losses:  20.69438362121582 7.43414306640625 0.6034010648727417
CurrentTrain: epoch  0, batch     5 | loss: 20.6943836Losses:  20.800745010375977 7.519517421722412 0.6097358465194702
CurrentTrain: epoch  0, batch     6 | loss: 20.8007450Losses:  21.975143432617188 9.034991264343262 0.574554443359375
CurrentTrain: epoch  0, batch     7 | loss: 21.9751434Losses:  23.164304733276367 10.19125747680664 0.5602355003356934
CurrentTrain: epoch  0, batch     8 | loss: 23.1643047Losses:  21.56110382080078 8.751775741577148 0.5500503778457642
CurrentTrain: epoch  0, batch     9 | loss: 21.5611038Losses:  20.822589874267578 8.096237182617188 0.5469988584518433
CurrentTrain: epoch  0, batch    10 | loss: 20.8225899Losses:  19.90032196044922 7.294508934020996 0.5579805970191956
CurrentTrain: epoch  0, batch    11 | loss: 19.9003220Losses:  21.06570816040039 8.336751937866211 0.5289536714553833
CurrentTrain: epoch  0, batch    12 | loss: 21.0657082Losses:  20.578697204589844 8.216236114501953 0.4648168683052063
CurrentTrain: epoch  0, batch    13 | loss: 20.5786972Losses:  21.27639389038086 9.091619491577148 0.4568149745464325
CurrentTrain: epoch  0, batch    14 | loss: 21.2763939Losses:  19.128314971923828 7.004145622253418 0.44535893201828003
CurrentTrain: epoch  0, batch    15 | loss: 19.1283150Losses:  21.862041473388672 10.270703315734863 0.4009968042373657
CurrentTrain: epoch  0, batch    16 | loss: 21.8620415Losses:  23.932912826538086 12.342273712158203 0.3586016893386841
CurrentTrain: epoch  0, batch    17 | loss: 23.9329128Losses:  25.5299072265625 13.652383804321289 0.40645521879196167
CurrentTrain: epoch  0, batch    18 | loss: 25.5299072Losses:  20.203922271728516 8.276185035705566 0.412800669670105
CurrentTrain: epoch  0, batch    19 | loss: 20.2039223Losses:  19.889785766601562 8.173059463500977 0.4022510349750519
CurrentTrain: epoch  0, batch    20 | loss: 19.8897858Losses:  22.06855583190918 9.937870025634766 0.4104510247707367
CurrentTrain: epoch  0, batch    21 | loss: 22.0685558Losses:  20.912595748901367 8.983195304870605 0.31304672360420227
CurrentTrain: epoch  0, batch    22 | loss: 20.9125957Losses:  19.39810562133789 8.182428359985352 0.31825998425483704
CurrentTrain: epoch  0, batch    23 | loss: 19.3981056Losses:  19.629039764404297 8.23455810546875 0.32000672817230225
CurrentTrain: epoch  0, batch    24 | loss: 19.6290398Losses:  18.632963180541992 7.124813556671143 0.31021809577941895
CurrentTrain: epoch  0, batch    25 | loss: 18.6329632Losses:  20.40264892578125 9.261884689331055 0.33603164553642273
CurrentTrain: epoch  0, batch    26 | loss: 20.4026489Losses:  18.87028694152832 8.227889060974121 0.26623424887657166
CurrentTrain: epoch  0, batch    27 | loss: 18.8702869Losses:  20.935583114624023 9.788764953613281 0.303941547870636
CurrentTrain: epoch  0, batch    28 | loss: 20.9355831Losses:  17.739852905273438 6.583773612976074 0.2770233750343323
CurrentTrain: epoch  0, batch    29 | loss: 17.7398529Losses:  20.613689422607422 9.936427116394043 0.2853127717971802
CurrentTrain: epoch  0, batch    30 | loss: 20.6136894Losses:  19.210586547851562 8.30224323272705 0.29395580291748047
CurrentTrain: epoch  0, batch    31 | loss: 19.2105865Losses:  19.014310836791992 8.715100288391113 0.2783839702606201
CurrentTrain: epoch  0, batch    32 | loss: 19.0143108Losses:  18.607547760009766 8.045026779174805 0.26448094844818115
CurrentTrain: epoch  0, batch    33 | loss: 18.6075478Losses:  21.60893440246582 11.594880104064941 0.19100400805473328
CurrentTrain: epoch  0, batch    34 | loss: 21.6089344Losses:  20.578155517578125 10.329682350158691 0.2349359691143036
CurrentTrain: epoch  0, batch    35 | loss: 20.5781555Losses:  19.20968246459961 8.578742027282715 0.2425384223461151
CurrentTrain: epoch  0, batch    36 | loss: 19.2096825Losses:  11.845403671264648 1.7446783781051636 0.20155882835388184
CurrentTrain: epoch  0, batch    37 | loss: 11.8454037Losses:  16.681373596191406 6.563851356506348 0.2209884077310562
CurrentTrain: epoch  1, batch     0 | loss: 16.6813736Losses:  17.26825523376465 6.601783752441406 0.21898256242275238
CurrentTrain: epoch  1, batch     1 | loss: 17.2682552Losses:  17.094345092773438 7.565075874328613 0.1790357232093811
CurrentTrain: epoch  1, batch     2 | loss: 17.0943451Losses:  15.67198657989502 5.869254112243652 0.21470186114311218
CurrentTrain: epoch  1, batch     3 | loss: 15.6719866Losses:  16.16132354736328 7.010287284851074 0.22521844506263733
CurrentTrain: epoch  1, batch     4 | loss: 16.1613235Losses:  20.673770904541016 10.340675354003906 0.16211232542991638
CurrentTrain: epoch  1, batch     5 | loss: 20.6737709Losses:  16.988306045532227 7.379750728607178 0.2067183554172516
CurrentTrain: epoch  1, batch     6 | loss: 16.9883060Losses:  17.240692138671875 7.693327903747559 0.22415150701999664
CurrentTrain: epoch  1, batch     7 | loss: 17.2406921Losses:  18.556344985961914 9.10007095336914 0.18723469972610474
CurrentTrain: epoch  1, batch     8 | loss: 18.5563450Losses:  16.15349769592285 6.745006561279297 0.17342770099639893
CurrentTrain: epoch  1, batch     9 | loss: 16.1534977Losses:  18.371896743774414 8.577134132385254 0.19236907362937927
CurrentTrain: epoch  1, batch    10 | loss: 18.3718967Losses:  15.883142471313477 5.937032222747803 0.1407412439584732
CurrentTrain: epoch  1, batch    11 | loss: 15.8831425Losses:  16.916257858276367 6.9367499351501465 0.2323269098997116
CurrentTrain: epoch  1, batch    12 | loss: 16.9162579Losses:  14.569223403930664 5.757464408874512 0.16333098709583282
CurrentTrain: epoch  1, batch    13 | loss: 14.5692234Losses:  17.005233764648438 7.415992259979248 0.15301959216594696
CurrentTrain: epoch  1, batch    14 | loss: 17.0052338Losses:  16.074113845825195 6.854150295257568 0.17481209337711334
CurrentTrain: epoch  1, batch    15 | loss: 16.0741138Losses:  16.287206649780273 7.0727691650390625 0.12997698783874512
CurrentTrain: epoch  1, batch    16 | loss: 16.2872066Losses:  18.224153518676758 8.7212553024292 0.1281939595937729
CurrentTrain: epoch  1, batch    17 | loss: 18.2241535Losses:  15.734225273132324 6.638611316680908 0.18284519016742706
CurrentTrain: epoch  1, batch    18 | loss: 15.7342253Losses:  18.375370025634766 8.739011764526367 0.1421445608139038
CurrentTrain: epoch  1, batch    19 | loss: 18.3753700Losses:  16.762134552001953 7.14921236038208 0.1414203941822052
CurrentTrain: epoch  1, batch    20 | loss: 16.7621346Losses:  19.383209228515625 9.681844711303711 0.09777003526687622
CurrentTrain: epoch  1, batch    21 | loss: 19.3832092Losses:  16.254905700683594 7.2042036056518555 0.11338020116090775
CurrentTrain: epoch  1, batch    22 | loss: 16.2549057Losses:  16.157384872436523 6.794975757598877 0.10579262673854828
CurrentTrain: epoch  1, batch    23 | loss: 16.1573849Losses:  16.89996910095215 7.751102924346924 0.11255672574043274
CurrentTrain: epoch  1, batch    24 | loss: 16.8999691Losses:  16.476974487304688 7.724733829498291 0.13233649730682373
CurrentTrain: epoch  1, batch    25 | loss: 16.4769745Losses:  19.191640853881836 9.345849990844727 0.11362624168395996
CurrentTrain: epoch  1, batch    26 | loss: 19.1916409Losses:  18.047746658325195 9.533241271972656 0.12103438377380371
CurrentTrain: epoch  1, batch    27 | loss: 18.0477467Losses:  20.492820739746094 11.449974060058594 0.09923461079597473
CurrentTrain: epoch  1, batch    28 | loss: 20.4928207Losses:  16.114418029785156 7.8910651206970215 0.13114196062088013
CurrentTrain: epoch  1, batch    29 | loss: 16.1144180Losses:  14.776122093200684 5.8517866134643555 0.0989823043346405
CurrentTrain: epoch  1, batch    30 | loss: 14.7761221Losses:  18.895702362060547 9.267267227172852 0.14180469512939453
CurrentTrain: epoch  1, batch    31 | loss: 18.8957024Losses:  15.365225791931152 6.077577114105225 0.08254300802946091
CurrentTrain: epoch  1, batch    32 | loss: 15.3652258Losses:  15.760005950927734 6.901933670043945 0.12052923440933228
CurrentTrain: epoch  1, batch    33 | loss: 15.7600060Losses:  14.957358360290527 6.342330455780029 0.10733550786972046
CurrentTrain: epoch  1, batch    34 | loss: 14.9573584Losses:  14.954917907714844 6.439545154571533 0.09133508056402206
CurrentTrain: epoch  1, batch    35 | loss: 14.9549179Losses:  15.298835754394531 6.451509475708008 0.12006188929080963
CurrentTrain: epoch  1, batch    36 | loss: 15.2988358Losses:  10.097110748291016 1.1443555355072021 0.11944793164730072
CurrentTrain: epoch  1, batch    37 | loss: 10.0971107Losses:  12.549827575683594 4.8256330490112305 0.1031932383775711
CurrentTrain: epoch  2, batch     0 | loss: 12.5498276Losses:  15.449358940124512 6.590486526489258 0.09543931484222412
CurrentTrain: epoch  2, batch     1 | loss: 15.4493589Losses:  14.691153526306152 5.586191177368164 0.094991534948349
CurrentTrain: epoch  2, batch     2 | loss: 14.6911535Losses:  18.143508911132812 9.88566780090332 0.11501652002334595
CurrentTrain: epoch  2, batch     3 | loss: 18.1435089Losses:  18.791473388671875 9.049774169921875 0.09923407435417175
CurrentTrain: epoch  2, batch     4 | loss: 18.7914734Losses:  22.61136817932129 12.787342071533203 0.0898229256272316
CurrentTrain: epoch  2, batch     5 | loss: 22.6113682Losses:  14.197394371032715 5.251014709472656 0.10088753700256348
CurrentTrain: epoch  2, batch     6 | loss: 14.1973944Losses:  16.013809204101562 7.259494304656982 0.089807890355587
CurrentTrain: epoch  2, batch     7 | loss: 16.0138092Losses:  14.93962574005127 6.630683898925781 0.08773836493492126
CurrentTrain: epoch  2, batch     8 | loss: 14.9396257Losses:  14.806646347045898 6.306816577911377 0.08983567357063293
CurrentTrain: epoch  2, batch     9 | loss: 14.8066463Losses:  14.406153678894043 6.358138084411621 0.0982389971613884
CurrentTrain: epoch  2, batch    10 | loss: 14.4061537Losses:  15.500833511352539 6.720797538757324 0.07263952493667603
CurrentTrain: epoch  2, batch    11 | loss: 15.5008335Losses:  13.994988441467285 5.924368381500244 0.08983922004699707
CurrentTrain: epoch  2, batch    12 | loss: 13.9949884Losses:  14.059629440307617 6.062726974487305 0.09090036898851395
CurrentTrain: epoch  2, batch    13 | loss: 14.0596294Losses:  17.32111167907715 9.98587417602539 0.10364887118339539
CurrentTrain: epoch  2, batch    14 | loss: 17.3211117Losses:  15.692609786987305 7.5553412437438965 0.07996978610754013
CurrentTrain: epoch  2, batch    15 | loss: 15.6926098Losses:  15.2481050491333 6.342137813568115 0.0666210949420929
CurrentTrain: epoch  2, batch    16 | loss: 15.2481050Losses:  15.087841987609863 5.923336982727051 0.06425689905881882
CurrentTrain: epoch  2, batch    17 | loss: 15.0878420Losses:  13.873205184936523 5.742640495300293 0.06718520075082779
CurrentTrain: epoch  2, batch    18 | loss: 13.8732052Losses:  14.92306900024414 6.728753089904785 0.07738826423883438
CurrentTrain: epoch  2, batch    19 | loss: 14.9230690Losses:  14.987604141235352 7.074288845062256 0.06407343596220016
CurrentTrain: epoch  2, batch    20 | loss: 14.9876041Losses:  14.236577987670898 6.43219518661499 0.07198747992515564
CurrentTrain: epoch  2, batch    21 | loss: 14.2365780Losses:  14.122085571289062 6.7977447509765625 0.08374782651662827
CurrentTrain: epoch  2, batch    22 | loss: 14.1220856Losses:  12.870366096496582 5.024600982666016 0.07436314225196838
CurrentTrain: epoch  2, batch    23 | loss: 12.8703661Losses:  13.497639656066895 5.479791641235352 0.06816481798887253
CurrentTrain: epoch  2, batch    24 | loss: 13.4976397Losses:  14.295621871948242 5.595289707183838 0.0591067373752594
CurrentTrain: epoch  2, batch    25 | loss: 14.2956219Losses:  14.005599021911621 6.611112594604492 0.0610036626458168
CurrentTrain: epoch  2, batch    26 | loss: 14.0055990Losses:  15.684307098388672 6.573163032531738 0.06580103933811188
CurrentTrain: epoch  2, batch    27 | loss: 15.6843071Losses:  13.248123168945312 6.526228904724121 0.07637203484773636
CurrentTrain: epoch  2, batch    28 | loss: 13.2481232Losses:  13.996313095092773 5.922971725463867 0.06154247745871544
CurrentTrain: epoch  2, batch    29 | loss: 13.9963131Losses:  16.864810943603516 9.004520416259766 0.08669024705886841
CurrentTrain: epoch  2, batch    30 | loss: 16.8648109Losses:  13.062887191772461 5.757694721221924 0.07367672026157379
CurrentTrain: epoch  2, batch    31 | loss: 13.0628872Losses:  16.38637351989746 8.772555351257324 0.07604347169399261
CurrentTrain: epoch  2, batch    32 | loss: 16.3863735Losses:  13.776101112365723 5.872143745422363 0.06138031929731369
CurrentTrain: epoch  2, batch    33 | loss: 13.7761011Losses:  17.83259391784668 8.469175338745117 0.06351478397846222
CurrentTrain: epoch  2, batch    34 | loss: 17.8325939Losses:  15.796531677246094 8.460408210754395 0.06576135754585266
CurrentTrain: epoch  2, batch    35 | loss: 15.7965317Losses:  16.111248016357422 8.058073043823242 0.06451845914125443
CurrentTrain: epoch  2, batch    36 | loss: 16.1112480Losses:  10.223536491394043 2.583271026611328 0.08442529290914536
CurrentTrain: epoch  2, batch    37 | loss: 10.2235365Losses:  14.379966735839844 6.600128173828125 0.061374545097351074
CurrentTrain: epoch  3, batch     0 | loss: 14.3799667Losses:  21.153696060180664 13.708731651306152 0.1212896853685379
CurrentTrain: epoch  3, batch     1 | loss: 21.1536961Losses:  14.323104858398438 6.2781476974487305 0.06105603650212288
CurrentTrain: epoch  3, batch     2 | loss: 14.3231049Losses:  14.308034896850586 5.8739118576049805 0.06624473631381989
CurrentTrain: epoch  3, batch     3 | loss: 14.3080349Losses:  13.425692558288574 5.632133483886719 0.0658080130815506
CurrentTrain: epoch  3, batch     4 | loss: 13.4256926Losses:  14.522281646728516 6.0928754806518555 0.06116481497883797
CurrentTrain: epoch  3, batch     5 | loss: 14.5222816Losses:  16.46607208251953 7.915857315063477 0.07670910656452179
CurrentTrain: epoch  3, batch     6 | loss: 16.4660721Losses:  11.649679183959961 4.787548065185547 0.06688889116048813
CurrentTrain: epoch  3, batch     7 | loss: 11.6496792Losses:  13.115738868713379 5.499449253082275 0.05960731953382492
CurrentTrain: epoch  3, batch     8 | loss: 13.1157389Losses:  12.928825378417969 5.104748725891113 0.050851788371801376
CurrentTrain: epoch  3, batch     9 | loss: 12.9288254Losses:  13.537358283996582 6.153368949890137 0.06640616059303284
CurrentTrain: epoch  3, batch    10 | loss: 13.5373583Losses:  14.5188627243042 7.6926493644714355 0.09301832318305969
CurrentTrain: epoch  3, batch    11 | loss: 14.5188627Losses:  13.02091121673584 4.932470321655273 0.05710650607943535
CurrentTrain: epoch  3, batch    12 | loss: 13.0209112Losses:  14.861547470092773 6.00023078918457 0.06447184830904007
CurrentTrain: epoch  3, batch    13 | loss: 14.8615475Losses:  13.396810531616211 5.6207451820373535 0.06880957633256912
CurrentTrain: epoch  3, batch    14 | loss: 13.3968105Losses:  14.146342277526855 6.449126243591309 0.06173212081193924
CurrentTrain: epoch  3, batch    15 | loss: 14.1463423Losses:  15.138412475585938 6.4769816398620605 0.05461471900343895
CurrentTrain: epoch  3, batch    16 | loss: 15.1384125Losses:  15.117936134338379 7.096402645111084 0.06994496285915375
CurrentTrain: epoch  3, batch    17 | loss: 15.1179361Losses:  13.932092666625977 6.285314083099365 0.056005872786045074
CurrentTrain: epoch  3, batch    18 | loss: 13.9320927Losses:  15.10561752319336 6.731127738952637 0.06229451298713684
CurrentTrain: epoch  3, batch    19 | loss: 15.1056175Losses:  20.585620880126953 12.79985237121582 0.05159373581409454
CurrentTrain: epoch  3, batch    20 | loss: 20.5856209Losses:  13.853235244750977 6.24882173538208 0.06710053980350494
CurrentTrain: epoch  3, batch    21 | loss: 13.8532352Losses:  15.368743896484375 6.82401180267334 0.05717569217085838
CurrentTrain: epoch  3, batch    22 | loss: 15.3687439Losses:  15.079859733581543 6.923610687255859 0.055112142115831375
CurrentTrain: epoch  3, batch    23 | loss: 15.0798597Losses:  12.519676208496094 5.596071243286133 0.06313960254192352
CurrentTrain: epoch  3, batch    24 | loss: 12.5196762Losses:  16.18533706665039 8.898902893066406 0.06189265474677086
CurrentTrain: epoch  3, batch    25 | loss: 16.1853371Losses:  13.365643501281738 6.417815208435059 0.05357450246810913
CurrentTrain: epoch  3, batch    26 | loss: 13.3656435Losses:  16.212844848632812 7.638328552246094 0.04926644265651703
CurrentTrain: epoch  3, batch    27 | loss: 16.2128448Losses:  12.450340270996094 4.695287704467773 0.04708311706781387
CurrentTrain: epoch  3, batch    28 | loss: 12.4503403Losses:  13.568570137023926 7.3741984367370605 0.06922624260187149
CurrentTrain: epoch  3, batch    29 | loss: 13.5685701Losses:  13.174165725708008 5.231393814086914 0.05603072792291641
CurrentTrain: epoch  3, batch    30 | loss: 13.1741657Losses:  14.848431587219238 6.8541059494018555 0.06014009565114975
CurrentTrain: epoch  3, batch    31 | loss: 14.8484316Losses:  16.57463836669922 9.889715194702148 0.06641754508018494
CurrentTrain: epoch  3, batch    32 | loss: 16.5746384Losses:  13.846667289733887 7.472382068634033 0.06742626428604126
CurrentTrain: epoch  3, batch    33 | loss: 13.8466673Losses:  12.861928939819336 5.6894612312316895 0.05628756806254387
CurrentTrain: epoch  3, batch    34 | loss: 12.8619289Losses:  12.731673240661621 5.483989715576172 0.050729021430015564
CurrentTrain: epoch  3, batch    35 | loss: 12.7316732Losses:  13.030635833740234 5.6950860023498535 0.05083003640174866
CurrentTrain: epoch  3, batch    36 | loss: 13.0306358Losses:  10.094674110412598 3.04046893119812 0.06964078545570374
CurrentTrain: epoch  3, batch    37 | loss: 10.0946741Losses:  13.6388578414917 5.868188858032227 0.07080232352018356
CurrentTrain: epoch  4, batch     0 | loss: 13.6388578Losses:  12.936854362487793 5.765562057495117 0.05692099407315254
CurrentTrain: epoch  4, batch     1 | loss: 12.9368544Losses:  13.09318733215332 6.855022430419922 0.061334576457738876
CurrentTrain: epoch  4, batch     2 | loss: 13.0931873Losses:  14.195549964904785 6.723272323608398 0.06312617659568787
CurrentTrain: epoch  4, batch     3 | loss: 14.1955500Losses:  15.892963409423828 8.040987014770508 0.06718074530363083
CurrentTrain: epoch  4, batch     4 | loss: 15.8929634Losses:  12.4418363571167 5.265856742858887 0.0540357381105423
CurrentTrain: epoch  4, batch     5 | loss: 12.4418364Losses:  14.095233917236328 7.638082981109619 0.0497530922293663
CurrentTrain: epoch  4, batch     6 | loss: 14.0952339Losses:  12.165914535522461 4.599689960479736 0.04598385840654373
CurrentTrain: epoch  4, batch     7 | loss: 12.1659145Losses:  18.64980697631836 9.52994155883789 0.06435281038284302
CurrentTrain: epoch  4, batch     8 | loss: 18.6498070Losses:  12.759356498718262 4.8912506103515625 0.04446534812450409
CurrentTrain: epoch  4, batch     9 | loss: 12.7593565Losses:  15.400309562683105 7.8494110107421875 0.04917656630277634
CurrentTrain: epoch  4, batch    10 | loss: 15.4003096Losses:  11.500475883483887 4.947891712188721 0.043397046625614166
CurrentTrain: epoch  4, batch    11 | loss: 11.5004759Losses:  15.709236145019531 7.826562404632568 0.05721689760684967
CurrentTrain: epoch  4, batch    12 | loss: 15.7092361Losses:  11.480772018432617 4.4059858322143555 0.04765596613287926
CurrentTrain: epoch  4, batch    13 | loss: 11.4807720Losses:  16.500701904296875 8.350202560424805 0.04829658567905426
CurrentTrain: epoch  4, batch    14 | loss: 16.5007019Losses:  15.549789428710938 8.293266296386719 0.06122542545199394
CurrentTrain: epoch  4, batch    15 | loss: 15.5497894Losses:  12.379892349243164 5.059671401977539 0.050199881196022034
CurrentTrain: epoch  4, batch    16 | loss: 12.3798923Losses:  13.264849662780762 6.024660110473633 0.0532698892056942
CurrentTrain: epoch  4, batch    17 | loss: 13.2648497Losses:  11.988496780395508 5.2043890953063965 0.048666927963495255
CurrentTrain: epoch  4, batch    18 | loss: 11.9884968Losses:  11.969176292419434 4.660446643829346 0.04071735218167305
CurrentTrain: epoch  4, batch    19 | loss: 11.9691763Losses:  13.08367919921875 6.23945426940918 0.067869633436203
CurrentTrain: epoch  4, batch    20 | loss: 13.0836792Losses:  13.239025115966797 5.831164360046387 0.05056249350309372
CurrentTrain: epoch  4, batch    21 | loss: 13.2390251Losses:  12.28039264678955 4.991103649139404 0.0473150908946991
CurrentTrain: epoch  4, batch    22 | loss: 12.2803926Losses:  12.257163047790527 5.937103271484375 0.06468697637319565
CurrentTrain: epoch  4, batch    23 | loss: 12.2571630Losses:  13.513687133789062 6.045299530029297 0.05513419583439827
CurrentTrain: epoch  4, batch    24 | loss: 13.5136871Losses:  17.229473114013672 8.89603042602539 0.05766227841377258
CurrentTrain: epoch  4, batch    25 | loss: 17.2294731Losses:  12.412206649780273 5.373465538024902 0.0417918860912323
CurrentTrain: epoch  4, batch    26 | loss: 12.4122066Losses:  15.762731552124023 6.480556488037109 0.05669957026839256
CurrentTrain: epoch  4, batch    27 | loss: 15.7627316Losses:  10.85152816772461 3.7928719520568848 0.04649162292480469
CurrentTrain: epoch  4, batch    28 | loss: 10.8515282Losses:  13.731266975402832 6.437259674072266 0.048926688730716705
CurrentTrain: epoch  4, batch    29 | loss: 13.7312670Losses:  14.023506164550781 6.334373474121094 0.08033095300197601
CurrentTrain: epoch  4, batch    30 | loss: 14.0235062Losses:  16.729108810424805 10.079326629638672 0.05030157044529915
CurrentTrain: epoch  4, batch    31 | loss: 16.7291088Losses:  13.558381080627441 5.180228233337402 0.046167369931936264
CurrentTrain: epoch  4, batch    32 | loss: 13.5583811Losses:  13.009918212890625 6.188647747039795 0.0595470629632473
CurrentTrain: epoch  4, batch    33 | loss: 13.0099182Losses:  15.679183006286621 7.225642681121826 0.044240180402994156
CurrentTrain: epoch  4, batch    34 | loss: 15.6791830Losses:  12.686823844909668 5.187812328338623 0.04814010113477707
CurrentTrain: epoch  4, batch    35 | loss: 12.6868238Losses:  12.452570915222168 5.346888542175293 0.047358639538288116
CurrentTrain: epoch  4, batch    36 | loss: 12.4525709Losses:  8.874615669250488 0.993375301361084 0.04589584842324257
CurrentTrain: epoch  4, batch    37 | loss: 8.8746157Losses:  12.897369384765625 6.504293441772461 0.061634838581085205
CurrentTrain: epoch  5, batch     0 | loss: 12.8973694Losses:  13.269571304321289 5.884230613708496 0.04372168704867363
CurrentTrain: epoch  5, batch     1 | loss: 13.2695713Losses:  14.031704902648926 6.205713748931885 0.05276923254132271
CurrentTrain: epoch  5, batch     2 | loss: 14.0317049Losses:  14.050281524658203 6.481691360473633 0.04576307535171509
CurrentTrain: epoch  5, batch     3 | loss: 14.0502815Losses:  12.812910079956055 5.254762649536133 0.05082686245441437
CurrentTrain: epoch  5, batch     4 | loss: 12.8129101Losses:  13.268198013305664 5.812814712524414 0.05124089494347572
CurrentTrain: epoch  5, batch     5 | loss: 13.2681980Losses:  12.403132438659668 4.689204216003418 0.05374838039278984
CurrentTrain: epoch  5, batch     6 | loss: 12.4031324Losses:  13.205946922302246 5.5268049240112305 0.054958924651145935
CurrentTrain: epoch  5, batch     7 | loss: 13.2059469Losses:  13.947978973388672 6.090076446533203 0.046510398387908936
CurrentTrain: epoch  5, batch     8 | loss: 13.9479790Losses:  12.432111740112305 5.400033473968506 0.04868748039007187
CurrentTrain: epoch  5, batch     9 | loss: 12.4321117Losses:  12.451760292053223 5.149831771850586 0.04031640291213989
CurrentTrain: epoch  5, batch    10 | loss: 12.4517603Losses:  12.452910423278809 4.885457992553711 0.04637440666556358
CurrentTrain: epoch  5, batch    11 | loss: 12.4529104Losses:  11.488569259643555 4.665980815887451 0.04124060645699501
CurrentTrain: epoch  5, batch    12 | loss: 11.4885693Losses:  11.813199996948242 4.479109287261963 0.041280873119831085
CurrentTrain: epoch  5, batch    13 | loss: 11.8132000Losses:  12.355844497680664 4.546241760253906 0.051183417439460754
CurrentTrain: epoch  5, batch    14 | loss: 12.3558445Losses:  15.206583976745605 7.768408298492432 0.04544587805867195
CurrentTrain: epoch  5, batch    15 | loss: 15.2065840Losses:  12.203655242919922 6.250925540924072 0.05679091811180115
CurrentTrain: epoch  5, batch    16 | loss: 12.2036552Losses:  12.592265129089355 6.179316997528076 0.05499345064163208
CurrentTrain: epoch  5, batch    17 | loss: 12.5922651Losses:  14.080955505371094 6.38789176940918 0.04553421586751938
CurrentTrain: epoch  5, batch    18 | loss: 14.0809555Losses:  15.670299530029297 7.2615532875061035 0.04933742433786392
CurrentTrain: epoch  5, batch    19 | loss: 15.6702995Losses:  11.677617073059082 5.343869209289551 0.051817137748003006
CurrentTrain: epoch  5, batch    20 | loss: 11.6776171Losses:  12.203232765197754 5.468866348266602 0.04271730035543442
CurrentTrain: epoch  5, batch    21 | loss: 12.2032328Losses:  10.972736358642578 4.477365970611572 0.04689766839146614
CurrentTrain: epoch  5, batch    22 | loss: 10.9727364Losses:  11.11876392364502 4.273497581481934 0.041221559047698975
CurrentTrain: epoch  5, batch    23 | loss: 11.1187639Losses:  13.959465026855469 6.081573486328125 0.04765153303742409
CurrentTrain: epoch  5, batch    24 | loss: 13.9594650Losses:  10.865632057189941 4.594961643218994 0.04464993625879288
CurrentTrain: epoch  5, batch    25 | loss: 10.8656321Losses:  12.10211181640625 5.160085678100586 0.04596296697854996
CurrentTrain: epoch  5, batch    26 | loss: 12.1021118Losses:  16.705018997192383 10.094310760498047 0.05299367383122444
CurrentTrain: epoch  5, batch    27 | loss: 16.7050190Losses:  18.007923126220703 9.004631042480469 0.04102667048573494
CurrentTrain: epoch  5, batch    28 | loss: 18.0079231Losses:  11.067571640014648 3.7802352905273438 0.03665151447057724
CurrentTrain: epoch  5, batch    29 | loss: 11.0675716Losses:  12.428025245666504 6.179388999938965 0.05196721851825714
CurrentTrain: epoch  5, batch    30 | loss: 12.4280252Losses:  13.166901588439941 6.67750883102417 0.061167508363723755
CurrentTrain: epoch  5, batch    31 | loss: 13.1669016Losses:  15.12236213684082 9.41091537475586 0.055447883903980255
CurrentTrain: epoch  5, batch    32 | loss: 15.1223621Losses:  13.762386322021484 6.757865905761719 0.052961453795433044
CurrentTrain: epoch  5, batch    33 | loss: 13.7623863Losses:  10.640857696533203 3.8588695526123047 0.04082859307527542
CurrentTrain: epoch  5, batch    34 | loss: 10.6408577Losses:  13.52189826965332 7.380572319030762 0.03929300233721733
CurrentTrain: epoch  5, batch    35 | loss: 13.5218983Losses:  12.001466751098633 4.716656684875488 0.03717879578471184
CurrentTrain: epoch  5, batch    36 | loss: 12.0014668Losses:  7.202620983123779 1.316162347793579 0.05385535582900047
CurrentTrain: epoch  5, batch    37 | loss: 7.2026210Losses:  11.571502685546875 5.773098945617676 0.045337654650211334
CurrentTrain: epoch  6, batch     0 | loss: 11.5715027Losses:  13.563324928283691 6.245720863342285 0.052813559770584106
CurrentTrain: epoch  6, batch     1 | loss: 13.5633249Losses:  15.680545806884766 8.244842529296875 0.04495023190975189
CurrentTrain: epoch  6, batch     2 | loss: 15.6805458Losses:  11.316670417785645 4.7025146484375 0.04186351224780083
CurrentTrain: epoch  6, batch     3 | loss: 11.3166704Losses:  12.308402061462402 5.926702499389648 0.0549345500767231
CurrentTrain: epoch  6, batch     4 | loss: 12.3084021Losses:  11.560629844665527 5.394756317138672 0.047447092831134796
CurrentTrain: epoch  6, batch     5 | loss: 11.5606298Losses:  16.718233108520508 9.484319686889648 0.045896075665950775
CurrentTrain: epoch  6, batch     6 | loss: 16.7182331Losses:  11.313870429992676 4.540957450866699 0.03792354837059975
CurrentTrain: epoch  6, batch     7 | loss: 11.3138704Losses:  11.83430004119873 5.792568206787109 0.038633283227682114
CurrentTrain: epoch  6, batch     8 | loss: 11.8343000Losses:  11.114747047424316 4.708328723907471 0.04506624862551689
CurrentTrain: epoch  6, batch     9 | loss: 11.1147470Losses:  10.032743453979492 3.990208387374878 0.03938824310898781
CurrentTrain: epoch  6, batch    10 | loss: 10.0327435Losses:  11.44247817993164 5.021182060241699 0.048608504235744476
CurrentTrain: epoch  6, batch    11 | loss: 11.4424782Losses:  10.023289680480957 4.168726444244385 0.04715220630168915
CurrentTrain: epoch  6, batch    12 | loss: 10.0232897Losses:  11.541033744812012 5.786853790283203 0.06538160145282745
CurrentTrain: epoch  6, batch    13 | loss: 11.5410337Losses:  10.639726638793945 4.29793119430542 0.040928326547145844
CurrentTrain: epoch  6, batch    14 | loss: 10.6397266Losses:  10.024368286132812 4.318648338317871 0.04179548844695091
CurrentTrain: epoch  6, batch    15 | loss: 10.0243683Losses:  13.367337226867676 6.724549293518066 0.05781884863972664
CurrentTrain: epoch  6, batch    16 | loss: 13.3673372Losses:  12.654500007629395 6.787648677825928 0.04597042500972748
CurrentTrain: epoch  6, batch    17 | loss: 12.6545000Losses:  11.739568710327148 5.341253280639648 0.048187077045440674
CurrentTrain: epoch  6, batch    18 | loss: 11.7395687Losses:  12.967484474182129 6.990166664123535 0.061832476407289505
CurrentTrain: epoch  6, batch    19 | loss: 12.9674845Losses:  15.611994743347168 8.644462585449219 0.045949872583150864
CurrentTrain: epoch  6, batch    20 | loss: 15.6119947Losses:  11.8108549118042 4.317327499389648 0.04634413495659828
CurrentTrain: epoch  6, batch    21 | loss: 11.8108549Losses:  11.878473281860352 5.5463056564331055 0.048273298889398575
CurrentTrain: epoch  6, batch    22 | loss: 11.8784733Losses:  10.082202911376953 3.743354320526123 0.03745849430561066
CurrentTrain: epoch  6, batch    23 | loss: 10.0822029Losses:  11.757678031921387 6.249102592468262 0.05720428749918938
CurrentTrain: epoch  6, batch    24 | loss: 11.7576780Losses:  15.804153442382812 8.168617248535156 0.06339222192764282
CurrentTrain: epoch  6, batch    25 | loss: 15.8041534Losses:  14.982641220092773 7.909543037414551 0.05035955458879471
CurrentTrain: epoch  6, batch    26 | loss: 14.9826412Losses:  12.060651779174805 6.083666801452637 0.0615895539522171
CurrentTrain: epoch  6, batch    27 | loss: 12.0606518Losses:  13.030901908874512 6.810239791870117 0.06602491438388824
CurrentTrain: epoch  6, batch    28 | loss: 13.0309019Losses:  12.823343276977539 6.6385393142700195 0.05879750847816467
CurrentTrain: epoch  6, batch    29 | loss: 12.8233433Losses:  12.529163360595703 6.226012229919434 0.04545339569449425
CurrentTrain: epoch  6, batch    30 | loss: 12.5291634Losses:  14.551374435424805 6.669133186340332 0.04649879038333893
CurrentTrain: epoch  6, batch    31 | loss: 14.5513744Losses:  10.836047172546387 5.263298034667969 0.04583749175071716
CurrentTrain: epoch  6, batch    32 | loss: 10.8360472Losses:  11.893218040466309 4.943545341491699 0.043372392654418945
CurrentTrain: epoch  6, batch    33 | loss: 11.8932180Losses:  10.401287078857422 4.87747859954834 0.043781135231256485
CurrentTrain: epoch  6, batch    34 | loss: 10.4012871Losses:  10.556368827819824 4.279298782348633 0.0544709637761116
CurrentTrain: epoch  6, batch    35 | loss: 10.5563688Losses:  10.61439037322998 4.458885192871094 0.04352208971977234
CurrentTrain: epoch  6, batch    36 | loss: 10.6143904Losses:  7.17257022857666 0.5206714868545532 0.043214570730924606
CurrentTrain: epoch  6, batch    37 | loss: 7.1725702Losses:  18.353979110717773 11.294157028198242 0.0628238394856453
CurrentTrain: epoch  7, batch     0 | loss: 18.3539791Losses:  10.172459602355957 4.372285842895508 0.04168950021266937
CurrentTrain: epoch  7, batch     1 | loss: 10.1724596Losses:  13.27603816986084 6.934520721435547 0.0606391504406929
CurrentTrain: epoch  7, batch     2 | loss: 13.2760382Losses:  11.93239974975586 6.900469779968262 0.05258969962596893
CurrentTrain: epoch  7, batch     3 | loss: 11.9323997Losses:  12.361150741577148 5.866969108581543 0.05943451449275017
CurrentTrain: epoch  7, batch     4 | loss: 12.3611507Losses:  11.00195026397705 5.342304229736328 0.047551676630973816
CurrentTrain: epoch  7, batch     5 | loss: 11.0019503Losses:  11.490260124206543 4.826471328735352 0.04395928978919983
CurrentTrain: epoch  7, batch     6 | loss: 11.4902601Losses:  10.392969131469727 4.651520252227783 0.03814411908388138
CurrentTrain: epoch  7, batch     7 | loss: 10.3929691Losses:  12.430315017700195 6.098506450653076 0.0494077131152153
CurrentTrain: epoch  7, batch     8 | loss: 12.4303150Losses:  11.779485702514648 6.056028366088867 0.05239163711667061
CurrentTrain: epoch  7, batch     9 | loss: 11.7794857Losses:  10.845199584960938 4.717237949371338 0.045820239931344986
CurrentTrain: epoch  7, batch    10 | loss: 10.8451996Losses:  12.279756546020508 5.678478717803955 0.05504805967211723
CurrentTrain: epoch  7, batch    11 | loss: 12.2797565Losses:  10.65109634399414 4.488014221191406 0.05062369257211685
CurrentTrain: epoch  7, batch    12 | loss: 10.6510963Losses:  11.6542387008667 5.368305206298828 0.048519134521484375
CurrentTrain: epoch  7, batch    13 | loss: 11.6542387Losses:  13.720139503479004 7.427069187164307 0.045207202434539795
CurrentTrain: epoch  7, batch    14 | loss: 13.7201395Losses:  9.764731407165527 4.033633708953857 0.044901736080646515
CurrentTrain: epoch  7, batch    15 | loss: 9.7647314Losses:  10.176881790161133 4.535053253173828 0.042329609394073486
CurrentTrain: epoch  7, batch    16 | loss: 10.1768818Losses:  11.186882019042969 5.732003211975098 0.04646937549114227
CurrentTrain: epoch  7, batch    17 | loss: 11.1868820Losses:  10.771500587463379 5.4873528480529785 0.047989621758461
CurrentTrain: epoch  7, batch    18 | loss: 10.7715006Losses:  11.535089492797852 6.144762992858887 0.04888707026839256
CurrentTrain: epoch  7, batch    19 | loss: 11.5350895Losses:  12.345934867858887 5.865185737609863 0.04324556514620781
CurrentTrain: epoch  7, batch    20 | loss: 12.3459349Losses:  10.444124221801758 5.130692958831787 0.04612354561686516
CurrentTrain: epoch  7, batch    21 | loss: 10.4441242Losses:  12.464742660522461 5.407098770141602 0.04838109016418457
CurrentTrain: epoch  7, batch    22 | loss: 12.4647427Losses:  15.599995613098145 9.433728218078613 0.09353995323181152
CurrentTrain: epoch  7, batch    23 | loss: 15.5999956Losses:  11.039265632629395 5.531931400299072 0.039429616183042526
CurrentTrain: epoch  7, batch    24 | loss: 11.0392656Losses:  12.138117790222168 6.613720417022705 0.05800566077232361
CurrentTrain: epoch  7, batch    25 | loss: 12.1381178Losses:  14.599740982055664 8.774820327758789 0.054004453122615814
CurrentTrain: epoch  7, batch    26 | loss: 14.5997410Losses:  11.629305839538574 6.116415977478027 0.05159345269203186
CurrentTrain: epoch  7, batch    27 | loss: 11.6293058Losses:  14.392234802246094 7.8302412033081055 0.07198874652385712
CurrentTrain: epoch  7, batch    28 | loss: 14.3922348Losses:  10.316468238830566 4.834024429321289 0.03736467659473419
CurrentTrain: epoch  7, batch    29 | loss: 10.3164682Losses:  10.306130409240723 4.819607257843018 0.04156617447733879
CurrentTrain: epoch  7, batch    30 | loss: 10.3061304Losses:  11.395466804504395 6.000797271728516 0.0459398552775383
CurrentTrain: epoch  7, batch    31 | loss: 11.3954668Losses:  9.764922142028809 4.398059844970703 0.047890763729810715
CurrentTrain: epoch  7, batch    32 | loss: 9.7649221Losses:  10.591438293457031 5.376374244689941 0.04294532537460327
CurrentTrain: epoch  7, batch    33 | loss: 10.5914383Losses:  11.675312042236328 6.614248275756836 0.04575123265385628
CurrentTrain: epoch  7, batch    34 | loss: 11.6753120Losses:  8.659830093383789 3.4907617568969727 0.03711435943841934
CurrentTrain: epoch  7, batch    35 | loss: 8.6598301Losses:  10.500957489013672 5.33123779296875 0.049340829253196716
CurrentTrain: epoch  7, batch    36 | loss: 10.5009575Losses:  6.578190803527832 1.566491723060608 0.06357245147228241
CurrentTrain: epoch  7, batch    37 | loss: 6.5781908Losses:  11.101285934448242 6.057458400726318 0.04485341161489487
CurrentTrain: epoch  8, batch     0 | loss: 11.1012859Losses:  9.245832443237305 4.191166877746582 0.038926851004362106
CurrentTrain: epoch  8, batch     1 | loss: 9.2458324Losses:  14.808436393737793 9.862798690795898 0.04799962043762207
CurrentTrain: epoch  8, batch     2 | loss: 14.8084364Losses:  9.231523513793945 4.231715679168701 0.04068782180547714
CurrentTrain: epoch  8, batch     3 | loss: 9.2315235Losses:  9.179291725158691 4.22799015045166 0.04160008206963539
CurrentTrain: epoch  8, batch     4 | loss: 9.1792917Losses:  9.71870231628418 4.703195095062256 0.04269177466630936
CurrentTrain: epoch  8, batch     5 | loss: 9.7187023Losses:  10.941804885864258 6.037097930908203 0.043649110943078995
CurrentTrain: epoch  8, batch     6 | loss: 10.9418049Losses:  12.018689155578613 7.090282917022705 0.047263242304325104
CurrentTrain: epoch  8, batch     7 | loss: 12.0186892Losses:  10.554306030273438 5.615505695343018 0.04317323863506317
CurrentTrain: epoch  8, batch     8 | loss: 10.5543060Losses:  9.872145652770996 4.155301094055176 0.040110453963279724
CurrentTrain: epoch  8, batch     9 | loss: 9.8721457Losses:  12.579780578613281 6.331524848937988 0.06394043564796448
CurrentTrain: epoch  8, batch    10 | loss: 12.5797806Losses:  11.231524467468262 6.03718376159668 0.04242958873510361
CurrentTrain: epoch  8, batch    11 | loss: 11.2315245Losses:  9.434995651245117 4.2847981452941895 0.03600453957915306
CurrentTrain: epoch  8, batch    12 | loss: 9.4349957Losses:  10.080517768859863 5.050829887390137 0.03749839961528778
CurrentTrain: epoch  8, batch    13 | loss: 10.0805178Losses:  11.09351634979248 4.93921422958374 0.04272684082388878
CurrentTrain: epoch  8, batch    14 | loss: 11.0935163Losses:  10.023612022399902 4.617489337921143 0.04229292646050453
CurrentTrain: epoch  8, batch    15 | loss: 10.0236120Losses:  12.222315788269043 7.164017200469971 0.05530408024787903
CurrentTrain: epoch  8, batch    16 | loss: 12.2223158Losses:  14.22872257232666 8.986705780029297 0.048810042440891266
CurrentTrain: epoch  8, batch    17 | loss: 14.2287226Losses:  10.198689460754395 4.635430335998535 0.04765266925096512
CurrentTrain: epoch  8, batch    18 | loss: 10.1986895Losses:  11.017378807067871 6.0165300369262695 0.050155892968177795
CurrentTrain: epoch  8, batch    19 | loss: 11.0173788Losses:  12.434974670410156 6.9067702293396 0.05333691090345383
CurrentTrain: epoch  8, batch    20 | loss: 12.4349747Losses:  10.278295516967773 4.740538597106934 0.03479752689599991
CurrentTrain: epoch  8, batch    21 | loss: 10.2782955Losses:  12.465810775756836 7.403905391693115 0.04782667011022568
CurrentTrain: epoch  8, batch    22 | loss: 12.4658108Losses:  10.924056053161621 4.976737976074219 0.040232814848423004
CurrentTrain: epoch  8, batch    23 | loss: 10.9240561Losses:  11.219626426696777 5.802973747253418 0.03908655792474747
CurrentTrain: epoch  8, batch    24 | loss: 11.2196264Losses:  10.214580535888672 5.068211555480957 0.04690728336572647
CurrentTrain: epoch  8, batch    25 | loss: 10.2145805Losses:  13.648488998413086 6.944785118103027 0.044301994144916534
CurrentTrain: epoch  8, batch    26 | loss: 13.6484890Losses:  11.406250953674316 6.423225402832031 0.05159429833292961
CurrentTrain: epoch  8, batch    27 | loss: 11.4062510Losses:  10.61247730255127 5.400761127471924 0.03840702772140503
CurrentTrain: epoch  8, batch    28 | loss: 10.6124773Losses:  13.775028228759766 8.305689811706543 0.0595795214176178
CurrentTrain: epoch  8, batch    29 | loss: 13.7750282Losses:  9.967694282531738 5.099582672119141 0.039301712065935135
CurrentTrain: epoch  8, batch    30 | loss: 9.9676943Losses:  14.801877975463867 9.646875381469727 0.06605254113674164
CurrentTrain: epoch  8, batch    31 | loss: 14.8018780Losses:  10.836819648742676 5.986272811889648 0.047599587589502335
CurrentTrain: epoch  8, batch    32 | loss: 10.8368196Losses:  9.68326187133789 4.613973617553711 0.03705204278230667
CurrentTrain: epoch  8, batch    33 | loss: 9.6832619Losses:  8.722150802612305 3.814552068710327 0.03446612134575844
CurrentTrain: epoch  8, batch    34 | loss: 8.7221508Losses:  10.36518669128418 5.652866363525391 0.04911772161722183
CurrentTrain: epoch  8, batch    35 | loss: 10.3651867Losses:  8.347092628479004 3.4172329902648926 0.03302130103111267
CurrentTrain: epoch  8, batch    36 | loss: 8.3470926Losses:  6.678256988525391 1.6033446788787842 0.06782635301351547
CurrentTrain: epoch  8, batch    37 | loss: 6.6782570Losses:  10.945164680480957 5.213252067565918 0.03272045776247978
CurrentTrain: epoch  9, batch     0 | loss: 10.9451647Losses:  15.018755912780762 9.195784568786621 0.03756814822554588
CurrentTrain: epoch  9, batch     1 | loss: 15.0187559Losses:  8.81583023071289 3.8742170333862305 0.030644841492176056
CurrentTrain: epoch  9, batch     2 | loss: 8.8158302Losses:  10.040724754333496 5.189330101013184 0.048178672790527344
CurrentTrain: epoch  9, batch     3 | loss: 10.0407248Losses:  10.042923927307129 5.064560890197754 0.03527866676449776
CurrentTrain: epoch  9, batch     4 | loss: 10.0429239Losses:  9.879067420959473 5.022988319396973 0.03961696848273277
CurrentTrain: epoch  9, batch     5 | loss: 9.8790674Losses:  10.094539642333984 5.230074882507324 0.03418587148189545
CurrentTrain: epoch  9, batch     6 | loss: 10.0945396Losses:  9.563907623291016 4.650809288024902 0.04163472726941109
CurrentTrain: epoch  9, batch     7 | loss: 9.5639076Losses:  8.345351219177246 3.4515857696533203 0.0303091648966074
CurrentTrain: epoch  9, batch     8 | loss: 8.3453512Losses:  14.645771980285645 9.725364685058594 0.036980412900447845
CurrentTrain: epoch  9, batch     9 | loss: 14.6457720Losses:  10.900511741638184 5.955677032470703 0.05304070562124252
CurrentTrain: epoch  9, batch    10 | loss: 10.9005117Losses:  12.437078475952148 7.60675048828125 0.045791544020175934
CurrentTrain: epoch  9, batch    11 | loss: 12.4370785Losses:  9.814504623413086 4.454162120819092 0.03768553584814072
CurrentTrain: epoch  9, batch    12 | loss: 9.8145046Losses:  9.614054679870605 4.712532997131348 0.041395027190446854
CurrentTrain: epoch  9, batch    13 | loss: 9.6140547Losses:  10.666211128234863 5.755980491638184 0.041215211153030396
CurrentTrain: epoch  9, batch    14 | loss: 10.6662111Losses:  10.005367279052734 4.521249771118164 0.03400037810206413
CurrentTrain: epoch  9, batch    15 | loss: 10.0053673Losses:  10.810829162597656 5.953219413757324 0.04834849387407303
CurrentTrain: epoch  9, batch    16 | loss: 10.8108292Losses:  15.149998664855957 8.256232261657715 0.038739483803510666
CurrentTrain: epoch  9, batch    17 | loss: 15.1499987Losses:  11.611742973327637 6.154958248138428 0.03660525754094124
CurrentTrain: epoch  9, batch    18 | loss: 11.6117430Losses:  10.972533226013184 5.825784683227539 0.04282963648438454
CurrentTrain: epoch  9, batch    19 | loss: 10.9725332Losses:  10.773042678833008 5.529105186462402 0.040521446615457535
CurrentTrain: epoch  9, batch    20 | loss: 10.7730427Losses:  10.916666984558105 6.082855701446533 0.03947705775499344
CurrentTrain: epoch  9, batch    21 | loss: 10.9166670Losses:  12.169917106628418 7.093170166015625 0.041748613119125366
CurrentTrain: epoch  9, batch    22 | loss: 12.1699171Losses:  13.381136894226074 7.990601062774658 0.05081881955265999
CurrentTrain: epoch  9, batch    23 | loss: 13.3811369Losses:  11.427901268005371 6.03435754776001 0.0486714132130146
CurrentTrain: epoch  9, batch    24 | loss: 11.4279013Losses:  10.35578441619873 5.1379170417785645 0.04164604842662811
CurrentTrain: epoch  9, batch    25 | loss: 10.3557844Losses:  12.27419376373291 6.072952747344971 0.05121144652366638
CurrentTrain: epoch  9, batch    26 | loss: 12.2741938Losses:  11.230582237243652 6.2340898513793945 0.04148772358894348
CurrentTrain: epoch  9, batch    27 | loss: 11.2305822Losses:  8.945135116577148 4.18138313293457 0.03633779287338257
CurrentTrain: epoch  9, batch    28 | loss: 8.9451351Losses:  11.770059585571289 6.126330852508545 0.04999363422393799
CurrentTrain: epoch  9, batch    29 | loss: 11.7700596Losses:  13.591240882873535 8.874677658081055 0.05013582482933998
CurrentTrain: epoch  9, batch    30 | loss: 13.5912409Losses:  12.153865814208984 6.999767303466797 0.048050519078969955
CurrentTrain: epoch  9, batch    31 | loss: 12.1538658Losses:  10.815380096435547 5.485726833343506 0.040189582854509354
CurrentTrain: epoch  9, batch    32 | loss: 10.8153801Losses:  10.93748664855957 5.34414005279541 0.03970656543970108
CurrentTrain: epoch  9, batch    33 | loss: 10.9374866Losses:  14.523512840270996 7.352354049682617 0.04004248231649399
CurrentTrain: epoch  9, batch    34 | loss: 14.5235128Losses:  10.071390151977539 5.111560821533203 0.038319554179906845
CurrentTrain: epoch  9, batch    35 | loss: 10.0713902Losses:  11.299003601074219 6.126816272735596 0.03655773401260376
CurrentTrain: epoch  9, batch    36 | loss: 11.2990036Losses:  6.130606174468994 0.6937470436096191 0.05373018980026245
CurrentTrain: epoch  9, batch    37 | loss: 6.1306062
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: head of the atomic energy organization of iran -lrb- aeoi -rrb- ali akbar salehi said thursday that iran has proposed russia to establish a joint nuclear consortium to supply fuel for the bushehr plant and plants to be built in iran , the english language satellite press tv reported .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist decided to return to her roots in Italy, where she felt a deep connection to her heritage.  
Head Entity: the renowned artist  
Tail Entity: Italy  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, John moved to Canada to enjoy a quieter life surrounded by nature and a vibrant community.  
Head Entity: John  
Tail Entity: Canada  

Relation: person countries of residence  
Context: The famous author, known for her best-selling novels, has spent the last decade residing in France, where she draws inspiration from the picturesque landscapes.  
Head Entity: the famous author  
Tail Entity: France  

Relation: person countries of residence  
Context: After completing his studies in Australia, Mark decided to settle in New Zealand, where he found a job that perfectly matched his skills and interests.  
Head Entity: Mark  
Tail Entity: New Zealand  

Relation: person countries of residence  
Context: The diplomat, who has represented his country in various international forums, currently lives in Germany, fostering relationships with European leaders.  
Head Entity: the diplomat  
Tail Entity: Germany  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  

Relation: organization top members employees  
Context: during the annual conference, the president of green earth initiatives, michael green, emphasized the importance of sustainability in the workplace.  
Head Entity: green earth initiatives  
Tail Entity: michael green  

Relation: organization top members employees  
Context: at the charity gala, the founder of helping hands foundation, emily roberts, shared her vision for expanding the organization's outreach programs.  
Head Entity: helping hands foundation  
Tail Entity: emily roberts  

Relation: organization top members employees  
Context: the board chair of global health alliance, dr. john smith, presented the new health policy recommendations to improve community health services.  
Head Entity: global health alliance  
Tail Entity: dr. john smith  

Relation: organization top members employees  
Context: the director of creative arts collective, lila thompson, unveiled a new mentorship program aimed at supporting young artists in the community.  
Head Entity: creative arts collective  
Tail Entity: lila thompson  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: at the request of local environmentalists , the taipei high administrative court ordered the environmental protection administration -lrb- epa -rrb- , the national science council -lrb- nsc -rrb- and the central taiwan science park administration -lrb- ctspa -rrb- july 30 to suspend the development of both the houli and the erlin branches .
Head Entity: nsc
Tail Entity: national science council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The United Nations Educational, Scientific and Cultural Organization (UNESCO) has been a member of various international coalitions aimed at promoting education and cultural preservation.  
Head Entity: UNESCO  
Tail Entity: United Nations Educational, Scientific and Cultural Organization  

Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) collaborates with numerous national societies to provide humanitarian aid worldwide.  
Head Entity: IFRC  
Tail Entity: International Federation of Red Cross and Red Crescent Societies  

Relation: organization member of  
Context: The World Health Organization (WHO) is a member of the Global Health Security Agenda, working alongside other health organizations to combat infectious diseases.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization member of  
Context: The International Olympic Committee (IOC) oversees the Olympic Games and is a member of various sports federations around the globe.  
Head Entity: IOC  
Tail Entity: International Olympic Committee  

Relation: organization member of  
Context: The European Union (EU) is a member of the World Trade Organization (WTO), participating in discussions about global trade policies.  
Head Entity: EU  
Tail Entity: European Union  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in Germany before moving to the United States.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage in her works.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  

Relation: person origin  
Context: The celebrated actor Sofia Vergara has often mentioned her Colombian roots in interviews.  
Head Entity: Sofia Vergara  
Tail Entity: Colombia  

Relation: person origin  
Context: The legendary musician Bob Marley is known for his Jamaican heritage and reggae music.  
Head Entity: Bob Marley  
Tail Entity: Jamaica  

Relation: person origin  
Context: The influential civil rights leader Martin Luther King Jr. was born in the United States and is celebrated for his American legacy.  
Head Entity: Martin Luther King Jr.  
Tail Entity: United States  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving growth."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the conference, Dr. Alan Smith, the lead researcher at BioHealth Labs, presented groundbreaking findings on cancer treatment."  
Head Entity: Dr. Alan Smith  
Tail Entity: lead researcher  

Relation: person title  
Context: "In her new role, Lisa Chen will serve as the Director of Marketing for Green Solutions, focusing on sustainable product development."  
Head Entity: Lisa Chen  
Tail Entity: Director of Marketing  

Relation: person title  
Context: "As the head coach of the national soccer team, John Doe has led the team to several championships over the past decade."  
Head Entity: John Doe  
Tail Entity: head coach  

Relation: person title  
Context: "With over 20 years of experience in the industry, Robert Johnson has taken on the role of Senior Vice President at Global Enterprises."  
Head Entity: Robert Johnson  
Tail Entity: Senior Vice President  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: `` if provocations do not end or military action intensifies , we wo n't just sit there , '' he told russia 's ria-novosti news agency .
Head Entity: ria-novosti
Tail Entity: russia
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: The headquarters of the tech giant is located in Silicon Valley, California, where it has been a major player in the industry.  
Head Entity: tech giant  
Tail Entity: California  

Relation: organization country of headquarters  
Context: The United Nations, based in New York City, plays a crucial role in international diplomacy and peacekeeping.  
Head Entity: United Nations  
Tail Entity: New York  

Relation: organization country of headquarters  
Context: Volkswagen has its main office in Wolfsburg, Germany, where it oversees its global operations.  
Head Entity: Volkswagen  
Tail Entity: Germany  

Relation: organization country of headquarters  
Context: The World Health Organization, headquartered in Geneva, Switzerland, is responsible for global public health initiatives.  
Head Entity: World Health Organization  
Tail Entity: Switzerland  

Relation: organization country of headquarters  
Context: Samsung Electronics operates from its headquarters in Suwon, South Korea, leading the market in technology innovation.  
Head Entity: Samsung Electronics  
Tail Entity: South Korea  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 2 1 1 3 0 1]
Losses:  15.480242729187012 8.14104175567627 0.3793518543243408
CurrentTrain: epoch  0, batch     0 | loss: 15.4802427Losses:  11.408994674682617 2.8118209838867188 0.33216139674186707
CurrentTrain: epoch  0, batch     1 | loss: 11.4089947Losses:  15.435354232788086 7.985107421875 0.3656344413757324
CurrentTrain: epoch  1, batch     0 | loss: 15.4353542Losses:  7.83984375 2.222308397293091 0.30229175090789795
CurrentTrain: epoch  1, batch     1 | loss: 7.8398438Losses:  13.143253326416016 7.225139141082764 0.22293853759765625
CurrentTrain: epoch  2, batch     0 | loss: 13.1432533Losses:  7.912862777709961 2.5627517700195312 0.28098490834236145
CurrentTrain: epoch  2, batch     1 | loss: 7.9128628Losses:  14.002577781677246 8.453619956970215 0.22652851045131683
CurrentTrain: epoch  3, batch     0 | loss: 14.0025778Losses:  8.01799201965332 3.2056150436401367 0.10445260256528854
CurrentTrain: epoch  3, batch     1 | loss: 8.0179920Losses:  11.96462631225586 6.684503078460693 0.2571052312850952
CurrentTrain: epoch  4, batch     0 | loss: 11.9646263Losses:  6.520382881164551 1.9926806688308716 0.20685988664627075
CurrentTrain: epoch  4, batch     1 | loss: 6.5203829Losses:  10.84306526184082 7.465819835662842 0.18381576240062714
CurrentTrain: epoch  5, batch     0 | loss: 10.8430653Losses:  5.735666751861572 2.4455511569976807 0.26386702060699463
CurrentTrain: epoch  5, batch     1 | loss: 5.7356668Losses:  9.789170265197754 6.409958362579346 0.19081032276153564
CurrentTrain: epoch  6, batch     0 | loss: 9.7891703Losses:  4.824623107910156 2.002527952194214 0.18518424034118652
CurrentTrain: epoch  6, batch     1 | loss: 4.8246231Losses:  9.414222717285156 6.736073970794678 0.17061273753643036
CurrentTrain: epoch  7, batch     0 | loss: 9.4142227Losses:  6.144002437591553 3.218719959259033 0.20754879713058472
CurrentTrain: epoch  7, batch     1 | loss: 6.1440024Losses:  9.789116859436035 7.053234100341797 0.1914445161819458
CurrentTrain: epoch  8, batch     0 | loss: 9.7891169Losses:  5.500617027282715 3.0113792419433594 0.1609555184841156
CurrentTrain: epoch  8, batch     1 | loss: 5.5006170Losses:  7.922000885009766 5.40251350402832 0.16714191436767578
CurrentTrain: epoch  9, batch     0 | loss: 7.9220009Losses:  3.5267109870910645 1.2165766954421997 0.17523585259914398
CurrentTrain: epoch  9, batch     1 | loss: 3.5267110
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  

Relation: person date of birth  
Context: the legendary musician freddie mercury was born farrokh bulsara on september 5, 1946, in zanzibar, tanzania.  
Head Entity: freddie mercury  
Tail Entity: september 5, 1946  

Relation: person date of birth  
Context: the influential civil rights leader martin luther king jr. was born on january 15, 1929, in atlanta, georgia.  
Head Entity: martin luther king jr.  
Tail Entity: january 15, 1929  

Relation: person date of birth  
Context: the famous painter pablo picasso was born on october 25, 1881, in malaga, spain.  
Head Entity: pablo picasso  
Tail Entity: october 25, 1881  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: marilyn monroe was born in los angeles, california, on june 1, 1926, and became a famous actress.  
Head Entity: marilyn monroe  
Tail Entity: california  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hawaii, on august 4, 1961, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: hawaii  

Relation: person stateorprovince of birth  
Context: agatha christie was born in torquay, devon, on september 15, 1890, and is known for her detective novels.  
Head Entity: agatha christie  
Tail Entity: devon  

Relation: person stateorprovince of birth  
Context: pablo picasso was born in mlaga, spain, on october 25, 1881, and became one of the most influential artists of the 20th century.  
Head Entity: pablo picasso  
Tail Entity: spain  

Relation: person stateorprovince of birth  
Context: frida kahlo was born in coyoacn, mexico, on july 6, 1907, and is celebrated for her unique painting style.  
Head Entity: frida kahlo  
Tail Entity: mexico  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Michael's father, John Smith, shared heartfelt stories about his son's childhood.  
Head Entity: Michael  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Emily's mother, Sarah Johnson, prepared her famous apple pie that everyone loves.  
Head Entity: Emily  
Tail Entity: Sarah Johnson  

Relation: person parents  
Context: At the graduation party, Lisa's dad, Robert Brown, proudly announced her acceptance into medical school.  
Head Entity: Lisa  
Tail Entity: Robert Brown  

Relation: person parents  
Context: While reminiscing about their childhood, Tom's mother, Patricia White, recalled the summer vacations they spent at the lake.  
Head Entity: Tom  
Tail Entity: Patricia White  

Relation: person parents  
Context: In the documentary, Anna's father, Michael Green, discussed the challenges of raising a child with special needs.  
Head Entity: Anna  
Tail Entity: Michael Green  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech company, where she collaborates with some of the brightest minds in the industry.  
Head Entity: Maria  
Tail Entity: prestigious tech company  

Relation: person employee of  
Context: John has been with the firm for over a decade, contributing to numerous successful projects and earning the respect of his colleagues.  
Head Entity: John  
Tail Entity: the firm  

Relation: person employee of  
Context: As a lead designer at the fashion house, Sarah showcases her creativity and innovation in every collection she produces.  
Head Entity: Sarah  
Tail Entity: the fashion house  

Relation: person employee of  
Context: After graduating from university, Tom accepted a position at a well-known financial institution, where he quickly climbed the corporate ladder.  
Head Entity: Tom  
Tail Entity: well-known financial institution  

Relation: person employee of  
Context: Emily's dedication to her role at the non-profit organization has made a significant impact on the community, helping countless individuals in need.  
Head Entity: Emily  
Tail Entity: non-profit organization  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away unexpectedly in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died peacefully in her home located in bel air, ca.  
Head Entity: elizabeth taylor  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found dead in his home in minneapolis, mn, shocking fans around the world.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that changed the course of history.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  11.705554008483887 5.334738731384277 0.29655587673187256
MemoryTrain:  epoch  0, batch     0 | loss: 11.7055540Losses:  8.025955200195312 2.1450254917144775 0.2992461323738098
MemoryTrain:  epoch  0, batch     1 | loss: 8.0259552Losses:  10.146951675415039 3.718895673751831 0.2018137276172638
MemoryTrain:  epoch  0, batch     2 | loss: 10.1469517Losses:  10.604593276977539 3.3832802772521973 0.3208230137825012
MemoryTrain:  epoch  0, batch     3 | loss: 10.6045933Losses:  3.9859321117401123 -0.0 0.22811976075172424
MemoryTrain:  epoch  0, batch     4 | loss: 3.9859321Losses:  10.142163276672363 2.8423478603363037 0.2947457432746887
MemoryTrain:  epoch  1, batch     0 | loss: 10.1421633Losses:  10.696203231811523 3.989427089691162 0.1558557152748108
MemoryTrain:  epoch  1, batch     1 | loss: 10.6962032Losses:  8.661163330078125 2.8810462951660156 0.2802770435810089
MemoryTrain:  epoch  1, batch     2 | loss: 8.6611633Losses:  8.285100936889648 2.9942946434020996 0.38133537769317627
MemoryTrain:  epoch  1, batch     3 | loss: 8.2851009Losses:  3.4827191829681396 -0.0 0.36086446046829224
MemoryTrain:  epoch  1, batch     4 | loss: 3.4827192Losses:  7.2908172607421875 2.223633050918579 0.18645942211151123
MemoryTrain:  epoch  2, batch     0 | loss: 7.2908173Losses:  8.464341163635254 3.6428050994873047 0.32004401087760925
MemoryTrain:  epoch  2, batch     1 | loss: 8.4643412Losses:  8.213183403015137 3.0126423835754395 0.33485686779022217
MemoryTrain:  epoch  2, batch     2 | loss: 8.2131834Losses:  8.415792465209961 2.6976418495178223 0.29231658577919006
MemoryTrain:  epoch  2, batch     3 | loss: 8.4157925Losses:  2.3000495433807373 -0.0 0.15732033550739288
MemoryTrain:  epoch  2, batch     4 | loss: 2.3000495Losses:  7.319577217102051 4.571494102478027 0.3135615587234497
MemoryTrain:  epoch  3, batch     0 | loss: 7.3195772Losses:  7.781561374664307 3.168844699859619 0.3322717547416687
MemoryTrain:  epoch  3, batch     1 | loss: 7.7815614Losses:  9.211328506469727 3.517979621887207 0.3716779053211212
MemoryTrain:  epoch  3, batch     2 | loss: 9.2113285Losses:  9.976971626281738 6.0839009284973145 0.20499281585216522
MemoryTrain:  epoch  3, batch     3 | loss: 9.9769716Losses:  7.788110256195068 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 7.7881103Losses:  8.688868522644043 4.127544403076172 0.2865857779979706
MemoryTrain:  epoch  4, batch     0 | loss: 8.6888685Losses:  9.325422286987305 4.4957170486450195 0.3569321632385254
MemoryTrain:  epoch  4, batch     1 | loss: 9.3254223Losses:  5.821463108062744 2.416566848754883 0.28104642033576965
MemoryTrain:  epoch  4, batch     2 | loss: 5.8214631Losses:  6.0463151931762695 2.263392448425293 0.20890121161937714
MemoryTrain:  epoch  4, batch     3 | loss: 6.0463152Losses:  7.293379783630371 -0.0 0.07155437022447586
MemoryTrain:  epoch  4, batch     4 | loss: 7.2933798Losses:  7.194293975830078 2.656759738922119 0.1834472417831421
MemoryTrain:  epoch  5, batch     0 | loss: 7.1942940Losses:  5.030355930328369 1.5542263984680176 0.30962127447128296
MemoryTrain:  epoch  5, batch     1 | loss: 5.0303559Losses:  7.957804203033447 3.2751681804656982 0.3367554545402527
MemoryTrain:  epoch  5, batch     2 | loss: 7.9578042Losses:  5.492966175079346 1.9950135946273804 0.3181045651435852
MemoryTrain:  epoch  5, batch     3 | loss: 5.4929662Losses:  1.9468426704406738 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 1.9468427Losses:  5.450529098510742 2.3568854331970215 0.060259804129600525
MemoryTrain:  epoch  6, batch     0 | loss: 5.4505291Losses:  7.839588165283203 4.207571506500244 0.2865114212036133
MemoryTrain:  epoch  6, batch     1 | loss: 7.8395882Losses:  6.270343780517578 3.4141693115234375 0.3407924175262451
MemoryTrain:  epoch  6, batch     2 | loss: 6.2703438Losses:  8.562365531921387 4.247450828552246 0.1884397715330124
MemoryTrain:  epoch  6, batch     3 | loss: 8.5623655Losses:  3.6390786170959473 -0.0 0.10631786286830902
MemoryTrain:  epoch  6, batch     4 | loss: 3.6390786Losses:  5.915499687194824 2.8675971031188965 0.3332046568393707
MemoryTrain:  epoch  7, batch     0 | loss: 5.9154997Losses:  6.934886932373047 4.3135223388671875 0.2865264415740967
MemoryTrain:  epoch  7, batch     1 | loss: 6.9348869Losses:  6.427268981933594 2.7756824493408203 0.25888293981552124
MemoryTrain:  epoch  7, batch     2 | loss: 6.4272690Losses:  4.948677062988281 2.240400791168213 0.19209198653697968
MemoryTrain:  epoch  7, batch     3 | loss: 4.9486771Losses:  3.646493434906006 -0.0 0.0837862491607666
MemoryTrain:  epoch  7, batch     4 | loss: 3.6464934Losses:  5.757976531982422 2.864266872406006 0.2727017402648926
MemoryTrain:  epoch  8, batch     0 | loss: 5.7579765Losses:  4.41184663772583 1.821579933166504 0.32397401332855225
MemoryTrain:  epoch  8, batch     1 | loss: 4.4118466Losses:  5.364590167999268 2.3755905628204346 0.320995956659317
MemoryTrain:  epoch  8, batch     2 | loss: 5.3645902Losses:  4.994659900665283 2.3657307624816895 0.17740873992443085
MemoryTrain:  epoch  8, batch     3 | loss: 4.9946599Losses:  2.588726758956909 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.5887268Losses:  4.153841018676758 1.590133786201477 0.20391711592674255
MemoryTrain:  epoch  9, batch     0 | loss: 4.1538410Losses:  4.921629905700684 2.1237056255340576 0.1814355105161667
MemoryTrain:  epoch  9, batch     1 | loss: 4.9216299Losses:  6.167572975158691 3.807363986968994 0.313060998916626
MemoryTrain:  epoch  9, batch     2 | loss: 6.1675730Losses:  4.884148120880127 2.3556041717529297 0.19117659330368042
MemoryTrain:  epoch  9, batch     3 | loss: 4.8841481Losses:  2.7911713123321533 -0.0 0.09875061362981796
MemoryTrain:  epoch  9, batch     4 | loss: 2.7911713
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 87.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 88.18%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 86.30%   
cur_acc:  ['0.8485', '0.8795']
his_acc:  ['0.8485', '0.8630']
Clustering into  7  clusters
Clusters:  [1 0 2 1 0 4 6 1 5 0 1 1 2 2 3 2]
Losses:  15.226107597351074 8.051651954650879 0.2913562059402466
CurrentTrain: epoch  0, batch     0 | loss: 15.2261076Losses:  12.975377082824707 4.284668922424316 0.2755027115345001
CurrentTrain: epoch  0, batch     1 | loss: 12.9753771Losses:  13.22933578491211 6.608489990234375 0.22824445366859436
CurrentTrain: epoch  1, batch     0 | loss: 13.2293358Losses:  11.342177391052246 3.513484239578247 0.3016819655895233
CurrentTrain: epoch  1, batch     1 | loss: 11.3421774Losses:  14.089207649230957 7.495888710021973 0.257598876953125
CurrentTrain: epoch  2, batch     0 | loss: 14.0892076Losses:  7.1981401443481445 1.8533766269683838 0.2278091162443161
CurrentTrain: epoch  2, batch     1 | loss: 7.1981401Losses:  12.228480339050293 7.099356651306152 0.2467777132987976
CurrentTrain: epoch  3, batch     0 | loss: 12.2284803Losses:  8.480128288269043 2.1221296787261963 0.2652568519115448
CurrentTrain: epoch  3, batch     1 | loss: 8.4801283Losses:  12.025723457336426 6.48841667175293 0.2048632949590683
CurrentTrain: epoch  4, batch     0 | loss: 12.0257235Losses:  6.979622840881348 2.628047466278076 0.254650741815567
CurrentTrain: epoch  4, batch     1 | loss: 6.9796228Losses:  10.7952880859375 6.418986797332764 0.19926071166992188
CurrentTrain: epoch  5, batch     0 | loss: 10.7952881Losses:  7.609441757202148 3.0409839153289795 0.225581094622612
CurrentTrain: epoch  5, batch     1 | loss: 7.6094418Losses:  10.572439193725586 6.444286346435547 0.22269460558891296
CurrentTrain: epoch  6, batch     0 | loss: 10.5724392Losses:  5.845552444458008 1.591520071029663 0.19022393226623535
CurrentTrain: epoch  6, batch     1 | loss: 5.8455524Losses:  12.972487449645996 8.189203262329102 0.20094776153564453
CurrentTrain: epoch  7, batch     0 | loss: 12.9724874Losses:  5.8707594871521 3.0246708393096924 0.19150862097740173
CurrentTrain: epoch  7, batch     1 | loss: 5.8707595Losses:  10.315547943115234 6.634852886199951 0.19179871678352356
CurrentTrain: epoch  8, batch     0 | loss: 10.3155479Losses:  6.85701847076416 2.646467685699463 0.19463074207305908
CurrentTrain: epoch  8, batch     1 | loss: 6.8570185Losses:  9.944604873657227 7.357756614685059 0.20035094022750854
CurrentTrain: epoch  9, batch     0 | loss: 9.9446049Losses:  10.988061904907227 5.696646690368652 0.1485845446586609
CurrentTrain: epoch  9, batch     1 | loss: 10.9880619
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous artist, who was born in spain but later moved to france to pursue his career.  
Head Entity: the famous artist  
Tail Entity: spain  

Relation: person country of birth  
Context: as a child, emma often spoke about her early years in nigeria, where she was born before her family relocated to canada.  
Head Entity: emma  
Tail Entity: nigeria  

Relation: person country of birth  
Context: the biography revealed that the renowned scientist was born in japan, a fact that influenced his research on environmental issues.  
Head Entity: the renowned scientist  
Tail Entity: japan  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information, check out the website at https://www.wikipedia.org.  
Head Entity: Wikipedia  
Tail Entity: https://www.wikipedia.org  

Relation: organization website  
Context: The tech giant's homepage can be found at http://www.apple.com.  
Head Entity: Apple  
Tail Entity: http://www.apple.com  

Relation: organization website  
Context: Explore the latest news on their site: https://www.bbc.com.  
Head Entity: BBC  
Tail Entity: https://www.bbc.com  

Relation: organization website  
Context: You can find their services at https://www.microsoft.com.  
Head Entity: Microsoft  
Tail Entity: https://www.microsoft.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  

Relation: organization shareholders  
Context: the renowned investment firm blackrock has acquired a substantial stake in the renewable energy company nextera energy.  
Head Entity: nextera energy  
Tail Entity: blackrock  

Relation: organization shareholders  
Context: the popular streaming service netflix has received funding from the venture capital firm sequoia capital.  
Head Entity: netflix  
Tail Entity: sequoia capital  

Relation: organization shareholders  
Context: the pharmaceutical company pfizer has been partially owned by the investment group fidelity investments for several years.  
Head Entity: pfizer  
Tail Entity: fidelity investments  

Relation: organization shareholders  
Context: the automotive manufacturer tesla has attracted investments from the tech company google.  
Head Entity: tesla  
Tail Entity: google  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local charity, Helping Hands, announced its closure in January 2019, leaving many in the community without support.  
Head Entity: Helping Hands  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic publishing house, Classic Reads, was dissolved in July 2021, marking the end of an era in literary history.  
Head Entity: Classic Reads  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the non-profit organization, Clean Water Initiative, was officially dissolved in February 2022.  
Head Entity: Clean Water Initiative  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The beloved local theater company, Stage Dreams, announced its dissolution in October 2018 due to declining ticket sales and lack of funding.  
Head Entity: Stage Dreams  
Tail Entity: October 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: The tech startup, GreenTech Solutions, was co-founded by environmentalist Mark Thompson and his partner Sarah Lee to develop sustainable energy solutions.  
Head Entity: GreenTech Solutions  
Tail Entity: Mark Thompson  

Relation: organization founded by  
Context: In 1998, the non-profit organization, Clean Oceans Initiative, was founded by marine biologist Dr. Lisa Carter to combat ocean pollution.  
Head Entity: Clean Oceans Initiative  
Tail Entity: Dr. Lisa Carter  

Relation: organization founded by  
Context: The educational institution, Future Leaders Academy, was established in 2015 by former mayor and educator, Robert Garcia, to empower youth through leadership training.  
Head Entity: Future Leaders Academy  
Tail Entity: Robert Garcia  
Losses:  7.690607070922852 1.5999197959899902 0.48821204900741577
MemoryTrain:  epoch  0, batch     0 | loss: 7.6906071Losses:  6.004996299743652 1.0619921684265137 0.4239278733730316
MemoryTrain:  epoch  0, batch     1 | loss: 6.0049963Losses:  5.881938457489014 0.865071713924408 0.6337596774101257
MemoryTrain:  epoch  0, batch     2 | loss: 5.8819385Losses:  7.034947395324707 2.5126571655273438 0.5328406095504761
MemoryTrain:  epoch  0, batch     3 | loss: 7.0349474Losses:  7.150362491607666 1.2354164123535156 0.6121000647544861
MemoryTrain:  epoch  0, batch     4 | loss: 7.1503625Losses:  6.501596450805664 1.2213430404663086 0.6349802017211914
MemoryTrain:  epoch  0, batch     5 | loss: 6.5015965Losses:  6.446406841278076 1.0457179546356201 0.5021805167198181
MemoryTrain:  epoch  1, batch     0 | loss: 6.4464068Losses:  7.917309284210205 2.445101261138916 0.3140467405319214
MemoryTrain:  epoch  1, batch     1 | loss: 7.9173093Losses:  6.210970878601074 1.8470616340637207 0.5108985900878906
MemoryTrain:  epoch  1, batch     2 | loss: 6.2109709Losses:  9.162847518920898 3.29266357421875 0.5641373991966248
MemoryTrain:  epoch  1, batch     3 | loss: 9.1628475Losses:  6.74007511138916 1.2682372331619263 0.3455532193183899
MemoryTrain:  epoch  1, batch     4 | loss: 6.7400751Losses:  5.242829322814941 1.0557034015655518 0.630359411239624
MemoryTrain:  epoch  1, batch     5 | loss: 5.2428293Losses:  6.339935779571533 1.3229172229766846 0.43221911787986755
MemoryTrain:  epoch  2, batch     0 | loss: 6.3399358Losses:  7.023126125335693 1.9373116493225098 0.386451780796051
MemoryTrain:  epoch  2, batch     1 | loss: 7.0231261Losses:  7.665503025054932 1.6756470203399658 0.548839271068573
MemoryTrain:  epoch  2, batch     2 | loss: 7.6655030Losses:  6.373917579650879 1.616647720336914 0.4575713276863098
MemoryTrain:  epoch  2, batch     3 | loss: 6.3739176Losses:  6.963322162628174 1.7769556045532227 0.5575176477432251
MemoryTrain:  epoch  2, batch     4 | loss: 6.9633222Losses:  6.013444423675537 1.935987114906311 0.5285540819168091
MemoryTrain:  epoch  2, batch     5 | loss: 6.0134444Losses:  5.912109375 1.0214886665344238 0.5193614959716797
MemoryTrain:  epoch  3, batch     0 | loss: 5.9121094Losses:  6.604931354522705 1.4347569942474365 0.3896680772304535
MemoryTrain:  epoch  3, batch     1 | loss: 6.6049314Losses:  6.180882453918457 2.0554232597351074 0.6159148216247559
MemoryTrain:  epoch  3, batch     2 | loss: 6.1808825Losses:  5.871657371520996 1.8506121635437012 0.553122341632843
MemoryTrain:  epoch  3, batch     3 | loss: 5.8716574Losses:  6.352721214294434 2.3847854137420654 0.5602216124534607
MemoryTrain:  epoch  3, batch     4 | loss: 6.3527212Losses:  6.539414882659912 2.3428168296813965 0.45710858702659607
MemoryTrain:  epoch  3, batch     5 | loss: 6.5394149Losses:  6.611082077026367 2.281235933303833 0.3617513179779053
MemoryTrain:  epoch  4, batch     0 | loss: 6.6110821Losses:  4.959573745727539 0.7648652791976929 0.5498397946357727
MemoryTrain:  epoch  4, batch     1 | loss: 4.9595737Losses:  4.7256011962890625 1.067152500152588 0.6137813329696655
MemoryTrain:  epoch  4, batch     2 | loss: 4.7256012Losses:  6.31065559387207 2.208263874053955 0.41894465684890747
MemoryTrain:  epoch  4, batch     3 | loss: 6.3106556Losses:  4.638423919677734 1.9862616062164307 0.41111013293266296
MemoryTrain:  epoch  4, batch     4 | loss: 4.6384239Losses:  6.161247730255127 1.7567601203918457 0.5381752848625183
MemoryTrain:  epoch  4, batch     5 | loss: 6.1612477Losses:  5.039784908294678 1.0561579465866089 0.48910459876060486
MemoryTrain:  epoch  5, batch     0 | loss: 5.0397849Losses:  5.13161039352417 1.4785596132278442 0.5105465650558472
MemoryTrain:  epoch  5, batch     1 | loss: 5.1316104Losses:  4.426113128662109 1.0237066745758057 0.5655086040496826
MemoryTrain:  epoch  5, batch     2 | loss: 4.4261131Losses:  6.02870512008667 1.793100118637085 0.4866805076599121
MemoryTrain:  epoch  5, batch     3 | loss: 6.0287051Losses:  5.306599140167236 2.6566519737243652 0.4109761416912079
MemoryTrain:  epoch  5, batch     4 | loss: 5.3065991Losses:  5.200177192687988 1.3530619144439697 0.4398798942565918
MemoryTrain:  epoch  5, batch     5 | loss: 5.2001772Losses:  5.055685043334961 1.2662231922149658 0.5225382447242737
MemoryTrain:  epoch  6, batch     0 | loss: 5.0556850Losses:  5.149840354919434 1.2238006591796875 0.5733742713928223
MemoryTrain:  epoch  6, batch     1 | loss: 5.1498404Losses:  4.138492107391357 1.025362491607666 0.392435759305954
MemoryTrain:  epoch  6, batch     2 | loss: 4.1384921Losses:  4.682090759277344 1.7474523782730103 0.31468236446380615
MemoryTrain:  epoch  6, batch     3 | loss: 4.6820908Losses:  4.187195777893066 0.9825000762939453 0.5368160605430603
MemoryTrain:  epoch  6, batch     4 | loss: 4.1871958Losses:  5.141833782196045 1.5512936115264893 0.5568451285362244
MemoryTrain:  epoch  6, batch     5 | loss: 5.1418338Losses:  4.561307430267334 1.3824546337127686 0.45931100845336914
MemoryTrain:  epoch  7, batch     0 | loss: 4.5613074Losses:  4.182888031005859 1.6601147651672363 0.5487486124038696
MemoryTrain:  epoch  7, batch     1 | loss: 4.1828880Losses:  6.587093353271484 2.5441811084747314 0.18747380375862122
MemoryTrain:  epoch  7, batch     2 | loss: 6.5870934Losses:  4.113813400268555 1.0119438171386719 0.6076311469078064
MemoryTrain:  epoch  7, batch     3 | loss: 4.1138134Losses:  5.29091739654541 2.1096386909484863 0.47527146339416504
MemoryTrain:  epoch  7, batch     4 | loss: 5.2909174Losses:  4.343910217285156 1.4037493467330933 0.3955644369125366
MemoryTrain:  epoch  7, batch     5 | loss: 4.3439102Losses:  5.887613296508789 2.718726396560669 0.4449172914028168
MemoryTrain:  epoch  8, batch     0 | loss: 5.8876133Losses:  4.707970142364502 1.8806989192962646 0.4299912452697754
MemoryTrain:  epoch  8, batch     1 | loss: 4.7079701Losses:  5.161551475524902 2.4651293754577637 0.5003575086593628
MemoryTrain:  epoch  8, batch     2 | loss: 5.1615515Losses:  4.052886009216309 1.2354152202606201 0.43770667910575867
MemoryTrain:  epoch  8, batch     3 | loss: 4.0528860Losses:  4.55148983001709 1.6852171421051025 0.553078830242157
MemoryTrain:  epoch  8, batch     4 | loss: 4.5514898Losses:  3.6229631900787354 1.1328057050704956 0.2759464383125305
MemoryTrain:  epoch  8, batch     5 | loss: 3.6229632Losses:  4.561684608459473 1.93290114402771 0.4899463653564453
MemoryTrain:  epoch  9, batch     0 | loss: 4.5616846Losses:  4.537535667419434 1.577868103981018 0.48872673511505127
MemoryTrain:  epoch  9, batch     1 | loss: 4.5375357Losses:  4.672519207000732 2.4897243976593018 0.28455400466918945
MemoryTrain:  epoch  9, batch     2 | loss: 4.6725192Losses:  6.035031795501709 3.378256320953369 0.4277394711971283
MemoryTrain:  epoch  9, batch     3 | loss: 6.0350318Losses:  5.450138092041016 2.5212526321411133 0.48673367500305176
MemoryTrain:  epoch  9, batch     4 | loss: 5.4501381Losses:  5.099104881286621 2.3021531105041504 0.3278465270996094
MemoryTrain:  epoch  9, batch     5 | loss: 5.0991049
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 37.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 82.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 78.66%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 77.31%   
cur_acc:  ['0.8485', '0.8795', '0.3750']
his_acc:  ['0.8485', '0.8630', '0.7731']
Clustering into  9  clusters
Clusters:  [0 3 2 0 8 5 6 0 1 3 0 0 2 2 4 2 7 2 5 1 0]
Losses:  12.10177230834961 7.714545726776123 0.4816344380378723
CurrentTrain: epoch  0, batch     0 | loss: 12.1017723Losses:  9.118268013000488 3.9265670776367188 0.4650546610355377
CurrentTrain: epoch  0, batch     1 | loss: 9.1182680Losses:  10.115608215332031 6.052546501159668 0.4352574348449707
CurrentTrain: epoch  1, batch     0 | loss: 10.1156082Losses:  5.058262348175049 1.283925175666809 0.4500909447669983
CurrentTrain: epoch  1, batch     1 | loss: 5.0582623Losses:  10.799400329589844 7.380656719207764 0.42964550852775574
CurrentTrain: epoch  2, batch     0 | loss: 10.7994003Losses:  5.641404151916504 2.8033268451690674 0.33342403173446655
CurrentTrain: epoch  2, batch     1 | loss: 5.6414042Losses:  9.12759017944336 6.026663780212402 0.40987101197242737
CurrentTrain: epoch  3, batch     0 | loss: 9.1275902Losses:  4.4330267906188965 1.7527775764465332 0.41313251852989197
CurrentTrain: epoch  3, batch     1 | loss: 4.4330268Losses:  8.834975242614746 6.068147659301758 0.4086233675479889
CurrentTrain: epoch  4, batch     0 | loss: 8.8349752Losses:  3.948915719985962 1.565240502357483 0.39246830344200134
CurrentTrain: epoch  4, batch     1 | loss: 3.9489157Losses:  8.869263648986816 6.246793746948242 0.40249964594841003
CurrentTrain: epoch  5, batch     0 | loss: 8.8692636Losses:  4.5201568603515625 2.045971155166626 0.29798197746276855
CurrentTrain: epoch  5, batch     1 | loss: 4.5201569Losses:  11.183117866516113 7.51973295211792 0.41068145632743835
CurrentTrain: epoch  6, batch     0 | loss: 11.1831179Losses:  5.584622859954834 2.831865072250366 0.38579902052879333
CurrentTrain: epoch  6, batch     1 | loss: 5.5846229Losses:  8.996040344238281 5.733262062072754 0.3887481391429901
CurrentTrain: epoch  7, batch     0 | loss: 8.9960403Losses:  3.9032111167907715 1.651197075843811 0.39307498931884766
CurrentTrain: epoch  7, batch     1 | loss: 3.9032111Losses:  7.674133777618408 5.4705657958984375 0.37724754214286804
CurrentTrain: epoch  8, batch     0 | loss: 7.6741338Losses:  3.828336000442505 1.6707899570465088 0.3904455602169037
CurrentTrain: epoch  8, batch     1 | loss: 3.8283360Losses:  7.988203048706055 5.85753059387207 0.37006646394729614
CurrentTrain: epoch  9, batch     0 | loss: 7.9882030Losses:  4.049356460571289 1.9576165676116943 0.3269258141517639
CurrentTrain: epoch  9, batch     1 | loss: 4.0493565
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned artist, elena smith, tragically lost her life due to a car accident while returning from an exhibition.  
Head Entity: elena smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved mayor, who died from a sudden stroke during a city council meeting.  
Head Entity: the mayor  
Tail Entity: stroke  

Relation: person cause of death  
Context: after a courageous fight against pancreatic cancer, sarah jones passed away, leaving behind a legacy of kindness and compassion.  
Head Entity: sarah jones  
Tail Entity: pancreatic cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: The Catholic Church has been actively involved in various social justice initiatives, reflecting its commitment to the teachings of Christianity.  
Head Entity: Catholic Church  
Tail Entity: Christianity  

Relation: organization political religious affiliation  
Context: The annual gathering of the National Council of Churches highlighted the role of religious organizations in advocating for peace and justice in society.  
Head Entity: National Council of Churches  
Tail Entity: Christianity  

Relation: organization political religious affiliation  
Context: The Jewish Federation's outreach programs aim to connect the community with the values and teachings of Judaism.  
Head Entity: Jewish Federation  
Tail Entity: Judaism  

Relation: organization political religious affiliation  
Context: The World Sikh Organization has been working to promote awareness of Sikh values and beliefs in various political discussions.  
Head Entity: World Sikh Organization  
Tail Entity: Sikhism  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon has become a leader in e-commerce and cloud computing.  
Head Entity: amazon  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is known for its electronics and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: based in redmond, washington, microsoft is a major player in software development and technology solutions.  
Head Entity: microsoft  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the famous fast-food chain mcdonald's has its headquarters in chicago, illinois, serving millions of customers worldwide.  
Head Entity: mcdonald's  
Tail Entity: illinois  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close relationship despite the challenges of fame.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who is now pursuing a career in photography.  
Head Entity: emily  
Tail Entity: uncle bob  

Relation: person other family  
Context: the documentary highlighted the bond between singer taylor swift and her younger brother, austin swift, showcasing their supportive relationship throughout her career.  
Head Entity: austin swift  
Tail Entity: taylor swift  

Relation: person other family  
Context: at the wedding, the bride's father, mr. johnson, gave a heartfelt speech about his daughter, sarah, and how proud he is of her accomplishments.  
Head Entity: sarah  
Tail Entity: mr. johnson  

Relation: person other family  
Context: in her memoir, actress drew barrymore reflects on her childhood and the influence of her grandmother, who played a significant role in her life.  
Head Entity: drew barrymore  
Tail Entity: her grandmother  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, where he had lived for many years while working at the institute for advanced study.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the world.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, leaving a profound impact on the music industry.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  6.251361846923828 1.0896532535552979 0.5772364139556885
MemoryTrain:  epoch  0, batch     0 | loss: 6.2513618Losses:  6.9803996086120605 1.8037519454956055 0.44156503677368164
MemoryTrain:  epoch  0, batch     1 | loss: 6.9803996Losses:  6.069705963134766 1.5203723907470703 0.5849593281745911
MemoryTrain:  epoch  0, batch     2 | loss: 6.0697060Losses:  6.270519733428955 1.9462157487869263 0.5027536153793335
MemoryTrain:  epoch  0, batch     3 | loss: 6.2705197Losses:  7.466496467590332 1.6327890157699585 0.5080074071884155
MemoryTrain:  epoch  0, batch     4 | loss: 7.4664965Losses:  6.792648792266846 1.8176054954528809 0.5854872465133667
MemoryTrain:  epoch  0, batch     5 | loss: 6.7926488Losses:  7.556576728820801 1.7670327425003052 0.7160073518753052
MemoryTrain:  epoch  0, batch     6 | loss: 7.5565767Losses:  5.465601921081543 0.5289592742919922 0.46580132842063904
MemoryTrain:  epoch  0, batch     7 | loss: 5.4656019Losses:  6.672515869140625 0.9931509494781494 0.6481132507324219
MemoryTrain:  epoch  1, batch     0 | loss: 6.6725159Losses:  6.629546642303467 1.9724221229553223 0.5511940717697144
MemoryTrain:  epoch  1, batch     1 | loss: 6.6295466Losses:  5.5998735427856445 0.8678296208381653 0.5732561349868774
MemoryTrain:  epoch  1, batch     2 | loss: 5.5998735Losses:  6.390400409698486 1.4391522407531738 0.5318194627761841
MemoryTrain:  epoch  1, batch     3 | loss: 6.3904004Losses:  6.060837268829346 0.7286034822463989 0.5062785148620605
MemoryTrain:  epoch  1, batch     4 | loss: 6.0608373Losses:  6.525659084320068 2.4536356925964355 0.6624998450279236
MemoryTrain:  epoch  1, batch     5 | loss: 6.5256591Losses:  4.522029399871826 0.2506406605243683 0.6654102802276611
MemoryTrain:  epoch  1, batch     6 | loss: 4.5220294Losses:  5.164791584014893 1.064633846282959 0.4702673554420471
MemoryTrain:  epoch  1, batch     7 | loss: 5.1647916Losses:  4.354720592498779 0.5643406510353088 0.48446330428123474
MemoryTrain:  epoch  2, batch     0 | loss: 4.3547206Losses:  5.097451686859131 0.5510249137878418 0.5789241194725037
MemoryTrain:  epoch  2, batch     1 | loss: 5.0974517Losses:  5.711514472961426 1.7037577629089355 0.4187886416912079
MemoryTrain:  epoch  2, batch     2 | loss: 5.7115145Losses:  6.611188888549805 1.7084336280822754 0.5102291703224182
MemoryTrain:  epoch  2, batch     3 | loss: 6.6111889Losses:  4.380070209503174 0.7920124530792236 0.4612637460231781
MemoryTrain:  epoch  2, batch     4 | loss: 4.3800702Losses:  5.069764614105225 1.0235347747802734 0.7093107104301453
MemoryTrain:  epoch  2, batch     5 | loss: 5.0697646Losses:  4.812906265258789 0.7407594919204712 0.6357686519622803
MemoryTrain:  epoch  2, batch     6 | loss: 4.8129063Losses:  4.601711273193359 1.3667924404144287 0.4827480912208557
MemoryTrain:  epoch  2, batch     7 | loss: 4.6017113Losses:  4.3689961433410645 1.0483088493347168 0.4935685098171234
MemoryTrain:  epoch  3, batch     0 | loss: 4.3689961Losses:  4.397280693054199 1.4373316764831543 0.4951488673686981
MemoryTrain:  epoch  3, batch     1 | loss: 4.3972807Losses:  4.769601345062256 1.0336158275604248 0.6075169444084167
MemoryTrain:  epoch  3, batch     2 | loss: 4.7696013Losses:  4.413008213043213 0.9620869755744934 0.6513949632644653
MemoryTrain:  epoch  3, batch     3 | loss: 4.4130082Losses:  4.890720367431641 1.0353047847747803 0.5907337665557861
MemoryTrain:  epoch  3, batch     4 | loss: 4.8907204Losses:  4.595696449279785 0.49598219990730286 0.623089075088501
MemoryTrain:  epoch  3, batch     5 | loss: 4.5956964Losses:  6.250674724578857 1.698401927947998 0.6027424931526184
MemoryTrain:  epoch  3, batch     6 | loss: 6.2506747Losses:  4.9152607917785645 0.8331738114356995 0.490740031003952
MemoryTrain:  epoch  3, batch     7 | loss: 4.9152608Losses:  5.733394145965576 1.2944371700286865 0.7301437258720398
MemoryTrain:  epoch  4, batch     0 | loss: 5.7333941Losses:  4.592862606048584 1.0458393096923828 0.4984551668167114
MemoryTrain:  epoch  4, batch     1 | loss: 4.5928626Losses:  4.378387928009033 0.9619501829147339 0.4915434718132019
MemoryTrain:  epoch  4, batch     2 | loss: 4.3783879Losses:  4.334901809692383 0.7913529872894287 0.5780146718025208
MemoryTrain:  epoch  4, batch     3 | loss: 4.3349018Losses:  3.7104744911193848 0.7706677913665771 0.6165107488632202
MemoryTrain:  epoch  4, batch     4 | loss: 3.7104745Losses:  5.737476348876953 1.9896727800369263 0.6133478283882141
MemoryTrain:  epoch  4, batch     5 | loss: 5.7374763Losses:  4.4993085861206055 0.7308779954910278 0.4967876374721527
MemoryTrain:  epoch  4, batch     6 | loss: 4.4993086Losses:  4.2360029220581055 0.779694139957428 0.6357537508010864
MemoryTrain:  epoch  4, batch     7 | loss: 4.2360029Losses:  3.8258490562438965 1.003000259399414 0.5283196568489075
MemoryTrain:  epoch  5, batch     0 | loss: 3.8258491Losses:  4.2947821617126465 0.7109524011611938 0.4675186574459076
MemoryTrain:  epoch  5, batch     1 | loss: 4.2947822Losses:  3.87593150138855 1.2817184925079346 0.5123671889305115
MemoryTrain:  epoch  5, batch     2 | loss: 3.8759315Losses:  4.077749729156494 0.7495678663253784 0.5030128359794617
MemoryTrain:  epoch  5, batch     3 | loss: 4.0777497Losses:  4.4888834953308105 1.2748911380767822 0.5824596285820007
MemoryTrain:  epoch  5, batch     4 | loss: 4.4888835Losses:  6.452084064483643 1.757326364517212 0.4786257743835449
MemoryTrain:  epoch  5, batch     5 | loss: 6.4520841Losses:  5.8490777015686035 2.347975730895996 0.701389729976654
MemoryTrain:  epoch  5, batch     6 | loss: 5.8490777Losses:  3.805647611618042 0.5484648942947388 0.5694548487663269
MemoryTrain:  epoch  5, batch     7 | loss: 3.8056476Losses:  3.955687999725342 0.7676601409912109 0.5821508169174194
MemoryTrain:  epoch  6, batch     0 | loss: 3.9556880Losses:  5.86564826965332 2.279527187347412 0.48709970712661743
MemoryTrain:  epoch  6, batch     1 | loss: 5.8656483Losses:  3.89194917678833 1.0177488327026367 0.509863018989563
MemoryTrain:  epoch  6, batch     2 | loss: 3.8919492Losses:  5.337099075317383 1.3894035816192627 0.4511888921260834
MemoryTrain:  epoch  6, batch     3 | loss: 5.3370991Losses:  4.728695392608643 1.6840413808822632 0.4536812901496887
MemoryTrain:  epoch  6, batch     4 | loss: 4.7286954Losses:  5.063888072967529 1.8274435997009277 0.651064395904541
MemoryTrain:  epoch  6, batch     5 | loss: 5.0638881Losses:  3.907834529876709 0.998913049697876 0.630107581615448
MemoryTrain:  epoch  6, batch     6 | loss: 3.9078345Losses:  4.019216060638428 0.7722981572151184 0.5613358020782471
MemoryTrain:  epoch  6, batch     7 | loss: 4.0192161Losses:  3.713972568511963 0.544503927230835 0.5848796963691711
MemoryTrain:  epoch  7, batch     0 | loss: 3.7139726Losses:  4.422569274902344 1.2977869510650635 0.6196153163909912
MemoryTrain:  epoch  7, batch     1 | loss: 4.4225693Losses:  3.8223133087158203 0.9489518404006958 0.584109902381897
MemoryTrain:  epoch  7, batch     2 | loss: 3.8223133Losses:  4.198223114013672 1.1193435192108154 0.5838515162467957
MemoryTrain:  epoch  7, batch     3 | loss: 4.1982231Losses:  3.7742953300476074 0.9436135292053223 0.5719311833381653
MemoryTrain:  epoch  7, batch     4 | loss: 3.7742953Losses:  4.095930099487305 1.4790430068969727 0.5058506727218628
MemoryTrain:  epoch  7, batch     5 | loss: 4.0959301Losses:  4.153627872467041 0.7823060154914856 0.5482391119003296
MemoryTrain:  epoch  7, batch     6 | loss: 4.1536279Losses:  4.462973594665527 1.6151463985443115 0.42818379402160645
MemoryTrain:  epoch  7, batch     7 | loss: 4.4629736Losses:  3.9939684867858887 1.1993834972381592 0.3930233418941498
MemoryTrain:  epoch  8, batch     0 | loss: 3.9939685Losses:  4.501128196716309 1.4417216777801514 0.5705609321594238
MemoryTrain:  epoch  8, batch     1 | loss: 4.5011282Losses:  4.292220592498779 1.510514259338379 0.519457995891571
MemoryTrain:  epoch  8, batch     2 | loss: 4.2922206Losses:  3.9121475219726562 1.0805315971374512 0.6884796619415283
MemoryTrain:  epoch  8, batch     3 | loss: 3.9121475Losses:  3.4043092727661133 0.5007091164588928 0.5050119161605835
MemoryTrain:  epoch  8, batch     4 | loss: 3.4043093Losses:  3.3917911052703857 0.949233889579773 0.4757741093635559
MemoryTrain:  epoch  8, batch     5 | loss: 3.3917911Losses:  3.2834696769714355 0.5579032301902771 0.5744665265083313
MemoryTrain:  epoch  8, batch     6 | loss: 3.2834697Losses:  4.205955505371094 1.7793152332305908 0.5439994931221008
MemoryTrain:  epoch  8, batch     7 | loss: 4.2059555Losses:  4.169781684875488 1.370240569114685 0.47341033816337585
MemoryTrain:  epoch  9, batch     0 | loss: 4.1697817Losses:  3.602839708328247 1.0088696479797363 0.48114103078842163
MemoryTrain:  epoch  9, batch     1 | loss: 3.6028397Losses:  4.143586158752441 1.8210053443908691 0.3399483263492584
MemoryTrain:  epoch  9, batch     2 | loss: 4.1435862Losses:  3.8991432189941406 1.2865140438079834 0.5477908849716187
MemoryTrain:  epoch  9, batch     3 | loss: 3.8991432Losses:  5.445301055908203 2.0781211853027344 0.37536898255348206
MemoryTrain:  epoch  9, batch     4 | loss: 5.4453011Losses:  3.805976629257202 0.7866369485855103 0.6398665904998779
MemoryTrain:  epoch  9, batch     5 | loss: 3.8059766Losses:  3.7353365421295166 0.9373372793197632 0.6948702335357666
MemoryTrain:  epoch  9, batch     6 | loss: 3.7353365Losses:  3.7007761001586914 1.0154836177825928 0.5298744440078735
MemoryTrain:  epoch  9, batch     7 | loss: 3.7007761
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.27%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 76.92%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 76.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 73.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 72.73%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273']
Clustering into  12  clusters
Clusters:  [ 4  1  0  4  8  3  6  2  5  1  2  4 11  0  9  0  7 10  3  5  2  0  0  0
  3  5]
Losses:  18.73726463317871 9.382184982299805 0.28126323223114014
CurrentTrain: epoch  0, batch     0 | loss: 18.7372646Losses:  10.614068031311035 2.63559627532959 0.19501416385173798
CurrentTrain: epoch  0, batch     1 | loss: 10.6140680Losses:  17.64893913269043 9.177844047546387 0.22720399498939514
CurrentTrain: epoch  1, batch     0 | loss: 17.6489391Losses:  11.788480758666992 3.882242441177368 0.2010962963104248
CurrentTrain: epoch  1, batch     1 | loss: 11.7884808Losses:  17.126482009887695 9.219996452331543 0.18149027228355408
CurrentTrain: epoch  2, batch     0 | loss: 17.1264820Losses:  9.004348754882812 2.9816510677337646 0.1831659972667694
CurrentTrain: epoch  2, batch     1 | loss: 9.0043488Losses:  15.02721881866455 7.943259239196777 0.18927906453609467
CurrentTrain: epoch  3, batch     0 | loss: 15.0272188Losses:  8.899301528930664 2.1761906147003174 0.1903529018163681
CurrentTrain: epoch  3, batch     1 | loss: 8.8993015Losses:  12.933361053466797 7.467369079589844 0.18890808522701263
CurrentTrain: epoch  4, batch     0 | loss: 12.9333611Losses:  12.488036155700684 4.6702423095703125 0.0774892121553421
CurrentTrain: epoch  4, batch     1 | loss: 12.4880362Losses:  14.932175636291504 8.378247261047363 0.19396772980690002
CurrentTrain: epoch  5, batch     0 | loss: 14.9321756Losses:  8.691380500793457 3.0418334007263184 0.20501390099525452
CurrentTrain: epoch  5, batch     1 | loss: 8.6913805Losses:  13.404080390930176 7.857137203216553 0.18942700326442719
CurrentTrain: epoch  6, batch     0 | loss: 13.4040804Losses:  10.284402847290039 3.637200355529785 0.0688287764787674
CurrentTrain: epoch  6, batch     1 | loss: 10.2844028Losses:  11.927131652832031 6.88836669921875 0.21224384009838104
CurrentTrain: epoch  7, batch     0 | loss: 11.9271317Losses:  9.503820419311523 2.8720200061798096 0.18927638232707977
CurrentTrain: epoch  7, batch     1 | loss: 9.5038204Losses:  12.837651252746582 7.327031135559082 0.18731705844402313
CurrentTrain: epoch  8, batch     0 | loss: 12.8376513Losses:  7.54429817199707 2.0542099475860596 0.17560380697250366
CurrentTrain: epoch  8, batch     1 | loss: 7.5442982Losses:  11.297778129577637 6.598347187042236 0.18543867766857147
CurrentTrain: epoch  9, batch     0 | loss: 11.2977781Losses:  8.36189079284668 2.8708794116973877 0.1841590404510498
CurrentTrain: epoch  9, batch     1 | loss: 8.3618908
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has transformed the way we consume video content online.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has a number of subsidiaries, one of which is Chevrolet, known for its popular line of trucks and cars.  
Head Entity: General Motors  
Tail Entity: Chevrolet  

Relation: organization subsidiaries  
Context: The multinational conglomerate Procter & Gamble owns various subsidiaries, including Gillette, which specializes in personal care products.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  

Relation: organization subsidiaries  
Context: The beverage company Coca-Cola has several subsidiaries, one of which is Minute Maid, famous for its fruit juices and drinks.  
Head Entity: Coca-Cola  
Tail Entity: Minute Maid  

Relation: organization subsidiaries  
Context: The financial services corporation Berkshire Hathaway has numerous subsidiaries, including Geico, which is well-known for its auto insurance services.  
Head Entity: Berkshire Hathaway  
Tail Entity: Geico  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is the parent company of Google, which has revolutionized the way we access information online.  
Head Entity: Alphabet Inc.  
Tail Entity: Google  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a significant influence, especially since it is the parent organization of several well-known banks, including Chase Bank, which serves millions of customers across the United States.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: Chase Bank  

Relation: organization parents  
Context: The Walt Disney Company has expanded its empire over the years, and one of its most notable acquisitions was Pixar Animation Studios, which has produced some of the most beloved animated films in history.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  

Relation: organization parents  
Context: In the realm of social media, Facebook, Inc. has grown tremendously, and it is important to note that it is the parent company of Instagram, a platform that has transformed the way people share photos and connect with each other.  
Head Entity: Facebook, Inc.  
Tail Entity: Instagram  

Relation: organization parents  
Context: The pharmaceutical industry is heavily influenced by large corporations, and Pfizer Inc. stands out as a major player, especially since it is the parent organization of several subsidiaries that focus on various aspects of healthcare.  
Head Entity: Pfizer Inc.  
Tail Entity: Pfizer subsidiaries
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes in the United States.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  

Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- provides financial assistance and advice to member countries.  
Head Entity: international monetary fund  
Tail Entity: imf  

Relation: organization alternate names  
Context: the national aeronautics and space administration -lrb- nasa -rrb- is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: national aeronautics and space administration  
Tail Entity: nasa  

Relation: organization alternate names  
Context: the united nations educational, scientific and cultural organization -lrb- unesco -rrb- aims to promote world peace and security through international cooperation in education, the sciences, and culture.  
Head Entity: united nations educational, scientific and cultural organization  
Tail Entity: unesco  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2010-11-15 12:00:00 utc salesforce.com, a cloud computing company, announced its expansion into new offices in the heart of san francisco, aiming to accommodate its growing workforce.  
Head Entity: salesforce.com  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2015-03-10 09:30:00 utc the headquarters of the international business machines corporation, commonly known as ibm, is located in the bustling city of new york, where it has been a key player in technology for decades.  
Head Entity: ibm  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2018-07-22 14:45:00 utc amazon.com, the e-commerce giant, continues to thrive with its headquarters situated in seattle, washington, contributing significantly to the local economy.  
Head Entity: amazon.com  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2021-01-05 11:15:00 utc the biopharmaceutical company moderna, known for its innovative vaccine technology, has its headquarters in boston, massachusetts, a hub for life sciences.  
Head Entity: moderna  
Tail Entity: boston  

Relation: organization city of headquarters  
Context: ------ austin 2019-09-30 16:00:00 utc the tech startup indeed.com, which specializes in job search engines, has established its headquarters in austin, texas, attracting talent from across the country.  
Head Entity: indeed.com  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: more than three decades ago , kerry 's work against the vietnam war set him on course to the senate - and , he often hoped , on to the presidency .
Head Entity: kerry
Tail Entity: he
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During their childhood, Emma and her brother often played together in the backyard, creating memories that would last a lifetime.  
Head Entity: Emma  
Tail Entity: her brother  

Relation: person siblings  
Context: After the family reunion, Sarah realized how much she missed her sister, who had moved to another state for work.  
Head Entity: Sarah  
Tail Entity: her sister  

Relation: person siblings  
Context: The documentary highlighted the bond between the two brothers, showcasing their adventures growing up in a small town.  
Head Entity: the two brothers  
Tail Entity: their adventures  

Relation: person siblings  
Context: When Michael received the news about his sister's promotion, he couldn't help but feel proud of her achievements.  
Head Entity: Michael  
Tail Entity: his sister  

Relation: person siblings  
Context: At the wedding, Jessica shared a heartfelt speech about the special connection she has with her twin brother.  
Head Entity: Jessica  
Tail Entity: her twin brother  
Losses:  5.97278356552124 0.8028687834739685 0.640291690826416
MemoryTrain:  epoch  0, batch     0 | loss: 5.9727836Losses:  5.038243770599365 0.5096907615661621 0.7333880662918091
MemoryTrain:  epoch  0, batch     1 | loss: 5.0382438Losses:  6.406179428100586 1.6517066955566406 0.5893038511276245
MemoryTrain:  epoch  0, batch     2 | loss: 6.4061794Losses:  7.856832504272461 2.0023465156555176 0.5969493389129639
MemoryTrain:  epoch  0, batch     3 | loss: 7.8568325Losses:  4.61195707321167 0.6920014023780823 0.5551513433456421
MemoryTrain:  epoch  0, batch     4 | loss: 4.6119571Losses:  5.614307880401611 1.3554677963256836 0.7502043843269348
MemoryTrain:  epoch  0, batch     5 | loss: 5.6143079Losses:  7.286420822143555 2.2359776496887207 0.5735117197036743
MemoryTrain:  epoch  0, batch     6 | loss: 7.2864208Losses:  5.142683029174805 0.7895286679267883 0.7160112857818604
MemoryTrain:  epoch  0, batch     7 | loss: 5.1426830Losses:  5.858975410461426 1.17098069190979 0.5226659774780273
MemoryTrain:  epoch  0, batch     8 | loss: 5.8589754Losses:  4.853116989135742 0.6376469135284424 0.7339591979980469
MemoryTrain:  epoch  0, batch     9 | loss: 4.8531170Losses:  5.689091682434082 0.5141700506210327 0.6612927913665771
MemoryTrain:  epoch  1, batch     0 | loss: 5.6890917Losses:  5.935831546783447 1.0200042724609375 0.6421637535095215
MemoryTrain:  epoch  1, batch     1 | loss: 5.9358315Losses:  4.586797714233398 0.7497057914733887 0.7005091905593872
MemoryTrain:  epoch  1, batch     2 | loss: 4.5867977Losses:  4.611212730407715 1.428661823272705 0.715216875076294
MemoryTrain:  epoch  1, batch     3 | loss: 4.6112127Losses:  4.103537559509277 0.7407838106155396 0.772686779499054
MemoryTrain:  epoch  1, batch     4 | loss: 4.1035376Losses:  4.988556861877441 1.0380765199661255 0.6749858260154724
MemoryTrain:  epoch  1, batch     5 | loss: 4.9885569Losses:  5.712064266204834 0.8200997710227966 0.747940182685852
MemoryTrain:  epoch  1, batch     6 | loss: 5.7120643Losses:  5.429947376251221 0.5557318925857544 0.6526122093200684
MemoryTrain:  epoch  1, batch     7 | loss: 5.4299474Losses:  5.123323917388916 0.5509913563728333 0.8007869124412537
MemoryTrain:  epoch  1, batch     8 | loss: 5.1233239Losses:  5.675206661224365 1.3679040670394897 0.5100955963134766
MemoryTrain:  epoch  1, batch     9 | loss: 5.6752067Losses:  5.473517894744873 1.5643072128295898 0.7283471822738647
MemoryTrain:  epoch  2, batch     0 | loss: 5.4735179Losses:  5.764753818511963 2.0282199382781982 0.5169539451599121
MemoryTrain:  epoch  2, batch     1 | loss: 5.7647538Losses:  4.68724250793457 0.733833372592926 0.7964970469474792
MemoryTrain:  epoch  2, batch     2 | loss: 4.6872425Losses:  3.850942611694336 0.7700080871582031 0.4820164442062378
MemoryTrain:  epoch  2, batch     3 | loss: 3.8509426Losses:  4.037509441375732 0.4948163330554962 0.7573626637458801
MemoryTrain:  epoch  2, batch     4 | loss: 4.0375094Losses:  4.057287216186523 0.5402039289474487 0.6593978404998779
MemoryTrain:  epoch  2, batch     5 | loss: 4.0572872Losses:  3.7699451446533203 0.49144288897514343 0.6280437707901001
MemoryTrain:  epoch  2, batch     6 | loss: 3.7699451Losses:  3.7599222660064697 0.4718691110610962 0.7163164615631104
MemoryTrain:  epoch  2, batch     7 | loss: 3.7599223Losses:  5.1043782234191895 0.8704809546470642 0.4129229485988617
MemoryTrain:  epoch  2, batch     8 | loss: 5.1043782Losses:  3.8281919956207275 0.6430026292800903 0.6798751950263977
MemoryTrain:  epoch  2, batch     9 | loss: 3.8281920Losses:  3.982879638671875 0.48982173204421997 0.7671871185302734
MemoryTrain:  epoch  3, batch     0 | loss: 3.9828796Losses:  3.7745611667633057 -0.0 0.7800895571708679
MemoryTrain:  epoch  3, batch     1 | loss: 3.7745612Losses:  3.9508752822875977 0.7553061246871948 0.6813477873802185
MemoryTrain:  epoch  3, batch     2 | loss: 3.9508753Losses:  3.4747769832611084 0.21394428610801697 0.7936643958091736
MemoryTrain:  epoch  3, batch     3 | loss: 3.4747770Losses:  4.4272050857543945 1.5376617908477783 0.6594970226287842
MemoryTrain:  epoch  3, batch     4 | loss: 4.4272051Losses:  3.981071949005127 0.7452647686004639 0.5690472722053528
MemoryTrain:  epoch  3, batch     5 | loss: 3.9810719Losses:  5.207291126251221 1.6953489780426025 0.6660895347595215
MemoryTrain:  epoch  3, batch     6 | loss: 5.2072911Losses:  6.452521800994873 3.0558252334594727 0.6377016305923462
MemoryTrain:  epoch  3, batch     7 | loss: 6.4525218Losses:  4.7927656173706055 1.9357868432998657 0.6710188388824463
MemoryTrain:  epoch  3, batch     8 | loss: 4.7927656Losses:  4.22971773147583 0.6932492256164551 0.413808673620224
MemoryTrain:  epoch  3, batch     9 | loss: 4.2297177Losses:  4.363141059875488 1.1584670543670654 0.5673744678497314
MemoryTrain:  epoch  4, batch     0 | loss: 4.3631411Losses:  4.365549564361572 0.8981881141662598 0.7286698222160339
MemoryTrain:  epoch  4, batch     1 | loss: 4.3655496Losses:  4.264801502227783 0.8598533272743225 0.7050949335098267
MemoryTrain:  epoch  4, batch     2 | loss: 4.2648015Losses:  4.830528259277344 1.85684072971344 0.5689606070518494
MemoryTrain:  epoch  4, batch     3 | loss: 4.8305283Losses:  4.269122123718262 1.5093011856079102 0.7285813093185425
MemoryTrain:  epoch  4, batch     4 | loss: 4.2691221Losses:  3.655519962310791 0.5070210695266724 0.7251981496810913
MemoryTrain:  epoch  4, batch     5 | loss: 3.6555200Losses:  3.926189422607422 1.1798101663589478 0.5399799346923828
MemoryTrain:  epoch  4, batch     6 | loss: 3.9261894Losses:  4.071280479431152 1.1262670755386353 0.6597365140914917
MemoryTrain:  epoch  4, batch     7 | loss: 4.0712805Losses:  4.669102668762207 1.2624776363372803 0.6683225035667419
MemoryTrain:  epoch  4, batch     8 | loss: 4.6691027Losses:  3.354902744293213 0.7995704412460327 0.5435161590576172
MemoryTrain:  epoch  4, batch     9 | loss: 3.3549027Losses:  5.011941909790039 2.034790515899658 0.7263709306716919
MemoryTrain:  epoch  5, batch     0 | loss: 5.0119419Losses:  4.144918918609619 1.0039994716644287 0.7481557130813599
MemoryTrain:  epoch  5, batch     1 | loss: 4.1449189Losses:  3.747321844100952 0.6256030797958374 0.6245386004447937
MemoryTrain:  epoch  5, batch     2 | loss: 3.7473218Losses:  4.330682277679443 1.3935410976409912 0.7243103384971619
MemoryTrain:  epoch  5, batch     3 | loss: 4.3306823Losses:  3.9590814113616943 1.2817249298095703 0.4868158996105194
MemoryTrain:  epoch  5, batch     4 | loss: 3.9590814Losses:  4.270862579345703 0.9487870931625366 0.5944764614105225
MemoryTrain:  epoch  5, batch     5 | loss: 4.2708626Losses:  3.963289260864258 1.1227591037750244 0.6304333209991455
MemoryTrain:  epoch  5, batch     6 | loss: 3.9632893Losses:  3.255394458770752 0.6957228779792786 0.6327129602432251
MemoryTrain:  epoch  5, batch     7 | loss: 3.2553945Losses:  3.4791784286499023 0.799285888671875 0.5586326122283936
MemoryTrain:  epoch  5, batch     8 | loss: 3.4791784Losses:  2.7839837074279785 0.26606449484825134 0.5635931491851807
MemoryTrain:  epoch  5, batch     9 | loss: 2.7839837Losses:  3.8972795009613037 0.9872788190841675 0.6405222415924072
MemoryTrain:  epoch  6, batch     0 | loss: 3.8972795Losses:  3.6322340965270996 0.7365239858627319 0.5070759057998657
MemoryTrain:  epoch  6, batch     1 | loss: 3.6322341Losses:  4.017459392547607 1.3957194089889526 0.5564678311347961
MemoryTrain:  epoch  6, batch     2 | loss: 4.0174594Losses:  5.017187595367432 2.3991384506225586 0.6202782392501831
MemoryTrain:  epoch  6, batch     3 | loss: 5.0171876Losses:  5.402376174926758 2.328742504119873 0.566270112991333
MemoryTrain:  epoch  6, batch     4 | loss: 5.4023762Losses:  3.1647229194641113 0.5187845230102539 0.5354571342468262
MemoryTrain:  epoch  6, batch     5 | loss: 3.1647229Losses:  3.685673236846924 1.0833096504211426 0.5683466196060181
MemoryTrain:  epoch  6, batch     6 | loss: 3.6856732Losses:  3.934821605682373 1.294198751449585 0.6314269304275513
MemoryTrain:  epoch  6, batch     7 | loss: 3.9348216Losses:  4.474922180175781 1.8834404945373535 0.5220125913619995
MemoryTrain:  epoch  6, batch     8 | loss: 4.4749222Losses:  3.607646942138672 0.5427307486534119 0.5408888459205627
MemoryTrain:  epoch  6, batch     9 | loss: 3.6076469Losses:  3.1306731700897217 0.22945961356163025 0.7886013388633728
MemoryTrain:  epoch  7, batch     0 | loss: 3.1306732Losses:  3.50740385055542 0.7500836849212646 0.7299955487251282
MemoryTrain:  epoch  7, batch     1 | loss: 3.5074039Losses:  3.7168655395507812 0.7115736603736877 0.6318032741546631
MemoryTrain:  epoch  7, batch     2 | loss: 3.7168655Losses:  3.336272716522217 0.69100022315979 0.6423553228378296
MemoryTrain:  epoch  7, batch     3 | loss: 3.3362727Losses:  3.418393850326538 0.7075358033180237 0.6831818222999573
MemoryTrain:  epoch  7, batch     4 | loss: 3.4183939Losses:  4.265026569366455 1.534528374671936 0.6674489974975586
MemoryTrain:  epoch  7, batch     5 | loss: 4.2650266Losses:  3.5076169967651367 0.8202993273735046 0.5544459223747253
MemoryTrain:  epoch  7, batch     6 | loss: 3.5076170Losses:  5.366388320922852 2.811962127685547 0.5604552030563354
MemoryTrain:  epoch  7, batch     7 | loss: 5.3663883Losses:  3.8865671157836914 0.7408056855201721 0.7184231877326965
MemoryTrain:  epoch  7, batch     8 | loss: 3.8865671Losses:  3.2851674556732178 0.7445098161697388 0.5821115970611572
MemoryTrain:  epoch  7, batch     9 | loss: 3.2851675Losses:  3.051557779312134 0.4652937650680542 0.6208459734916687
MemoryTrain:  epoch  8, batch     0 | loss: 3.0515578Losses:  4.535499572753906 1.6313502788543701 0.628827691078186
MemoryTrain:  epoch  8, batch     1 | loss: 4.5354996Losses:  3.973815441131592 1.3825699090957642 0.5795774459838867
MemoryTrain:  epoch  8, batch     2 | loss: 3.9738154Losses:  3.1214139461517334 0.47735852003097534 0.6207805275917053
MemoryTrain:  epoch  8, batch     3 | loss: 3.1214139Losses:  3.7980716228485107 0.5406607985496521 0.7631122469902039
MemoryTrain:  epoch  8, batch     4 | loss: 3.7980716Losses:  4.139718055725098 1.2709779739379883 0.693914532661438
MemoryTrain:  epoch  8, batch     5 | loss: 4.1397181Losses:  3.441718816757202 0.9043184518814087 0.6241424083709717
MemoryTrain:  epoch  8, batch     6 | loss: 3.4417188Losses:  3.378392457962036 0.7185624837875366 0.7640144228935242
MemoryTrain:  epoch  8, batch     7 | loss: 3.3783925Losses:  3.955625534057617 1.337834358215332 0.5940070152282715
MemoryTrain:  epoch  8, batch     8 | loss: 3.9556255Losses:  3.0837976932525635 0.5085150003433228 0.5808431506156921
MemoryTrain:  epoch  8, batch     9 | loss: 3.0837977Losses:  4.920775890350342 2.187380075454712 0.7007741332054138
MemoryTrain:  epoch  9, batch     0 | loss: 4.9207759Losses:  3.491685152053833 0.9442083835601807 0.4790270924568176
MemoryTrain:  epoch  9, batch     1 | loss: 3.4916852Losses:  4.925623416900635 2.0198144912719727 0.7158983945846558
MemoryTrain:  epoch  9, batch     2 | loss: 4.9256234Losses:  3.8452084064483643 1.086421251296997 0.7688595652580261
MemoryTrain:  epoch  9, batch     3 | loss: 3.8452084Losses:  3.3266022205352783 0.7108080387115479 0.6394346356391907
MemoryTrain:  epoch  9, batch     4 | loss: 3.3266022Losses:  3.640977621078491 1.0086653232574463 0.7068086266517639
MemoryTrain:  epoch  9, batch     5 | loss: 3.6409776Losses:  3.9001150131225586 1.42626953125 0.5653380751609802
MemoryTrain:  epoch  9, batch     6 | loss: 3.9001150Losses:  3.9853920936584473 1.0896129608154297 0.6381720304489136
MemoryTrain:  epoch  9, batch     7 | loss: 3.9853921Losses:  3.8095505237579346 1.3567795753479004 0.5645436644554138
MemoryTrain:  epoch  9, batch     8 | loss: 3.8095505Losses:  2.773772716522217 0.246852844953537 0.5596803426742554
MemoryTrain:  epoch  9, batch     9 | loss: 2.7737727
