#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  10.924942016601562 1.7946100234985352
CurrentTrain: epoch  0, batch     0 | loss: 10.9249420Losses:  10.995424270629883 2.0578651428222656
CurrentTrain: epoch  0, batch     1 | loss: 10.9954243Losses:  11.027371406555176 1.895566701889038
CurrentTrain: epoch  0, batch     2 | loss: 11.0273714Losses:  10.88780689239502 1.766021728515625
CurrentTrain: epoch  0, batch     3 | loss: 10.8878069Losses:  10.228809356689453 1.9586539268493652
CurrentTrain: epoch  0, batch     4 | loss: 10.2288094Losses:  11.277570724487305 1.6817632913589478
CurrentTrain: epoch  0, batch     5 | loss: 11.2775707Losses:  10.509210586547852 1.907130241394043
CurrentTrain: epoch  0, batch     6 | loss: 10.5092106Losses:  9.879103660583496 1.8509485721588135
CurrentTrain: epoch  0, batch     7 | loss: 9.8791037Losses:  10.066309928894043 1.9094722270965576
CurrentTrain: epoch  0, batch     8 | loss: 10.0663099Losses:  10.674617767333984 1.9160534143447876
CurrentTrain: epoch  0, batch     9 | loss: 10.6746178Losses:  9.842134475708008 1.7465028762817383
CurrentTrain: epoch  0, batch    10 | loss: 9.8421345Losses:  9.300650596618652 1.7155197858810425
CurrentTrain: epoch  0, batch    11 | loss: 9.3006506Losses:  10.01288890838623 1.7701202630996704
CurrentTrain: epoch  0, batch    12 | loss: 10.0128889Losses:  9.991678237915039 1.9234750270843506
CurrentTrain: epoch  0, batch    13 | loss: 9.9916782Losses:  9.075039863586426 1.8390240669250488
CurrentTrain: epoch  0, batch    14 | loss: 9.0750399Losses:  9.859254837036133 2.0333995819091797
CurrentTrain: epoch  0, batch    15 | loss: 9.8592548Losses:  10.017661094665527 1.579325795173645
CurrentTrain: epoch  0, batch    16 | loss: 10.0176611Losses:  10.034493446350098 2.0498757362365723
CurrentTrain: epoch  0, batch    17 | loss: 10.0344934Losses:  9.882950782775879 1.8172800540924072
CurrentTrain: epoch  0, batch    18 | loss: 9.8829508Losses:  9.64188289642334 2.009021759033203
CurrentTrain: epoch  0, batch    19 | loss: 9.6418829Losses:  8.998373031616211 1.9627145528793335
CurrentTrain: epoch  0, batch    20 | loss: 8.9983730Losses:  9.180394172668457 1.979718804359436
CurrentTrain: epoch  0, batch    21 | loss: 9.1803942Losses:  9.70967960357666 1.933288812637329
CurrentTrain: epoch  0, batch    22 | loss: 9.7096796Losses:  9.097502708435059 1.918447494506836
CurrentTrain: epoch  0, batch    23 | loss: 9.0975027Losses:  8.755414009094238 1.9837981462478638
CurrentTrain: epoch  0, batch    24 | loss: 8.7554140Losses:  8.952749252319336 1.883960247039795
CurrentTrain: epoch  0, batch    25 | loss: 8.9527493Losses:  9.116084098815918 1.8026232719421387
CurrentTrain: epoch  0, batch    26 | loss: 9.1160841Losses:  8.761796951293945 2.0296030044555664
CurrentTrain: epoch  0, batch    27 | loss: 8.7617970Losses:  9.139631271362305 1.8726119995117188
CurrentTrain: epoch  0, batch    28 | loss: 9.1396313Losses:  9.141315460205078 1.9677006006240845
CurrentTrain: epoch  0, batch    29 | loss: 9.1413155Losses:  9.590543746948242 2.077651023864746
CurrentTrain: epoch  0, batch    30 | loss: 9.5905437Losses:  8.897761344909668 1.7811453342437744
CurrentTrain: epoch  0, batch    31 | loss: 8.8977613Losses:  8.708892822265625 1.4619684219360352
CurrentTrain: epoch  0, batch    32 | loss: 8.7088928Losses:  8.281454086303711 1.6540122032165527
CurrentTrain: epoch  0, batch    33 | loss: 8.2814541Losses:  8.793854713439941 1.8239805698394775
CurrentTrain: epoch  0, batch    34 | loss: 8.7938547Losses:  8.661948204040527 1.8661017417907715
CurrentTrain: epoch  0, batch    35 | loss: 8.6619482Losses:  8.667070388793945 1.8998699188232422
CurrentTrain: epoch  0, batch    36 | loss: 8.6670704Losses:  8.705330848693848 1.917996883392334
CurrentTrain: epoch  0, batch    37 | loss: 8.7053308Losses:  9.564361572265625 2.1167402267456055
CurrentTrain: epoch  0, batch    38 | loss: 9.5643616Losses:  9.09254264831543 2.1517515182495117
CurrentTrain: epoch  0, batch    39 | loss: 9.0925426Losses:  9.445405960083008 2.0335655212402344
CurrentTrain: epoch  0, batch    40 | loss: 9.4454060Losses:  8.413825988769531 1.9755167961120605
CurrentTrain: epoch  0, batch    41 | loss: 8.4138260Losses:  8.23581600189209 1.9381781816482544
CurrentTrain: epoch  0, batch    42 | loss: 8.2358160Losses:  7.8480353355407715 1.6457264423370361
CurrentTrain: epoch  0, batch    43 | loss: 7.8480353Losses:  8.207594871520996 1.7457165718078613
CurrentTrain: epoch  0, batch    44 | loss: 8.2075949Losses:  8.339546203613281 2.0196170806884766
CurrentTrain: epoch  0, batch    45 | loss: 8.3395462Losses:  9.020026206970215 1.7926455736160278
CurrentTrain: epoch  0, batch    46 | loss: 9.0200262Losses:  7.597173690795898 2.0124921798706055
CurrentTrain: epoch  0, batch    47 | loss: 7.5971737Losses:  8.527713775634766 1.9166303873062134
CurrentTrain: epoch  0, batch    48 | loss: 8.5277138Losses:  8.247014999389648 1.920115351676941
CurrentTrain: epoch  0, batch    49 | loss: 8.2470150Losses:  8.672405242919922 1.9697428941726685
CurrentTrain: epoch  0, batch    50 | loss: 8.6724052Losses:  8.256975173950195 2.086707592010498
CurrentTrain: epoch  0, batch    51 | loss: 8.2569752Losses:  7.923994064331055 1.9142018556594849
CurrentTrain: epoch  0, batch    52 | loss: 7.9239941Losses:  8.080602645874023 1.8867741823196411
CurrentTrain: epoch  0, batch    53 | loss: 8.0806026Losses:  8.756624221801758 2.0438995361328125
CurrentTrain: epoch  0, batch    54 | loss: 8.7566242Losses:  7.913088321685791 1.8322975635528564
CurrentTrain: epoch  0, batch    55 | loss: 7.9130883Losses:  7.656713962554932 1.7942759990692139
CurrentTrain: epoch  0, batch    56 | loss: 7.6567140Losses:  8.487066268920898 1.9806222915649414
CurrentTrain: epoch  0, batch    57 | loss: 8.4870663Losses:  8.231640815734863 1.723626732826233
CurrentTrain: epoch  0, batch    58 | loss: 8.2316408Losses:  7.88890266418457 1.7149338722229004
CurrentTrain: epoch  0, batch    59 | loss: 7.8889027Losses:  7.476370811462402 1.9322774410247803
CurrentTrain: epoch  0, batch    60 | loss: 7.4763708Losses:  8.300910949707031 1.7504870891571045
CurrentTrain: epoch  0, batch    61 | loss: 8.3009109Losses:  9.64002513885498 1.4586751461029053
CurrentTrain: epoch  0, batch    62 | loss: 9.6400251Losses:  8.28358268737793 1.7753897905349731
CurrentTrain: epoch  1, batch     0 | loss: 8.2835827Losses:  7.930608749389648 1.873429775238037
CurrentTrain: epoch  1, batch     1 | loss: 7.9306087Losses:  7.614602088928223 1.7803704738616943
CurrentTrain: epoch  1, batch     2 | loss: 7.6146021Losses:  7.970501899719238 2.044762134552002
CurrentTrain: epoch  1, batch     3 | loss: 7.9705019Losses:  8.285381317138672 2.019646167755127
CurrentTrain: epoch  1, batch     4 | loss: 8.2853813Losses:  7.704388618469238 1.9130992889404297
CurrentTrain: epoch  1, batch     5 | loss: 7.7043886Losses:  7.842707633972168 1.9000511169433594
CurrentTrain: epoch  1, batch     6 | loss: 7.8427076Losses:  7.176374435424805 2.0432658195495605
CurrentTrain: epoch  1, batch     7 | loss: 7.1763744Losses:  7.1755595207214355 1.7215430736541748
CurrentTrain: epoch  1, batch     8 | loss: 7.1755595Losses:  7.993206977844238 1.9681146144866943
CurrentTrain: epoch  1, batch     9 | loss: 7.9932070Losses:  7.8864312171936035 1.9325910806655884
CurrentTrain: epoch  1, batch    10 | loss: 7.8864312Losses:  7.517273426055908 1.9489179849624634
CurrentTrain: epoch  1, batch    11 | loss: 7.5172734Losses:  7.807710647583008 2.007434368133545
CurrentTrain: epoch  1, batch    12 | loss: 7.8077106Losses:  7.932061672210693 1.6406748294830322
CurrentTrain: epoch  1, batch    13 | loss: 7.9320617Losses:  7.222705841064453 1.9499402046203613
CurrentTrain: epoch  1, batch    14 | loss: 7.2227058Losses:  8.814767837524414 1.881704568862915
CurrentTrain: epoch  1, batch    15 | loss: 8.8147678Losses:  7.4607086181640625 1.7754533290863037
CurrentTrain: epoch  1, batch    16 | loss: 7.4607086Losses:  6.7505903244018555 1.6107268333435059
CurrentTrain: epoch  1, batch    17 | loss: 6.7505903Losses:  7.315695285797119 1.8347660303115845
CurrentTrain: epoch  1, batch    18 | loss: 7.3156953Losses:  7.652059078216553 1.962752342224121
CurrentTrain: epoch  1, batch    19 | loss: 7.6520591Losses:  7.972830772399902 2.062492847442627
CurrentTrain: epoch  1, batch    20 | loss: 7.9728308Losses:  7.963074684143066 2.0530619621276855
CurrentTrain: epoch  1, batch    21 | loss: 7.9630747Losses:  7.667511463165283 1.8887969255447388
CurrentTrain: epoch  1, batch    22 | loss: 7.6675115Losses:  7.733736038208008 1.724808931350708
CurrentTrain: epoch  1, batch    23 | loss: 7.7337360Losses:  7.4998345375061035 1.903849482536316
CurrentTrain: epoch  1, batch    24 | loss: 7.4998345Losses:  6.485595703125 1.836641550064087
CurrentTrain: epoch  1, batch    25 | loss: 6.4855957Losses:  7.182641983032227 2.058074474334717
CurrentTrain: epoch  1, batch    26 | loss: 7.1826420Losses:  7.064003944396973 1.937194585800171
CurrentTrain: epoch  1, batch    27 | loss: 7.0640039Losses:  7.506629943847656 1.9712424278259277
CurrentTrain: epoch  1, batch    28 | loss: 7.5066299Losses:  6.950041770935059 2.0475540161132812
CurrentTrain: epoch  1, batch    29 | loss: 6.9500418Losses:  7.23481559753418 1.8926076889038086
CurrentTrain: epoch  1, batch    30 | loss: 7.2348156Losses:  6.732495307922363 2.0839896202087402
CurrentTrain: epoch  1, batch    31 | loss: 6.7324953Losses:  6.425685405731201 1.9497406482696533
CurrentTrain: epoch  1, batch    32 | loss: 6.4256854Losses:  6.763206481933594 1.714476227760315
CurrentTrain: epoch  1, batch    33 | loss: 6.7632065Losses:  7.7241973876953125 2.018435001373291
CurrentTrain: epoch  1, batch    34 | loss: 7.7241974Losses:  6.591939926147461 1.8720111846923828
CurrentTrain: epoch  1, batch    35 | loss: 6.5919399Losses:  7.749372959136963 1.7197020053863525
CurrentTrain: epoch  1, batch    36 | loss: 7.7493730Losses:  7.621406078338623 2.002958297729492
CurrentTrain: epoch  1, batch    37 | loss: 7.6214061Losses:  8.10269546508789 2.056959390640259
CurrentTrain: epoch  1, batch    38 | loss: 8.1026955Losses:  6.592630386352539 1.9693169593811035
CurrentTrain: epoch  1, batch    39 | loss: 6.5926304Losses:  7.560776710510254 1.878314733505249
CurrentTrain: epoch  1, batch    40 | loss: 7.5607767Losses:  7.86714506149292 1.9990962743759155
CurrentTrain: epoch  1, batch    41 | loss: 7.8671451Losses:  7.841784954071045 1.8905904293060303
CurrentTrain: epoch  1, batch    42 | loss: 7.8417850Losses:  8.281129837036133 1.9187933206558228
CurrentTrain: epoch  1, batch    43 | loss: 8.2811298Losses:  8.258454322814941 1.7896686792373657
CurrentTrain: epoch  1, batch    44 | loss: 8.2584543Losses:  6.899630546569824 2.022886276245117
CurrentTrain: epoch  1, batch    45 | loss: 6.8996305Losses:  6.44118595123291 1.9575440883636475
CurrentTrain: epoch  1, batch    46 | loss: 6.4411860Losses:  7.542442798614502 2.138747215270996
CurrentTrain: epoch  1, batch    47 | loss: 7.5424428Losses:  7.192800998687744 1.9105749130249023
CurrentTrain: epoch  1, batch    48 | loss: 7.1928010Losses:  6.872893333435059 1.9251492023468018
CurrentTrain: epoch  1, batch    49 | loss: 6.8728933Losses:  7.142368316650391 1.8709676265716553
CurrentTrain: epoch  1, batch    50 | loss: 7.1423683Losses:  8.03874683380127 1.774733304977417
CurrentTrain: epoch  1, batch    51 | loss: 8.0387468Losses:  7.211973190307617 1.8500592708587646
CurrentTrain: epoch  1, batch    52 | loss: 7.2119732Losses:  7.315563678741455 1.8750096559524536
CurrentTrain: epoch  1, batch    53 | loss: 7.3155637Losses:  7.269051551818848 1.9599504470825195
CurrentTrain: epoch  1, batch    54 | loss: 7.2690516Losses:  6.169029235839844 1.7237153053283691
CurrentTrain: epoch  1, batch    55 | loss: 6.1690292Losses:  6.217070579528809 1.7427650690078735
CurrentTrain: epoch  1, batch    56 | loss: 6.2170706Losses:  7.100582599639893 1.994004249572754
CurrentTrain: epoch  1, batch    57 | loss: 7.1005826Losses:  6.401694297790527 1.772447109222412
CurrentTrain: epoch  1, batch    58 | loss: 6.4016943Losses:  7.680833339691162 2.060476303100586
CurrentTrain: epoch  1, batch    59 | loss: 7.6808333Losses:  7.271816730499268 2.003835678100586
CurrentTrain: epoch  1, batch    60 | loss: 7.2718167Losses:  6.81345272064209 2.030515670776367
CurrentTrain: epoch  1, batch    61 | loss: 6.8134527Losses:  5.458530426025391 1.3960610628128052
CurrentTrain: epoch  1, batch    62 | loss: 5.4585304Losses:  6.8219404220581055 1.6794629096984863
CurrentTrain: epoch  2, batch     0 | loss: 6.8219404Losses:  6.828607559204102 1.6917723417282104
CurrentTrain: epoch  2, batch     1 | loss: 6.8286076Losses:  6.892764091491699 1.9635090827941895
CurrentTrain: epoch  2, batch     2 | loss: 6.8927641Losses:  6.134873390197754 1.8956952095031738
CurrentTrain: epoch  2, batch     3 | loss: 6.1348734Losses:  6.575732707977295 1.8843916654586792
CurrentTrain: epoch  2, batch     4 | loss: 6.5757327Losses:  6.519352436065674 2.127095937728882
CurrentTrain: epoch  2, batch     5 | loss: 6.5193524Losses:  6.317441463470459 1.756413459777832
CurrentTrain: epoch  2, batch     6 | loss: 6.3174415Losses:  7.106202125549316 1.9491028785705566
CurrentTrain: epoch  2, batch     7 | loss: 7.1062021Losses:  7.660501003265381 1.9308879375457764
CurrentTrain: epoch  2, batch     8 | loss: 7.6605010Losses:  6.197761535644531 1.833216905593872
CurrentTrain: epoch  2, batch     9 | loss: 6.1977615Losses:  6.101714134216309 1.9169764518737793
CurrentTrain: epoch  2, batch    10 | loss: 6.1017141Losses:  6.953758716583252 1.8776590824127197
CurrentTrain: epoch  2, batch    11 | loss: 6.9537587Losses:  7.213698863983154 1.8193018436431885
CurrentTrain: epoch  2, batch    12 | loss: 7.2136989Losses:  6.837021350860596 1.7878178358078003
CurrentTrain: epoch  2, batch    13 | loss: 6.8370214Losses:  6.658900737762451 1.9889800548553467
CurrentTrain: epoch  2, batch    14 | loss: 6.6589007Losses:  6.3549580574035645 2.0316200256347656
CurrentTrain: epoch  2, batch    15 | loss: 6.3549581Losses:  5.995148658752441 1.907086968421936
CurrentTrain: epoch  2, batch    16 | loss: 5.9951487Losses:  5.560458660125732 1.5843812227249146
CurrentTrain: epoch  2, batch    17 | loss: 5.5604587Losses:  7.032905578613281 2.0660459995269775
CurrentTrain: epoch  2, batch    18 | loss: 7.0329056Losses:  6.499136447906494 1.8871681690216064
CurrentTrain: epoch  2, batch    19 | loss: 6.4991364Losses:  6.404768943786621 1.9574533700942993
CurrentTrain: epoch  2, batch    20 | loss: 6.4047689Losses:  6.218321323394775 1.9193546772003174
CurrentTrain: epoch  2, batch    21 | loss: 6.2183213Losses:  5.999676704406738 1.9209988117218018
CurrentTrain: epoch  2, batch    22 | loss: 5.9996767Losses:  6.648899555206299 1.9731395244598389
CurrentTrain: epoch  2, batch    23 | loss: 6.6488996Losses:  6.527874946594238 1.8655505180358887
CurrentTrain: epoch  2, batch    24 | loss: 6.5278749Losses:  6.193826675415039 1.9267160892486572
CurrentTrain: epoch  2, batch    25 | loss: 6.1938267Losses:  5.956718921661377 1.9132200479507446
CurrentTrain: epoch  2, batch    26 | loss: 5.9567189Losses:  6.120643138885498 1.6693464517593384
CurrentTrain: epoch  2, batch    27 | loss: 6.1206431Losses:  6.021193027496338 1.8004330396652222
CurrentTrain: epoch  2, batch    28 | loss: 6.0211930Losses:  6.4908037185668945 2.1348633766174316
CurrentTrain: epoch  2, batch    29 | loss: 6.4908037Losses:  6.651697158813477 2.0121073722839355
CurrentTrain: epoch  2, batch    30 | loss: 6.6516972Losses:  6.115280628204346 2.163288116455078
CurrentTrain: epoch  2, batch    31 | loss: 6.1152806Losses:  7.169517993927002 1.7767951488494873
CurrentTrain: epoch  2, batch    32 | loss: 7.1695180Losses:  5.743316650390625 1.8379669189453125
CurrentTrain: epoch  2, batch    33 | loss: 5.7433167Losses:  6.522890090942383 2.0154356956481934
CurrentTrain: epoch  2, batch    34 | loss: 6.5228901Losses:  6.509458065032959 1.8938928842544556
CurrentTrain: epoch  2, batch    35 | loss: 6.5094581Losses:  5.973417282104492 2.004361391067505
CurrentTrain: epoch  2, batch    36 | loss: 5.9734173Losses:  6.293575286865234 1.7568724155426025
CurrentTrain: epoch  2, batch    37 | loss: 6.2935753Losses:  6.182838439941406 1.8603062629699707
CurrentTrain: epoch  2, batch    38 | loss: 6.1828384Losses:  5.587825775146484 1.7505685091018677
CurrentTrain: epoch  2, batch    39 | loss: 5.5878258Losses:  6.291042327880859 1.6751713752746582
CurrentTrain: epoch  2, batch    40 | loss: 6.2910423Losses:  6.709837913513184 1.5061873197555542
CurrentTrain: epoch  2, batch    41 | loss: 6.7098379Losses:  6.127506732940674 1.927057147026062
CurrentTrain: epoch  2, batch    42 | loss: 6.1275067Losses:  6.360723972320557 1.8956069946289062
CurrentTrain: epoch  2, batch    43 | loss: 6.3607240Losses:  6.109841823577881 2.0144641399383545
CurrentTrain: epoch  2, batch    44 | loss: 6.1098418Losses:  6.449042320251465 1.9683685302734375
CurrentTrain: epoch  2, batch    45 | loss: 6.4490423Losses:  5.850163459777832 1.9699105024337769
CurrentTrain: epoch  2, batch    46 | loss: 5.8501635Losses:  6.619283199310303 1.9657447338104248
CurrentTrain: epoch  2, batch    47 | loss: 6.6192832Losses:  5.8191094398498535 1.8840898275375366
CurrentTrain: epoch  2, batch    48 | loss: 5.8191094Losses:  6.417281150817871 1.9603028297424316
CurrentTrain: epoch  2, batch    49 | loss: 6.4172812Losses:  5.907623767852783 1.7732467651367188
CurrentTrain: epoch  2, batch    50 | loss: 5.9076238Losses:  5.806114673614502 2.0069186687469482
CurrentTrain: epoch  2, batch    51 | loss: 5.8061147Losses:  5.986079692840576 1.7750974893569946
CurrentTrain: epoch  2, batch    52 | loss: 5.9860797Losses:  6.04480504989624 2.0598905086517334
CurrentTrain: epoch  2, batch    53 | loss: 6.0448050Losses:  5.868671417236328 1.9958131313323975
CurrentTrain: epoch  2, batch    54 | loss: 5.8686714Losses:  5.984792709350586 2.0587339401245117
CurrentTrain: epoch  2, batch    55 | loss: 5.9847927Losses:  6.391628742218018 2.132580518722534
CurrentTrain: epoch  2, batch    56 | loss: 6.3916287Losses:  6.185764312744141 1.8479877710342407
CurrentTrain: epoch  2, batch    57 | loss: 6.1857643Losses:  5.918489456176758 1.798290491104126
CurrentTrain: epoch  2, batch    58 | loss: 5.9184895Losses:  5.8537187576293945 1.9420685768127441
CurrentTrain: epoch  2, batch    59 | loss: 5.8537188Losses:  5.638113975524902 1.931380271911621
CurrentTrain: epoch  2, batch    60 | loss: 5.6381140Losses:  5.910083770751953 1.8585164546966553
CurrentTrain: epoch  2, batch    61 | loss: 5.9100838Losses:  5.673315048217773 1.6562976837158203
CurrentTrain: epoch  2, batch    62 | loss: 5.6733150Losses:  6.503618240356445 1.9068446159362793
CurrentTrain: epoch  3, batch     0 | loss: 6.5036182Losses:  6.953463554382324 1.8113036155700684
CurrentTrain: epoch  3, batch     1 | loss: 6.9534636Losses:  5.935388565063477 2.006964921951294
CurrentTrain: epoch  3, batch     2 | loss: 5.9353886Losses:  6.2318549156188965 1.7543631792068481
CurrentTrain: epoch  3, batch     3 | loss: 6.2318549Losses:  6.128890037536621 1.9076640605926514
CurrentTrain: epoch  3, batch     4 | loss: 6.1288900Losses:  6.222866058349609 1.8923907279968262
CurrentTrain: epoch  3, batch     5 | loss: 6.2228661Losses:  5.874363422393799 1.918418049812317
CurrentTrain: epoch  3, batch     6 | loss: 5.8743634Losses:  5.953296661376953 2.0123353004455566
CurrentTrain: epoch  3, batch     7 | loss: 5.9532967Losses:  6.288697242736816 1.9207499027252197
CurrentTrain: epoch  3, batch     8 | loss: 6.2886972Losses:  6.274263858795166 2.0546417236328125
CurrentTrain: epoch  3, batch     9 | loss: 6.2742639Losses:  6.173764228820801 2.1396093368530273
CurrentTrain: epoch  3, batch    10 | loss: 6.1737642Losses:  5.960148334503174 1.9112892150878906
CurrentTrain: epoch  3, batch    11 | loss: 5.9601483Losses:  5.456364154815674 1.8671057224273682
CurrentTrain: epoch  3, batch    12 | loss: 5.4563642Losses:  6.119230270385742 2.1324353218078613
CurrentTrain: epoch  3, batch    13 | loss: 6.1192303Losses:  5.756135940551758 1.9746708869934082
CurrentTrain: epoch  3, batch    14 | loss: 5.7561359Losses:  5.6928606033325195 2.005155563354492
CurrentTrain: epoch  3, batch    15 | loss: 5.6928606Losses:  6.045166969299316 1.9588226079940796
CurrentTrain: epoch  3, batch    16 | loss: 6.0451670Losses:  6.160354137420654 1.7518622875213623
CurrentTrain: epoch  3, batch    17 | loss: 6.1603541Losses:  5.990131378173828 1.8755172491073608
CurrentTrain: epoch  3, batch    18 | loss: 5.9901314Losses:  5.895315647125244 1.899134874343872
CurrentTrain: epoch  3, batch    19 | loss: 5.8953156Losses:  5.765773773193359 1.9082109928131104
CurrentTrain: epoch  3, batch    20 | loss: 5.7657738Losses:  5.809528350830078 2.0494961738586426
CurrentTrain: epoch  3, batch    21 | loss: 5.8095284Losses:  5.761928081512451 1.773413896560669
CurrentTrain: epoch  3, batch    22 | loss: 5.7619281Losses:  5.741996765136719 2.0428640842437744
CurrentTrain: epoch  3, batch    23 | loss: 5.7419968Losses:  5.549655914306641 1.673128604888916
CurrentTrain: epoch  3, batch    24 | loss: 5.5496559Losses:  5.649470329284668 2.125415325164795
CurrentTrain: epoch  3, batch    25 | loss: 5.6494703Losses:  5.878159523010254 2.137777805328369
CurrentTrain: epoch  3, batch    26 | loss: 5.8781595Losses:  5.729049205780029 1.9772179126739502
CurrentTrain: epoch  3, batch    27 | loss: 5.7290492Losses:  5.909395694732666 2.1684494018554688
CurrentTrain: epoch  3, batch    28 | loss: 5.9093957Losses:  6.182805061340332 1.8858957290649414
CurrentTrain: epoch  3, batch    29 | loss: 6.1828051Losses:  6.512803077697754 2.0963048934936523
CurrentTrain: epoch  3, batch    30 | loss: 6.5128031Losses:  5.470202445983887 1.902014970779419
CurrentTrain: epoch  3, batch    31 | loss: 5.4702024Losses:  5.708702087402344 1.6733936071395874
CurrentTrain: epoch  3, batch    32 | loss: 5.7087021Losses:  5.756781578063965 1.8462088108062744
CurrentTrain: epoch  3, batch    33 | loss: 5.7567816Losses:  5.5762834548950195 1.914108395576477
CurrentTrain: epoch  3, batch    34 | loss: 5.5762835Losses:  5.616513729095459 1.9819563627243042
CurrentTrain: epoch  3, batch    35 | loss: 5.6165137Losses:  6.379594802856445 1.9962196350097656
CurrentTrain: epoch  3, batch    36 | loss: 6.3795948Losses:  5.755342960357666 1.9304133653640747
CurrentTrain: epoch  3, batch    37 | loss: 5.7553430Losses:  5.512217998504639 2.020655632019043
CurrentTrain: epoch  3, batch    38 | loss: 5.5122180Losses:  6.234543800354004 1.6936726570129395
CurrentTrain: epoch  3, batch    39 | loss: 6.2345438Losses:  5.656240463256836 2.0430712699890137
CurrentTrain: epoch  3, batch    40 | loss: 5.6562405Losses:  5.323291778564453 1.757959008216858
CurrentTrain: epoch  3, batch    41 | loss: 5.3232918Losses:  5.920009136199951 1.7608153820037842
CurrentTrain: epoch  3, batch    42 | loss: 5.9200091Losses:  5.630336761474609 1.830188512802124
CurrentTrain: epoch  3, batch    43 | loss: 5.6303368Losses:  5.96096134185791 2.0522091388702393
CurrentTrain: epoch  3, batch    44 | loss: 5.9609613Losses:  5.829765319824219 1.8179091215133667
CurrentTrain: epoch  3, batch    45 | loss: 5.8297653Losses:  5.428478717803955 2.0165092945098877
CurrentTrain: epoch  3, batch    46 | loss: 5.4284787Losses:  5.8308610916137695 1.7598583698272705
CurrentTrain: epoch  3, batch    47 | loss: 5.8308611Losses:  5.376391410827637 1.5918331146240234
CurrentTrain: epoch  3, batch    48 | loss: 5.3763914Losses:  5.511918067932129 1.6281752586364746
CurrentTrain: epoch  3, batch    49 | loss: 5.5119181Losses:  5.555154800415039 2.014660358428955
CurrentTrain: epoch  3, batch    50 | loss: 5.5551548Losses:  5.713916778564453 1.7795112133026123
CurrentTrain: epoch  3, batch    51 | loss: 5.7139168Losses:  5.498112678527832 1.88370680809021
CurrentTrain: epoch  3, batch    52 | loss: 5.4981127Losses:  6.1691060066223145 1.9595935344696045
CurrentTrain: epoch  3, batch    53 | loss: 6.1691060Losses:  5.471038818359375 1.9548182487487793
CurrentTrain: epoch  3, batch    54 | loss: 5.4710388Losses:  5.690363883972168 2.1326632499694824
CurrentTrain: epoch  3, batch    55 | loss: 5.6903639Losses:  5.402604579925537 1.861109733581543
CurrentTrain: epoch  3, batch    56 | loss: 5.4026046Losses:  5.822402000427246 1.8738579750061035
CurrentTrain: epoch  3, batch    57 | loss: 5.8224020Losses:  5.874794006347656 1.856905221939087
CurrentTrain: epoch  3, batch    58 | loss: 5.8747940Losses:  5.410567760467529 1.6646721363067627
CurrentTrain: epoch  3, batch    59 | loss: 5.4105678Losses:  5.58536434173584 2.0755958557128906
CurrentTrain: epoch  3, batch    60 | loss: 5.5853643Losses:  5.399686813354492 1.9394270181655884
CurrentTrain: epoch  3, batch    61 | loss: 5.3996868Losses:  5.160121917724609 1.4625449180603027
CurrentTrain: epoch  3, batch    62 | loss: 5.1601219Losses:  5.176025867462158 1.5544157028198242
CurrentTrain: epoch  4, batch     0 | loss: 5.1760259Losses:  5.377666473388672 1.776752233505249
CurrentTrain: epoch  4, batch     1 | loss: 5.3776665Losses:  5.499210357666016 1.9225749969482422
CurrentTrain: epoch  4, batch     2 | loss: 5.4992104Losses:  5.221890926361084 1.7296864986419678
CurrentTrain: epoch  4, batch     3 | loss: 5.2218909Losses:  5.605284690856934 2.1397013664245605
CurrentTrain: epoch  4, batch     4 | loss: 5.6052847Losses:  5.719307899475098 2.142220973968506
CurrentTrain: epoch  4, batch     5 | loss: 5.7193079Losses:  5.438889980316162 1.8652890920639038
CurrentTrain: epoch  4, batch     6 | loss: 5.4388900Losses:  5.286947250366211 1.718101978302002
CurrentTrain: epoch  4, batch     7 | loss: 5.2869473Losses:  5.475618362426758 2.1277432441711426
CurrentTrain: epoch  4, batch     8 | loss: 5.4756184Losses:  5.381706714630127 1.8920055627822876
CurrentTrain: epoch  4, batch     9 | loss: 5.3817067Losses:  5.402637004852295 1.8439624309539795
CurrentTrain: epoch  4, batch    10 | loss: 5.4026370Losses:  5.827864170074463 1.8856571912765503
CurrentTrain: epoch  4, batch    11 | loss: 5.8278642Losses:  5.484126567840576 1.8930671215057373
CurrentTrain: epoch  4, batch    12 | loss: 5.4841266Losses:  5.725451469421387 2.0089666843414307
CurrentTrain: epoch  4, batch    13 | loss: 5.7254515Losses:  5.081750869750977 1.7046277523040771
CurrentTrain: epoch  4, batch    14 | loss: 5.0817509Losses:  5.329619407653809 1.807044506072998
CurrentTrain: epoch  4, batch    15 | loss: 5.3296194Losses:  5.389671802520752 1.8557803630828857
CurrentTrain: epoch  4, batch    16 | loss: 5.3896718Losses:  5.27488899230957 1.805616021156311
CurrentTrain: epoch  4, batch    17 | loss: 5.2748890Losses:  5.407386779785156 2.0002522468566895
CurrentTrain: epoch  4, batch    18 | loss: 5.4073868Losses:  5.419643402099609 1.9835009574890137
CurrentTrain: epoch  4, batch    19 | loss: 5.4196434Losses:  5.757012367248535 1.7856557369232178
CurrentTrain: epoch  4, batch    20 | loss: 5.7570124Losses:  5.489175796508789 1.6763300895690918
CurrentTrain: epoch  4, batch    21 | loss: 5.4891758Losses:  5.164386749267578 1.7963409423828125
CurrentTrain: epoch  4, batch    22 | loss: 5.1643867Losses:  5.014568328857422 1.5450239181518555
CurrentTrain: epoch  4, batch    23 | loss: 5.0145683Losses:  5.338383197784424 1.5812146663665771
CurrentTrain: epoch  4, batch    24 | loss: 5.3383832Losses:  5.440707206726074 2.054734230041504
CurrentTrain: epoch  4, batch    25 | loss: 5.4407072Losses:  5.248624801635742 1.933326244354248
CurrentTrain: epoch  4, batch    26 | loss: 5.2486248Losses:  5.209159851074219 1.8317049741744995
CurrentTrain: epoch  4, batch    27 | loss: 5.2091599Losses:  5.325283527374268 1.8323757648468018
CurrentTrain: epoch  4, batch    28 | loss: 5.3252835Losses:  5.293791770935059 1.8186089992523193
CurrentTrain: epoch  4, batch    29 | loss: 5.2937918Losses:  5.2096734046936035 1.8867298364639282
CurrentTrain: epoch  4, batch    30 | loss: 5.2096734Losses:  5.335544586181641 1.8475209474563599
CurrentTrain: epoch  4, batch    31 | loss: 5.3355446Losses:  5.463431358337402 1.9158058166503906
CurrentTrain: epoch  4, batch    32 | loss: 5.4634314Losses:  5.4573974609375 2.015167713165283
CurrentTrain: epoch  4, batch    33 | loss: 5.4573975Losses:  5.27451229095459 1.8818795680999756
CurrentTrain: epoch  4, batch    34 | loss: 5.2745123Losses:  5.601098537445068 2.0586299896240234
CurrentTrain: epoch  4, batch    35 | loss: 5.6010985Losses:  5.279469966888428 1.7294281721115112
CurrentTrain: epoch  4, batch    36 | loss: 5.2794700Losses:  5.784677505493164 2.025966167449951
CurrentTrain: epoch  4, batch    37 | loss: 5.7846775Losses:  5.350693702697754 1.8458127975463867
CurrentTrain: epoch  4, batch    38 | loss: 5.3506937Losses:  5.251248836517334 1.8921650648117065
CurrentTrain: epoch  4, batch    39 | loss: 5.2512488Losses:  5.189571380615234 1.8327674865722656
CurrentTrain: epoch  4, batch    40 | loss: 5.1895714Losses:  5.211346626281738 1.9452545642852783
CurrentTrain: epoch  4, batch    41 | loss: 5.2113466Losses:  5.731501579284668 1.8981270790100098
CurrentTrain: epoch  4, batch    42 | loss: 5.7315016Losses:  5.578059196472168 1.6175484657287598
CurrentTrain: epoch  4, batch    43 | loss: 5.5780592Losses:  5.027814865112305 1.681781530380249
CurrentTrain: epoch  4, batch    44 | loss: 5.0278149Losses:  5.89167594909668 1.729154109954834
CurrentTrain: epoch  4, batch    45 | loss: 5.8916759Losses:  5.463005065917969 1.9109690189361572
CurrentTrain: epoch  4, batch    46 | loss: 5.4630051Losses:  5.397526264190674 1.9306690692901611
CurrentTrain: epoch  4, batch    47 | loss: 5.3975263Losses:  5.338970184326172 2.0523672103881836
CurrentTrain: epoch  4, batch    48 | loss: 5.3389702Losses:  5.328720569610596 1.9500939846038818
CurrentTrain: epoch  4, batch    49 | loss: 5.3287206Losses:  5.505989074707031 2.132582187652588
CurrentTrain: epoch  4, batch    50 | loss: 5.5059891Losses:  5.2683820724487305 1.8142131567001343
CurrentTrain: epoch  4, batch    51 | loss: 5.2683821Losses:  5.143280982971191 1.7071151733398438
CurrentTrain: epoch  4, batch    52 | loss: 5.1432810Losses:  5.604602336883545 1.8867712020874023
CurrentTrain: epoch  4, batch    53 | loss: 5.6046023Losses:  5.281954765319824 1.7689533233642578
CurrentTrain: epoch  4, batch    54 | loss: 5.2819548Losses:  5.272385597229004 1.9494071006774902
CurrentTrain: epoch  4, batch    55 | loss: 5.2723856Losses:  5.201140403747559 1.7599775791168213
CurrentTrain: epoch  4, batch    56 | loss: 5.2011404Losses:  5.4098711013793945 2.13368558883667
CurrentTrain: epoch  4, batch    57 | loss: 5.4098711Losses:  5.227296829223633 1.8734290599822998
CurrentTrain: epoch  4, batch    58 | loss: 5.2272968Losses:  5.2355265617370605 1.8946559429168701
CurrentTrain: epoch  4, batch    59 | loss: 5.2355266Losses:  5.216190338134766 1.873126745223999
CurrentTrain: epoch  4, batch    60 | loss: 5.2161903Losses:  5.385258674621582 1.9603502750396729
CurrentTrain: epoch  4, batch    61 | loss: 5.3852587Losses:  5.085047245025635 1.4979838132858276
CurrentTrain: epoch  4, batch    62 | loss: 5.0850472Losses:  5.4734392166137695 2.1036744117736816
CurrentTrain: epoch  5, batch     0 | loss: 5.4734392Losses:  5.314419746398926 1.9976330995559692
CurrentTrain: epoch  5, batch     1 | loss: 5.3144197Losses:  5.150791168212891 1.7241463661193848
CurrentTrain: epoch  5, batch     2 | loss: 5.1507912Losses:  5.21353816986084 1.8740555047988892
CurrentTrain: epoch  5, batch     3 | loss: 5.2135382Losses:  5.30188512802124 1.827030897140503
CurrentTrain: epoch  5, batch     4 | loss: 5.3018851Losses:  5.302245140075684 1.8799867630004883
CurrentTrain: epoch  5, batch     5 | loss: 5.3022451Losses:  5.3188090324401855 1.9624345302581787
CurrentTrain: epoch  5, batch     6 | loss: 5.3188090Losses:  5.209373950958252 1.991559624671936
CurrentTrain: epoch  5, batch     7 | loss: 5.2093740Losses:  5.339854717254639 1.9801753759384155
CurrentTrain: epoch  5, batch     8 | loss: 5.3398547Losses:  5.092962741851807 1.7110893726348877
CurrentTrain: epoch  5, batch     9 | loss: 5.0929627Losses:  5.6189165115356445 2.031163215637207
CurrentTrain: epoch  5, batch    10 | loss: 5.6189165Losses:  5.317385196685791 2.0601911544799805
CurrentTrain: epoch  5, batch    11 | loss: 5.3173852Losses:  5.171952247619629 1.911528468132019
CurrentTrain: epoch  5, batch    12 | loss: 5.1719522Losses:  5.154870510101318 1.8445123434066772
CurrentTrain: epoch  5, batch    13 | loss: 5.1548705Losses:  5.249539852142334 2.1579809188842773
CurrentTrain: epoch  5, batch    14 | loss: 5.2495399Losses:  5.169101715087891 1.8863295316696167
CurrentTrain: epoch  5, batch    15 | loss: 5.1691017Losses:  5.368924617767334 2.107250452041626
CurrentTrain: epoch  5, batch    16 | loss: 5.3689246Losses:  5.211706161499023 2.0343775749206543
CurrentTrain: epoch  5, batch    17 | loss: 5.2117062Losses:  5.2573137283325195 1.9710971117019653
CurrentTrain: epoch  5, batch    18 | loss: 5.2573137Losses:  5.1926679611206055 1.9009249210357666
CurrentTrain: epoch  5, batch    19 | loss: 5.1926680Losses:  5.156154155731201 1.9223530292510986
CurrentTrain: epoch  5, batch    20 | loss: 5.1561542Losses:  5.194775581359863 1.875846266746521
CurrentTrain: epoch  5, batch    21 | loss: 5.1947756Losses:  5.169376850128174 1.789703130722046
CurrentTrain: epoch  5, batch    22 | loss: 5.1693769Losses:  5.253571510314941 1.8890948295593262
CurrentTrain: epoch  5, batch    23 | loss: 5.2535715Losses:  5.120816230773926 1.782483696937561
CurrentTrain: epoch  5, batch    24 | loss: 5.1208162Losses:  5.2226033210754395 2.0117597579956055
CurrentTrain: epoch  5, batch    25 | loss: 5.2226033Losses:  5.252376079559326 1.9661864042282104
CurrentTrain: epoch  5, batch    26 | loss: 5.2523761Losses:  5.163111209869385 1.8473790884017944
CurrentTrain: epoch  5, batch    27 | loss: 5.1631112Losses:  5.209349632263184 2.059116840362549
CurrentTrain: epoch  5, batch    28 | loss: 5.2093496Losses:  5.18428897857666 2.0030770301818848
CurrentTrain: epoch  5, batch    29 | loss: 5.1842890Losses:  5.051234245300293 1.748337984085083
CurrentTrain: epoch  5, batch    30 | loss: 5.0512342Losses:  5.088627815246582 1.832125186920166
CurrentTrain: epoch  5, batch    31 | loss: 5.0886278Losses:  5.181734561920166 1.844735026359558
CurrentTrain: epoch  5, batch    32 | loss: 5.1817346Losses:  4.924243927001953 1.606185793876648
CurrentTrain: epoch  5, batch    33 | loss: 4.9242439Losses:  5.16710090637207 1.8753077983856201
CurrentTrain: epoch  5, batch    34 | loss: 5.1671009Losses:  5.023087024688721 1.757524847984314
CurrentTrain: epoch  5, batch    35 | loss: 5.0230870Losses:  5.047969341278076 1.7484216690063477
CurrentTrain: epoch  5, batch    36 | loss: 5.0479693Losses:  5.049936771392822 1.835326075553894
CurrentTrain: epoch  5, batch    37 | loss: 5.0499368Losses:  5.112685680389404 1.8286086320877075
CurrentTrain: epoch  5, batch    38 | loss: 5.1126857Losses:  5.252568244934082 1.9857871532440186
CurrentTrain: epoch  5, batch    39 | loss: 5.2525682Losses:  5.182589530944824 2.1231937408447266
CurrentTrain: epoch  5, batch    40 | loss: 5.1825895Losses:  5.10903263092041 1.8421900272369385
CurrentTrain: epoch  5, batch    41 | loss: 5.1090326Losses:  5.322175979614258 2.0174143314361572
CurrentTrain: epoch  5, batch    42 | loss: 5.3221760Losses:  5.4014811515808105 1.8319237232208252
CurrentTrain: epoch  5, batch    43 | loss: 5.4014812Losses:  5.122259140014648 1.9048629999160767
CurrentTrain: epoch  5, batch    44 | loss: 5.1222591Losses:  5.079071044921875 1.889688491821289
CurrentTrain: epoch  5, batch    45 | loss: 5.0790710Losses:  5.13167667388916 1.9620051383972168
CurrentTrain: epoch  5, batch    46 | loss: 5.1316767Losses:  5.138265132904053 1.975340485572815
CurrentTrain: epoch  5, batch    47 | loss: 5.1382651Losses:  5.197354316711426 2.0082309246063232
CurrentTrain: epoch  5, batch    48 | loss: 5.1973543Losses:  5.161183834075928 1.9820469617843628
CurrentTrain: epoch  5, batch    49 | loss: 5.1611838Losses:  5.09620475769043 1.9613652229309082
CurrentTrain: epoch  5, batch    50 | loss: 5.0962048Losses:  5.02938175201416 1.7758631706237793
CurrentTrain: epoch  5, batch    51 | loss: 5.0293818Losses:  5.261659145355225 2.087876319885254
CurrentTrain: epoch  5, batch    52 | loss: 5.2616591Losses:  5.138605117797852 1.9219653606414795
CurrentTrain: epoch  5, batch    53 | loss: 5.1386051Losses:  5.169519424438477 2.094977378845215
CurrentTrain: epoch  5, batch    54 | loss: 5.1695194Losses:  5.222578048706055 1.974487066268921
CurrentTrain: epoch  5, batch    55 | loss: 5.2225780Losses:  5.2068891525268555 2.0886292457580566
CurrentTrain: epoch  5, batch    56 | loss: 5.2068892Losses:  5.169273376464844 1.7730308771133423
CurrentTrain: epoch  5, batch    57 | loss: 5.1692734Losses:  5.286040782928467 1.8279484510421753
CurrentTrain: epoch  5, batch    58 | loss: 5.2860408Losses:  5.099650859832764 1.7663891315460205
CurrentTrain: epoch  5, batch    59 | loss: 5.0996509Losses:  5.056172847747803 1.868981122970581
CurrentTrain: epoch  5, batch    60 | loss: 5.0561728Losses:  5.034465312957764 1.8182848691940308
CurrentTrain: epoch  5, batch    61 | loss: 5.0344653Losses:  5.06464147567749 1.669602632522583
CurrentTrain: epoch  5, batch    62 | loss: 5.0646415Losses:  5.153501510620117 1.8655591011047363
CurrentTrain: epoch  6, batch     0 | loss: 5.1535015Losses:  5.090620994567871 1.9664329290390015
CurrentTrain: epoch  6, batch     1 | loss: 5.0906210Losses:  5.002836227416992 1.8265665769577026
CurrentTrain: epoch  6, batch     2 | loss: 5.0028362Losses:  5.426968574523926 2.1643824577331543
CurrentTrain: epoch  6, batch     3 | loss: 5.4269686Losses:  4.998269081115723 1.6894854307174683
CurrentTrain: epoch  6, batch     4 | loss: 4.9982691Losses:  5.1616621017456055 2.0128486156463623
CurrentTrain: epoch  6, batch     5 | loss: 5.1616621Losses:  5.165875434875488 2.018616199493408
CurrentTrain: epoch  6, batch     6 | loss: 5.1658754Losses:  5.102224826812744 1.8684265613555908
CurrentTrain: epoch  6, batch     7 | loss: 5.1022248Losses:  5.119239807128906 2.0268030166625977
CurrentTrain: epoch  6, batch     8 | loss: 5.1192398Losses:  4.96726131439209 1.6479288339614868
CurrentTrain: epoch  6, batch     9 | loss: 4.9672613Losses:  5.1473517417907715 2.004652976989746
CurrentTrain: epoch  6, batch    10 | loss: 5.1473517Losses:  5.019248008728027 1.6903960704803467
CurrentTrain: epoch  6, batch    11 | loss: 5.0192480Losses:  5.248378753662109 2.1044774055480957
CurrentTrain: epoch  6, batch    12 | loss: 5.2483788Losses:  5.011748313903809 1.7896842956542969
CurrentTrain: epoch  6, batch    13 | loss: 5.0117483Losses:  5.189356803894043 2.0683846473693848
CurrentTrain: epoch  6, batch    14 | loss: 5.1893568Losses:  5.144333362579346 1.9919211864471436
CurrentTrain: epoch  6, batch    15 | loss: 5.1443334Losses:  5.159704208374023 2.0283396244049072
CurrentTrain: epoch  6, batch    16 | loss: 5.1597042Losses:  5.134222030639648 2.0071496963500977
CurrentTrain: epoch  6, batch    17 | loss: 5.1342220Losses:  4.842704772949219 1.3726954460144043
CurrentTrain: epoch  6, batch    18 | loss: 4.8427048Losses:  5.122360706329346 2.0113537311553955
CurrentTrain: epoch  6, batch    19 | loss: 5.1223607Losses:  5.152649879455566 2.064702033996582
CurrentTrain: epoch  6, batch    20 | loss: 5.1526499Losses:  5.137869358062744 1.8961235284805298
CurrentTrain: epoch  6, batch    21 | loss: 5.1378694Losses:  5.05833101272583 1.7532367706298828
CurrentTrain: epoch  6, batch    22 | loss: 5.0583310Losses:  5.229727745056152 1.5742242336273193
CurrentTrain: epoch  6, batch    23 | loss: 5.2297277Losses:  5.066423416137695 1.8078771829605103
CurrentTrain: epoch  6, batch    24 | loss: 5.0664234Losses:  5.217123508453369 2.013399362564087
CurrentTrain: epoch  6, batch    25 | loss: 5.2171235Losses:  5.1657843589782715 2.0109450817108154
CurrentTrain: epoch  6, batch    26 | loss: 5.1657844Losses:  5.002638816833496 1.8095916509628296
CurrentTrain: epoch  6, batch    27 | loss: 5.0026388Losses:  5.064513683319092 1.955609679222107
CurrentTrain: epoch  6, batch    28 | loss: 5.0645137Losses:  5.130610942840576 2.0003862380981445
CurrentTrain: epoch  6, batch    29 | loss: 5.1306109Losses:  5.095703125 2.0034546852111816
CurrentTrain: epoch  6, batch    30 | loss: 5.0957031Losses:  4.84420108795166 1.6188116073608398
CurrentTrain: epoch  6, batch    31 | loss: 4.8442011Losses:  5.061354160308838 1.8105602264404297
CurrentTrain: epoch  6, batch    32 | loss: 5.0613542Losses:  5.142277717590332 2.0019659996032715
CurrentTrain: epoch  6, batch    33 | loss: 5.1422777Losses:  5.061581611633301 1.8464055061340332
CurrentTrain: epoch  6, batch    34 | loss: 5.0615816Losses:  4.998204231262207 1.636810541152954
CurrentTrain: epoch  6, batch    35 | loss: 4.9982042Losses:  4.9237823486328125 1.482095718383789
CurrentTrain: epoch  6, batch    36 | loss: 4.9237823Losses:  5.1540703773498535 2.0294103622436523
CurrentTrain: epoch  6, batch    37 | loss: 5.1540704Losses:  5.223405838012695 2.0962095260620117
CurrentTrain: epoch  6, batch    38 | loss: 5.2234058Losses:  4.97796630859375 1.8019142150878906
CurrentTrain: epoch  6, batch    39 | loss: 4.9779663Losses:  5.185825824737549 2.051945686340332
CurrentTrain: epoch  6, batch    40 | loss: 5.1858258Losses:  5.1749444007873535 2.1197729110717773
CurrentTrain: epoch  6, batch    41 | loss: 5.1749444Losses:  5.112331390380859 2.041041612625122
CurrentTrain: epoch  6, batch    42 | loss: 5.1123314Losses:  4.9464898109436035 1.795540452003479
CurrentTrain: epoch  6, batch    43 | loss: 4.9464898Losses:  5.02219820022583 1.859499216079712
CurrentTrain: epoch  6, batch    44 | loss: 5.0221982Losses:  5.123002529144287 1.9660526514053345
CurrentTrain: epoch  6, batch    45 | loss: 5.1230025Losses:  5.019992828369141 1.819352149963379
CurrentTrain: epoch  6, batch    46 | loss: 5.0199928Losses:  5.019227981567383 1.7731432914733887
CurrentTrain: epoch  6, batch    47 | loss: 5.0192280Losses:  5.16591739654541 2.1269469261169434
CurrentTrain: epoch  6, batch    48 | loss: 5.1659174Losses:  4.950334548950195 1.6934208869934082
CurrentTrain: epoch  6, batch    49 | loss: 4.9503345Losses:  5.165542125701904 2.0822877883911133
CurrentTrain: epoch  6, batch    50 | loss: 5.1655421Losses:  5.1233320236206055 2.046395778656006
CurrentTrain: epoch  6, batch    51 | loss: 5.1233320Losses:  5.110407829284668 2.0060906410217285
CurrentTrain: epoch  6, batch    52 | loss: 5.1104078Losses:  4.830049514770508 1.4094560146331787
CurrentTrain: epoch  6, batch    53 | loss: 4.8300495Losses:  5.058385848999023 1.9423139095306396
CurrentTrain: epoch  6, batch    54 | loss: 5.0583858Losses:  4.962876796722412 1.7897090911865234
CurrentTrain: epoch  6, batch    55 | loss: 4.9628768Losses:  4.957479476928711 1.626762866973877
CurrentTrain: epoch  6, batch    56 | loss: 4.9574795Losses:  5.078097820281982 2.031092405319214
CurrentTrain: epoch  6, batch    57 | loss: 5.0780978Losses:  5.04893684387207 1.9131425619125366
CurrentTrain: epoch  6, batch    58 | loss: 5.0489368Losses:  5.08500862121582 1.99074125289917
CurrentTrain: epoch  6, batch    59 | loss: 5.0850086Losses:  4.942700386047363 1.621199131011963
CurrentTrain: epoch  6, batch    60 | loss: 4.9427004Losses:  5.080169200897217 1.860070824623108
CurrentTrain: epoch  6, batch    61 | loss: 5.0801692Losses:  4.866640567779541 1.5794286727905273
CurrentTrain: epoch  6, batch    62 | loss: 4.8666406Losses:  5.01070499420166 1.7913243770599365
CurrentTrain: epoch  7, batch     0 | loss: 5.0107050Losses:  4.981878280639648 1.749210000038147
CurrentTrain: epoch  7, batch     1 | loss: 4.9818783Losses:  5.027920722961426 1.8630331754684448
CurrentTrain: epoch  7, batch     2 | loss: 5.0279207Losses:  5.108556270599365 2.0472068786621094
CurrentTrain: epoch  7, batch     3 | loss: 5.1085563Losses:  4.948459148406982 1.695542335510254
CurrentTrain: epoch  7, batch     4 | loss: 4.9484591Losses:  5.05858039855957 1.987840175628662
CurrentTrain: epoch  7, batch     5 | loss: 5.0585804Losses:  5.073729515075684 1.9882051944732666
CurrentTrain: epoch  7, batch     6 | loss: 5.0737295Losses:  5.17399787902832 2.0277562141418457
CurrentTrain: epoch  7, batch     7 | loss: 5.1739979Losses:  4.893335342407227 1.7446084022521973
CurrentTrain: epoch  7, batch     8 | loss: 4.8933353Losses:  4.970964431762695 1.8012409210205078
CurrentTrain: epoch  7, batch     9 | loss: 4.9709644Losses:  5.001093864440918 1.8938826322555542
CurrentTrain: epoch  7, batch    10 | loss: 5.0010939Losses:  4.928445816040039 1.8012523651123047
CurrentTrain: epoch  7, batch    11 | loss: 4.9284458Losses:  4.854579925537109 1.6296005249023438
CurrentTrain: epoch  7, batch    12 | loss: 4.8545799Losses:  4.981645584106445 1.7992664575576782
CurrentTrain: epoch  7, batch    13 | loss: 4.9816456Losses:  4.938452243804932 1.7949588298797607
CurrentTrain: epoch  7, batch    14 | loss: 4.9384522Losses:  4.955848693847656 1.8841626644134521
CurrentTrain: epoch  7, batch    15 | loss: 4.9558487Losses:  5.023920059204102 1.847771167755127
CurrentTrain: epoch  7, batch    16 | loss: 5.0239201Losses:  5.289437294006348 1.8703160285949707
CurrentTrain: epoch  7, batch    17 | loss: 5.2894373Losses:  5.090715408325195 2.041515827178955
CurrentTrain: epoch  7, batch    18 | loss: 5.0907154Losses:  5.113227844238281 2.002941608428955
CurrentTrain: epoch  7, batch    19 | loss: 5.1132278Losses:  4.9909234046936035 1.773253083229065
CurrentTrain: epoch  7, batch    20 | loss: 4.9909234Losses:  4.943692684173584 1.8032951354980469
CurrentTrain: epoch  7, batch    21 | loss: 4.9436927Losses:  4.9261579513549805 1.713373064994812
CurrentTrain: epoch  7, batch    22 | loss: 4.9261580Losses:  4.892197132110596 1.635924220085144
CurrentTrain: epoch  7, batch    23 | loss: 4.8921971Losses:  4.975316047668457 1.8556959629058838
CurrentTrain: epoch  7, batch    24 | loss: 4.9753160Losses:  5.00062370300293 1.6434803009033203
CurrentTrain: epoch  7, batch    25 | loss: 5.0006237Losses:  4.941440582275391 1.807542324066162
CurrentTrain: epoch  7, batch    26 | loss: 4.9414406Losses:  5.043752670288086 1.9659641981124878
CurrentTrain: epoch  7, batch    27 | loss: 5.0437527Losses:  5.007016181945801 1.8751893043518066
CurrentTrain: epoch  7, batch    28 | loss: 5.0070162Losses:  4.966177463531494 1.7816592454910278
CurrentTrain: epoch  7, batch    29 | loss: 4.9661775Losses:  5.063262939453125 1.930187463760376
CurrentTrain: epoch  7, batch    30 | loss: 5.0632629Losses:  5.015124320983887 1.8800280094146729
CurrentTrain: epoch  7, batch    31 | loss: 5.0151243Losses:  4.8952765464782715 1.636265754699707
CurrentTrain: epoch  7, batch    32 | loss: 4.8952765Losses:  5.070453643798828 1.7828421592712402
CurrentTrain: epoch  7, batch    33 | loss: 5.0704536Losses:  4.950034141540527 1.6738845109939575
CurrentTrain: epoch  7, batch    34 | loss: 4.9500341Losses:  5.029110908508301 1.9352967739105225
CurrentTrain: epoch  7, batch    35 | loss: 5.0291109Losses:  5.065667629241943 1.859838843345642
CurrentTrain: epoch  7, batch    36 | loss: 5.0656676Losses:  4.929945468902588 1.7885081768035889
CurrentTrain: epoch  7, batch    37 | loss: 4.9299455Losses:  5.090427398681641 2.042452335357666
CurrentTrain: epoch  7, batch    38 | loss: 5.0904274Losses:  4.919871807098389 1.784030795097351
CurrentTrain: epoch  7, batch    39 | loss: 4.9198718Losses:  5.0474748611450195 1.9839138984680176
CurrentTrain: epoch  7, batch    40 | loss: 5.0474749Losses:  4.901320457458496 1.7609577178955078
CurrentTrain: epoch  7, batch    41 | loss: 4.9013205Losses:  4.964520454406738 1.8504908084869385
CurrentTrain: epoch  7, batch    42 | loss: 4.9645205Losses:  4.977524757385254 1.8289856910705566
CurrentTrain: epoch  7, batch    43 | loss: 4.9775248Losses:  5.013912677764893 1.9379303455352783
CurrentTrain: epoch  7, batch    44 | loss: 5.0139127Losses:  4.995124340057373 1.9920942783355713
CurrentTrain: epoch  7, batch    45 | loss: 4.9951243Losses:  4.9529290199279785 1.7908942699432373
CurrentTrain: epoch  7, batch    46 | loss: 4.9529290Losses:  5.014832496643066 1.9255043268203735
CurrentTrain: epoch  7, batch    47 | loss: 5.0148325Losses:  4.996880054473877 1.939824104309082
CurrentTrain: epoch  7, batch    48 | loss: 4.9968801Losses:  5.031345844268799 1.9799950122833252
CurrentTrain: epoch  7, batch    49 | loss: 5.0313458Losses:  4.829410552978516 1.70755934715271
CurrentTrain: epoch  7, batch    50 | loss: 4.8294106Losses:  5.04442024230957 1.9968208074569702
CurrentTrain: epoch  7, batch    51 | loss: 5.0444202Losses:  4.915063858032227 1.6170713901519775
CurrentTrain: epoch  7, batch    52 | loss: 4.9150639Losses:  5.040951251983643 1.9156075716018677
CurrentTrain: epoch  7, batch    53 | loss: 5.0409513Losses:  4.993255615234375 1.8233661651611328
CurrentTrain: epoch  7, batch    54 | loss: 4.9932556Losses:  4.922205924987793 1.7514472007751465
CurrentTrain: epoch  7, batch    55 | loss: 4.9222059Losses:  4.886435508728027 1.6428050994873047
CurrentTrain: epoch  7, batch    56 | loss: 4.8864355Losses:  5.028570175170898 2.002849817276001
CurrentTrain: epoch  7, batch    57 | loss: 5.0285702Losses:  5.034693241119385 2.0234298706054688
CurrentTrain: epoch  7, batch    58 | loss: 5.0346932Losses:  4.88560676574707 1.6399497985839844
CurrentTrain: epoch  7, batch    59 | loss: 4.8856068Losses:  5.082305431365967 2.0317468643188477
CurrentTrain: epoch  7, batch    60 | loss: 5.0823054Losses:  4.987597465515137 1.867147445678711
CurrentTrain: epoch  7, batch    61 | loss: 4.9875975Losses:  4.900842666625977 1.6346070766448975
CurrentTrain: epoch  7, batch    62 | loss: 4.9008427Losses:  5.087041854858398 2.089846134185791
CurrentTrain: epoch  8, batch     0 | loss: 5.0870419Losses:  5.027148246765137 1.910856008529663
CurrentTrain: epoch  8, batch     1 | loss: 5.0271482Losses:  4.986818313598633 1.8850560188293457
CurrentTrain: epoch  8, batch     2 | loss: 4.9868183Losses:  4.931766510009766 1.7310447692871094
CurrentTrain: epoch  8, batch     3 | loss: 4.9317665Losses:  4.893102169036865 1.7730112075805664
CurrentTrain: epoch  8, batch     4 | loss: 4.8931022Losses:  4.997735023498535 1.9106590747833252
CurrentTrain: epoch  8, batch     5 | loss: 4.9977350Losses:  5.020073890686035 1.9577882289886475
CurrentTrain: epoch  8, batch     6 | loss: 5.0200739Losses:  5.0560760498046875 1.9709093570709229
CurrentTrain: epoch  8, batch     7 | loss: 5.0560760Losses:  5.01651668548584 1.9394378662109375
CurrentTrain: epoch  8, batch     8 | loss: 5.0165167Losses:  5.044407367706299 1.9806206226348877
CurrentTrain: epoch  8, batch     9 | loss: 5.0444074Losses:  4.964755058288574 1.904293417930603
CurrentTrain: epoch  8, batch    10 | loss: 4.9647551Losses:  4.957904815673828 1.8511368036270142
CurrentTrain: epoch  8, batch    11 | loss: 4.9579048Losses:  5.028491020202637 1.9133703708648682
CurrentTrain: epoch  8, batch    12 | loss: 5.0284910Losses:  4.969290733337402 1.8550233840942383
CurrentTrain: epoch  8, batch    13 | loss: 4.9692907Losses:  4.957210540771484 1.8584668636322021
CurrentTrain: epoch  8, batch    14 | loss: 4.9572105Losses:  4.902812480926514 1.763268232345581
CurrentTrain: epoch  8, batch    15 | loss: 4.9028125Losses:  4.932276248931885 1.8675615787506104
CurrentTrain: epoch  8, batch    16 | loss: 4.9322762Losses:  5.0271782875061035 1.9884475469589233
CurrentTrain: epoch  8, batch    17 | loss: 5.0271783Losses:  5.003046989440918 1.9274144172668457
CurrentTrain: epoch  8, batch    18 | loss: 5.0030470Losses:  5.087031364440918 1.9485888481140137
CurrentTrain: epoch  8, batch    19 | loss: 5.0870314Losses:  4.845248699188232 1.789756417274475
CurrentTrain: epoch  8, batch    20 | loss: 4.8452487Losses:  4.972002029418945 1.894468903541565
CurrentTrain: epoch  8, batch    21 | loss: 4.9720020Losses:  4.82731819152832 1.614809513092041
CurrentTrain: epoch  8, batch    22 | loss: 4.8273182Losses:  5.096547603607178 2.1137115955352783
CurrentTrain: epoch  8, batch    23 | loss: 5.0965476Losses:  5.134003639221191 2.149448871612549
CurrentTrain: epoch  8, batch    24 | loss: 5.1340036Losses:  4.870556354522705 1.693955659866333
CurrentTrain: epoch  8, batch    25 | loss: 4.8705564Losses:  4.917428970336914 1.7492140531539917
CurrentTrain: epoch  8, batch    26 | loss: 4.9174290Losses:  5.012764930725098 1.8550362586975098
CurrentTrain: epoch  8, batch    27 | loss: 5.0127649Losses:  5.07588005065918 2.107916831970215
CurrentTrain: epoch  8, batch    28 | loss: 5.0758801Losses:  5.006864547729492 1.9400733709335327
CurrentTrain: epoch  8, batch    29 | loss: 5.0068645Losses:  4.944463729858398 1.8339085578918457
CurrentTrain: epoch  8, batch    30 | loss: 4.9444637Losses:  4.915658473968506 1.769270658493042
CurrentTrain: epoch  8, batch    31 | loss: 4.9156585Losses:  4.8770751953125 1.658029556274414
CurrentTrain: epoch  8, batch    32 | loss: 4.8770752Losses:  4.996507167816162 1.84549081325531
CurrentTrain: epoch  8, batch    33 | loss: 4.9965072Losses:  5.113054275512695 2.1447625160217285
CurrentTrain: epoch  8, batch    34 | loss: 5.1130543Losses:  4.956893444061279 1.8575094938278198
CurrentTrain: epoch  8, batch    35 | loss: 4.9568934Losses:  4.950048446655273 1.8033254146575928
CurrentTrain: epoch  8, batch    36 | loss: 4.9500484Losses:  4.989344596862793 1.824546456336975
CurrentTrain: epoch  8, batch    37 | loss: 4.9893446Losses:  4.875731945037842 1.7620526552200317
CurrentTrain: epoch  8, batch    38 | loss: 4.8757319Losses:  5.027316093444824 1.907153606414795
CurrentTrain: epoch  8, batch    39 | loss: 5.0273161Losses:  5.115484714508057 2.0035321712493896
CurrentTrain: epoch  8, batch    40 | loss: 5.1154847Losses:  5.060612678527832 2.0451009273529053
CurrentTrain: epoch  8, batch    41 | loss: 5.0606127Losses:  5.026740074157715 1.960979700088501
CurrentTrain: epoch  8, batch    42 | loss: 5.0267401Losses:  5.083558082580566 1.9835395812988281
CurrentTrain: epoch  8, batch    43 | loss: 5.0835581Losses:  5.040449142456055 1.9909902811050415
CurrentTrain: epoch  8, batch    44 | loss: 5.0404491Losses:  4.949141979217529 1.7229082584381104
CurrentTrain: epoch  8, batch    45 | loss: 4.9491420Losses:  4.848716735839844 1.710343360900879
CurrentTrain: epoch  8, batch    46 | loss: 4.8487167Losses:  4.840803146362305 1.5855610370635986
CurrentTrain: epoch  8, batch    47 | loss: 4.8408031Losses:  4.8953399658203125 1.6976487636566162
CurrentTrain: epoch  8, batch    48 | loss: 4.8953400Losses:  4.928134918212891 1.798309087753296
CurrentTrain: epoch  8, batch    49 | loss: 4.9281349Losses:  4.970580101013184 1.902757167816162
CurrentTrain: epoch  8, batch    50 | loss: 4.9705801Losses:  4.827240943908691 1.5536713600158691
CurrentTrain: epoch  8, batch    51 | loss: 4.8272409Losses:  5.038095474243164 1.987634539604187
CurrentTrain: epoch  8, batch    52 | loss: 5.0380955Losses:  4.992341041564941 1.8949755430221558
CurrentTrain: epoch  8, batch    53 | loss: 4.9923410Losses:  5.030803680419922 2.0262551307678223
CurrentTrain: epoch  8, batch    54 | loss: 5.0308037Losses:  4.863114356994629 1.7210333347320557
CurrentTrain: epoch  8, batch    55 | loss: 4.8631144Losses:  4.957163333892822 1.831618309020996
CurrentTrain: epoch  8, batch    56 | loss: 4.9571633Losses:  5.090780735015869 2.116544485092163
CurrentTrain: epoch  8, batch    57 | loss: 5.0907807Losses:  4.955683708190918 1.828345775604248
CurrentTrain: epoch  8, batch    58 | loss: 4.9556837Losses:  4.992655277252197 1.9216995239257812
CurrentTrain: epoch  8, batch    59 | loss: 4.9926553Losses:  4.86358642578125 1.7826921939849854
CurrentTrain: epoch  8, batch    60 | loss: 4.8635864Losses:  5.061110019683838 2.0614147186279297
CurrentTrain: epoch  8, batch    61 | loss: 5.0611100Losses:  4.674477577209473 1.3283803462982178
CurrentTrain: epoch  8, batch    62 | loss: 4.6744776Losses:  4.935126304626465 1.853710651397705
CurrentTrain: epoch  9, batch     0 | loss: 4.9351263Losses:  4.98699426651001 1.7554843425750732
CurrentTrain: epoch  9, batch     1 | loss: 4.9869943Losses:  5.053516387939453 2.0124306678771973
CurrentTrain: epoch  9, batch     2 | loss: 5.0535164Losses:  4.978916168212891 1.832836627960205
CurrentTrain: epoch  9, batch     3 | loss: 4.9789162Losses:  4.862236022949219 1.6740598678588867
CurrentTrain: epoch  9, batch     4 | loss: 4.8622360Losses:  4.942348480224609 1.8689709901809692
CurrentTrain: epoch  9, batch     5 | loss: 4.9423485Losses:  4.949113845825195 1.8200396299362183
CurrentTrain: epoch  9, batch     6 | loss: 4.9491138Losses:  4.961385726928711 1.8743470907211304
CurrentTrain: epoch  9, batch     7 | loss: 4.9613857Losses:  4.83574914932251 1.6973145008087158
CurrentTrain: epoch  9, batch     8 | loss: 4.8357491Losses:  4.97927713394165 1.9124305248260498
CurrentTrain: epoch  9, batch     9 | loss: 4.9792771Losses:  5.0359416007995605 2.034846067428589
CurrentTrain: epoch  9, batch    10 | loss: 5.0359416Losses:  5.048305034637451 2.0190324783325195
CurrentTrain: epoch  9, batch    11 | loss: 5.0483050Losses:  5.015083312988281 1.9793859720230103
CurrentTrain: epoch  9, batch    12 | loss: 5.0150833Losses:  5.010476112365723 1.952768325805664
CurrentTrain: epoch  9, batch    13 | loss: 5.0104761Losses:  5.088418006896973 2.139774799346924
CurrentTrain: epoch  9, batch    14 | loss: 5.0884180Losses:  5.097729206085205 2.090452194213867
CurrentTrain: epoch  9, batch    15 | loss: 5.0977292Losses:  4.982409954071045 1.939036250114441
CurrentTrain: epoch  9, batch    16 | loss: 4.9824100Losses:  5.039085388183594 1.9644112586975098
CurrentTrain: epoch  9, batch    17 | loss: 5.0390854Losses:  4.981080055236816 1.9116965532302856
CurrentTrain: epoch  9, batch    18 | loss: 4.9810801Losses:  4.792715072631836 1.6104655265808105
CurrentTrain: epoch  9, batch    19 | loss: 4.7927151Losses:  5.013646602630615 1.9117050170898438
CurrentTrain: epoch  9, batch    20 | loss: 5.0136466Losses:  4.9673309326171875 1.907381296157837
CurrentTrain: epoch  9, batch    21 | loss: 4.9673309Losses:  5.029951572418213 1.9317443370819092
CurrentTrain: epoch  9, batch    22 | loss: 5.0299516Losses:  4.956136703491211 1.9082598686218262
CurrentTrain: epoch  9, batch    23 | loss: 4.9561367Losses:  4.918594837188721 1.8061474561691284
CurrentTrain: epoch  9, batch    24 | loss: 4.9185948Losses:  4.922804832458496 1.7782490253448486
CurrentTrain: epoch  9, batch    25 | loss: 4.9228048Losses:  4.946132183074951 1.9063148498535156
CurrentTrain: epoch  9, batch    26 | loss: 4.9461322Losses:  4.956537246704102 1.8985750675201416
CurrentTrain: epoch  9, batch    27 | loss: 4.9565372Losses:  5.049441814422607 2.1012661457061768
CurrentTrain: epoch  9, batch    28 | loss: 5.0494418Losses:  4.914726257324219 1.8079904317855835
CurrentTrain: epoch  9, batch    29 | loss: 4.9147263Losses:  4.986896991729736 1.8975050449371338
CurrentTrain: epoch  9, batch    30 | loss: 4.9868970Losses:  4.954468727111816 1.926701545715332
CurrentTrain: epoch  9, batch    31 | loss: 4.9544687Losses:  4.899764060974121 1.7969095706939697
CurrentTrain: epoch  9, batch    32 | loss: 4.8997641Losses:  4.7871413230896 1.6311595439910889
CurrentTrain: epoch  9, batch    33 | loss: 4.7871413Losses:  5.047153472900391 2.0647215843200684
CurrentTrain: epoch  9, batch    34 | loss: 5.0471535Losses:  5.065706253051758 2.1121270656585693
CurrentTrain: epoch  9, batch    35 | loss: 5.0657063Losses:  4.809412479400635 1.6079390048980713
CurrentTrain: epoch  9, batch    36 | loss: 4.8094125Losses:  5.019942283630371 2.018498659133911
CurrentTrain: epoch  9, batch    37 | loss: 5.0199423Losses:  4.993959426879883 1.8786060810089111
CurrentTrain: epoch  9, batch    38 | loss: 4.9939594Losses:  5.180331230163574 1.961707592010498
CurrentTrain: epoch  9, batch    39 | loss: 5.1803312Losses:  4.964028835296631 1.7488107681274414
CurrentTrain: epoch  9, batch    40 | loss: 4.9640288Losses:  4.845365524291992 1.7004374265670776
CurrentTrain: epoch  9, batch    41 | loss: 4.8453655Losses:  4.942477226257324 1.8296983242034912
CurrentTrain: epoch  9, batch    42 | loss: 4.9424772Losses:  4.876123428344727 1.7566663026809692
CurrentTrain: epoch  9, batch    43 | loss: 4.8761234Losses:  4.9187116622924805 1.803727388381958
CurrentTrain: epoch  9, batch    44 | loss: 4.9187117Losses:  4.869002342224121 1.7613883018493652
CurrentTrain: epoch  9, batch    45 | loss: 4.8690023Losses:  4.860486030578613 1.7490448951721191
CurrentTrain: epoch  9, batch    46 | loss: 4.8604860Losses:  4.819891452789307 1.585921049118042
CurrentTrain: epoch  9, batch    47 | loss: 4.8198915Losses:  5.021603584289551 1.9950350522994995
CurrentTrain: epoch  9, batch    48 | loss: 5.0216036Losses:  5.0799055099487305 1.8758740425109863
CurrentTrain: epoch  9, batch    49 | loss: 5.0799055Losses:  5.105196475982666 2.0271787643432617
CurrentTrain: epoch  9, batch    50 | loss: 5.1051965Losses:  5.071650505065918 2.0284576416015625
CurrentTrain: epoch  9, batch    51 | loss: 5.0716505Losses:  4.977426528930664 1.9111418724060059
CurrentTrain: epoch  9, batch    52 | loss: 4.9774265Losses:  4.947582244873047 1.9026732444763184
CurrentTrain: epoch  9, batch    53 | loss: 4.9475822Losses:  4.945158004760742 1.8541598320007324
CurrentTrain: epoch  9, batch    54 | loss: 4.9451580Losses:  4.926809787750244 1.846092939376831
CurrentTrain: epoch  9, batch    55 | loss: 4.9268098Losses:  4.997505187988281 2.0241878032684326
CurrentTrain: epoch  9, batch    56 | loss: 4.9975052Losses:  5.017673492431641 1.9831275939941406
CurrentTrain: epoch  9, batch    57 | loss: 5.0176735Losses:  5.119117259979248 1.8862569332122803
CurrentTrain: epoch  9, batch    58 | loss: 5.1191173Losses:  4.976982116699219 1.9588274955749512
CurrentTrain: epoch  9, batch    59 | loss: 4.9769821Losses:  4.987293243408203 1.927889347076416
CurrentTrain: epoch  9, batch    60 | loss: 4.9872932Losses:  4.996402263641357 1.9499006271362305
CurrentTrain: epoch  9, batch    61 | loss: 4.9964023Losses:  4.809760570526123 1.6156642436981201
CurrentTrain: epoch  9, batch    62 | loss: 4.8097606
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
cur_acc:  ['0.9425']
his_acc:  ['0.9425']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  7.456461429595947 1.8929307460784912
CurrentTrain: epoch  0, batch     0 | loss: 7.4564614Losses:  6.805246353149414 1.9129581451416016
CurrentTrain: epoch  0, batch     1 | loss: 6.8052464Losses:  7.152182579040527 2.008298873901367
CurrentTrain: epoch  0, batch     2 | loss: 7.1521826Losses:  8.02199935913086 0.0
CurrentTrain: epoch  0, batch     3 | loss: 8.0219994Losses:  6.936007976531982 2.05881404876709
CurrentTrain: epoch  1, batch     0 | loss: 6.9360080Losses:  6.843667507171631 2.1796987056732178
CurrentTrain: epoch  1, batch     1 | loss: 6.8436675Losses:  6.2483062744140625 2.122004985809326
CurrentTrain: epoch  1, batch     2 | loss: 6.2483063Losses:  3.4801008701324463 0.603096604347229
CurrentTrain: epoch  1, batch     3 | loss: 3.4801009Losses:  6.193305015563965 1.8861979246139526
CurrentTrain: epoch  2, batch     0 | loss: 6.1933050Losses:  5.544570446014404 1.9740285873413086
CurrentTrain: epoch  2, batch     1 | loss: 5.5445704Losses:  5.365284442901611 2.021510124206543
CurrentTrain: epoch  2, batch     2 | loss: 5.3652844Losses:  7.69124174118042 0.6517843008041382
CurrentTrain: epoch  2, batch     3 | loss: 7.6912417Losses:  6.133454322814941 2.1381173133850098
CurrentTrain: epoch  3, batch     0 | loss: 6.1334543Losses:  4.591394424438477 1.9368157386779785
CurrentTrain: epoch  3, batch     1 | loss: 4.5913944Losses:  5.309965133666992 2.0242645740509033
CurrentTrain: epoch  3, batch     2 | loss: 5.3099651Losses:  3.344454526901245 0.6791443824768066
CurrentTrain: epoch  3, batch     3 | loss: 3.3444545Losses:  4.7204179763793945 1.8862138986587524
CurrentTrain: epoch  4, batch     0 | loss: 4.7204180Losses:  4.798135757446289 1.8462042808532715
CurrentTrain: epoch  4, batch     1 | loss: 4.7981358Losses:  4.754001140594482 1.9026079177856445
CurrentTrain: epoch  4, batch     2 | loss: 4.7540011Losses:  3.802464008331299 0.6439467668533325
CurrentTrain: epoch  4, batch     3 | loss: 3.8024640Losses:  4.683130264282227 1.8936398029327393
CurrentTrain: epoch  5, batch     0 | loss: 4.6831303Losses:  3.800323963165283 1.9016168117523193
CurrentTrain: epoch  5, batch     1 | loss: 3.8003240Losses:  4.059212684631348 2.1616897583007812
CurrentTrain: epoch  5, batch     2 | loss: 4.0592127Losses:  5.587011814117432 0.6469886898994446
CurrentTrain: epoch  5, batch     3 | loss: 5.5870118Losses:  4.231133460998535 1.9415583610534668
CurrentTrain: epoch  6, batch     0 | loss: 4.2311335Losses:  3.9538748264312744 1.86628258228302
CurrentTrain: epoch  6, batch     1 | loss: 3.9538748Losses:  3.7132787704467773 1.9363677501678467
CurrentTrain: epoch  6, batch     2 | loss: 3.7132788Losses:  2.446897506713867 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4468975Losses:  4.156674385070801 2.0234103202819824
CurrentTrain: epoch  7, batch     0 | loss: 4.1566744Losses:  3.826580047607422 2.1275033950805664
CurrentTrain: epoch  7, batch     1 | loss: 3.8265800Losses:  3.6784889698028564 1.92800772190094
CurrentTrain: epoch  7, batch     2 | loss: 3.6784890Losses:  2.0731658935546875 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.0731659Losses:  3.65039324760437 2.0108890533447266
CurrentTrain: epoch  8, batch     0 | loss: 3.6503932Losses:  3.842123508453369 1.777894139289856
CurrentTrain: epoch  8, batch     1 | loss: 3.8421235Losses:  3.6874589920043945 2.008653163909912
CurrentTrain: epoch  8, batch     2 | loss: 3.6874590Losses:  2.3473706245422363 0.6124715805053711
CurrentTrain: epoch  8, batch     3 | loss: 2.3473706Losses:  3.4302854537963867 2.0044138431549072
CurrentTrain: epoch  9, batch     0 | loss: 3.4302855Losses:  4.046009063720703 1.768646240234375
CurrentTrain: epoch  9, batch     1 | loss: 4.0460091Losses:  3.296851873397827 1.9849505424499512
CurrentTrain: epoch  9, batch     2 | loss: 3.2968519Losses:  2.1602349281311035 0.6151047348976135
CurrentTrain: epoch  9, batch     3 | loss: 2.1602349
Losses:  3.5975987911224365 2.6668314933776855
MemoryTrain:  epoch  0, batch     0 | loss: 3.5975988Losses:  0.7635893821716309 1.2922897338867188
MemoryTrain:  epoch  0, batch     1 | loss: 0.7635894Losses:  3.0214762687683105 2.6677541732788086
MemoryTrain:  epoch  1, batch     0 | loss: 3.0214763Losses:  1.494971752166748 1.2951817512512207
MemoryTrain:  epoch  1, batch     1 | loss: 1.4949718Losses:  2.835468053817749 2.661839008331299
MemoryTrain:  epoch  2, batch     0 | loss: 2.8354681Losses:  0.9507535696029663 1.3140510320663452
MemoryTrain:  epoch  2, batch     1 | loss: 0.9507536Losses:  2.0879111289978027 2.673884391784668
MemoryTrain:  epoch  3, batch     0 | loss: 2.0879111Losses:  2.68030047416687 1.301516056060791
MemoryTrain:  epoch  3, batch     1 | loss: 2.6803005Losses:  2.3305044174194336 2.6627588272094727
MemoryTrain:  epoch  4, batch     0 | loss: 2.3305044Losses:  0.7743920683860779 1.29817795753479
MemoryTrain:  epoch  4, batch     1 | loss: 0.7743921Losses:  2.196185827255249 2.661187171936035
MemoryTrain:  epoch  5, batch     0 | loss: 2.1961858Losses:  0.7631198763847351 1.277767300605774
MemoryTrain:  epoch  5, batch     1 | loss: 0.7631199Losses:  2.063366651535034 2.648163318634033
MemoryTrain:  epoch  6, batch     0 | loss: 2.0633667Losses:  0.7533142566680908 1.316460371017456
MemoryTrain:  epoch  6, batch     1 | loss: 0.7533143Losses:  1.699735164642334 2.656144618988037
MemoryTrain:  epoch  7, batch     0 | loss: 1.6997352Losses:  1.8817243576049805 1.2948477268218994
MemoryTrain:  epoch  7, batch     1 | loss: 1.8817244Losses:  1.959486961364746 2.6570401191711426
MemoryTrain:  epoch  8, batch     0 | loss: 1.9594870Losses:  0.6754594445228577 1.2972100973129272
MemoryTrain:  epoch  8, batch     1 | loss: 0.6754594Losses:  1.8260760307312012 2.6607913970947266
MemoryTrain:  epoch  9, batch     0 | loss: 1.8260760Losses:  0.9851598739624023 1.2828497886657715
MemoryTrain:  epoch  9, batch     1 | loss: 0.9851599
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 74.31%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 71.61%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.10%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.64%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.68%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.00%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 93.14%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.98%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.82%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.76%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 92.23%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.56%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.46%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.27%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.84%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.52%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 90.48%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.31%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 90.07%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.01%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 89.85%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.30%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 89.01%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 88.80%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 88.40%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 88.14%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 87.88%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.13%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.89%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.47%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 86.18%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 85.67%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 85.28%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 84.72%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 84.17%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.75%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 83.22%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.92%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.69%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.84%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 83.32%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 84.20%   
cur_acc:  ['0.9425', '0.7510']
his_acc:  ['0.9425', '0.8420']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  7.926303863525391 1.9592280387878418
CurrentTrain: epoch  0, batch     0 | loss: 7.9263039Losses:  7.108555793762207 2.0339272022247314
CurrentTrain: epoch  0, batch     1 | loss: 7.1085558Losses:  7.8904266357421875 2.1484217643737793
CurrentTrain: epoch  0, batch     2 | loss: 7.8904266Losses:  7.820187568664551 0.6452452540397644
CurrentTrain: epoch  0, batch     3 | loss: 7.8201876Losses:  6.511488914489746 2.1749725341796875
CurrentTrain: epoch  1, batch     0 | loss: 6.5114889Losses:  7.452488899230957 1.958581805229187
CurrentTrain: epoch  1, batch     1 | loss: 7.4524889Losses:  6.200709342956543 2.145503520965576
CurrentTrain: epoch  1, batch     2 | loss: 6.2007093Losses:  6.169683933258057 0.6628811359405518
CurrentTrain: epoch  1, batch     3 | loss: 6.1696839Losses:  5.353061676025391 1.9037302732467651
CurrentTrain: epoch  2, batch     0 | loss: 5.3530617Losses:  6.2844648361206055 2.1227529048919678
CurrentTrain: epoch  2, batch     1 | loss: 6.2844648Losses:  6.806375503540039 1.9924416542053223
CurrentTrain: epoch  2, batch     2 | loss: 6.8063755Losses:  3.7565391063690186 0.652064323425293
CurrentTrain: epoch  2, batch     3 | loss: 3.7565391Losses:  5.793807029724121 2.111393451690674
CurrentTrain: epoch  3, batch     0 | loss: 5.7938070Losses:  5.01329231262207 1.8963279724121094
CurrentTrain: epoch  3, batch     1 | loss: 5.0132923Losses:  6.033456802368164 2.0666885375976562
CurrentTrain: epoch  3, batch     2 | loss: 6.0334568Losses:  5.467104911804199 0.6598477363586426
CurrentTrain: epoch  3, batch     3 | loss: 5.4671049Losses:  5.888797760009766 1.9845623970031738
CurrentTrain: epoch  4, batch     0 | loss: 5.8887978Losses:  5.433630466461182 2.154815673828125
CurrentTrain: epoch  4, batch     1 | loss: 5.4336305Losses:  4.100625038146973 1.8690412044525146
CurrentTrain: epoch  4, batch     2 | loss: 4.1006250Losses:  4.8006815910339355 0.6520048379898071
CurrentTrain: epoch  4, batch     3 | loss: 4.8006816Losses:  5.471444129943848 1.786454677581787
CurrentTrain: epoch  5, batch     0 | loss: 5.4714441Losses:  4.626938343048096 1.9769504070281982
CurrentTrain: epoch  5, batch     1 | loss: 4.6269383Losses:  4.50641393661499 1.820021390914917
CurrentTrain: epoch  5, batch     2 | loss: 4.5064139Losses:  7.578824043273926 0.6701343655586243
CurrentTrain: epoch  5, batch     3 | loss: 7.5788240Losses:  5.465084075927734 2.156259059906006
CurrentTrain: epoch  6, batch     0 | loss: 5.4650841Losses:  4.976613521575928 2.0799617767333984
CurrentTrain: epoch  6, batch     1 | loss: 4.9766135Losses:  4.225855350494385 2.0625627040863037
CurrentTrain: epoch  6, batch     2 | loss: 4.2258554Losses:  2.9870071411132812 0.6382397413253784
CurrentTrain: epoch  6, batch     3 | loss: 2.9870071Losses:  4.210323333740234 2.008192539215088
CurrentTrain: epoch  7, batch     0 | loss: 4.2103233Losses:  4.577839374542236 1.9973725080490112
CurrentTrain: epoch  7, batch     1 | loss: 4.5778394Losses:  4.540804862976074 1.9868359565734863
CurrentTrain: epoch  7, batch     2 | loss: 4.5408049Losses:  4.98920202255249 0.6675513386726379
CurrentTrain: epoch  7, batch     3 | loss: 4.9892020Losses:  4.516106605529785 2.157975435256958
CurrentTrain: epoch  8, batch     0 | loss: 4.5161066Losses:  4.585793495178223 2.1082186698913574
CurrentTrain: epoch  8, batch     1 | loss: 4.5857935Losses:  4.135127067565918 1.9788670539855957
CurrentTrain: epoch  8, batch     2 | loss: 4.1351271Losses:  2.8321032524108887 0.6700894832611084
CurrentTrain: epoch  8, batch     3 | loss: 2.8321033Losses:  3.9563467502593994 1.9014712572097778
CurrentTrain: epoch  9, batch     0 | loss: 3.9563468Losses:  3.4719061851501465 1.8276498317718506
CurrentTrain: epoch  9, batch     1 | loss: 3.4719062Losses:  4.615365028381348 1.9239394664764404
CurrentTrain: epoch  9, batch     2 | loss: 4.6153650Losses:  5.250555038452148 0.6796517372131348
CurrentTrain: epoch  9, batch     3 | loss: 5.2505550
Losses:  2.4508161544799805 2.6858949661254883
MemoryTrain:  epoch  0, batch     0 | loss: 2.4508162Losses:  1.7651124000549316 2.537482976913452
MemoryTrain:  epoch  0, batch     1 | loss: 1.7651124Losses:  2.378993511199951 2.689180374145508
MemoryTrain:  epoch  1, batch     0 | loss: 2.3789935Losses:  2.0545971393585205 2.5313198566436768
MemoryTrain:  epoch  1, batch     1 | loss: 2.0545971Losses:  1.6979389190673828 2.66359543800354
MemoryTrain:  epoch  2, batch     0 | loss: 1.6979389Losses:  1.8644471168518066 2.558302402496338
MemoryTrain:  epoch  2, batch     1 | loss: 1.8644471Losses:  1.6635438203811646 2.6784262657165527
MemoryTrain:  epoch  3, batch     0 | loss: 1.6635438Losses:  1.4344195127487183 2.5320522785186768
MemoryTrain:  epoch  3, batch     1 | loss: 1.4344195Losses:  1.560179591178894 2.6706128120422363
MemoryTrain:  epoch  4, batch     0 | loss: 1.5601796Losses:  1.4861656427383423 2.547586441040039
MemoryTrain:  epoch  4, batch     1 | loss: 1.4861656Losses:  1.5003888607025146 2.6849234104156494
MemoryTrain:  epoch  5, batch     0 | loss: 1.5003889Losses:  1.369360327720642 2.5251946449279785
MemoryTrain:  epoch  5, batch     1 | loss: 1.3693603Losses:  1.4369810819625854 2.6666927337646484
MemoryTrain:  epoch  6, batch     0 | loss: 1.4369811Losses:  1.345012903213501 2.5428950786590576
MemoryTrain:  epoch  6, batch     1 | loss: 1.3450129Losses:  1.4269042015075684 2.663100242614746
MemoryTrain:  epoch  7, batch     0 | loss: 1.4269042Losses:  1.3270403146743774 2.5449864864349365
MemoryTrain:  epoch  7, batch     1 | loss: 1.3270403Losses:  1.400820016860962 2.682781457901001
MemoryTrain:  epoch  8, batch     0 | loss: 1.4008200Losses:  1.3008160591125488 2.525336742401123
MemoryTrain:  epoch  8, batch     1 | loss: 1.3008161Losses:  1.4261338710784912 2.690737724304199
MemoryTrain:  epoch  9, batch     0 | loss: 1.4261339Losses:  1.288763165473938 2.517197370529175
MemoryTrain:  epoch  9, batch     1 | loss: 1.2887632
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.72%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 73.28%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 73.41%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 73.29%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.72%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 92.16%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 90.82%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.14%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.43%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 91.18%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 90.96%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.87%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.71%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.51%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 90.08%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 89.86%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 89.53%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 89.21%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 89.12%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 88.95%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 88.22%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.07%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 87.77%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.37%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 86.78%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 86.13%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 85.70%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 85.20%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 84.91%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 84.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.03%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.82%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 83.50%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.29%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.10%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.90%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.48%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 81.83%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.25%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.74%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 79.80%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.59%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 80.58%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.69%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 80.70%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.75%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 80.71%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.20%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 80.67%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.18%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 79.65%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 79.09%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 78.58%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.26%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.36%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 78.85%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 78.62%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 78.35%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 78.08%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 77.82%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 77.52%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.67%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.76%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 78.74%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 78.60%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 78.55%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.42%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 78.38%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 78.40%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 78.31%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.33%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 78.28%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.06%   
cur_acc:  ['0.9425', '0.7510', '0.7272']
his_acc:  ['0.9425', '0.8420', '0.7806']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  6.390504360198975 1.9604734182357788
CurrentTrain: epoch  0, batch     0 | loss: 6.3905044Losses:  5.159316539764404 1.9420595169067383
CurrentTrain: epoch  0, batch     1 | loss: 5.1593165Losses:  6.219059944152832 1.7998297214508057
CurrentTrain: epoch  0, batch     2 | loss: 6.2190599Losses:  3.845532178878784 0.6389113664627075
CurrentTrain: epoch  0, batch     3 | loss: 3.8455322Losses:  5.39113187789917 2.0835227966308594
CurrentTrain: epoch  1, batch     0 | loss: 5.3911319Losses:  4.690423488616943 2.0226681232452393
CurrentTrain: epoch  1, batch     1 | loss: 4.6904235Losses:  4.835367679595947 2.0922694206237793
CurrentTrain: epoch  1, batch     2 | loss: 4.8353677Losses:  2.597381114959717 0.624972403049469
CurrentTrain: epoch  1, batch     3 | loss: 2.5973811Losses:  5.437568664550781 1.9658567905426025
CurrentTrain: epoch  2, batch     0 | loss: 5.4375687Losses:  4.125575065612793 1.9940330982208252
CurrentTrain: epoch  2, batch     1 | loss: 4.1255751Losses:  3.5135014057159424 1.9126591682434082
CurrentTrain: epoch  2, batch     2 | loss: 3.5135014Losses:  2.284066915512085 0.6310569047927856
CurrentTrain: epoch  2, batch     3 | loss: 2.2840669Losses:  4.112152576446533 1.988723635673523
CurrentTrain: epoch  3, batch     0 | loss: 4.1121526Losses:  3.9643020629882812 2.0993244647979736
CurrentTrain: epoch  3, batch     1 | loss: 3.9643021Losses:  4.085832595825195 2.0153565406799316
CurrentTrain: epoch  3, batch     2 | loss: 4.0858326Losses:  2.3820741176605225 0.6300128698348999
CurrentTrain: epoch  3, batch     3 | loss: 2.3820741Losses:  3.722115993499756 1.841670274734497
CurrentTrain: epoch  4, batch     0 | loss: 3.7221160Losses:  3.5378289222717285 1.8060493469238281
CurrentTrain: epoch  4, batch     1 | loss: 3.5378289Losses:  4.076600551605225 1.625754714012146
CurrentTrain: epoch  4, batch     2 | loss: 4.0766006Losses:  2.2809178829193115 0.625778079032898
CurrentTrain: epoch  4, batch     3 | loss: 2.2809179Losses:  4.061561107635498 1.9932830333709717
CurrentTrain: epoch  5, batch     0 | loss: 4.0615611Losses:  3.705495595932007 1.877119541168213
CurrentTrain: epoch  5, batch     1 | loss: 3.7054956Losses:  3.347989320755005 2.111077308654785
CurrentTrain: epoch  5, batch     2 | loss: 3.3479893Losses:  1.8097925186157227 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8097925Losses:  2.9995083808898926 1.8486931324005127
CurrentTrain: epoch  6, batch     0 | loss: 2.9995084Losses:  3.639458417892456 1.8614457845687866
CurrentTrain: epoch  6, batch     1 | loss: 3.6394584Losses:  3.612421989440918 1.9812672138214111
CurrentTrain: epoch  6, batch     2 | loss: 3.6124220Losses:  2.5123164653778076 0.6752099990844727
CurrentTrain: epoch  6, batch     3 | loss: 2.5123165Losses:  3.23811674118042 1.9831774234771729
CurrentTrain: epoch  7, batch     0 | loss: 3.2381167Losses:  3.0847206115722656 1.9510554075241089
CurrentTrain: epoch  7, batch     1 | loss: 3.0847206Losses:  3.3554418087005615 1.9968619346618652
CurrentTrain: epoch  7, batch     2 | loss: 3.3554418Losses:  3.509103775024414 0.6738569736480713
CurrentTrain: epoch  7, batch     3 | loss: 3.5091038Losses:  3.4232561588287354 1.7754807472229004
CurrentTrain: epoch  8, batch     0 | loss: 3.4232562Losses:  3.1549129486083984 2.0136311054229736
CurrentTrain: epoch  8, batch     1 | loss: 3.1549129Losses:  2.8775784969329834 1.981848120689392
CurrentTrain: epoch  8, batch     2 | loss: 2.8775785Losses:  2.413207769393921 0.6349261999130249
CurrentTrain: epoch  8, batch     3 | loss: 2.4132078Losses:  3.106274366378784 1.9976611137390137
CurrentTrain: epoch  9, batch     0 | loss: 3.1062744Losses:  3.3279683589935303 1.8897396326065063
CurrentTrain: epoch  9, batch     1 | loss: 3.3279684Losses:  2.8006153106689453 1.936987042427063
CurrentTrain: epoch  9, batch     2 | loss: 2.8006153Losses:  2.159879207611084 0.6050736904144287
CurrentTrain: epoch  9, batch     3 | loss: 2.1598792
Losses:  1.6965007781982422 2.6634087562561035
MemoryTrain:  epoch  0, batch     0 | loss: 1.6965008Losses:  2.8949391841888428 2.682957649230957
MemoryTrain:  epoch  0, batch     1 | loss: 2.8949392Losses:  2.1201443672180176 1.9818041324615479
MemoryTrain:  epoch  0, batch     2 | loss: 2.1201444Losses:  2.2685885429382324 2.6786746978759766
MemoryTrain:  epoch  1, batch     0 | loss: 2.2685885Losses:  2.7141454219818115 2.660806179046631
MemoryTrain:  epoch  1, batch     1 | loss: 2.7141454Losses:  2.567537307739258 1.9980576038360596
MemoryTrain:  epoch  1, batch     2 | loss: 2.5675373Losses:  2.1995370388031006 2.6633214950561523
MemoryTrain:  epoch  2, batch     0 | loss: 2.1995370Losses:  2.1739590167999268 2.6896190643310547
MemoryTrain:  epoch  2, batch     1 | loss: 2.1739590Losses:  1.6820886135101318 1.963923692703247
MemoryTrain:  epoch  2, batch     2 | loss: 1.6820886Losses:  2.203298568725586 2.6719608306884766
MemoryTrain:  epoch  3, batch     0 | loss: 2.2032986Losses:  1.8820281028747559 2.675969123840332
MemoryTrain:  epoch  3, batch     1 | loss: 1.8820281Losses:  1.1237465143203735 1.9647023677825928
MemoryTrain:  epoch  3, batch     2 | loss: 1.1237465Losses:  1.570608139038086 2.6568799018859863
MemoryTrain:  epoch  4, batch     0 | loss: 1.5706081Losses:  1.8243086338043213 2.6682941913604736
MemoryTrain:  epoch  4, batch     1 | loss: 1.8243086Losses:  2.335064172744751 2.003894805908203
MemoryTrain:  epoch  4, batch     2 | loss: 2.3350642Losses:  2.0310516357421875 2.6535463333129883
MemoryTrain:  epoch  5, batch     0 | loss: 2.0310516Losses:  1.507859468460083 2.6703298091888428
MemoryTrain:  epoch  5, batch     1 | loss: 1.5078595Losses:  1.0887833833694458 1.985945701599121
MemoryTrain:  epoch  5, batch     2 | loss: 1.0887834Losses:  1.3730013370513916 2.650249481201172
MemoryTrain:  epoch  6, batch     0 | loss: 1.3730013Losses:  1.609551191329956 2.6686699390411377
MemoryTrain:  epoch  6, batch     1 | loss: 1.6095512Losses:  1.2748923301696777 2.0026772022247314
MemoryTrain:  epoch  6, batch     2 | loss: 1.2748923Losses:  1.3804900646209717 2.651020050048828
MemoryTrain:  epoch  7, batch     0 | loss: 1.3804901Losses:  1.4848108291625977 2.679077386856079
MemoryTrain:  epoch  7, batch     1 | loss: 1.4848108Losses:  1.0440279245376587 1.972350835800171
MemoryTrain:  epoch  7, batch     2 | loss: 1.0440279Losses:  1.396866798400879 2.6512980461120605
MemoryTrain:  epoch  8, batch     0 | loss: 1.3968668Losses:  1.5111140012741089 2.673123836517334
MemoryTrain:  epoch  8, batch     1 | loss: 1.5111140Losses:  1.0417542457580566 1.966591238975525
MemoryTrain:  epoch  8, batch     2 | loss: 1.0417542Losses:  1.4103410243988037 2.6803362369537354
MemoryTrain:  epoch  9, batch     0 | loss: 1.4103410Losses:  1.3845535516738892 2.6584866046905518
MemoryTrain:  epoch  9, batch     1 | loss: 1.3845536Losses:  1.0881426334381104 1.9511828422546387
MemoryTrain:  epoch  9, batch     2 | loss: 1.0881426
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 90.35%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 89.87%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 89.51%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 89.17%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.83%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.71%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.57%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 88.83%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 88.90%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.13%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.23%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.12%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 88.77%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 88.44%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 88.43%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.11%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.69%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 86.41%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.56%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 85.44%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 85.39%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 85.35%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 85.44%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 85.15%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 84.77%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 84.28%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 83.72%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 83.31%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 82.83%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.31%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.80%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.55%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.13%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.72%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.09%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.53%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.98%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 78.38%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.01%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.82%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.35%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 79.37%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 79.39%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.64%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 80.03%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 79.50%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 79.02%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 78.50%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 77.95%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 77.40%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 77.77%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 77.43%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 77.12%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 76.83%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 76.53%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 76.24%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 77.01%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.11%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 77.17%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 77.30%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 77.25%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 77.20%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 77.15%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 77.07%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 77.09%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 77.05%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 77.02%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 77.06%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 77.20%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 76.98%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 76.81%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 76.58%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 76.29%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 76.12%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 75.92%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 75.66%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 75.51%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 75.36%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 75.21%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.97%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 74.76%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 75.37%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 75.34%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.33%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 75.25%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.83%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.90%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.11%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.40%   
cur_acc:  ['0.9425', '0.7510', '0.7272', '0.7788']
his_acc:  ['0.9425', '0.8420', '0.7806', '0.7740']
Clustering into  24  clusters
Clusters:  [ 2  8  1  0 14 23  1 22  3  3  1  3 21 18 11  4 10 18 12  4  2  4  0  7
  0  8  9  0  6 15  1  9  2  0  5  2 19 16 22  6 20 17  7  1 22  0  2  1
 13 10]
Losses:  6.667246341705322 1.9313888549804688
CurrentTrain: epoch  0, batch     0 | loss: 6.6672463Losses:  6.73438024520874 1.9636545181274414
CurrentTrain: epoch  0, batch     1 | loss: 6.7343802Losses:  6.6104207038879395 1.919356107711792
CurrentTrain: epoch  0, batch     2 | loss: 6.6104207Losses:  3.530552387237549 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.5305524Losses:  6.117247581481934 1.9605193138122559
CurrentTrain: epoch  1, batch     0 | loss: 6.1172476Losses:  5.599539756774902 2.069746494293213
CurrentTrain: epoch  1, batch     1 | loss: 5.5995398Losses:  5.328483581542969 1.9773672819137573
CurrentTrain: epoch  1, batch     2 | loss: 5.3284836Losses:  4.562590599060059 0.6782097816467285
CurrentTrain: epoch  1, batch     3 | loss: 4.5625906Losses:  5.184114456176758 2.0707311630249023
CurrentTrain: epoch  2, batch     0 | loss: 5.1841145Losses:  5.953912734985352 2.0761330127716064
CurrentTrain: epoch  2, batch     1 | loss: 5.9539127Losses:  5.119003772735596 2.063021659851074
CurrentTrain: epoch  2, batch     2 | loss: 5.1190038Losses:  3.0549814701080322 0.6732982397079468
CurrentTrain: epoch  2, batch     3 | loss: 3.0549815Losses:  5.465921401977539 2.0753538608551025
CurrentTrain: epoch  3, batch     0 | loss: 5.4659214Losses:  4.8373332023620605 1.961050271987915
CurrentTrain: epoch  3, batch     1 | loss: 4.8373332Losses:  4.521045207977295 1.9744549989700317
CurrentTrain: epoch  3, batch     2 | loss: 4.5210452Losses:  4.701047897338867 0.6699607968330383
CurrentTrain: epoch  3, batch     3 | loss: 4.7010479Losses:  4.319616317749023 1.912104845046997
CurrentTrain: epoch  4, batch     0 | loss: 4.3196163Losses:  4.965114116668701 1.7895594835281372
CurrentTrain: epoch  4, batch     1 | loss: 4.9651141Losses:  4.676634788513184 2.1596922874450684
CurrentTrain: epoch  4, batch     2 | loss: 4.6766348Losses:  2.108027219772339 0.6659103631973267
CurrentTrain: epoch  4, batch     3 | loss: 2.1080272Losses:  4.281846523284912 2.0691545009613037
CurrentTrain: epoch  5, batch     0 | loss: 4.2818465Losses:  4.387472629547119 1.9872208833694458
CurrentTrain: epoch  5, batch     1 | loss: 4.3874726Losses:  4.512509346008301 2.0370240211486816
CurrentTrain: epoch  5, batch     2 | loss: 4.5125093Losses:  5.114104747772217 0.6694508790969849
CurrentTrain: epoch  5, batch     3 | loss: 5.1141047Losses:  4.126221656799316 1.921048641204834
CurrentTrain: epoch  6, batch     0 | loss: 4.1262217Losses:  4.107424259185791 1.967336893081665
CurrentTrain: epoch  6, batch     1 | loss: 4.1074243Losses:  4.202211380004883 2.0378174781799316
CurrentTrain: epoch  6, batch     2 | loss: 4.2022114Losses:  3.0041685104370117 0.6680229306221008
CurrentTrain: epoch  6, batch     3 | loss: 3.0041685Losses:  3.929750919342041 2.0719375610351562
CurrentTrain: epoch  7, batch     0 | loss: 3.9297509Losses:  3.9832825660705566 2.041356325149536
CurrentTrain: epoch  7, batch     1 | loss: 3.9832826Losses:  4.076985836029053 1.9932012557983398
CurrentTrain: epoch  7, batch     2 | loss: 4.0769858Losses:  3.228342294692993 0.6734929084777832
CurrentTrain: epoch  7, batch     3 | loss: 3.2283423Losses:  3.6692943572998047 2.161372184753418
CurrentTrain: epoch  8, batch     0 | loss: 3.6692944Losses:  4.26277494430542 1.92164945602417
CurrentTrain: epoch  8, batch     1 | loss: 4.2627749Losses:  3.9232048988342285 2.0377984046936035
CurrentTrain: epoch  8, batch     2 | loss: 3.9232049Losses:  2.1273415088653564 0.6193464994430542
CurrentTrain: epoch  8, batch     3 | loss: 2.1273415Losses:  3.951136827468872 2.0812320709228516
CurrentTrain: epoch  9, batch     0 | loss: 3.9511368Losses:  3.115082263946533 1.8941471576690674
CurrentTrain: epoch  9, batch     1 | loss: 3.1150823Losses:  4.070663928985596 1.9031567573547363
CurrentTrain: epoch  9, batch     2 | loss: 4.0706639Losses:  2.384571075439453 0.6670427322387695
CurrentTrain: epoch  9, batch     3 | loss: 2.3845711
Losses:  2.02009916305542 2.663919448852539
MemoryTrain:  epoch  0, batch     0 | loss: 2.0200992Losses:  1.9082515239715576 2.6924374103546143
MemoryTrain:  epoch  0, batch     1 | loss: 1.9082515Losses:  1.716783046722412 2.692561149597168
MemoryTrain:  epoch  0, batch     2 | loss: 1.7167830Losses:  1.4828829765319824 0.6691613793373108
MemoryTrain:  epoch  0, batch     3 | loss: 1.4828830Losses:  2.5326766967773438 2.6781857013702393
MemoryTrain:  epoch  1, batch     0 | loss: 2.5326767Losses:  2.201632022857666 2.6937201023101807
MemoryTrain:  epoch  1, batch     1 | loss: 2.2016320Losses:  1.6359351873397827 2.6720428466796875
MemoryTrain:  epoch  1, batch     2 | loss: 1.6359352Losses:  0.3560367226600647 0.5957617163658142
MemoryTrain:  epoch  1, batch     3 | loss: 0.3560367Losses:  1.9402406215667725 2.6784818172454834
MemoryTrain:  epoch  2, batch     0 | loss: 1.9402406Losses:  1.697746753692627 2.676919460296631
MemoryTrain:  epoch  2, batch     1 | loss: 1.6977468Losses:  1.7268346548080444 2.6730923652648926
MemoryTrain:  epoch  2, batch     2 | loss: 1.7268347Losses:  0.38146060705184937 0.6687613725662231
MemoryTrain:  epoch  2, batch     3 | loss: 0.3814606Losses:  1.6185393333435059 2.6838793754577637
MemoryTrain:  epoch  3, batch     0 | loss: 1.6185393Losses:  1.458492636680603 2.66760516166687
MemoryTrain:  epoch  3, batch     1 | loss: 1.4584926Losses:  1.578476905822754 2.670532703399658
MemoryTrain:  epoch  3, batch     2 | loss: 1.5784769Losses:  0.8367682099342346 0.6951967477798462
MemoryTrain:  epoch  3, batch     3 | loss: 0.8367682Losses:  1.586930513381958 2.682417392730713
MemoryTrain:  epoch  4, batch     0 | loss: 1.5869305Losses:  1.48093581199646 2.693556308746338
MemoryTrain:  epoch  4, batch     1 | loss: 1.4809358Losses:  1.408260464668274 2.650635004043579
MemoryTrain:  epoch  4, batch     2 | loss: 1.4082605Losses:  0.41240260004997253 0.6101868152618408
MemoryTrain:  epoch  4, batch     3 | loss: 0.4124026Losses:  1.4410094022750854 2.6795668601989746
MemoryTrain:  epoch  5, batch     0 | loss: 1.4410094Losses:  1.470005989074707 2.6625566482543945
MemoryTrain:  epoch  5, batch     1 | loss: 1.4700060Losses:  1.4155256748199463 2.6854822635650635
MemoryTrain:  epoch  5, batch     2 | loss: 1.4155257Losses:  0.31618380546569824 0.60792076587677
MemoryTrain:  epoch  5, batch     3 | loss: 0.3161838Losses:  1.4270285367965698 2.65397047996521
MemoryTrain:  epoch  6, batch     0 | loss: 1.4270285Losses:  1.4380451440811157 2.688538074493408
MemoryTrain:  epoch  6, batch     1 | loss: 1.4380451Losses:  1.4031726121902466 2.671544075012207
MemoryTrain:  epoch  6, batch     2 | loss: 1.4031726Losses:  0.33922386169433594 0.6405453681945801
MemoryTrain:  epoch  6, batch     3 | loss: 0.3392239Losses:  1.4198496341705322 2.668797492980957
MemoryTrain:  epoch  7, batch     0 | loss: 1.4198496Losses:  1.3891706466674805 2.673952102661133
MemoryTrain:  epoch  7, batch     1 | loss: 1.3891706Losses:  1.4008978605270386 2.6744096279144287
MemoryTrain:  epoch  7, batch     2 | loss: 1.4008979Losses:  0.36193162202835083 0.6392261981964111
MemoryTrain:  epoch  7, batch     3 | loss: 0.3619316Losses:  1.3731586933135986 2.6669211387634277
MemoryTrain:  epoch  8, batch     0 | loss: 1.3731587Losses:  1.3880267143249512 2.68060040473938
MemoryTrain:  epoch  8, batch     1 | loss: 1.3880267Losses:  1.374663233757019 2.6619210243225098
MemoryTrain:  epoch  8, batch     2 | loss: 1.3746632Losses:  0.3735661208629608 0.6521328687667847
MemoryTrain:  epoch  8, batch     3 | loss: 0.3735661Losses:  1.375819444656372 2.6699039936065674
MemoryTrain:  epoch  9, batch     0 | loss: 1.3758194Losses:  1.3910226821899414 2.6706080436706543
MemoryTrain:  epoch  9, batch     1 | loss: 1.3910227Losses:  1.3883768320083618 2.6757664680480957
MemoryTrain:  epoch  9, batch     2 | loss: 1.3883768Losses:  0.3225288987159729 0.6100355386734009
MemoryTrain:  epoch  9, batch     3 | loss: 0.3225289
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 65.26%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 61.99%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 61.84%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.81%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.62%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.79%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.11%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.99%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 88.06%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.30%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.18%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.09%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 87.83%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 87.34%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 86.94%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 86.71%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 86.25%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 86.03%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 85.17%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 84.67%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 84.19%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 83.79%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 83.05%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 82.79%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 82.78%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.90%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 82.80%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.31%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 81.78%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 80.93%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 79.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.70%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.53%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 79.25%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.99%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.45%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.84%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.29%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.70%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 76.13%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.73%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.22%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 77.33%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 77.33%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.41%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 77.76%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 77.20%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 76.70%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 76.20%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 75.66%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 75.13%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 74.78%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.04%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 74.80%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 74.47%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 74.15%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 73.80%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 74.96%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 75.15%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 75.11%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 75.14%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.07%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 75.07%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 75.10%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 75.14%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.10%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.59%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 75.51%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 75.29%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 75.10%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 74.87%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 74.62%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 74.47%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 74.32%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 74.17%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 74.05%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 73.85%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.65%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.50%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 73.27%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 73.05%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 72.82%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.35%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 73.22%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.10%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 75.60%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 75.51%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 75.49%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 75.24%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 75.09%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 74.91%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 74.86%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 74.77%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 74.93%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 74.98%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 75.09%   [EVAL] batch:  274 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 75.05%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 74.91%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 74.87%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 74.87%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 74.78%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 74.71%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 74.43%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 74.32%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 74.15%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 73.91%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 73.90%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.08%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.06%   
cur_acc:  ['0.9425', '0.7510', '0.7272', '0.7788', '0.7262']
his_acc:  ['0.9425', '0.8420', '0.7806', '0.7740', '0.7506']
Clustering into  29  clusters
Clusters:  [11  4  0  5 14 28  0  5 22 22  0 22 24  9 15  8 10  9  1  8 21  8 25 16
  5 18  3  5 26 27  0  3  3  4 23 11 19 17  5 26  4  7  6  0  5 12 21 10
 13 10 12  8  2 20  6  1  2 10  4  3]
Losses:  7.261175155639648 1.9880492687225342
CurrentTrain: epoch  0, batch     0 | loss: 7.2611752Losses:  6.950503349304199 1.733453392982483
CurrentTrain: epoch  0, batch     1 | loss: 6.9505033Losses:  7.407003402709961 1.9858399629592896
CurrentTrain: epoch  0, batch     2 | loss: 7.4070034Losses:  4.957629203796387 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.9576292Losses:  6.815361976623535 2.164440155029297
CurrentTrain: epoch  1, batch     0 | loss: 6.8153620Losses:  6.090157508850098 2.045565128326416
CurrentTrain: epoch  1, batch     1 | loss: 6.0901575Losses:  5.428409099578857 2.04550838470459
CurrentTrain: epoch  1, batch     2 | loss: 5.4284091Losses:  5.331608772277832 0.6558352708816528
CurrentTrain: epoch  1, batch     3 | loss: 5.3316088Losses:  5.429087162017822 1.9017927646636963
CurrentTrain: epoch  2, batch     0 | loss: 5.4290872Losses:  5.747715473175049 2.02607798576355
CurrentTrain: epoch  2, batch     1 | loss: 5.7477155Losses:  5.120760440826416 2.043914556503296
CurrentTrain: epoch  2, batch     2 | loss: 5.1207604Losses:  5.370513439178467 0.6639129519462585
CurrentTrain: epoch  2, batch     3 | loss: 5.3705134Losses:  4.344252586364746 1.927535057067871
CurrentTrain: epoch  3, batch     0 | loss: 4.3442526Losses:  5.536627769470215 2.1158504486083984
CurrentTrain: epoch  3, batch     1 | loss: 5.5366278Losses:  5.427145004272461 2.20060658454895
CurrentTrain: epoch  3, batch     2 | loss: 5.4271450Losses:  4.555287837982178 0.6455252170562744
CurrentTrain: epoch  3, batch     3 | loss: 4.5552878Losses:  4.278436660766602 1.8358861207962036
CurrentTrain: epoch  4, batch     0 | loss: 4.2784367Losses:  5.555474281311035 1.928213357925415
CurrentTrain: epoch  4, batch     1 | loss: 5.5554743Losses:  4.936528205871582 2.117217540740967
CurrentTrain: epoch  4, batch     2 | loss: 4.9365282Losses:  3.474656343460083 0.6700595021247864
CurrentTrain: epoch  4, batch     3 | loss: 3.4746563Losses:  4.226446151733398 2.1056318283081055
CurrentTrain: epoch  5, batch     0 | loss: 4.2264462Losses:  5.2437825202941895 1.850716471672058
CurrentTrain: epoch  5, batch     1 | loss: 5.2437825Losses:  4.126382827758789 2.03475022315979
CurrentTrain: epoch  5, batch     2 | loss: 4.1263828Losses:  3.277571201324463 0.6667215824127197
CurrentTrain: epoch  5, batch     3 | loss: 3.2775712Losses:  3.749068021774292 2.042224407196045
CurrentTrain: epoch  6, batch     0 | loss: 3.7490680Losses:  4.688893795013428 2.111774444580078
CurrentTrain: epoch  6, batch     1 | loss: 4.6888938Losses:  4.56757116317749 2.0783843994140625
CurrentTrain: epoch  6, batch     2 | loss: 4.5675712Losses:  3.386146306991577 0.6616491079330444
CurrentTrain: epoch  6, batch     3 | loss: 3.3861463Losses:  4.432033538818359 2.0455222129821777
CurrentTrain: epoch  7, batch     0 | loss: 4.4320335Losses:  4.126762390136719 2.0816664695739746
CurrentTrain: epoch  7, batch     1 | loss: 4.1267624Losses:  3.8595824241638184 1.897934079170227
CurrentTrain: epoch  7, batch     2 | loss: 3.8595824Losses:  4.280102252960205 0.6659368276596069
CurrentTrain: epoch  7, batch     3 | loss: 4.2801023Losses:  3.7393250465393066 1.8725736141204834
CurrentTrain: epoch  8, batch     0 | loss: 3.7393250Losses:  3.4506471157073975 1.852868676185608
CurrentTrain: epoch  8, batch     1 | loss: 3.4506471Losses:  4.78860330581665 1.959455966949463
CurrentTrain: epoch  8, batch     2 | loss: 4.7886033Losses:  2.1357979774475098 0.6673214435577393
CurrentTrain: epoch  8, batch     3 | loss: 2.1357980Losses:  3.4633047580718994 1.9951413869857788
CurrentTrain: epoch  9, batch     0 | loss: 3.4633048Losses:  3.694061040878296 1.849238395690918
CurrentTrain: epoch  9, batch     1 | loss: 3.6940610Losses:  3.879244804382324 1.8372716903686523
CurrentTrain: epoch  9, batch     2 | loss: 3.8792448Losses:  5.464142799377441 0.675104022026062
CurrentTrain: epoch  9, batch     3 | loss: 5.4641428
Losses:  2.005354404449463 2.69594669342041
MemoryTrain:  epoch  0, batch     0 | loss: 2.0053544Losses:  1.9345088005065918 2.6938509941101074
MemoryTrain:  epoch  0, batch     1 | loss: 1.9345088Losses:  1.9854493141174316 2.6809897422790527
MemoryTrain:  epoch  0, batch     2 | loss: 1.9854493Losses:  1.6536896228790283 2.3876161575317383
MemoryTrain:  epoch  0, batch     3 | loss: 1.6536896Losses:  2.4965133666992188 2.6746363639831543
MemoryTrain:  epoch  1, batch     0 | loss: 2.4965134Losses:  1.8435540199279785 2.674588680267334
MemoryTrain:  epoch  1, batch     1 | loss: 1.8435540Losses:  1.9581279754638672 2.6897616386413574
MemoryTrain:  epoch  1, batch     2 | loss: 1.9581280Losses:  1.5564147233963013 2.4105873107910156
MemoryTrain:  epoch  1, batch     3 | loss: 1.5564147Losses:  1.7368674278259277 2.6818623542785645
MemoryTrain:  epoch  2, batch     0 | loss: 1.7368674Losses:  1.7316495180130005 2.6866745948791504
MemoryTrain:  epoch  2, batch     1 | loss: 1.7316495Losses:  2.004390001296997 2.687274217605591
MemoryTrain:  epoch  2, batch     2 | loss: 2.0043900Losses:  1.3865346908569336 2.367919921875
MemoryTrain:  epoch  2, batch     3 | loss: 1.3865347Losses:  1.4782326221466064 2.668059825897217
MemoryTrain:  epoch  3, batch     0 | loss: 1.4782326Losses:  1.5186494588851929 2.662546396255493
MemoryTrain:  epoch  3, batch     1 | loss: 1.5186495Losses:  1.637494683265686 2.681845188140869
MemoryTrain:  epoch  3, batch     2 | loss: 1.6374947Losses:  1.468315601348877 2.41347599029541
MemoryTrain:  epoch  3, batch     3 | loss: 1.4683156Losses:  1.6384083032608032 2.6753363609313965
MemoryTrain:  epoch  4, batch     0 | loss: 1.6384083Losses:  1.5335984230041504 2.6817057132720947
MemoryTrain:  epoch  4, batch     1 | loss: 1.5335984Losses:  1.3951839208602905 2.6617350578308105
MemoryTrain:  epoch  4, batch     2 | loss: 1.3951839Losses:  1.2931355237960815 2.392620325088501
MemoryTrain:  epoch  4, batch     3 | loss: 1.2931355Losses:  1.3863320350646973 2.662221908569336
MemoryTrain:  epoch  5, batch     0 | loss: 1.3863320Losses:  1.4138076305389404 2.6871824264526367
MemoryTrain:  epoch  5, batch     1 | loss: 1.4138076Losses:  1.466522455215454 2.679385185241699
MemoryTrain:  epoch  5, batch     2 | loss: 1.4665225Losses:  1.8487075567245483 2.373145580291748
MemoryTrain:  epoch  5, batch     3 | loss: 1.8487076Losses:  1.5261919498443604 2.665865421295166
MemoryTrain:  epoch  6, batch     0 | loss: 1.5261919Losses:  1.400665044784546 2.664438247680664
MemoryTrain:  epoch  6, batch     1 | loss: 1.4006650Losses:  1.3887414932250977 2.6769134998321533
MemoryTrain:  epoch  6, batch     2 | loss: 1.3887415Losses:  1.2592071294784546 2.3951964378356934
MemoryTrain:  epoch  6, batch     3 | loss: 1.2592071Losses:  1.4163748025894165 2.676132917404175
MemoryTrain:  epoch  7, batch     0 | loss: 1.4163748Losses:  1.4010175466537476 2.6817569732666016
MemoryTrain:  epoch  7, batch     1 | loss: 1.4010175Losses:  1.3827837705612183 2.6634469032287598
MemoryTrain:  epoch  7, batch     2 | loss: 1.3827838Losses:  1.2346758842468262 2.373872995376587
MemoryTrain:  epoch  7, batch     3 | loss: 1.2346759Losses:  1.3763290643692017 2.663423538208008
MemoryTrain:  epoch  8, batch     0 | loss: 1.3763291Losses:  1.3874999284744263 2.666694402694702
MemoryTrain:  epoch  8, batch     1 | loss: 1.3874999Losses:  1.3716036081314087 2.6677498817443848
MemoryTrain:  epoch  8, batch     2 | loss: 1.3716036Losses:  1.2879931926727295 2.391950845718384
MemoryTrain:  epoch  8, batch     3 | loss: 1.2879932Losses:  1.3742010593414307 2.6682190895080566
MemoryTrain:  epoch  9, batch     0 | loss: 1.3742011Losses:  1.3731567859649658 2.6614794731140137
MemoryTrain:  epoch  9, batch     1 | loss: 1.3731568Losses:  1.3758270740509033 2.6828083992004395
MemoryTrain:  epoch  9, batch     2 | loss: 1.3758271Losses:  1.2241512537002563 2.3697354793548584
MemoryTrain:  epoch  9, batch     3 | loss: 1.2241513
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 72.19%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.73%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 68.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.26%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 88.38%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 86.65%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 86.04%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 85.35%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 84.98%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 84.62%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 83.43%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 83.02%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.94%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 81.57%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 81.17%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 80.72%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 80.20%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 79.69%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 79.19%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 78.78%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 78.09%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 77.98%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 78.02%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 78.40%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 77.50%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 76.47%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 76.33%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 75.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.74%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 75.61%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.24%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.06%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 74.65%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.07%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 72.90%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 72.35%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 71.99%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.79%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 73.14%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 73.10%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 73.12%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 73.00%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.88%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 72.90%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 73.01%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 73.14%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.87%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 73.34%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.86%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 72.38%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 71.37%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 71.94%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 71.55%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.28%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 70.73%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.38%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.53%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 70.86%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 70.85%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 70.55%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 70.28%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 69.93%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.91%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 70.32%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.10%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 70.93%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 70.79%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 70.55%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 70.35%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.23%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 70.02%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 69.62%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 69.43%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 69.82%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 69.76%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 69.75%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 69.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 72.41%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 72.37%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 72.24%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 72.14%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 72.00%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 71.80%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 71.64%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 72.08%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 71.93%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 71.90%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 71.83%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 71.75%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 71.72%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 71.46%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 71.13%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 70.91%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 70.81%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 70.84%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.00%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 71.66%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 71.42%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 71.34%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 71.46%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.46%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 71.37%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.30%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.79%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 72.36%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 72.16%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 72.03%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 71.67%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 71.49%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 71.94%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 71.86%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 71.76%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.72%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 71.56%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 71.54%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 71.52%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 71.57%   
cur_acc:  ['0.9425', '0.7510', '0.7272', '0.7788', '0.7262', '0.6835']
his_acc:  ['0.9425', '0.8420', '0.7806', '0.7740', '0.7506', '0.7157']
Clustering into  34  clusters
Clusters:  [ 6 30  0  1 27 25  0 12  2  2  0  2 26 13 33  5 16 13  3  5 14  5 15 19
  1 30 22  1 21 23 28 22 14  1 24  6 20 31 12 21 17 18  8  0 12  7  4 10
 29 16  7  5 32 11  8  3  9 10  1 22 30  5 22 33 15  3  4  1  4  1]
Losses:  6.42586088180542 1.7379180192947388
CurrentTrain: epoch  0, batch     0 | loss: 6.4258609Losses:  8.041805267333984 1.7796255350112915
CurrentTrain: epoch  0, batch     1 | loss: 8.0418053Losses:  7.616250514984131 1.9899013042449951
CurrentTrain: epoch  0, batch     2 | loss: 7.6162505Losses:  8.818901062011719 0.6538679599761963
CurrentTrain: epoch  0, batch     3 | loss: 8.8189011Losses:  6.819282531738281 2.071638584136963
CurrentTrain: epoch  1, batch     0 | loss: 6.8192825Losses:  6.497456073760986 1.9083234071731567
CurrentTrain: epoch  1, batch     1 | loss: 6.4974561Losses:  5.8545026779174805 2.0305798053741455
CurrentTrain: epoch  1, batch     2 | loss: 5.8545027Losses:  6.69448184967041 0.6418280601501465
CurrentTrain: epoch  1, batch     3 | loss: 6.6944818Losses:  5.887119293212891 1.8675284385681152
CurrentTrain: epoch  2, batch     0 | loss: 5.8871193Losses:  5.629951000213623 2.021575927734375
CurrentTrain: epoch  2, batch     1 | loss: 5.6299510Losses:  5.865535736083984 2.187349796295166
CurrentTrain: epoch  2, batch     2 | loss: 5.8655357Losses:  7.5489020347595215 0.6656251549720764
CurrentTrain: epoch  2, batch     3 | loss: 7.5489020Losses:  5.587924480438232 1.8127660751342773
CurrentTrain: epoch  3, batch     0 | loss: 5.5879245Losses:  5.74208927154541 2.0584776401519775
CurrentTrain: epoch  3, batch     1 | loss: 5.7420893Losses:  5.397776126861572 2.0344293117523193
CurrentTrain: epoch  3, batch     2 | loss: 5.3977761Losses:  4.630252361297607 0.6683737635612488
CurrentTrain: epoch  3, batch     3 | loss: 4.6302524Losses:  5.8131608963012695 2.0208919048309326
CurrentTrain: epoch  4, batch     0 | loss: 5.8131609Losses:  4.38797664642334 1.8158278465270996
CurrentTrain: epoch  4, batch     1 | loss: 4.3879766Losses:  5.161635875701904 2.0970726013183594
CurrentTrain: epoch  4, batch     2 | loss: 5.1616359Losses:  6.3026580810546875 0.6828140020370483
CurrentTrain: epoch  4, batch     3 | loss: 6.3026581Losses:  5.166933536529541 1.6275722980499268
CurrentTrain: epoch  5, batch     0 | loss: 5.1669335Losses:  4.592440605163574 2.035689353942871
CurrentTrain: epoch  5, batch     1 | loss: 4.5924406Losses:  4.798144340515137 2.0693893432617188
CurrentTrain: epoch  5, batch     2 | loss: 4.7981443Losses:  3.5771291255950928 0.6460042595863342
CurrentTrain: epoch  5, batch     3 | loss: 3.5771291Losses:  5.043891429901123 1.9751542806625366
CurrentTrain: epoch  6, batch     0 | loss: 5.0438914Losses:  4.664402484893799 1.720776915550232
CurrentTrain: epoch  6, batch     1 | loss: 4.6644025Losses:  3.783229112625122 1.8194364309310913
CurrentTrain: epoch  6, batch     2 | loss: 3.7832291Losses:  5.258682727813721 0.6628367900848389
CurrentTrain: epoch  6, batch     3 | loss: 5.2586827Losses:  4.052087783813477 1.9068753719329834
CurrentTrain: epoch  7, batch     0 | loss: 4.0520878Losses:  4.833497524261475 1.8760976791381836
CurrentTrain: epoch  7, batch     1 | loss: 4.8334975Losses:  4.250814914703369 2.0661916732788086
CurrentTrain: epoch  7, batch     2 | loss: 4.2508149Losses:  4.144384384155273 0.683302640914917
CurrentTrain: epoch  7, batch     3 | loss: 4.1443844Losses:  4.193533897399902 1.922088623046875
CurrentTrain: epoch  8, batch     0 | loss: 4.1935339Losses:  4.112127304077148 1.914207935333252
CurrentTrain: epoch  8, batch     1 | loss: 4.1121273Losses:  4.394476890563965 2.183969497680664
CurrentTrain: epoch  8, batch     2 | loss: 4.3944769Losses:  3.051978588104248 0.660457193851471
CurrentTrain: epoch  8, batch     3 | loss: 3.0519786Losses:  3.931755781173706 2.1143856048583984
CurrentTrain: epoch  9, batch     0 | loss: 3.9317558Losses:  4.527128219604492 2.0469770431518555
CurrentTrain: epoch  9, batch     1 | loss: 4.5271282Losses:  3.8141305446624756 1.979787826538086
CurrentTrain: epoch  9, batch     2 | loss: 3.8141305Losses:  2.5641937255859375 0.6758455634117126
CurrentTrain: epoch  9, batch     3 | loss: 2.5641937
Losses:  1.8020474910736084 2.705246925354004
MemoryTrain:  epoch  0, batch     0 | loss: 1.8020475Losses:  1.7365126609802246 2.6748342514038086
MemoryTrain:  epoch  0, batch     1 | loss: 1.7365127Losses:  2.0815882682800293 2.6831934452056885
MemoryTrain:  epoch  0, batch     2 | loss: 2.0815883Losses:  1.5279130935668945 2.696324110031128
MemoryTrain:  epoch  0, batch     3 | loss: 1.5279131Losses:  0.8710862994194031 1.651212453842163
MemoryTrain:  epoch  0, batch     4 | loss: 0.8710863Losses:  1.669398546218872 2.6746671199798584
MemoryTrain:  epoch  1, batch     0 | loss: 1.6693985Losses:  2.0225670337677 2.683872699737549
MemoryTrain:  epoch  1, batch     1 | loss: 2.0225670Losses:  1.6926946640014648 2.6939587593078613
MemoryTrain:  epoch  1, batch     2 | loss: 1.6926947Losses:  2.124342203140259 2.675096273422241
MemoryTrain:  epoch  1, batch     3 | loss: 2.1243422Losses:  1.0525062084197998 1.6982202529907227
MemoryTrain:  epoch  1, batch     4 | loss: 1.0525062Losses:  1.5128751993179321 2.6805505752563477
MemoryTrain:  epoch  2, batch     0 | loss: 1.5128752Losses:  1.5236961841583252 2.676845073699951
MemoryTrain:  epoch  2, batch     1 | loss: 1.5236962Losses:  1.517553448677063 2.6804091930389404
MemoryTrain:  epoch  2, batch     2 | loss: 1.5175534Losses:  1.9804866313934326 2.681206226348877
MemoryTrain:  epoch  2, batch     3 | loss: 1.9804866Losses:  1.012186884880066 1.741288661956787
MemoryTrain:  epoch  2, batch     4 | loss: 1.0121869Losses:  1.504311203956604 2.6711812019348145
MemoryTrain:  epoch  3, batch     0 | loss: 1.5043112Losses:  1.5283745527267456 2.6621766090393066
MemoryTrain:  epoch  3, batch     1 | loss: 1.5283746Losses:  1.47393000125885 2.691964626312256
MemoryTrain:  epoch  3, batch     2 | loss: 1.4739300Losses:  1.6394226551055908 2.6900877952575684
MemoryTrain:  epoch  3, batch     3 | loss: 1.6394227Losses:  1.1490120887756348 1.713966727256775
MemoryTrain:  epoch  3, batch     4 | loss: 1.1490121Losses:  1.409424066543579 2.673959732055664
MemoryTrain:  epoch  4, batch     0 | loss: 1.4094241Losses:  1.4157649278640747 2.677361011505127
MemoryTrain:  epoch  4, batch     1 | loss: 1.4157649Losses:  1.5655328035354614 2.6833906173706055
MemoryTrain:  epoch  4, batch     2 | loss: 1.5655328Losses:  1.5589025020599365 2.6881299018859863
MemoryTrain:  epoch  4, batch     3 | loss: 1.5589025Losses:  0.9338626265525818 1.712437391281128
MemoryTrain:  epoch  4, batch     4 | loss: 0.9338626Losses:  1.458207607269287 2.6731152534484863
MemoryTrain:  epoch  5, batch     0 | loss: 1.4582076Losses:  1.4259800910949707 2.69284725189209
MemoryTrain:  epoch  5, batch     1 | loss: 1.4259801Losses:  1.4084259271621704 2.6689414978027344
MemoryTrain:  epoch  5, batch     2 | loss: 1.4084259Losses:  1.3930383920669556 2.676464319229126
MemoryTrain:  epoch  5, batch     3 | loss: 1.3930384Losses:  0.8960186243057251 1.6902049779891968
MemoryTrain:  epoch  5, batch     4 | loss: 0.8960186Losses:  1.3602426052093506 2.6577179431915283
MemoryTrain:  epoch  6, batch     0 | loss: 1.3602426Losses:  1.3823413848876953 2.6763534545898438
MemoryTrain:  epoch  6, batch     1 | loss: 1.3823414Losses:  1.429248571395874 2.6737723350524902
MemoryTrain:  epoch  6, batch     2 | loss: 1.4292486Losses:  1.4334627389907837 2.690558433532715
MemoryTrain:  epoch  6, batch     3 | loss: 1.4334627Losses:  0.9078942537307739 1.7001699209213257
MemoryTrain:  epoch  6, batch     4 | loss: 0.9078943Losses:  1.4041072130203247 2.684465169906616
MemoryTrain:  epoch  7, batch     0 | loss: 1.4041072Losses:  1.3942742347717285 2.6819119453430176
MemoryTrain:  epoch  7, batch     1 | loss: 1.3942742Losses:  1.3868407011032104 2.665454387664795
MemoryTrain:  epoch  7, batch     2 | loss: 1.3868407Losses:  1.4103230237960815 2.664532423019409
MemoryTrain:  epoch  7, batch     3 | loss: 1.4103230Losses:  0.8831591606140137 1.693150281906128
MemoryTrain:  epoch  7, batch     4 | loss: 0.8831592Losses:  1.3844146728515625 2.687159538269043
MemoryTrain:  epoch  8, batch     0 | loss: 1.3844147Losses:  1.3727365732192993 2.6672067642211914
MemoryTrain:  epoch  8, batch     1 | loss: 1.3727366Losses:  1.3808891773223877 2.670004367828369
MemoryTrain:  epoch  8, batch     2 | loss: 1.3808892Losses:  1.3763738870620728 2.6680312156677246
MemoryTrain:  epoch  8, batch     3 | loss: 1.3763739Losses:  0.8943206667900085 1.6962478160858154
MemoryTrain:  epoch  8, batch     4 | loss: 0.8943207Losses:  1.398268461227417 2.676840305328369
MemoryTrain:  epoch  9, batch     0 | loss: 1.3982685Losses:  1.4011188745498657 2.665865898132324
MemoryTrain:  epoch  9, batch     1 | loss: 1.4011189Losses:  1.3822864294052124 2.6836986541748047
MemoryTrain:  epoch  9, batch     2 | loss: 1.3822864Losses:  1.3608108758926392 2.659649133682251
MemoryTrain:  epoch  9, batch     3 | loss: 1.3608109Losses:  0.8955826759338379 1.7062430381774902
MemoryTrain:  epoch  9, batch     4 | loss: 0.8955827
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 58.70%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.57%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 52.55%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 50.89%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 49.35%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 48.12%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 46.77%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 47.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 48.67%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 49.63%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 51.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 52.87%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 54.11%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 54.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.94%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 59.23%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 61.04%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 62.24%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 63.33%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.14%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 62.92%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.60%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 61.81%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 86.57%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 82.50%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 81.86%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.55%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 80.96%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 80.40%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 80.04%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 79.50%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.35%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 79.61%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 79.06%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 78.69%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 78.56%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 78.28%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 78.01%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 77.74%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 77.11%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 76.49%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 75.81%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 75.29%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 74.64%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 74.29%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 74.23%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 73.91%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 73.59%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 73.34%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 72.46%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 72.23%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 71.81%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 71.12%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 70.15%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 69.04%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 68.47%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 67.52%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.51%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.06%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.13%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 67.66%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.36%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 68.25%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 67.89%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 67.61%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 67.29%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 67.02%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 66.68%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 67.29%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 66.88%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 66.48%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 66.08%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 65.29%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 65.01%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 64.91%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 64.55%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 64.46%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 64.34%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 64.23%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.34%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.59%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 65.29%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 64.95%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 64.64%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 64.54%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 64.25%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 64.00%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 63.85%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 63.03%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.98%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 63.92%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 63.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 67.21%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 67.39%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.39%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 67.37%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 66.84%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 67.39%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 67.24%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 67.17%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 66.84%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 66.71%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 66.33%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 66.12%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 65.92%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 65.69%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  292 | acc: 12.50%,  total acc: 65.08%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 64.92%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 65.15%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 66.28%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 66.13%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.74%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 65.70%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 65.69%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 65.69%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 67.18%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 67.01%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 66.89%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 66.72%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 66.36%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 66.89%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 66.71%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 66.63%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 66.61%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 66.26%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 65.76%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 66.33%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 66.15%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 66.02%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.79%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 65.48%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.33%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 65.20%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 65.34%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  431 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 66.17%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 66.10%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 66.06%   [EVAL] batch:  435 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 65.88%   
cur_acc:  ['0.9425', '0.7510', '0.7272', '0.7788', '0.7262', '0.6835', '0.6181']
his_acc:  ['0.9425', '0.8420', '0.7806', '0.7740', '0.7506', '0.7157', '0.6588']
Clustering into  38  clusters
Clusters:  [ 7 34  5  0 33 27  5  0 26 26  2 26 28  9 30 14 18  9 12 14 15 14  3 21
  0 34 17  0 10 29  5 17 15  8 31  7 13 19  0 35 20 37 11 36  0  6  1  4
 32 18  6 14 16 24 11 12  2  4  8 17 34 14 17 30  3 12  1  5  1  8  5 26
 10 14 23 22 25 16 36  2]
Losses:  7.149064540863037 2.116213798522949
CurrentTrain: epoch  0, batch     0 | loss: 7.1490645Losses:  7.406021595001221 1.9363750219345093
CurrentTrain: epoch  0, batch     1 | loss: 7.4060216Losses:  6.66684103012085 2.0157902240753174
CurrentTrain: epoch  0, batch     2 | loss: 6.6668410Losses:  6.2014570236206055 0.645243763923645
CurrentTrain: epoch  0, batch     3 | loss: 6.2014570Losses:  6.082850456237793 1.8273963928222656
CurrentTrain: epoch  1, batch     0 | loss: 6.0828505Losses:  6.270224571228027 1.899167537689209
CurrentTrain: epoch  1, batch     1 | loss: 6.2702246Losses:  6.023237228393555 1.8701153993606567
CurrentTrain: epoch  1, batch     2 | loss: 6.0232372Losses:  3.011159896850586 0.0
CurrentTrain: epoch  1, batch     3 | loss: 3.0111599Losses:  5.575530052185059 1.9178931713104248
CurrentTrain: epoch  2, batch     0 | loss: 5.5755301Losses:  5.8791913986206055 1.9081835746765137
CurrentTrain: epoch  2, batch     1 | loss: 5.8791914Losses:  5.597691535949707 1.8997178077697754
CurrentTrain: epoch  2, batch     2 | loss: 5.5976915Losses:  3.447885513305664 0.6307351589202881
CurrentTrain: epoch  2, batch     3 | loss: 3.4478855Losses:  5.508074760437012 2.073307514190674
CurrentTrain: epoch  3, batch     0 | loss: 5.5080748Losses:  5.184757709503174 1.8986213207244873
CurrentTrain: epoch  3, batch     1 | loss: 5.1847577Losses:  4.966276168823242 2.053239345550537
CurrentTrain: epoch  3, batch     2 | loss: 4.9662762Losses:  5.922360897064209 0.6781090497970581
CurrentTrain: epoch  3, batch     3 | loss: 5.9223609Losses:  4.7122673988342285 2.0497870445251465
CurrentTrain: epoch  4, batch     0 | loss: 4.7122674Losses:  4.4982099533081055 2.054783344268799
CurrentTrain: epoch  4, batch     1 | loss: 4.4982100Losses:  5.70281982421875 2.002565860748291
CurrentTrain: epoch  4, batch     2 | loss: 5.7028198Losses:  7.470558166503906 0.6722220182418823
CurrentTrain: epoch  4, batch     3 | loss: 7.4705582Losses:  5.219143867492676 2.0864388942718506
CurrentTrain: epoch  5, batch     0 | loss: 5.2191439Losses:  4.9749555587768555 1.8468255996704102
CurrentTrain: epoch  5, batch     1 | loss: 4.9749556Losses:  4.632126331329346 1.787323236465454
CurrentTrain: epoch  5, batch     2 | loss: 4.6321263Losses:  2.588329792022705 0.632164716720581
CurrentTrain: epoch  5, batch     3 | loss: 2.5883298Losses:  5.191418647766113 2.148812770843506
CurrentTrain: epoch  6, batch     0 | loss: 5.1914186Losses:  4.59786319732666 1.9745478630065918
CurrentTrain: epoch  6, batch     1 | loss: 4.5978632Losses:  3.984185218811035 1.9308110475540161
CurrentTrain: epoch  6, batch     2 | loss: 3.9841852Losses:  2.4385087490081787 0.664380669593811
CurrentTrain: epoch  6, batch     3 | loss: 2.4385087Losses:  4.624914169311523 1.9636516571044922
CurrentTrain: epoch  7, batch     0 | loss: 4.6249142Losses:  4.279417991638184 1.9292161464691162
CurrentTrain: epoch  7, batch     1 | loss: 4.2794180Losses:  4.055103302001953 2.1376001834869385
CurrentTrain: epoch  7, batch     2 | loss: 4.0551033Losses:  3.0649313926696777 0.6346113681793213
CurrentTrain: epoch  7, batch     3 | loss: 3.0649314Losses:  4.667560577392578 1.9315516948699951
CurrentTrain: epoch  8, batch     0 | loss: 4.6675606Losses:  3.80576753616333 1.8626301288604736
CurrentTrain: epoch  8, batch     1 | loss: 3.8057675Losses:  3.7459630966186523 2.047715425491333
CurrentTrain: epoch  8, batch     2 | loss: 3.7459631Losses:  3.9819135665893555 0.6508161425590515
CurrentTrain: epoch  8, batch     3 | loss: 3.9819136Losses:  3.5238637924194336 1.8892583847045898
CurrentTrain: epoch  9, batch     0 | loss: 3.5238638Losses:  4.148984909057617 2.1453332901000977
CurrentTrain: epoch  9, batch     1 | loss: 4.1489849Losses:  3.9467356204986572 1.908708930015564
CurrentTrain: epoch  9, batch     2 | loss: 3.9467356Losses:  3.0986368656158447 0.6539026498794556
CurrentTrain: epoch  9, batch     3 | loss: 3.0986369
Losses:  1.4717671871185303 2.6741600036621094
MemoryTrain:  epoch  0, batch     0 | loss: 1.4717672Losses:  2.1257987022399902 2.6862645149230957
MemoryTrain:  epoch  0, batch     1 | loss: 2.1257987Losses:  1.5951476097106934 2.702254295349121
MemoryTrain:  epoch  0, batch     2 | loss: 1.5951476Losses:  1.700742244720459 2.6634373664855957
MemoryTrain:  epoch  0, batch     3 | loss: 1.7007422Losses:  1.9276788234710693 2.6813883781433105
MemoryTrain:  epoch  0, batch     4 | loss: 1.9276788Losses:  2.4838309288024902 2.6768269538879395
MemoryTrain:  epoch  1, batch     0 | loss: 2.4838309Losses:  1.5071641206741333 2.685331344604492
MemoryTrain:  epoch  1, batch     1 | loss: 1.5071641Losses:  1.8537753820419312 2.694230794906616
MemoryTrain:  epoch  1, batch     2 | loss: 1.8537754Losses:  1.8660751581192017 2.672511577606201
MemoryTrain:  epoch  1, batch     3 | loss: 1.8660752Losses:  1.64359712600708 2.6602730751037598
MemoryTrain:  epoch  1, batch     4 | loss: 1.6435971Losses:  1.624006748199463 2.6907925605773926
MemoryTrain:  epoch  2, batch     0 | loss: 1.6240067Losses:  1.6463944911956787 2.6869916915893555
MemoryTrain:  epoch  2, batch     1 | loss: 1.6463945Losses:  1.6120613813400269 2.675644636154175
MemoryTrain:  epoch  2, batch     2 | loss: 1.6120614Losses:  1.4076025485992432 2.6697590351104736
MemoryTrain:  epoch  2, batch     3 | loss: 1.4076025Losses:  1.5315988063812256 2.6672260761260986
MemoryTrain:  epoch  2, batch     4 | loss: 1.5315988Losses:  1.4251805543899536 2.672534942626953
MemoryTrain:  epoch  3, batch     0 | loss: 1.4251806Losses:  1.517444372177124 2.6796228885650635
MemoryTrain:  epoch  3, batch     1 | loss: 1.5174444Losses:  1.4326010942459106 2.6832406520843506
MemoryTrain:  epoch  3, batch     2 | loss: 1.4326011Losses:  1.4540300369262695 2.6726841926574707
MemoryTrain:  epoch  3, batch     3 | loss: 1.4540300Losses:  1.515081763267517 2.675436496734619
MemoryTrain:  epoch  3, batch     4 | loss: 1.5150818Losses:  1.450528860092163 2.683070659637451
MemoryTrain:  epoch  4, batch     0 | loss: 1.4505289Losses:  1.4536852836608887 2.653472900390625
MemoryTrain:  epoch  4, batch     1 | loss: 1.4536853Losses:  1.463374137878418 2.670168161392212
MemoryTrain:  epoch  4, batch     2 | loss: 1.4633741Losses:  1.4505650997161865 2.691249370574951
MemoryTrain:  epoch  4, batch     3 | loss: 1.4505651Losses:  1.4386664628982544 2.6749091148376465
MemoryTrain:  epoch  4, batch     4 | loss: 1.4386665Losses:  1.4048097133636475 2.664806842803955
MemoryTrain:  epoch  5, batch     0 | loss: 1.4048097Losses:  1.3668321371078491 2.657496929168701
MemoryTrain:  epoch  5, batch     1 | loss: 1.3668321Losses:  1.4203070402145386 2.6766223907470703
MemoryTrain:  epoch  5, batch     2 | loss: 1.4203070Losses:  1.4021705389022827 2.667912006378174
MemoryTrain:  epoch  5, batch     3 | loss: 1.4021705Losses:  1.4487061500549316 2.690337657928467
MemoryTrain:  epoch  5, batch     4 | loss: 1.4487062Losses:  1.4283009767532349 2.6679296493530273
MemoryTrain:  epoch  6, batch     0 | loss: 1.4283010Losses:  1.3747464418411255 2.655794858932495
MemoryTrain:  epoch  6, batch     1 | loss: 1.3747464Losses:  1.3986930847167969 2.669428586959839
MemoryTrain:  epoch  6, batch     2 | loss: 1.3986931Losses:  1.4150868654251099 2.6836841106414795
MemoryTrain:  epoch  6, batch     3 | loss: 1.4150869Losses:  1.3821253776550293 2.6755452156066895
MemoryTrain:  epoch  6, batch     4 | loss: 1.3821254Losses:  1.3781081438064575 2.6368281841278076
MemoryTrain:  epoch  7, batch     0 | loss: 1.3781081Losses:  1.3956745862960815 2.6762540340423584
MemoryTrain:  epoch  7, batch     1 | loss: 1.3956746Losses:  1.3725857734680176 2.682347297668457
MemoryTrain:  epoch  7, batch     2 | loss: 1.3725858Losses:  1.387026309967041 2.6817893981933594
MemoryTrain:  epoch  7, batch     3 | loss: 1.3870263Losses:  1.3971951007843018 2.6786556243896484
MemoryTrain:  epoch  7, batch     4 | loss: 1.3971951Losses:  1.3832067251205444 2.677112340927124
MemoryTrain:  epoch  8, batch     0 | loss: 1.3832067Losses:  1.3506635427474976 2.6450560092926025
MemoryTrain:  epoch  8, batch     1 | loss: 1.3506635Losses:  1.4149889945983887 2.6884350776672363
MemoryTrain:  epoch  8, batch     2 | loss: 1.4149890Losses:  1.3684070110321045 2.6745035648345947
MemoryTrain:  epoch  8, batch     3 | loss: 1.3684070Losses:  1.3633145093917847 2.6651175022125244
MemoryTrain:  epoch  8, batch     4 | loss: 1.3633145Losses:  1.3742355108261108 2.6837210655212402
MemoryTrain:  epoch  9, batch     0 | loss: 1.3742355Losses:  1.3757216930389404 2.6647677421569824
MemoryTrain:  epoch  9, batch     1 | loss: 1.3757217Losses:  1.3850260972976685 2.671159505844116
MemoryTrain:  epoch  9, batch     2 | loss: 1.3850261Losses:  1.36867094039917 2.6614952087402344
MemoryTrain:  epoch  9, batch     3 | loss: 1.3686709Losses:  1.3645262718200684 2.666325569152832
MemoryTrain:  epoch  9, batch     4 | loss: 1.3645263
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 75.27%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 74.47%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 75.11%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 73.20%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 72.29%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 70.97%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 84.65%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 83.59%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.80%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 82.06%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.59%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 79.42%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 78.02%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 76.19%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 75.29%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 74.62%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 73.58%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 71.78%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 71.20%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 72.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 71.83%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 71.55%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 71.44%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 71.41%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 71.34%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 70.78%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 70.24%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 69.71%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 69.33%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 68.47%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 68.41%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 67.17%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 66.20%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 65.85%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 65.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 65.15%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 64.29%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 63.76%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 62.73%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 62.39%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 64.30%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 64.12%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 65.72%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 65.24%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 64.82%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 64.41%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 63.95%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 63.51%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 63.24%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 64.24%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 63.86%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 63.60%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 63.27%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 62.86%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 63.46%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 63.38%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 63.00%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 62.61%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 62.24%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 61.86%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 61.50%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 61.24%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 61.18%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 61.26%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 61.12%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 61.02%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 61.06%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 61.07%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 60.87%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 60.70%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 60.53%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 60.41%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 60.21%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 60.22%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 60.27%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 60.38%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 60.56%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 60.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 60.89%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 61.03%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 61.38%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 61.58%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 62.18%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 61.99%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 61.80%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 61.62%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 61.46%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 61.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 61.42%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 61.42%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 61.31%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 61.28%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 61.23%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 60.99%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 60.76%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 60.62%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 60.36%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 59.91%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 59.92%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 60.29%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 60.66%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 60.84%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 60.99%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 61.00%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 61.01%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 60.99%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 61.02%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 60.92%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 61.26%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 61.43%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 62.39%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 62.55%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 62.92%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 63.15%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 63.30%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 64.19%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 64.30%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 64.32%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 64.31%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 64.37%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 64.29%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 64.02%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 63.90%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 64.33%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 64.24%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 64.16%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.00%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 63.75%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 63.55%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 63.37%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 63.17%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 62.95%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 62.74%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 62.54%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 62.35%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 62.20%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 62.25%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 62.42%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 62.54%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.47%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 63.57%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 63.71%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 63.53%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 63.17%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 62.85%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 62.89%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 63.04%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 62.96%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 62.88%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 62.80%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 62.73%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 62.63%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 62.61%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 63.42%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 64.23%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 64.04%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 63.86%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 63.68%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 63.50%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 63.36%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 63.84%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 63.73%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 63.70%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 63.73%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 63.65%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 63.63%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 63.54%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 63.59%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 63.38%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 63.26%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 63.13%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 63.03%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 62.89%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 62.76%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 63.08%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:  391 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  394 | acc: 6.25%,  total acc: 63.26%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 63.13%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 63.02%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 62.89%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 62.66%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 62.52%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 62.36%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.21%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 62.05%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 61.90%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 61.76%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 61.83%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 62.08%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 62.21%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 62.36%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 62.43%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.51%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 62.60%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 62.68%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 62.74%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  422 | acc: 87.50%,  total acc: 62.84%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 62.99%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 62.91%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 62.91%   [EVAL] batch:  427 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 62.91%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 62.93%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 62.85%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 62.79%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:  435 | acc: 25.00%,  total acc: 62.67%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 62.68%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 62.74%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 62.74%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 62.74%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 62.81%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  446 | acc: 50.00%,  total acc: 62.82%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 62.86%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 62.89%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 62.93%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 63.04%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  454 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 63.11%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 62.97%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.05%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 63.95%   [EVAL] batch:  478 | acc: 62.50%,  total acc: 63.95%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 63.98%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 63.95%   [EVAL] batch:  482 | acc: 18.75%,  total acc: 63.86%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 63.79%   [EVAL] batch:  484 | acc: 56.25%,  total acc: 63.78%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 63.73%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 64.02%   [EVAL] batch:  494 | acc: 31.25%,  total acc: 63.95%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 63.87%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 63.85%   [EVAL] batch:  497 | acc: 31.25%,  total acc: 63.78%   [EVAL] batch:  498 | acc: 18.75%,  total acc: 63.69%   [EVAL] batch:  499 | acc: 31.25%,  total acc: 63.62%   
cur_acc:  ['0.9425', '0.7510', '0.7272', '0.7788', '0.7262', '0.6835', '0.6181', '0.7014']
his_acc:  ['0.9425', '0.8420', '0.7806', '0.7740', '0.7506', '0.7157', '0.6588', '0.6362']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  11.358258247375488 1.9253628253936768
CurrentTrain: epoch  0, batch     0 | loss: 11.3582582Losses:  10.998990058898926 2.0885913372039795
CurrentTrain: epoch  0, batch     1 | loss: 10.9989901Losses:  10.62999439239502 1.7875231504440308
CurrentTrain: epoch  0, batch     2 | loss: 10.6299944Losses:  11.120280265808105 2.19905424118042
CurrentTrain: epoch  0, batch     3 | loss: 11.1202803Losses:  10.873442649841309 1.9093797206878662
CurrentTrain: epoch  0, batch     4 | loss: 10.8734426Losses:  10.179046630859375 1.9966771602630615
CurrentTrain: epoch  0, batch     5 | loss: 10.1790466Losses:  10.525503158569336 1.8928296566009521
CurrentTrain: epoch  0, batch     6 | loss: 10.5255032Losses:  10.175249099731445 1.8417518138885498
CurrentTrain: epoch  0, batch     7 | loss: 10.1752491Losses:  10.532530784606934 2.0540261268615723
CurrentTrain: epoch  0, batch     8 | loss: 10.5325308Losses:  9.963665008544922 2.020362138748169
CurrentTrain: epoch  0, batch     9 | loss: 9.9636650Losses:  10.534697532653809 2.071371555328369
CurrentTrain: epoch  0, batch    10 | loss: 10.5346975Losses:  9.656057357788086 2.0335445404052734
CurrentTrain: epoch  0, batch    11 | loss: 9.6560574Losses:  9.6796293258667 1.7908227443695068
CurrentTrain: epoch  0, batch    12 | loss: 9.6796293Losses:  9.677530288696289 2.0670289993286133
CurrentTrain: epoch  0, batch    13 | loss: 9.6775303Losses:  10.009037017822266 1.833444356918335
CurrentTrain: epoch  0, batch    14 | loss: 10.0090370Losses:  9.160542488098145 2.0129683017730713
CurrentTrain: epoch  0, batch    15 | loss: 9.1605425Losses:  9.610183715820312 2.0734705924987793
CurrentTrain: epoch  0, batch    16 | loss: 9.6101837Losses:  9.340691566467285 1.8308147192001343
CurrentTrain: epoch  0, batch    17 | loss: 9.3406916Losses:  9.706151008605957 1.7878763675689697
CurrentTrain: epoch  0, batch    18 | loss: 9.7061510Losses:  8.896862030029297 2.0187599658966064
CurrentTrain: epoch  0, batch    19 | loss: 8.8968620Losses:  8.939189910888672 1.832984209060669
CurrentTrain: epoch  0, batch    20 | loss: 8.9391899Losses:  9.388198852539062 1.8670756816864014
CurrentTrain: epoch  0, batch    21 | loss: 9.3881989Losses:  8.723284721374512 1.6744005680084229
CurrentTrain: epoch  0, batch    22 | loss: 8.7232847Losses:  8.99166202545166 1.990477442741394
CurrentTrain: epoch  0, batch    23 | loss: 8.9916620Losses:  9.621352195739746 2.0668368339538574
CurrentTrain: epoch  0, batch    24 | loss: 9.6213522Losses:  8.853668212890625 1.7195383310317993
CurrentTrain: epoch  0, batch    25 | loss: 8.8536682Losses:  9.442800521850586 2.0127458572387695
CurrentTrain: epoch  0, batch    26 | loss: 9.4428005Losses:  9.582098007202148 2.0798327922821045
CurrentTrain: epoch  0, batch    27 | loss: 9.5820980Losses:  9.171624183654785 1.8944220542907715
CurrentTrain: epoch  0, batch    28 | loss: 9.1716242Losses:  8.963388442993164 1.771702527999878
CurrentTrain: epoch  0, batch    29 | loss: 8.9633884Losses:  8.897431373596191 2.029674768447876
CurrentTrain: epoch  0, batch    30 | loss: 8.8974314Losses:  9.16209602355957 2.188523292541504
CurrentTrain: epoch  0, batch    31 | loss: 9.1620960Losses:  9.440577507019043 2.0667083263397217
CurrentTrain: epoch  0, batch    32 | loss: 9.4405775Losses:  8.982353210449219 2.0958750247955322
CurrentTrain: epoch  0, batch    33 | loss: 8.9823532Losses:  8.661284446716309 2.048476219177246
CurrentTrain: epoch  0, batch    34 | loss: 8.6612844Losses:  8.147836685180664 1.8798246383666992
CurrentTrain: epoch  0, batch    35 | loss: 8.1478367Losses:  8.961817741394043 1.6312651634216309
CurrentTrain: epoch  0, batch    36 | loss: 8.9618177Losses:  8.539031028747559 2.0095438957214355
CurrentTrain: epoch  0, batch    37 | loss: 8.5390310Losses:  8.863781929016113 2.0664196014404297
CurrentTrain: epoch  0, batch    38 | loss: 8.8637819Losses:  9.021777153015137 2.0575194358825684
CurrentTrain: epoch  0, batch    39 | loss: 9.0217772Losses:  8.646954536437988 2.024179458618164
CurrentTrain: epoch  0, batch    40 | loss: 8.6469545Losses:  8.57384967803955 1.9752799272537231
CurrentTrain: epoch  0, batch    41 | loss: 8.5738497Losses:  8.405529975891113 1.8688719272613525
CurrentTrain: epoch  0, batch    42 | loss: 8.4055300Losses:  8.668790817260742 1.8524975776672363
CurrentTrain: epoch  0, batch    43 | loss: 8.6687908Losses:  8.27130126953125 1.850342035293579
CurrentTrain: epoch  0, batch    44 | loss: 8.2713013Losses:  8.600872039794922 1.8586866855621338
CurrentTrain: epoch  0, batch    45 | loss: 8.6008720Losses:  8.720293045043945 1.9142627716064453
CurrentTrain: epoch  0, batch    46 | loss: 8.7202930Losses:  8.1428804397583 1.9660096168518066
CurrentTrain: epoch  0, batch    47 | loss: 8.1428804Losses:  8.786876678466797 1.9221405982971191
CurrentTrain: epoch  0, batch    48 | loss: 8.7868767Losses:  8.07455062866211 2.0323855876922607
CurrentTrain: epoch  0, batch    49 | loss: 8.0745506Losses:  9.358478546142578 1.5273046493530273
CurrentTrain: epoch  0, batch    50 | loss: 9.3584785Losses:  7.602519512176514 1.8085930347442627
CurrentTrain: epoch  0, batch    51 | loss: 7.6025195Losses:  8.3646879196167 2.015393018722534
CurrentTrain: epoch  0, batch    52 | loss: 8.3646879Losses:  8.318044662475586 1.6801527738571167
CurrentTrain: epoch  0, batch    53 | loss: 8.3180447Losses:  9.673336029052734 1.9528387784957886
CurrentTrain: epoch  0, batch    54 | loss: 9.6733360Losses:  9.342101097106934 1.9522196054458618
CurrentTrain: epoch  0, batch    55 | loss: 9.3421011Losses:  7.6254048347473145 1.9104670286178589
CurrentTrain: epoch  0, batch    56 | loss: 7.6254048Losses:  7.643649101257324 1.7517918348312378
CurrentTrain: epoch  0, batch    57 | loss: 7.6436491Losses:  8.27759075164795 1.979539155960083
CurrentTrain: epoch  0, batch    58 | loss: 8.2775908Losses:  8.617254257202148 1.6532423496246338
CurrentTrain: epoch  0, batch    59 | loss: 8.6172543Losses:  8.389081954956055 1.9247541427612305
CurrentTrain: epoch  0, batch    60 | loss: 8.3890820Losses:  7.9325714111328125 1.852124810218811
CurrentTrain: epoch  0, batch    61 | loss: 7.9325714Losses:  7.4617919921875 1.6636137962341309
CurrentTrain: epoch  0, batch    62 | loss: 7.4617920Losses:  9.40252685546875 1.8683619499206543
CurrentTrain: epoch  1, batch     0 | loss: 9.4025269Losses:  7.330815315246582 1.9218454360961914
CurrentTrain: epoch  1, batch     1 | loss: 7.3308153Losses:  8.238710403442383 2.0139293670654297
CurrentTrain: epoch  1, batch     2 | loss: 8.2387104Losses:  7.672242641448975 1.7333027124404907
CurrentTrain: epoch  1, batch     3 | loss: 7.6722426Losses:  7.263276100158691 1.9776842594146729
CurrentTrain: epoch  1, batch     4 | loss: 7.2632761Losses:  7.133970737457275 1.754868745803833
CurrentTrain: epoch  1, batch     5 | loss: 7.1339707Losses:  8.127798080444336 1.7702264785766602
CurrentTrain: epoch  1, batch     6 | loss: 8.1277981Losses:  8.057364463806152 1.939797282218933
CurrentTrain: epoch  1, batch     7 | loss: 8.0573645Losses:  8.022777557373047 2.037144660949707
CurrentTrain: epoch  1, batch     8 | loss: 8.0227776Losses:  7.210246562957764 1.4523632526397705
CurrentTrain: epoch  1, batch     9 | loss: 7.2102466Losses:  7.759590148925781 1.8737789392471313
CurrentTrain: epoch  1, batch    10 | loss: 7.7595901Losses:  8.6473970413208 1.9968616962432861
CurrentTrain: epoch  1, batch    11 | loss: 8.6473970Losses:  7.767533302307129 1.8815346956253052
CurrentTrain: epoch  1, batch    12 | loss: 7.7675333Losses:  7.824026584625244 1.8949286937713623
CurrentTrain: epoch  1, batch    13 | loss: 7.8240266Losses:  8.199854850769043 2.0563182830810547
CurrentTrain: epoch  1, batch    14 | loss: 8.1998549Losses:  7.820784568786621 1.8312249183654785
CurrentTrain: epoch  1, batch    15 | loss: 7.8207846Losses:  7.38732385635376 1.9984455108642578
CurrentTrain: epoch  1, batch    16 | loss: 7.3873239Losses:  8.712005615234375 1.9180837869644165
CurrentTrain: epoch  1, batch    17 | loss: 8.7120056Losses:  7.577951431274414 2.054311752319336
CurrentTrain: epoch  1, batch    18 | loss: 7.5779514Losses:  7.540454387664795 1.9936892986297607
CurrentTrain: epoch  1, batch    19 | loss: 7.5404544Losses:  7.770913600921631 2.0614614486694336
CurrentTrain: epoch  1, batch    20 | loss: 7.7709136Losses:  7.687095642089844 2.056968927383423
CurrentTrain: epoch  1, batch    21 | loss: 7.6870956Losses:  7.361689567565918 2.033294916152954
CurrentTrain: epoch  1, batch    22 | loss: 7.3616896Losses:  7.7243804931640625 1.9328230619430542
CurrentTrain: epoch  1, batch    23 | loss: 7.7243805Losses:  7.900519847869873 1.9739387035369873
CurrentTrain: epoch  1, batch    24 | loss: 7.9005198Losses:  7.1376190185546875 2.020720958709717
CurrentTrain: epoch  1, batch    25 | loss: 7.1376190Losses:  8.202679634094238 1.9181445837020874
CurrentTrain: epoch  1, batch    26 | loss: 8.2026796Losses:  6.809338569641113 1.7210767269134521
CurrentTrain: epoch  1, batch    27 | loss: 6.8093386Losses:  6.695901393890381 1.746529459953308
CurrentTrain: epoch  1, batch    28 | loss: 6.6959014Losses:  7.343115329742432 2.057131767272949
CurrentTrain: epoch  1, batch    29 | loss: 7.3431153Losses:  7.503020286560059 2.029431104660034
CurrentTrain: epoch  1, batch    30 | loss: 7.5030203Losses:  7.663934707641602 1.8571569919586182
CurrentTrain: epoch  1, batch    31 | loss: 7.6639347Losses:  6.984493732452393 1.6701555252075195
CurrentTrain: epoch  1, batch    32 | loss: 6.9844937Losses:  7.211524486541748 1.9674251079559326
CurrentTrain: epoch  1, batch    33 | loss: 7.2115245Losses:  7.4532246589660645 1.9353368282318115
CurrentTrain: epoch  1, batch    34 | loss: 7.4532247Losses:  7.6426897048950195 1.9968435764312744
CurrentTrain: epoch  1, batch    35 | loss: 7.6426897Losses:  7.2708611488342285 2.143423318862915
CurrentTrain: epoch  1, batch    36 | loss: 7.2708611Losses:  7.153193473815918 2.0565316677093506
CurrentTrain: epoch  1, batch    37 | loss: 7.1531935Losses:  7.825572967529297 2.1543688774108887
CurrentTrain: epoch  1, batch    38 | loss: 7.8255730Losses:  6.067594528198242 1.441818118095398
CurrentTrain: epoch  1, batch    39 | loss: 6.0675945Losses:  7.639055252075195 1.8890461921691895
CurrentTrain: epoch  1, batch    40 | loss: 7.6390553Losses:  7.944948673248291 2.0423686504364014
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  11.822247505187988 1.7946100234985352
CurrentTrain: epoch  0, batch     0 | loss: 11.8222475Losses:  12.024381637573242 2.0578439235687256
CurrentTrain: epoch  0, batch     1 | loss: 12.0243816Losses:  11.974482536315918 1.8955256938934326
CurrentTrain: epoch  0, batch     2 | loss: 11.9744825Losses:  11.770575523376465 1.7659683227539062
CurrentTrain: epoch  0, batch     3 | loss: 11.7705755Losses:  11.207915306091309 1.9585809707641602
CurrentTrain: epoch  0, batch     4 | loss: 11.2079153Losses:  12.117916107177734 1.6816767454147339
CurrentTrain: epoch  0, batch     5 | loss: 12.1179161Losses:  11.46302604675293 1.9070019721984863
CurrentTrain: epoch  0, batch     6 | loss: 11.4630260Losses:  10.803938865661621 1.8507782220840454
CurrentTrain: epoch  0, batch     7 | loss: 10.8039389Losses:  11.0195894241333 1.90924870967865
CurrentTrain: epoch  0, batch     8 | loss: 11.0195894Losses:  11.632707595825195 1.9158982038497925
CurrentTrain: epoch  0, batch     9 | loss: 11.6327076Losses:  10.713784217834473 1.7462854385375977
CurrentTrain: epoch  0, batch    10 | loss: 10.7137842Losses:  10.157379150390625 1.7152104377746582
CurrentTrain: epoch  0, batch    11 | loss: 10.1573792Losses:  10.899659156799316 1.769860029220581
CurrentTrain: epoch  0, batch    12 | loss: 10.8996592Losses:  10.953350067138672 1.9231852293014526
CurrentTrain: epoch  0, batch    13 | loss: 10.9533501Losses:  9.993746757507324 1.8386659622192383
CurrentTrain: epoch  0, batch    14 | loss: 9.9937468Losses:  10.873373031616211 2.03297758102417
CurrentTrain: epoch  0, batch    15 | loss: 10.8733730Losses:  10.809783935546875 1.5790233612060547
CurrentTrain: epoch  0, batch    16 | loss: 10.8097839Losses:  11.061325073242188 2.0494818687438965
CurrentTrain: epoch  0, batch    17 | loss: 11.0613251Losses:  10.792543411254883 1.8168083429336548
CurrentTrain: epoch  0, batch    18 | loss: 10.7925434Losses:  10.645845413208008 2.008537530899048
CurrentTrain: epoch  0, batch    19 | loss: 10.6458454Losses:  9.976994514465332 1.9619722366333008
CurrentTrain: epoch  0, batch    20 | loss: 9.9769945Losses:  10.168614387512207 1.979183554649353
CurrentTrain: epoch  0, batch    21 | loss: 10.1686144Losses:  10.67384147644043 1.932677984237671
CurrentTrain: epoch  0, batch    22 | loss: 10.6738415Losses:  10.053910255432129 1.9177439212799072
CurrentTrain: epoch  0, batch    23 | loss: 10.0539103Losses:  9.746173858642578 1.98309326171875
CurrentTrain: epoch  0, batch    24 | loss: 9.7461739Losses:  9.892935752868652 1.8832637071609497
CurrentTrain: epoch  0, batch    25 | loss: 9.8929358Losses:  10.015974998474121 1.8018438816070557
CurrentTrain: epoch  0, batch    26 | loss: 10.0159750Losses:  9.772150039672852 2.0287418365478516
CurrentTrain: epoch  0, batch    27 | loss: 9.7721500Losses:  10.074874877929688 1.8718838691711426
CurrentTrain: epoch  0, batch    28 | loss: 10.0748749Losses:  10.11726188659668 1.9666043519973755
CurrentTrain: epoch  0, batch    29 | loss: 10.1172619Losses:  10.631096839904785 2.076991081237793
CurrentTrain: epoch  0, batch    30 | loss: 10.6310968Losses:  9.78180980682373 1.7801653146743774
CurrentTrain: epoch  0, batch    31 | loss: 9.7818098Losses:  9.434821128845215 1.4613122940063477
CurrentTrain: epoch  0, batch    32 | loss: 9.4348211Losses:  9.104872703552246 1.653035283088684
CurrentTrain: epoch  0, batch    33 | loss: 9.1048727Losses:  9.699578285217285 1.8228559494018555
CurrentTrain: epoch  0, batch    34 | loss: 9.6995783Losses:  9.593276977539062 1.8653221130371094
CurrentTrain: epoch  0, batch    35 | loss: 9.5932770Losses:  9.6148099899292 1.899040699005127
CurrentTrain: epoch  0, batch    36 | loss: 9.6148100Losses:  9.65835189819336 1.9168058633804321
CurrentTrain: epoch  0, batch    37 | loss: 9.6583519Losses:  10.620696067810059 2.1155948638916016
CurrentTrain: epoch  0, batch    38 | loss: 10.6206961Losses:  10.16646957397461 2.150613307952881
CurrentTrain: epoch  0, batch    39 | loss: 10.1664696Losses:  10.458769798278809 2.032524347305298
CurrentTrain: epoch  0, batch    40 | loss: 10.4587698Losses:  9.39407730102539 1.974230408668518
CurrentTrain: epoch  0, batch    41 | loss: 9.3940773Losses:  9.20484733581543 1.9367986917495728
CurrentTrain: epoch  0, batch    42 | loss: 9.2048473Losses:  8.666779518127441 1.6444140672683716
CurrentTrain: epoch  0, batch    43 | loss: 8.6667795Losses:  9.079657554626465 1.7442622184753418
CurrentTrain: epoch  0, batch    44 | loss: 9.0796576Losses:  9.34709358215332 2.018223285675049
CurrentTrain: epoch  0, batch    45 | loss: 9.3470936Losses:  9.914682388305664 1.791323184967041
CurrentTrain: epoch  0, batch    46 | loss: 9.9146824Losses:  8.59722900390625 2.0108981132507324
CurrentTrain: epoch  0, batch    47 | loss: 8.5972290Losses:  9.4835786819458 1.9152553081512451
CurrentTrain: epoch  0, batch    48 | loss: 9.4835787Losses:  9.20748519897461 1.9187994003295898
CurrentTrain: epoch  0, batch    49 | loss: 9.2074852Losses:  9.655864715576172 1.968374252319336
CurrentTrain: epoch  0, batch    50 | loss: 9.6558647Losses:  9.300230979919434 2.0851426124572754
CurrentTrain: epoch  0, batch    51 | loss: 9.3002310Losses:  8.878984451293945 1.9124865531921387
CurrentTrain: epoch  0, batch    52 | loss: 8.8789845Losses:  9.019617080688477 1.8851890563964844
CurrentTrain: epoch  0, batch    53 | loss: 9.0196171Losses:  9.77613639831543 2.042708158493042
CurrentTrain: epoch  0, batch    54 | loss: 9.7761364Losses:  8.825751304626465 1.8307257890701294
CurrentTrain: epoch  0, batch    55 | loss: 8.8257513Losses:  8.551468849182129 1.7922003269195557
CurrentTrain: epoch  0, batch    56 | loss: 8.5514688Losses:  9.474181175231934 1.9789186716079712
CurrentTrain: epoch  0, batch    57 | loss: 9.4741812Losses:  9.091450691223145 1.7218990325927734
CurrentTrain: epoch  0, batch    58 | loss: 9.0914507Losses:  8.748722076416016 1.713090181350708
CurrentTrain: epoch  0, batch    59 | loss: 8.7487221Losses:  8.445503234863281 1.930284023284912
CurrentTrain: epoch  0, batch    60 | loss: 8.4455032Losses:  9.177824974060059 1.7486907243728638
CurrentTrain: epoch  0, batch    61 | loss: 9.1778250Losses:  10.373208045959473 1.4577839374542236
CurrentTrain: epoch  0, batch    62 | loss: 10.3732080Losses:  9.169873237609863 1.7740023136138916
CurrentTrain: epoch  1, batch     0 | loss: 9.1698732Losses:  8.863645553588867 1.8713459968566895
CurrentTrain: epoch  1, batch     1 | loss: 8.8636456Losses:  8.501132011413574 1.778637170791626
CurrentTrain: epoch  1, batch     2 | loss: 8.5011320Losses:  8.994280815124512 2.0424678325653076
CurrentTrain: epoch  1, batch     3 | loss: 8.9942808Losses:  9.289648056030273 2.0178728103637695
CurrentTrain: epoch  1, batch     4 | loss: 9.2896481Losses:  8.659710884094238 1.9110162258148193
CurrentTrain: epoch  1, batch     5 | loss: 8.6597109Losses:  8.790841102600098 1.8980923891067505
CurrentTrain: epoch  1, batch     6 | loss: 8.7908411Losses:  8.19517707824707 2.0407543182373047
CurrentTrain: epoch  1, batch     7 | loss: 8.1951771Losses:  8.03210735321045 1.7190583944320679
CurrentTrain: epoch  1, batch     8 | loss: 8.0321074Losses:  8.973485946655273 1.9658069610595703
CurrentTrain: epoch  1, batch     9 | loss: 8.9734859Losses:  8.856561660766602 1.9306411743164062
CurrentTrain: epoch  1, batch    10 | loss: 8.8565617Losses:  8.48607063293457 1.9469120502471924
CurrentTrain: epoch  1, batch    11 | loss: 8.4860706Losses:  8.811304092407227 2.00516414642334
CurrentTrain: epoch  1, batch    12 | loss: 8.8113041Losses:  8.753872871398926 1.638258695602417
CurrentTrain: epoch  1, batch    13 | loss: 8.7538729Losses:  8.19471549987793 1.9477602243423462
CurrentTrain: epoch  1, batch    14 | loss: 8.1947155Losses:  9.750614166259766 1.8800532817840576
CurrentTrain: epoch  1, batch    15 | loss: 9.7506142Losses:  8.345575332641602 1.7727036476135254
CurrentTrain: epoch  1, batch    16 | loss: 8.3455753Losses:  7.549742221832275 1.6079249382019043
CurrentTrain: epoch  1, batch    17 | loss: 7.5497422Losses:  8.231653213500977 1.8322997093200684
CurrentTrain: epoch  1, batch    18 | loss: 8.2316532Losses:  8.62293815612793 1.9597173929214478
CurrentTrain: epoch  1, batch    19 | loss: 8.6229382Losses:  8.99839973449707 2.0598411560058594
CurrentTrain: epoch  1, batch    20 | loss: 8.9983997Losses:  8.980754852294922 2.050067901611328
CurrentTrain: epoch  1, batch    21 | loss: 8.9807549Losses:  8.600215911865234 1.8859483003616333
CurrentTrain: epoch  1, batch    22 | loss: 8.6002159Losses:  8.590490341186523 1.7222402095794678
CurrentTrain: epoch  1, batch    23 | loss: 8.5904903Losses:  8.44637393951416 1.900984764099121
CurrentTrain: epoch  1, batch    24 | loss: 8.4463739Losses:  7.398235321044922 1.8330962657928467
CurrentTrain: epoch  1, batch    25 | loss: 7.3982353Losses:  8.203048706054688 2.0549368858337402
CurrentTrain: epoch  1, batch    26 | loss: 8.2030487Losses:  8.023688316345215 1.9343112707138062
CurrentTrain: epoch  1, batch    27 | loss: 8.0236883Losses:  8.476688385009766 1.9680867195129395
CurrentTrain: epoch  1, batch    28 | loss: 8.4766884Losses:  7.965878486633301 2.0439870357513428
CurrentTrain: epoch  1, batch    29 | loss: 7.9658785Losses:  8.178144454956055 1.8898203372955322
CurrentTrain: epoch  1, batch    30 | loss: 8.1781445Losses:  7.763594150543213 2.0805106163024902
CurrentTrain: epoch  1, batch    31 | loss: 7.7635942Losses:  7.39365291595459 1.9456499814987183
CurrentTrain: epoch  1, batch    32 | loss: 7.3936529Losses:  7.610587120056152 1.711379051208496
CurrentTrain: epoch  1, batch    33 | loss: 7.6105871Losses:  8.717252731323242 2.015094757080078
CurrentTrain: epoch  1, batch    34 | loss: 8.7172527Losses:  7.515501976013184 1.8682222366333008
CurrentTrain: epoch  1, batch    35 | loss: 7.5155020Losses:  8.595856666564941 1.7169508934020996
CurrentTrain: epoch  1, batch    36 | loss: 8.5958567Losses:  8.619948387145996 1.9999830722808838
CurrentTrain: epoch  1, batch    37 | loss: 8.6199484Losses:  9.118426322937012 2.0533695220947266
CurrentTrain: epoch  1, batch    38 | loss: 9.1184263Losses:  7.568685531616211 1.9659147262573242
CurrentTrain: epoch  1, batch    39 | loss: 7.5686855Losses:  8.489702224731445 1.8747167587280273
CurrentTrain: epoch  1, batch    40 | loss: 8.4897022Losses:  8.855988502502441 1.99640953540802
CurrentTrain: epoch  1, batch    41 | loss: 8.8559885Losses:  8.788667678833008 1.8869547843933105
CurrentTrain: epoch  1, batch    42 | loss: 8.7886677Losses:  9.237432479858398 1.9152894020080566
CurrentTrain: epoch  1, batch    43 | loss: 9.2374325Losses:  9.151065826416016 1.786874771118164
CurrentTrain: epoch  1, batch    44 | loss: 9.1510658Losses:  7.895082473754883 2.018779993057251
CurrentTrain: epoch  1, batch    45 | loss: 7.8950825Losses:  7.410953998565674 1.9532017707824707
CurrentTrain: epoch  1, batch    46 | loss: 7.4109540Losses:  8.604068756103516 2.1348979473114014
CurrentTrain: epoch  1, batch    47 | loss: 8.6040688Losses:  8.134878158569336 1.90714693069458
CurrentTrain: epoch  1, batch    48 | loss: 8.1348782Losses:  7.826756954193115 1.920945644378662
CurrentTrain: epoch  1, batch    49 | loss: 7.8267570Losses:  8.07823371887207 1.8666021823883057
CurrentTrain: epoch  1, batch    50 | loss: 8.0782337Losses:  8.923225402832031 1.7712435722351074
CurrentTrain: epoch  1, batch    51 | loss: 8.9232254Losses:  8.14305305480957 1.8455361127853394
CurrentTrain: epoch  1, batch    52 | loss: 8.1430531Losses:  8.242971420288086 1.8711578845977783
CurrentTrain: epoch  1, batch    53 | loss: 8.2429714Losses:  8.248449325561523 1.9557065963745117
CurrentTrain: epoch  1, batch    54 | loss: 8.2484493Losses:  7.022468090057373 1.719062328338623
CurrentTrain: epoch  1, batch    55 | loss: 7.0224681Losses:  7.087942123413086 1.737884759902954
CurrentTrain: epoch  1, batch    56 | loss: 7.0879421Losses:  8.101158142089844 1.9902607202529907
CurrentTrain: epoch  1, batch    57 | loss: 8.1011581Losses:  7.295104026794434 1.7684481143951416
CurrentTrain: epoch  1, batch    58 | loss: 7.2951040Losses:  8.710687637329102 2.0563178062438965
CurrentTrain: epoch  1, batch    59 | loss: 8.7106876Losses:  8.243114471435547 1.9990880489349365
CurrentTrain: epoch  1, batch    60 | loss: 8.2431145Losses:  7.796777725219727 2.026205062866211
CurrentTrain: epoch  1, batch    61 | loss: 7.7967777Losses:  6.145471572875977 1.3907458782196045
CurrentTrain: epoch  1, batch    62 | loss: 6.1454716Losses:  7.6539130210876465 1.6753090620040894
CurrentTrain: epoch  2, batch     0 | loss: 7.6539130Losses:  7.638369560241699 1.6877038478851318
CurrentTrain: epoch  2, batch     1 | loss: 7.6383696Losses:  7.857037544250488 1.958578109741211
CurrentTrain: epoch  2, batch     2 | loss: 7.8570375Losses:  7.081042289733887 1.8904119729995728
CurrentTrain: epoch  2, batch     3 | loss: 7.0810423Losses:  7.507321357727051 1.878629207611084
CurrentTrain: epoch  2, batch     4 | loss: 7.5073214Losses:  7.568510055541992 2.1217730045318604
CurrentTrain: epoch  2, batch     5 | loss: 7.5685101Losses:  7.194374084472656 1.7517234086990356
CurrentTrain: epoch  2, batch     6 | loss: 7.1943741Losses:  8.065010070800781 1.9448485374450684
CurrentTrain: epoch  2, batch     7 | loss: 8.0650101Losses:  8.61500072479248 1.9257338047027588
CurrentTrain: epoch  2, batch     8 | loss: 8.6150007Losses:  7.088397979736328 1.827521562576294
CurrentTrain: epoch  2, batch     9 | loss: 7.0883980Losses:  7.055172920227051 1.911207675933838
CurrentTrain: epoch  2, batch    10 | loss: 7.0551729Losses:  7.892908096313477 1.8732116222381592
CurrentTrain: epoch  2, batch    11 | loss: 7.8929081Losses:  8.107831001281738 1.8145906925201416
CurrentTrain: epoch  2, batch    12 | loss: 8.1078310Losses:  7.720789432525635 1.783975601196289
CurrentTrain: epoch  2, batch    13 | loss: 7.7207894Losses:  7.655569076538086 1.9850189685821533
CurrentTrain: epoch  2, batch    14 | loss: 7.6555691Losses:  7.363029479980469 2.027297019958496
CurrentTrain: epoch  2, batch    15 | loss: 7.3630295Losses:  6.941915035247803 1.9011869430541992
CurrentTrain: epoch  2, batch    16 | loss: 6.9419150Losses:  6.352853775024414 1.5784997940063477
CurrentTrain: epoch  2, batch    17 | loss: 6.3528538Losses:  8.065256118774414 2.061185836791992
CurrentTrain: epoch  2, batch    18 | loss: 8.0652561Losses:  7.428759574890137 1.8816803693771362
CurrentTrain: epoch  2, batch    19 | loss: 7.4287596Losses:  7.369271278381348 1.9513309001922607
CurrentTrain: epoch  2, batch    20 | loss: 7.3692713Losses:  7.149947643280029 1.9127531051635742
CurrentTrain: epoch  2, batch    21 | loss: 7.1499476Losses:  6.946835041046143 1.914393424987793
CurrentTrain: epoch  2, batch    22 | loss: 6.9468350Losses:  7.618229389190674 1.9677777290344238
CurrentTrain: epoch  2, batch    23 | loss: 7.6182294Losses:  7.4475202560424805 1.8587646484375
CurrentTrain: epoch  2, batch    24 | loss: 7.4475203Losses:  7.145178318023682 1.9206291437149048
CurrentTrain: epoch  2, batch    25 | loss: 7.1451783Losses:  6.914348125457764 1.9069037437438965
CurrentTrain: epoch  2, batch    26 | loss: 6.9143481Losses:  6.936372756958008 1.6645101308822632
CurrentTrain: epoch  2, batch    27 | loss: 6.9363728Losses:  6.905079364776611 1.7937370538711548
CurrentTrain: epoch  2, batch    28 | loss: 6.9050794Losses:  7.54852819442749 2.1278672218322754
CurrentTrain: epoch  2, batch    29 | loss: 7.5485282Losses:  7.6578521728515625 2.005669355392456
CurrentTrain: epoch  2, batch    30 | loss: 7.6578522Losses:  7.213098049163818 2.1567835807800293
CurrentTrain: epoch  2, batch    31 | loss: 7.2130980Losses:  8.053305625915527 1.7717245817184448
CurrentTrain: epoch  2, batch    32 | loss: 8.0533056Losses:  6.642202377319336 1.8314955234527588
CurrentTrain: epoch  2, batch    33 | loss: 6.6422024Losses:  7.505664825439453 2.0084943771362305
CurrentTrain: epoch  2, batch    34 | loss: 7.5056648Losses:  7.4486918449401855 1.8881115913391113
CurrentTrain: epoch  2, batch    35 | loss: 7.4486918Losses:  6.965763568878174 1.9964971542358398
CurrentTrain: epoch  2, batch    36 | loss: 6.9657636Losses:  7.1636199951171875 1.7503025531768799
CurrentTrain: epoch  2, batch    37 | loss: 7.1636200Losses:  7.100020408630371 1.8545607328414917
CurrentTrain: epoch  2, batch    38 | loss: 7.1000204Losses:  6.461753845214844 1.743483304977417
CurrentTrain: epoch  2, batch    39 | loss: 6.4617538Losses:  7.122011184692383 1.6675937175750732
CurrentTrain: epoch  2, batch    40 | loss: 7.1220112Losses:  7.4411444664001465 1.4999374151229858
CurrentTrain: epoch  2, batch    41 | loss: 7.4411445Losses:  7.077902793884277 1.919668197631836
CurrentTrain: epoch  2, batch    42 | loss: 7.0779028Losses:  7.29815673828125 1.8898611068725586
CurrentTrain: epoch  2, batch    43 | loss: 7.2981567Losses:  7.098387241363525 2.007007122039795
CurrentTrain: epoch  2, batch    44 | loss: 7.0983872Losses:  7.441466331481934 1.9620708227157593
CurrentTrain: epoch  2, batch    45 | loss: 7.4414663Losses:  6.826598167419434 1.9634339809417725
CurrentTrain: epoch  2, batch    46 | loss: 6.8265982Losses:  7.5941267013549805 1.9595556259155273
CurrentTrain: epoch  2, batch    47 | loss: 7.5941267Losses:  6.751944065093994 1.8768091201782227
CurrentTrain: epoch  2, batch    48 | loss: 6.7519441Losses:  7.3924784660339355 1.9531484842300415
CurrentTrain: epoch  2, batch    49 | loss: 7.3924785Losses:  6.801380634307861 1.7664326429367065
CurrentTrain: epoch  2, batch    50 | loss: 6.8013806Losses:  6.799872398376465 1.9989769458770752
CurrentTrain: epoch  2, batch    51 | loss: 6.7998724Losses:  6.867944717407227 1.7680683135986328
CurrentTrain: epoch  2, batch    52 | loss: 6.8679447Losses:  7.078394412994385 2.053384304046631
CurrentTrain: epoch  2, batch    53 | loss: 7.0783944Losses:  6.862066268920898 1.9883482456207275
CurrentTrain: epoch  2, batch    54 | loss: 6.8620663Losses:  7.004034042358398 2.051551342010498
CurrentTrain: epoch  2, batch    55 | loss: 7.0040340Losses:  7.446051597595215 2.124511957168579
CurrentTrain: epoch  2, batch    56 | loss: 7.4460516Losses:  7.08989143371582 1.8421931266784668
CurrentTrain: epoch  2, batch    57 | loss: 7.0898914Losses:  6.802097320556641 1.7907845973968506
CurrentTrain: epoch  2, batch    58 | loss: 6.8020973Losses:  6.817715644836426 1.9347949028015137
CurrentTrain: epoch  2, batch    59 | loss: 6.8177156Losses:  6.586709022521973 1.923969030380249
CurrentTrain: epoch  2, batch    60 | loss: 6.5867090Losses:  6.830798149108887 1.8520060777664185
CurrentTrain: epoch  2, batch    61 | loss: 6.8307981Losses:  6.4793500900268555 1.6490541696548462
CurrentTrain: epoch  2, batch    62 | loss: 6.4793501Losses:  7.42903995513916 1.8998582363128662
CurrentTrain: epoch  3, batch     0 | loss: 7.4290400Losses:  7.856770992279053 1.805211067199707
CurrentTrain: epoch  3, batch     1 | loss: 7.8567710Losses:  6.9217610359191895 1.9991583824157715
CurrentTrain: epoch  3, batch     2 | loss: 6.9217610Losses:  7.088998317718506 1.746875286102295
CurrentTrain: epoch  3, batch     3 | loss: 7.0889983Losses:  7.079517841339111 1.8999133110046387
CurrentTrain: epoch  3, batch     4 | loss: 7.0795178Losses:  7.156682968139648 1.88447105884552
CurrentTrain: epoch  3, batch     5 | loss: 7.1566830Losses:  6.812797546386719 1.9110691547393799
CurrentTrain: epoch  3, batch     6 | loss: 6.8127975Losses:  6.94692325592041 2.0035617351531982
CurrentTrain: epoch  3, batch     7 | loss: 6.9469233Losses:  7.237068176269531 1.911317229270935
CurrentTrain: epoch  3, batch     8 | loss: 7.2370682Losses:  7.2972307205200195 2.0468645095825195
CurrentTrain: epoch  3, batch     9 | loss: 7.2972307Losses:  7.2286152839660645 2.1313395500183105
CurrentTrain: epoch  3, batch    10 | loss: 7.2286153Losses:  6.904603958129883 1.903895378112793
CurrentTrain: epoch  3, batch    11 | loss: 6.9046040Losses:  6.384324073791504 1.8587868213653564
CurrentTrain: epoch  3, batch    12 | loss: 6.3843241Losses:  7.167515754699707 2.123070478439331
CurrentTrain: epoch  3, batch    13 | loss: 7.1675158Losses:  6.73466682434082 1.967059850692749
CurrentTrain: epoch  3, batch    14 | loss: 6.7346668Losses:  6.687305450439453 1.995068907737732
CurrentTrain: epoch  3, batch    15 | loss: 6.6873055Losses:  7.0075178146362305 1.9497530460357666
CurrentTrain: epoch  3, batch    16 | loss: 7.0075178Losses:  7.034663200378418 1.7447726726531982
CurrentTrain: epoch  3, batch    17 | loss: 7.0346632Losses:  6.914463043212891 1.8667831420898438
CurrentTrain: epoch  3, batch    18 | loss: 6.9144630Losses:  6.828632831573486 1.8915961980819702
CurrentTrain: epoch  3, batch    19 | loss: 6.8286328Losses:  6.701827049255371 1.9003371000289917
CurrentTrain: epoch  3, batch    20 | loss: 6.7018270Losses:  6.822340965270996 2.0403199195861816
CurrentTrain: epoch  3, batch    21 | loss: 6.8223410Losses:  6.636643409729004 1.764728307723999
CurrentTrain: epoch  3, batch    22 | loss: 6.6366434Losses:  6.746415138244629 2.0331668853759766
CurrentTrain: epoch  3, batch    23 | loss: 6.7464151Losses:  6.381742477416992 1.6638143062591553
CurrentTrain: epoch  3, batch    24 | loss: 6.3817425Losses:  6.697848320007324 2.1155595779418945
CurrentTrain: epoch  3, batch    25 | loss: 6.6978483Losses:  6.938750267028809 2.128755807876587
CurrentTrain: epoch  3, batch    26 | loss: 6.9387503Losses:  6.7123212814331055 1.968672752380371
CurrentTrain: epoch  3, batch    27 | loss: 6.7123213Losses:  6.977300643920898 2.1588807106018066
CurrentTrain: epoch  3, batch    28 | loss: 6.9773006Losses:  7.1086554527282715 1.8780406713485718
CurrentTrain: epoch  3, batch    29 | loss: 7.1086555Losses:  7.534843921661377 2.0882325172424316
CurrentTrain: epoch  3, batch    30 | loss: 7.5348439Losses:  6.419075965881348 1.892007827758789
CurrentTrain: epoch  3, batch    31 | loss: 6.4190760Losses:  6.528183937072754 1.6639330387115479
CurrentTrain: epoch  3, batch    32 | loss: 6.5281839Losses:  6.66472864151001 1.8360449075698853
CurrentTrain: epoch  3, batch    33 | loss: 6.6647286Losses:  6.523360252380371 1.903191328048706
CurrentTrain: epoch  3, batch    34 | loss: 6.5233603Losses:  6.58892297744751 1.9709066152572632
CurrentTrain: epoch  3, batch    35 | loss: 6.5889230Losses:  7.357027530670166 1.9875768423080444
CurrentTrain: epoch  3, batch    36 | loss: 7.3570275Losses:  6.701379299163818 1.9209742546081543
CurrentTrain: epoch  3, batch    37 | loss: 6.7013793Losses:  6.515340805053711 2.010319709777832
CurrentTrain: epoch  3, batch    38 | loss: 6.5153408Losses:  7.097935199737549 1.685076355934143
CurrentTrain: epoch  3, batch    39 | loss: 7.0979352Losses:  6.67226505279541 2.032498598098755
CurrentTrain: epoch  3, batch    40 | loss: 6.6722651Losses:  6.184354305267334 1.7457367181777954
CurrentTrain: epoch  3, batch    41 | loss: 6.1843543Losses:  6.797619819641113 1.753516674041748
CurrentTrain: epoch  3, batch    42 | loss: 6.7976198Losses:  6.537425994873047 1.8195134401321411
CurrentTrain: epoch  3, batch    43 | loss: 6.5374260Losses:  6.966761589050293 2.04221248626709
CurrentTrain: epoch  3, batch    44 | loss: 6.9667616Losses:  6.720396518707275 1.8093403577804565
CurrentTrain: epoch  3, batch    45 | loss: 6.7203965Losses:  6.423373222351074 2.006121873855591
CurrentTrain: epoch  3, batch    46 | loss: 6.4233732Losses:  6.71018123626709 1.7495430707931519
CurrentTrain: epoch  3, batch    47 | loss: 6.7101812Losses:  6.157498836517334 1.580991268157959
CurrentTrain: epoch  3, batch    48 | loss: 6.1574988Losses:  6.321231842041016 1.6173713207244873
CurrentTrain: epoch  3, batch    49 | loss: 6.3212318Losses:  6.5491557121276855 2.0040173530578613
CurrentTrain: epoch  3, batch    50 | loss: 6.5491557Losses:  6.592812538146973 1.771688461303711
CurrentTrain: epoch  3, batch    51 | loss: 6.5928125Losses:  6.419159412384033 1.8749117851257324
CurrentTrain: epoch  3, batch    52 | loss: 6.4191594Losses:  7.139337539672852 1.9478904008865356
CurrentTrain: epoch  3, batch    53 | loss: 7.1393375Losses:  6.434030055999756 1.943713665008545
CurrentTrain: epoch  3, batch    54 | loss: 6.4340301Losses:  6.741998195648193 2.121677875518799
CurrentTrain: epoch  3, batch    55 | loss: 6.7419982Losses:  6.30790901184082 1.8497028350830078
CurrentTrain: epoch  3, batch    56 | loss: 6.3079090Losses:  6.74188232421875 1.863691806793213
CurrentTrain: epoch  3, batch    57 | loss: 6.7418823Losses:  6.796734809875488 1.8472957611083984
CurrentTrain: epoch  3, batch    58 | loss: 6.7967348Losses:  6.230075836181641 1.652367115020752
CurrentTrain: epoch  3, batch    59 | loss: 6.2300758Losses:  6.595651149749756 2.063736915588379
CurrentTrain: epoch  3, batch    60 | loss: 6.5956511Losses:  6.353038787841797 1.9290282726287842
CurrentTrain: epoch  3, batch    61 | loss: 6.3530388Losses:  5.8870978355407715 1.4510260820388794
CurrentTrain: epoch  3, batch    62 | loss: 5.8870978Losses:  5.936740875244141 1.5445854663848877
CurrentTrain: epoch  4, batch     0 | loss: 5.9367409Losses:  6.262340068817139 1.7683054208755493
CurrentTrain: epoch  4, batch     1 | loss: 6.2623401Losses:  6.455382347106934 1.9128150939941406
CurrentTrain: epoch  4, batch     2 | loss: 6.4553823Losses:  6.080427646636963 1.7179447412490845
CurrentTrain: epoch  4, batch     3 | loss: 6.0804276Losses:  6.657110214233398 2.128706932067871
CurrentTrain: epoch  4, batch     4 | loss: 6.6571102Losses:  6.777443885803223 2.131054639816284
CurrentTrain: epoch  4, batch     5 | loss: 6.7774439Losses:  6.353972434997559 1.851507306098938
CurrentTrain: epoch  4, batch     6 | loss: 6.3539724Losses:  6.132174491882324 1.7070841789245605
CurrentTrain: epoch  4, batch     7 | loss: 6.1321745Losses:  6.526014804840088 2.1152257919311523
CurrentTrain: epoch  4, batch     8 | loss: 6.5260148Losses:  6.313532829284668 1.8800184726715088
CurrentTrain: epoch  4, batch     9 | loss: 6.3135328Losses:  6.322489261627197 1.834077000617981
CurrentTrain: epoch  4, batch    10 | loss: 6.3224893Losses:  6.750209808349609 1.8754425048828125
CurrentTrain: epoch  4, batch    11 | loss: 6.7502098Losses:  6.425768852233887 1.8826638460159302
CurrentTrain: epoch  4, batch    12 | loss: 6.4257689Losses:  6.6925482749938965 1.996751308441162
CurrentTrain: epoch  4, batch    13 | loss: 6.6925483Losses:  5.919069290161133 1.6906821727752686
CurrentTrain: epoch  4, batch    14 | loss: 5.9190693Losses:  6.222261428833008 1.7950646877288818
CurrentTrain: epoch  4, batch    15 | loss: 6.2222614Losses:  6.305578708648682 1.8451638221740723
CurrentTrain: epoch  4, batch    16 | loss: 6.3055787Losses:  6.170517444610596 1.793402075767517
CurrentTrain: epoch  4, batch    17 | loss: 6.1705174Losses:  6.393485069274902 1.9868603944778442
CurrentTrain: epoch  4, batch    18 | loss: 6.3934851Losses:  6.391160011291504 1.9693167209625244
CurrentTrain: epoch  4, batch    19 | loss: 6.3911600Losses:  6.62667989730835 1.7731575965881348
CurrentTrain: epoch  4, batch    20 | loss: 6.6266799Losses:  6.3153557777404785 1.664793610572815
CurrentTrain: epoch  4, batch    21 | loss: 6.3153558Losses:  6.048146724700928 1.7844443321228027
CurrentTrain: epoch  4, batch    22 | loss: 6.0481467Losses:  5.776537895202637 1.5328795909881592
CurrentTrain: epoch  4, batch    23 | loss: 5.7765379Losses:  6.120265960693359 1.572780728340149
CurrentTrain: epoch  4, batch    24 | loss: 6.1202660Losses:  6.457749366760254 2.0420637130737305
CurrentTrain: epoch  4, batch    25 | loss: 6.4577494Losses:  6.204476833343506 1.9215922355651855
CurrentTrain: epoch  4, batch    26 | loss: 6.2044768Losses:  6.106625080108643 1.8173900842666626
CurrentTrain: epoch  4, batch    27 | loss: 6.1066251Losses:  6.236149311065674 1.8177319765090942
CurrentTrain: epoch  4, batch    28 | loss: 6.2361493Losses:  6.198351860046387 1.8077418804168701
CurrentTrain: epoch  4, batch    29 | loss: 6.1983519Losses:  6.137991905212402 1.8727271556854248
CurrentTrain: epoch  4, batch    30 | loss: 6.1379919Losses:  6.251092910766602 1.8346765041351318
CurrentTrain: epoch  4, batch    31 | loss: 6.2510929Losses:  6.411288261413574 1.901796579360962
CurrentTrain: epoch  4, batch    32 | loss: 6.4112883Losses:  6.4582839012146 2.0023932456970215
CurrentTrain: epoch  4, batch    33 | loss: 6.4582839Losses:  6.205041885375977 1.8689019680023193
CurrentTrain: epoch  4, batch    34 | loss: 6.2050419Losses:  6.573591232299805 2.0468814373016357
CurrentTrain: epoch  4, batch    35 | loss: 6.5735912Losses:  6.136496543884277 1.7181986570358276
CurrentTrain: epoch  4, batch    36 | loss: 6.1364965Losses:  6.78099250793457 2.0139122009277344
CurrentTrain: epoch  4, batch    37 | loss: 6.7809925Losses:  6.2864484786987305 1.832977056503296
CurrentTrain: epoch  4, batch    38 | loss: 6.2864485Losses:  6.179660797119141 1.8779716491699219
CurrentTrain: epoch  4, batch    39 | loss: 6.1796608Losses:  6.090456008911133 1.8179552555084229
CurrentTrain: epoch  4, batch    40 | loss: 6.0904560Losses:  6.171661376953125 1.9308650493621826
CurrentTrain: epoch  4, batch    41 | loss: 6.1716614Losses:  6.726455211639404 1.8871488571166992
CurrentTrain: epoch  4, batch    42 | loss: 6.7264552Losses:  6.328505516052246 1.6044586896896362
CurrentTrain: epoch  4, batch    43 | loss: 6.3285055Losses:  5.850447654724121 1.667506456375122
CurrentTrain: epoch  4, batch    44 | loss: 5.8504477Losses:  6.711187839508057 1.715511679649353
CurrentTrain: epoch  4, batch    45 | loss: 6.7111878Losses:  6.44743537902832 1.8993256092071533
CurrentTrain: epoch  4, batch    46 | loss: 6.4474354Losses:  6.363856315612793 1.9160797595977783
CurrentTrain: epoch  4, batch    47 | loss: 6.3638563Losses:  6.357919692993164 2.03743839263916
CurrentTrain: epoch  4, batch    48 | loss: 6.3579197Losses:  6.285356044769287 1.9348559379577637
CurrentTrain: epoch  4, batch    49 | loss: 6.2853560Losses:  6.571635723114014 2.1181206703186035
CurrentTrain: epoch  4, batch    50 | loss: 6.5716357Losses:  6.17237663269043 1.8007917404174805
CurrentTrain: epoch  4, batch    51 | loss: 6.1723766Losses:  5.977936744689941 1.691438913345337
CurrentTrain: epoch  4, batch    52 | loss: 5.9779367Losses:  6.529983043670654 1.874402642250061
CurrentTrain: epoch  4, batch    53 | loss: 6.5299830Losses:  6.1665754318237305 1.757786512374878
CurrentTrain: epoch  4, batch    54 | loss: 6.1665754Losses:  6.231439590454102 1.9340580701828003
CurrentTrain: epoch  4, batch    55 | loss: 6.2314396Losses:  6.077754974365234 1.7460086345672607
CurrentTrain: epoch  4, batch    56 | loss: 6.0777550Losses:  6.463813781738281 2.11885929107666
CurrentTrain: epoch  4, batch    57 | loss: 6.4638138Losses:  6.1604323387146 1.8599224090576172
CurrentTrain: epoch  4, batch    58 | loss: 6.1604323Losses:  6.1699628829956055 1.8798120021820068
CurrentTrain: epoch  4, batch    59 | loss: 6.1699629Losses:  6.136468887329102 1.8587706089019775
CurrentTrain: epoch  4, batch    60 | loss: 6.1364689Losses:  6.338703155517578 1.945694923400879
CurrentTrain: epoch  4, batch    61 | loss: 6.3387032Losses:  5.813983917236328 1.4872349500656128
CurrentTrain: epoch  4, batch    62 | loss: 5.8139839Losses:  6.513975143432617 2.088848114013672
CurrentTrain: epoch  5, batch     0 | loss: 6.5139751Losses:  6.299875259399414 1.982900857925415
CurrentTrain: epoch  5, batch     1 | loss: 6.2998753Losses:  5.9988603591918945 1.7092831134796143
CurrentTrain: epoch  5, batch     2 | loss: 5.9988604Losses:  6.141266822814941 1.8599092960357666
CurrentTrain: epoch  5, batch     3 | loss: 6.1412668Losses:  6.206720352172852 1.8158152103424072
CurrentTrain: epoch  5, batch     4 | loss: 6.2067204Losses:  6.235012054443359 1.8655047416687012
CurrentTrain: epoch  5, batch     5 | loss: 6.2350121Losses:  6.299488067626953 1.9483935832977295
CurrentTrain: epoch  5, batch     6 | loss: 6.2994881Losses:  6.183954238891602 1.975921630859375
CurrentTrain: epoch  5, batch     7 | loss: 6.1839542Losses:  6.316330432891846 1.967441439628601
CurrentTrain: epoch  5, batch     8 | loss: 6.3163304Losses:  5.931252479553223 1.6940171718597412
CurrentTrain: epoch  5, batch     9 | loss: 5.9312525Losses:  6.608859062194824 2.0138485431671143
CurrentTrain: epoch  5, batch    10 | loss: 6.6088591Losses:  6.330935955047607 2.0453810691833496
CurrentTrain: epoch  5, batch    11 | loss: 6.3309360Losses:  6.108575820922852 1.8937041759490967
CurrentTrain: epoch  5, batch    12 | loss: 6.1085758Losses:  6.058676719665527 1.8285224437713623
CurrentTrain: epoch  5, batch    13 | loss: 6.0586767Losses:  6.307979106903076 2.141083240509033
CurrentTrain: epoch  5, batch    14 | loss: 6.3079791Losses:  6.097900867462158 1.8727039098739624
CurrentTrain: epoch  5, batch    15 | loss: 6.0979009Losses:  6.401391983032227 2.09159517288208
CurrentTrain: epoch  5, batch    16 | loss: 6.4013920Losses:  6.210906028747559 2.0175676345825195
CurrentTrain: epoch  5, batch    17 | loss: 6.2109060Losses:  6.227911949157715 1.956686019897461
CurrentTrain: epoch  5, batch    18 | loss: 6.2279119Losses:  6.128261089324951 1.8860478401184082
CurrentTrain: epoch  5, batch    19 | loss: 6.1282611Losses:  6.09248161315918 1.9049614667892456
CurrentTrain: epoch  5, batch    20 | loss: 6.0924816Losses:  6.117338180541992 1.8601787090301514
CurrentTrain: epoch  5, batch    21 | loss: 6.1173382Losses:  6.044260025024414 1.7730417251586914
CurrentTrain: epoch  5, batch    22 | loss: 6.0442600Losses:  6.17122745513916 1.8750067949295044
CurrentTrain: epoch  5, batch    23 | loss: 6.1712275Losses:  5.992870807647705 1.763999581336975
CurrentTrain: epoch  5, batch    24 | loss: 5.9928708Losses:  6.207035064697266 1.9948394298553467
CurrentTrain: epoch  5, batch    25 | loss: 6.2070351Losses:  6.217975616455078 1.952214241027832
CurrentTrain: epoch  5, batch    26 | loss: 6.2179756Losses:  6.0540900230407715 1.8324469327926636
CurrentTrain: epoch  5, batch    27 | loss: 6.0540900Losses:  6.224791526794434 2.0434815883636475
CurrentTrain: epoch  5, batch    28 | loss: 6.2247915Losses:  6.164121627807617 1.9878313541412354
CurrentTrain: epoch  5, batch    29 | loss: 6.1641216Losses:  5.900179862976074 1.7308123111724854
CurrentTrain: epoch  5, batch    30 | loss: 5.9001799Losses:  5.9849443435668945 1.8143870830535889
CurrentTrain: epoch  5, batch    31 | loss: 5.9849443Losses:  6.086416244506836 1.8276557922363281
CurrentTrain: epoch  5, batch    32 | loss: 6.0864162Losses:  5.702802658081055 1.5886386632919312
CurrentTrain: epoch  5, batch    33 | loss: 5.7028027Losses:  6.089211463928223 1.8566513061523438
CurrentTrain: epoch  5, batch    34 | loss: 6.0892115Losses:  5.881636619567871 1.73995041847229
CurrentTrain: epoch  5, batch    35 | loss: 5.8816366Losses:  5.906307220458984 1.7312605381011963
CurrentTrain: epoch  5, batch    36 | loss: 5.9063072Losses:  5.947727203369141 1.8169865608215332
CurrentTrain: epoch  5, batch    37 | loss: 5.9477272Losses:  6.005985260009766 1.8154897689819336
CurrentTrain: epoch  5, batch    38 | loss: 6.0059853Losses:  6.224289894104004 1.968675136566162
CurrentTrain: epoch  5, batch    39 | loss: 6.2242899Losses:  6.222259044647217 2.1033692359924316
CurrentTrain: epoch  5, batch    40 | loss: 6.2222590Losses:  6.013338088989258 1.8273937702178955
CurrentTrain: epoch  5, batch    41 | loss: 6.0133381Losses:  6.309241771697998 2.000946044921875
CurrentTrain: epoch  5, batch    42 | loss: 6.3092418Losses:  6.292210102081299 1.8193756341934204
CurrentTrain: epoch  5, batch    43 | loss: 6.2922101Losses:  6.049311637878418 1.8854742050170898
CurrentTrain: epoch  5, batch    44 | loss: 6.0493116Losses:  6.002300262451172 1.8718688488006592
CurrentTrain: epoch  5, batch    45 | loss: 6.0023003Losses:  6.095152854919434 1.9457406997680664
CurrentTrain: epoch  5, batch    46 | loss: 6.0951529Losses:  6.104931831359863 1.9555943012237549
CurrentTrain: epoch  5, batch    47 | loss: 6.1049318Losses:  6.18815803527832 1.9904208183288574
CurrentTrain: epoch  5, batch    48 | loss: 6.1881580Losses:  6.130157470703125 1.9639137983322144
CurrentTrain: epoch  5, batch    49 | loss: 6.1301575Losses:  6.058941841125488 1.9450013637542725
CurrentTrain: epoch  5, batch    50 | loss: 6.0589418Losses:  5.8970723152160645 1.760385513305664
CurrentTrain: epoch  5, batch    51 | loss: 5.8970723Losses:  6.280232906341553 2.067683696746826
CurrentTrain: epoch  5, batch    52 | loss: 6.2802329Losses:  6.0805182456970215 1.9039463996887207
CurrentTrain: epoch  5, batch    53 | loss: 6.0805182Losses:  6.1945390701293945 2.076373815536499
CurrentTrain: epoch  5, batch    54 | loss: 6.1945391Losses:  6.195619583129883 1.9572980403900146
CurrentTrain: epoch  5, batch    55 | loss: 6.1956196Losses:  6.2274627685546875 2.069734811782837
CurrentTrain: epoch  5, batch    56 | loss: 6.2274628Losses:  6.041012763977051 1.7589585781097412
CurrentTrain: epoch  5, batch    57 | loss: 6.0410128Losses:  6.17525053024292 1.8078550100326538
CurrentTrain: epoch  5, batch    58 | loss: 6.1752505Losses:  5.971752166748047 1.7517402172088623
CurrentTrain: epoch  5, batch    59 | loss: 5.9717522Losses:  5.971064567565918 1.85005784034729
CurrentTrain: epoch  5, batch    60 | loss: 5.9710646Losses:  5.919588088989258 1.7992866039276123
CurrentTrain: epoch  5, batch    61 | loss: 5.9195881Losses:  5.883039474487305 1.6546258926391602
CurrentTrain: epoch  5, batch    62 | loss: 5.8830395Losses:  6.065134525299072 1.8482718467712402
CurrentTrain: epoch  6, batch     0 | loss: 6.0651345Losses:  6.055058479309082 1.9491355419158936
CurrentTrain: epoch  6, batch     1 | loss: 6.0550585Losses:  5.890270709991455 1.8052479028701782
CurrentTrain: epoch  6, batch     2 | loss: 5.8902707Losses:  6.477765083312988 2.1444149017333984
CurrentTrain: epoch  6, batch     3 | loss: 6.4777651Losses:  5.820769309997559 1.6708488464355469
CurrentTrain: epoch  6, batch     4 | loss: 5.8207693Losses:  6.147409439086914 1.9935038089752197
CurrentTrain: epoch  6, batch     5 | loss: 6.1474094Losses:  6.158333778381348 2.0006606578826904
CurrentTrain: epoch  6, batch     6 | loss: 6.1583338Losses:  6.015596389770508 1.8506133556365967
CurrentTrain: epoch  6, batch     7 | loss: 6.0155964Losses:  6.108617782592773 2.004913568496704
CurrentTrain: epoch  6, batch     8 | loss: 6.1086178Losses:  5.7757887840271 1.6316369771957397
CurrentTrain: epoch  6, batch     9 | loss: 5.7757888Losses:  6.127418518066406 1.9838954210281372
CurrentTrain: epoch  6, batch    10 | loss: 6.1274185Losses:  5.8497819900512695 1.672255039215088
CurrentTrain: epoch  6, batch    11 | loss: 5.8497820Losses:  6.281014442443848 2.0855185985565186
CurrentTrain: epoch  6, batch    12 | loss: 6.2810144Losses:  5.8858232498168945 1.768925428390503
CurrentTrain: epoch  6, batch    13 | loss: 5.8858232Losses:  6.202637672424316 2.048004388809204
CurrentTrain: epoch  6, batch    14 | loss: 6.2026377Losses:  6.122873783111572 1.97416353225708
CurrentTrain: epoch  6, batch    15 | loss: 6.1228738Losses:  6.146661281585693 2.006542205810547
CurrentTrain: epoch  6, batch    16 | loss: 6.1466613Losses:  6.11629581451416 1.987518310546875
CurrentTrain: epoch  6, batch    17 | loss: 6.1162958Losses:  5.513725280761719 1.3585689067840576
CurrentTrain: epoch  6, batch    18 | loss: 5.5137253Losses:  6.102921485900879 1.9918577671051025
CurrentTrain: epoch  6, batch    19 | loss: 6.1029215Losses:  6.1634626388549805 2.0418436527252197
CurrentTrain: epoch  6, batch    20 | loss: 6.1634626Losses:  6.0666303634643555 1.8763998746871948
CurrentTrain: epoch  6, batch    21 | loss: 6.0666304Losses:  5.915982723236084 1.735912799835205
CurrentTrain: epoch  6, batch    22 | loss: 5.9159827Losses:  5.988516330718994 1.557446002960205
CurrentTrain: epoch  6, batch    23 | loss: 5.9885163Losses:  5.9504780769348145 1.7881702184677124
CurrentTrain: epoch  6, batch    24 | loss: 5.9504781Losses:  6.200963497161865 1.994676947593689
CurrentTrain: epoch  6, batch    25 | loss: 6.2009635Losses:  6.148121356964111 1.991143822669983
CurrentTrain: epoch  6, batch    26 | loss: 6.1481214Losses:  5.887491226196289 1.7909247875213623
CurrentTrain: epoch  6, batch    27 | loss: 5.8874912Losses:  6.017490386962891 1.9352567195892334
CurrentTrain: epoch  6, batch    28 | loss: 6.0174904Losses:  6.10825252532959 1.9775168895721436
CurrentTrain: epoch  6, batch    29 | loss: 6.1082525Losses:  6.07090950012207 1.980400800704956
CurrentTrain: epoch  6, batch    30 | loss: 6.0709095Losses:  5.632997512817383 1.5965107679367065
CurrentTrain: epoch  6, batch    31 | loss: 5.6329975Losses:  5.9487481117248535 1.7899274826049805
CurrentTrain: epoch  6, batch    32 | loss: 5.9487481Losses:  6.11933708190918 1.9811500310897827
CurrentTrain: epoch  6, batch    33 | loss: 6.1193371Losses:  5.9575724601745605 1.8219388723373413
CurrentTrain: epoch  6, batch    34 | loss: 5.9575725Losses:  5.780582427978516 1.6174936294555664
CurrentTrain: epoch  6, batch    35 | loss: 5.7805824Losses:  5.640478610992432 1.4631118774414062
CurrentTrain: epoch  6, batch    36 | loss: 5.6404786Losses:  6.145758152008057 2.0102643966674805
CurrentTrain: epoch  6, batch    37 | loss: 6.1457582Losses:  6.247448444366455 2.0761609077453613
CurrentTrain: epoch  6, batch    38 | loss: 6.2474484Losses:  5.85431432723999 1.7788329124450684
CurrentTrain: epoch  6, batch    39 | loss: 5.8543143Losses:  6.186438083648682 2.0332674980163574
CurrentTrain: epoch  6, batch    40 | loss: 6.1864381Losses:  6.2079291343688965 2.098128318786621
CurrentTrain: epoch  6, batch    41 | loss: 6.2079291Losses:  6.105841159820557 2.0192818641662598
CurrentTrain: epoch  6, batch    42 | loss: 6.1058412Losses:  5.818789958953857 1.7738795280456543
CurrentTrain: epoch  6, batch    43 | loss: 5.8187900Losses:  5.921963214874268 1.838152289390564
CurrentTrain: epoch  6, batch    44 | loss: 5.9219632Losses:  6.085131645202637 1.9471356868743896
CurrentTrain: epoch  6, batch    45 | loss: 6.0851316Losses:  5.901684284210205 1.79597806930542
CurrentTrain: epoch  6, batch    46 | loss: 5.9016843Losses:  5.880210876464844 1.7525532245635986
CurrentTrain: epoch  6, batch    47 | loss: 5.8802109Losses:  6.2006425857543945 2.104717969894409
CurrentTrain: epoch  6, batch    48 | loss: 6.2006426Losses:  5.768393039703369 1.6700530052185059
CurrentTrain: epoch  6, batch    49 | loss: 5.7683930Losses:  6.183605194091797 2.0622119903564453
CurrentTrain: epoch  6, batch    50 | loss: 6.1836052Losses:  6.112807273864746 2.024118423461914
CurrentTrain: epoch  6, batch    51 | loss: 6.1128073Losses:  6.086396217346191 1.985154151916504
CurrentTrain: epoch  6, batch    52 | loss: 6.0863962Losses:  5.508411407470703 1.3904327154159546
CurrentTrain: epoch  6, batch    53 | loss: 5.5084114Losses:  6.004807472229004 1.9202252626419067
CurrentTrain: epoch  6, batch    54 | loss: 6.0048075Losses:  5.8322296142578125 1.7699956893920898
CurrentTrain: epoch  6, batch    55 | loss: 5.8322296Losses:  5.7473368644714355 1.6056256294250488
CurrentTrain: epoch  6, batch    56 | loss: 5.7473369Losses:  6.065404891967773 2.0082356929779053
CurrentTrain: epoch  6, batch    57 | loss: 6.0654049Losses:  5.977472305297852 1.8897629976272583
CurrentTrain: epoch  6, batch    58 | loss: 5.9774723Losses:  6.054187774658203 1.9646713733673096
CurrentTrain: epoch  6, batch    59 | loss: 6.0541878Losses:  5.734964847564697 1.603052020072937
CurrentTrain: epoch  6, batch    60 | loss: 5.7349648Losses:  5.978246688842773 1.8373322486877441
CurrentTrain: epoch  6, batch    61 | loss: 5.9782467Losses:  5.6279473304748535 1.559342861175537
CurrentTrain: epoch  6, batch    62 | loss: 5.6279473Losses:  5.879626750946045 1.7672823667526245
CurrentTrain: epoch  7, batch     0 | loss: 5.8796268Losses:  5.826096534729004 1.7256393432617188
CurrentTrain: epoch  7, batch     1 | loss: 5.8260965Losses:  5.93301248550415 1.8406015634536743
CurrentTrain: epoch  7, batch     2 | loss: 5.9330125Losses:  6.109071731567383 2.0266170501708984
CurrentTrain: epoch  7, batch     3 | loss: 6.1090717Losses:  5.771002769470215 1.6773135662078857
CurrentTrain: epoch  7, batch     4 | loss: 5.7710028Losses:  6.025472164154053 1.9646577835083008
CurrentTrain: epoch  7, batch     5 | loss: 6.0254722Losses:  6.040823459625244 1.966959834098816
CurrentTrain: epoch  7, batch     6 | loss: 6.0408235Losses:  6.1579508781433105 2.0080742835998535
CurrentTrain: epoch  7, batch     7 | loss: 6.1579509Losses:  5.736242294311523 1.719562292098999
CurrentTrain: epoch  7, batch     8 | loss: 5.7362423Losses:  5.841519355773926 1.7780429124832153
CurrentTrain: epoch  7, batch     9 | loss: 5.8415194Losses:  5.917194843292236 1.8711037635803223
CurrentTrain: epoch  7, batch    10 | loss: 5.9171948Losses:  5.801783561706543 1.7788608074188232
CurrentTrain: epoch  7, batch    11 | loss: 5.8017836Losses:  5.637251853942871 1.6026685237884521
CurrentTrain: epoch  7, batch    12 | loss: 5.6372519Losses:  5.861018180847168 1.7797162532806396
CurrentTrain: epoch  7, batch    13 | loss: 5.8610182Losses:  5.805806636810303 1.7717832326889038
CurrentTrain: epoch  7, batch    14 | loss: 5.8058066Losses:  5.863834381103516 1.8567131757736206
CurrentTrain: epoch  7, batch    15 | loss: 5.8638344Losses:  5.920531272888184 1.822616696357727
CurrentTrain: epoch  7, batch    16 | loss: 5.9205313Losses:  6.169553279876709 1.842686653137207
CurrentTrain: epoch  7, batch    17 | loss: 6.1695533Losses:  6.0830583572387695 2.019019365310669
CurrentTrain: epoch  7, batch    18 | loss: 6.0830584Losses:  6.0844035148620605 1.9800549745559692
CurrentTrain: epoch  7, batch    19 | loss: 6.0844035Losses:  5.853431701660156 1.754439353942871
CurrentTrain: epoch  7, batch    20 | loss: 5.8534317Losses:  5.814550876617432 1.7816624641418457
CurrentTrain: epoch  7, batch    21 | loss: 5.8145509Losses:  5.757751941680908 1.6890438795089722
CurrentTrain: epoch  7, batch    22 | loss: 5.7577519Losses:  5.685253143310547 1.6133091449737549
CurrentTrain: epoch  7, batch    23 | loss: 5.6852531Losses:  5.875029563903809 1.8299072980880737
CurrentTrain: epoch  7, batch    24 | loss: 5.8750296Losses:  5.792967796325684 1.6236201524734497
CurrentTrain: epoch  7, batch    25 | loss: 5.7929678Losses:  5.815385341644287 1.7832269668579102
CurrentTrain: epoch  7, batch    26 | loss: 5.8153853Losses:  6.003852367401123 1.944799780845642
CurrentTrain: epoch  7, batch    27 | loss: 6.0038524Losses:  5.9163618087768555 1.848891019821167
CurrentTrain: epoch  7, batch    28 | loss: 5.9163618Losses:  5.829435348510742 1.7557523250579834
CurrentTrain: epoch  7, batch    29 | loss: 5.8294353Losses:  5.998331069946289 1.9068297147750854
CurrentTrain: epoch  7, batch    30 | loss: 5.9983311Losses:  5.930816650390625 1.8595330715179443
CurrentTrain: epoch  7, batch    31 | loss: 5.9308167Losses:  5.6835479736328125 1.6121695041656494
CurrentTrain: epoch  7, batch    32 | loss: 5.6835480Losses:  5.932159900665283 1.7611080408096313
CurrentTrain: epoch  7, batch    33 | loss: 5.9321599Losses:  5.758859634399414 1.6512212753295898
CurrentTrain: epoch  7, batch    34 | loss: 5.7588596Losses:  5.974481582641602 1.9142954349517822
CurrentTrain: epoch  7, batch    35 | loss: 5.9744816Losses:  5.969215393066406 1.8393046855926514
CurrentTrain: epoch  7, batch    36 | loss: 5.9692154Losses:  5.793118476867676 1.7609822750091553
CurrentTrain: epoch  7, batch    37 | loss: 5.7931185Losses:  6.0813422203063965 2.0165042877197266
CurrentTrain: epoch  7, batch    38 | loss: 6.0813422Losses:  5.785858631134033 1.7599921226501465
CurrentTrain: epoch  7, batch    39 | loss: 5.7858586Losses:  6.012294769287109 1.960143804550171
CurrentTrain: epoch  7, batch    40 | loss: 6.0122948Losses:  5.750321865081787 1.733086109161377
CurrentTrain: epoch  7, batch    41 | loss: 5.7503219Losses:  5.866207122802734 1.830346703529358
CurrentTrain: epoch  7, batch    42 | loss: 5.8662071Losses:  5.861663341522217 1.8055071830749512
CurrentTrain: epoch  7, batch    43 | loss: 5.8616633Losses:  5.951190948486328 1.9105520248413086
CurrentTrain: epoch  7, batch    44 | loss: 5.9511909Losses:  5.958580493927002 1.9630682468414307
CurrentTrain: epoch  7, batch    45 | loss: 5.9585805Losses:  5.81594181060791 1.7635056972503662
CurrentTrain: epoch  7, batch    46 | loss: 5.8159418Losses:  5.94806432723999 1.9023314714431763
CurrentTrain: epoch  7, batch    47 | loss: 5.9480643Losses:  5.938544273376465 1.9139889478683472
CurrentTrain: epoch  7, batch    48 | loss: 5.9385443Losses:  5.987353324890137 1.9539034366607666
CurrentTrain: epoch  7, batch    49 | loss: 5.9873533Losses:  5.65216064453125 1.6818671226501465
CurrentTrain: epoch  7, batch    50 | loss: 5.6521606Losses:  6.008781909942627 1.9703556299209595
CurrentTrain: epoch  7, batch    51 | loss: 6.0087819Losses:  5.689957618713379 1.591322898864746
CurrentTrain: epoch  7, batch    52 | loss: 5.6899576Losses:  5.965965270996094 1.8897967338562012
CurrentTrain: epoch  7, batch    53 | loss: 5.9659653Losses:  5.873574733734131 1.799522876739502
CurrentTrain: epoch  7, batch    54 | loss: 5.8735747Losses:  5.769388198852539 1.7279713153839111
CurrentTrain: epoch  7, batch    55 | loss: 5.7693882Losses:  5.678241729736328 1.6180226802825928
CurrentTrain: epoch  7, batch    56 | loss: 5.6782417Losses:  5.9996490478515625 1.976893663406372
CurrentTrain: epoch  7, batch    57 | loss: 5.9996490Losses:  6.015331268310547 1.9949266910552979
CurrentTrain: epoch  7, batch    58 | loss: 6.0153313Losses:  5.6729631423950195 1.6136988401412964
CurrentTrain: epoch  7, batch    59 | loss: 5.6729631Losses:  6.070030212402344 2.004973888397217
CurrentTrain: epoch  7, batch    60 | loss: 6.0700302Losses:  5.89098596572876 1.8406767845153809
CurrentTrain: epoch  7, batch    61 | loss: 5.8909860Losses:  5.684309959411621 1.605987787246704
CurrentTrain: epoch  7, batch    62 | loss: 5.6843100Losses:  6.100086212158203 2.06217885017395
CurrentTrain: epoch  8, batch     0 | loss: 6.1000862Losses:  5.951777935028076 1.8855990171432495
CurrentTrain: epoch  8, batch     1 | loss: 5.9517779Losses:  5.902956008911133 1.8597321510314941
CurrentTrain: epoch  8, batch     2 | loss: 5.9029560Losses:  5.76359224319458 1.7034345865249634
CurrentTrain: epoch  8, batch     3 | loss: 5.7635922Losses:  5.746676921844482 1.744512915611267
CurrentTrain: epoch  8, batch     4 | loss: 5.7466769Losses:  5.923933982849121 1.8825292587280273
CurrentTrain: epoch  8, batch     5 | loss: 5.9239340Losses:  5.972918510437012 1.9322869777679443
CurrentTrain: epoch  8, batch     6 | loss: 5.9729185Losses:  6.0101141929626465 1.9462484121322632
CurrentTrain: epoch  8, batch     7 | loss: 6.0101142Losses:  5.953500747680664 1.91461181640625
CurrentTrain: epoch  8, batch     8 | loss: 5.9535007Losses:  6.005650997161865 1.9547628164291382
CurrentTrain: epoch  8, batch     9 | loss: 6.0056510Losses:  5.888910293579102 1.8800804615020752
CurrentTrain: epoch  8, batch    10 | loss: 5.8889103Losses:  5.857138633728027 1.8287849426269531
CurrentTrain: epoch  8, batch    11 | loss: 5.8571386Losses:  5.955586910247803 1.8911458253860474
CurrentTrain: epoch  8, batch    12 | loss: 5.9555869Losses:  5.870723724365234 1.83134925365448
CurrentTrain: epoch  8, batch    13 | loss: 5.8707237Losses:  5.847472190856934 1.8290822505950928
CurrentTrain: epoch  8, batch    14 | loss: 5.8474722Losses:  5.755903244018555 1.7357640266418457
CurrentTrain: epoch  8, batch    15 | loss: 5.7559032Losses:  5.834825038909912 1.8417637348175049
CurrentTrain: epoch  8, batch    16 | loss: 5.8348250Losses:  5.987713813781738 1.957693099975586
CurrentTrain: epoch  8, batch    17 | loss: 5.9877138Losses:  5.9403228759765625 1.902497410774231
CurrentTrain: epoch  8, batch    18 | loss: 5.9403229Losses:  6.026541233062744 1.9208866357803345
CurrentTrain: epoch  8, batch    19 | loss: 6.0265412Losses:  5.710438251495361 1.7602900266647339
CurrentTrain: epoch  8, batch    20 | loss: 5.7104383Losses:  5.886566638946533 1.8665423393249512
CurrentTrain: epoch  8, batch    21 | loss: 5.8865666Losses:  5.601723670959473 1.583404302597046
CurrentTrain: epoch  8, batch    22 | loss: 5.6017237Losses:  6.123231887817383 2.085660219192505
CurrentTrain: epoch  8, batch    23 | loss: 6.1232319Losses:  6.170691967010498 2.1186676025390625
CurrentTrain: epoch  8, batch    24 | loss: 6.1706920Losses:  5.690633773803711 1.6680705547332764
CurrentTrain: epoch  8, batch    25 | loss: 5.6906338Losses:  5.766488075256348 1.7243351936340332
CurrentTrain: epoch  8, batch    26 | loss: 5.7664881Losses:  5.911583423614502 1.8281322717666626
CurrentTrain: epoch  8, batch    27 | loss: 5.9115834Losses:  6.093481063842773 2.075822353363037
CurrentTrain: epoch  8, batch    28 | loss: 6.0934811Losses:  5.943429470062256 1.9086785316467285
CurrentTrain: epoch  8, batch    29 | loss: 5.9434295Losses:  5.830306053161621 1.8071377277374268
CurrentTrain: epoch  8, batch    30 | loss: 5.8303061Losses:  5.76579475402832 1.7384583950042725
CurrentTrain: epoch  8, batch    31 | loss: 5.7657948Losses:  5.677124977111816 1.630049705505371
CurrentTrain: epoch  8, batch    32 | loss: 5.6771250Losses:  5.893983840942383 1.8241606950759888
CurrentTrain: epoch  8, batch    33 | loss: 5.8939838Losses:  6.151468276977539 2.113802671432495
CurrentTrain: epoch  8, batch    34 | loss: 6.1514683Losses:  5.848505020141602 1.8248273134231567
CurrentTrain: epoch  8, batch    35 | loss: 5.8485050Losses:  5.820429801940918 1.7751054763793945
CurrentTrain: epoch  8, batch    36 | loss: 5.8204298Losses:  5.869712829589844 1.7950034141540527
CurrentTrain: epoch  8, batch    37 | loss: 5.8697128Losses:  5.72047233581543 1.730692982673645
CurrentTrain: epoch  8, batch    38 | loss: 5.7204723Losses:  5.94804573059082 1.8780298233032227
CurrentTrain: epoch  8, batch    39 | loss: 5.9480457Losses:  6.06790018081665 1.9762682914733887
CurrentTrain: epoch  8, batch    40 | loss: 6.0679002Losses:  6.05235481262207 2.01648211479187
CurrentTrain: epoch  8, batch    41 | loss: 6.0523548Losses:  5.978288650512695 1.9330357313156128
CurrentTrain: epoch  8, batch    42 | loss: 5.9782887Losses:  6.04833984375 1.9537169933319092
CurrentTrain: epoch  8, batch    43 | loss: 6.0483398Losses:  6.005925178527832 1.9617035388946533
CurrentTrain: epoch  8, batch    44 | loss: 6.0059252Losses:  5.778920650482178 1.6982630491256714
CurrentTrain: epoch  8, batch    45 | loss: 5.7789207Losses:  5.673755645751953 1.6829774379730225
CurrentTrain: epoch  8, batch    46 | loss: 5.6737556Losses:  5.599513053894043 1.5536549091339111
CurrentTrain: epoch  8, batch    47 | loss: 5.5995131Losses:  5.712163925170898 1.6680787801742554
CurrentTrain: epoch  8, batch    48 | loss: 5.7121639Losses:  5.793821334838867 1.7676818370819092
CurrentTrain: epoch  8, batch    49 | loss: 5.7938213Losses:  5.887028694152832 1.8723244667053223
CurrentTrain: epoch  8, batch    50 | loss: 5.8870287Losses:  5.575043201446533 1.5284312963485718
CurrentTrain: epoch  8, batch    51 | loss: 5.5750432Losses:  5.993338584899902 1.9555270671844482
CurrentTrain: epoch  8, batch    52 | loss: 5.9933386Losses:  5.908294677734375 1.8681950569152832
CurrentTrain: epoch  8, batch    53 | loss: 5.9082947Losses:  6.012477874755859 1.9954957962036133
CurrentTrain: epoch  8, batch    54 | loss: 6.0124779Losses:  5.688627243041992 1.688410758972168
CurrentTrain: epoch  8, batch    55 | loss: 5.6886272Losses:  5.837560653686523 1.8039886951446533
CurrentTrain: epoch  8, batch    56 | loss: 5.8375607Losses:  6.113264083862305 2.085714340209961
CurrentTrain: epoch  8, batch    57 | loss: 6.1132641Losses:  5.837018013000488 1.798350214958191
CurrentTrain: epoch  8, batch    58 | loss: 5.8370180Losses:  5.9162492752075195 1.8877313137054443
CurrentTrain: epoch  8, batch    59 | loss: 5.9162493Losses:  5.726251602172852 1.755717158317566
CurrentTrain: epoch  8, batch    60 | loss: 5.7262516Losses:  6.060917854309082 2.0303077697753906
CurrentTrain: epoch  8, batch    61 | loss: 6.0609179Losses:  5.317378520965576 1.306443691253662
CurrentTrain: epoch  8, batch    62 | loss: 5.3173785Losses:  5.828575134277344 1.8231604099273682
CurrentTrain: epoch  9, batch     0 | loss: 5.8285751Losses:  5.8280744552612305 1.7287569046020508
CurrentTrain: epoch  9, batch     1 | loss: 5.8280745Losses:  6.031120777130127 1.984126091003418
CurrentTrain: epoch  9, batch     2 | loss: 6.0311208Losses:  5.866174697875977 1.8037700653076172
CurrentTrain: epoch  9, batch     3 | loss: 5.8661747Losses:  5.673036098480225 1.649955153465271
CurrentTrain: epoch  9, batch     4 | loss: 5.6730361Losses:  5.844772815704346 1.8380924463272095
CurrentTrain: epoch  9, batch     5 | loss: 5.8447728Losses:  5.825832843780518 1.7882992029190063
CurrentTrain: epoch  9, batch     6 | loss: 5.8258328Losses:  5.862342834472656 1.843127727508545
CurrentTrain: epoch  9, batch     7 | loss: 5.8623428Losses:  5.6509785652160645 1.6655491590499878
CurrentTrain: epoch  9, batch     8 | loss: 5.6509786Losses:  5.903563022613525 1.8816839456558228
CurrentTrain: epoch  9, batch     9 | loss: 5.9035630Losses:  6.023614406585693 2.0042004585266113
CurrentTrain: epoch  9, batch    10 | loss: 6.0236144Losses:  6.024298667907715 1.9843448400497437
CurrentTrain: epoch  9, batch    11 | loss: 6.0242987Losses:  5.970182418823242 1.9479849338531494
CurrentTrain: epoch  9, batch    12 | loss: 5.9701824Losses:  5.94435977935791 1.9191503524780273
CurrentTrain: epoch  9, batch    13 | loss: 5.9443598Losses:  6.121660232543945 2.1070525646209717
CurrentTrain: epoch  9, batch    14 | loss: 6.1216602Losses:  6.112500190734863 2.059995174407959
CurrentTrain: epoch  9, batch    15 | loss: 6.1125002Losses:  5.919212818145752 1.908563494682312
CurrentTrain: epoch  9, batch    16 | loss: 5.9192128Losses:  5.9852986335754395 1.931287407875061
CurrentTrain: epoch  9, batch    17 | loss: 5.9852986Losses:  5.902432441711426 1.8825006484985352
CurrentTrain: epoch  9, batch    18 | loss: 5.9024324Losses:  5.574894905090332 1.5876413583755493
CurrentTrain: epoch  9, batch    19 | loss: 5.5748949Losses:  5.9357147216796875 1.8820514678955078
CurrentTrain: epoch  9, batch    20 | loss: 5.9357147Losses:  5.882813930511475 1.871700406074524
CurrentTrain: epoch  9, batch    21 | loss: 5.8828139Losses:  5.960446357727051 1.899369478225708
CurrentTrain: epoch  9, batch    22 | loss: 5.9604464Losses:  5.876682758331299 1.8757025003433228
CurrentTrain: epoch  9, batch    23 | loss: 5.8766828Losses:  5.78835916519165 1.7739373445510864
CurrentTrain: epoch  9, batch    24 | loss: 5.7883592Losses:  5.775705337524414 1.745827555656433
CurrentTrain: epoch  9, batch    25 | loss: 5.7757053Losses:  5.865325927734375 1.8735923767089844
CurrentTrain: epoch  9, batch    26 | loss: 5.8653259Losses:  5.874975681304932 1.8676667213439941
CurrentTrain: epoch  9, batch    27 | loss: 5.8749757Losses:  6.066636085510254 2.06754469871521
CurrentTrain: epoch  9, batch    28 | loss: 6.0666361Losses:  5.793459892272949 1.7822611331939697
CurrentTrain: epoch  9, batch    29 | loss: 5.7934599Losses:  5.898177623748779 1.8628296852111816
CurrentTrain: epoch  9, batch    30 | loss: 5.8981776Losses:  5.883652687072754 1.894425630569458
CurrentTrain: epoch  9, batch    31 | loss: 5.8836527Losses:  5.764449119567871 1.7693960666656494
CurrentTrain: epoch  9, batch    32 | loss: 5.7644491Losses:  5.5727314949035645 1.6032757759094238
CurrentTrain: epoch  9, batch    33 | loss: 5.5727315Losses:  6.041008949279785 2.029200553894043
CurrentTrain: epoch  9, batch    34 | loss: 6.0410089Losses:  6.086369514465332 2.0786514282226562
CurrentTrain: epoch  9, batch    35 | loss: 6.0863695Losses:  5.579588890075684 1.5763378143310547
CurrentTrain: epoch  9, batch    36 | loss: 5.5795889Losses:  5.99382209777832 1.9893028736114502
CurrentTrain: epoch  9, batch    37 | loss: 5.9938221Losses:  5.896677494049072 1.845668911933899
CurrentTrain: epoch  9, batch    38 | loss: 5.8966775Losses:  6.120080947875977 1.928276538848877
CurrentTrain: epoch  9, batch    39 | loss: 6.1200809Losses:  5.8045430183410645 1.718395709991455
CurrentTrain: epoch  9, batch    40 | loss: 5.8045430Losses:  5.668461799621582 1.673592448234558
CurrentTrain: epoch  9, batch    41 | loss: 5.6684618Losses:  5.825073719024658 1.7995885610580444
CurrentTrain: epoch  9, batch    42 | loss: 5.8250737Losses:  5.712685585021973 1.7208586931228638
CurrentTrain: epoch  9, batch    43 | loss: 5.7126856Losses:  5.785345554351807 1.771614909172058
CurrentTrain: epoch  9, batch    44 | loss: 5.7853456Losses:  5.714073181152344 1.729341745376587
CurrentTrain: epoch  9, batch    45 | loss: 5.7140732Losses:  5.705291748046875 1.7168502807617188
CurrentTrain: epoch  9, batch    46 | loss: 5.7052917Losses:  5.5818634033203125 1.555699110031128
CurrentTrain: epoch  9, batch    47 | loss: 5.5818634Losses:  5.978808403015137 1.9627602100372314
CurrentTrain: epoch  9, batch    48 | loss: 5.9788084Losses:  5.997827529907227 1.8439948558807373
CurrentTrain: epoch  9, batch    49 | loss: 5.9978275Losses:  6.077507972717285 1.9942405223846436
CurrentTrain: epoch  9, batch    50 | loss: 6.0775080Losses:  6.050467491149902 1.9957401752471924
CurrentTrain: epoch  9, batch    51 | loss: 6.0504675Losses:  5.898476600646973 1.8779470920562744
CurrentTrain: epoch  9, batch    52 | loss: 5.8984766Losses:  5.860475540161133 1.867363691329956
CurrentTrain: epoch  9, batch    53 | loss: 5.8604755Losses:  5.835903644561768 1.820512294769287
CurrentTrain: epoch  9, batch    54 | loss: 5.8359036Losses:  5.820150852203369 1.8179012537002563
CurrentTrain: epoch  9, batch    55 | loss: 5.8201509Losses:  5.968985557556152 1.9900219440460205
CurrentTrain: epoch  9, batch    56 | loss: 5.9689856Losses:  5.970165729522705 1.9474490880966187
CurrentTrain: epoch  9, batch    57 | loss: 5.9701657Losses:  5.994111061096191 1.8567532300949097
CurrentTrain: epoch  9, batch    58 | loss: 5.9941111Losses:  5.92148494720459 1.9251055717468262
CurrentTrain: epoch  9, batch    59 | loss: 5.9214849Losses:  5.914236545562744 1.8906043767929077
CurrentTrain: epoch  9, batch    60 | loss: 5.9142365Losses:  5.93763542175293 1.9162814617156982
CurrentTrain: epoch  9, batch    61 | loss: 5.9376354Losses:  5.581435680389404 1.5792765617370605
CurrentTrain: epoch  9, batch    62 | loss: 5.5814357
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  8.366403579711914 1.8841900825500488
CurrentTrain: epoch  0, batch     0 | loss: 8.3664036Losses:  7.78703498840332 1.903944730758667
CurrentTrain: epoch  0, batch     1 | loss: 7.7870350Losses:  8.167144775390625 1.9984346628189087
CurrentTrain: epoch  0, batch     2 | loss: 8.1671448Losses:  7.801248550415039 0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.8012486Losses:  7.906655788421631 2.0490760803222656
CurrentTrain: epoch  1, batch     0 | loss: 7.9066558Losses:  7.920907974243164 2.1711015701293945
CurrentTrain: epoch  1, batch     1 | loss: 7.9209080Losses:  7.284328460693359 2.1146013736724854
CurrentTrain: epoch  1, batch     2 | loss: 7.2843285Losses:  3.748751640319824 0.587934136390686
CurrentTrain: epoch  1, batch     3 | loss: 3.7487516Losses:  7.115254878997803 1.8766781091690063
CurrentTrain: epoch  2, batch     0 | loss: 7.1152549Losses:  6.411632537841797 1.9638615846633911
CurrentTrain: epoch  2, batch     1 | loss: 6.4116325Losses:  6.361319065093994 2.0123848915100098
CurrentTrain: epoch  2, batch     2 | loss: 6.3613191Losses:  8.14201831817627 0.6497167348861694
CurrentTrain: epoch  2, batch     3 | loss: 8.1420183Losses:  7.1623735427856445 2.1258606910705566
CurrentTrain: epoch  3, batch     0 | loss: 7.1623735Losses:  5.512232780456543 1.9274500608444214
CurrentTrain: epoch  3, batch     1 | loss: 5.5122328Losses:  6.2654643058776855 2.0159335136413574
CurrentTrain: epoch  3, batch     2 | loss: 6.2654643Losses:  3.6977016925811768 0.6785385012626648
CurrentTrain: epoch  3, batch     3 | loss: 3.6977017Losses:  5.618470191955566 1.8796539306640625
CurrentTrain: epoch  4, batch     0 | loss: 5.6184702Losses:  5.7473649978637695 1.836411714553833
CurrentTrain: epoch  4, batch     1 | loss: 5.7473650Losses:  5.661539077758789 1.8913981914520264
CurrentTrain: epoch  4, batch     2 | loss: 5.6615391Losses:  3.923332691192627 0.6351717710494995
CurrentTrain: epoch  4, batch     3 | loss: 3.9233327Losses:  5.595383644104004 1.8823775053024292
CurrentTrain: epoch  5, batch     0 | loss: 5.5953836Losses:  4.696844577789307 1.8937537670135498
CurrentTrain: epoch  5, batch     1 | loss: 4.6968446Losses:  5.116850852966309 2.1516306400299072
CurrentTrain: epoch  5, batch     2 | loss: 5.1168509Losses:  5.787115097045898 0.6387996077537537
CurrentTrain: epoch  5, batch     3 | loss: 5.7871151Losses:  5.207602500915527 1.9360616207122803
CurrentTrain: epoch  6, batch     0 | loss: 5.2076025Losses:  4.847526550292969 1.8531320095062256
CurrentTrain: epoch  6, batch     1 | loss: 4.8475266Losses:  4.684558868408203 1.9273234605789185
CurrentTrain: epoch  6, batch     2 | loss: 4.6845589Losses:  2.4525961875915527 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4525962Losses:  5.165283679962158 2.0163192749023438
CurrentTrain: epoch  7, batch     0 | loss: 5.1652837Losses:  4.929999351501465 2.117469310760498
CurrentTrain: epoch  7, batch     1 | loss: 4.9299994Losses:  4.635007858276367 1.9180855751037598
CurrentTrain: epoch  7, batch     2 | loss: 4.6350079Losses:  2.0395140647888184 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.0395141Losses:  4.636137962341309 2.001307964324951
CurrentTrain: epoch  8, batch     0 | loss: 4.6361380Losses:  4.740623474121094 1.7726542949676514
CurrentTrain: epoch  8, batch     1 | loss: 4.7406235Losses:  4.685428619384766 1.998631477355957
CurrentTrain: epoch  8, batch     2 | loss: 4.6854286Losses:  2.6307380199432373 0.6019735336303711
CurrentTrain: epoch  8, batch     3 | loss: 2.6307380Losses:  4.435206890106201 1.9925743341445923
CurrentTrain: epoch  9, batch     0 | loss: 4.4352069Losses:  4.970603942871094 1.7627954483032227
CurrentTrain: epoch  9, batch     1 | loss: 4.9706039Losses:  4.299695014953613 1.9757890701293945
CurrentTrain: epoch  9, batch     2 | loss: 4.2996950Losses:  2.448075294494629 0.6028838157653809
CurrentTrain: epoch  9, batch     3 | loss: 2.4480753
Losses:  4.906091690063477 2.6435346603393555
MemoryTrain:  epoch  0, batch     0 | loss: 4.9060917Losses:  1.365653395652771 1.274308681488037
MemoryTrain:  epoch  0, batch     1 | loss: 1.3656534Losses:  4.331711292266846 2.6456825733184814
MemoryTrain:  epoch  1, batch     0 | loss: 4.3317113Losses:  1.9895596504211426 1.2771902084350586
MemoryTrain:  epoch  1, batch     1 | loss: 1.9895597Losses:  4.079768657684326 2.6396567821502686
MemoryTrain:  epoch  2, batch     0 | loss: 4.0797687Losses:  1.749738335609436 1.3020579814910889
MemoryTrain:  epoch  2, batch     1 | loss: 1.7497383Losses:  3.341096878051758 2.650456190109253
MemoryTrain:  epoch  3, batch     0 | loss: 3.3410969Losses:  3.4183077812194824 1.2892074584960938
MemoryTrain:  epoch  3, batch     1 | loss: 3.4183078Losses:  3.614581346511841 2.6398346424102783
MemoryTrain:  epoch  4, batch     0 | loss: 3.6145813Losses:  1.385561466217041 1.278822660446167
MemoryTrain:  epoch  4, batch     1 | loss: 1.3855615Losses:  3.510566234588623 2.640582799911499
MemoryTrain:  epoch  5, batch     0 | loss: 3.5105662Losses:  1.4311522245407104 1.255756139755249
MemoryTrain:  epoch  5, batch     1 | loss: 1.4311522Losses:  3.387147903442383 2.624636173248291
MemoryTrain:  epoch  6, batch     0 | loss: 3.3871479Losses:  1.397882103919983 1.3015427589416504
MemoryTrain:  epoch  6, batch     1 | loss: 1.3978821Losses:  3.0175747871398926 2.6362555027008057
MemoryTrain:  epoch  7, batch     0 | loss: 3.0175748Losses:  2.6554083824157715 1.27292001247406
MemoryTrain:  epoch  7, batch     1 | loss: 2.6554084Losses:  3.326799154281616 2.6345574855804443
MemoryTrain:  epoch  8, batch     0 | loss: 3.3267992Losses:  1.3171356916427612 1.2874501943588257
MemoryTrain:  epoch  8, batch     1 | loss: 1.3171357Losses:  3.1621711254119873 2.6387648582458496
MemoryTrain:  epoch  9, batch     0 | loss: 3.1621711Losses:  1.6859440803527832 1.2666118144989014
MemoryTrain:  epoch  9, batch     1 | loss: 1.6859441
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 79.90%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.10%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.55%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 92.89%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.96%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.88%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.72%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.57%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.52%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 92.29%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 91.91%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.69%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.25%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.04%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 90.85%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.74%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.30%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 90.27%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.17%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 89.86%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 89.81%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 89.65%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 89.23%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 88.95%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 88.74%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 88.34%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 88.07%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.19%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.95%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.53%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 86.24%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 85.95%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 85.67%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.22%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 84.61%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 84.00%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.58%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 83.05%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 82.70%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.47%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 84.00%   
cur_acc:  ['0.9435', '0.7510']
his_acc:  ['0.9435', '0.8400']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  8.974143981933594 1.955000400543213
CurrentTrain: epoch  0, batch     0 | loss: 8.9741440Losses:  8.144403457641602 2.027637481689453
CurrentTrain: epoch  0, batch     1 | loss: 8.1444035Losses:  8.962723731994629 2.142590045928955
CurrentTrain: epoch  0, batch     2 | loss: 8.9627237Losses:  8.18850326538086 0.643065333366394
CurrentTrain: epoch  0, batch     3 | loss: 8.1885033Losses:  7.635771751403809 2.1690492630004883
CurrentTrain: epoch  1, batch     0 | loss: 7.6357718Losses:  8.45300579071045 1.9538196325302124
CurrentTrain: epoch  1, batch     1 | loss: 8.4530058Losses:  7.27397346496582 2.138049840927124
CurrentTrain: epoch  1, batch     2 | loss: 7.2739735Losses:  6.4659342765808105 0.6607308983802795
CurrentTrain: epoch  1, batch     3 | loss: 6.4659343Losses:  6.332544803619385 1.8961024284362793
CurrentTrain: epoch  2, batch     0 | loss: 6.3325448Losses:  7.343780517578125 2.116992473602295
CurrentTrain: epoch  2, batch     1 | loss: 7.3437805Losses:  7.811250686645508 1.9876184463500977
CurrentTrain: epoch  2, batch     2 | loss: 7.8112507Losses:  4.122745513916016 0.6486978530883789
CurrentTrain: epoch  2, batch     3 | loss: 4.1227455Losses:  6.925069808959961 2.1049304008483887
CurrentTrain: epoch  3, batch     0 | loss: 6.9250698Losses:  6.0311689376831055 1.8913942575454712
CurrentTrain: epoch  3, batch     1 | loss: 6.0311689Losses:  7.068007946014404 2.058781147003174
CurrentTrain: epoch  3, batch     2 | loss: 7.0680079Losses:  5.716468334197998 0.6580051183700562
CurrentTrain: epoch  3, batch     3 | loss: 5.7164683Losses:  6.893666744232178 1.9789832830429077
CurrentTrain: epoch  4, batch     0 | loss: 6.8936667Losses:  6.555470943450928 2.1489791870117188
CurrentTrain: epoch  4, batch     1 | loss: 6.5554709Losses:  5.058072090148926 1.8617517948150635
CurrentTrain: epoch  4, batch     2 | loss: 5.0580721Losses:  4.995095252990723 0.6477932333946228
CurrentTrain: epoch  4, batch     3 | loss: 4.9950953Losses:  6.371408462524414 1.7821416854858398
CurrentTrain: epoch  5, batch     0 | loss: 6.3714085Losses:  5.650019645690918 1.969506025314331
CurrentTrain: epoch  5, batch     1 | loss: 5.6500196Losses:  5.383268356323242 1.8128576278686523
CurrentTrain: epoch  5, batch     2 | loss: 5.3832684Losses:  7.996073246002197 0.6680317521095276
CurrentTrain: epoch  5, batch     3 | loss: 7.9960732Losses:  6.566743850708008 2.1500368118286133
CurrentTrain: epoch  6, batch     0 | loss: 6.5667439Losses:  6.020474433898926 2.0748367309570312
CurrentTrain: epoch  6, batch     1 | loss: 6.0204744Losses:  5.257316589355469 2.054043769836426
CurrentTrain: epoch  6, batch     2 | loss: 5.2573166Losses:  3.2837705612182617 0.6337904930114746
CurrentTrain: epoch  6, batch     3 | loss: 3.2837706Losses:  5.222054481506348 1.9996001720428467
CurrentTrain: epoch  7, batch     0 | loss: 5.2220545Losses:  5.604193210601807 1.9917353391647339
CurrentTrain: epoch  7, batch     1 | loss: 5.6041932Losses:  5.538519859313965 1.981896162033081
CurrentTrain: epoch  7, batch     2 | loss: 5.5385199Losses:  5.185331344604492 0.6667423248291016
CurrentTrain: epoch  7, batch     3 | loss: 5.1853313Losses:  5.586332321166992 2.151759624481201
CurrentTrain: epoch  8, batch     0 | loss: 5.5863323Losses:  5.69243049621582 2.1022496223449707
CurrentTrain: epoch  8, batch     1 | loss: 5.6924305Losses:  5.132424831390381 1.970844030380249
CurrentTrain: epoch  8, batch     2 | loss: 5.1324248Losses:  3.243764877319336 0.6677044630050659
CurrentTrain: epoch  8, batch     3 | loss: 3.2437649Losses:  4.846648216247559 1.8949508666992188
CurrentTrain: epoch  9, batch     0 | loss: 4.8466482Losses:  4.398008346557617 1.8190281391143799
CurrentTrain: epoch  9, batch     1 | loss: 4.3980083Losses:  5.597474575042725 1.918259620666504
CurrentTrain: epoch  9, batch     2 | loss: 5.5974746Losses:  5.643558025360107 0.6791801452636719
CurrentTrain: epoch  9, batch     3 | loss: 5.6435580
Losses:  3.7335124015808105 2.6698756217956543
MemoryTrain:  epoch  0, batch     0 | loss: 3.7335124Losses:  2.9655814170837402 2.518263816833496
MemoryTrain:  epoch  0, batch     1 | loss: 2.9655814Losses:  3.6608831882476807 2.675529956817627
MemoryTrain:  epoch  1, batch     0 | loss: 3.6608832Losses:  3.311809539794922 2.512819766998291
MemoryTrain:  epoch  1, batch     1 | loss: 3.3118095Losses:  3.037703275680542 2.645691394805908
MemoryTrain:  epoch  2, batch     0 | loss: 3.0377033Losses:  3.020108222961426 2.5433433055877686
MemoryTrain:  epoch  2, batch     1 | loss: 3.0201082Losses:  2.9769160747528076 2.6598432064056396
MemoryTrain:  epoch  3, batch     0 | loss: 2.9769161Losses:  2.6600818634033203 2.5167198181152344
MemoryTrain:  epoch  3, batch     1 | loss: 2.6600819Losses:  2.8700318336486816 2.6563925743103027
MemoryTrain:  epoch  4, batch     0 | loss: 2.8700318Losses:  2.759859085083008 2.5293807983398438
MemoryTrain:  epoch  4, batch     1 | loss: 2.7598591Losses:  2.819098949432373 2.671505928039551
MemoryTrain:  epoch  5, batch     0 | loss: 2.8190989Losses:  2.6254894733428955 2.5061440467834473
MemoryTrain:  epoch  5, batch     1 | loss: 2.6254895Losses:  2.7743759155273438 2.6500182151794434
MemoryTrain:  epoch  6, batch     0 | loss: 2.7743759Losses:  2.5712971687316895 2.5268642902374268
MemoryTrain:  epoch  6, batch     1 | loss: 2.5712972Losses:  2.7440695762634277 2.64943790435791
MemoryTrain:  epoch  7, batch     0 | loss: 2.7440696Losses:  2.5844156742095947 2.527778148651123
MemoryTrain:  epoch  7, batch     1 | loss: 2.5844157Losses:  2.7321431636810303 2.672991991043091
MemoryTrain:  epoch  8, batch     0 | loss: 2.7321432Losses:  2.5355489253997803 2.5030453205108643
MemoryTrain:  epoch  8, batch     1 | loss: 2.5355489Losses:  2.7407724857330322 2.679121971130371
MemoryTrain:  epoch  9, batch     0 | loss: 2.7407725Losses:  2.531848669052124 2.497830629348755
MemoryTrain:  epoch  9, batch     1 | loss: 2.5318487
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 74.07%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 73.83%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.65%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 73.79%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.21%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.27%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 91.02%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.36%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.61%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.70%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.61%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.56%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.43%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.22%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 90.78%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.82%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 90.47%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 90.06%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 89.73%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 89.63%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 89.39%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 88.65%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.49%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 86.97%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 86.33%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 85.89%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 85.33%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.04%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 84.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.16%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 83.62%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.41%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.21%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.02%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.59%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.00%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 80.85%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 80.41%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.02%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.76%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.78%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 80.79%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.00%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 80.90%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.91%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 80.96%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 81.01%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 81.06%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.75%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.52%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 80.98%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.49%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 79.96%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 79.40%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 78.89%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.56%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 78.91%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 78.64%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 78.37%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 78.10%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 77.76%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.07%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 79.07%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.97%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 78.81%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.74%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 78.68%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.63%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.61%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 78.58%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.60%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 78.58%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.36%   
cur_acc:  ['0.9435', '0.7510', '0.7321']
his_acc:  ['0.9435', '0.8400', '0.7836']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  7.377167701721191 1.954326868057251
CurrentTrain: epoch  0, batch     0 | loss: 7.3771677Losses:  6.093158721923828 1.9349071979522705
CurrentTrain: epoch  0, batch     1 | loss: 6.0931587Losses:  7.1188859939575195 1.7933262586593628
CurrentTrain: epoch  0, batch     2 | loss: 7.1188860Losses:  4.209996700286865 0.6349602937698364
CurrentTrain: epoch  0, batch     3 | loss: 4.2099967Losses:  6.449620246887207 2.075856924057007
CurrentTrain: epoch  1, batch     0 | loss: 6.4496202Losses:  5.7261762619018555 2.013725757598877
CurrentTrain: epoch  1, batch     1 | loss: 5.7261763Losses:  5.9008283615112305 2.0846095085144043
CurrentTrain: epoch  1, batch     2 | loss: 5.9008284Losses:  2.952493667602539 0.6166688203811646
CurrentTrain: epoch  1, batch     3 | loss: 2.9524937Losses:  6.449845314025879 1.9592063426971436
CurrentTrain: epoch  2, batch     0 | loss: 6.4498453Losses:  5.135258197784424 1.9844543933868408
CurrentTrain: epoch  2, batch     1 | loss: 5.1352582Losses:  4.438241004943848 1.902176856994629
CurrentTrain: epoch  2, batch     2 | loss: 4.4382410Losses:  2.6479320526123047 0.6259926557540894
CurrentTrain: epoch  2, batch     3 | loss: 2.6479321Losses:  5.129204750061035 1.9799671173095703
CurrentTrain: epoch  3, batch     0 | loss: 5.1292048Losses:  5.077664375305176 2.0901854038238525
CurrentTrain: epoch  3, batch     1 | loss: 5.0776644Losses:  5.083494663238525 2.005949020385742
CurrentTrain: epoch  3, batch     2 | loss: 5.0834947Losses:  2.6970067024230957 0.6272491216659546
CurrentTrain: epoch  3, batch     3 | loss: 2.6970067Losses:  4.681188106536865 1.8318792581558228
CurrentTrain: epoch  4, batch     0 | loss: 4.6811881Losses:  4.447267532348633 1.7957561016082764
CurrentTrain: epoch  4, batch     1 | loss: 4.4472675Losses:  4.869973659515381 1.6174933910369873
CurrentTrain: epoch  4, batch     2 | loss: 4.8699737Losses:  2.579510450363159 0.6214520931243896
CurrentTrain: epoch  4, batch     3 | loss: 2.5795105Losses:  5.0534892082214355 1.9834883213043213
CurrentTrain: epoch  5, batch     0 | loss: 5.0534892Losses:  4.673213481903076 1.8660286664962769
CurrentTrain: epoch  5, batch     1 | loss: 4.6732135Losses:  4.41597843170166 2.100100040435791
CurrentTrain: epoch  5, batch     2 | loss: 4.4159784Losses:  1.8028314113616943 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8028314Losses:  3.925114154815674 1.837568998336792
CurrentTrain: epoch  6, batch     0 | loss: 3.9251142Losses:  4.631624698638916 1.8503985404968262
CurrentTrain: epoch  6, batch     1 | loss: 4.6316247Losses:  4.683620452880859 1.9709827899932861
CurrentTrain: epoch  6, batch     2 | loss: 4.6836205Losses:  2.9016001224517822 0.6743871569633484
CurrentTrain: epoch  6, batch     3 | loss: 2.9016001Losses:  4.272710800170898 1.9706658124923706
CurrentTrain: epoch  7, batch     0 | loss: 4.2727108Losses:  4.096715450286865 1.9394422769546509
CurrentTrain: epoch  7, batch     1 | loss: 4.0967155Losses:  4.4109978675842285 1.9862233400344849
CurrentTrain: epoch  7, batch     2 | loss: 4.4109979Losses:  3.820547580718994 0.6696123480796814
CurrentTrain: epoch  7, batch     3 | loss: 3.8205476Losses:  4.2834391593933105 1.7646205425262451
CurrentTrain: epoch  8, batch     0 | loss: 4.2834392Losses:  4.159946441650391 2.0021326541900635
CurrentTrain: epoch  8, batch     1 | loss: 4.1599464Losses:  3.878307342529297 1.9676530361175537
CurrentTrain: epoch  8, batch     2 | loss: 3.8783073Losses:  2.7604453563690186 0.6307373642921448
CurrentTrain: epoch  8, batch     3 | loss: 2.7604454Losses:  4.0860795974731445 1.9831318855285645
CurrentTrain: epoch  9, batch     0 | loss: 4.0860796Losses:  4.338753700256348 1.8770042657852173
CurrentTrain: epoch  9, batch     1 | loss: 4.3387537Losses:  3.7728943824768066 1.923880934715271
CurrentTrain: epoch  9, batch     2 | loss: 3.7728944Losses:  2.4575717449188232 0.5950456261634827
CurrentTrain: epoch  9, batch     3 | loss: 2.4575717
Losses:  3.0163915157318115 2.6466448307037354
MemoryTrain:  epoch  0, batch     0 | loss: 3.0163915Losses:  4.208089351654053 2.6634817123413086
MemoryTrain:  epoch  0, batch     1 | loss: 4.2080894Losses:  3.173346996307373 1.9642407894134521
MemoryTrain:  epoch  0, batch     2 | loss: 3.1733470Losses:  3.5516536235809326 2.6646344661712646
MemoryTrain:  epoch  1, batch     0 | loss: 3.5516536Losses:  3.9758307933807373 2.637174606323242
MemoryTrain:  epoch  1, batch     1 | loss: 3.9758308Losses:  3.525066614151001 1.9870851039886475
MemoryTrain:  epoch  1, batch     2 | loss: 3.5250666Losses:  3.529318332672119 2.645148754119873
MemoryTrain:  epoch  2, batch     0 | loss: 3.5293183Losses:  3.443535566329956 2.6778857707977295
MemoryTrain:  epoch  2, batch     1 | loss: 3.4435356Losses:  2.512627601623535 1.9442269802093506
MemoryTrain:  epoch  2, batch     2 | loss: 2.5126276Losses:  3.4570493698120117 2.6560423374176025
MemoryTrain:  epoch  3, batch     0 | loss: 3.4570494Losses:  3.1427669525146484 2.658687114715576
MemoryTrain:  epoch  3, batch     1 | loss: 3.1427670Losses:  2.1309304237365723 1.9454350471496582
MemoryTrain:  epoch  3, batch     2 | loss: 2.1309304Losses:  2.837069272994995 2.6391220092773438
MemoryTrain:  epoch  4, batch     0 | loss: 2.8370693Losses:  3.0864486694335938 2.648792266845703
MemoryTrain:  epoch  4, batch     1 | loss: 3.0864487Losses:  3.2823376655578613 1.990229606628418
MemoryTrain:  epoch  4, batch     2 | loss: 3.2823377Losses:  3.345391035079956 2.6329236030578613
MemoryTrain:  epoch  5, batch     0 | loss: 3.3453910Losses:  2.788447618484497 2.6550710201263428
MemoryTrain:  epoch  5, batch     1 | loss: 2.7884476Losses:  2.099008798599243 1.969626545906067
MemoryTrain:  epoch  5, batch     2 | loss: 2.0990088Losses:  2.6762447357177734 2.6265783309936523
MemoryTrain:  epoch  6, batch     0 | loss: 2.6762447Losses:  2.906064987182617 2.6529624462127686
MemoryTrain:  epoch  6, batch     1 | loss: 2.9060650Losses:  2.2576537132263184 1.9940334558486938
MemoryTrain:  epoch  6, batch     2 | loss: 2.2576537Losses:  2.683263063430786 2.6312272548675537
MemoryTrain:  epoch  7, batch     0 | loss: 2.6832631Losses:  2.8118529319763184 2.665081024169922
MemoryTrain:  epoch  7, batch     1 | loss: 2.8118529Losses:  2.0177526473999023 1.954874038696289
MemoryTrain:  epoch  7, batch     2 | loss: 2.0177526Losses:  2.708498001098633 2.6329989433288574
MemoryTrain:  epoch  8, batch     0 | loss: 2.7084980Losses:  2.8055355548858643 2.656850576400757
MemoryTrain:  epoch  8, batch     1 | loss: 2.8055356Losses:  2.008861780166626 1.947638750076294
MemoryTrain:  epoch  8, batch     2 | loss: 2.0088618Losses:  2.7274885177612305 2.6654558181762695
MemoryTrain:  epoch  9, batch     0 | loss: 2.7274885Losses:  2.679231643676758 2.6353015899658203
MemoryTrain:  epoch  9, batch     1 | loss: 2.6792316Losses:  2.050830125808716 1.9367566108703613
MemoryTrain:  epoch  9, batch     2 | loss: 2.0508301
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 58.07%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.75%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 90.02%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 89.55%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 89.09%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 88.18%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 88.45%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.77%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.08%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.10%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 88.96%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 88.78%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.61%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 88.44%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.19%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 86.98%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 86.62%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 86.19%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.34%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 85.18%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 84.88%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 84.57%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 84.01%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 83.46%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 83.05%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 82.59%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 82.26%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 81.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.44%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.01%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.60%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.20%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.57%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.01%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.47%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.87%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.51%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.49%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.74%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 78.86%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 78.92%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 78.89%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 79.62%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 79.05%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 78.57%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 78.06%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 77.51%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.97%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.61%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.00%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.42%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 77.32%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 77.06%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 76.76%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 76.46%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 76.21%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.88%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.28%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.50%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 76.83%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 76.86%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 77.01%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 77.00%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 76.98%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.07%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.99%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.87%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.75%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 76.67%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 76.62%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 76.58%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 76.72%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.85%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.09%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:  194 | acc: 18.75%,  total acc: 76.92%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 76.69%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 76.46%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 76.20%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 75.91%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 75.72%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.68%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 75.52%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.49%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 75.18%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 75.06%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 74.97%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.73%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 74.56%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.14%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.68%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.28%   
cur_acc:  ['0.9435', '0.7510', '0.7321', '0.7817']
his_acc:  ['0.9435', '0.8400', '0.7836', '0.7728']
Clustering into  24  clusters
Clusters:  [ 2  8  1  0 14 23  1 22  3  3  1  3 21 18 11  4 10 18 12  4  2  4  0  7
  0  8  9  0  6 15  1  9  2  0  5  2 19 16 22  6 20 17  7  1 22  0  2  1
 13 10]
Losses:  7.661703586578369 1.9252409934997559
CurrentTrain: epoch  0, batch     0 | loss: 7.6617036Losses:  7.771719455718994 1.958569049835205
CurrentTrain: epoch  0, batch     1 | loss: 7.7717195Losses:  7.575647830963135 1.9121447801589966
CurrentTrain: epoch  0, batch     2 | loss: 7.5756478Losses:  3.354910373687744 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.3549104Losses:  7.1634039878845215 1.9544082880020142
CurrentTrain: epoch  1, batch     0 | loss: 7.1634040Losses:  6.6052985191345215 2.062399387359619
CurrentTrain: epoch  1, batch     1 | loss: 6.6052985Losses:  6.3558030128479 1.9688829183578491
CurrentTrain: epoch  1, batch     2 | loss: 6.3558030Losses:  4.9348955154418945 0.676277756690979
CurrentTrain: epoch  1, batch     3 | loss: 4.9348955Losses:  6.234988689422607 2.0654053688049316
CurrentTrain: epoch  2, batch     0 | loss: 6.2349887Losses:  7.091183662414551 2.070425510406494
CurrentTrain: epoch  2, batch     1 | loss: 7.0911837Losses:  6.183499336242676 2.0545527935028076
CurrentTrain: epoch  2, batch     2 | loss: 6.1834993Losses:  3.371270179748535 0.6692591905593872
CurrentTrain: epoch  2, batch     3 | loss: 3.3712702Losses:  6.546627521514893 2.0683817863464355
CurrentTrain: epoch  3, batch     0 | loss: 6.5466275Losses:  5.837250709533691 1.9535377025604248
CurrentTrain: epoch  3, batch     1 | loss: 5.8372507Losses:  5.515163898468018 1.9668853282928467
CurrentTrain: epoch  3, batch     2 | loss: 5.5151639Losses:  5.2668914794921875 0.6677266359329224
CurrentTrain: epoch  3, batch     3 | loss: 5.2668915Losses:  5.323952674865723 1.904066562652588
CurrentTrain: epoch  4, batch     0 | loss: 5.3239527Losses:  5.851759433746338 1.7822877168655396
CurrentTrain: epoch  4, batch     1 | loss: 5.8517594Losses:  5.80949592590332 2.1514835357666016
CurrentTrain: epoch  4, batch     2 | loss: 5.8094959Losses:  2.4290666580200195 0.6643954515457153
CurrentTrain: epoch  4, batch     3 | loss: 2.4290667Losses:  5.342074394226074 2.063288927078247
CurrentTrain: epoch  5, batch     0 | loss: 5.3420744Losses:  5.423827648162842 1.977837324142456
CurrentTrain: epoch  5, batch     1 | loss: 5.4238276Losses:  5.523008346557617 2.0283708572387695
CurrentTrain: epoch  5, batch     2 | loss: 5.5230083Losses:  5.543684005737305 0.6659485101699829
CurrentTrain: epoch  5, batch     3 | loss: 5.5436840Losses:  5.165901184082031 1.911924123764038
CurrentTrain: epoch  6, batch     0 | loss: 5.1659012Losses:  5.127920627593994 1.9602071046829224
CurrentTrain: epoch  6, batch     1 | loss: 5.1279206Losses:  5.272378921508789 2.030188798904419
CurrentTrain: epoch  6, batch     2 | loss: 5.2723789Losses:  3.28072452545166 0.6646924018859863
CurrentTrain: epoch  6, batch     3 | loss: 3.2807245Losses:  4.994664192199707 2.0618252754211426
CurrentTrain: epoch  7, batch     0 | loss: 4.9946642Losses:  5.0747175216674805 2.0330491065979004
CurrentTrain: epoch  7, batch     1 | loss: 5.0747175Losses:  5.077320098876953 1.985257625579834
CurrentTrain: epoch  7, batch     2 | loss: 5.0773201Losses:  3.490709066390991 0.6721060276031494
CurrentTrain: epoch  7, batch     3 | loss: 3.4907091Losses:  4.772550106048584 2.1529579162597656
CurrentTrain: epoch  8, batch     0 | loss: 4.7725501Losses:  5.2703375816345215 1.9135522842407227
CurrentTrain: epoch  8, batch     1 | loss: 5.2703376Losses:  4.955984115600586 2.0287320613861084
CurrentTrain: epoch  8, batch     2 | loss: 4.9559841Losses:  2.3965914249420166 0.6158061027526855
CurrentTrain: epoch  8, batch     3 | loss: 2.3965914Losses:  5.018004894256592 2.0726962089538574
CurrentTrain: epoch  9, batch     0 | loss: 5.0180049Losses:  4.061928749084473 1.8843395709991455
CurrentTrain: epoch  9, batch     1 | loss: 4.0619287Losses:  5.044149398803711 1.89408278465271
CurrentTrain: epoch  9, batch     2 | loss: 5.0441494Losses:  2.7314653396606445 0.6612247228622437
CurrentTrain: epoch  9, batch     3 | loss: 2.7314653
Losses:  3.2115421295166016 2.640166997909546
MemoryTrain:  epoch  0, batch     0 | loss: 3.2115421Losses:  3.1734464168548584 2.677306652069092
MemoryTrain:  epoch  0, batch     1 | loss: 3.1734464Losses:  3.122760057449341 2.6771693229675293
MemoryTrain:  epoch  0, batch     2 | loss: 3.1227601Losses:  2.0573630332946777 0.6641643047332764
MemoryTrain:  epoch  0, batch     3 | loss: 2.0573630Losses:  3.887890100479126 2.6627323627471924
MemoryTrain:  epoch  1, batch     0 | loss: 3.8878901Losses:  3.523974895477295 2.6836259365081787
MemoryTrain:  epoch  1, batch     1 | loss: 3.5239749Losses:  2.941258430480957 2.6502609252929688
MemoryTrain:  epoch  1, batch     2 | loss: 2.9412584Losses:  0.6424189209938049 0.5905907154083252
MemoryTrain:  epoch  1, batch     3 | loss: 0.6424189Losses:  3.350651502609253 2.662067413330078
MemoryTrain:  epoch  2, batch     0 | loss: 3.3506515Losses:  2.9512939453125 2.6609601974487305
MemoryTrain:  epoch  2, batch     1 | loss: 2.9512939Losses:  3.156343936920166 2.659085273742676
MemoryTrain:  epoch  2, batch     2 | loss: 3.1563439Losses:  0.7058311700820923 0.6611478328704834
MemoryTrain:  epoch  2, batch     3 | loss: 0.7058312Losses:  2.995720863342285 2.6675472259521484
MemoryTrain:  epoch  3, batch     0 | loss: 2.9957209Losses:  2.763849973678589 2.6518592834472656
MemoryTrain:  epoch  3, batch     1 | loss: 2.7638500Losses:  2.990550994873047 2.6533617973327637
MemoryTrain:  epoch  3, batch     2 | loss: 2.9905510Losses:  1.2213488817214966 0.686408519744873
MemoryTrain:  epoch  3, batch     3 | loss: 1.2213489Losses:  2.9270081520080566 2.669227123260498
MemoryTrain:  epoch  4, batch     0 | loss: 2.9270082Losses:  2.809067487716675 2.6781342029571533
MemoryTrain:  epoch  4, batch     1 | loss: 2.8090675Losses:  2.7179982662200928 2.63077712059021
MemoryTrain:  epoch  4, batch     2 | loss: 2.7179983Losses:  0.7186644077301025 0.6018875241279602
MemoryTrain:  epoch  4, batch     3 | loss: 0.7186644Losses:  2.7701408863067627 2.6632986068725586
MemoryTrain:  epoch  5, batch     0 | loss: 2.7701409Losses:  2.7650363445281982 2.6447577476501465
MemoryTrain:  epoch  5, batch     1 | loss: 2.7650363Losses:  2.7457022666931152 2.672804355621338
MemoryTrain:  epoch  5, batch     2 | loss: 2.7457023Losses:  0.6134099364280701 0.5980778932571411
MemoryTrain:  epoch  5, batch     3 | loss: 0.6134099Losses:  2.7511534690856934 2.6343955993652344
MemoryTrain:  epoch  6, batch     0 | loss: 2.7511535Losses:  2.7443885803222656 2.6718943119049072
MemoryTrain:  epoch  6, batch     1 | loss: 2.7443886Losses:  2.730034351348877 2.660398483276367
MemoryTrain:  epoch  6, batch     2 | loss: 2.7300344Losses:  0.6474632620811462 0.6282165050506592
MemoryTrain:  epoch  6, batch     3 | loss: 0.6474633Losses:  2.7344346046447754 2.6538186073303223
MemoryTrain:  epoch  7, batch     0 | loss: 2.7344346Losses:  2.7061874866485596 2.6537842750549316
MemoryTrain:  epoch  7, batch     1 | loss: 2.7061875Losses:  2.728565216064453 2.6568431854248047
MemoryTrain:  epoch  7, batch     2 | loss: 2.7285652Losses:  0.6818910241127014 0.6356889009475708
MemoryTrain:  epoch  7, batch     3 | loss: 0.6818910Losses:  2.690992593765259 2.650223970413208
MemoryTrain:  epoch  8, batch     0 | loss: 2.6909926Losses:  2.7316834926605225 2.662668228149414
MemoryTrain:  epoch  8, batch     1 | loss: 2.7316835Losses:  2.6948421001434326 2.644800901412964
MemoryTrain:  epoch  8, batch     2 | loss: 2.6948421Losses:  0.6912097930908203 0.6427278518676758
MemoryTrain:  epoch  8, batch     3 | loss: 0.6912098Losses:  2.703545570373535 2.6529879570007324
MemoryTrain:  epoch  9, batch     0 | loss: 2.7035456Losses:  2.711437463760376 2.6516547203063965
MemoryTrain:  epoch  9, batch     1 | loss: 2.7114375Losses:  2.7289018630981445 2.6594741344451904
MemoryTrain:  epoch  9, batch     2 | loss: 2.7289019Losses:  0.6135422587394714 0.595702052116394
MemoryTrain:  epoch  9, batch     3 | loss: 0.6135423
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 61.98%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 60.47%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 60.03%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 64.19%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.79%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.79%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.97%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.56%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.68%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 88.49%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 87.99%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.66%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 87.03%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 86.73%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 86.36%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 85.77%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 85.04%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 84.41%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 83.79%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 82.90%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 82.74%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 82.65%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 82.18%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 81.64%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 81.12%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 80.80%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 80.29%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 79.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.21%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 78.82%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.63%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.10%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.49%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.95%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.36%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 75.79%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.39%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.17%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 76.60%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 76.86%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 76.98%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 76.87%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 77.10%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.18%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 77.67%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 77.11%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 76.61%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 76.11%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 75.57%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 75.04%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 74.84%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 74.27%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.84%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.42%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 74.66%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.85%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 74.93%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.11%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.28%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.30%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 75.67%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 75.45%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 75.25%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 75.03%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.81%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 74.62%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 74.60%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 74.42%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 74.42%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 74.21%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 74.04%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 73.75%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 73.55%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.35%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 73.75%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 76.10%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 76.08%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 76.05%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.01%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 75.86%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 75.66%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 75.49%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 75.49%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 75.44%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 75.55%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:  274 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 75.59%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 75.43%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 75.16%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 75.07%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 74.71%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 74.61%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.41%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.17%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 74.07%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 74.07%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 74.10%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 74.14%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 74.13%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 74.13%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:  296 | acc: 93.75%,  total acc: 74.24%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.27%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.24%   
cur_acc:  ['0.9435', '0.7510', '0.7321', '0.7817', '0.7143']
his_acc:  ['0.9435', '0.8400', '0.7836', '0.7728', '0.7524']
Clustering into  29  clusters
Clusters:  [11  4  0  5 14 28  0  5 22 22  0 22 24  9 15  8 10  9  1  8 21  8 25 16
  5 18  3  5 26 27  0  3  3  4 23 11 19 17  5 26  4  7  6  0  5 12 21 10
 13 10 12  8  2 20  6  1  2 10  4  3]
Losses:  8.2073974609375 1.9810686111450195
CurrentTrain: epoch  0, batch     0 | loss: 8.2073975Losses:  7.811195373535156 1.7280652523040771
CurrentTrain: epoch  0, batch     1 | loss: 7.8111954Losses:  8.3868408203125 1.979491949081421
CurrentTrain: epoch  0, batch     2 | loss: 8.3868408Losses:  4.791815280914307 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.7918153Losses:  7.9118804931640625 2.15738582611084
CurrentTrain: epoch  1, batch     0 | loss: 7.9118805Losses:  7.110029697418213 2.038517475128174
CurrentTrain: epoch  1, batch     1 | loss: 7.1100297Losses:  6.319501876831055 2.037311553955078
CurrentTrain: epoch  1, batch     2 | loss: 6.3195019Losses:  5.616929531097412 0.650417685508728
CurrentTrain: epoch  1, batch     3 | loss: 5.6169295Losses:  6.261742115020752 1.8952058553695679
CurrentTrain: epoch  2, batch     0 | loss: 6.2617421Losses:  6.822959899902344 2.0176048278808594
CurrentTrain: epoch  2, batch     1 | loss: 6.8229599Losses:  6.116378307342529 2.036238193511963
CurrentTrain: epoch  2, batch     2 | loss: 6.1163783Losses:  5.615625381469727 0.6629689931869507
CurrentTrain: epoch  2, batch     3 | loss: 5.6156254Losses:  5.281200885772705 1.9191004037857056
CurrentTrain: epoch  3, batch     0 | loss: 5.2812009Losses:  6.6542439460754395 2.1087117195129395
CurrentTrain: epoch  3, batch     1 | loss: 6.6542439Losses:  6.536122798919678 2.1934070587158203
CurrentTrain: epoch  3, batch     2 | loss: 6.5361228Losses:  4.9724531173706055 0.6414706707000732
CurrentTrain: epoch  3, batch     3 | loss: 4.9724531Losses:  5.133962154388428 1.8297113180160522
CurrentTrain: epoch  4, batch     0 | loss: 5.1339622Losses:  6.646171569824219 1.9216958284378052
CurrentTrain: epoch  4, batch     1 | loss: 6.6461716Losses:  6.059337615966797 2.1118741035461426
CurrentTrain: epoch  4, batch     2 | loss: 6.0593376Losses:  3.721817970275879 0.6666669845581055
CurrentTrain: epoch  4, batch     3 | loss: 3.7218180Losses:  5.351773262023926 2.098039388656616
CurrentTrain: epoch  5, batch     0 | loss: 5.3517733Losses:  6.2084550857543945 1.8457094430923462
CurrentTrain: epoch  5, batch     1 | loss: 6.2084551Losses:  5.065272331237793 2.026111125946045
CurrentTrain: epoch  5, batch     2 | loss: 5.0652723Losses:  3.8097853660583496 0.6609934568405151
CurrentTrain: epoch  5, batch     3 | loss: 3.8097854Losses:  4.742337226867676 2.0349061489105225
CurrentTrain: epoch  6, batch     0 | loss: 4.7423372Losses:  5.7298583984375 2.1046347618103027
CurrentTrain: epoch  6, batch     1 | loss: 5.7298584Losses:  5.619029998779297 2.0701351165771484
CurrentTrain: epoch  6, batch     2 | loss: 5.6190300Losses:  3.755873680114746 0.6550320982933044
CurrentTrain: epoch  6, batch     3 | loss: 3.7558737Losses:  5.442957401275635 2.0380923748016357
CurrentTrain: epoch  7, batch     0 | loss: 5.4429574Losses:  5.164834499359131 2.0741238594055176
CurrentTrain: epoch  7, batch     1 | loss: 5.1648345Losses:  4.7593889236450195 1.8902478218078613
CurrentTrain: epoch  7, batch     2 | loss: 4.7593889Losses:  4.616678237915039 0.6609865427017212
CurrentTrain: epoch  7, batch     3 | loss: 4.6166782Losses:  4.636008262634277 1.8631312847137451
CurrentTrain: epoch  8, batch     0 | loss: 4.6360083Losses:  4.343550682067871 1.8452980518341064
CurrentTrain: epoch  8, batch     1 | loss: 4.3435507Losses:  5.745419502258301 1.952529788017273
CurrentTrain: epoch  8, batch     2 | loss: 5.7454195Losses:  2.50630259513855 0.6619433164596558
CurrentTrain: epoch  8, batch     3 | loss: 2.5063026Losses:  4.487442970275879 1.9879940748214722
CurrentTrain: epoch  9, batch     0 | loss: 4.4874430Losses:  4.625700950622559 1.8409733772277832
CurrentTrain: epoch  9, batch     1 | loss: 4.6257010Losses:  4.753264427185059 1.8306787014007568
CurrentTrain: epoch  9, batch     2 | loss: 4.7532644Losses:  5.859042167663574 0.6680793762207031
CurrentTrain: epoch  9, batch     3 | loss: 5.8590422
Losses:  3.3230462074279785 2.6822476387023926
MemoryTrain:  epoch  0, batch     0 | loss: 3.3230462Losses:  3.552278757095337 2.680298328399658
MemoryTrain:  epoch  0, batch     1 | loss: 3.5522788Losses:  3.226807117462158 2.6636505126953125
MemoryTrain:  epoch  0, batch     2 | loss: 3.2268071Losses:  2.969679594039917 2.368706464767456
MemoryTrain:  epoch  0, batch     3 | loss: 2.9696796Losses:  3.840247631072998 2.6583967208862305
MemoryTrain:  epoch  1, batch     0 | loss: 3.8402476Losses:  3.273329496383667 2.6557650566101074
MemoryTrain:  epoch  1, batch     1 | loss: 3.2733295Losses:  3.458728790283203 2.67496657371521
MemoryTrain:  epoch  1, batch     2 | loss: 3.4587288Losses:  2.8405508995056152 2.4005885124206543
MemoryTrain:  epoch  1, batch     3 | loss: 2.8405509Losses:  3.0583655834198 2.6631669998168945
MemoryTrain:  epoch  2, batch     0 | loss: 3.0583656Losses:  3.084291934967041 2.6689605712890625
MemoryTrain:  epoch  2, batch     1 | loss: 3.0842919Losses:  3.4325828552246094 2.6768622398376465
MemoryTrain:  epoch  2, batch     2 | loss: 3.4325829Losses:  2.611938953399658 2.350295066833496
MemoryTrain:  epoch  2, batch     3 | loss: 2.6119390Losses:  2.8191752433776855 2.648665189743042
MemoryTrain:  epoch  3, batch     0 | loss: 2.8191752Losses:  2.8679535388946533 2.6479978561401367
MemoryTrain:  epoch  3, batch     1 | loss: 2.8679535Losses:  3.0087637901306152 2.661914825439453
MemoryTrain:  epoch  3, batch     2 | loss: 3.0087638Losses:  2.7296054363250732 2.404543399810791
MemoryTrain:  epoch  3, batch     3 | loss: 2.7296054Losses:  2.9790730476379395 2.660759687423706
MemoryTrain:  epoch  4, batch     0 | loss: 2.9790730Losses:  2.8358898162841797 2.665640354156494
MemoryTrain:  epoch  4, batch     1 | loss: 2.8358898Losses:  2.7180140018463135 2.6452953815460205
MemoryTrain:  epoch  4, batch     2 | loss: 2.7180140Losses:  2.4976518154144287 2.381101131439209
MemoryTrain:  epoch  4, batch     3 | loss: 2.4976518Losses:  2.7078418731689453 2.6470932960510254
MemoryTrain:  epoch  5, batch     0 | loss: 2.7078419Losses:  2.764322280883789 2.6747422218322754
MemoryTrain:  epoch  5, batch     1 | loss: 2.7643223Losses:  2.775228500366211 2.6633331775665283
MemoryTrain:  epoch  5, batch     2 | loss: 2.7752285Losses:  3.0016260147094727 2.354663372039795
MemoryTrain:  epoch  5, batch     3 | loss: 3.0016260Losses:  2.8177285194396973 2.646824359893799
MemoryTrain:  epoch  6, batch     0 | loss: 2.8177285Losses:  2.707148313522339 2.6508517265319824
MemoryTrain:  epoch  6, batch     1 | loss: 2.7071483Losses:  2.7099342346191406 2.657320976257324
MemoryTrain:  epoch  6, batch     2 | loss: 2.7099342Losses:  2.4473114013671875 2.3828370571136475
MemoryTrain:  epoch  6, batch     3 | loss: 2.4473114Losses:  2.7465457916259766 2.660407066345215
MemoryTrain:  epoch  7, batch     0 | loss: 2.7465458Losses:  2.723438024520874 2.6662821769714355
MemoryTrain:  epoch  7, batch     1 | loss: 2.7234380Losses:  2.718508720397949 2.646129608154297
MemoryTrain:  epoch  7, batch     2 | loss: 2.7185087Losses:  2.4059183597564697 2.358109474182129
MemoryTrain:  epoch  7, batch     3 | loss: 2.4059184Losses:  2.6932296752929688 2.645170211791992
MemoryTrain:  epoch  8, batch     0 | loss: 2.6932297Losses:  2.713439464569092 2.648139238357544
MemoryTrain:  epoch  8, batch     1 | loss: 2.7134395Losses:  2.6884450912475586 2.6508865356445312
MemoryTrain:  epoch  8, batch     2 | loss: 2.6884451Losses:  2.4550271034240723 2.384411334991455
MemoryTrain:  epoch  8, batch     3 | loss: 2.4550271Losses:  2.6945858001708984 2.650315761566162
MemoryTrain:  epoch  9, batch     0 | loss: 2.6945858Losses:  2.6859660148620605 2.64387845993042
MemoryTrain:  epoch  9, batch     1 | loss: 2.6859660Losses:  2.7042348384857178 2.6693053245544434
MemoryTrain:  epoch  9, batch     2 | loss: 2.7042348Losses:  2.395987033843994 2.351938009262085
MemoryTrain:  epoch  9, batch     3 | loss: 2.3959870
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 73.08%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 68.45%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.61%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 66.81%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.67%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 88.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 87.83%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 86.96%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 85.14%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 84.78%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 84.42%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 83.94%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 83.14%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 82.65%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 82.17%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.48%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 82.89%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 82.22%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 81.89%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 81.65%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 81.33%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 81.10%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 80.79%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 80.27%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 79.69%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 79.12%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 78.49%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 77.66%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 77.56%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 77.64%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 77.82%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 77.85%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 77.66%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 76.76%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 76.48%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 76.01%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 75.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.94%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.42%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.84%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 73.28%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 72.73%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 72.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 71.82%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 72.97%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 73.29%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.81%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 72.34%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.83%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 71.33%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.01%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 71.90%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 71.59%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 71.02%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 70.81%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 70.43%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.22%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 71.23%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 71.14%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 71.05%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 71.03%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 70.94%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 70.87%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 70.68%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 70.48%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 70.47%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.44%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 70.36%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 70.40%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 70.89%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.42%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 71.44%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 71.33%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 71.26%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 70.89%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 70.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 70.37%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 70.28%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 70.09%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 69.75%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.72%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 70.46%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 70.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.09%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.07%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.89%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 72.75%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 72.67%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 72.51%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 72.45%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 72.34%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 72.63%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 72.38%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 72.30%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 72.01%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 71.80%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 71.59%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 71.36%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 71.27%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.26%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.30%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 72.16%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 71.87%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 71.74%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 71.65%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.83%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.68%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 72.67%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 72.46%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 72.31%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 72.12%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 71.92%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 71.73%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 72.03%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 71.95%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 71.63%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 71.60%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 71.62%   
cur_acc:  ['0.9435', '0.7510', '0.7321', '0.7817', '0.7143', '0.6667']
his_acc:  ['0.9435', '0.8400', '0.7836', '0.7728', '0.7524', '0.7162']
Clustering into  34  clusters
Clusters:  [ 6 30  0  1 27 25  0 12  2  2  0  2 26 13 33  5 16 13  3  5 14  5 15 19
  1 30 22  1 21 23 28 22 14  1 24  6 20 31 12 21 17 18  8  0 12  7  4 10
 29 16  7  5 32 11  8  3  9 10  1 22 30  5 22 33 15  3  4  1  4  1]
Losses:  7.295766830444336 1.728847622871399
CurrentTrain: epoch  0, batch     0 | loss: 7.2957668Losses:  8.958843231201172 1.7731242179870605
CurrentTrain: epoch  0, batch     1 | loss: 8.9588432Losses:  8.623932838439941 1.9827808141708374
CurrentTrain: epoch  0, batch     2 | loss: 8.6239328Losses:  9.459954261779785 0.6481295824050903
CurrentTrain: epoch  0, batch     3 | loss: 9.4599543Losses:  7.933690071105957 2.0661373138427734
CurrentTrain: epoch  1, batch     0 | loss: 7.9336901Losses:  7.558140754699707 1.8982517719268799
CurrentTrain: epoch  1, batch     1 | loss: 7.5581408Losses:  6.952049255371094 2.021395683288574
CurrentTrain: epoch  1, batch     2 | loss: 6.9520493Losses:  7.230876445770264 0.6407017111778259
CurrentTrain: epoch  1, batch     3 | loss: 7.2308764Losses:  6.879858016967773 1.8615272045135498
CurrentTrain: epoch  2, batch     0 | loss: 6.8798580Losses:  6.657991409301758 2.0109851360321045
CurrentTrain: epoch  2, batch     1 | loss: 6.6579914Losses:  7.043978691101074 2.178178548812866
CurrentTrain: epoch  2, batch     2 | loss: 7.0439787Losses:  7.593541145324707 0.663223385810852
CurrentTrain: epoch  2, batch     3 | loss: 7.5935411Losses:  6.600790977478027 1.805656909942627
CurrentTrain: epoch  3, batch     0 | loss: 6.6007910Losses:  6.774888038635254 2.049224853515625
CurrentTrain: epoch  3, batch     1 | loss: 6.7748880Losses:  6.457039833068848 2.0247411727905273
CurrentTrain: epoch  3, batch     2 | loss: 6.4570398Losses:  4.9520182609558105 0.6647459864616394
CurrentTrain: epoch  3, batch     3 | loss: 4.9520183Losses:  6.963263511657715 2.012162923812866
CurrentTrain: epoch  4, batch     0 | loss: 6.9632635Losses:  5.297457218170166 1.8051855564117432
CurrentTrain: epoch  4, batch     1 | loss: 5.2974572Losses:  6.284542560577393 2.087374687194824
CurrentTrain: epoch  4, batch     2 | loss: 6.2845426Losses:  6.603607177734375 0.6805093288421631
CurrentTrain: epoch  4, batch     3 | loss: 6.6036072Losses:  6.027873516082764 1.6174330711364746
CurrentTrain: epoch  5, batch     0 | loss: 6.0278735Losses:  5.763978481292725 2.0265417098999023
CurrentTrain: epoch  5, batch     1 | loss: 5.7639785Losses:  5.840372085571289 2.060387134552002
CurrentTrain: epoch  5, batch     2 | loss: 5.8403721Losses:  3.8122189044952393 0.6422168612480164
CurrentTrain: epoch  5, batch     3 | loss: 3.8122189Losses:  6.12821102142334 1.9645025730133057
CurrentTrain: epoch  6, batch     0 | loss: 6.1282110Losses:  5.5276970863342285 1.712764024734497
CurrentTrain: epoch  6, batch     1 | loss: 5.5276971Losses:  4.684878349304199 1.8074250221252441
CurrentTrain: epoch  6, batch     2 | loss: 4.6848783Losses:  5.659915447235107 0.6596990823745728
CurrentTrain: epoch  6, batch     3 | loss: 5.6599154Losses:  4.87054443359375 1.8942314386367798
CurrentTrain: epoch  7, batch     0 | loss: 4.8705444Losses:  5.830951690673828 1.8687740564346313
CurrentTrain: epoch  7, batch     1 | loss: 5.8309517Losses:  5.332304000854492 2.0547432899475098
CurrentTrain: epoch  7, batch     2 | loss: 5.3323040Losses:  4.7766523361206055 0.6784147024154663
CurrentTrain: epoch  7, batch     3 | loss: 4.7766523Losses:  5.088120937347412 1.913978099822998
CurrentTrain: epoch  8, batch     0 | loss: 5.0881209Losses:  5.040337562561035 1.9017882347106934
CurrentTrain: epoch  8, batch     1 | loss: 5.0403376Losses:  5.433864593505859 2.1718790531158447
CurrentTrain: epoch  8, batch     2 | loss: 5.4338646Losses:  3.353184461593628 0.6597838401794434
CurrentTrain: epoch  8, batch     3 | loss: 3.3531845Losses:  4.986196994781494 2.1034255027770996
CurrentTrain: epoch  9, batch     0 | loss: 4.9861970Losses:  5.570612907409668 2.039576530456543
CurrentTrain: epoch  9, batch     1 | loss: 5.5706129Losses:  4.837190628051758 1.9690475463867188
CurrentTrain: epoch  9, batch     2 | loss: 4.8371906Losses:  2.903635263442993 0.6713642477989197
CurrentTrain: epoch  9, batch     3 | loss: 2.9036353
Losses:  3.1585888862609863 2.6851303577423096
MemoryTrain:  epoch  0, batch     0 | loss: 3.1585889Losses:  3.0819456577301025 2.6609578132629395
MemoryTrain:  epoch  0, batch     1 | loss: 3.0819457Losses:  3.5497796535491943 2.6654868125915527
MemoryTrain:  epoch  0, batch     2 | loss: 3.5497797Losses:  2.8206069469451904 2.6840720176696777
MemoryTrain:  epoch  0, batch     3 | loss: 2.8206069Losses:  1.731878399848938 1.6287400722503662
MemoryTrain:  epoch  0, batch     4 | loss: 1.7318784Losses:  2.981431245803833 2.6539759635925293
MemoryTrain:  epoch  1, batch     0 | loss: 2.9814312Losses:  3.3182008266448975 2.6661505699157715
MemoryTrain:  epoch  1, batch     1 | loss: 3.3182008Losses:  3.1326112747192383 2.6752305030822754
MemoryTrain:  epoch  1, batch     2 | loss: 3.1326113Losses:  3.3947317600250244 2.658212184906006
MemoryTrain:  epoch  1, batch     3 | loss: 3.3947318Losses:  1.8396625518798828 1.682352066040039
MemoryTrain:  epoch  1, batch     4 | loss: 1.8396626Losses:  2.890094757080078 2.6572041511535645
MemoryTrain:  epoch  2, batch     0 | loss: 2.8900948Losses:  2.822784662246704 2.657134532928467
MemoryTrain:  epoch  2, batch     1 | loss: 2.8227847Losses:  2.8271563053131104 2.664222478866577
MemoryTrain:  epoch  2, batch     2 | loss: 2.8271563Losses:  3.2848613262176514 2.6668052673339844
MemoryTrain:  epoch  2, batch     3 | loss: 3.2848613Losses:  1.8260178565979004 1.7264573574066162
MemoryTrain:  epoch  2, batch     4 | loss: 1.8260179Losses:  2.805356025695801 2.6540627479553223
MemoryTrain:  epoch  3, batch     0 | loss: 2.8053560Losses:  2.814049243927002 2.637439250946045
MemoryTrain:  epoch  3, batch     1 | loss: 2.8140492Losses:  2.785747766494751 2.6725964546203613
MemoryTrain:  epoch  3, batch     2 | loss: 2.7857478Losses:  2.926931381225586 2.6700892448425293
MemoryTrain:  epoch  3, batch     3 | loss: 2.9269314Losses:  1.9066967964172363 1.702201247215271
MemoryTrain:  epoch  3, batch     4 | loss: 1.9066968Losses:  2.7022042274475098 2.652456760406494
MemoryTrain:  epoch  4, batch     0 | loss: 2.7022042Losses:  2.771876335144043 2.6557023525238037
MemoryTrain:  epoch  4, batch     1 | loss: 2.7718763Losses:  2.7185540199279785 2.662285566329956
MemoryTrain:  epoch  4, batch     2 | loss: 2.7185540Losses:  2.802936553955078 2.6720099449157715
MemoryTrain:  epoch  4, batch     3 | loss: 2.8029366Losses:  1.7512195110321045 1.6968228816986084
MemoryTrain:  epoch  4, batch     4 | loss: 1.7512195Losses:  2.7672603130340576 2.6537067890167236
MemoryTrain:  epoch  5, batch     0 | loss: 2.7672603Losses:  2.764575242996216 2.675412893295288
MemoryTrain:  epoch  5, batch     1 | loss: 2.7645752Losses:  2.7073230743408203 2.6487808227539062
MemoryTrain:  epoch  5, batch     2 | loss: 2.7073231Losses:  2.703503370285034 2.6607208251953125
MemoryTrain:  epoch  5, batch     3 | loss: 2.7035034Losses:  1.7038553953170776 1.665510892868042
MemoryTrain:  epoch  5, batch     4 | loss: 1.7038554Losses:  2.669361114501953 2.6358375549316406
MemoryTrain:  epoch  6, batch     0 | loss: 2.6693611Losses:  2.702153444290161 2.6594083309173584
MemoryTrain:  epoch  6, batch     1 | loss: 2.7021534Losses:  2.7490978240966797 2.6540722846984863
MemoryTrain:  epoch  6, batch     2 | loss: 2.7490978Losses:  2.73181414604187 2.6754751205444336
MemoryTrain:  epoch  6, batch     3 | loss: 2.7318141Losses:  1.7257850170135498 1.6807587146759033
MemoryTrain:  epoch  6, batch     4 | loss: 1.7257850Losses:  2.7057888507843018 2.665029525756836
MemoryTrain:  epoch  7, batch     0 | loss: 2.7057889Losses:  2.7126500606536865 2.665928363800049
MemoryTrain:  epoch  7, batch     1 | loss: 2.7126501Losses:  2.7011120319366455 2.648566722869873
MemoryTrain:  epoch  7, batch     2 | loss: 2.7011120Losses:  2.7144393920898438 2.643700122833252
MemoryTrain:  epoch  7, batch     3 | loss: 2.7144394Losses:  1.7045924663543701 1.6737759113311768
MemoryTrain:  epoch  7, batch     4 | loss: 1.7045925Losses:  2.7104532718658447 2.6653552055358887
MemoryTrain:  epoch  8, batch     0 | loss: 2.7104533Losses:  2.685499906539917 2.6491127014160156
MemoryTrain:  epoch  8, batch     1 | loss: 2.6854999Losses:  2.6949543952941895 2.6526405811309814
MemoryTrain:  epoch  8, batch     2 | loss: 2.6949544Losses:  2.6960432529449463 2.650034189224243
MemoryTrain:  epoch  8, batch     3 | loss: 2.6960433Losses:  1.7494628429412842 1.6808137893676758
MemoryTrain:  epoch  8, batch     4 | loss: 1.7494628Losses:  2.7141852378845215 2.659111738204956
MemoryTrain:  epoch  9, batch     0 | loss: 2.7141852Losses:  2.7073075771331787 2.6445822715759277
MemoryTrain:  epoch  9, batch     1 | loss: 2.7073076Losses:  2.7116589546203613 2.666473388671875
MemoryTrain:  epoch  9, batch     2 | loss: 2.7116590Losses:  2.6753122806549072 2.637690782546997
MemoryTrain:  epoch  9, batch     3 | loss: 2.6753123Losses:  1.7360694408416748 1.6929395198822021
MemoryTrain:  epoch  9, batch     4 | loss: 1.7360694
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 53.37%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 54.43%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 52.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.72%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 49.07%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.32%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 45.91%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 44.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 43.15%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 43.55%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 45.08%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 46.14%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 47.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 48.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 49.83%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 50.99%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 51.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 53.81%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 54.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 55.38%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 56.39%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 57.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 57.88%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 58.51%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 59.24%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 60.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 61.20%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 61.93%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 61.53%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 60.83%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 60.08%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 59.33%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.37%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 86.81%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 86.14%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 84.05%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 83.47%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 82.27%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 81.65%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 81.45%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 81.35%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 80.68%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 80.22%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 79.78%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 80.67%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 80.43%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 79.79%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 79.59%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.38%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 78.89%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 78.09%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 77.38%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 76.62%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 75.80%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 75.07%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 74.79%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 74.58%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 74.59%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 74.27%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 73.20%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 72.83%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 72.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 71.81%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.49%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 71.40%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 70.43%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 69.78%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 69.20%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 68.64%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 68.19%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 69.66%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 69.39%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 70.56%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 70.05%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.60%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 68.18%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 68.83%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.42%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 67.35%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 67.57%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 67.87%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.45%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 67.05%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 66.64%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 66.24%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 65.85%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 65.57%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 65.35%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 65.36%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 65.24%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 66.70%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 66.52%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 66.32%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 65.37%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 65.11%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 64.54%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 64.30%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.44%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 68.33%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.34%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 68.21%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 67.83%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 67.63%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.44%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 67.01%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 66.80%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 66.57%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 66.34%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 66.14%   [EVAL] batch:  292 | acc: 12.50%,  total acc: 65.96%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 67.16%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 66.91%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 66.76%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 66.58%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 66.70%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 66.64%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.43%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 67.91%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 67.72%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 67.56%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 67.37%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 67.32%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 67.24%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 67.18%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 67.10%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.99%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 66.92%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 66.68%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 66.57%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 66.21%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.46%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 66.66%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 66.44%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 66.33%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 66.05%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 65.75%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.44%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 65.29%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.13%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 65.11%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  421 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  432 | acc: 18.75%,  total acc: 66.22%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  435 | acc: 18.75%,  total acc: 66.00%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 65.85%   
cur_acc:  ['0.9435', '0.7510', '0.7321', '0.7817', '0.7143', '0.6667', '0.5933']
his_acc:  ['0.9435', '0.8400', '0.7836', '0.7728', '0.7524', '0.7162', '0.6585']
Clustering into  38  clusters
Clusters:  [ 7 34  5  0 33 27  5  0 26 26  2 26 28  9 30 14 18  9 12 14 15 14  3 21
  0 34 17  0 10 29  5 17 15  8 31  7 13 19  0 35 20 37 11 36  0  6  1  4
 32 18  6 14 16 24 11 12  2  4  8 17 34 14 17 30  3 12  1  5  1  8  5 26
 10 14 23 22 25 16 36  2]
Losses:  8.27209758758545 2.10418963432312
CurrentTrain: epoch  0, batch     0 | loss: 8.2720976Losses:  8.34028148651123 1.921269178390503
CurrentTrain: epoch  0, batch     1 | loss: 8.3402815Losses:  7.637877464294434 2.000437021255493
CurrentTrain: epoch  0, batch     2 | loss: 7.6378775Losses:  6.533463001251221 0.6373047232627869
CurrentTrain: epoch  0, batch     3 | loss: 6.5334630Losses:  7.075214385986328 1.8137009143829346
CurrentTrain: epoch  1, batch     0 | loss: 7.0752144Losses:  7.254627227783203 1.8853352069854736
CurrentTrain: epoch  1, batch     1 | loss: 7.2546272Losses:  7.133647918701172 1.8529207706451416
CurrentTrain: epoch  1, batch     2 | loss: 7.1336479Losses:  2.614978790283203 0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.6149788Losses:  6.587142467498779 1.902144432067871
CurrentTrain: epoch  2, batch     0 | loss: 6.5871425Losses:  6.972304344177246 1.892744779586792
CurrentTrain: epoch  2, batch     1 | loss: 6.9723043Losses:  6.641629219055176 1.8837764263153076
CurrentTrain: epoch  2, batch     2 | loss: 6.6416292Losses:  3.8185598850250244 0.62803715467453
CurrentTrain: epoch  2, batch     3 | loss: 3.8185599Losses:  6.675804138183594 2.061938762664795
CurrentTrain: epoch  3, batch     0 | loss: 6.6758041Losses:  6.213829040527344 1.8807780742645264
CurrentTrain: epoch  3, batch     1 | loss: 6.2138290Losses:  6.069461345672607 2.0364127159118652
CurrentTrain: epoch  3, batch     2 | loss: 6.0694613Losses:  6.446115493774414 0.6762464642524719
CurrentTrain: epoch  3, batch     3 | loss: 6.4461155Losses:  5.8360371589660645 2.031370162963867
CurrentTrain: epoch  4, batch     0 | loss: 5.8360372Losses:  5.5684895515441895 2.038431167602539
CurrentTrain: epoch  4, batch     1 | loss: 5.5684896Losses:  6.6468505859375 1.988505482673645
CurrentTrain: epoch  4, batch     2 | loss: 6.6468506Losses:  8.162168502807617 0.6660946011543274
CurrentTrain: epoch  4, batch     3 | loss: 8.1621685Losses:  6.2189435958862305 2.0739099979400635
CurrentTrain: epoch  5, batch     0 | loss: 6.2189436Losses:  5.863123416900635 1.8328298330307007
CurrentTrain: epoch  5, batch     1 | loss: 5.8631234Losses:  5.537291526794434 1.7720105648040771
CurrentTrain: epoch  5, batch     2 | loss: 5.5372915Losses:  2.9960217475891113 0.6287392973899841
CurrentTrain: epoch  5, batch     3 | loss: 2.9960217Losses:  6.243292808532715 2.1333110332489014
CurrentTrain: epoch  6, batch     0 | loss: 6.2432928Losses:  5.568580627441406 1.9630006551742554
CurrentTrain: epoch  6, batch     1 | loss: 5.5685806Losses:  4.906992435455322 1.9166274070739746
CurrentTrain: epoch  6, batch     2 | loss: 4.9069924Losses:  2.945244550704956 0.6592996120452881
CurrentTrain: epoch  6, batch     3 | loss: 2.9452446Losses:  5.482168197631836 1.9543399810791016
CurrentTrain: epoch  7, batch     0 | loss: 5.4821682Losses:  5.1373395919799805 1.9151962995529175
CurrentTrain: epoch  7, batch     1 | loss: 5.1373396Losses:  5.130756378173828 2.1253409385681152
CurrentTrain: epoch  7, batch     2 | loss: 5.1307564Losses:  3.5408239364624023 0.6196975708007812
CurrentTrain: epoch  7, batch     3 | loss: 3.5408239Losses:  5.4933366775512695 1.9238137006759644
CurrentTrain: epoch  8, batch     0 | loss: 5.4933367Losses:  4.672389507293701 1.8460633754730225
CurrentTrain: epoch  8, batch     1 | loss: 4.6723895Losses:  4.670332908630371 2.0355629920959473
CurrentTrain: epoch  8, batch     2 | loss: 4.6703329Losses:  4.032398700714111 0.6476489305496216
CurrentTrain: epoch  8, batch     3 | loss: 4.0323987Losses:  4.399295806884766 1.873467206954956
CurrentTrain: epoch  9, batch     0 | loss: 4.3992958Losses:  5.100352764129639 2.1338183879852295
CurrentTrain: epoch  9, batch     1 | loss: 5.1003528Losses:  4.6942925453186035 1.89971125125885
CurrentTrain: epoch  9, batch     2 | loss: 4.6942925Losses:  3.1362009048461914 0.6488733291625977
CurrentTrain: epoch  9, batch     3 | loss: 3.1362009
Losses:  2.783449172973633 2.6520018577575684
MemoryTrain:  epoch  0, batch     0 | loss: 2.7834492Losses:  3.4599592685699463 2.6697006225585938
MemoryTrain:  epoch  0, batch     1 | loss: 3.4599593Losses:  3.131552219390869 2.6893534660339355
MemoryTrain:  epoch  0, batch     2 | loss: 3.1315522Losses:  3.082376480102539 2.643185615539551
MemoryTrain:  epoch  0, batch     3 | loss: 3.0823765Losses:  3.1969919204711914 2.6642072200775146
MemoryTrain:  epoch  0, batch     4 | loss: 3.1969919Losses:  3.74194598197937 2.6579504013061523
MemoryTrain:  epoch  1, batch     0 | loss: 3.7419460Losses:  2.839470624923706 2.6672475337982178
MemoryTrain:  epoch  1, batch     1 | loss: 2.8394706Losses:  3.261483907699585 2.67482328414917
MemoryTrain:  epoch  1, batch     2 | loss: 3.2614839Losses:  3.433328151702881 2.6543655395507812
MemoryTrain:  epoch  1, batch     3 | loss: 3.4333282Losses:  2.994907855987549 2.6387438774108887
MemoryTrain:  epoch  1, batch     4 | loss: 2.9949079Losses:  2.9509291648864746 2.671130657196045
MemoryTrain:  epoch  2, batch     0 | loss: 2.9509292Losses:  3.227020025253296 2.67368221282959
MemoryTrain:  epoch  2, batch     1 | loss: 3.2270200Losses:  2.8978374004364014 2.655302047729492
MemoryTrain:  epoch  2, batch     2 | loss: 2.8978374Losses:  2.7168447971343994 2.651620388031006
MemoryTrain:  epoch  2, batch     3 | loss: 2.7168448Losses:  2.7767136096954346 2.644094944000244
MemoryTrain:  epoch  2, batch     4 | loss: 2.7767136Losses:  2.7793760299682617 2.6502106189727783
MemoryTrain:  epoch  3, batch     0 | loss: 2.7793760Losses:  2.8277335166931152 2.6621556282043457
MemoryTrain:  epoch  3, batch     1 | loss: 2.8277335Losses:  2.7887980937957764 2.665066719055176
MemoryTrain:  epoch  3, batch     2 | loss: 2.7887981Losses:  2.7635622024536133 2.6515393257141113
MemoryTrain:  epoch  3, batch     3 | loss: 2.7635622Losses:  2.942225217819214 2.6604156494140625
MemoryTrain:  epoch  3, batch     4 | loss: 2.9422252Losses:  2.819343328475952 2.6676435470581055
MemoryTrain:  epoch  4, batch     0 | loss: 2.8193433Losses:  2.7248995304107666 2.6312384605407715
MemoryTrain:  epoch  4, batch     1 | loss: 2.7248995Losses:  2.75553560256958 2.645887851715088
MemoryTrain:  epoch  4, batch     2 | loss: 2.7555356Losses:  2.7990570068359375 2.6738898754119873
MemoryTrain:  epoch  4, batch     3 | loss: 2.7990570Losses:  2.7736432552337646 2.660189628601074
MemoryTrain:  epoch  4, batch     4 | loss: 2.7736433Losses:  2.705183267593384 2.6393423080444336
MemoryTrain:  epoch  5, batch     0 | loss: 2.7051833Losses:  2.671757936477661 2.6366119384765625
MemoryTrain:  epoch  5, batch     1 | loss: 2.6717579Losses:  2.7456607818603516 2.6600475311279297
MemoryTrain:  epoch  5, batch     2 | loss: 2.7456608Losses:  2.749978542327881 2.6514053344726562
MemoryTrain:  epoch  5, batch     3 | loss: 2.7499785Losses:  2.783977746963501 2.6720094680786133
MemoryTrain:  epoch  5, batch     4 | loss: 2.7839777Losses:  2.7536230087280273 2.6471099853515625
MemoryTrain:  epoch  6, batch     0 | loss: 2.7536230Losses:  2.689103364944458 2.64243745803833
MemoryTrain:  epoch  6, batch     1 | loss: 2.6891034Losses:  2.727074384689331 2.6499147415161133
MemoryTrain:  epoch  6, batch     2 | loss: 2.7270744Losses:  2.7465806007385254 2.6686103343963623
MemoryTrain:  epoch  6, batch     3 | loss: 2.7465806Losses:  2.6958141326904297 2.650597095489502
MemoryTrain:  epoch  6, batch     4 | loss: 2.6958141Losses:  2.663768768310547 2.6182961463928223
MemoryTrain:  epoch  7, batch     0 | loss: 2.6637688Losses:  2.728811025619507 2.65918231010437
MemoryTrain:  epoch  7, batch     1 | loss: 2.7288110Losses:  2.693519115447998 2.6620683670043945
MemoryTrain:  epoch  7, batch     2 | loss: 2.6935191Losses:  2.7184898853302 2.6655969619750977
MemoryTrain:  epoch  7, batch     3 | loss: 2.7184899Losses:  2.713569164276123 2.656857967376709
MemoryTrain:  epoch  7, batch     4 | loss: 2.7135692Losses:  2.6980652809143066 2.657208204269409
MemoryTrain:  epoch  8, batch     0 | loss: 2.6980653Losses:  2.652200937271118 2.6206955909729004
MemoryTrain:  epoch  8, batch     1 | loss: 2.6522009Losses:  2.747650384902954 2.671910285949707
MemoryTrain:  epoch  8, batch     2 | loss: 2.7476504Losses:  2.6885032653808594 2.6566476821899414
MemoryTrain:  epoch  8, batch     3 | loss: 2.6885033Losses:  2.6743578910827637 2.6449766159057617
MemoryTrain:  epoch  8, batch     4 | loss: 2.6743579Losses:  2.697946310043335 2.6624393463134766
MemoryTrain:  epoch  9, batch     0 | loss: 2.6979463Losses:  2.69270658493042 2.641458034515381
MemoryTrain:  epoch  9, batch     1 | loss: 2.6927066Losses:  2.700571060180664 2.6529593467712402
MemoryTrain:  epoch  9, batch     2 | loss: 2.7005711Losses:  2.679107427597046 2.6416587829589844
MemoryTrain:  epoch  9, batch     3 | loss: 2.6791074Losses:  2.6801602840423584 2.649000883102417
MemoryTrain:  epoch  9, batch     4 | loss: 2.6801603
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 75.15%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 74.55%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 74.42%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 71.60%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 71.01%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 70.55%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 69.69%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 68.35%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 67.56%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.32%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.57%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 84.01%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 83.96%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 82.48%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 81.69%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 80.71%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 79.98%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 77.58%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 76.06%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 73.97%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 73.16%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 72.64%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 73.64%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 72.67%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.95%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 71.32%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 70.71%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.11%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 69.82%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 69.65%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 69.51%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 69.22%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 68.49%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 68.10%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 67.91%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 67.47%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 65.95%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 65.39%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 64.20%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 63.68%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 63.34%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 63.22%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 65.16%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 64.74%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 64.02%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 65.11%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 64.68%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 64.16%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 63.62%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 63.65%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 63.91%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 63.52%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 63.14%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 62.76%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 62.13%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 62.17%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 62.25%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 62.39%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 62.14%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 62.04%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 61.87%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 61.77%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 61.53%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 61.53%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 62.57%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 62.73%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 63.09%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 63.27%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 63.11%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 62.91%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 62.75%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 62.66%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 62.66%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 62.65%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 62.65%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 62.44%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 62.38%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 61.90%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 61.46%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 61.29%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 61.08%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 61.09%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 61.63%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 62.13%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.22%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 62.30%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 62.33%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 62.42%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 62.36%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 62.53%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.77%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.78%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.67%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 65.60%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 65.34%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 65.26%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 65.72%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 65.59%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  278 | acc: 37.50%,  total acc: 65.46%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 65.33%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 65.24%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 65.16%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 64.88%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 64.51%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 64.08%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 63.88%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 63.66%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 63.45%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 63.25%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 63.05%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 62.94%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 62.96%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 63.09%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 64.54%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 64.41%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 64.16%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 63.98%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 63.82%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 63.73%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.84%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 63.98%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 63.80%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 63.72%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 63.48%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 64.13%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 64.36%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 64.90%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 64.72%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 64.54%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 64.17%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 63.99%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 64.02%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 64.42%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 64.35%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 64.33%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 64.25%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 64.13%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 64.03%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 64.01%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.05%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 64.05%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 63.91%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 63.81%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 63.66%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 63.60%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 63.47%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 63.34%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.33%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 63.52%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.55%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  391 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  394 | acc: 6.25%,  total acc: 63.84%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 63.72%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 63.57%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 63.44%   [EVAL] batch:  398 | acc: 0.00%,  total acc: 63.28%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 63.14%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.00%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 62.86%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.70%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 62.56%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.41%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.25%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 62.32%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 62.52%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 62.68%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 62.83%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 62.89%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 63.20%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 63.34%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 63.35%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 63.37%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.36%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.38%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 63.25%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 63.19%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  435 | acc: 18.75%,  total acc: 63.04%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 63.00%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 63.09%   [EVAL] batch:  442 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  446 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 63.20%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 63.21%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 63.25%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 63.30%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 63.34%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 63.53%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 63.47%   [EVAL] batch:  458 | acc: 18.75%,  total acc: 63.37%   [EVAL] batch:  459 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 63.25%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 63.19%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 64.08%   [EVAL] batch:  476 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 64.03%   [EVAL] batch:  478 | acc: 50.00%,  total acc: 64.00%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 64.01%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 63.99%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  482 | acc: 18.75%,  total acc: 63.85%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 63.79%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 63.73%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 63.93%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:  494 | acc: 31.25%,  total acc: 63.96%   [EVAL] batch:  495 | acc: 31.25%,  total acc: 63.90%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 63.86%   [EVAL] batch:  497 | acc: 25.00%,  total acc: 63.78%   [EVAL] batch:  498 | acc: 18.75%,  total acc: 63.69%   [EVAL] batch:  499 | acc: 31.25%,  total acc: 63.62%   
cur_acc:  ['0.9435', '0.7510', '0.7321', '0.7817', '0.7143', '0.6667', '0.5933', '0.6756']
his_acc:  ['0.9435', '0.8400', '0.7836', '0.7728', '0.7524', '0.7162', '0.6585', '0.6362']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  12.320939064025879 1.9253628253936768
CurrentTrain: epoch  0, batch     0 | loss: 12.3209391Losses:  12.043356895446777 2.0885744094848633
CurrentTrain: epoch  0, batch     1 | loss: 12.0433569Losses:  11.52326488494873 1.787480115890503
CurrentTrain: epoch  0, batch     2 | loss: 11.5232649Losses:  12.219881057739258 2.1989998817443848
CurrentTrain: epoch  0, batch     3 | loss: 12.2198811Losses:  11.827938079833984 1.9093050956726074
CurrentTrain: epoch  0, batch     4 | loss: 11.8279381Losses:  11.176996231079102 1.9965665340423584
CurrentTrain: epoch  0, batch     5 | loss: 11.1769962Losses:  11.471430778503418 1.8926995992660522
CurrentTrain: epoch  0, batch     6 | loss: 11.4714308Losses:  11.096760749816895 1.8415985107421875
CurrentTrain: epoch  0, batch     7 | loss: 11.0967607Losses:  11.558794021606445 2.053853750228882
CurrentTrain: epoch  0, batch     8 | loss: 11.5587940Losses:  10.974002838134766 2.02012038230896
CurrentTrain: epoch  0, batch     9 | loss: 10.9740028Losses:  11.569525718688965 2.0711286067962646
CurrentTrain: epoch  0, batch    10 | loss: 11.5695257Losses:  10.671391487121582 2.0332136154174805
CurrentTrain: epoch  0, batch    11 | loss: 10.6713915Losses:  10.5765380859375 1.7905782461166382
CurrentTrain: epoch  0, batch    12 | loss: 10.5765381Losses:  10.708693504333496 2.0666377544403076
CurrentTrain: epoch  0, batch    13 | loss: 10.7086935Losses:  10.925827980041504 1.8331043720245361
CurrentTrain: epoch  0, batch    14 | loss: 10.9258280Losses:  10.165472030639648 2.012495279312134
CurrentTrain: epoch  0, batch    15 | loss: 10.1654720Losses:  10.646049499511719 2.0730645656585693
CurrentTrain: epoch  0, batch    16 | loss: 10.6460495Losses:  10.254289627075195 1.8302828073501587
CurrentTrain: epoch  0, batch    17 | loss: 10.2542896Losses:  10.596220016479492 1.7873854637145996
CurrentTrain: epoch  0, batch    18 | loss: 10.5962200Losses:  9.90095329284668 2.0180602073669434
CurrentTrain: epoch  0, batch    19 | loss: 9.9009533Losses:  9.851794242858887 1.8323535919189453
CurrentTrain: epoch  0, batch    20 | loss: 9.8517942Losses:  10.318522453308105 1.866490125656128
CurrentTrain: epoch  0, batch    21 | loss: 10.3185225Losses:  9.555028915405273 1.6736305952072144
CurrentTrain: epoch  0, batch    22 | loss: 9.5550289Losses:  9.983268737792969 1.989854335784912
CurrentTrain: epoch  0, batch    23 | loss: 9.9832687Losses:  10.650634765625 2.066000461578369
CurrentTrain: epoch  0, batch    24 | loss: 10.6506348Losses:  9.706335067749023 1.718624472618103
CurrentTrain: epoch  0, batch    25 | loss: 9.7063351Losses:  10.444920539855957 2.0119991302490234
CurrentTrain: epoch  0, batch    26 | loss: 10.4449205Losses:  10.620189666748047 2.0792086124420166
CurrentTrain: epoch  0, batch    27 | loss: 10.6201897Losses:  10.114112854003906 1.89354407787323
CurrentTrain: epoch  0, batch    28 | loss: 10.1141129Losses:  9.8483247756958 1.7706120014190674
CurrentTrain: epoch  0, batch    29 | loss: 9.8483248Losses:  9.910028457641602 2.0286831855773926
CurrentTrain: epoch  0, batch    30 | loss: 9.9100285Losses:  10.254396438598633 2.1875364780426025
CurrentTrain: epoch  0, batch    31 | loss: 10.2543964Losses:  10.47146224975586 2.065814733505249
CurrentTrain: epoch  0, batch    32 | loss: 10.4714622Losses:  10.028681755065918 2.095036506652832
CurrentTrain: epoch  0, batch    33 | loss: 10.0286818Losses:  9.681631088256836 2.0471224784851074
CurrentTrain: epoch  0, batch    34 | loss: 9.6816311Losses:  9.084551811218262 1.8785687685012817
CurrentTrain: epoch  0, batch    35 | loss: 9.0845518Losses:  9.776763916015625 1.6306039094924927
CurrentTrain: epoch  0, batch    36 | loss: 9.7767639Losses:  9.539539337158203 2.008309841156006
CurrentTrain: epoch  0, batch    37 | loss: 9.5395393Losses:  9.895519256591797 2.0652482509613037
CurrentTrain: epoch  0, batch    38 | loss: 9.8955193Losses:  10.055326461791992 2.056234121322632
CurrentTrain: epoch  0, batch    39 | loss: 10.0553265Losses:  9.6556978225708 2.0229055881500244
CurrentTrain: epoch  0, batch    40 | loss: 9.6556978Losses:  9.558837890625 1.9740705490112305
CurrentTrain: epoch  0, batch    41 | loss: 9.5588379Losses:  9.339550971984863 1.867518424987793
CurrentTrain: epoch  0, batch    42 | loss: 9.3395510Losses:  9.593077659606934 1.851241111755371
CurrentTrain: epoch  0, batch    43 | loss: 9.5930777Losses:  9.196403503417969 1.848973035812378
CurrentTrain: epoch  0, batch    44 | loss: 9.1964035Losses:  9.525945663452148 1.8573408126831055
CurrentTrain: epoch  0, batch    45 | loss: 9.5259457Losses:  9.672826766967773 1.9130902290344238
CurrentTrain: epoch  0, batch    46 | loss: 9.6728268Losses:  9.124312400817871 1.9645781517028809
CurrentTrain: epoch  0, batch    47 | loss: 9.1243124Losses:  9.745495796203613 1.9205615520477295
CurrentTrain: epoch  0, batch    48 | loss: 9.7454958Losses:  9.085163116455078 2.030569076538086
CurrentTrain: epoch  0, batch    49 | loss: 9.0851631Losses:  10.124286651611328 1.5264849662780762
CurrentTrain: epoch  0, batch    50 | loss: 10.1242867Losses:  8.502859115600586 1.8069744110107422
CurrentTrain: epoch  0, batch    51 | loss: 8.5028591Losses:  9.369003295898438 2.0136518478393555
CurrentTrain: epoch  0, batch    52 | loss: 9.3690033Losses:  9.160669326782227 1.678598403930664
CurrentTrain: epoch  0, batch    53 | loss: 9.1606693Losses:  10.645626068115234 1.9516172409057617
CurrentTrain: epoch  0, batch    54 | loss: 10.6456261Losses:  10.31761646270752 1.9507124423980713
CurrentTrain: epoch  0, batch    55 | loss: 10.3176165Losses:  8.580365180969238 1.908584713935852
CurrentTrain: epoch  0, batch    56 | loss: 8.5803652Losses:  8.514065742492676 1.7500934600830078
CurrentTrain: epoch  0, batch    57 | loss: 8.5140657Losses:  9.267128944396973 1.9776769876480103
CurrentTrain: epoch  0, batch    58 | loss: 9.2671289Losses:  9.44949722290039 1.6517128944396973
CurrentTrain: epoch  0, batch    59 | loss: 9.4494972Losses:  9.344548225402832 1.922886610031128
CurrentTrain: epoch  0, batch    60 | loss: 9.3445482Losses:  8.85901165008545 1.8504886627197266
CurrentTrain: epoch  0, batch    61 | loss: 8.8590117Losses:  8.291149139404297 1.661859154701233
CurrentTrain: epoch  0, batch    62 | loss: 8.2911491Losses:  10.337631225585938 1.8668978214263916
CurrentTrain: epoch  1, batch     0 | loss: 10.3376312Losses:  8.291543006896973 1.9193826913833618
CurrentTrain: epoch  1, batch     1 | loss: 8.2915430Losses:  9.241493225097656 2.0114712715148926
CurrentTrain: epoch  1, batch     2 | loss: 9.2414932Losses:  8.534614562988281 1.7308440208435059
CurrentTrain: epoch  1, batch     3 | loss: 8.5346146Losses:  8.256171226501465 1.9754726886749268
CurrentTrain: epoch  1, batch     4 | loss: 8.2561712Losses:  8.005708694458008 1.7519993782043457
CurrentTrain: epoch  1, batch     5 | loss: 8.0057087Losses:  9.00490951538086 1.7682918310165405
CurrentTrain: epoch  1, batch     6 | loss: 9.0049095Losses:  9.023919105529785 1.937917947769165
CurrentTrain: epoch  1, batch     7 | loss: 9.0239191Losses:  9.036096572875977 2.0351715087890625
CurrentTrain: epoch  1, batch     8 | loss: 9.0360966Losses:  7.9316253662109375 1.4500608444213867
CurrentTrain: epoch  1, batch     9 | loss: 7.9316254Losses:  8.698495864868164 1.8710730075836182
CurrentTrain: epoch  1, batch    10 | loss: 8.6984959Losses:  9.644872665405273 1.9949997663497925
CurrentTrain: epoch  1, batch    11 | loss: 9.6448727Losses:  8.70096206665039 1.8790709972381592
CurrentTrain: epoch  1, batch    12 | loss: 8.7009621Losses:  8.773088455200195 1.8926124572753906
CurrentTrain: epoch  1, batch    13 | loss: 8.7730885Losses:  9.229450225830078 2.05387020111084
CurrentTrain: epoch  1, batch    14 | loss: 9.2294502Losses:  8.730472564697266 1.828438401222229
CurrentTrain: epoch  1, batch    15 | loss: 8.7304726Losses:  8.380642890930176 1.9960527420043945
CurrentTrain: epoch  1, batch    16 | loss: 8.3806429Losses:  9.675942420959473 1.9159232378005981
CurrentTrain: epoch  1, batch    17 | loss: 9.6759424Losses:  8.600412368774414 2.0514965057373047
CurrentTrain: epoch  1, batch    18 | loss: 8.6004124Losses:  8.534942626953125 1.990981101989746
CurrentTrain: epoch  1, batch    19 | loss: 8.5349426Losses:  8.80661392211914 2.0587098598480225
CurrentTrain: epoch  1, batch    20 | loss: 8.8066139Losses:  8.709868431091309 2.053820848464966
CurrentTrain: epoch  1, batch    21 | loss: 8.7098684Losses:  8.37590217590332 2.030831813812256
CurrentTrain: epoch  1, batch    22 | loss: 8.3759022Losses:  8.684462547302246 1.9297935962677002
CurrentTrain: epoch  1, batch    23 | loss: 8.6844625Losses:  8.883292198181152 1.9713212251663208
CurrentTrain: epoch  1, batch    24 | loss: 8.8832922Losses:  8.148582458496094 2.017514705657959
CurrentTrain: epoch  1, batch    25 | loss: 8.1485825Losses:  9.151445388793945 1.9154536724090576
CurrentTrain: epoch  1, batch    26 | loss: 9.1514454Losses:  7.6628923416137695 1.7181358337402344
CurrentTrain: epoch  1, batch    27 | loss: 7.6628923Losses:  7.570077896118164 1.7435448169708252
CurrentTrain: epoch  1, batch    28 | loss: 7.5700779Losses:  8.36577033996582 2.0539121627807617
CurrentTrain: epoch  1, batch    29 | loss: 8.3657703Losses:  8.513314247131348 2.025970697402954
CurrentTrain: epoch  1, batch    30 | loss: 8.5133142Losses:  8.580272674560547 1.854212760925293
CurrentTrain: epoch  1, batch    31 | loss: 8.5802727Losses:  7.811164855957031 1.6675184965133667
CurrentTrain: epoch  1, batch    32 | loss: 7.8111649Losses:  8.186416625976562 1.964005947113037
CurrentTrain: epoch  1, batch    33 | loss: 8.1864166Losses:  8.412744522094727 1.9322553873062134
CurrentTrain: epoch  1, batch    34 | loss: 8.4127445Losses:  8.637503623962402 1.99358069896698
CurrentTrain: epoch  1, batch    35 | loss: 8.6375036Losses:  8.344306945800781 2.140087127685547
CurrentTrain: epoch  1, batch    36 | loss: 8.3443069Losses:  8.183439254760742 2.0527544021606445
CurrentTrain: epoch  1, batch    37 | loss: 8.1834393Losses:  8.8840970993042 2.1510121822357178
CurrentTrain: epoch  1, batch    38 | loss: 8.8840971Losses:  6.780614852905273 1.4378421306610107
CurrentTrain: epoch  1, batch    39 | loss: 6.7806149Losses:  8.571186065673828 1.8857109546661377
CurrentTrain: epoch  1, batch    40 | loss: 8.5711861Losses:  8.961400032043457 2.0395328998565674
CurrentTrain: epoch  1, batch    41 | loss: 8.9614000Losses:  7.697259902954102 2.0118236541748047
CurrentTrain: epoch  1, batch    42 | loss: 7.6972599Losses:  8.389506340026855 2.054144859313965
CurrentTrain: epoch  1, batch    43 | loss: 8.3895063Losses:  9.189350128173828 1.9529523849487305
CurrentTrain: epoch  1, batch    44 | loss: 9.1893501Losses:  8.054344177246094 2.047490119934082
CurrentTrain: epoch  1, batch    45 | loss: 8.0543442Losses:  7.765761375427246 1.741067886352539
CurrentTrain: epoch  1, batch    46 | loss: 7.7657614Losses:  7.970037460327148 1.871158480644226
CurrentTrain: epoch  1, batch    47 | loss: 7.9700375Losses:  8.949621200561523 1.816019058227539
CurrentTrain: epoch  1, batch    48 | loss: 8.9496212Losses:  7.799050807952881 1.9719661474227905
CurrentTrain: epoch  1, batch    49 | loss: 7.7990508Losses:  7.735546112060547 1.9349110126495361
CurrentTrain: epoch  1, batch    50 | loss: 7.7355461Losses:  6.805608749389648 1.5953037738800049
CurrentTrain: epoch  1, batch    51 | loss: 6.8056087Losses:  7.856903553009033 1.7632077932357788
CurrentTrain: epoch  1, batch    52 | loss: 7.8569036Losses:  7.104640007019043 1.862507700920105
CurrentTrain: epoch  1, batch    53 | loss: 7.1046400Losses:  8.62596321105957 1.9868998527526855
CurrentTrain: epoch  1, batch    54 | loss: 8.6259632Losses:  7.286556243896484 1.6918866634368896
CurrentTrain: epoch  1, batch    55 | loss: 7.2865562Losses:  8.160140991210938 1.9465290307998657
CurrentTrain: epoch  1, batch    56 | loss: 8.1601410Losses:  8.180392265319824 1.8694850206375122
CurrentTrain: epoch  1, batch    57 | loss: 8.1803923Losses:  7.8536176681518555 1.909825325012207
CurrentTrain: epoch  1, batch    58 | loss: 7.8536177Losses:  7.504696846008301 1.9361684322357178
CurrentTrain: epoch  1, batch    59 | loss: 7.5046968Losses:  7.477985858917236 1.9887183904647827
CurrentTrain: epoch  1, batch    60 | loss: 7.4779859Losses:  8.271167755126953 1.888174295425415
CurrentTrain: epoch  1, batch    61 | loss: 8.2711678Losses:  8.193435668945312 1.4266998767852783
CurrentTrain: epoch  1, batch    62 | loss: 8.1934357Losses:  7.593574523925781 1.8922312259674072
CurrentTrain: epoch  2, batch     0 | loss: 7.5935745Losses:  7.226202964782715 2.0091500282287598
CurrentTrain: epoch  2, batch     1 | loss: 7.2262030Losses:  7.077308654785156 1.9352720975875854
CurrentTrain: epoch  2, batch     2 | loss: 7.0773087Losses:  7.1550493240356445 1.8843729496002197
CurrentTrain: epoch  2, batch     3 | loss: 7.1550493Losses:  7.134820938110352 1.8607558012008667
CurrentTrain: epoch  2, batch     4 | loss: 7.1348209Losses:  7.837984085083008 1.8386943340301514
CurrentTrain: epoch  2, batch     5 | loss: 7.8379841Losses:  7.222995758056641 1.9088809490203857
CurrentTrain: epoch  2, batch     6 | loss: 7.2229958Losses:  8.2874116897583 2.0647714138031006
CurrentTrain: epoch  2, batch     7 | loss: 8.2874117Losses:  7.5684003829956055 1.674830675125122
CurrentTrain: epoch  2, batch     8 | loss: 7.5684004Losses:  6.879971027374268 1.8561474084854126
CurrentTrain: epoch  2, batch     9 | loss: 6.8799710Losses:  7.381166458129883 1.842378854751587
CurrentTrain: epoch  2, batch    10 | loss: 7.3811665Losses:  7.285000801086426 1.9692450761795044
CurrentTrain: epoch  2, batch    11 | loss: 7.2850008Losses:  7.265419960021973 1.9020864963531494
CurrentTrain: epoch  2, batch    12 | loss: 7.2654200Losses:  7.360637187957764 1.9013571739196777
CurrentTrain: epoch  2, batch    13 | loss: 7.3606372Losses:  7.603045463562012 1.8062803745269775
CurrentTrain: epoch  2, batch    14 | loss: 7.6030455Losses:  7.041354656219482 1.9298032522201538
CurrentTrain: epoch  2, batch    15 | loss: 7.0413547Losses:  7.354680061340332 1.764365315437317
CurrentTrain: epoch  2, batch    16 | loss: 7.3546801Losses:  8.408536911010742 1.828319787979126
CurrentTrain: epoch  2, batch    17 | loss: 8.4085369Losses:  6.923899173736572 1.7381128072738647
CurrentTrain: epoch  2, batch    18 | loss: 6.9238992Losses:  7.678236484527588 2.002903938293457
CurrentTrain: epoch  2, batch    19 | loss: 7.6782365Losses:  8.164984703063965 1.4455633163452148
CurrentTrain: epoch  2, batch    20 | loss: 8.1649847Losses:  7.927718162536621 2.097381591796875
CurrentTrain: epoch  2, batch    21 | loss: 7.9277182Losses:  6.946998596191406 2.0433883666992188
CurrentTrain: epoch  2, batch    22 | loss: 6.9469986Losses:  7.4100542068481445 1.9031870365142822
CurrentTrain: epoch  2, batch    23 | loss: 7.4100542Losses:  6.8752617835998535 1.9125076532363892
CurrentTrain: epoch  2, batch    24 | loss: 6.8752618Losses:  6.788371562957764 1.4894194602966309
CurrentTrain: epoch  2, batch    25 | loss: 6.7883716Losses:  6.759486198425293 1.9933500289916992
CurrentTrain: epoch  2, batch    26 | loss: 6.7594862Losses:  7.242794990539551 2.022268056869507
CurrentTrain: epoch  2, batch    27 | loss: 7.2427950Losses:  6.6222710609436035 1.6554678678512573
CurrentTrain: epoch  2, batch    28 | loss: 6.6222711Losses:  7.79025936126709 2.170750379562378
CurrentTrain: epoch  2, batch    29 | loss: 7.7902594Losses:  7.341249465942383 2.016737937927246
CurrentTrain: epoch  2, batch    30 | loss: 7.3412495Losses:  7.696434497833252 2.0203700065612793
CurrentTrain: epoch  2, batch    31 | loss: 7.6964345Losses:  7.110629558563232 1.7592897415161133
CurrentTrain: epoch  2, batch    32 | loss: 7.1106296Losses:  7.288687705993652 1.8626008033752441
CurrentTrain: epoch  2, batch    33 | loss: 7.2886877Losses:  8.163715362548828 1.9391684532165527
CurrentTrain: epoch  2, batch    34 | loss: 8.1637154Losses:  7.211040496826172 1.8390685319900513
CurrentTrain: epoch  2, batch    35 | loss: 7.2110405Losses:  8.033496856689453 2.131657600402832
CurrentTrain: epoch  2, batch    36 | loss: 8.0334969Losses:  7.286369323730469 2.051590919494629
CurrentTrain: epoch  2, batch    37 | loss: 7.2863693Losses:  7.40463924407959 1.9259369373321533
CurrentTrain: epoch  2, batch    38 | loss: 7.4046392Losses:  7.766664505004883 2.139120101928711
CurrentTrain: epoch  2, batch    39 | loss: 7.7666645Losses:  8.210527420043945 2.013695478439331
CurrentTrain: epoch  2, batch    40 | loss: 8.2105274Losses:  6.943934440612793 1.9596338272094727
CurrentTrain: epoch  2, batch    41 | loss: 6.9439344Losses:  7.078886032104492 1.880873441696167
CurrentTrain: epoch  2, batch    42 | loss: 7.0788860Losses:  7.515069007873535 1.9487998485565186
CurrentTrain: epoch  2, batch    43 | loss: 7.5150690Losses:  6.8879570960998535 1.9238271713256836
CurrentTrain: epoch  2, batch    44 | loss: 6.8879571Losses:  6.485312461853027 1.6002514362335205
CurrentTrain: epoch  2, batch    45 | loss: 6.4853125Losses:  7.484326362609863 1.9690206050872803
CurrentTrain: epoch  2, batch    46 | loss: 7.4843264Losses:  6.7325825691223145 1.7153515815734863
CurrentTrain: epoch  2, batch    47 | loss: 6.7325826Losses:  7.429931640625 1.859410047531128
CurrentTrain: epoch  2, batch    48 | loss: 7.4299316Losses:  7.184952735900879 1.6935746669769287
CurrentTrain: epoch  2, batch    49 | loss: 7.1849527Losses:  6.938666343688965 1.7525193691253662
CurrentTrain: epoch  2, batch    50 | loss: 6.9386663Losses:  7.1984663009643555 1.8478467464447021
CurrentTrain: epoch  2, batch    51 | loss: 7.1984663Losses:  7.0996198654174805 1.7306249141693115
CurrentTrain: epoch  2, batch    52 | loss: 7.0996199Losses:  7.131993293762207 1.8208539485931396
CurrentTrain: epoch  2, batch    53 | loss: 7.1319933Losses:  7.273769378662109 1.694885015487671
CurrentTrain: epoch  2, batch    54 | loss: 7.2737694Losses:  7.3965044021606445 1.8057899475097656
CurrentTrain: epoch  2, batch    55 | loss: 7.3965044Losses:  7.0625410079956055 1.865508794784546
CurrentTrain: epoch  2, batch    56 | loss: 7.0625410Losses:  7.374505996704102 1.8667000532150269
CurrentTrain: epoch  2, batch    57 | loss: 7.3745060Losses:  6.736032485961914 1.863747239112854
CurrentTrain: epoch  2, batch    58 | loss: 6.7360325Losses:  6.997819900512695 2.0736446380615234
CurrentTrain: epoch  2, batch    59 | loss: 6.9978199Losses:  6.644171237945557 1.8688549995422363
CurrentTrain: epoch  2, batch    60 | loss: 6.6441712Losses:  6.452017784118652 1.7741119861602783
CurrentTrain: epoch  2, batch    61 | loss: 6.4520178Losses:  6.207493782043457 1.409529209136963
CurrentTrain: epoch  2, batch    62 | loss: 6.2074938Losses:  6.728118896484375 1.8420696258544922
CurrentTrain: epoch  3, batch     0 | loss: 6.7281189Losses:  6.640377998352051 1.821929693222046
CurrentTrain: epoch  3, batch     1 | loss: 6.6403780Losses:  6.618452072143555 1.7977180480957031
CurrentTrain: epoch  3, batch     2 | loss: 6.6184521Losses:  6.435725212097168 1.863839864730835
CurrentTrain: epoch  3, batch     3 | loss: 6.4357252Losses:  6.72786808013916 1.8512523174285889
CurrentTrain: epoch  3, batch     4 | loss: 6.7278681Losses:  7.365653038024902 2.086411952972412
CurrentTrain: epoch  3, batch     5 | loss: 7.3656530Losses:  6.876543045043945 2.085164785385132
CurrentTrain: epoch  3, batch     6 | loss: 6.8765430Losses:  6.9029998779296875 1.8850417137145996
CurrentTrain: epoch  3, batch     7 | loss: 6.9029999Losses:  6.774196624755859 1.79482102394104
CurrentTrain: epoch  3, batch     8 | loss: 6.7741966Losses:  6.964456081390381 1.9357404708862305
CurrentTrain: epoch  3, batch     9 | loss: 6.9644561Losses:  7.077068328857422 1.7581613063812256
CurrentTrain: epoch  3, batch    10 | loss: 7.0770683Losses:  6.590060234069824 1.7982709407806396
CurrentTrain: epoch  3, batch    11 | loss: 6.5900602Losses:  7.095149993896484 2.1016156673431396
CurrentTrain: epoch  3, batch    12 | loss: 7.0951500Losses:  6.439091682434082 1.7745996713638306
CurrentTrain: epoch  3, batch    13 | loss: 6.4390917Losses:  7.717701435089111 1.9898072481155396
CurrentTrain: epoch  3, batch    14 | loss: 7.7177014Losses:  7.397966384887695 1.9313260316848755
CurrentTrain: epoch  3, batch    15 | loss: 7.3979664Losses:  7.378935813903809 2.0259206295013428
CurrentTrain: epoch  3, batch    16 | loss: 7.3789358Losses:  6.564975738525391 1.6687352657318115
CurrentTrain: epoch  3, batch    17 | loss: 6.5649757Losses:  6.74641227722168 1.8084536790847778
CurrentTrain: epoch  3, batch    18 | loss: 6.7464123Losses:  6.660845756530762 1.9623783826828003
CurrentTrain: epoch  3, batch    19 | loss: 6.6608458Losses:  6.6527557373046875 2.0017664432525635
CurrentTrain: epoch  3, batch    20 | loss: 6.6527557Losses:  7.141201496124268 1.8689484596252441
CurrentTrain: epoch  3, batch    21 | loss: 7.1412015Losses:  6.784801006317139 2.003185749053955
CurrentTrain: epoch  3, batch    22 | loss: 6.7848010Losses:  6.818755626678467 1.5713634490966797
CurrentTrain: epoch  3, batch    23 | loss: 6.8187556Losses:  6.548161506652832 1.7592260837554932
CurrentTrain: epoch  3, batch    24 | loss: 6.5481615Losses:  6.80460786819458 1.9224281311035156
CurrentTrain: epoch  3, batch    25 | loss: 6.8046079Losses:  6.591193199157715 1.9173897504806519
CurrentTrain: epoch  3, batch    26 | loss: 6.5911932Losses:  6.846182823181152 1.991072177886963
CurrentTrain: epoch  3, batch    27 | loss: 6.8461828Losses:  6.9202470779418945 1.584040641784668
CurrentTrain: epoch  3, batch    28 | loss: 6.9202471Losses:  6.838804244995117 2.0104482173919678
CurrentTrain: epoch  3, batch    29 | loss: 6.8388042Losses:  6.512490272521973 1.948775053024292
CurrentTrain: epoch  3, batch    30 | loss: 6.5124903Losses:  7.493096828460693 1.8994221687316895
CurrentTrain: epoch  3, batch    31 | loss: 7.4930968Losses:  6.652522087097168 1.8930625915527344
CurrentTrain: epoch  3, batch    32 | loss: 6.6525221Losses:  6.78363037109375 1.7751237154006958
CurrentTrain: epoch  3, batch    33 | loss: 6.7836304Losses:  7.417835712432861 2.042020320892334
CurrentTrain: epoch  3, batch    34 | loss: 7.4178357Losses:  6.3011322021484375 1.8269035816192627
CurrentTrain: epoch  3, batch    35 | loss: 6.3011322Losses:  6.648099899291992 1.8633849620819092
CurrentTrain: epoch  3, batch    36 | loss: 6.6480999Losses:  6.36262321472168 1.8616868257522583
CurrentTrain: epoch  3, batch    37 | loss: 6.3626232Losses:  6.704541206359863 1.9248926639556885
CurrentTrain: epoch  3, batch    38 | loss: 6.7045412Losses:  5.980686664581299 1.5233783721923828
CurrentTrain: epoch  3, batch    39 | loss: 5.9806867Losses:  6.847938060760498 2.0331950187683105
CurrentTrain: epoch  3, batch    40 | loss: 6.8479381Losses:  6.8768415451049805 1.7705426216125488
CurrentTrain: epoch  3, batch    41 | loss: 6.8768415Losses:  6.840828895568848 1.7632067203521729
CurrentTrain: epoch  3, batch    42 | loss: 6.8408289Losses:  6.29198694229126 1.724671483039856
CurrentTrain: epoch  3, batch    43 | loss: 6.2919869Losses:  6.435694694519043 1.8631534576416016
CurrentTrain: epoch  3, batch    44 | loss: 6.4356947Losses:  7.241008758544922 2.139129638671875
CurrentTrain: epoch  3, batch    45 | loss: 7.2410088Losses:  6.272653579711914 1.7148345708847046
CurrentTrain: epoch  3, batch    46 | loss: 6.2726536Losses:  6.653359889984131 1.8964842557907104
CurrentTrain: epoch  3, batch    47 | loss: 6.6533599Losses:  6.807246208190918 2.0392229557037354
CurrentTrain: epoch  3, batch    48 | loss: 6.8072462Losses:  6.102056980133057 1.7981065511703491
CurrentTrain: epoch  3, batch    49 | loss: 6.1020570Losses:  7.203871726989746 2.034837484359741
CurrentTrain: epoch  3, batch    50 | loss: 7.2038717Losses:  6.204172134399414 1.7584863901138306
CurrentTrain: epoch  3, batch    51 | loss: 6.2041721Losses:  6.5845723152160645 2.0014100074768066
CurrentTrain: epoch  3, batch    52 | loss: 6.5845723Losses:  6.382689952850342 1.8821932077407837
CurrentTrain: epoch  3, batch    53 | loss: 6.3826900Losses:  6.586953639984131 1.9467252492904663
CurrentTrain: epoch  3, batch    54 | loss: 6.5869536Losses:  6.314849853515625 1.6978323459625244
CurrentTrain: epoch  3, batch    55 | loss: 6.3148499Losses:  6.410813331604004 1.7675750255584717
CurrentTrain: epoch  3, batch    56 | loss: 6.4108133Losses:  6.022071361541748 1.7055268287658691
CurrentTrain: epoch  3, batch    57 | loss: 6.0220714Losses:  6.548226356506348 1.8824512958526611
CurrentTrain: epoch  3, batch    58 | loss: 6.5482264Losses:  6.534182548522949 1.8443069458007812
CurrentTrain: epoch  3, batch    59 | loss: 6.5341825Losses:  6.95853328704834 1.6100013256072998
CurrentTrain: epoch  3, batch    60 | loss: 6.9585333Losses:  6.752867698669434 2.013000965118408
CurrentTrain: epoch  3, batch    61 | loss: 6.7528677Losses:  6.670354843139648 1.6729410886764526
CurrentTrain: epoch  3, batch    62 | loss: 6.6703548Losses:  6.718294143676758 2.130232810974121
CurrentTrain: epoch  4, batch     0 | loss: 6.7182941Losses:  6.216004848480225 1.826724648475647
CurrentTrain: epoch  4, batch     1 | loss: 6.2160048Losses:  6.911196708679199 1.8245632648468018
CurrentTrain: epoch  4, batch     2 | loss: 6.9111967Losses:  6.829808235168457 1.8373843431472778
CurrentTrain: epoch  4, batch     3 | loss: 6.8298082Losses:  6.418810844421387 1.7554521560668945
CurrentTrain: epoch  4, batch     4 | loss: 6.4188108Losses:  6.476500511169434 1.7656224966049194
CurrentTrain: epoch  4, batch     5 | loss: 6.4765005Losses:  6.433436870574951 1.9101203680038452
CurrentTrain: epoch  4, batch     6 | loss: 6.4334369Losses:  6.540709972381592 1.8882479667663574
CurrentTrain: epoch  4, batch     7 | loss: 6.5407100Losses:  6.712996006011963 1.9837712049484253
CurrentTrain: epoch  4, batch     8 | loss: 6.7129960Losses:  6.749024391174316 2.0744760036468506
CurrentTrain: epoch  4, batch     9 | loss: 6.7490244Losses:  6.350050926208496 1.8673288822174072
CurrentTrain: epoch  4, batch    10 | loss: 6.3500509Losses:  6.012622833251953 1.5800995826721191
CurrentTrain: epoch  4, batch    11 | loss: 6.0126228Losses:  6.336566925048828 1.916691541671753
CurrentTrain: epoch  4, batch    12 | loss: 6.3365669Losses:  6.74072265625 2.0841658115386963
CurrentTrain: epoch  4, batch    13 | loss: 6.7407227Losses:  6.343598365783691 1.7748417854309082
CurrentTrain: epoch  4, batch    14 | loss: 6.3435984Losses:  6.4324049949646 1.912061095237732
CurrentTrain: epoch  4, batch    15 | loss: 6.4324050Losses:  6.520517349243164 2.0860395431518555
CurrentTrain: epoch  4, batch    16 | loss: 6.5205173Losses:  6.569478988647461 2.046808958053589
CurrentTrain: epoch  4, batch    17 | loss: 6.5694790Losses:  6.543097972869873 2.1148276329040527
CurrentTrain: epoch  4, batch    18 | loss: 6.5430980Losses:  6.285580635070801 1.774582028388977
CurrentTrain: epoch  4, batch    19 | loss: 6.2855806Losses:  6.334821701049805 1.9939424991607666
CurrentTrain: epoch  4, batch    20 | loss: 6.3348217Losses:  5.963130950927734 1.7116363048553467
CurrentTrain: epoch  4, batch    21 | loss: 5.9631310Losses:  6.757828712463379 1.8991031646728516
CurrentTrain: epoch  4, batch    22 | loss: 6.7578287Losses:  6.601726055145264 2.126471996307373
CurrentTrain: epoch  4, batch    23 | loss: 6.6017261Losses:  6.515182018280029 2.1275463104248047
CurrentTrain: epoch  4, batch    24 | loss: 6.5151820Losses:  6.4525251388549805 1.999037742614746
CurrentTrain: epoch  4, batch    25 | loss: 6.4525251Losses:  6.186056613922119 1.8338361978530884
CurrentTrain: epoch  4, batch    26 | loss: 6.1860566Losses:  6.532645225524902 2.115833282470703
CurrentTrain: epoch  4, batch    27 | loss: 6.5326452Losses:  6.4899444580078125 2.035132884979248
CurrentTrain: epoch  4, batch    28 | loss: 6.4899445Losses:  6.112196445465088 1.745595097541809
CurrentTrain: epoch  4, batch    29 | loss: 6.1121964Losses:  6.37011194229126 1.9013333320617676
CurrentTrain: epoch  4, batch    30 | loss: 6.3701119Losses:  6.048274040222168 1.6594855785369873
CurrentTrain: epoch  4, batch    31 | loss: 6.0482740Losses:  6.449891090393066 1.860214352607727
CurrentTrain: epoch  4, batch    32 | loss: 6.4498911Losses:  6.242801189422607 1.7648062705993652
CurrentTrain: epoch  4, batch    33 | loss: 6.2428012Losses:  6.068795204162598 1.6436944007873535
CurrentTrain: epoch  4, batch    34 | loss: 6.0687952Losses:  6.130007266998291 1.7311254739761353
CurrentTrain: epoch  4, batch    35 | loss: 6.1300073Losses:  6.150674343109131 1.904017448425293
CurrentTrain: epoch  4, batch    36 | loss: 6.1506743Losses:  6.453058242797852 1.8233611583709717
CurrentTrain: epoch  4, batch    37 | loss: 6.4530582Losses:  6.1995415687561035 1.9009946584701538
CurrentTrain: epoch  4, batch    38 | loss: 6.1995416Losses:  6.641922950744629 1.8066678047180176
CurrentTrain: epoch  4, batch    39 | loss: 6.6419230Losses:  6.327340602874756 1.976952075958252
CurrentTrain: epoch  4, batch    40 | loss: 6.3273406Losses:  6.333792209625244 1.9829646348953247
CurrentTrain: epoch  4, batch    41 | loss: 6.3337922Losses:  6.440346717834473 1.9823884963989258
CurrentTrain: epoch  4, batch    42 | loss: 6.4403467Losses:  6.329011917114258 1.9390757083892822
CurrentTrain: epoch  4, batch    43 | loss: 6.3290119Losses:  6.072811603546143 1.7458252906799316
CurrentTrain: epoch  4, batch    44 | loss: 6.0728116Losses:  6.356540203094482 1.8221224546432495
CurrentTrain: epoch  4, batch    45 | loss: 6.3565402Losses:  6.3034515380859375 1.8656752109527588
CurrentTrain: epoch  4, batch    46 | loss: 6.3034515Losses:  5.979886054992676 1.7044053077697754
CurrentTrain: epoch  4, batch    47 | loss: 5.9798861Losses:  6.188904762268066 1.8127644062042236
CurrentTrain: epoch  4, batch    48 | loss: 6.1889048Losses:  6.194185256958008 1.791607141494751
CurrentTrain: epoch  4, batch    49 | loss: 6.1941853Losses:  6.156448841094971 1.8982820510864258
CurrentTrain: epoch  4, batch    50 | loss: 6.1564488Losses:  6.2923808097839355 1.9886102676391602
CurrentTrain: epoch  4, batch    51 | loss: 6.2923808Losses:  6.279339790344238 1.9112799167633057
CurrentTrain: epoch  4, batch    52 | loss: 6.2793398Losses:  6.272928714752197 1.8816646337509155
CurrentTrain: epoch  4, batch    53 | loss: 6.2729287Losses:  6.6689534187316895 1.8619362115859985
CurrentTrain: epoch  4, batch    54 | loss: 6.6689534Losses:  6.380533218383789 2.0002031326293945
CurrentTrain: epoch  4, batch    55 | loss: 6.3805332Losses:  6.38556432723999 1.9967752695083618
CurrentTrain: epoch  4, batch    56 | loss: 6.3855643Losses:  6.163816452026367 1.962075114250183
CurrentTrain: epoch  4, batch    57 | loss: 6.1638165Losses:  6.558295726776123 1.9895015954971313
CurrentTrain: epoch  4, batch    58 | loss: 6.5582957Losses:  6.329880237579346 2.040369987487793
CurrentTrain: epoch  4, batch    59 | loss: 6.3298802Losses:  6.095402240753174 1.6157242059707642
CurrentTrain: epoch  4, batch    60 | loss: 6.0954022Losses:  6.204913139343262 1.9930384159088135
CurrentTrain: epoch  4, batch    61 | loss: 6.2049131Losses:  5.658100128173828 1.4536821842193604
CurrentTrain: epoch  4, batch    62 | loss: 5.6581001Losses:  6.260163307189941 1.9429419040679932
CurrentTrain: epoch  5, batch     0 | loss: 6.2601633Losses:  6.279607772827148 2.022528648376465
CurrentTrain: epoch  5, batch     1 | loss: 6.2796078Losses:  6.145036220550537 1.8528718948364258
CurrentTrain: epoch  5, batch     2 | loss: 6.1450362Losses:  6.35493278503418 2.0701143741607666
CurrentTrain: epoch  5, batch     3 | loss: 6.3549328Losses:  6.202617645263672 1.8396246433258057
CurrentTrain: epoch  5, batch     4 | loss: 6.2026176Losses:  5.806874752044678 1.6211684942245483
CurrentTrain: epoch  5, batch     5 | loss: 5.8068748Losses:  6.326321601867676 2.0911338329315186
CurrentTrain: epoch  5, batch     6 | loss: 6.3263216Losses:  6.217842102050781 1.8871209621429443
CurrentTrain: epoch  5, batch     7 | loss: 6.2178421Losses:  6.261775016784668 1.9680535793304443
CurrentTrain: epoch  5, batch     8 | loss: 6.2617750Losses:  6.070014953613281 1.8272382020950317
CurrentTrain: epoch  5, batch     9 | loss: 6.0700150Losses:  6.127538204193115 1.9188750982284546
CurrentTrain: epoch  5, batch    10 | loss: 6.1275382Losses:  6.09145450592041 1.7768700122833252
CurrentTrain: epoch  5, batch    11 | loss: 6.0914545Losses:  6.068822860717773 1.870995283126831
CurrentTrain: epoch  5, batch    12 | loss: 6.0688229Losses:  6.1415629386901855 1.8835034370422363
CurrentTrain: epoch  5, batch    13 | loss: 6.1415629Losses:  6.184650897979736 1.88401460647583
CurrentTrain: epoch  5, batch    14 | loss: 6.1846509Losses:  6.080730438232422 1.8988492488861084
CurrentTrain: epoch  5, batch    15 | loss: 6.0807304Losses:  6.400097846984863 2.1144230365753174
CurrentTrain: epoch  5, batch    16 | loss: 6.4000978Losses:  6.089055061340332 1.9005992412567139
CurrentTrain: epoch  5, batch    17 | loss: 6.0890551Losses:  6.320225238800049 2.043912887573242
CurrentTrain: epoch  5, batch    18 | loss: 6.3202252Losses:  6.319760322570801 2.1006057262420654
CurrentTrain: epoch  5, batch    19 | loss: 6.3197603Losses:  6.175542831420898 1.8647384643554688
CurrentTrain: epoch  5, batch    20 | loss: 6.1755428Losses:  6.0194573402404785 1.8485463857650757
CurrentTrain: epoch  5, batch    21 | loss: 6.0194573Losses:  6.230599403381348 1.954767107963562
CurrentTrain: epoch  5, batch    22 | loss: 6.2305994Losses:  6.019807815551758 1.7952889204025269
CurrentTrain: epoch  5, batch    23 | loss: 6.0198078Losses:  6.168632984161377 1.9145169258117676
CurrentTrain: epoch  5, batch    24 | loss: 6.1686330Losses:  6.339301586151123 2.0100698471069336
CurrentTrain: epoch  5, batch    25 | loss: 6.3393016Losses:  5.926192283630371 1.6344698667526245
CurrentTrain: epoch  5, batch    26 | loss: 5.9261923Losses:  6.6486663818359375 1.9668867588043213
CurrentTrain: epoch  5, batch    27 | loss: 6.6486664Losses:  5.916894912719727 1.5767050981521606
CurrentTrain: epoch  5, batch    28 | loss: 5.9168949Losses:  6.17009162902832 1.9945625066757202
CurrentTrain: epoch  5, batch    29 | loss: 6.1700916Losses:  5.987066268920898 1.81488037109375
CurrentTrain: epoch  5, batch    30 | loss: 5.9870663Losses:  6.113104343414307 1.9369316101074219
CurrentTrain: epoch  5, batch    31 | loss: 6.1131043Losses:  6.0047783851623535 1.7773337364196777
CurrentTrain: epoch  5, batch    32 | loss: 6.0047784Losses:  6.50339937210083 1.9472249746322632
CurrentTrain: epoch  5, batch    33 | loss: 6.5033994Losses:  6.341268539428711 2.115922689437866
CurrentTrain: epoch  5, batch    34 | loss: 6.3412685Losses:  6.417325973510742 1.8809391260147095
CurrentTrain: epoch  5, batch    35 | loss: 6.4173260Losses:  6.025272369384766 1.665892481803894
CurrentTrain: epoch  5, batch    36 | loss: 6.0252724Losses:  6.194137096405029 1.8579940795898438
CurrentTrain: epoch  5, batch    37 | loss: 6.1941371Losses:  6.148209571838379 1.9070394039154053
CurrentTrain: epoch  5, batch    38 | loss: 6.1482096Losses:  6.091076374053955 1.7671494483947754
CurrentTrain: epoch  5, batch    39 | loss: 6.0910764Losses:  6.184135437011719 1.8988553285598755
CurrentTrain: epoch  5, batch    40 | loss: 6.1841354Losses:  5.987222671508789 1.5257911682128906
CurrentTrain: epoch  5, batch    41 | loss: 5.9872227Losses:  5.991657257080078 1.8031930923461914
CurrentTrain: epoch  5, batch    42 | loss: 5.9916573Losses:  6.265027046203613 1.9894095659255981
CurrentTrain: epoch  5, batch    43 | loss: 6.2650270Losses:  5.953433036804199 1.763869285583496
CurrentTrain: epoch  5, batch    44 | loss: 5.9534330Losses:  6.061880111694336 1.9088295698165894
CurrentTrain: epoch  5, batch    45 | loss: 6.0618801Losses:  5.972254276275635 1.8327699899673462
CurrentTrain: epoch  5, batch    46 | loss: 5.9722543Losses:  6.712915420532227 1.957379698753357
CurrentTrain: epoch  5, batch    47 | loss: 6.7129154Losses:  6.185408592224121 2.056631565093994
CurrentTrain: epoch  5, batch    48 | loss: 6.1854086Losses:  6.112886428833008 1.9271105527877808
CurrentTrain: epoch  5, batch    49 | loss: 6.1128864Losses:  6.120924472808838 1.9606479406356812
CurrentTrain: epoch  5, batch    50 | loss: 6.1209245Losses:  6.077969551086426 1.8301341533660889
CurrentTrain: epoch  5, batch    51 | loss: 6.0779696Losses:  6.192222595214844 1.992799162864685
CurrentTrain: epoch  5, batch    52 | loss: 6.1922226Losses:  6.234593391418457 2.0469810962677
CurrentTrain: epoch  5, batch    53 | loss: 6.2345934Losses:  6.271406173706055 2.075680732727051
CurrentTrain: epoch  5, batch    54 | loss: 6.2714062Losses:  6.124141216278076 1.9674086570739746
CurrentTrain: epoch  5, batch    55 | loss: 6.1241412Losses:  5.884491920471191 1.7725802659988403
CurrentTrain: epoch  5, batch    56 | loss: 5.8844919Losses:  5.947587490081787 1.696046233177185
CurrentTrain: epoch  5, batch    57 | loss: 5.9475875Losses:  5.880263328552246 1.710350751876831
CurrentTrain: epoch  5, batch    58 | loss: 5.8802633Losses:  6.032474040985107 1.895373821258545
CurrentTrain: epoch  5, batch    59 | loss: 6.0324740Losses:  5.866151332855225 1.6211371421813965
CurrentTrain: epoch  5, batch    60 | loss: 5.8661513Losses:  5.954554080963135 1.7704917192459106
CurrentTrain: epoch  5, batch    61 | loss: 5.9545541Losses:  5.740954875946045 1.5645861625671387
CurrentTrain: epoch  5, batch    62 | loss: 5.7409549Losses:  5.807388782501221 1.6307491064071655
CurrentTrain: epoch  6, batch     0 | loss: 5.8073888Losses:  5.699355125427246 1.544318675994873
CurrentTrain: epoch  6, batch     1 | loss: 5.6993551Losses:  5.949501991271973 1.77396821975708
CurrentTrain: epoch  6, batch     2 | loss: 5.9495020Losses:  6.166801929473877 1.9707903861999512
CurrentTrain: epoch  6, batch     3 | loss: 6.1668019Losses:  5.9562177658081055 1.8105223178863525
CurrentTrain: epoch  6, batch     4 | loss: 5.9562178Losses:  6.007809638977051 1.8253334760665894
CurrentTrain: epoch  6, batch     5 | loss: 6.0078096Losses:  6.149563789367676 1.983285903930664
CurrentTrain: epoch  6, batch     6 | loss: 6.1495638Losses:  5.908576965332031 1.7756551504135132
CurrentTrain: epoch  6, batch     7 | loss: 5.9085770Losses:  6.298248767852783 2.0347390174865723
CurrentTrain: epoch  6, batch     8 | loss: 6.2982488Losses:  6.0417046546936035 1.811629295349121
CurrentTrain: epoch  6, batch     9 | loss: 6.0417047Losses:  5.823511123657227 1.6369309425354004
CurrentTrain: epoch  6, batch    10 | loss: 5.8235111Losses:  5.941611289978027 1.794729232788086
CurrentTrain: epoch  6, batch    11 | loss: 5.9416113Losses:  5.975821495056152 1.8657972812652588
CurrentTrain: epoch  6, batch    12 | loss: 5.9758215Losses:  6.251980781555176 2.040898561477661
CurrentTrain: epoch  6, batch    13 | loss: 6.2519808Losses:  6.139087677001953 2.009545087814331
CurrentTrain: epoch  6, batch    14 | loss: 6.1390877Losses:  6.163458824157715 2.003812789916992
CurrentTrain: epoch  6, batch    15 | loss: 6.1634588Losses:  6.122642517089844 1.9836230278015137
CurrentTrain: epoch  6, batch    16 | loss: 6.1226425Losses:  5.975482940673828 1.8372920751571655
CurrentTrain: epoch  6, batch    17 | loss: 5.9754829Losses:  5.674673557281494 1.5102391242980957
CurrentTrain: epoch  6, batch    18 | loss: 5.6746736Losses:  5.984543800354004 1.8625268936157227
CurrentTrain: epoch  6, batch    19 | loss: 5.9845438Losses:  5.895455837249756 1.7012215852737427
CurrentTrain: epoch  6, batch    20 | loss: 5.8954558Losses:  6.445921421051025 1.9308271408081055
CurrentTrain: epoch  6, batch    21 | loss: 6.4459214Losses:  6.182458877563477 1.9796993732452393
CurrentTrain: epoch  6, batch    22 | loss: 6.1824589Losses:  6.07222843170166 1.9384958744049072
CurrentTrain: epoch  6, batch    23 | loss: 6.0722284Losses:  6.06951904296875 1.8907506465911865
CurrentTrain: epoch  6, batch    24 | loss: 6.0695190Losses:  6.100154876708984 1.94091796875
CurrentTrain: epoch  6, batch    25 | loss: 6.1001549Losses:  6.030913352966309 1.8545892238616943
CurrentTrain: epoch  6, batch    26 | loss: 6.0309134Losses:  5.969259738922119 1.793527603149414
CurrentTrain: epoch  6, batch    27 | loss: 5.9692597Losses:  6.077750205993652 1.9729074239730835
CurrentTrain: epoch  6, batch    28 | loss: 6.0777502Losses:  6.083967208862305 1.8906919956207275
CurrentTrain: epoch  6, batch    29 | loss: 6.0839672Losses:  6.014151573181152 1.9259297847747803
CurrentTrain: epoch  6, batch    30 | loss: 6.0141516Losses:  6.131309509277344 2.0022430419921875
CurrentTrain: epoch  6, batch    31 | loss: 6.1313095Losses:  6.053809642791748 1.9231948852539062
CurrentTrain: epoch  6, batch    32 | loss: 6.0538096Losses:  6.087920188903809 1.9766789674758911
CurrentTrain: epoch  6, batch    33 | loss: 6.0879202Losses:  6.2405853271484375 2.1374645233154297
CurrentTrain: epoch  6, batch    34 | loss: 6.2405853Losses:  5.977378845214844 1.88163423538208
CurrentTrain: epoch  6, batch    35 | loss: 5.9773788Losses:  6.0549774169921875 1.9315946102142334
CurrentTrain: epoch  6, batch    36 | loss: 6.0549774Losses:  5.885754585266113 1.8461568355560303
CurrentTrain: epoch  6, batch    37 | loss: 5.8857546Losses:  5.928440570831299 1.8499494791030884
CurrentTrain: epoch  6, batch    38 | loss: 5.9284406Losses:  6.079144477844238 1.979722261428833
CurrentTrain: epoch  6, batch    39 | loss: 6.0791445Losses:  5.691516876220703 1.5869117975234985
CurrentTrain: epoch  6, batch    40 | loss: 5.6915169Losses:  6.111824989318848 1.9799423217773438
CurrentTrain: epoch  6, batch    41 | loss: 6.1118250Losses:  6.122412204742432 1.9695807695388794
CurrentTrain: epoch  6, batch    42 | loss: 6.1224122Losses:  6.421637058258057 1.9162917137145996
CurrentTrain: epoch  6, batch    43 | loss: 6.4216371Losses:  5.850632667541504 1.723682165145874
CurrentTrain: epoch  6, batch    44 | loss: 5.8506327Losses:  5.907095909118652 1.734084129333496
CurrentTrain: epoch  6, batch    45 | loss: 5.9070959Losses:  6.058779716491699 1.955329179763794
CurrentTrain: epoch  6, batch    46 | loss: 6.0587797Losses:  6.074090480804443 1.8915925025939941
CurrentTrain: epoch  6, batch    47 | loss: 6.0740905Losses:  6.268928527832031 1.9755001068115234
CurrentTrain: epoch  6, batch    48 | loss: 6.2689285Losses:  5.905577182769775 1.7382607460021973
CurrentTrain: epoch  6, batch    49 | loss: 5.9055772Losses:  6.216526985168457 2.063237190246582
CurrentTrain: epoch  6, batch    50 | loss: 6.2165270Losses:  5.978809833526611 1.7931923866271973
CurrentTrain: epoch  6, batch    51 | loss: 5.9788098Losses:  5.842600345611572 1.795982003211975
CurrentTrain: epoch  6, batch    52 | loss: 5.8426003Losses:  5.910898208618164 1.836902379989624
CurrentTrain: epoch  6, batch    53 | loss: 5.9108982Losses:  6.011037349700928 1.9620823860168457
CurrentTrain: epoch  6, batch    54 | loss: 6.0110373Losses:  6.045746326446533 1.9344905614852905
CurrentTrain: epoch  6, batch    55 | loss: 6.0457463Losses:  6.111098766326904 1.9364433288574219
CurrentTrain: epoch  6, batch    56 | loss: 6.1110988Losses:  5.887378215789795 1.8045272827148438
CurrentTrain: epoch  6, batch    57 | loss: 5.8873782Losses:  5.88400936126709 1.768439531326294
CurrentTrain: epoch  6, batch    58 | loss: 5.8840094Losses:  6.136975288391113 2.0129528045654297
CurrentTrain: epoch  6, batch    59 | loss: 6.1369753Losses:  6.095429420471191 1.949913740158081
CurrentTrain: epoch  6, batch    60 | loss: 6.0954294Losses:  5.853273391723633 1.79951012134552
CurrentTrain: epoch  6, batch    61 | loss: 5.8532734Losses:  5.923674583435059 1.802288293838501
CurrentTrain: epoch  6, batch    62 | loss: 5.9236746Losses:  5.871531009674072 1.839791178703308
CurrentTrain: epoch  7, batch     0 | loss: 5.8715310Losses:  6.064445972442627 1.9240862131118774
CurrentTrain: epoch  7, batch     1 | loss: 6.0644460Losses:  6.034731864929199 1.9379260540008545
CurrentTrain: epoch  7, batch     2 | loss: 6.0347319Losses:  6.120528221130371 1.7609739303588867
CurrentTrain: epoch  7, batch     3 | loss: 6.1205282Losses:  6.1235198974609375 1.9838601350784302
CurrentTrain: epoch  7, batch     4 | loss: 6.1235199Losses:  6.014988899230957 1.8942407369613647
CurrentTrain: epoch  7, batch     5 | loss: 6.0149889Losses:  6.027357578277588 1.9548416137695312
CurrentTrain: epoch  7, batch     6 | loss: 6.0273576Losses:  6.046118259429932 1.964104175567627
CurrentTrain: epoch  7, batch     7 | loss: 6.0461183Losses:  6.031323432922363 1.861814260482788
CurrentTrain: epoch  7, batch     8 | loss: 6.0313234Losses:  6.1171064376831055 1.9899451732635498
CurrentTrain: epoch  7, batch     9 | loss: 6.1171064Losses:  5.875557899475098 1.7725021839141846
CurrentTrain: epoch  7, batch    10 | loss: 5.8755579Losses:  6.0719451904296875 1.9438823461532593
CurrentTrain: epoch  7, batch    11 | loss: 6.0719452Losses:  5.811748504638672 1.7493364810943604
CurrentTrain: epoch  7, batch    12 | loss: 5.8117485Losses:  6.171921730041504 1.906438946723938
CurrentTrain: epoch  7, batch    13 | loss: 6.1719217Losses:  6.116269588470459 2.0281786918640137
CurrentTrain: epoch  7, batch    14 | loss: 6.1162696Losses:  6.100446701049805 2.0014452934265137
CurrentTrain: epoch  7, batch    15 | loss: 6.1004467Losses:  5.934843063354492 1.8559010028839111
CurrentTrain: epoch  7, batch    16 | loss: 5.9348431Losses:  5.976381301879883 1.75905179977417
CurrentTrain: epoch  7, batch    17 | loss: 5.9763813Losses:  5.938440799713135 1.821485996246338
CurrentTrain: epoch  7, batch    18 | loss: 5.9384408Losses:  6.135430335998535 1.982285499572754
CurrentTrain: epoch  7, batch    19 | loss: 6.1354303Losses:  5.9235758781433105 1.8784403800964355
CurrentTrain: epoch  7, batch    20 | loss: 5.9235759Losses:  5.942381858825684 1.88661789894104
CurrentTrain: epoch  7, batch    21 | loss: 5.9423819Losses:  5.89564847946167 1.8418707847595215
CurrentTrain: epoch  7, batch    22 | loss: 5.8956485Losses:  6.075830459594727 1.9598455429077148
CurrentTrain: epoch  7, batch    23 | loss: 6.0758305Losses:  5.967392921447754 1.9112398624420166
CurrentTrain: epoch  7, batch    24 | loss: 5.9673929Losses:  6.029239654541016 1.9337987899780273
CurrentTrain: epoch  7, batch    25 | loss: 6.0292397Losses:  6.192041397094727 2.116130828857422
CurrentTrain: epoch  7, batch    26 | loss: 6.1920414Losses:  5.9437971115112305 1.8275890350341797
CurrentTrain: epoch  7, batch    27 | loss: 5.9437971Losses:  6.007143974304199 1.9212331771850586
CurrentTrain: epoch  7, batch    28 | loss: 6.0071440Losses:  6.200555801391602 2.0805797576904297
CurrentTrain: epoch  7, batch    29 | loss: 6.2005558Losses:  6.337282180786133 1.8868169784545898
CurrentTrain: epoch  7, batch    30 | loss: 6.3372822Losses:  6.059000492095947 1.9561423063278198
CurrentTrain: epoch  7, batch    31 | loss: 6.0590005Losses:  5.6884307861328125 1.6688992977142334
CurrentTrain: epoch  7, batch    32 | loss: 5.6884308Losses:  5.895069122314453 1.7920130491256714
CurrentTrain: epoch  7, batch    33 | loss: 5.8950691Losses:  6.122732162475586 2.057337522506714
CurrentTrain: epoch  7, batch    34 | loss: 6.1227322Losses:  5.842731475830078 1.7847647666931152
CurrentTrain: epoch  7, batch    35 | loss: 5.8427315Losses:  5.943528175354004 1.869476556777954
CurrentTrain: epoch  7, batch    36 | loss: 5.9435282Losses:  5.939809322357178 1.8481765985488892
CurrentTrain: epoch  7, batch    37 | loss: 5.9398093Losses:  5.747305870056152 1.6343533992767334
CurrentTrain: epoch  7, batch    38 | loss: 5.7473059Losses:  5.740354061126709 1.674454689025879
CurrentTrain: epoch  7, batch    39 | loss: 5.7403541Losses:  5.6486053466796875 1.6374262571334839
CurrentTrain: epoch  7, batch    40 | loss: 5.6486053Losses:  5.923501014709473 1.8862131834030151
CurrentTrain: epoch  7, batch    41 | loss: 5.9235010Losses:  5.789647102355957 1.733445644378662
CurrentTrain: epoch  7, batch    42 | loss: 5.7896471Losses:  5.89421272277832 1.8326187133789062
CurrentTrain: epoch  7, batch    43 | loss: 5.8942127Losses:  5.839896202087402 1.7492830753326416
CurrentTrain: epoch  7, batch    44 | loss: 5.8398962Losses:  6.0602803230285645 2.005767345428467
CurrentTrain: epoch  7, batch    45 | loss: 6.0602803Losses:  6.031924247741699 1.9791443347930908
CurrentTrain: epoch  7, batch    46 | loss: 6.0319242Losses:  6.18198823928833 2.0985398292541504
CurrentTrain: epoch  7, batch    47 | loss: 6.1819882Losses:  6.083341121673584 2.026946544647217
CurrentTrain: epoch  7, batch    48 | loss: 6.0833411Losses:  5.514028549194336 1.4640343189239502
CurrentTrain: epoch  7, batch    49 | loss: 5.5140285Losses:  6.019008636474609 1.9522868394851685
CurrentTrain: epoch  7, batch    50 | loss: 6.0190086Losses:  5.916617393493652 1.8364554643630981
CurrentTrain: epoch  7, batch    51 | loss: 5.9166174Losses:  5.939807891845703 1.8629854917526245
CurrentTrain: epoch  7, batch    52 | loss: 5.9398079Losses:  5.9858078956604 1.9694113731384277
CurrentTrain: epoch  7, batch    53 | loss: 5.9858079Losses:  5.976469039916992 1.8819572925567627
CurrentTrain: epoch  7, batch    54 | loss: 5.9764690Losses:  5.703037738800049 1.7533459663391113
CurrentTrain: epoch  7, batch    55 | loss: 5.7030377Losses:  5.942635536193848 1.8787298202514648
CurrentTrain: epoch  7, batch    56 | loss: 5.9426355Losses:  5.799016952514648 1.7704179286956787
CurrentTrain: epoch  7, batch    57 | loss: 5.7990170Losses:  6.1061296463012695 2.0401804447174072
CurrentTrain: epoch  7, batch    58 | loss: 6.1061296Losses:  5.863777160644531 1.721616506576538
CurrentTrain: epoch  7, batch    59 | loss: 5.8637772Losses:  6.040833473205566 1.9627903699874878
CurrentTrain: epoch  7, batch    60 | loss: 6.0408335Losses:  5.875481605529785 1.8284215927124023
CurrentTrain: epoch  7, batch    61 | loss: 5.8754816Losses:  5.585274696350098 1.4688465595245361
CurrentTrain: epoch  7, batch    62 | loss: 5.5852747Losses:  5.959214210510254 1.9237661361694336
CurrentTrain: epoch  8, batch     0 | loss: 5.9592142Losses:  5.881095886230469 1.8309961557388306
CurrentTrain: epoch  8, batch     1 | loss: 5.8810959Losses:  6.025693893432617 1.8541489839553833
CurrentTrain: epoch  8, batch     2 | loss: 6.0256939Losses:  5.800580024719238 1.7483006715774536
CurrentTrain: epoch  8, batch     3 | loss: 5.8005800Losses:  5.854348182678223 1.80936861038208
CurrentTrain: epoch  8, batch     4 | loss: 5.8543482Losses:  5.6135573387146 1.5584993362426758
CurrentTrain: epoch  8, batch     5 | loss: 5.6135573Losses:  6.013958930969238 2.015090227127075
CurrentTrain: epoch  8, batch     6 | loss: 6.0139589Losses:  5.987259864807129 1.9671965837478638
CurrentTrain: epoch  8, batch     7 | loss: 5.9872599Losses:  5.859996318817139 1.7881417274475098
CurrentTrain: epoch  8, batch     8 | loss: 5.8599963Losses:  5.8161420822143555 1.7900135517120361
CurrentTrain: epoch  8, batch     9 | loss: 5.8161421Losses:  6.008405685424805 1.973175287246704
CurrentTrain: epoch  8, batch    10 | loss: 6.0084057Losses:  5.757580757141113 1.7371656894683838
CurrentTrain: epoch  8, batch    11 | loss: 5.7575808Losses:  5.949077606201172 1.9197275638580322
CurrentTrain: epoch  8, batch    12 | loss: 5.9490776Losses:  5.915842056274414 1.8419549465179443
CurrentTrain: epoch  8, batch    13 | loss: 5.9158421Losses:  5.915343284606934 1.8370827436447144
CurrentTrain: epoch  8, batch    14 | loss: 5.9153433Losses:  6.02482795715332 1.9703729152679443
CurrentTrain: epoch  8, batch    15 | loss: 6.0248280Losses:  5.614177227020264 1.5924558639526367
CurrentTrain: epoch  8, batch    16 | loss: 5.6141772Losses:  5.829183578491211 1.786531686782837
CurrentTrain: epoch  8, batch    17 | loss: 5.8291836Losses:  5.773837089538574 1.7110614776611328
CurrentTrain: epoch  8, batch    18 | loss: 5.7738371Losses:  6.105719566345215 2.077181577682495
CurrentTrain: epoch  8, batch    19 | loss: 6.1057196Losses:  5.992703914642334 1.9189971685409546
CurrentTrain: epoch  8, batch    20 | loss: 5.9927039Losses:  5.966224193572998 1.8875055313110352
CurrentTrain: epoch  8, batch    21 | loss: 5.9662242Losses:  6.050338268280029 1.9726592302322388
CurrentTrain: epoch  8, batch    22 | loss: 6.0503383Losses:  5.957813262939453 1.9104235172271729
CurrentTrain: epoch  8, batch    23 | loss: 5.9578133Losses:  6.089966297149658 2.037163257598877
CurrentTrain: epoch  8, batch    24 | loss: 6.0899663Losses:  5.968231201171875 1.820923089981079
CurrentTrain: epoch  8, batch    25 | loss: 5.9682312Losses:  5.995116710662842 1.9733384847640991
CurrentTrain: epoch  8, batch    26 | loss: 5.9951167Losses:  5.9045209884643555 1.8400553464889526
CurrentTrain: epoch  8, batch    27 | loss: 5.9045210Losses:  6.241539001464844 1.9634865522384644
CurrentTrain: epoch  8, batch    28 | loss: 6.2415390Losses:  5.882087230682373 1.8482604026794434
CurrentTrain: epoch  8, batch    29 | loss: 5.8820872Losses:  5.86870813369751 1.8498364686965942
CurrentTrain: epoch  8, batch    30 | loss: 5.8687081Losses:  5.697749137878418 1.6259973049163818
CurrentTrain: epoch  8, batch    31 | loss: 5.6977491Losses:  5.902130603790283 1.8599284887313843
CurrentTrain: epoch  8, batch    32 | loss: 5.9021306Losses:  5.72385835647583 1.7270070314407349
CurrentTrain: epoch  8, batch    33 | loss: 5.7238584Losses:  5.966405868530273 1.9163568019866943
CurrentTrain: epoch  8, batch    34 | loss: 5.9664059Losses:  6.02780818939209 1.9988270998001099
CurrentTrain: epoch  8, batch    35 | loss: 6.0278082Losses:  5.897004127502441 1.8329880237579346
CurrentTrain: epoch  8, batch    36 | loss: 5.8970041Losses:  5.965245246887207 1.8509719371795654
CurrentTrain: epoch  8, batch    37 | loss: 5.9652452Losses:  5.61470890045166 1.610061526298523
CurrentTrain: epoch  8, batch    38 | loss: 5.6147089Losses:  6.0084123611450195 1.9479799270629883
CurrentTrain: epoch  8, batch    39 | loss: 6.0084124Losses:  5.862316131591797 1.7800973653793335
CurrentTrain: epoch  8, batch    40 | loss: 5.8623161Losses:  6.085060119628906 2.009455680847168
CurrentTrain: epoch  8, batch    41 | loss: 6.0850601Losses:  6.030008792877197 1.9470314979553223
CurrentTrain: epoch  8, batch    42 | loss: 6.0300088Losses:  5.8139262199401855 1.8157120943069458
CurrentTrain: epoch  8, batch    43 | loss: 5.8139262Losses:  5.994057655334473 1.9287548065185547
CurrentTrain: epoch  8, batch    44 | loss: 5.9940577Losses:  5.875248432159424 1.8736672401428223
CurrentTrain: epoch  8, batch    45 | loss: 5.8752484Losses:  6.093825817108154 2.049624443054199
CurrentTrain: epoch  8, batch    46 | loss: 6.0938258Losses:  5.752170085906982 1.6560121774673462
CurrentTrain: epoch  8, batch    47 | loss: 5.7521701Losses:  5.826239585876465 1.7414149045944214
CurrentTrain: epoch  8, batch    48 | loss: 5.8262396Losses:  5.917117118835449 1.8784219026565552
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  13.616851806640625 1.794607400894165
CurrentTrain: epoch  0, batch     0 | loss: 13.6168518Losses:  14.082209587097168 2.057797908782959
CurrentTrain: epoch  0, batch     1 | loss: 14.0822096Losses:  13.868608474731445 1.8954432010650635
CurrentTrain: epoch  0, batch     2 | loss: 13.8686085Losses:  13.536031723022461 1.7658617496490479
CurrentTrain: epoch  0, batch     3 | loss: 13.5360317Losses:  13.166014671325684 1.9584354162216187
CurrentTrain: epoch  0, batch     4 | loss: 13.1660147Losses:  13.798460960388184 1.681506633758545
CurrentTrain: epoch  0, batch     5 | loss: 13.7984610Losses:  13.370471954345703 1.9067479372024536
CurrentTrain: epoch  0, batch     6 | loss: 13.3704720Losses:  12.653215408325195 1.850441575050354
CurrentTrain: epoch  0, batch     7 | loss: 12.6532154Losses:  12.925658226013184 1.9088057279586792
CurrentTrain: epoch  0, batch     8 | loss: 12.9256582Losses:  13.548599243164062 1.9155902862548828
CurrentTrain: epoch  0, batch     9 | loss: 13.5485992Losses:  12.456710815429688 1.7458570003509521
CurrentTrain: epoch  0, batch    10 | loss: 12.4567108Losses:  11.870162963867188 1.7146029472351074
CurrentTrain: epoch  0, batch    11 | loss: 11.8701630Losses:  12.672829627990723 1.7693486213684082
CurrentTrain: epoch  0, batch    12 | loss: 12.6728296Losses:  12.876190185546875 1.922614336013794
CurrentTrain: epoch  0, batch    13 | loss: 12.8761902Losses:  11.830431938171387 1.8379607200622559
CurrentTrain: epoch  0, batch    14 | loss: 11.8304319Losses:  12.900869369506836 2.0321474075317383
CurrentTrain: epoch  0, batch    15 | loss: 12.9008694Losses:  12.393510818481445 1.578428030014038
CurrentTrain: epoch  0, batch    16 | loss: 12.3935108Losses:  13.114032745361328 2.0487070083618164
CurrentTrain: epoch  0, batch    17 | loss: 13.1140327Losses:  12.610737800598145 1.815878987312317
CurrentTrain: epoch  0, batch    18 | loss: 12.6107378Losses:  12.652891159057617 2.0075888633728027
CurrentTrain: epoch  0, batch    19 | loss: 12.6528912Losses:  11.932497024536133 1.9605151414871216
CurrentTrain: epoch  0, batch    20 | loss: 11.9324970Losses:  12.14401912689209 1.9781361818313599
CurrentTrain: epoch  0, batch    21 | loss: 12.1440191Losses:  12.601165771484375 1.9314775466918945
CurrentTrain: epoch  0, batch    22 | loss: 12.6011658Losses:  11.965542793273926 1.9163566827774048
CurrentTrain: epoch  0, batch    23 | loss: 11.9655428Losses:  11.726140975952148 1.9817051887512207
CurrentTrain: epoch  0, batch    24 | loss: 11.7261410Losses:  11.771926879882812 1.8818881511688232
CurrentTrain: epoch  0, batch    25 | loss: 11.7719269Losses:  11.81440544128418 1.8003015518188477
CurrentTrain: epoch  0, batch    26 | loss: 11.8144054Losses:  11.791301727294922 2.027034282684326
CurrentTrain: epoch  0, batch    27 | loss: 11.7913017Losses:  11.944002151489258 1.870457410812378
CurrentTrain: epoch  0, batch    28 | loss: 11.9440022Losses:  12.066634178161621 1.9644322395324707
CurrentTrain: epoch  0, batch    29 | loss: 12.0666342Losses:  12.710947036743164 2.0756819248199463
CurrentTrain: epoch  0, batch    30 | loss: 12.7109470Losses:  11.546189308166504 1.778157114982605
CurrentTrain: epoch  0, batch    31 | loss: 11.5461893Losses:  10.885680198669434 1.4599907398223877
CurrentTrain: epoch  0, batch    32 | loss: 10.8856802Losses:  10.749422073364258 1.6510603427886963
CurrentTrain: epoch  0, batch    33 | loss: 10.7494221Losses:  11.507747650146484 1.8206089735031128
CurrentTrain: epoch  0, batch    34 | loss: 11.5077477Losses:  11.455047607421875 1.8637609481811523
CurrentTrain: epoch  0, batch    35 | loss: 11.4550476Losses:  11.509191513061523 1.897369623184204
CurrentTrain: epoch  0, batch    36 | loss: 11.5091915Losses:  11.560746192932129 1.9144165515899658
CurrentTrain: epoch  0, batch    37 | loss: 11.5607462Losses:  12.731849670410156 2.11320161819458
CurrentTrain: epoch  0, batch    38 | loss: 12.7318497Losses:  12.311857223510742 2.1482772827148438
CurrentTrain: epoch  0, batch    39 | loss: 12.3118572Losses:  12.481128692626953 2.0304441452026367
CurrentTrain: epoch  0, batch    40 | loss: 12.4811287Losses:  11.348945617675781 1.9715948104858398
CurrentTrain: epoch  0, batch    41 | loss: 11.3489456Losses:  11.140987396240234 1.9339779615402222
CurrentTrain: epoch  0, batch    42 | loss: 11.1409874Losses:  10.299968719482422 1.6417348384857178
CurrentTrain: epoch  0, batch    43 | loss: 10.2999687Losses:  10.821189880371094 1.741447925567627
CurrentTrain: epoch  0, batch    44 | loss: 10.8211899Losses:  11.357999801635742 2.015425205230713
CurrentTrain: epoch  0, batch    45 | loss: 11.3579998Losses:  11.699685096740723 1.7886662483215332
CurrentTrain: epoch  0, batch    46 | loss: 11.6996851Losses:  10.596442222595215 2.0077614784240723
CurrentTrain: epoch  0, batch    47 | loss: 10.5964422Losses:  11.390073776245117 1.9124526977539062
CurrentTrain: epoch  0, batch    48 | loss: 11.3900738Losses:  11.126153945922852 1.9161512851715088
CurrentTrain: epoch  0, batch    49 | loss: 11.1261539Losses:  11.62155818939209 1.965672492980957
CurrentTrain: epoch  0, batch    50 | loss: 11.6215582Losses:  11.385502815246582 2.082063674926758
CurrentTrain: epoch  0, batch    51 | loss: 11.3855028Losses:  10.785900115966797 1.909156322479248
CurrentTrain: epoch  0, batch    52 | loss: 10.7859001Losses:  10.893942832946777 1.8821073770523071
CurrentTrain: epoch  0, batch    53 | loss: 10.8939428Losses:  11.811929702758789 2.0404253005981445
CurrentTrain: epoch  0, batch    54 | loss: 11.8119297Losses:  10.644855499267578 1.827615737915039
CurrentTrain: epoch  0, batch    55 | loss: 10.6448555Losses:  10.335043907165527 1.7882437705993652
CurrentTrain: epoch  0, batch    56 | loss: 10.3350439Losses:  11.441566467285156 1.9755749702453613
CurrentTrain: epoch  0, batch    57 | loss: 11.4415665Losses:  10.805642127990723 1.7185609340667725
CurrentTrain: epoch  0, batch    58 | loss: 10.8056421Losses:  10.465105056762695 1.709524393081665
CurrentTrain: epoch  0, batch    59 | loss: 10.4651051Losses:  10.379833221435547 1.9263813495635986
CurrentTrain: epoch  0, batch    60 | loss: 10.3798332Losses:  10.928055763244629 1.7451777458190918
CurrentTrain: epoch  0, batch    61 | loss: 10.9280558Losses:  11.838296890258789 1.4560415744781494
CurrentTrain: epoch  0, batch    62 | loss: 11.8382969Losses:  10.939484596252441 1.771337866783142
CurrentTrain: epoch  1, batch     0 | loss: 10.9394846Losses:  10.723270416259766 1.8672605752944946
CurrentTrain: epoch  1, batch     1 | loss: 10.7232704Losses:  10.269319534301758 1.7752950191497803
CurrentTrain: epoch  1, batch     2 | loss: 10.2693195Losses:  11.036171913146973 2.0380187034606934
CurrentTrain: epoch  1, batch     3 | loss: 11.0361719Losses:  11.293488502502441 2.0143423080444336
CurrentTrain: epoch  1, batch     4 | loss: 11.2934885Losses:  10.563847541809082 1.9068900346755981
CurrentTrain: epoch  1, batch     5 | loss: 10.5638475Losses:  10.681591987609863 1.8942644596099854
CurrentTrain: epoch  1, batch     6 | loss: 10.6815920Losses:  10.227588653564453 2.035944938659668
CurrentTrain: epoch  1, batch     7 | loss: 10.2275887Losses:  9.739425659179688 1.7142035961151123
CurrentTrain: epoch  1, batch     8 | loss: 9.7394257Losses:  10.928444862365723 1.9613322019577026
CurrentTrain: epoch  1, batch     9 | loss: 10.9284449Losses:  10.789758682250977 1.926896333694458
CurrentTrain: epoch  1, batch    10 | loss: 10.7897587Losses:  10.418245315551758 1.9430177211761475
CurrentTrain: epoch  1, batch    11 | loss: 10.4182453Losses:  10.811476707458496 2.0007190704345703
CurrentTrain: epoch  1, batch    12 | loss: 10.8114767Losses:  10.391407012939453 1.6336519718170166
CurrentTrain: epoch  1, batch    13 | loss: 10.3914070Losses:  10.130285263061523 1.943498134613037
CurrentTrain: epoch  1, batch    14 | loss: 10.1302853Losses:  11.61441421508789 1.8767783641815186
CurrentTrain: epoch  1, batch    15 | loss: 11.6144142Losses:  10.10634708404541 1.7674083709716797
CurrentTrain: epoch  1, batch    16 | loss: 10.1063471Losses:  9.141118049621582 1.602464199066162
CurrentTrain: epoch  1, batch    17 | loss: 9.1411180Losses:  10.054938316345215 1.8275066614151
CurrentTrain: epoch  1, batch    18 | loss: 10.0549383Losses:  10.558894157409668 1.9538427591323853
CurrentTrain: epoch  1, batch    19 | loss: 10.5588942Losses:  11.043560028076172 2.0545918941497803
CurrentTrain: epoch  1, batch    20 | loss: 11.0435600Losses:  11.007722854614258 2.0442466735839844
CurrentTrain: epoch  1, batch    21 | loss: 11.0077229Losses:  10.460015296936035 1.8804208040237427
CurrentTrain: epoch  1, batch    22 | loss: 10.4600153Losses:  10.295059204101562 1.7171354293823242
CurrentTrain: epoch  1, batch    23 | loss: 10.2950592Losses:  10.333597183227539 1.8954517841339111
CurrentTrain: epoch  1, batch    24 | loss: 10.3335972Losses:  9.214546203613281 1.8261609077453613
CurrentTrain: epoch  1, batch    25 | loss: 9.2145462Losses:  10.237232208251953 2.0488429069519043
CurrentTrain: epoch  1, batch    26 | loss: 10.2372322Losses:  9.935023307800293 1.9286839962005615
CurrentTrain: epoch  1, batch    27 | loss: 9.9350233Losses:  10.409223556518555 1.9618875980377197
CurrentTrain: epoch  1, batch    28 | loss: 10.4092236Losses:  9.989437103271484 2.037140369415283
CurrentTrain: epoch  1, batch    29 | loss: 9.9894371Losses:  10.056798934936523 1.8843615055084229
CurrentTrain: epoch  1, batch    30 | loss: 10.0567989Losses:  9.81791877746582 2.0737786293029785
CurrentTrain: epoch  1, batch    31 | loss: 9.8179188Losses:  9.320383071899414 1.9381418228149414
CurrentTrain: epoch  1, batch    32 | loss: 9.3203831Losses:  9.294126510620117 1.705211877822876
CurrentTrain: epoch  1, batch    33 | loss: 9.2941265Losses:  10.698038101196289 2.0085766315460205
CurrentTrain: epoch  1, batch    34 | loss: 10.6980381Losses:  9.356376647949219 1.8609890937805176
CurrentTrain: epoch  1, batch    35 | loss: 9.3563766Losses:  10.28284740447998 1.711529016494751
CurrentTrain: epoch  1, batch    36 | loss: 10.2828474Losses:  10.588214874267578 1.9940638542175293
CurrentTrain: epoch  1, batch    37 | loss: 10.5882149Losses:  11.136687278747559 2.046522617340088
CurrentTrain: epoch  1, batch    38 | loss: 11.1366873Losses:  9.51327896118164 1.9593784809112549
CurrentTrain: epoch  1, batch    39 | loss: 9.5132790Losses:  10.339644432067871 1.8678584098815918
CurrentTrain: epoch  1, batch    40 | loss: 10.3396444Losses:  10.82325553894043 1.9909979104995728
CurrentTrain: epoch  1, batch    41 | loss: 10.8232555Losses:  10.674348831176758 1.880047082901001
CurrentTrain: epoch  1, batch    42 | loss: 10.6743488Losses:  11.140172958374023 1.9085571765899658
CurrentTrain: epoch  1, batch    43 | loss: 11.1401730Losses:  10.926202774047852 1.781400442123413
CurrentTrain: epoch  1, batch    44 | loss: 10.9262028Losses:  9.885169982910156 2.010835647583008
CurrentTrain: epoch  1, batch    45 | loss: 9.8851700Losses:  9.337606430053711 1.9448776245117188
CurrentTrain: epoch  1, batch    46 | loss: 9.3376064Losses:  10.723627090454102 2.127561092376709
CurrentTrain: epoch  1, batch    47 | loss: 10.7236271Losses:  10.018436431884766 1.9006314277648926
CurrentTrain: epoch  1, batch    48 | loss: 10.0184364Losses:  9.724689483642578 1.9128575325012207
CurrentTrain: epoch  1, batch    49 | loss: 9.7246895Losses:  9.940445899963379 1.8583606481552124
CurrentTrain: epoch  1, batch    50 | loss: 9.9404459Losses:  10.686724662780762 1.764775037765503
CurrentTrain: epoch  1, batch    51 | loss: 10.6867247Losses:  9.988454818725586 1.8370516300201416
CurrentTrain: epoch  1, batch    52 | loss: 9.9884548Losses:  10.093154907226562 1.8634536266326904
CurrentTrain: epoch  1, batch    53 | loss: 10.0931549Losses:  10.190820693969727 1.9474513530731201
CurrentTrain: epoch  1, batch    54 | loss: 10.1908207Losses:  8.714279174804688 1.710205078125
CurrentTrain: epoch  1, batch    55 | loss: 8.7142792Losses:  8.809286117553711 1.7289936542510986
CurrentTrain: epoch  1, batch    56 | loss: 8.8092861Losses:  10.08388900756836 1.9829211235046387
CurrentTrain: epoch  1, batch    57 | loss: 10.0838890Losses:  9.048974990844727 1.7607711553573608
CurrentTrain: epoch  1, batch    58 | loss: 9.0489750Losses:  10.74862289428711 2.0492148399353027
CurrentTrain: epoch  1, batch    59 | loss: 10.7486229Losses:  10.236167907714844 1.991424560546875
CurrentTrain: epoch  1, batch    60 | loss: 10.2361679Losses:  9.777494430541992 2.018437623977661
CurrentTrain: epoch  1, batch    61 | loss: 9.7774944Losses:  7.494307041168213 1.379908800125122
CurrentTrain: epoch  1, batch    62 | loss: 7.4943070Losses:  9.295185089111328 1.6676111221313477
CurrentTrain: epoch  2, batch     0 | loss: 9.2951851Losses:  9.294281005859375 1.6806771755218506
CurrentTrain: epoch  2, batch     1 | loss: 9.2942810Losses:  9.792577743530273 1.949286699295044
CurrentTrain: epoch  2, batch     2 | loss: 9.7925777Losses:  8.956266403198242 1.880975604057312
CurrentTrain: epoch  2, batch     3 | loss: 8.9562664Losses:  9.37417984008789 1.8685874938964844
CurrentTrain: epoch  2, batch     4 | loss: 9.3741798Losses:  9.666854858398438 2.1121628284454346
CurrentTrain: epoch  2, batch     5 | loss: 9.6668549Losses:  8.932449340820312 1.7427079677581787
CurrentTrain: epoch  2, batch     6 | loss: 8.9324493Losses:  9.973387718200684 1.9367135763168335
CurrentTrain: epoch  2, batch     7 | loss: 9.9733877Losses:  10.519286155700684 1.9159502983093262
CurrentTrain: epoch  2, batch     8 | loss: 10.5192862Losses:  8.890604972839355 1.8169150352478027
CurrentTrain: epoch  2, batch     9 | loss: 8.8906050Losses:  8.936372756958008 1.9004765748977661
CurrentTrain: epoch  2, batch    10 | loss: 8.9363728Losses:  9.750452041625977 1.8648860454559326
CurrentTrain: epoch  2, batch    11 | loss: 9.7504520Losses:  9.889522552490234 1.805945873260498
CurrentTrain: epoch  2, batch    12 | loss: 9.8895226Losses:  9.493589401245117 1.7768709659576416
CurrentTrain: epoch  2, batch    13 | loss: 9.4935894Losses:  9.617908477783203 1.9775760173797607
CurrentTrain: epoch  2, batch    14 | loss: 9.6179085Losses:  9.360929489135742 2.0185890197753906
CurrentTrain: epoch  2, batch    15 | loss: 9.3609295Losses:  8.815185546875 1.8897464275360107
CurrentTrain: epoch  2, batch    16 | loss: 8.8151855Losses:  7.891928672790527 1.5674657821655273
CurrentTrain: epoch  2, batch    17 | loss: 7.8919287Losses:  10.112337112426758 2.051584482192993
CurrentTrain: epoch  2, batch    18 | loss: 10.1123371Losses:  9.277350425720215 1.8715864419937134
CurrentTrain: epoch  2, batch    19 | loss: 9.2773504Losses:  9.289754867553711 1.939667820930481
CurrentTrain: epoch  2, batch    20 | loss: 9.2897549Losses:  9.030393600463867 1.9011095762252808
CurrentTrain: epoch  2, batch    21 | loss: 9.0303936Losses:  8.829902648925781 1.902604103088379
CurrentTrain: epoch  2, batch    22 | loss: 8.8299026Losses:  9.567440032958984 1.958036184310913
CurrentTrain: epoch  2, batch    23 | loss: 9.5674400Losses:  9.275028228759766 1.8460708856582642
CurrentTrain: epoch  2, batch    24 | loss: 9.2750282Losses:  9.040580749511719 1.9087756872177124
CurrentTrain: epoch  2, batch    25 | loss: 9.0405807Losses:  8.799819946289062 1.8952248096466064
CurrentTrain: epoch  2, batch    26 | loss: 8.7998199Losses:  8.568857192993164 1.6554410457611084
CurrentTrain: epoch  2, batch    27 | loss: 8.5688572Losses:  8.689788818359375 1.781417965888977
CurrentTrain: epoch  2, batch    28 | loss: 8.6897888Losses:  9.654985427856445 2.115107774734497
CurrentTrain: epoch  2, batch    29 | loss: 9.6549854Losses:  9.634612083435059 1.9937015771865845
CurrentTrain: epoch  2, batch    30 | loss: 9.6346121Losses:  9.314529418945312 2.1444010734558105
CurrentTrain: epoch  2, batch    31 | loss: 9.3145294Losses:  9.804072380065918 1.7622770071029663
CurrentTrain: epoch  2, batch    32 | loss: 9.8040724Losses:  8.446533203125 1.819132685661316
CurrentTrain: epoch  2, batch    33 | loss: 8.4465332Losses:  9.480478286743164 1.9960768222808838
CurrentTrain: epoch  2, batch    34 | loss: 9.4804783Losses:  9.326436996459961 1.8774032592773438
CurrentTrain: epoch  2, batch    35 | loss: 9.3264370Losses:  8.929498672485352 1.9827744960784912
CurrentTrain: epoch  2, batch    36 | loss: 8.9294987Losses:  8.881593704223633 1.7390551567077637
CurrentTrain: epoch  2, batch    37 | loss: 8.8815937Losses:  8.936395645141602 1.8436551094055176
CurrentTrain: epoch  2, batch    38 | loss: 8.9363956Losses:  8.168560028076172 1.7300400733947754
CurrentTrain: epoch  2, batch    39 | loss: 8.1685600Losses:  8.758889198303223 1.6537609100341797
CurrentTrain: epoch  2, batch    40 | loss: 8.7588892Losses:  8.935059547424316 1.488023281097412
CurrentTrain: epoch  2, batch    41 | loss: 8.9350595Losses:  8.959918975830078 1.9058759212493896
CurrentTrain: epoch  2, batch    42 | loss: 8.9599190Losses:  9.16090202331543 1.878540277481079
CurrentTrain: epoch  2, batch    43 | loss: 9.1609020Losses:  9.073780059814453 1.9934616088867188
CurrentTrain: epoch  2, batch    44 | loss: 9.0737801Losses:  9.367145538330078 1.9500508308410645
CurrentTrain: epoch  2, batch    45 | loss: 9.3671455Losses:  8.758286476135254 1.9514317512512207
CurrentTrain: epoch  2, batch    46 | loss: 8.7582865Losses:  9.522598266601562 1.9475187063217163
CurrentTrain: epoch  2, batch    47 | loss: 9.5225983Losses:  8.599981307983398 1.8626580238342285
CurrentTrain: epoch  2, batch    48 | loss: 8.5999813Losses:  9.31374454498291 1.9404311180114746
CurrentTrain: epoch  2, batch    49 | loss: 9.3137445Losses:  8.552008628845215 1.7542424201965332
CurrentTrain: epoch  2, batch    50 | loss: 8.5520086Losses:  8.76290512084961 1.9843738079071045
CurrentTrain: epoch  2, batch    51 | loss: 8.7629051Losses:  8.596219062805176 1.7549386024475098
CurrentTrain: epoch  2, batch    52 | loss: 8.5962191Losses:  9.11385726928711 2.0402884483337402
CurrentTrain: epoch  2, batch    53 | loss: 9.1138573Losses:  8.818086624145508 1.973592758178711
CurrentTrain: epoch  2, batch    54 | loss: 8.8180866Losses:  9.029638290405273 2.0380210876464844
CurrentTrain: epoch  2, batch    55 | loss: 9.0296383Losses:  9.544583320617676 2.109628200531006
CurrentTrain: epoch  2, batch    56 | loss: 9.5445833Losses:  8.910652160644531 1.8309495449066162
CurrentTrain: epoch  2, batch    57 | loss: 8.9106522Losses:  8.551192283630371 1.7766088247299194
CurrentTrain: epoch  2, batch    58 | loss: 8.5511923Losses:  8.726749420166016 1.9205281734466553
CurrentTrain: epoch  2, batch    59 | loss: 8.7267494Losses:  8.465719223022461 1.9095449447631836
CurrentTrain: epoch  2, batch    60 | loss: 8.4657192Losses:  8.650972366333008 1.839430809020996
CurrentTrain: epoch  2, batch    61 | loss: 8.6509724Losses:  8.069265365600586 1.634774088859558
CurrentTrain: epoch  2, batch    62 | loss: 8.0692654Losses:  9.295154571533203 1.8864715099334717
CurrentTrain: epoch  3, batch     0 | loss: 9.2951546Losses:  9.62807846069336 1.7927253246307373
CurrentTrain: epoch  3, batch     1 | loss: 9.6280785Losses:  8.889067649841309 1.9840048551559448
CurrentTrain: epoch  3, batch     2 | loss: 8.8890676Losses:  8.801725387573242 1.7314941883087158
CurrentTrain: epoch  3, batch     3 | loss: 8.8017254Losses:  8.954578399658203 1.8847987651824951
CurrentTrain: epoch  3, batch     4 | loss: 8.9545784Losses:  8.984785079956055 1.8693293333053589
CurrentTrain: epoch  3, batch     5 | loss: 8.9847851Losses:  8.679184913635254 1.8967821598052979
CurrentTrain: epoch  3, batch     6 | loss: 8.6791849Losses:  8.916742324829102 1.9864915609359741
CurrentTrain: epoch  3, batch     7 | loss: 8.9167423Losses:  9.115745544433594 1.8942244052886963
CurrentTrain: epoch  3, batch     8 | loss: 9.1157455Losses:  9.32697582244873 2.031264305114746
CurrentTrain: epoch  3, batch     9 | loss: 9.3269758Losses:  9.313573837280273 2.1153416633605957
CurrentTrain: epoch  3, batch    10 | loss: 9.3135738Losses:  8.775774002075195 1.8892529010772705
CurrentTrain: epoch  3, batch    11 | loss: 8.7757740Losses:  8.206374168395996 1.8426666259765625
CurrentTrain: epoch  3, batch    12 | loss: 8.2063742Losses:  9.238056182861328 2.105363130569458
CurrentTrain: epoch  3, batch    13 | loss: 9.2380562Losses:  8.657806396484375 1.952354907989502
CurrentTrain: epoch  3, batch    14 | loss: 8.6578064Losses:  8.631284713745117 1.976870059967041
CurrentTrain: epoch  3, batch    15 | loss: 8.6312847Losses:  8.903434753417969 1.9324162006378174
CurrentTrain: epoch  3, batch    16 | loss: 8.9034348Losses:  8.755960464477539 1.7311174869537354
CurrentTrain: epoch  3, batch    17 | loss: 8.7559605Losses:  8.757173538208008 1.850092887878418
CurrentTrain: epoch  3, batch    18 | loss: 8.7571735Losses:  8.68027114868164 1.8770533800125122
CurrentTrain: epoch  3, batch    19 | loss: 8.6802711Losses:  8.549392700195312 1.8855669498443604
CurrentTrain: epoch  3, batch    20 | loss: 8.5493927Losses:  8.827814102172852 2.022794246673584
CurrentTrain: epoch  3, batch    21 | loss: 8.8278141Losses:  8.358097076416016 1.7489575147628784
CurrentTrain: epoch  3, batch    22 | loss: 8.3580971Losses:  8.726869583129883 2.0147480964660645
CurrentTrain: epoch  3, batch    23 | loss: 8.7268696Losses:  7.9988203048706055 1.6471911668777466
CurrentTrain: epoch  3, batch    24 | loss: 7.9988203Losses:  8.764698028564453 2.0971407890319824
CurrentTrain: epoch  3, batch    25 | loss: 8.7646980Losses:  9.026531219482422 2.1119837760925293
CurrentTrain: epoch  3, batch    26 | loss: 9.0265312Losses:  8.650906562805176 1.9533815383911133
CurrentTrain: epoch  3, batch    27 | loss: 8.6509066Losses:  9.083910942077637 2.1411046981811523
CurrentTrain: epoch  3, batch    28 | loss: 9.0839109Losses:  8.937349319458008 1.863542079925537
CurrentTrain: epoch  3, batch    29 | loss: 8.9373493Losses:  9.603561401367188 2.0733132362365723
CurrentTrain: epoch  3, batch    30 | loss: 9.6035614Losses:  8.277301788330078 1.8740054368972778
CurrentTrain: epoch  3, batch    31 | loss: 8.2773018Losses:  8.11581802368164 1.646730661392212
CurrentTrain: epoch  3, batch    32 | loss: 8.1158180Losses:  8.461201667785645 1.8175621032714844
CurrentTrain: epoch  3, batch    33 | loss: 8.4612017Losses:  8.377482414245605 1.8834948539733887
CurrentTrain: epoch  3, batch    34 | loss: 8.3774824Losses:  8.52785873413086 1.9513441324234009
CurrentTrain: epoch  3, batch    35 | loss: 8.5278587Losses:  9.329713821411133 1.9711052179336548
CurrentTrain: epoch  3, batch    36 | loss: 9.3297138Losses:  8.576617240905762 1.9032890796661377
CurrentTrain: epoch  3, batch    37 | loss: 8.5766172Losses:  8.48588752746582 1.9913558959960938
CurrentTrain: epoch  3, batch    38 | loss: 8.4858875Losses:  8.792903900146484 1.669050931930542
CurrentTrain: epoch  3, batch    39 | loss: 8.7929039Losses:  8.672059059143066 2.0131125450134277
CurrentTrain: epoch  3, batch    40 | loss: 8.6720591Losses:  7.886436462402344 1.724663496017456
CurrentTrain: epoch  3, batch    41 | loss: 7.8864365Losses:  8.498069763183594 1.739556074142456
CurrentTrain: epoch  3, batch    42 | loss: 8.4980698Losses:  8.311059951782227 1.8003900051116943
CurrentTrain: epoch  3, batch    43 | loss: 8.3110600Losses:  8.949213027954102 2.023808479309082
CurrentTrain: epoch  3, batch    44 | loss: 8.9492130Losses:  8.489669799804688 1.7932381629943848
CurrentTrain: epoch  3, batch    45 | loss: 8.4896698Losses:  8.385324478149414 1.9873954057693481
CurrentTrain: epoch  3, batch    46 | loss: 8.3853245Losses:  8.385772705078125 1.73152756690979
CurrentTrain: epoch  3, batch    47 | loss: 8.3857727Losses:  7.701952934265137 1.5617066621780396
CurrentTrain: epoch  3, batch    48 | loss: 7.7019529Losses:  7.881699562072754 1.598353385925293
CurrentTrain: epoch  3, batch    49 | loss: 7.8816996Losses:  8.506952285766602 1.9856579303741455
CurrentTrain: epoch  3, batch    50 | loss: 8.5069523Losses:  8.316555976867676 1.7563196420669556
CurrentTrain: epoch  3, batch    51 | loss: 8.3165560Losses:  8.241449356079102 1.858642339706421
CurrentTrain: epoch  3, batch    52 | loss: 8.2414494Losses:  9.0226469039917 1.9271821975708008
CurrentTrain: epoch  3, batch    53 | loss: 9.0226469Losses:  8.332555770874023 1.924238681793213
CurrentTrain: epoch  3, batch    54 | loss: 8.3325558Losses:  8.833728790283203 2.102811813354492
CurrentTrain: epoch  3, batch    55 | loss: 8.8337288Losses:  8.087940216064453 1.829018473625183
CurrentTrain: epoch  3, batch    56 | loss: 8.0879402Losses:  8.555286407470703 1.8456833362579346
CurrentTrain: epoch  3, batch    57 | loss: 8.5552864Losses:  8.61018180847168 1.8298702239990234
CurrentTrain: epoch  3, batch    58 | loss: 8.6101818Losses:  7.830152988433838 1.6313211917877197
CurrentTrain: epoch  3, batch    59 | loss: 7.8301530Losses:  8.591957092285156 2.042891263961792
CurrentTrain: epoch  3, batch    60 | loss: 8.5919571Losses:  8.232316970825195 1.9103608131408691
CurrentTrain: epoch  3, batch    61 | loss: 8.2323170Losses:  7.292895793914795 1.4308321475982666
CurrentTrain: epoch  3, batch    62 | loss: 7.2928958Losses:  7.43642520904541 1.526803970336914
CurrentTrain: epoch  4, batch     0 | loss: 7.4364252Losses:  8.010859489440918 1.7529748678207397
CurrentTrain: epoch  4, batch     1 | loss: 8.0108595Losses:  8.31701374053955 1.8944100141525269
CurrentTrain: epoch  4, batch     2 | loss: 8.3170137Losses:  7.759693622589111 1.6973686218261719
CurrentTrain: epoch  4, batch     3 | loss: 7.7596936Losses:  8.72675895690918 2.108590841293335
CurrentTrain: epoch  4, batch     4 | loss: 8.7267590Losses:  8.856744766235352 2.1108434200286865
CurrentTrain: epoch  4, batch     5 | loss: 8.8567448Losses:  8.153985977172852 1.8280675411224365
CurrentTrain: epoch  4, batch     6 | loss: 8.1539860Losses:  7.794252872467041 1.687579870223999
CurrentTrain: epoch  4, batch     7 | loss: 7.7942529Losses:  8.588396072387695 2.092921495437622
CurrentTrain: epoch  4, batch     8 | loss: 8.5883961Losses:  8.127813339233398 1.8585257530212402
CurrentTrain: epoch  4, batch     9 | loss: 8.1278133Losses:  8.112449645996094 1.8158421516418457
CurrentTrain: epoch  4, batch    10 | loss: 8.1124496Losses:  8.541993141174316 1.856584072113037
CurrentTrain: epoch  4, batch    11 | loss: 8.5419931Losses:  8.259106636047363 1.863946557044983
CurrentTrain: epoch  4, batch    12 | loss: 8.2591066Losses:  8.568338394165039 1.974814534187317
CurrentTrain: epoch  4, batch    13 | loss: 8.5683384Losses:  7.557609558105469 1.6665496826171875
CurrentTrain: epoch  4, batch    14 | loss: 7.5576096Losses:  7.967307090759277 1.7737133502960205
CurrentTrain: epoch  4, batch    15 | loss: 7.9673071Losses:  8.101207733154297 1.8261971473693848
CurrentTrain: epoch  4, batch    16 | loss: 8.1012077Losses:  7.8923797607421875 1.7714729309082031
CurrentTrain: epoch  4, batch    17 | loss: 7.8923798Losses:  8.327778816223145 1.9641640186309814
CurrentTrain: epoch  4, batch    18 | loss: 8.3277788Losses:  8.299159049987793 1.9449751377105713
CurrentTrain: epoch  4, batch    19 | loss: 8.2991590Losses:  8.338613510131836 1.7514004707336426
CurrentTrain: epoch  4, batch    20 | loss: 8.3386135Losses:  7.9170074462890625 1.6440730094909668
CurrentTrain: epoch  4, batch    21 | loss: 7.9170074Losses:  7.782403945922852 1.7632676362991333
CurrentTrain: epoch  4, batch    22 | loss: 7.7824039Losses:  7.257081508636475 1.5114693641662598
CurrentTrain: epoch  4, batch    23 | loss: 7.2570815Losses:  7.656803131103516 1.5572080612182617
CurrentTrain: epoch  4, batch    24 | loss: 7.6568031Losses:  8.449762344360352 2.019364595413208
CurrentTrain: epoch  4, batch    25 | loss: 8.4497623Losses:  8.075011253356934 1.9004930257797241
CurrentTrain: epoch  4, batch    26 | loss: 8.0750113Losses:  7.838098049163818 1.7930593490600586
CurrentTrain: epoch  4, batch    27 | loss: 7.8380980Losses:  7.979063987731934 1.792341709136963
CurrentTrain: epoch  4, batch    28 | loss: 7.9790640Losses:  7.953925609588623 1.7878124713897705
CurrentTrain: epoch  4, batch    29 | loss: 7.9539256Losses:  7.948136806488037 1.8478374481201172
CurrentTrain: epoch  4, batch    30 | loss: 7.9481368Losses:  8.04339599609375 1.812744379043579
CurrentTrain: epoch  4, batch    31 | loss: 8.0433960Losses:  8.29381275177002 1.8774548768997192
CurrentTrain: epoch  4, batch    32 | loss: 8.2938128Losses:  8.420459747314453 1.9795050621032715
CurrentTrain: epoch  4, batch    33 | loss: 8.4204597Losses:  8.019974708557129 1.8457574844360352
CurrentTrain: epoch  4, batch    34 | loss: 8.0199747Losses:  8.549429893493652 2.0259809494018555
CurrentTrain: epoch  4, batch    35 | loss: 8.5494299Losses:  7.802701950073242 1.6981501579284668
CurrentTrain: epoch  4, batch    36 | loss: 7.8027020Losses:  8.74712085723877 1.9927523136138916
CurrentTrain: epoch  4, batch    37 | loss: 8.7471209Losses:  8.080309867858887 1.8101221323013306
CurrentTrain: epoch  4, batch    38 | loss: 8.0803099Losses:  7.995589256286621 1.8528423309326172
CurrentTrain: epoch  4, batch    39 | loss: 7.9955893Losses:  7.848905563354492 1.792676329612732
CurrentTrain: epoch  4, batch    40 | loss: 7.8489056Losses:  8.030024528503418 1.905596375465393
CurrentTrain: epoch  4, batch    41 | loss: 8.0300245Losses:  8.764896392822266 1.8677585124969482
CurrentTrain: epoch  4, batch    42 | loss: 8.7648964Losses:  7.890200614929199 1.5821160078048706
CurrentTrain: epoch  4, batch    43 | loss: 7.8902006Losses:  7.464685440063477 1.6429743766784668
CurrentTrain: epoch  4, batch    44 | loss: 7.4646854Losses:  8.363251686096191 1.692335605621338
CurrentTrain: epoch  4, batch    45 | loss: 8.3632517Losses:  8.429605484008789 1.878343939781189
CurrentTrain: epoch  4, batch    46 | loss: 8.4296055Losses:  8.328874588012695 1.8906323909759521
CurrentTrain: epoch  4, batch    47 | loss: 8.3288746Losses:  8.368027687072754 2.010986328125
CurrentTrain: epoch  4, batch    48 | loss: 8.3680277Losses:  8.16758918762207 1.9085988998413086
CurrentTrain: epoch  4, batch    49 | loss: 8.1675892Losses:  8.708751678466797 2.0927305221557617
CurrentTrain: epoch  4, batch    50 | loss: 8.7087517Losses:  7.89499568939209 1.7768460512161255
CurrentTrain: epoch  4, batch    51 | loss: 7.8949957Losses:  7.605556964874268 1.6644251346588135
CurrentTrain: epoch  4, batch    52 | loss: 7.6055570Losses:  8.286941528320312 1.8525806665420532
CurrentTrain: epoch  4, batch    53 | loss: 8.2869415Losses:  7.929494857788086 1.7375807762145996
CurrentTrain: epoch  4, batch    54 | loss: 7.9294949Losses:  8.106269836425781 1.9074430465698242
CurrentTrain: epoch  4, batch    55 | loss: 8.1062698Losses:  7.789730072021484 1.7217351198196411
CurrentTrain: epoch  4, batch    56 | loss: 7.7897301Losses:  8.53567123413086 2.0923538208007812
CurrentTrain: epoch  4, batch    57 | loss: 8.5356712Losses:  7.969917297363281 1.8362704515457153
CurrentTrain: epoch  4, batch    58 | loss: 7.9699173Losses:  7.997737407684326 1.8535659313201904
CurrentTrain: epoch  4, batch    59 | loss: 7.9977374Losses:  7.943148612976074 1.8335723876953125
CurrentTrain: epoch  4, batch    60 | loss: 7.9431486Losses:  8.235663414001465 1.919967770576477
CurrentTrain: epoch  4, batch    61 | loss: 8.2356634Losses:  7.2409186363220215 1.4672491550445557
CurrentTrain: epoch  4, batch    62 | loss: 7.2409186Losses:  8.545866012573242 2.063190460205078
CurrentTrain: epoch  5, batch     0 | loss: 8.5458660Losses:  8.231391906738281 1.9572656154632568
CurrentTrain: epoch  5, batch     1 | loss: 8.2313919Losses:  7.6521687507629395 1.6837475299835205
CurrentTrain: epoch  5, batch     2 | loss: 7.6521688Losses:  7.954137325286865 1.8348183631896973
CurrentTrain: epoch  5, batch     3 | loss: 7.9541373Losses:  7.96924352645874 1.7951161861419678
CurrentTrain: epoch  5, batch     4 | loss: 7.9692435Losses:  8.047731399536133 1.8401803970336914
CurrentTrain: epoch  5, batch     5 | loss: 8.0477314Losses:  8.21943187713623 1.9234870672225952
CurrentTrain: epoch  5, batch     6 | loss: 8.2194319Losses:  8.105477333068848 1.9488319158554077
CurrentTrain: epoch  5, batch     7 | loss: 8.1054773Losses:  8.23319149017334 1.9439088106155396
CurrentTrain: epoch  5, batch     8 | loss: 8.2331915Losses:  7.557633399963379 1.6647419929504395
CurrentTrain: epoch  5, batch     9 | loss: 7.5576334Losses:  8.546856880187988 1.9839991331100464
CurrentTrain: epoch  5, batch    10 | loss: 8.5468569Losses:  8.324063301086426 2.019205093383789
CurrentTrain: epoch  5, batch    11 | loss: 8.3240633Losses:  7.944587707519531 1.8634618520736694
CurrentTrain: epoch  5, batch    12 | loss: 7.9445877Losses:  7.828973770141602 1.801052212715149
CurrentTrain: epoch  5, batch    13 | loss: 7.8289738Losses:  8.38779354095459 2.1114721298217773
CurrentTrain: epoch  5, batch    14 | loss: 8.3877935Losses:  7.9268693923950195 1.848001480102539
CurrentTrain: epoch  5, batch    15 | loss: 7.9268694Losses:  8.429788589477539 2.064100742340088
CurrentTrain: epoch  5, batch    16 | loss: 8.4297886Losses:  8.164077758789062 1.988457202911377
CurrentTrain: epoch  5, batch    17 | loss: 8.1640778Losses:  8.12667179107666 1.9316415786743164
CurrentTrain: epoch  5, batch    18 | loss: 8.1266718Losses:  7.959962368011475 1.860217571258545
CurrentTrain: epoch  5, batch    19 | loss: 7.9599624Losses:  7.936380386352539 1.874794363975525
CurrentTrain: epoch  5, batch    20 | loss: 7.9363804Losses:  7.9357757568359375 1.8321925401687622
CurrentTrain: epoch  5, batch    21 | loss: 7.9357758Losses:  7.752396583557129 1.7444466352462769
CurrentTrain: epoch  5, batch    22 | loss: 7.7523966Losses:  7.972159385681152 1.8492059707641602
CurrentTrain: epoch  5, batch    23 | loss: 7.9721594Losses:  7.696560859680176 1.7337340116500854
CurrentTrain: epoch  5, batch    24 | loss: 7.6965609Losses:  8.167559623718262 1.965752124786377
CurrentTrain: epoch  5, batch    25 | loss: 8.1675596Losses:  8.119182586669922 1.9276381731033325
CurrentTrain: epoch  5, batch    26 | loss: 8.1191826Losses:  7.819541931152344 1.8060555458068848
CurrentTrain: epoch  5, batch    27 | loss: 7.8195419Losses:  8.222869873046875 2.015942096710205
CurrentTrain: epoch  5, batch    28 | loss: 8.2228699Losses:  8.08785629272461 1.9607977867126465
CurrentTrain: epoch  5, batch    29 | loss: 8.0878563Losses:  7.563993453979492 1.7010606527328491
CurrentTrain: epoch  5, batch    30 | loss: 7.5639935Losses:  7.73762321472168 1.7848215103149414
CurrentTrain: epoch  5, batch    31 | loss: 7.7376232Losses:  7.864370822906494 1.7977015972137451
CurrentTrain: epoch  5, batch    32 | loss: 7.8643708Losses:  7.22476053237915 1.5596239566802979
CurrentTrain: epoch  5, batch    33 | loss: 7.2247605Losses:  7.895878791809082 1.8255740404129028
CurrentTrain: epoch  5, batch    34 | loss: 7.8958788Losses:  7.558323383331299 1.7098231315612793
CurrentTrain: epoch  5, batch    35 | loss: 7.5583234Losses:  7.581402778625488 1.7021970748901367
CurrentTrain: epoch  5, batch    36 | loss: 7.5814028Losses:  7.704799652099609 1.785789966583252
CurrentTrain: epoch  5, batch    37 | loss: 7.7047997Losses:  7.7642598152160645 1.7920732498168945
CurrentTrain: epoch  5, batch    38 | loss: 7.7642598Losses:  8.127100944519043 1.9387871026992798
CurrentTrain: epoch  5, batch    39 | loss: 8.1271009Losses:  8.25455093383789 2.070828437805176
CurrentTrain: epoch  5, batch    40 | loss: 8.2545509Losses:  7.787802696228027 1.8014321327209473
CurrentTrain: epoch  5, batch    41 | loss: 7.7878027Losses:  8.244161605834961 1.9725404977798462
CurrentTrain: epoch  5, batch    42 | loss: 8.2441616Losses:  8.058958053588867 1.7966995239257812
CurrentTrain: epoch  5, batch    43 | loss: 8.0589581Losses:  7.867504596710205 1.854461669921875
CurrentTrain: epoch  5, batch    44 | loss: 7.8675046Losses:  7.805296421051025 1.8412983417510986
CurrentTrain: epoch  5, batch    45 | loss: 7.8052964Losses:  7.987892150878906 1.9171148538589478
CurrentTrain: epoch  5, batch    46 | loss: 7.9878922Losses:  7.993176460266113 1.9226994514465332
CurrentTrain: epoch  5, batch    47 | loss: 7.9931765Losses:  8.08980655670166 1.9596896171569824
CurrentTrain: epoch  5, batch    48 | loss: 8.0898066Losses:  8.030807495117188 1.932988166809082
CurrentTrain: epoch  5, batch    49 | loss: 8.0308075Losses:  7.943866729736328 1.9159204959869385
CurrentTrain: epoch  5, batch    50 | loss: 7.9438667Losses:  7.597559928894043 1.7331061363220215
CurrentTrain: epoch  5, batch    51 | loss: 7.5975599Losses:  8.28420639038086 2.0342495441436768
CurrentTrain: epoch  5, batch    52 | loss: 8.2842064Losses:  7.921302318572998 1.873547077178955
CurrentTrain: epoch  5, batch    53 | loss: 7.9213023Losses:  8.201309204101562 2.044644832611084
CurrentTrain: epoch  5, batch    54 | loss: 8.2013092Losses:  8.083778381347656 1.9275763034820557
CurrentTrain: epoch  5, batch    55 | loss: 8.0837784Losses:  8.229703903198242 2.037503480911255
CurrentTrain: epoch  5, batch    56 | loss: 8.2297039Losses:  7.736993312835693 1.7341508865356445
CurrentTrain: epoch  5, batch    57 | loss: 7.7369933Losses:  7.909491539001465 1.7752580642700195
CurrentTrain: epoch  5, batch    58 | loss: 7.9094915Losses:  7.651821136474609 1.7256969213485718
CurrentTrain: epoch  5, batch    59 | loss: 7.6518211Losses:  7.7562360763549805 1.8181931972503662
CurrentTrain: epoch  5, batch    60 | loss: 7.7562361Losses:  7.645423412322998 1.767076015472412
CurrentTrain: epoch  5, batch    61 | loss: 7.6454234Losses:  7.4632134437561035 1.628079891204834
CurrentTrain: epoch  5, batch    62 | loss: 7.4632134Losses:  7.841533660888672 1.818916916847229
CurrentTrain: epoch  6, batch     0 | loss: 7.8415337Losses:  7.937456130981445 1.9197540283203125
CurrentTrain: epoch  6, batch     1 | loss: 7.9374561Losses:  7.614173412322998 1.7707951068878174
CurrentTrain: epoch  6, batch     2 | loss: 7.6141734Losses:  8.552108764648438 2.111035108566284
CurrentTrain: epoch  6, batch     3 | loss: 8.5521088Losses:  7.4134039878845215 1.6391181945800781
CurrentTrain: epoch  6, batch     4 | loss: 7.4134040Losses:  8.074036598205566 1.9605603218078613
CurrentTrain: epoch  6, batch     5 | loss: 8.0740366Losses:  8.08936882019043 1.9694883823394775
CurrentTrain: epoch  6, batch     6 | loss: 8.0893688Losses:  7.79410457611084 1.820336937904358
CurrentTrain: epoch  6, batch     7 | loss: 7.7941046Losses:  8.0286865234375 1.969103455543518
CurrentTrain: epoch  6, batch     8 | loss: 8.0286865Losses:  7.340909481048584 1.6032521724700928
CurrentTrain: epoch  6, batch     9 | loss: 7.3409095Losses:  8.037882804870605 1.949145793914795
CurrentTrain: epoch  6, batch    10 | loss: 8.0378828Losses:  7.453300476074219 1.6414082050323486
CurrentTrain: epoch  6, batch    11 | loss: 7.4533005Losses:  8.295473098754883 2.0523769855499268
CurrentTrain: epoch  6, batch    12 | loss: 8.2954731Losses:  7.588573455810547 1.7344905138015747
CurrentTrain: epoch  6, batch    13 | loss: 7.5885735Losses:  8.178886413574219 2.0138626098632812
CurrentTrain: epoch  6, batch    14 | loss: 8.1788864Losses:  8.032817840576172 1.943556547164917
CurrentTrain: epoch  6, batch    15 | loss: 8.0328178Losses:  8.078401565551758 1.9711337089538574
CurrentTrain: epoch  6, batch    16 | loss: 8.0784016Losses:  8.027624130249023 1.9546434879302979
CurrentTrain: epoch  6, batch    17 | loss: 8.0276241Losses:  6.820315837860107 1.333390474319458
CurrentTrain: epoch  6, batch    18 | loss: 6.8203158Losses:  8.027002334594727 1.9592156410217285
CurrentTrain: epoch  6, batch    19 | loss: 8.0270023Losses:  8.122795104980469 2.005937099456787
CurrentTrain: epoch  6, batch    20 | loss: 8.1227951Losses:  7.8817853927612305 1.84381902217865
CurrentTrain: epoch  6, batch    21 | loss: 7.8817854Losses:  7.58653450012207 1.7059950828552246
CurrentTrain: epoch  6, batch    22 | loss: 7.5865345Losses:  7.445865631103516 1.5281147956848145
CurrentTrain: epoch  6, batch    23 | loss: 7.4458656Losses:  7.675257682800293 1.756338357925415
CurrentTrain: epoch  6, batch    24 | loss: 7.6752577Losses:  8.12602424621582 1.9629108905792236
CurrentTrain: epoch  6, batch    25 | loss: 8.1260242Losses:  8.070539474487305 1.9578213691711426
CurrentTrain: epoch  6, batch    26 | loss: 8.0705395Losses:  7.616668701171875 1.7593121528625488
CurrentTrain: epoch  6, batch    27 | loss: 7.6166687Losses:  7.877345085144043 1.9009888172149658
CurrentTrain: epoch  6, batch    28 | loss: 7.8773451Losses:  8.011603355407715 1.9400086402893066
CurrentTrain: epoch  6, batch    29 | loss: 8.0116034Losses:  7.981113433837891 1.9436962604522705
CurrentTrain: epoch  6, batch    30 | loss: 7.9811134Losses:  7.160399913787842 1.5600922107696533
CurrentTrain: epoch  6, batch    31 | loss: 7.1603999Losses:  7.669491767883301 1.7556530237197876
CurrentTrain: epoch  6, batch    32 | loss: 7.6694918Losses:  8.02975082397461 1.9465537071228027
CurrentTrain: epoch  6, batch    33 | loss: 8.0297508Losses:  7.700177192687988 1.783996820449829
CurrentTrain: epoch  6, batch    34 | loss: 7.7001772Losses:  7.316658973693848 1.5858726501464844
CurrentTrain: epoch  6, batch    35 | loss: 7.3166590Losses:  7.0356764793396 1.4317548274993896
CurrentTrain: epoch  6, batch    36 | loss: 7.0356765Losses:  8.085765838623047 1.9776945114135742
CurrentTrain: epoch  6, batch    37 | loss: 8.0857658Losses:  8.232222557067871 2.042491912841797
CurrentTrain: epoch  6, batch    38 | loss: 8.2322226Losses:  7.555019378662109 1.7417746782302856
CurrentTrain: epoch  6, batch    39 | loss: 7.5550194Losses:  8.1455078125 2.0011866092681885
CurrentTrain: epoch  6, batch    40 | loss: 8.1455078Losses:  8.232990264892578 2.0626370906829834
CurrentTrain: epoch  6, batch    41 | loss: 8.2329903Losses:  8.050848007202148 1.9845657348632812
CurrentTrain: epoch  6, batch    42 | loss: 8.0508480Losses:  7.51386022567749 1.7384331226348877
CurrentTrain: epoch  6, batch    43 | loss: 7.5138602Losses:  7.681358337402344 1.8029394149780273
CurrentTrain: epoch  6, batch    44 | loss: 7.6813583Losses:  7.9611310958862305 1.915345311164856
CurrentTrain: epoch  6, batch    45 | loss: 7.9611311Losses:  7.621025085449219 1.758948564529419
CurrentTrain: epoch  6, batch    46 | loss: 7.6210251Losses:  7.553683280944824 1.7177494764328003
CurrentTrain: epoch  6, batch    47 | loss: 7.5536833Losses:  8.218442916870117 2.0683023929595947
CurrentTrain: epoch  6, batch    48 | loss: 8.2184429Losses:  7.362348556518555 1.6335158348083496
CurrentTrain: epoch  6, batch    49 | loss: 7.3623486Losses:  8.168342590332031 2.02756929397583
CurrentTrain: epoch  6, batch    50 | loss: 8.1683426Losses:  8.035958290100098 1.9877997636795044
CurrentTrain: epoch  6, batch    51 | loss: 8.0359583Losses:  7.988517761230469 1.9509928226470947
CurrentTrain: epoch  6, batch    52 | loss: 7.9885178Losses:  6.821084976196289 1.3593859672546387
CurrentTrain: epoch  6, batch    53 | loss: 6.8210850Losses:  7.84331750869751 1.8838434219360352
CurrentTrain: epoch  6, batch    54 | loss: 7.8433175Losses:  7.525946617126465 1.7367148399353027
CurrentTrain: epoch  6, batch    55 | loss: 7.5259466Losses:  7.278201103210449 1.5710301399230957
CurrentTrain: epoch  6, batch    56 | loss: 7.2782011Losses:  7.995122909545898 1.9712142944335938
CurrentTrain: epoch  6, batch    57 | loss: 7.9951229Losses:  7.782035827636719 1.8517169952392578
CurrentTrain: epoch  6, batch    58 | loss: 7.7820358Losses:  7.931780815124512 1.9240106344223022
CurrentTrain: epoch  6, batch    59 | loss: 7.9317808Losses:  7.2699480056762695 1.5727496147155762
CurrentTrain: epoch  6, batch    60 | loss: 7.2699480Losses:  7.734367370605469 1.8008471727371216
CurrentTrain: epoch  6, batch    61 | loss: 7.7343674Losses:  7.108091831207275 1.5266854763031006
CurrentTrain: epoch  6, batch    62 | loss: 7.1080918Losses:  7.568775653839111 1.7292401790618896
CurrentTrain: epoch  7, batch     0 | loss: 7.5687757Losses:  7.477301597595215 1.6891154050827026
CurrentTrain: epoch  7, batch     1 | loss: 7.4773016Losses:  7.6899333000183105 1.8049499988555908
CurrentTrain: epoch  7, batch     2 | loss: 7.6899333Losses:  8.063457489013672 1.9922462701797485
CurrentTrain: epoch  7, batch     3 | loss: 8.0634575Losses:  7.373038291931152 1.6451233625411987
CurrentTrain: epoch  7, batch     4 | loss: 7.3730383Losses:  7.909992218017578 1.926705002784729
CurrentTrain: epoch  7, batch     5 | loss: 7.9099922Losses:  7.929323673248291 1.9310901165008545
CurrentTrain: epoch  7, batch     6 | loss: 7.9293237Losses:  8.07909107208252 1.973427414894104
CurrentTrain: epoch  7, batch     7 | loss: 8.0790911Losses:  7.366987228393555 1.6804156303405762
CurrentTrain: epoch  7, batch     8 | loss: 7.3669872Losses:  7.533885955810547 1.7402080297470093
CurrentTrain: epoch  7, batch     9 | loss: 7.5338860Losses:  7.709079742431641 1.8344391584396362
CurrentTrain: epoch  7, batch    10 | loss: 7.7090797Losses:  7.500114917755127 1.74269700050354
CurrentTrain: epoch  7, batch    11 | loss: 7.5001149Losses:  7.153509140014648 1.5619655847549438
CurrentTrain: epoch  7, batch    12 | loss: 7.1535091Losses:  7.569838523864746 1.7464027404785156
CurrentTrain: epoch  7, batch    13 | loss: 7.5698385Losses:  7.492146015167236 1.7338318824768066
CurrentTrain: epoch  7, batch    14 | loss: 7.4921460Losses:  7.625349521636963 1.8146553039550781
CurrentTrain: epoch  7, batch    15 | loss: 7.6253495Losses:  7.654397010803223 1.7827692031860352
CurrentTrain: epoch  7, batch    16 | loss: 7.6543970Losses:  7.844181060791016 1.8001983165740967
CurrentTrain: epoch  7, batch    17 | loss: 7.8441811Losses:  8.019498825073242 1.9813530445098877
CurrentTrain: epoch  7, batch    18 | loss: 8.0194988Losses:  7.980893135070801 1.941882848739624
CurrentTrain: epoch  7, batch    19 | loss: 7.9808931Losses:  7.534156322479248 1.7216567993164062
CurrentTrain: epoch  7, batch    20 | loss: 7.5341563Losses:  7.512347221374512 1.7465591430664062
CurrentTrain: epoch  7, batch    21 | loss: 7.5123472Losses:  7.361924171447754 1.6499409675598145
CurrentTrain: epoch  7, batch    22 | loss: 7.3619242Losses:  7.222955226898193 1.5775668621063232
CurrentTrain: epoch  7, batch    23 | loss: 7.2229552Losses:  7.619348526000977 1.7896002531051636
CurrentTrain: epoch  7, batch    24 | loss: 7.6193485Losses:  7.338090896606445 1.5913817882537842
CurrentTrain: epoch  7, batch    25 | loss: 7.3380909Losses:  7.512208461761475 1.7445347309112549
CurrentTrain: epoch  7, batch    26 | loss: 7.5122085Losses:  7.870909690856934 1.9082858562469482
CurrentTrain: epoch  7, batch    27 | loss: 7.8709097Losses:  7.6802077293396 1.8076832294464111
CurrentTrain: epoch  7, batch    28 | loss: 7.6802077Losses:  7.504878997802734 1.7151786088943481
CurrentTrain: epoch  7, batch    29 | loss: 7.5048790Losses:  7.822309494018555 1.8676505088806152
CurrentTrain: epoch  7, batch    30 | loss: 7.8223095Losses:  7.722001075744629 1.8251829147338867
CurrentTrain: epoch  7, batch    31 | loss: 7.7220011Losses:  7.21235466003418 1.573992371559143
CurrentTrain: epoch  7, batch    32 | loss: 7.2123547Losses:  7.610291004180908 1.7249095439910889
CurrentTrain: epoch  7, batch    33 | loss: 7.6102910Losses:  7.334602355957031 1.6146026849746704
CurrentTrain: epoch  7, batch    34 | loss: 7.3346024Losses:  7.819300651550293 1.8800058364868164
CurrentTrain: epoch  7, batch    35 | loss: 7.8193007Losses:  7.730158805847168 1.8043384552001953
CurrentTrain: epoch  7, batch    36 | loss: 7.7301588Losses:  7.46702766418457 1.7193617820739746
CurrentTrain: epoch  7, batch    37 | loss: 7.4670277Losses:  8.005849838256836 1.9746956825256348
CurrentTrain: epoch  7, batch    38 | loss: 8.0058498Losses:  7.463845729827881 1.7223689556121826
CurrentTrain: epoch  7, batch    39 | loss: 7.4638457Losses:  7.888315200805664 1.9223577976226807
CurrentTrain: epoch  7, batch    40 | loss: 7.8883152Losses:  7.393502235412598 1.6906768083572388
CurrentTrain: epoch  7, batch    41 | loss: 7.3935022Losses:  7.622469425201416 1.7954893112182617
CurrentTrain: epoch  7, batch    42 | loss: 7.6224694Losses:  7.586244583129883 1.7675576210021973
CurrentTrain: epoch  7, batch    43 | loss: 7.5862446Losses:  7.771670341491699 1.8675861358642578
CurrentTrain: epoch  7, batch    44 | loss: 7.7716703Losses:  7.825562477111816 1.9191373586654663
CurrentTrain: epoch  7, batch    45 | loss: 7.8255625Losses:  7.489297866821289 1.7225594520568848
CurrentTrain: epoch  7, batch    46 | loss: 7.4892979Losses:  7.762199401855469 1.8638641834259033
CurrentTrain: epoch  7, batch    47 | loss: 7.7621994Losses:  7.7657976150512695 1.8728687763214111
CurrentTrain: epoch  7, batch    48 | loss: 7.7657976Losses:  7.840625762939453 1.91238534450531
CurrentTrain: epoch  7, batch    49 | loss: 7.8406258Losses:  7.249118804931641 1.6423261165618896
CurrentTrain: epoch  7, batch    50 | loss: 7.2491188Losses:  7.8860063552856445 1.9281868934631348
CurrentTrain: epoch  7, batch    51 | loss: 7.8860064Losses:  7.190245628356934 1.5508676767349243
CurrentTrain: epoch  7, batch    52 | loss: 7.1902456Losses:  7.7660017013549805 1.8491618633270264
CurrentTrain: epoch  7, batch    53 | loss: 7.7660017Losses:  7.588858127593994 1.7608544826507568
CurrentTrain: epoch  7, batch    54 | loss: 7.5888581Losses:  7.41610860824585 1.6904804706573486
CurrentTrain: epoch  7, batch    55 | loss: 7.4161086Losses:  7.208001613616943 1.5779764652252197
CurrentTrain: epoch  7, batch    56 | loss: 7.2080016Losses:  7.890430927276611 1.9345755577087402
CurrentTrain: epoch  7, batch    57 | loss: 7.8904309Losses:  7.917438507080078 1.950493574142456
CurrentTrain: epoch  7, batch    58 | loss: 7.9174385Losses:  7.1972246170043945 1.5729269981384277
CurrentTrain: epoch  7, batch    59 | loss: 7.1972246Losses:  7.982028961181641 1.9616175889968872
CurrentTrain: epoch  7, batch    60 | loss: 7.9820290Losses:  7.640623092651367 1.7988861799240112
CurrentTrain: epoch  7, batch    61 | loss: 7.6406231Losses:  7.195688247680664 1.562382698059082
CurrentTrain: epoch  7, batch    62 | loss: 7.1956882Losses:  8.065673828125 2.0182156562805176
CurrentTrain: epoch  8, batch     0 | loss: 8.0656738Losses:  7.747337818145752 1.8445451259613037
CurrentTrain: epoch  8, batch     1 | loss: 7.7473378Losses:  7.689638137817383 1.8198609352111816
CurrentTrain: epoch  8, batch     2 | loss: 7.6896381Losses:  7.377501487731934 1.6614912748336792
CurrentTrain: epoch  8, batch     3 | loss: 7.3775015Losses:  7.397731781005859 1.701092004776001
CurrentTrain: epoch  8, batch     4 | loss: 7.3977318Losses:  7.714289665222168 1.8385177850723267
CurrentTrain: epoch  8, batch     5 | loss: 7.7142897Losses:  7.828402519226074 1.891363501548767
CurrentTrain: epoch  8, batch     6 | loss: 7.8284025Losses:  7.865853786468506 1.904343843460083
CurrentTrain: epoch  8, batch     7 | loss: 7.8658538Losses:  7.776318550109863 1.873425841331482
CurrentTrain: epoch  8, batch     8 | loss: 7.7763186Losses:  7.871363162994385 1.912787675857544
CurrentTrain: epoch  8, batch     9 | loss: 7.8713632Losses:  7.684741973876953 1.8398815393447876
CurrentTrain: epoch  8, batch    10 | loss: 7.6847420Losses:  7.618262767791748 1.7920942306518555
CurrentTrain: epoch  8, batch    11 | loss: 7.6182628Losses:  7.770262718200684 1.85422945022583
CurrentTrain: epoch  8, batch    12 | loss: 7.7702627Losses:  7.626652717590332 1.7930169105529785
CurrentTrain: epoch  8, batch    13 | loss: 7.6266527Losses:  7.571999549865723 1.7835524082183838
CurrentTrain: epoch  8, batch    14 | loss: 7.5719995Losses:  7.4042205810546875 1.6924501657485962
CurrentTrain: epoch  8, batch    15 | loss: 7.4042206Losses:  7.586523056030273 1.800655722618103
CurrentTrain: epoch  8, batch    16 | loss: 7.5865231Losses:  7.848378658294678 1.911254644393921
CurrentTrain: epoch  8, batch    17 | loss: 7.8483787Losses:  7.758933067321777 1.860902190208435
CurrentTrain: epoch  8, batch    18 | loss: 7.7589331Losses:  7.836780071258545 1.8781650066375732
CurrentTrain: epoch  8, batch    19 | loss: 7.8367801Losses:  7.380009174346924 1.7157566547393799
CurrentTrain: epoch  8, batch    20 | loss: 7.3800092Losses:  7.6606855392456055 1.8221768140792847
CurrentTrain: epoch  8, batch    21 | loss: 7.6606855Losses:  7.088077545166016 1.5377415418624878
CurrentTrain: epoch  8, batch    22 | loss: 7.0880775Losses:  8.117376327514648 2.04113507270813
CurrentTrain: epoch  8, batch    23 | loss: 8.1173763Losses:  8.18458366394043 2.070972442626953
CurrentTrain: epoch  8, batch    24 | loss: 8.1845837Losses:  7.2677836418151855 1.6247074604034424
CurrentTrain: epoch  8, batch    25 | loss: 7.2677836Losses:  7.404467582702637 1.6847866773605347
CurrentTrain: epoch  8, batch    26 | loss: 7.4044676Losses:  7.647291660308838 1.785823106765747
CurrentTrain: epoch  8, batch    27 | loss: 7.6472917Losses:  8.06690788269043 2.0273754596710205
CurrentTrain: epoch  8, batch    28 | loss: 8.0669079Losses:  7.753450393676758 1.861945629119873
CurrentTrain: epoch  8, batch    29 | loss: 7.7534504Losses:  7.543115615844727 1.7640323638916016
CurrentTrain: epoch  8, batch    30 | loss: 7.5431156Losses:  7.403621196746826 1.6924083232879639
CurrentTrain: epoch  8, batch    31 | loss: 7.4036212Losses:  7.218472957611084 1.5879402160644531
CurrentTrain: epoch  8, batch    32 | loss: 7.2184730Losses:  7.631802082061768 1.7881627082824707
CurrentTrain: epoch  8, batch    33 | loss: 7.6318021Losses:  8.166351318359375 2.066608428955078
CurrentTrain: epoch  8, batch    34 | loss: 8.1663513Losses:  7.575604438781738 1.7784262895584106
CurrentTrain: epoch  8, batch    35 | loss: 7.5756044Losses:  7.505610942840576 1.7304584980010986
CurrentTrain: epoch  8, batch    36 | loss: 7.5056109Losses:  7.568586826324463 1.7501823902130127
CurrentTrain: epoch  8, batch    37 | loss: 7.5685868Losses:  7.344377517700195 1.6842235326766968
CurrentTrain: epoch  8, batch    38 | loss: 7.3443775Losses:  7.727456092834473 1.832871913909912
CurrentTrain: epoch  8, batch    39 | loss: 7.7274561Losses:  7.936354637145996 1.933180809020996
CurrentTrain: epoch  8, batch    40 | loss: 7.9363546Losses:  7.975530624389648 1.9708856344223022
CurrentTrain: epoch  8, batch    41 | loss: 7.9755306Losses:  7.823733806610107 1.888747215270996
CurrentTrain: epoch  8, batch    42 | loss: 7.8237338Losses:  7.897881984710693 1.9073879718780518
CurrentTrain: epoch  8, batch    43 | loss: 7.8978820Losses:  7.872542381286621 1.9167890548706055
CurrentTrain: epoch  8, batch    44 | loss: 7.8725424Losses:  7.387386322021484 1.6585431098937988
CurrentTrain: epoch  8, batch    45 | loss: 7.3873863Losses:  7.270978927612305 1.641464114189148
CurrentTrain: epoch  8, batch    46 | loss: 7.2709789Losses:  7.0498433113098145 1.5079991817474365
CurrentTrain: epoch  8, batch    47 | loss: 7.0498433Losses:  7.289784908294678 1.622873067855835
CurrentTrain: epoch  8, batch    48 | loss: 7.2897849Losses:  7.460065841674805 1.7218273878097534
CurrentTrain: epoch  8, batch    49 | loss: 7.4600658Losses:  7.66156530380249 1.8263227939605713
CurrentTrain: epoch  8, batch    50 | loss: 7.6615653Losses:  7.015874862670898 1.4878441095352173
CurrentTrain: epoch  8, batch    51 | loss: 7.0158749Losses:  7.842960357666016 1.907787799835205
CurrentTrain: epoch  8, batch    52 | loss: 7.8429604Losses:  7.682576656341553 1.8245587348937988
CurrentTrain: epoch  8, batch    53 | loss: 7.6825767Losses:  7.906963348388672 1.9491479396820068
CurrentTrain: epoch  8, batch    54 | loss: 7.9069633Losses:  7.28329610824585 1.6422576904296875
CurrentTrain: epoch  8, batch    55 | loss: 7.2832961Losses:  7.54798698425293 1.7618818283081055
CurrentTrain: epoch  8, batch    56 | loss: 7.5479870Losses:  8.094009399414062 2.0383105278015137
CurrentTrain: epoch  8, batch    57 | loss: 8.0940094Losses:  7.538752555847168 1.7532254457473755
CurrentTrain: epoch  8, batch    58 | loss: 7.5387526Losses:  7.702174186706543 1.8397672176361084
CurrentTrain: epoch  8, batch    59 | loss: 7.7021742Losses:  7.388535499572754 1.7148405313491821
CurrentTrain: epoch  8, batch    60 | loss: 7.3885355Losses:  8.00311279296875 1.9830074310302734
CurrentTrain: epoch  8, batch    61 | loss: 8.0031128Losses:  6.543025016784668 1.2691172361373901
CurrentTrain: epoch  8, batch    62 | loss: 6.5430250Losses:  7.555708408355713 1.7778007984161377
CurrentTrain: epoch  9, batch     0 | loss: 7.5557084Losses:  7.46225643157959 1.6864348649978638
CurrentTrain: epoch  9, batch     1 | loss: 7.4622564Losses:  7.924590587615967 1.9384820461273193
CurrentTrain: epoch  9, batch     2 | loss: 7.9245906Losses:  7.57362174987793 1.7577241659164429
CurrentTrain: epoch  9, batch     3 | loss: 7.5736217Losses:  7.240481376647949 1.610392689704895
CurrentTrain: epoch  9, batch     4 | loss: 7.2404814Losses:  7.58411169052124 1.7909624576568604
CurrentTrain: epoch  9, batch     5 | loss: 7.5841117Losses:  7.517394065856934 1.7405048608779907
CurrentTrain: epoch  9, batch     6 | loss: 7.5173941Losses:  7.6057257652282715 1.79618239402771
CurrentTrain: epoch  9, batch     7 | loss: 7.6057258Losses:  7.220095634460449 1.6176438331604004
CurrentTrain: epoch  9, batch     8 | loss: 7.2200956Losses:  7.685996055603027 1.8352118730545044
CurrentTrain: epoch  9, batch     9 | loss: 7.6859961Losses:  7.927458763122559 1.9568926095962524
CurrentTrain: epoch  9, batch    10 | loss: 7.9274588Losses:  7.9060516357421875 1.9334886074066162
CurrentTrain: epoch  9, batch    11 | loss: 7.9060516Losses:  7.8177876472473145 1.899078607559204
CurrentTrain: epoch  9, batch    12 | loss: 7.8177876Losses:  7.760096549987793 1.870990514755249
CurrentTrain: epoch  9, batch    13 | loss: 7.7600965Losses:  8.126056671142578 2.057410717010498
CurrentTrain: epoch  9, batch    14 | loss: 8.1260567Losses:  8.072061538696289 2.012495279312134
CurrentTrain: epoch  9, batch    15 | loss: 8.0720615Losses:  7.727855205535889 1.862147331237793
CurrentTrain: epoch  9, batch    16 | loss: 7.7278552Losses:  7.811352729797363 1.8813731670379639
CurrentTrain: epoch  9, batch    17 | loss: 7.8113527Losses:  7.690868854522705 1.8378875255584717
CurrentTrain: epoch  9, batch    18 | loss: 7.6908689Losses:  7.083771705627441 1.5506398677825928
CurrentTrain: epoch  9, batch    19 | loss: 7.0837717Losses:  7.7229766845703125 1.8355324268341064
CurrentTrain: epoch  9, batch    20 | loss: 7.7229767Losses:  7.642466068267822 1.8196964263916016
CurrentTrain: epoch  9, batch    21 | loss: 7.6424661Losses:  7.756641387939453 1.85159170627594
CurrentTrain: epoch  9, batch    22 | loss: 7.7566414Losses:  7.647098064422607 1.826136827468872
CurrentTrain: epoch  9, batch    23 | loss: 7.6470981Losses:  7.464361190795898 1.7273730039596558
CurrentTrain: epoch  9, batch    24 | loss: 7.4643612Losses:  7.420232772827148 1.6995625495910645
CurrentTrain: epoch  9, batch    25 | loss: 7.4202328Losses:  7.633794784545898 1.8252291679382324
CurrentTrain: epoch  9, batch    26 | loss: 7.6337948Losses:  7.642817974090576 1.8213489055633545
CurrentTrain: epoch  9, batch    27 | loss: 7.6428180Losses:  8.034018516540527 2.018118143081665
CurrentTrain: epoch  9, batch    28 | loss: 8.0340185Losses:  7.487286567687988 1.7391937971115112
CurrentTrain: epoch  9, batch    29 | loss: 7.4872866Losses:  7.653652191162109 1.8133282661437988
CurrentTrain: epoch  9, batch    30 | loss: 7.6536522Losses:  7.681218147277832 1.846254587173462
CurrentTrain: epoch  9, batch    31 | loss: 7.6812181Losses:  7.4364213943481445 1.725943922996521
CurrentTrain: epoch  9, batch    32 | loss: 7.4364214Losses:  7.087713241577148 1.5607211589813232
CurrentTrain: epoch  9, batch    33 | loss: 7.0877132Losses:  7.961843490600586 1.9788739681243896
CurrentTrain: epoch  9, batch    34 | loss: 7.9618435Losses:  8.062280654907227 2.028660297393799
CurrentTrain: epoch  9, batch    35 | loss: 8.0622807Losses:  7.055978298187256 1.5298264026641846
CurrentTrain: epoch  9, batch    36 | loss: 7.0559783Losses:  7.8856706619262695 1.943286657333374
CurrentTrain: epoch  9, batch    37 | loss: 7.8856707Losses:  7.640655517578125 1.7971858978271484
CurrentTrain: epoch  9, batch    38 | loss: 7.6406555Losses:  7.962364196777344 1.8787217140197754
CurrentTrain: epoch  9, batch    39 | loss: 7.9623642Losses:  7.423813819885254 1.6717456579208374
CurrentTrain: epoch  9, batch    40 | loss: 7.4238138Losses:  7.253050804138184 1.6315536499023438
CurrentTrain: epoch  9, batch    41 | loss: 7.2530508Losses:  7.527885437011719 1.7526612281799316
CurrentTrain: epoch  9, batch    42 | loss: 7.5278854Losses:  7.325362682342529 1.6690276861190796
CurrentTrain: epoch  9, batch    43 | loss: 7.3253627Losses:  7.459397315979004 1.7226357460021973
CurrentTrain: epoch  9, batch    44 | loss: 7.4593973Losses:  7.346944808959961 1.6824076175689697
CurrentTrain: epoch  9, batch    45 | loss: 7.3469448Losses:  7.323922157287598 1.6701860427856445
CurrentTrain: epoch  9, batch    46 | loss: 7.3239222Losses:  7.0434980392456055 1.5094366073608398
CurrentTrain: epoch  9, batch    47 | loss: 7.0434980Losses:  7.849314212799072 1.9143168926239014
CurrentTrain: epoch  9, batch    48 | loss: 7.8493142Losses:  7.8546061515808105 1.7956185340881348
CurrentTrain: epoch  9, batch    49 | loss: 7.8546062Losses:  7.9697089195251465 1.9440641403198242
CurrentTrain: epoch  9, batch    50 | loss: 7.9697089Losses:  7.9401445388793945 1.9451770782470703
CurrentTrain: epoch  9, batch    51 | loss: 7.9401445Losses:  7.672067642211914 1.828086018562317
CurrentTrain: epoch  9, batch    52 | loss: 7.6720676Losses:  7.618239879608154 1.8154115676879883
CurrentTrain: epoch  9, batch    53 | loss: 7.6182399Losses:  7.556175231933594 1.7709171772003174
CurrentTrain: epoch  9, batch    54 | loss: 7.5561752Losses:  7.550096035003662 1.7726333141326904
CurrentTrain: epoch  9, batch    55 | loss: 7.5500960Losses:  7.84594202041626 1.9380719661712646
CurrentTrain: epoch  9, batch    56 | loss: 7.8459420Losses:  7.808150768280029 1.8960201740264893
CurrentTrain: epoch  9, batch    57 | loss: 7.8081508Losses:  7.63241720199585 1.80873703956604
CurrentTrain: epoch  9, batch    58 | loss: 7.6324172Losses:  7.745169639587402 1.8747951984405518
CurrentTrain: epoch  9, batch    59 | loss: 7.7451696Losses:  7.778643608093262 1.8394675254821777
CurrentTrain: epoch  9, batch    60 | loss: 7.7786436Losses:  7.7544426918029785 1.865441083908081
CurrentTrain: epoch  9, batch    61 | loss: 7.7544427Losses:  7.055356025695801 1.5293138027191162
CurrentTrain: epoch  9, batch    62 | loss: 7.0553560
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  10.299437522888184 1.8728876113891602
CurrentTrain: epoch  0, batch     0 | loss: 10.2994375Losses:  9.714021682739258 1.8940021991729736
CurrentTrain: epoch  0, batch     1 | loss: 9.7140217Losses:  10.240933418273926 1.9869177341461182
CurrentTrain: epoch  0, batch     2 | loss: 10.2409334Losses:  7.596541881561279 0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.5965419Losses:  9.84920883178711 2.0373027324676514
CurrentTrain: epoch  1, batch     0 | loss: 9.8492088Losses:  10.15006160736084 2.1616950035095215
CurrentTrain: epoch  1, batch     1 | loss: 10.1500616Losses:  9.434070587158203 2.1054420471191406
CurrentTrain: epoch  1, batch     2 | loss: 9.4340706Losses:  4.2778472900390625 0.5732275247573853
CurrentTrain: epoch  1, batch     3 | loss: 4.2778473Losses:  9.004911422729492 1.865051507949829
CurrentTrain: epoch  2, batch     0 | loss: 9.0049114Losses:  8.221649169921875 1.9525814056396484
CurrentTrain: epoch  2, batch     1 | loss: 8.2216492Losses:  8.294589042663574 2.0018539428710938
CurrentTrain: epoch  2, batch     2 | loss: 8.2945890Losses:  8.982354164123535 0.6449593901634216
CurrentTrain: epoch  2, batch     3 | loss: 8.9823542Losses:  9.268823623657227 2.1131224632263184
CurrentTrain: epoch  3, batch     0 | loss: 9.2688236Losses:  7.463706970214844 1.917067527770996
CurrentTrain: epoch  3, batch     1 | loss: 7.4637070Losses:  8.240596771240234 2.005382776260376
CurrentTrain: epoch  3, batch     2 | loss: 8.2405968Losses:  4.339450836181641 0.6780692338943481
CurrentTrain: epoch  3, batch     3 | loss: 4.3394508Losses:  7.420453071594238 1.8722398281097412
CurrentTrain: epoch  4, batch     0 | loss: 7.4204531Losses:  7.643031120300293 1.8256553411483765
CurrentTrain: epoch  4, batch     1 | loss: 7.6430311Losses:  7.648292541503906 1.8787908554077148
CurrentTrain: epoch  4, batch     2 | loss: 7.6482925Losses:  4.774464130401611 0.6266741752624512
CurrentTrain: epoch  4, batch     3 | loss: 4.7744641Losses:  7.398697376251221 1.8705065250396729
CurrentTrain: epoch  5, batch     0 | loss: 7.3986974Losses:  6.598794460296631 1.8835484981536865
CurrentTrain: epoch  5, batch     1 | loss: 6.5987945Losses:  7.289093971252441 2.140481948852539
CurrentTrain: epoch  5, batch     2 | loss: 7.2890940Losses:  6.258091449737549 0.6296381950378418
CurrentTrain: epoch  5, batch     3 | loss: 6.2580914Losses:  7.2068376541137695 1.9290070533752441
CurrentTrain: epoch  6, batch     0 | loss: 7.2068377Losses:  6.718404293060303 1.838585615158081
CurrentTrain: epoch  6, batch     1 | loss: 6.7184043Losses:  6.627158164978027 1.9164270162582397
CurrentTrain: epoch  6, batch     2 | loss: 6.6271582Losses:  2.471041440963745 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4710414Losses:  7.210156440734863 2.007328510284424
CurrentTrain: epoch  7, batch     0 | loss: 7.2101564Losses:  7.045461654663086 2.105550527572632
CurrentTrain: epoch  7, batch     1 | loss: 7.0454617Losses:  6.558348655700684 1.9057103395462036
CurrentTrain: epoch  7, batch     2 | loss: 6.5583487Losses:  1.9924533367156982 0.0
CurrentTrain: epoch  7, batch     3 | loss: 1.9924533Losses:  6.613288402557373 1.9896979331970215
CurrentTrain: epoch  8, batch     0 | loss: 6.6132884Losses:  6.498302459716797 1.7648160457611084
CurrentTrain: epoch  8, batch     1 | loss: 6.4983025Losses:  6.683970928192139 1.9859247207641602
CurrentTrain: epoch  8, batch     2 | loss: 6.6839709Losses:  3.1636717319488525 0.592159628868103
CurrentTrain: epoch  8, batch     3 | loss: 3.1636717Losses:  6.429444313049316 1.9784590005874634
CurrentTrain: epoch  9, batch     0 | loss: 6.4294443Losses:  6.779129981994629 1.7543082237243652
CurrentTrain: epoch  9, batch     1 | loss: 6.7791300Losses:  6.297107219696045 1.9636716842651367
CurrentTrain: epoch  9, batch     2 | loss: 6.2971072Losses:  3.058715581893921 0.5892512798309326
CurrentTrain: epoch  9, batch     3 | loss: 3.0587156
Losses:  7.538418292999268 2.611909866333008
MemoryTrain:  epoch  0, batch     0 | loss: 7.5384183Losses:  2.5929627418518066 1.2500908374786377
MemoryTrain:  epoch  0, batch     1 | loss: 2.5929627Losses:  6.912027359008789 2.6140806674957275
MemoryTrain:  epoch  1, batch     0 | loss: 6.9120274Losses:  3.157046318054199 1.25102961063385
MemoryTrain:  epoch  1, batch     1 | loss: 3.1570463Losses:  6.672173023223877 2.60671329498291
MemoryTrain:  epoch  2, batch     0 | loss: 6.6721730Losses:  2.730496406555176 1.2837399244308472
MemoryTrain:  epoch  2, batch     1 | loss: 2.7304964Losses:  5.917693138122559 2.619349479675293
MemoryTrain:  epoch  3, batch     0 | loss: 5.9176931Losses:  4.880408763885498 1.2764344215393066
MemoryTrain:  epoch  3, batch     1 | loss: 4.8804088Losses:  6.343079566955566 2.609551429748535
MemoryTrain:  epoch  4, batch     0 | loss: 6.3430796Losses:  2.6065683364868164 1.2475377321243286
MemoryTrain:  epoch  4, batch     1 | loss: 2.6065683Losses:  6.176407337188721 2.611624240875244
MemoryTrain:  epoch  5, batch     0 | loss: 6.1764073Losses:  2.5411245822906494 1.2220005989074707
MemoryTrain:  epoch  5, batch     1 | loss: 2.5411246Losses:  6.031752586364746 2.5943126678466797
MemoryTrain:  epoch  6, batch     0 | loss: 6.0317526Losses:  2.6340227127075195 1.2774107456207275
MemoryTrain:  epoch  6, batch     1 | loss: 2.6340227Losses:  5.7254109382629395 2.6063060760498047
MemoryTrain:  epoch  7, batch     0 | loss: 5.7254109Losses:  4.430662631988525 1.2443515062332153
MemoryTrain:  epoch  7, batch     1 | loss: 4.4306626Losses:  6.082193851470947 2.601382255554199
MemoryTrain:  epoch  8, batch     0 | loss: 6.0821939Losses:  2.561779737472534 1.2686007022857666
MemoryTrain:  epoch  8, batch     1 | loss: 2.5617797Losses:  5.9522833824157715 2.6048920154571533
MemoryTrain:  epoch  9, batch     0 | loss: 5.9522834Losses:  2.5803909301757812 1.2416210174560547
MemoryTrain:  epoch  9, batch     1 | loss: 2.5803909
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 73.47%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 71.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.93%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.84%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.96%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.10%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.11%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.32%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 93.15%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.93%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 92.69%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 92.39%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.25%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.88%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 91.77%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.57%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 91.29%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.18%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.06%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 90.66%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 90.21%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 89.95%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 89.78%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 89.36%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 88.95%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 88.54%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 88.14%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 87.69%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.06%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 86.70%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.46%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.50%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 85.11%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 84.43%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 83.83%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 83.30%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 82.71%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 82.37%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.13%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.79%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.35%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.70%   
cur_acc:  ['0.9454', '0.7391']
his_acc:  ['0.9454', '0.8370']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  11.014425277709961 1.9492383003234863
CurrentTrain: epoch  0, batch     0 | loss: 11.0144253Losses:  10.298690795898438 2.0198049545288086
CurrentTrain: epoch  0, batch     1 | loss: 10.2986908Losses:  11.143972396850586 2.134052276611328
CurrentTrain: epoch  0, batch     2 | loss: 11.1439724Losses:  9.000141143798828 0.6383064985275269
CurrentTrain: epoch  0, batch     3 | loss: 9.0001411Losses:  9.860248565673828 2.1597886085510254
CurrentTrain: epoch  1, batch     0 | loss: 9.8602486Losses:  10.421073913574219 1.9479546546936035
CurrentTrain: epoch  1, batch     1 | loss: 10.4210739Losses:  9.468637466430664 2.1283926963806152
CurrentTrain: epoch  1, batch     2 | loss: 9.4686375Losses:  7.13828182220459 0.658249020576477
CurrentTrain: epoch  1, batch     3 | loss: 7.1382818Losses:  8.234391212463379 1.8868988752365112
CurrentTrain: epoch  2, batch     0 | loss: 8.2343912Losses:  9.526677131652832 2.1088976860046387
CurrentTrain: epoch  2, batch     1 | loss: 9.5266771Losses:  9.837210655212402 1.9824577569961548
CurrentTrain: epoch  2, batch     2 | loss: 9.8372107Losses:  4.526617527008057 0.6456623673439026
CurrentTrain: epoch  2, batch     3 | loss: 4.5266175Losses:  9.079200744628906 2.0968925952911377
CurrentTrain: epoch  3, batch     0 | loss: 9.0792007Losses:  7.9833269119262695 1.8855862617492676
CurrentTrain: epoch  3, batch     1 | loss: 7.9833269Losses:  9.197656631469727 2.04980206489563
CurrentTrain: epoch  3, batch     2 | loss: 9.1976566Losses:  6.562160015106201 0.6556612849235535
CurrentTrain: epoch  3, batch     3 | loss: 6.5621600Losses:  8.958903312683105 1.9723126888275146
CurrentTrain: epoch  4, batch     0 | loss: 8.9589033Losses:  8.767749786376953 2.142942428588867
CurrentTrain: epoch  4, batch     1 | loss: 8.7677498Losses:  6.917102813720703 1.8516477346420288
CurrentTrain: epoch  4, batch     2 | loss: 6.9171028Losses:  5.816558837890625 0.6448797583580017
CurrentTrain: epoch  4, batch     3 | loss: 5.8165588Losses:  8.201403617858887 1.776976466178894
CurrentTrain: epoch  5, batch     0 | loss: 8.2014036Losses:  7.629310607910156 1.9604504108428955
CurrentTrain: epoch  5, batch     1 | loss: 7.6293106Losses:  7.08282470703125 1.8027048110961914
CurrentTrain: epoch  5, batch     2 | loss: 7.0828247Losses:  8.543859481811523 0.6644861698150635
CurrentTrain: epoch  5, batch     3 | loss: 8.5438595Losses:  8.729727745056152 2.1415462493896484
CurrentTrain: epoch  6, batch     0 | loss: 8.7297277Losses:  8.128353118896484 2.068943738937378
CurrentTrain: epoch  6, batch     1 | loss: 8.1283531Losses:  7.237253665924072 2.041443347930908
CurrentTrain: epoch  6, batch     2 | loss: 7.2372537Losses:  3.948118209838867 0.6285617351531982
CurrentTrain: epoch  6, batch     3 | loss: 3.9481182Losses:  7.1908979415893555 1.9864412546157837
CurrentTrain: epoch  7, batch     0 | loss: 7.1908979Losses:  7.594135284423828 1.982590913772583
CurrentTrain: epoch  7, batch     1 | loss: 7.5941353Losses:  7.513675212860107 1.9749722480773926
CurrentTrain: epoch  7, batch     2 | loss: 7.5136752Losses:  5.66948127746582 0.6655126810073853
CurrentTrain: epoch  7, batch     3 | loss: 5.6694813Losses:  7.698535442352295 2.141947031021118
CurrentTrain: epoch  8, batch     0 | loss: 7.6985354Losses:  7.740462779998779 2.0928993225097656
CurrentTrain: epoch  8, batch     1 | loss: 7.7404628Losses:  7.050018787384033 1.96000075340271
CurrentTrain: epoch  8, batch     2 | loss: 7.0500188Losses:  3.8769216537475586 0.662620484828949
CurrentTrain: epoch  8, batch     3 | loss: 3.8769217Losses:  6.677867889404297 1.885818600654602
CurrentTrain: epoch  9, batch     0 | loss: 6.6778679Losses:  6.227557182312012 1.8071401119232178
CurrentTrain: epoch  9, batch     1 | loss: 6.2275572Losses:  7.449048042297363 1.9106918573379517
CurrentTrain: epoch  9, batch     2 | loss: 7.4490480Losses:  6.165831565856934 0.6788052320480347
CurrentTrain: epoch  9, batch     3 | loss: 6.1658316
Losses:  6.631716728210449 2.6434519290924072
MemoryTrain:  epoch  0, batch     0 | loss: 6.6317167Losses:  5.583585739135742 2.4906911849975586
MemoryTrain:  epoch  0, batch     1 | loss: 5.5835857Losses:  6.381064414978027 2.6533355712890625
MemoryTrain:  epoch  1, batch     0 | loss: 6.3810644Losses:  6.088976860046387 2.4838781356811523
MemoryTrain:  epoch  1, batch     1 | loss: 6.0889769Losses:  6.018918991088867 2.620431900024414
MemoryTrain:  epoch  2, batch     0 | loss: 6.0189190Losses:  5.555225849151611 2.5210072994232178
MemoryTrain:  epoch  2, batch     1 | loss: 5.5552258Losses:  5.714493274688721 2.6303117275238037
MemoryTrain:  epoch  3, batch     0 | loss: 5.7144933Losses:  5.298068046569824 2.495190143585205
MemoryTrain:  epoch  3, batch     1 | loss: 5.2980680Losses:  5.589315414428711 2.6377131938934326
MemoryTrain:  epoch  4, batch     0 | loss: 5.5893154Losses:  5.218759536743164 2.497952699661255
MemoryTrain:  epoch  4, batch     1 | loss: 5.2187595Losses:  5.472883701324463 2.6505630016326904
MemoryTrain:  epoch  5, batch     0 | loss: 5.4728837Losses:  5.08421516418457 2.474867582321167
MemoryTrain:  epoch  5, batch     1 | loss: 5.0842152Losses:  5.392771244049072 2.6218488216400146
MemoryTrain:  epoch  6, batch     0 | loss: 5.3927712Losses:  5.058187961578369 2.5048065185546875
MemoryTrain:  epoch  6, batch     1 | loss: 5.0581880Losses:  5.3783955574035645 2.6233763694763184
MemoryTrain:  epoch  7, batch     0 | loss: 5.3783956Losses:  5.083849906921387 2.500180959701538
MemoryTrain:  epoch  7, batch     1 | loss: 5.0838499Losses:  5.383011341094971 2.6544604301452637
MemoryTrain:  epoch  8, batch     0 | loss: 5.3830113Losses:  4.9756178855896 2.4672837257385254
MemoryTrain:  epoch  8, batch     1 | loss: 4.9756179Losses:  5.370804309844971 2.660391330718994
MemoryTrain:  epoch  9, batch     0 | loss: 5.3708043Losses:  4.979596138000488 2.468174457550049
MemoryTrain:  epoch  9, batch     1 | loss: 4.9795961
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 74.47%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 73.94%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.52%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 92.26%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.16%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 92.47%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 92.35%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.15%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.64%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 91.46%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.11%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 90.70%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 90.66%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 90.41%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 89.87%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 89.54%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 89.24%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 89.01%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 88.51%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 88.03%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 86.78%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 86.34%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 85.71%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 84.75%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.47%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.25%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.77%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.57%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.37%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 82.89%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.23%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 81.59%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 80.97%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 80.35%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.91%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.65%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.84%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 81.00%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 81.01%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 81.06%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.57%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.12%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.62%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 80.10%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 79.62%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 79.15%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.82%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 79.47%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 79.15%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 78.88%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 78.61%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 78.31%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 77.92%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.91%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.11%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.20%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.36%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 79.22%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 79.09%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.95%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 78.90%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.88%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.78%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.71%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.42%   
cur_acc:  ['0.9454', '0.7391', '0.7331']
his_acc:  ['0.9454', '0.8370', '0.7842']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  9.412294387817383 1.9487268924713135
CurrentTrain: epoch  0, batch     0 | loss: 9.4122944Losses:  8.02486801147461 1.928501844406128
CurrentTrain: epoch  0, batch     1 | loss: 8.0248680Losses:  8.898801803588867 1.7867565155029297
CurrentTrain: epoch  0, batch     2 | loss: 8.8988018Losses:  4.88628625869751 0.6296656131744385
CurrentTrain: epoch  0, batch     3 | loss: 4.8862863Losses:  8.522451400756836 2.0675294399261475
CurrentTrain: epoch  1, batch     0 | loss: 8.5224514Losses:  7.76754093170166 2.0038931369781494
CurrentTrain: epoch  1, batch     1 | loss: 7.7675409Losses:  7.983327865600586 2.07568097114563
CurrentTrain: epoch  1, batch     2 | loss: 7.9833279Losses:  3.5539610385894775 0.6053562164306641
CurrentTrain: epoch  1, batch     3 | loss: 3.5539610Losses:  8.538084983825684 1.9522767066955566
CurrentTrain: epoch  2, batch     0 | loss: 8.5380850Losses:  7.103889465332031 1.9732962846755981
CurrentTrain: epoch  2, batch     1 | loss: 7.1038895Losses:  6.307598114013672 1.8890855312347412
CurrentTrain: epoch  2, batch     2 | loss: 6.3075981Losses:  3.2951159477233887 0.617609977722168
CurrentTrain: epoch  2, batch     3 | loss: 3.2951159Losses:  7.160208702087402 1.9698399305343628
CurrentTrain: epoch  3, batch     0 | loss: 7.1602087Losses:  7.281595230102539 2.0779972076416016
CurrentTrain: epoch  3, batch     1 | loss: 7.2815952Losses:  7.089204788208008 1.9945430755615234
CurrentTrain: epoch  3, batch     2 | loss: 7.0892048Losses:  3.296245574951172 0.6222571730613708
CurrentTrain: epoch  3, batch     3 | loss: 3.2962456Losses:  6.606832504272461 1.8183281421661377
CurrentTrain: epoch  4, batch     0 | loss: 6.6068325Losses:  6.292157173156738 1.7828238010406494
CurrentTrain: epoch  4, batch     1 | loss: 6.2921572Losses:  6.4253740310668945 1.6068713665008545
CurrentTrain: epoch  4, batch     2 | loss: 6.4253740Losses:  3.185760498046875 0.6169488430023193
CurrentTrain: epoch  4, batch     3 | loss: 3.1857605Losses:  7.021425724029541 1.9699556827545166
CurrentTrain: epoch  5, batch     0 | loss: 7.0214257Losses:  6.605208873748779 1.8502496480941772
CurrentTrain: epoch  5, batch     1 | loss: 6.6052089Losses:  6.51251220703125 2.0856411457061768
CurrentTrain: epoch  5, batch     2 | loss: 6.5125122Losses:  1.8228795528411865 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8228796Losses:  5.744879722595215 1.8228566646575928
CurrentTrain: epoch  6, batch     0 | loss: 5.7448797Losses:  6.301592826843262 1.8353602886199951
CurrentTrain: epoch  6, batch     1 | loss: 6.3015928Losses:  6.858791828155518 1.9565449953079224
CurrentTrain: epoch  6, batch     2 | loss: 6.8587918Losses:  3.8040003776550293 0.6733371019363403
CurrentTrain: epoch  6, batch     3 | loss: 3.8040004Losses:  6.280149459838867 1.95249342918396
CurrentTrain: epoch  7, batch     0 | loss: 6.2801495Losses:  6.052684783935547 1.9237143993377686
CurrentTrain: epoch  7, batch     1 | loss: 6.0526848Losses:  6.559328079223633 1.9716098308563232
CurrentTrain: epoch  7, batch     2 | loss: 6.5593281Losses:  5.240721702575684 0.6627997159957886
CurrentTrain: epoch  7, batch     3 | loss: 5.2407217Losses:  6.045729160308838 1.7502037286758423
CurrentTrain: epoch  8, batch     0 | loss: 6.0457292Losses:  6.220911026000977 1.985656976699829
CurrentTrain: epoch  8, batch     1 | loss: 6.2209110Losses:  5.875640392303467 1.9460667371749878
CurrentTrain: epoch  8, batch     2 | loss: 5.8756404Losses:  3.4383397102355957 0.6267641186714172
CurrentTrain: epoch  8, batch     3 | loss: 3.4383397Losses:  6.10530424118042 1.9625375270843506
CurrentTrain: epoch  9, batch     0 | loss: 6.1053042Losses:  6.2242937088012695 1.8596218824386597
CurrentTrain: epoch  9, batch     1 | loss: 6.2242937Losses:  5.729833602905273 1.9070945978164673
CurrentTrain: epoch  9, batch     2 | loss: 5.7298336Losses:  3.0288333892822266 0.5836098194122314
CurrentTrain: epoch  9, batch     3 | loss: 3.0288334
Losses:  5.648348808288574 2.6238961219787598
MemoryTrain:  epoch  0, batch     0 | loss: 5.6483488Losses:  6.779542446136475 2.632333755493164
MemoryTrain:  epoch  0, batch     1 | loss: 6.7795424Losses:  5.284512042999268 1.9371223449707031
MemoryTrain:  epoch  0, batch     2 | loss: 5.2845120Losses:  6.195730209350586 2.640082359313965
MemoryTrain:  epoch  1, batch     0 | loss: 6.1957302Losses:  6.595551013946533 2.6026265621185303
MemoryTrain:  epoch  1, batch     1 | loss: 6.5955510Losses:  5.231344699859619 1.9626691341400146
MemoryTrain:  epoch  1, batch     2 | loss: 5.2313447Losses:  6.132812023162842 2.6164321899414062
MemoryTrain:  epoch  2, batch     0 | loss: 6.1328120Losses:  6.00681209564209 2.6538150310516357
MemoryTrain:  epoch  2, batch     1 | loss: 6.0068121Losses:  4.1682891845703125 1.9085118770599365
MemoryTrain:  epoch  2, batch     2 | loss: 4.1682892Losses:  5.880093097686768 2.628413677215576
MemoryTrain:  epoch  3, batch     0 | loss: 5.8800931Losses:  5.631777763366699 2.6270570755004883
MemoryTrain:  epoch  3, batch     1 | loss: 5.6317778Losses:  4.010228633880615 1.918528437614441
MemoryTrain:  epoch  3, batch     2 | loss: 4.0102286Losses:  5.380435466766357 2.6122002601623535
MemoryTrain:  epoch  4, batch     0 | loss: 5.3804355Losses:  5.446996688842773 2.6175661087036133
MemoryTrain:  epoch  4, batch     1 | loss: 5.4469967Losses:  5.058763027191162 1.9660359621047974
MemoryTrain:  epoch  4, batch     2 | loss: 5.0587630Losses:  5.783588886260986 2.601773977279663
MemoryTrain:  epoch  5, batch     0 | loss: 5.7835889Losses:  5.32379674911499 2.6304171085357666
MemoryTrain:  epoch  5, batch     1 | loss: 5.3237967Losses:  3.946666717529297 1.9412392377853394
MemoryTrain:  epoch  5, batch     2 | loss: 3.9466667Losses:  5.244500160217285 2.5942983627319336
MemoryTrain:  epoch  6, batch     0 | loss: 5.2445002Losses:  5.420544147491455 2.625619888305664
MemoryTrain:  epoch  6, batch     1 | loss: 5.4205441Losses:  4.085683822631836 1.974013328552246
MemoryTrain:  epoch  6, batch     2 | loss: 4.0856838Losses:  5.260628700256348 2.602036237716675
MemoryTrain:  epoch  7, batch     0 | loss: 5.2606287Losses:  5.349814414978027 2.639406204223633
MemoryTrain:  epoch  7, batch     1 | loss: 5.3498144Losses:  3.912924289703369 1.9299287796020508
MemoryTrain:  epoch  7, batch     2 | loss: 3.9129243Losses:  5.245824813842773 2.6007885932922363
MemoryTrain:  epoch  8, batch     0 | loss: 5.2458248Losses:  5.377895832061768 2.632148504257202
MemoryTrain:  epoch  8, batch     1 | loss: 5.3778958Losses:  3.8796420097351074 1.919364094734192
MemoryTrain:  epoch  8, batch     2 | loss: 3.8796420Losses:  5.323554039001465 2.638807773590088
MemoryTrain:  epoch  9, batch     0 | loss: 5.3235540Losses:  5.23473596572876 2.6004481315612793
MemoryTrain:  epoch  9, batch     1 | loss: 5.2347360Losses:  3.871835231781006 1.9105526208877563
MemoryTrain:  epoch  9, batch     2 | loss: 3.8718352
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.96%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.02%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.24%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.76%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.94%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 89.36%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.19%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.29%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.21%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 89.82%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.64%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.10%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 88.63%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 88.24%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.01%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 87.86%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 87.07%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 87.00%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 86.80%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 86.74%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 86.68%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 86.55%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 85.97%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 85.39%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 84.83%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 84.34%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 83.74%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 83.33%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 82.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.36%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.92%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.49%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.02%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 80.32%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.98%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 78.38%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.96%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.71%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 79.66%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 79.63%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 79.64%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 79.70%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 79.71%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 79.72%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 80.21%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 79.63%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 79.15%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 78.63%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 78.08%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 77.53%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.21%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 77.94%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 77.63%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 77.29%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 76.99%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 76.65%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 76.28%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 77.11%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 77.17%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 77.29%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 77.30%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.39%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 77.15%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 76.92%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 76.81%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 76.73%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 76.68%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.71%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 76.77%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 76.85%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 77.38%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 77.18%   [EVAL] batch:  195 | acc: 37.50%,  total acc: 76.98%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 76.78%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 76.55%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 76.32%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 76.08%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 75.92%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.88%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 75.60%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 75.48%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 75.21%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.65%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 75.70%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.94%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.44%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.51%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.75%   
cur_acc:  ['0.9454', '0.7391', '0.7331', '0.7976']
his_acc:  ['0.9454', '0.8370', '0.7842', '0.7775']
Clustering into  24  clusters
Clusters:  [ 2  8  1  0 14 23  1 22  3  3  1  3 21 18 11  4 10 18 12  4  2  4  0  7
  0  8  9  0  6 15  1  9  2  0  5  2 19 16 22  6 20 17  7  1 22  0  2  1
 13 10]
Losses:  9.750232696533203 1.9140279293060303
CurrentTrain: epoch  0, batch     0 | loss: 9.7502327Losses:  9.922746658325195 1.9494538307189941
CurrentTrain: epoch  0, batch     1 | loss: 9.9227467Losses:  9.625848770141602 1.9013770818710327
CurrentTrain: epoch  0, batch     2 | loss: 9.6258488Losses:  3.260836124420166 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.2608361Losses:  9.276463508605957 1.9438912868499756
CurrentTrain: epoch  1, batch     0 | loss: 9.2764635Losses:  8.66885757446289 2.0512759685516357
CurrentTrain: epoch  1, batch     1 | loss: 8.6688576Losses:  8.535082817077637 1.9570554494857788
CurrentTrain: epoch  1, batch     2 | loss: 8.5350828Losses:  5.84340763092041 0.6715777516365051
CurrentTrain: epoch  1, batch     3 | loss: 5.8434076Losses:  8.352293968200684 2.0567028522491455
CurrentTrain: epoch  2, batch     0 | loss: 8.3522940Losses:  9.427335739135742 2.060736656188965
CurrentTrain: epoch  2, batch     1 | loss: 9.4273357Losses:  8.34471321105957 2.041407585144043
CurrentTrain: epoch  2, batch     2 | loss: 8.3447132Losses:  4.3232421875 0.6634914875030518
CurrentTrain: epoch  2, batch     3 | loss: 4.3232422Losses:  8.78656005859375 2.058004856109619
CurrentTrain: epoch  3, batch     0 | loss: 8.7865601Losses:  7.837652206420898 1.9424532651901245
CurrentTrain: epoch  3, batch     1 | loss: 7.8376522Losses:  7.576241493225098 1.9558124542236328
CurrentTrain: epoch  3, batch     2 | loss: 7.5762415Losses:  5.886219501495361 0.6642048358917236
CurrentTrain: epoch  3, batch     3 | loss: 5.8862195Losses:  7.277594089508057 1.8921289443969727
CurrentTrain: epoch  4, batch     0 | loss: 7.2775941Losses:  7.615510940551758 1.7715095281600952
CurrentTrain: epoch  4, batch     1 | loss: 7.6155109Losses:  8.077629089355469 2.139003276824951
CurrentTrain: epoch  4, batch     2 | loss: 8.0776291Losses:  3.1387529373168945 0.6619015336036682
CurrentTrain: epoch  4, batch     3 | loss: 3.1387529Losses:  7.431492328643799 2.054272174835205
CurrentTrain: epoch  5, batch     0 | loss: 7.4314923Losses:  7.41676139831543 1.965567946434021
CurrentTrain: epoch  5, batch     1 | loss: 7.4167614Losses:  7.617044448852539 2.0150146484375
CurrentTrain: epoch  5, batch     2 | loss: 7.6170444Losses:  6.396879196166992 0.6606009006500244
CurrentTrain: epoch  5, batch     3 | loss: 6.3968792Losses:  7.143162250518799 1.9006340503692627
CurrentTrain: epoch  6, batch     0 | loss: 7.1431623Losses:  7.178685188293457 1.9476436376571655
CurrentTrain: epoch  6, batch     1 | loss: 7.1786852Losses:  7.351470947265625 2.0184991359710693
CurrentTrain: epoch  6, batch     2 | loss: 7.3514709Losses:  3.8653039932250977 0.6600989699363708
CurrentTrain: epoch  6, batch     3 | loss: 3.8653040Losses:  7.1193437576293945 2.0483338832855225
CurrentTrain: epoch  7, batch     0 | loss: 7.1193438Losses:  7.118572235107422 2.0200419425964355
CurrentTrain: epoch  7, batch     1 | loss: 7.1185722Losses:  7.081014633178711 1.972109079360962
CurrentTrain: epoch  7, batch     2 | loss: 7.0810146Losses:  3.9991841316223145 0.6648275256156921
CurrentTrain: epoch  7, batch     3 | loss: 3.9991841Losses:  6.904175758361816 2.1401681900024414
CurrentTrain: epoch  8, batch     0 | loss: 6.9041758Losses:  7.229072570800781 1.9010977745056152
CurrentTrain: epoch  8, batch     1 | loss: 7.2290726Losses:  7.007505893707275 2.0157101154327393
CurrentTrain: epoch  8, batch     2 | loss: 7.0075059Losses:  2.9737253189086914 0.6072808504104614
CurrentTrain: epoch  8, batch     3 | loss: 2.9737253Losses:  7.121335029602051 2.0595500469207764
CurrentTrain: epoch  9, batch     0 | loss: 7.1213350Losses:  5.906911849975586 1.8699610233306885
CurrentTrain: epoch  9, batch     1 | loss: 5.9069118Losses:  6.946931838989258 1.8813132047653198
CurrentTrain: epoch  9, batch     2 | loss: 6.9469318Losses:  3.4509246349334717 0.6507025957107544
CurrentTrain: epoch  9, batch     3 | loss: 3.4509246
Losses:  5.6003594398498535 2.6030526161193848
MemoryTrain:  epoch  0, batch     0 | loss: 5.6003594Losses:  5.746818542480469 2.6546568870544434
MemoryTrain:  epoch  0, batch     1 | loss: 5.7468185Losses:  5.842144966125488 2.6548898220062256
MemoryTrain:  epoch  0, batch     2 | loss: 5.8421450Losses:  2.902865171432495 0.6566698551177979
MemoryTrain:  epoch  0, batch     3 | loss: 2.9028652Losses:  6.504105091094971 2.637638568878174
MemoryTrain:  epoch  1, batch     0 | loss: 6.5041051Losses:  5.897024154663086 2.662424325942993
MemoryTrain:  epoch  1, batch     1 | loss: 5.8970242Losses:  5.4135026931762695 2.6182878017425537
MemoryTrain:  epoch  1, batch     2 | loss: 5.4135027Losses:  1.2136220932006836 0.5795292854309082
MemoryTrain:  epoch  1, batch     3 | loss: 1.2136221Losses:  5.802567481994629 2.6354923248291016
MemoryTrain:  epoch  2, batch     0 | loss: 5.8025675Losses:  5.3669538497924805 2.6318001747131348
MemoryTrain:  epoch  2, batch     1 | loss: 5.3669538Losses:  5.698812961578369 2.6371970176696777
MemoryTrain:  epoch  2, batch     2 | loss: 5.6988130Losses:  1.3287684917449951 0.6468808054924011
MemoryTrain:  epoch  2, batch     3 | loss: 1.3287685Losses:  5.518001556396484 2.6355338096618652
MemoryTrain:  epoch  3, batch     0 | loss: 5.5180016Losses:  5.35612678527832 2.6320807933807373
MemoryTrain:  epoch  3, batch     1 | loss: 5.3561268Losses:  5.579699516296387 2.625397205352783
MemoryTrain:  epoch  3, batch     2 | loss: 5.5796995Losses:  1.5866472721099854 0.6653149127960205
MemoryTrain:  epoch  3, batch     3 | loss: 1.5866473Losses:  5.558540344238281 2.645629405975342
MemoryTrain:  epoch  4, batch     0 | loss: 5.5585403Losses:  5.405428886413574 2.652535915374756
MemoryTrain:  epoch  4, batch     1 | loss: 5.4054289Losses:  5.28243350982666 2.5995616912841797
MemoryTrain:  epoch  4, batch     2 | loss: 5.2824335Losses:  1.2504901885986328 0.5901555418968201
MemoryTrain:  epoch  4, batch     3 | loss: 1.2504902Losses:  5.363598823547363 2.632303237915039
MemoryTrain:  epoch  5, batch     0 | loss: 5.3635988Losses:  5.343377113342285 2.616779088973999
MemoryTrain:  epoch  5, batch     1 | loss: 5.3433771Losses:  5.374983310699463 2.650798797607422
MemoryTrain:  epoch  5, batch     2 | loss: 5.3749833Losses:  1.17738938331604 0.5793967247009277
MemoryTrain:  epoch  5, batch     3 | loss: 1.1773894Losses:  5.370723247528076 2.605842113494873
MemoryTrain:  epoch  6, batch     0 | loss: 5.3707232Losses:  5.332526683807373 2.639220714569092
MemoryTrain:  epoch  6, batch     1 | loss: 5.3325267Losses:  5.3356032371521 2.637697219848633
MemoryTrain:  epoch  6, batch     2 | loss: 5.3356032Losses:  1.2365933656692505 0.6086335182189941
MemoryTrain:  epoch  6, batch     3 | loss: 1.2365934Losses:  5.346580505371094 2.6318893432617188
MemoryTrain:  epoch  7, batch     0 | loss: 5.3465805Losses:  5.276198387145996 2.6185827255249023
MemoryTrain:  epoch  7, batch     1 | loss: 5.2761984Losses:  5.342283248901367 2.6269712448120117
MemoryTrain:  epoch  7, batch     2 | loss: 5.3422832Losses:  1.2711411714553833 0.6214049458503723
MemoryTrain:  epoch  7, batch     3 | loss: 1.2711412Losses:  5.277578353881836 2.6185402870178223
MemoryTrain:  epoch  8, batch     0 | loss: 5.2775784Losses:  5.319501876831055 2.6283745765686035
MemoryTrain:  epoch  8, batch     1 | loss: 5.3195019Losses:  5.313270568847656 2.620995044708252
MemoryTrain:  epoch  8, batch     2 | loss: 5.3132706Losses:  1.2881534099578857 0.6263241171836853
MemoryTrain:  epoch  8, batch     3 | loss: 1.2881534Losses:  5.294219970703125 2.6268205642700195
MemoryTrain:  epoch  9, batch     0 | loss: 5.2942200Losses:  5.297078609466553 2.61855411529541
MemoryTrain:  epoch  9, batch     1 | loss: 5.2970786Losses:  5.317915439605713 2.632206916809082
MemoryTrain:  epoch  9, batch     2 | loss: 5.3179154Losses:  1.1517291069030762 0.5694072246551514
MemoryTrain:  epoch  9, batch     3 | loss: 1.1517291
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 64.34%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 60.36%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 61.92%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 62.36%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.73%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.46%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 89.98%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.48%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.14%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.01%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.09%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.96%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.13%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 89.20%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 88.86%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 88.61%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 88.04%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.73%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 87.27%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 86.98%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 86.69%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 86.56%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 85.58%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 85.46%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 85.39%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 85.28%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 84.84%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 84.21%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 83.53%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 83.05%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 82.46%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 82.07%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 81.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.13%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 80.70%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.25%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 79.79%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 79.11%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 78.44%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 77.78%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.55%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 78.44%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 78.56%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 78.82%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 78.42%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 77.95%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 77.44%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.35%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 76.04%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 76.66%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 76.40%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 76.02%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 75.73%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 75.40%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 74.96%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.96%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.72%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 75.90%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 76.13%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 76.21%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.05%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 75.87%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 76.68%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 76.47%   [EVAL] batch:  195 | acc: 37.50%,  total acc: 76.28%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 76.08%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 75.85%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 75.63%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 75.47%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.21%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 75.03%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 74.79%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 74.67%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 74.49%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.26%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 74.06%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.61%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 74.50%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.84%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 76.72%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 76.73%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 76.66%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 76.58%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 76.57%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.54%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 76.51%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 76.35%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 76.25%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 76.13%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 76.15%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 76.06%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 75.96%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 75.78%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 75.60%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.55%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 75.33%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 75.20%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 74.98%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.74%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 74.65%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 74.63%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 74.63%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 74.66%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.66%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.60%   
cur_acc:  ['0.9454', '0.7391', '0.7331', '0.7976', '0.7073']
his_acc:  ['0.9454', '0.8370', '0.7842', '0.7775', '0.7560']
Clustering into  29  clusters
Clusters:  [11  4  0  5 14 28  0  5 22 22  0 22 24  9 15  8 10  9  1  8 21  8 25 16
  5 18  3  5 26 27  0  3  3  4 23 11 19 17  5 26  4  7  6  0  5 12 21 10
 13 10 12  8  2 20  6  1  2 10  4  3]
Losses:  10.231403350830078 1.972808837890625
CurrentTrain: epoch  0, batch     0 | loss: 10.2314034Losses:  9.588766098022461 1.719642162322998
CurrentTrain: epoch  0, batch     1 | loss: 9.5887661Losses:  10.430822372436523 1.9685146808624268
CurrentTrain: epoch  0, batch     2 | loss: 10.4308224Losses:  4.824183464050293 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.8241835Losses:  10.14947509765625 2.145773410797119
CurrentTrain: epoch  1, batch     0 | loss: 10.1494751Losses:  9.215560913085938 2.028590202331543
CurrentTrain: epoch  1, batch     1 | loss: 9.2155609Losses:  8.27235221862793 2.026033878326416
CurrentTrain: epoch  1, batch     2 | loss: 8.2723522Losses:  6.202958106994629 0.6453372240066528
CurrentTrain: epoch  1, batch     3 | loss: 6.2029581Losses:  8.052608489990234 1.8863838911056519
CurrentTrain: epoch  2, batch     0 | loss: 8.0526085Losses:  8.962163925170898 2.0065805912017822
CurrentTrain: epoch  2, batch     1 | loss: 8.9621639Losses:  8.114326477050781 2.025221586227417
CurrentTrain: epoch  2, batch     2 | loss: 8.1143265Losses:  6.584079742431641 0.663864016532898
CurrentTrain: epoch  2, batch     3 | loss: 6.5840797Losses:  7.229182243347168 1.9068498611450195
CurrentTrain: epoch  3, batch     0 | loss: 7.2291822Losses:  8.933069229125977 2.0976967811584473
CurrentTrain: epoch  3, batch     1 | loss: 8.9330692Losses:  8.768936157226562 2.1837401390075684
CurrentTrain: epoch  3, batch     2 | loss: 8.7689362Losses:  5.776494979858398 0.6371136903762817
CurrentTrain: epoch  3, batch     3 | loss: 5.7764950Losses:  6.836093902587891 1.8192524909973145
CurrentTrain: epoch  4, batch     0 | loss: 6.8360939Losses:  8.766291618347168 1.9096856117248535
CurrentTrain: epoch  4, batch     1 | loss: 8.7662916Losses:  8.259713172912598 2.1058740615844727
CurrentTrain: epoch  4, batch     2 | loss: 8.2597132Losses:  4.402882099151611 0.6603102087974548
CurrentTrain: epoch  4, batch     3 | loss: 4.4028821Losses:  7.500734806060791 2.0860109329223633
CurrentTrain: epoch  5, batch     0 | loss: 7.5007348Losses:  8.2285795211792 1.8383125066757202
CurrentTrain: epoch  5, batch     1 | loss: 8.2285795Losses:  7.121562480926514 2.0150434970855713
CurrentTrain: epoch  5, batch     2 | loss: 7.1215625Losses:  4.621038913726807 0.6573307514190674
CurrentTrain: epoch  5, batch     3 | loss: 4.6210389Losses:  6.767614841461182 2.026608467102051
CurrentTrain: epoch  6, batch     0 | loss: 6.7676148Losses:  7.995924472808838 2.097102642059326
CurrentTrain: epoch  6, batch     1 | loss: 7.9959245Losses:  7.7616801261901855 2.0599653720855713
CurrentTrain: epoch  6, batch     2 | loss: 7.7616801Losses:  4.775412559509277 0.6468489170074463
CurrentTrain: epoch  6, batch     3 | loss: 4.7754126Losses:  7.615830898284912 2.0293002128601074
CurrentTrain: epoch  7, batch     0 | loss: 7.6158309Losses:  7.343883514404297 2.0669445991516113
CurrentTrain: epoch  7, batch     1 | loss: 7.3438835Losses:  6.600831985473633 1.8801403045654297
CurrentTrain: epoch  7, batch     2 | loss: 6.6008320Losses:  5.891297817230225 0.6550264954566956
CurrentTrain: epoch  7, batch     3 | loss: 5.8912978Losses:  6.544902801513672 1.8525192737579346
CurrentTrain: epoch  8, batch     0 | loss: 6.5449028Losses:  6.141472816467285 1.8336379528045654
CurrentTrain: epoch  8, batch     1 | loss: 6.1414728Losses:  7.849241256713867 1.9437105655670166
CurrentTrain: epoch  8, batch     2 | loss: 7.8492413Losses:  3.1587352752685547 0.6520868539810181
CurrentTrain: epoch  8, batch     3 | loss: 3.1587353Losses:  6.5526628494262695 1.976679801940918
CurrentTrain: epoch  9, batch     0 | loss: 6.5526628Losses:  6.486534118652344 1.829121470451355
CurrentTrain: epoch  9, batch     1 | loss: 6.4865341Losses:  6.671992301940918 1.823225498199463
CurrentTrain: epoch  9, batch     2 | loss: 6.6719923Losses:  6.9966888427734375 0.6628223657608032
CurrentTrain: epoch  9, batch     3 | loss: 6.9966888
Losses:  5.961762428283691 2.655954122543335
MemoryTrain:  epoch  0, batch     0 | loss: 5.9617624Losses:  6.155205726623535 2.6594772338867188
MemoryTrain:  epoch  0, batch     1 | loss: 6.1552057Losses:  5.763144493103027 2.633680582046509
MemoryTrain:  epoch  0, batch     2 | loss: 5.7631445Losses:  5.231226921081543 2.33347225189209
MemoryTrain:  epoch  0, batch     3 | loss: 5.2312269Losses:  6.306830406188965 2.625786781311035
MemoryTrain:  epoch  1, batch     0 | loss: 6.3068304Losses:  5.944393157958984 2.6289401054382324
MemoryTrain:  epoch  1, batch     1 | loss: 5.9443932Losses:  5.803667068481445 2.6522340774536133
MemoryTrain:  epoch  1, batch     2 | loss: 5.8036671Losses:  5.168222904205322 2.3767948150634766
MemoryTrain:  epoch  1, batch     3 | loss: 5.1682229Losses:  5.616286277770996 2.6393604278564453
MemoryTrain:  epoch  2, batch     0 | loss: 5.6162863Losses:  5.638729095458984 2.637256145477295
MemoryTrain:  epoch  2, batch     1 | loss: 5.6387291Losses:  5.889198303222656 2.6517210006713867
MemoryTrain:  epoch  2, batch     2 | loss: 5.8891983Losses:  4.944642066955566 2.3246657848358154
MemoryTrain:  epoch  2, batch     3 | loss: 4.9446421Losses:  5.393538475036621 2.616642475128174
MemoryTrain:  epoch  3, batch     0 | loss: 5.3935385Losses:  5.397340297698975 2.6236753463745117
MemoryTrain:  epoch  3, batch     1 | loss: 5.3973403Losses:  5.61521577835083 2.6292238235473633
MemoryTrain:  epoch  3, batch     2 | loss: 5.6152158Losses:  5.323646545410156 2.387960195541382
MemoryTrain:  epoch  3, batch     3 | loss: 5.3236465Losses:  5.618586540222168 2.6392822265625
MemoryTrain:  epoch  4, batch     0 | loss: 5.6185865Losses:  5.403957366943359 2.6343274116516113
MemoryTrain:  epoch  4, batch     1 | loss: 5.4039574Losses:  5.309581756591797 2.617733955383301
MemoryTrain:  epoch  4, batch     2 | loss: 5.3095818Losses:  4.806668281555176 2.354578971862793
MemoryTrain:  epoch  4, batch     3 | loss: 4.8066683Losses:  5.315986633300781 2.622307300567627
MemoryTrain:  epoch  5, batch     0 | loss: 5.3159866Losses:  5.399259090423584 2.648998737335205
MemoryTrain:  epoch  5, batch     1 | loss: 5.3992591Losses:  5.379326343536377 2.6328463554382324
MemoryTrain:  epoch  5, batch     2 | loss: 5.3793263Losses:  5.1673994064331055 2.328322410583496
MemoryTrain:  epoch  5, batch     3 | loss: 5.1673994Losses:  5.355924129486084 2.6180295944213867
MemoryTrain:  epoch  6, batch     0 | loss: 5.3559241Losses:  5.307148456573486 2.6285688877105713
MemoryTrain:  epoch  6, batch     1 | loss: 5.3071485Losses:  5.320462703704834 2.628848075866699
MemoryTrain:  epoch  6, batch     2 | loss: 5.3204627Losses:  4.7674784660339355 2.354665756225586
MemoryTrain:  epoch  6, batch     3 | loss: 4.7674785Losses:  5.36386775970459 2.6313765048980713
MemoryTrain:  epoch  7, batch     0 | loss: 5.3638678Losses:  5.332054138183594 2.638296127319336
MemoryTrain:  epoch  7, batch     1 | loss: 5.3320541Losses:  5.283627510070801 2.6167409420013428
MemoryTrain:  epoch  7, batch     2 | loss: 5.2836275Losses:  4.701975345611572 2.329697370529175
MemoryTrain:  epoch  7, batch     3 | loss: 4.7019753Losses:  5.268503189086914 2.615990400314331
MemoryTrain:  epoch  8, batch     0 | loss: 5.2685032Losses:  5.298797607421875 2.619987726211548
MemoryTrain:  epoch  8, batch     1 | loss: 5.2987976Losses:  5.283712387084961 2.6238768100738525
MemoryTrain:  epoch  8, batch     2 | loss: 5.2837124Losses:  4.771435737609863 2.3591136932373047
MemoryTrain:  epoch  8, batch     3 | loss: 4.7714357Losses:  5.2901611328125 2.6215503215789795
MemoryTrain:  epoch  9, batch     0 | loss: 5.2901611Losses:  5.264636039733887 2.6129183769226074
MemoryTrain:  epoch  9, batch     1 | loss: 5.2646360Losses:  5.317734718322754 2.64383602142334
MemoryTrain:  epoch  9, batch     2 | loss: 5.3177347Losses:  4.691720008850098 2.3238682746887207
MemoryTrain:  epoch  9, batch     3 | loss: 4.6917200
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 72.60%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 67.98%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.76%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.39%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 90.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.14%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 87.19%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.90%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 85.94%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 85.67%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 84.66%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 83.46%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 83.24%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 83.72%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 82.75%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 82.33%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 82.01%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 81.63%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 81.32%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.10%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 80.96%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 80.24%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 80.34%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 80.35%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 80.37%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 80.31%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 79.99%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 79.47%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 78.91%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 78.54%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 78.00%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 77.65%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 77.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.72%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.31%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.76%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.17%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 74.54%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 73.92%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 73.37%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 72.99%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 74.17%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 74.13%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.02%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 74.02%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 74.13%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.09%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 74.91%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 74.37%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 73.93%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 72.93%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 72.42%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.14%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 72.93%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 72.53%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 72.18%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 71.15%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 71.38%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 71.48%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 71.41%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 71.17%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 72.39%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 72.28%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 71.86%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 71.78%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 71.57%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 71.41%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 71.27%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 70.95%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 70.85%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 70.67%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 70.63%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 71.44%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 71.51%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 71.42%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 73.92%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.80%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.78%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.77%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 73.52%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 73.37%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 73.37%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 73.79%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 73.62%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 73.35%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 73.24%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 72.82%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 72.61%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 72.40%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 72.15%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 72.07%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 71.95%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 71.90%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 72.69%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 72.50%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 72.33%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 72.18%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 71.99%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 72.17%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 72.18%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 72.09%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 72.01%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 72.75%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 72.99%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 72.78%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 72.65%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 72.49%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 72.31%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 72.14%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 72.46%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 72.34%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 72.33%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 72.28%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 72.19%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 72.13%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 72.13%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 72.15%   
cur_acc:  ['0.9454', '0.7391', '0.7331', '0.7976', '0.7073', '0.6776']
his_acc:  ['0.9454', '0.8370', '0.7842', '0.7775', '0.7560', '0.7215']
Clustering into  34  clusters
Clusters:  [ 6 30  0  1 27 25  0 12  2  2  0  2 26 13 33  5 16 13  3  5 14  5 15 19
  1 30 22  1 21 23 28 22 14  1 24  6 20 31 12 21 17 18  8  0 12  7  4 10
 29 16  7  5 32 11  8  3  9 10  1 22 30  5 22 33 15  3  4  1  4  1]
Losses:  9.026508331298828 1.7164256572723389
CurrentTrain: epoch  0, batch     0 | loss: 9.0265083Losses:  10.816888809204102 1.7630023956298828
CurrentTrain: epoch  0, batch     1 | loss: 10.8168888Losses:  10.624909400939941 1.9736328125
CurrentTrain: epoch  0, batch     2 | loss: 10.6249094Losses:  10.216423034667969 0.6418249607086182
CurrentTrain: epoch  0, batch     3 | loss: 10.2164230Losses:  10.06956672668457 2.056849479675293
CurrentTrain: epoch  1, batch     0 | loss: 10.0695667Losses:  9.490998268127441 1.8850260972976685
CurrentTrain: epoch  1, batch     1 | loss: 9.4909983Losses:  8.985294342041016 2.0106582641601562
CurrentTrain: epoch  1, batch     2 | loss: 8.9852943Losses:  8.078668594360352 0.6381926536560059
CurrentTrain: epoch  1, batch     3 | loss: 8.0786686Losses:  8.85043716430664 1.8510043621063232
CurrentTrain: epoch  2, batch     0 | loss: 8.8504372Losses:  8.662395477294922 1.9987233877182007
CurrentTrain: epoch  2, batch     1 | loss: 8.6623955Losses:  9.265081405639648 2.1666550636291504
CurrentTrain: epoch  2, batch     2 | loss: 9.2650814Losses:  8.504655838012695 0.6627997159957886
CurrentTrain: epoch  2, batch     3 | loss: 8.5046558Losses:  8.503976821899414 1.7957346439361572
CurrentTrain: epoch  3, batch     0 | loss: 8.5039768Losses:  8.782116889953613 2.037524938583374
CurrentTrain: epoch  3, batch     1 | loss: 8.7821169Losses:  8.427693367004395 2.0133934020996094
CurrentTrain: epoch  3, batch     2 | loss: 8.4276934Losses:  5.60184907913208 0.657990038394928
CurrentTrain: epoch  3, batch     3 | loss: 5.6018491Losses:  8.978839874267578 2.0028128623962402
CurrentTrain: epoch  4, batch     0 | loss: 8.9788399Losses:  7.083129405975342 1.7933549880981445
CurrentTrain: epoch  4, batch     1 | loss: 7.0831294Losses:  8.305075645446777 2.0766115188598633
CurrentTrain: epoch  4, batch     2 | loss: 8.3050756Losses:  7.079113960266113 0.6758537888526917
CurrentTrain: epoch  4, batch     3 | loss: 7.0791140Losses:  7.513343811035156 1.6094064712524414
CurrentTrain: epoch  5, batch     0 | loss: 7.5133438Losses:  7.815030574798584 2.0143425464630127
CurrentTrain: epoch  5, batch     1 | loss: 7.8150306Losses:  7.827211856842041 2.0502827167510986
CurrentTrain: epoch  5, batch     2 | loss: 7.8272119Losses:  4.510180473327637 0.6370684504508972
CurrentTrain: epoch  5, batch     3 | loss: 4.5101805Losses:  7.987412452697754 1.9549914598464966
CurrentTrain: epoch  6, batch     0 | loss: 7.9874125Losses:  7.324996471405029 1.7025797367095947
CurrentTrain: epoch  6, batch     1 | loss: 7.3249965Losses:  6.467677116394043 1.7974557876586914
CurrentTrain: epoch  6, batch     2 | loss: 6.4676771Losses:  6.38681173324585 0.6545080542564392
CurrentTrain: epoch  6, batch     3 | loss: 6.3868117Losses:  6.657750606536865 1.8834130764007568
CurrentTrain: epoch  7, batch     0 | loss: 6.6577506Losses:  7.729279518127441 1.8613766431808472
CurrentTrain: epoch  7, batch     1 | loss: 7.7292795Losses:  7.388195037841797 2.0436060428619385
CurrentTrain: epoch  7, batch     2 | loss: 7.3881950Losses:  5.697213172912598 0.6721736192703247
CurrentTrain: epoch  7, batch     3 | loss: 5.6972132Losses:  6.961996078491211 1.9035959243774414
CurrentTrain: epoch  8, batch     0 | loss: 6.9619961Losses:  7.0181169509887695 1.889876127243042
CurrentTrain: epoch  8, batch     1 | loss: 7.0181170Losses:  7.58012580871582 2.1598446369171143
CurrentTrain: epoch  8, batch     2 | loss: 7.5801258Losses:  4.114675045013428 0.6560614109039307
CurrentTrain: epoch  8, batch     3 | loss: 4.1146750Losses:  7.1189446449279785 2.0930590629577637
CurrentTrain: epoch  9, batch     0 | loss: 7.1189446Losses:  7.712033271789551 2.0276756286621094
CurrentTrain: epoch  9, batch     1 | loss: 7.7120333Losses:  6.824996471405029 1.9572844505310059
CurrentTrain: epoch  9, batch     2 | loss: 6.8249965Losses:  3.6653354167938232 0.665924072265625
CurrentTrain: epoch  9, batch     3 | loss: 3.6653354
Losses:  5.8287482261657715 2.6547434329986572
MemoryTrain:  epoch  0, batch     0 | loss: 5.8287482Losses:  5.641881465911865 2.635986804962158
MemoryTrain:  epoch  0, batch     1 | loss: 5.6418815Losses:  6.05155611038208 2.64506459236145
MemoryTrain:  epoch  0, batch     2 | loss: 6.0515561Losses:  5.4589433670043945 2.659456729888916
MemoryTrain:  epoch  0, batch     3 | loss: 5.4589434Losses:  3.306490421295166 1.5841023921966553
MemoryTrain:  epoch  0, batch     4 | loss: 3.3064904Losses:  5.501976490020752 2.6233348846435547
MemoryTrain:  epoch  1, batch     0 | loss: 5.5019765Losses:  5.808135986328125 2.6365561485290527
MemoryTrain:  epoch  1, batch     1 | loss: 5.8081360Losses:  5.863805770874023 2.6467232704162598
MemoryTrain:  epoch  1, batch     2 | loss: 5.8638058Losses:  6.038674354553223 2.631258487701416
MemoryTrain:  epoch  1, batch     3 | loss: 6.0386744Losses:  3.50854229927063 1.6600236892700195
MemoryTrain:  epoch  1, batch     4 | loss: 3.5085423Losses:  5.409835338592529 2.62111234664917
MemoryTrain:  epoch  2, batch     0 | loss: 5.4098353Losses:  5.401126384735107 2.6262764930725098
MemoryTrain:  epoch  2, batch     1 | loss: 5.4011264Losses:  5.437820911407471 2.639308452606201
MemoryTrain:  epoch  2, batch     2 | loss: 5.4378209Losses:  5.912717342376709 2.6434879302978516
MemoryTrain:  epoch  2, batch     3 | loss: 5.9127173Losses:  3.4732894897460938 1.696359634399414
MemoryTrain:  epoch  2, batch     4 | loss: 3.4732895Losses:  5.388984203338623 2.625762462615967
MemoryTrain:  epoch  3, batch     0 | loss: 5.3889842Losses:  5.417015075683594 2.601632595062256
MemoryTrain:  epoch  3, batch     1 | loss: 5.4170151Losses:  5.412736892700195 2.6435132026672363
MemoryTrain:  epoch  3, batch     2 | loss: 5.4127369Losses:  5.496354579925537 2.641775131225586
MemoryTrain:  epoch  3, batch     3 | loss: 5.4963546Losses:  3.5195841789245605 1.685927391052246
MemoryTrain:  epoch  3, batch     4 | loss: 3.5195842Losses:  5.296573638916016 2.6238670349121094
MemoryTrain:  epoch  4, batch     0 | loss: 5.2965736Losses:  5.367687225341797 2.626295566558838
MemoryTrain:  epoch  4, batch     1 | loss: 5.3676872Losses:  5.314582824707031 2.6332497596740723
MemoryTrain:  epoch  4, batch     2 | loss: 5.3145828Losses:  5.375659942626953 2.6425845623016357
MemoryTrain:  epoch  4, batch     3 | loss: 5.3756599Losses:  3.4183454513549805 1.6704553365707397
MemoryTrain:  epoch  4, batch     4 | loss: 3.4183455Losses:  5.366543769836426 2.6235244274139404
MemoryTrain:  epoch  5, batch     0 | loss: 5.3665438Losses:  5.3813700675964355 2.6454455852508545
MemoryTrain:  epoch  5, batch     1 | loss: 5.3813701Losses:  5.285507678985596 2.620227336883545
MemoryTrain:  epoch  5, batch     2 | loss: 5.2855077Losses:  5.297832012176514 2.630049228668213
MemoryTrain:  epoch  5, batch     3 | loss: 5.2978320Losses:  3.2984211444854736 1.6349471807479858
MemoryTrain:  epoch  5, batch     4 | loss: 3.2984211Losses:  5.241711616516113 2.602217674255371
MemoryTrain:  epoch  6, batch     0 | loss: 5.2417116Losses:  5.296142578125 2.626789093017578
MemoryTrain:  epoch  6, batch     1 | loss: 5.2961426Losses:  5.378394603729248 2.625411033630371
MemoryTrain:  epoch  6, batch     2 | loss: 5.3783946Losses:  5.337465763092041 2.6477150917053223
MemoryTrain:  epoch  6, batch     3 | loss: 5.3374658Losses:  3.349097967147827 1.6545374393463135
MemoryTrain:  epoch  6, batch     4 | loss: 3.3490980Losses:  5.292924404144287 2.6320550441741943
MemoryTrain:  epoch  7, batch     0 | loss: 5.2929244Losses:  5.32808780670166 2.639774799346924
MemoryTrain:  epoch  7, batch     1 | loss: 5.3280878Losses:  5.287022113800049 2.618931293487549
MemoryTrain:  epoch  7, batch     2 | loss: 5.2870221Losses:  5.325052261352539 2.6110076904296875
MemoryTrain:  epoch  7, batch     3 | loss: 5.3250523Losses:  3.3477866649627686 1.6498005390167236
MemoryTrain:  epoch  7, batch     4 | loss: 3.3477867Losses:  5.3142170906066895 2.632965087890625
MemoryTrain:  epoch  8, batch     0 | loss: 5.3142171Losses:  5.285391807556152 2.6228389739990234
MemoryTrain:  epoch  8, batch     1 | loss: 5.2853918Losses:  5.28179407119751 2.621978759765625
MemoryTrain:  epoch  8, batch     2 | loss: 5.2817941Losses:  5.278233528137207 2.61436128616333
MemoryTrain:  epoch  8, batch     3 | loss: 5.2782335Losses:  3.352649688720703 1.6527338027954102
MemoryTrain:  epoch  8, batch     4 | loss: 3.3526497Losses:  5.3033294677734375 2.6292483806610107
MemoryTrain:  epoch  9, batch     0 | loss: 5.3033295Losses:  5.270914077758789 2.613478183746338
MemoryTrain:  epoch  9, batch     1 | loss: 5.2709141Losses:  5.319555759429932 2.6352949142456055
MemoryTrain:  epoch  9, batch     2 | loss: 5.3195558Losses:  5.242787837982178 2.6001017093658447
MemoryTrain:  epoch  9, batch     3 | loss: 5.2427878Losses:  3.3912551403045654 1.6761512756347656
MemoryTrain:  epoch  9, batch     4 | loss: 3.3912551
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 51.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 49.76%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 48.15%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 46.43%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 45.04%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 43.54%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 42.14%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 42.77%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 44.13%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 45.22%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 46.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 47.92%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 48.99%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 50.16%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 50.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.19%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 53.05%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 53.72%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 54.65%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.54%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 56.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 57.07%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 57.71%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 59.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 59.44%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 60.02%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.53%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.16%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 61.07%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 60.24%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 59.75%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 59.48%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 58.81%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 58.77%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 58.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 87.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 87.27%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 86.82%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 85.13%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.75%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 83.61%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.84%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 81.53%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 81.06%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 80.79%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 80.62%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 80.76%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 80.28%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 80.05%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 79.98%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.27%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 78.69%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 77.57%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 77.18%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 76.51%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 76.42%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 76.18%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 75.68%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 75.07%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 74.61%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 73.55%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 72.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 72.67%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 72.51%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 71.85%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 71.24%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 70.58%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 69.94%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 69.31%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 68.69%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 70.55%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.37%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 70.53%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 70.56%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 71.00%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 70.54%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 70.08%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 69.59%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 69.10%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 69.54%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.08%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 67.67%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.76%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.10%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.68%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 67.27%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 66.87%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 66.47%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 66.07%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 66.09%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 66.97%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 66.27%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 65.79%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 65.53%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 65.37%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.09%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 64.65%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 65.60%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 68.97%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.80%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.82%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 68.47%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.66%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 68.62%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 68.28%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 68.22%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 67.74%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.55%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.31%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:  288 | acc: 0.00%,  total acc: 66.87%   [EVAL] batch:  289 | acc: 6.25%,  total acc: 66.66%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 67.12%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 66.60%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 66.43%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 66.34%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.39%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 66.43%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 67.72%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 67.52%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 66.82%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 67.12%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 67.08%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.89%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 66.83%   [EVAL] batch:  375 | acc: 0.00%,  total acc: 66.66%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 66.55%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 66.44%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  394 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 66.40%   [EVAL] batch:  396 | acc: 12.50%,  total acc: 66.26%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 66.02%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 65.88%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.42%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.27%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 65.11%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.95%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.34%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  421 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  432 | acc: 12.50%,  total acc: 65.95%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  435 | acc: 18.75%,  total acc: 65.71%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 65.69%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 65.57%   
cur_acc:  ['0.9454', '0.7391', '0.7331', '0.7976', '0.7073', '0.6776', '0.5804']
his_acc:  ['0.9454', '0.8370', '0.7842', '0.7775', '0.7560', '0.7215', '0.6557']
Clustering into  38  clusters
Clusters:  [ 7 34  5  0 33 27  5  0 26 26  2 26 28  9 30 14 18  9 12 14 15 14  3 21
  0 34 17  0 10 29  5 17 15  8 31  7 13 19  0 35 20 37 11 36  0  6  1  4
 32 18  6 14 16 24 11 12  2  4  8 17 34 14 17 30  3 12  1  5  1  8  5 26
 10 14 23 22 25 16 36  2]
Losses:  10.395097732543945 2.089826822280884
CurrentTrain: epoch  0, batch     0 | loss: 10.3950977Losses:  10.228353500366211 1.9025382995605469
CurrentTrain: epoch  0, batch     1 | loss: 10.2283535Losses:  9.669309616088867 1.9843101501464844
CurrentTrain: epoch  0, batch     2 | loss: 9.6693096Losses:  7.101135730743408 0.6297616958618164
CurrentTrain: epoch  0, batch     3 | loss: 7.1011357Losses:  8.957151412963867 1.799929141998291
CurrentTrain: epoch  1, batch     0 | loss: 8.9571514Losses:  9.171552658081055 1.8690497875213623
CurrentTrain: epoch  1, batch     1 | loss: 9.1715527Losses:  8.938879013061523 1.8329776525497437
CurrentTrain: epoch  1, batch     2 | loss: 8.9388790Losses:  2.5038347244262695 0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.5038347Losses:  8.455477714538574 1.8834471702575684
CurrentTrain: epoch  2, batch     0 | loss: 8.4554777Losses:  8.72488784790039 1.8761608600616455
CurrentTrain: epoch  2, batch     1 | loss: 8.7248878Losses:  8.577676773071289 1.8657164573669434
CurrentTrain: epoch  2, batch     2 | loss: 8.5776768Losses:  4.41409969329834 0.6248645782470703
CurrentTrain: epoch  2, batch     3 | loss: 4.4140997Losses:  8.58133316040039 2.044081211090088
CurrentTrain: epoch  3, batch     0 | loss: 8.5813332Losses:  8.02783203125 1.8628673553466797
CurrentTrain: epoch  3, batch     1 | loss: 8.0278320Losses:  8.161869049072266 2.018343687057495
CurrentTrain: epoch  3, batch     2 | loss: 8.1618690Losses:  7.167968273162842 0.6711671352386475
CurrentTrain: epoch  3, batch     3 | loss: 7.1679683Losses:  7.777083396911621 2.0117225646972656
CurrentTrain: epoch  4, batch     0 | loss: 7.7770834Losses:  7.53538703918457 2.0200161933898926
CurrentTrain: epoch  4, batch     1 | loss: 7.5353870Losses:  8.562807083129883 1.97465181350708
CurrentTrain: epoch  4, batch     2 | loss: 8.5628071Losses:  8.910408973693848 0.6570390462875366
CurrentTrain: epoch  4, batch     3 | loss: 8.9104090Losses:  8.14622974395752 2.0569326877593994
CurrentTrain: epoch  5, batch     0 | loss: 8.1462297Losses:  7.7645745277404785 1.8143396377563477
CurrentTrain: epoch  5, batch     1 | loss: 7.7645745Losses:  7.298275470733643 1.75441312789917
CurrentTrain: epoch  5, batch     2 | loss: 7.2982755Losses:  3.583893060684204 0.6195117235183716
CurrentTrain: epoch  5, batch     3 | loss: 3.5838931Losses:  8.301069259643555 2.115943431854248
CurrentTrain: epoch  6, batch     0 | loss: 8.3010693Losses:  7.501370429992676 1.9481358528137207
CurrentTrain: epoch  6, batch     1 | loss: 7.5013704Losses:  6.957327842712402 1.896949291229248
CurrentTrain: epoch  6, batch     2 | loss: 6.9573278Losses:  3.656621217727661 0.6528843641281128
CurrentTrain: epoch  6, batch     3 | loss: 3.6566212Losses:  7.533764362335205 1.9393000602722168
CurrentTrain: epoch  7, batch     0 | loss: 7.5337644Losses:  7.12171745300293 1.8967313766479492
CurrentTrain: epoch  7, batch     1 | loss: 7.1217175Losses:  7.2374267578125 2.1044163703918457
CurrentTrain: epoch  7, batch     2 | loss: 7.2374268Losses:  3.98915958404541 0.601934552192688
CurrentTrain: epoch  7, batch     3 | loss: 3.9891596Losses:  7.23353910446167 1.9076197147369385
CurrentTrain: epoch  8, batch     0 | loss: 7.2335391Losses:  6.642679214477539 1.8262035846710205
CurrentTrain: epoch  8, batch     1 | loss: 6.6426792Losses:  6.690389156341553 2.0114216804504395
CurrentTrain: epoch  8, batch     2 | loss: 6.6903892Losses:  5.005707263946533 0.6372668743133545
CurrentTrain: epoch  8, batch     3 | loss: 5.0057073Losses:  6.262519836425781 1.850995421409607
CurrentTrain: epoch  9, batch     0 | loss: 6.2625198Losses:  7.214224815368652 2.115489959716797
CurrentTrain: epoch  9, batch     1 | loss: 7.2142248Losses:  6.594248294830322 1.8828058242797852
CurrentTrain: epoch  9, batch     2 | loss: 6.5942483Losses:  3.652139186859131 0.6415038108825684
CurrentTrain: epoch  9, batch     3 | loss: 3.6521392
Losses:  5.411154270172119 2.615734577178955
MemoryTrain:  epoch  0, batch     0 | loss: 5.4111543Losses:  6.088597297668457 2.6402602195739746
MemoryTrain:  epoch  0, batch     1 | loss: 6.0885973Losses:  5.887293815612793 2.665234327316284
MemoryTrain:  epoch  0, batch     2 | loss: 5.8872938Losses:  5.745948314666748 2.6090760231018066
MemoryTrain:  epoch  0, batch     3 | loss: 5.7459483Losses:  5.733320713043213 2.6340484619140625
MemoryTrain:  epoch  0, batch     4 | loss: 5.7333207Losses:  6.366699695587158 2.628434181213379
MemoryTrain:  epoch  1, batch     0 | loss: 6.3666997Losses:  5.416077613830566 2.6369729042053223
MemoryTrain:  epoch  1, batch     1 | loss: 5.4160776Losses:  5.81840181350708 2.648414134979248
MemoryTrain:  epoch  1, batch     2 | loss: 5.8184018Losses:  5.8506388664245605 2.6231026649475098
MemoryTrain:  epoch  1, batch     3 | loss: 5.8506389Losses:  5.594580173492432 2.610173463821411
MemoryTrain:  epoch  1, batch     4 | loss: 5.5945802Losses:  5.587316513061523 2.6408181190490723
MemoryTrain:  epoch  2, batch     0 | loss: 5.5873165Losses:  5.796133518218994 2.646965980529785
MemoryTrain:  epoch  2, batch     1 | loss: 5.7961335Losses:  5.488654136657715 2.6254849433898926
MemoryTrain:  epoch  2, batch     2 | loss: 5.4886541Losses:  5.310566425323486 2.6184005737304688
MemoryTrain:  epoch  2, batch     3 | loss: 5.3105664Losses:  5.374194145202637 2.61191725730896
MemoryTrain:  epoch  2, batch     4 | loss: 5.3741941Losses:  5.347034454345703 2.6223678588867188
MemoryTrain:  epoch  3, batch     0 | loss: 5.3470345Losses:  5.394840240478516 2.630797863006592
MemoryTrain:  epoch  3, batch     1 | loss: 5.3948402Losses:  5.383039474487305 2.6352977752685547
MemoryTrain:  epoch  3, batch     2 | loss: 5.3830395Losses:  5.422624588012695 2.6175591945648193
MemoryTrain:  epoch  3, batch     3 | loss: 5.4226246Losses:  5.426419258117676 2.629115581512451
MemoryTrain:  epoch  3, batch     4 | loss: 5.4264193Losses:  5.406837463378906 2.636388063430786
MemoryTrain:  epoch  4, batch     0 | loss: 5.4068375Losses:  5.276394367218018 2.6005196571350098
MemoryTrain:  epoch  4, batch     1 | loss: 5.2763944Losses:  5.321626663208008 2.6160976886749268
MemoryTrain:  epoch  4, batch     2 | loss: 5.3216267Losses:  5.386409759521484 2.6462581157684326
MemoryTrain:  epoch  4, batch     3 | loss: 5.3864098Losses:  5.369114398956299 2.632927417755127
MemoryTrain:  epoch  4, batch     4 | loss: 5.3691144Losses:  5.298715591430664 2.6039671897888184
MemoryTrain:  epoch  5, batch     0 | loss: 5.2987156Losses:  5.234763145446777 2.604492664337158
MemoryTrain:  epoch  5, batch     1 | loss: 5.2347631Losses:  5.3633131980896 2.632173538208008
MemoryTrain:  epoch  5, batch     2 | loss: 5.3633132Losses:  5.36923360824585 2.624730110168457
MemoryTrain:  epoch  5, batch     3 | loss: 5.3692336Losses:  5.361551761627197 2.645693778991699
MemoryTrain:  epoch  5, batch     4 | loss: 5.3615518Losses:  5.338109016418457 2.6191561222076416
MemoryTrain:  epoch  6, batch     0 | loss: 5.3381090Losses:  5.26372766494751 2.609619617462158
MemoryTrain:  epoch  6, batch     1 | loss: 5.2637277Losses:  5.298254489898682 2.6186161041259766
MemoryTrain:  epoch  6, batch     2 | loss: 5.2982545Losses:  5.347158908843994 2.6430466175079346
MemoryTrain:  epoch  6, batch     3 | loss: 5.3471589Losses:  5.272669315338135 2.6148009300231934
MemoryTrain:  epoch  6, batch     4 | loss: 5.2726693Losses:  5.216793060302734 2.5859007835388184
MemoryTrain:  epoch  7, batch     0 | loss: 5.2167931Losses:  5.305274963378906 2.632275104522705
MemoryTrain:  epoch  7, batch     1 | loss: 5.3052750Losses:  5.286340713500977 2.62809681892395
MemoryTrain:  epoch  7, batch     2 | loss: 5.2863407Losses:  5.330635070800781 2.6380720138549805
MemoryTrain:  epoch  7, batch     3 | loss: 5.3306351Losses:  5.313931941986084 2.621293783187866
MemoryTrain:  epoch  7, batch     4 | loss: 5.3139319Losses:  5.2801103591918945 2.6222386360168457
MemoryTrain:  epoch  8, batch     0 | loss: 5.2801104Losses:  5.206236839294434 2.5857458114624023
MemoryTrain:  epoch  8, batch     1 | loss: 5.2062368Losses:  5.334630966186523 2.641914129257202
MemoryTrain:  epoch  8, batch     2 | loss: 5.3346310Losses:  5.291010856628418 2.6297903060913086
MemoryTrain:  epoch  8, batch     3 | loss: 5.2910109Losses:  5.256742477416992 2.610962390899658
MemoryTrain:  epoch  8, batch     4 | loss: 5.2567425Losses:  5.304134368896484 2.633945941925049
MemoryTrain:  epoch  9, batch     0 | loss: 5.3041344Losses:  5.263450622558594 2.6031534671783447
MemoryTrain:  epoch  9, batch     1 | loss: 5.2634506Losses:  5.295693874359131 2.625004768371582
MemoryTrain:  epoch  9, batch     2 | loss: 5.2956939Losses:  5.2441511154174805 2.6050801277160645
MemoryTrain:  epoch  9, batch     3 | loss: 5.2441511Losses:  5.271842956542969 2.6197519302368164
MemoryTrain:  epoch  9, batch     4 | loss: 5.2718430
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 57.29%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 55.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 67.59%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 66.81%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 65.76%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 64.29%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 65.78%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 65.27%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 64.52%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 63.89%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.81%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.70%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 84.49%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 83.88%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 82.08%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 81.56%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.65%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 79.88%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 79.04%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 77.94%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 76.87%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 76.10%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 75.54%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 76.06%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 76.03%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 76.07%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 75.73%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 75.63%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 75.46%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 75.23%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 74.93%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 74.85%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 74.93%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 74.43%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 74.02%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 73.89%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 73.37%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 73.01%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 71.30%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 70.90%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 69.96%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 69.88%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 69.81%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 69.39%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 68.81%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 68.18%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 67.56%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 66.35%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 68.13%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.24%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 68.79%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 67.40%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.10%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 67.88%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 67.43%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 67.03%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 66.68%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 66.29%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 65.91%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.45%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 66.04%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 65.64%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 65.25%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 64.86%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 64.47%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 64.20%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 64.36%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 64.49%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 64.26%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 64.07%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 63.84%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.19%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 64.59%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 65.15%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 64.72%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.72%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 64.55%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 64.38%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 64.13%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 63.85%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 63.67%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.36%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 62.91%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 62.91%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.26%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.07%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 66.93%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 67.12%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 67.09%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 66.75%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.90%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 66.82%   [EVAL] batch:  278 | acc: 31.25%,  total acc: 66.69%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 66.54%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.36%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 66.28%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 66.09%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 65.92%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.73%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 65.53%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 65.34%   [EVAL] batch:  288 | acc: 0.00%,  total acc: 65.12%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 64.89%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 64.73%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 64.51%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 64.31%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 64.20%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 65.60%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 65.45%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 65.30%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 64.84%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 64.73%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 64.97%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 64.77%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 64.65%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 64.55%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 64.41%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.36%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 64.99%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 65.43%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 65.24%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 64.88%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 64.69%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 65.08%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 64.95%   [EVAL] batch:  365 | acc: 12.50%,  total acc: 64.81%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 64.62%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 64.50%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 64.44%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 64.38%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 64.37%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 64.21%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 64.09%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 63.80%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 63.65%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 63.52%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 63.62%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 63.65%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 63.74%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  391 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 64.07%   [EVAL] batch:  394 | acc: 0.00%,  total acc: 63.91%   [EVAL] batch:  395 | acc: 6.25%,  total acc: 63.76%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 63.62%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 63.49%   [EVAL] batch:  398 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  399 | acc: 0.00%,  total acc: 63.17%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.03%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 62.89%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.73%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 62.59%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.44%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.28%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.29%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 62.53%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 62.56%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 62.61%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 62.65%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 63.11%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 63.29%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 63.34%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.33%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.36%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 63.31%   [EVAL] batch:  432 | acc: 18.75%,  total acc: 63.21%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 63.13%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  435 | acc: 25.00%,  total acc: 63.02%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 62.96%   [EVAL] batch:  439 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 62.94%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 62.99%   [EVAL] batch:  445 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:  446 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 62.96%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 62.95%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 63.27%   [EVAL] batch:  456 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  457 | acc: 12.50%,  total acc: 63.07%   [EVAL] batch:  458 | acc: 12.50%,  total acc: 62.96%   [EVAL] batch:  459 | acc: 6.25%,  total acc: 62.84%   [EVAL] batch:  460 | acc: 6.25%,  total acc: 62.72%   [EVAL] batch:  461 | acc: 12.50%,  total acc: 62.61%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 62.58%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 62.66%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 63.47%   [EVAL] batch:  476 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:  477 | acc: 43.75%,  total acc: 63.40%   [EVAL] batch:  478 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  479 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 63.36%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 63.26%   [EVAL] batch:  483 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 63.14%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 63.12%   [EVAL] batch:  486 | acc: 50.00%,  total acc: 63.09%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 63.13%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 63.29%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 63.36%   [EVAL] batch:  495 | acc: 31.25%,  total acc: 63.29%   [EVAL] batch:  496 | acc: 62.50%,  total acc: 63.29%   [EVAL] batch:  497 | acc: 43.75%,  total acc: 63.25%   [EVAL] batch:  498 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:  499 | acc: 37.50%,  total acc: 63.14%   
cur_acc:  ['0.9454', '0.7391', '0.7331', '0.7976', '0.7073', '0.6776', '0.5804', '0.6389']
his_acc:  ['0.9454', '0.8370', '0.7842', '0.7775', '0.7560', '0.7215', '0.6557', '0.6314']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  14.246295928955078 1.9253597259521484
CurrentTrain: epoch  0, batch     0 | loss: 14.2462959Losses:  14.132061004638672 2.088536262512207
CurrentTrain: epoch  0, batch     1 | loss: 14.1320610Losses:  13.309758186340332 1.7873934507369995
CurrentTrain: epoch  0, batch     2 | loss: 13.3097582Losses:  14.419020652770996 2.198890209197998
CurrentTrain: epoch  0, batch     3 | loss: 14.4190207Losses:  13.736875534057617 1.909156322479248
CurrentTrain: epoch  0, batch     4 | loss: 13.7368755Losses:  13.172618865966797 1.9963464736938477
CurrentTrain: epoch  0, batch     5 | loss: 13.1726189Losses:  13.36294174194336 1.892439842224121
CurrentTrain: epoch  0, batch     6 | loss: 13.3629417Losses:  12.939414978027344 1.8412954807281494
CurrentTrain: epoch  0, batch     7 | loss: 12.9394150Losses:  13.610971450805664 2.053514242172241
CurrentTrain: epoch  0, batch     8 | loss: 13.6109715Losses:  12.994237899780273 2.0196454524993896
CurrentTrain: epoch  0, batch     9 | loss: 12.9942379Losses:  13.638620376586914 2.070650100708008
CurrentTrain: epoch  0, batch    10 | loss: 13.6386204Losses:  12.701495170593262 2.032561779022217
CurrentTrain: epoch  0, batch    11 | loss: 12.7014952Losses:  12.369874954223633 1.7900936603546143
CurrentTrain: epoch  0, batch    12 | loss: 12.3698750Losses:  12.770174026489258 2.0658693313598633
CurrentTrain: epoch  0, batch    13 | loss: 12.7701740Losses:  12.75848388671875 1.8324339389801025
CurrentTrain: epoch  0, batch    14 | loss: 12.7584839Losses:  12.17438793182373 2.0115628242492676
CurrentTrain: epoch  0, batch    15 | loss: 12.1743879Losses:  12.717083930969238 2.0722641944885254
CurrentTrain: epoch  0, batch    16 | loss: 12.7170839Losses:  12.081045150756836 1.8292343616485596
CurrentTrain: epoch  0, batch    17 | loss: 12.0810452Losses:  12.375391006469727 1.7864153385162354
CurrentTrain: epoch  0, batch    18 | loss: 12.3753910Losses:  11.908025741577148 2.016667604446411
CurrentTrain: epoch  0, batch    19 | loss: 11.9080257Losses:  11.677350997924805 1.8311328887939453
CurrentTrain: epoch  0, batch    20 | loss: 11.6773510Losses:  12.17853832244873 1.865334153175354
CurrentTrain: epoch  0, batch    21 | loss: 12.1785383Losses:  11.216344833374023 1.6720969676971436
CurrentTrain: epoch  0, batch    22 | loss: 11.2163448Losses:  11.96551513671875 1.9885932207107544
CurrentTrain: epoch  0, batch    23 | loss: 11.9655151Losses:  12.708758354187012 2.0643367767333984
CurrentTrain: epoch  0, batch    24 | loss: 12.7087584Losses:  11.409926414489746 1.7167986631393433
CurrentTrain: epoch  0, batch    25 | loss: 11.4099264Losses:  12.446249008178711 2.0104894638061523
CurrentTrain: epoch  0, batch    26 | loss: 12.4462490Losses:  12.695228576660156 2.0779504776000977
CurrentTrain: epoch  0, batch    27 | loss: 12.6952286Losses:  11.997907638549805 1.891758918762207
CurrentTrain: epoch  0, batch    28 | loss: 11.9979076Losses:  11.616575241088867 1.768460988998413
CurrentTrain: epoch  0, batch    29 | loss: 11.6165752Losses:  11.933572769165039 2.0267319679260254
CurrentTrain: epoch  0, batch    30 | loss: 11.9335728Losses:  12.436908721923828 2.1856002807617188
CurrentTrain: epoch  0, batch    31 | loss: 12.4369087Losses:  12.532027244567871 2.064073085784912
CurrentTrain: epoch  0, batch    32 | loss: 12.5320272Losses:  12.119491577148438 2.0933730602264404
CurrentTrain: epoch  0, batch    33 | loss: 12.1194916Losses:  11.720470428466797 2.0444610118865967
CurrentTrain: epoch  0, batch    34 | loss: 11.7204704Losses:  10.955103874206543 1.8761019706726074
CurrentTrain: epoch  0, batch    35 | loss: 10.9551039Losses:  11.405329704284668 1.6292662620544434
CurrentTrain: epoch  0, batch    36 | loss: 11.4053297Losses:  11.536859512329102 2.0058465003967285
CurrentTrain: epoch  0, batch    37 | loss: 11.5368595Losses:  11.955192565917969 2.062933921813965
CurrentTrain: epoch  0, batch    38 | loss: 11.9551926Losses:  12.117050170898438 2.053710460662842
CurrentTrain: epoch  0, batch    39 | loss: 12.1170502Losses:  11.672370910644531 2.0203726291656494
CurrentTrain: epoch  0, batch    40 | loss: 11.6723709Losses:  11.526590347290039 1.9717049598693848
CurrentTrain: epoch  0, batch    41 | loss: 11.5265903Losses:  11.203227996826172 1.8648139238357544
CurrentTrain: epoch  0, batch    42 | loss: 11.2032280Losses:  11.444164276123047 1.848933219909668
CurrentTrain: epoch  0, batch    43 | loss: 11.4441643Losses:  11.041824340820312 1.84621000289917
CurrentTrain: epoch  0, batch    44 | loss: 11.0418243Losses:  11.373720169067383 1.8546302318572998
CurrentTrain: epoch  0, batch    45 | loss: 11.3737202Losses:  11.569443702697754 1.910689353942871
CurrentTrain: epoch  0, batch    46 | loss: 11.5694437Losses:  11.08432388305664 1.9616024494171143
CurrentTrain: epoch  0, batch    47 | loss: 11.0843239Losses:  11.65434741973877 1.917454481124878
CurrentTrain: epoch  0, batch    48 | loss: 11.6543474Losses:  11.105438232421875 2.027031183242798
CurrentTrain: epoch  0, batch    49 | loss: 11.1054382Losses:  11.657637596130371 1.5248446464538574
CurrentTrain: epoch  0, batch    50 | loss: 11.6576376Losses:  10.302180290222168 1.8038965463638306
CurrentTrain: epoch  0, batch    51 | loss: 10.3021803Losses:  11.372352600097656 2.010075569152832
CurrentTrain: epoch  0, batch    52 | loss: 11.3723526Losses:  10.841387748718262 1.675374984741211
CurrentTrain: epoch  0, batch    53 | loss: 10.8413877Losses:  12.587034225463867 1.9492895603179932
CurrentTrain: epoch  0, batch    54 | loss: 12.5870342Losses:  12.269882202148438 1.9476642608642578
CurrentTrain: epoch  0, batch    55 | loss: 12.2698822Losses:  10.481813430786133 1.904848575592041
CurrentTrain: epoch  0, batch    56 | loss: 10.4818134Losses:  10.248205184936523 1.7465450763702393
CurrentTrain: epoch  0, batch    57 | loss: 10.2482052Losses:  11.240057945251465 1.974195957183838
CurrentTrain: epoch  0, batch    58 | loss: 11.2400579Losses:  11.104825973510742 1.648745059967041
CurrentTrain: epoch  0, batch    59 | loss: 11.1048260Losses:  11.269968032836914 1.9193342924118042
CurrentTrain: epoch  0, batch    60 | loss: 11.2699680Losses:  10.703654289245605 1.8472050428390503
CurrentTrain: epoch  0, batch    61 | loss: 10.7036543Losses:  9.944024085998535 1.6580047607421875
CurrentTrain: epoch  0, batch    62 | loss: 9.9440241Losses:  12.207403182983398 1.8640753030776978
CurrentTrain: epoch  1, batch     0 | loss: 12.2074032Losses:  10.205978393554688 1.9141676425933838
CurrentTrain: epoch  1, batch     1 | loss: 10.2059784Losses:  11.256811141967773 2.0066723823547363
CurrentTrain: epoch  1, batch     2 | loss: 11.2568111Losses:  10.251677513122559 1.725857138633728
CurrentTrain: epoch  1, batch     3 | loss: 10.2516775Losses:  10.228784561157227 1.9708213806152344
CurrentTrain: epoch  1, batch     4 | loss: 10.2287846Losses:  9.740413665771484 1.7467103004455566
CurrentTrain: epoch  1, batch     5 | loss: 9.7404137Losses:  10.760668754577637 1.7642415761947632
CurrentTrain: epoch  1, batch     6 | loss: 10.7606688Losses:  10.964869499206543 1.9341204166412354
CurrentTrain: epoch  1, batch     7 | loss: 10.9648695Losses:  11.06059741973877 2.0307412147521973
CurrentTrain: epoch  1, batch     8 | loss: 11.0605974Losses:  9.380450248718262 1.445269227027893
CurrentTrain: epoch  1, batch     9 | loss: 9.3804502Losses:  10.568428039550781 1.8657300472259521
CurrentTrain: epoch  1, batch    10 | loss: 10.5684280Losses:  11.640495300292969 1.9912233352661133
CurrentTrain: epoch  1, batch    11 | loss: 11.6404953Losses:  10.566665649414062 1.8748133182525635
CurrentTrain: epoch  1, batch    12 | loss: 10.5666656Losses:  10.665355682373047 1.8877590894699097
CurrentTrain: epoch  1, batch    13 | loss: 10.6653557Losses:  11.283206939697266 2.0492000579833984
CurrentTrain: epoch  1, batch    14 | loss: 11.2832069Losses:  10.554200172424316 1.8233531713485718
CurrentTrain: epoch  1, batch    15 | loss: 10.5542002Losses:  10.37031364440918 1.9911255836486816
CurrentTrain: epoch  1, batch    16 | loss: 10.3703136Losses:  11.599010467529297 1.9114844799041748
CurrentTrain: epoch  1, batch    17 | loss: 11.5990105Losses:  10.645050048828125 2.0461010932922363
CurrentTrain: epoch  1, batch    18 | loss: 10.6450500Losses:  10.516923904418945 1.9855751991271973
CurrentTrain: epoch  1, batch    19 | loss: 10.5169239Losses:  10.8629732131958 2.0533108711242676
CurrentTrain: epoch  1, batch    20 | loss: 10.8629732Losses:  10.752717018127441 2.0479164123535156
CurrentTrain: epoch  1, batch    21 | loss: 10.7527170Losses:  10.39974308013916 2.025853395462036
CurrentTrain: epoch  1, batch    22 | loss: 10.3997431Losses:  10.597267150878906 1.92363703250885
CurrentTrain: epoch  1, batch    23 | loss: 10.5972672Losses:  10.844558715820312 1.9659044742584229
CurrentTrain: epoch  1, batch    24 | loss: 10.8445587Losses:  10.147047996520996 2.0115017890930176
CurrentTrain: epoch  1, batch    25 | loss: 10.1470480Losses:  11.058853149414062 1.9099860191345215
CurrentTrain: epoch  1, batch    26 | loss: 11.0588531Losses:  9.366558074951172 1.7118792533874512
CurrentTrain: epoch  1, batch    27 | loss: 9.3665581Losses:  9.305325508117676 1.7377198934555054
CurrentTrain: epoch  1, batch    28 | loss: 9.3053255Losses:  10.408288955688477 2.047064781188965
CurrentTrain: epoch  1, batch    29 | loss: 10.4082890Losses:  10.539710998535156 2.0196070671081543
CurrentTrain: epoch  1, batch    30 | loss: 10.5397110Losses:  10.431696891784668 1.847930908203125
CurrentTrain: epoch  1, batch    31 | loss: 10.4316969Losses:  9.462994575500488 1.6627792119979858
CurrentTrain: epoch  1, batch    32 | loss: 9.4629946Losses:  10.136202812194824 1.95747971534729
CurrentTrain: epoch  1, batch    33 | loss: 10.1362028Losses:  10.33089828491211 1.9260469675064087
CurrentTrain: epoch  1, batch    34 | loss: 10.3308983Losses:  10.607765197753906 1.9873497486114502
CurrentTrain: epoch  1, batch    35 | loss: 10.6077652Losses:  10.471954345703125 2.133286952972412
CurrentTrain: epoch  1, batch    36 | loss: 10.4719543Losses:  10.237409591674805 2.0458810329437256
CurrentTrain: epoch  1, batch    37 | loss: 10.2374096Losses:  11.023622512817383 2.144663095474243
CurrentTrain: epoch  1, batch    38 | loss: 11.0236225Losses:  8.205877304077148 1.4303525686264038
CurrentTrain: epoch  1, batch    39 | loss: 8.2058773Losses:  10.446552276611328 1.8793853521347046
CurrentTrain: epoch  1, batch    40 | loss: 10.4465523Losses:  10.989213943481445 2.033762216567993
CurrentTrain: epoch  1, batch    41 | loss: 10.9892139Losses:  9.685649871826172 2.004258632659912
CurrentTrain: epoch  1, batch    42 | loss: 9.6856499Losses:  10.418851852416992 2.0463147163391113
CurrentTrain: epoch  1, batch    43 | loss: 10.4188519Losses:  11.122783660888672 1.946847677230835
CurrentTrain: epoch  1, batch    44 | loss: 11.1227837Losses:  10.077741622924805 2.0393881797790527
CurrentTrain: epoch  1, batch    45 | loss: 10.0777416Losses:  9.474058151245117 1.735172986984253
CurrentTrain: epoch  1, batch    46 | loss: 9.4740582Losses:  9.830284118652344 1.8633131980895996
CurrentTrain: epoch  1, batch    47 | loss: 9.8302841Losses:  10.766950607299805 1.8095896244049072
CurrentTrain: epoch  1, batch    48 | loss: 10.7669506Losses:  9.743168830871582 1.963963508605957
CurrentTrain: epoch  1, batch    49 | loss: 9.7431688Losses:  9.649588584899902 1.9270261526107788
CurrentTrain: epoch  1, batch    50 | loss: 9.6495886Losses:  8.368764877319336 1.586977243423462
CurrentTrain: epoch  1, batch    51 | loss: 8.3687649Losses:  9.582959175109863 1.7551665306091309
CurrentTrain: epoch  1, batch    52 | loss: 9.5829592Losses:  8.934919357299805 1.8526909351348877
CurrentTrain: epoch  1, batch    53 | loss: 8.9349194Losses:  10.615866661071777 1.9802532196044922
CurrentTrain: epoch  1, batch    54 | loss: 10.6158667Losses:  8.953558921813965 1.6849544048309326
CurrentTrain: epoch  1, batch    55 | loss: 8.9535589Losses:  10.099349021911621 1.938941240310669
CurrentTrain: epoch  1, batch    56 | loss: 10.0993490Losses:  10.051589965820312 1.8632934093475342
CurrentTrain: epoch  1, batch    57 | loss: 10.0515900Losses:  9.74459457397461 1.902665138244629
CurrentTrain: epoch  1, batch    58 | loss: 9.7445946Losses:  9.419045448303223 1.9267940521240234
CurrentTrain: epoch  1, batch    59 | loss: 9.4190454Losses:  9.44587516784668 1.979746699333191
CurrentTrain: epoch  1, batch    60 | loss: 9.4458752Losses:  10.147754669189453 1.8808226585388184
CurrentTrain: epoch  1, batch    61 | loss: 10.1477547Losses:  9.603460311889648 1.4181346893310547
CurrentTrain: epoch  1, batch    62 | loss: 9.6034603Losses:  9.476774215698242 1.8835628032684326
CurrentTrain: epoch  2, batch     0 | loss: 9.4767742Losses:  9.20961856842041 1.9989278316497803
CurrentTrain: epoch  2, batch     1 | loss: 9.2096186Losses:  8.988421440124512 1.9265841245651245
CurrentTrain: epoch  2, batch     2 | loss: 8.9884214Losses:  9.010658264160156 1.8737382888793945
CurrentTrain: epoch  2, batch     3 | loss: 9.0106583Losses:  8.974742889404297 1.849994421005249
CurrentTrain: epoch  2, batch     4 | loss: 8.9747429Losses:  9.662429809570312 1.8301153182983398
CurrentTrain: epoch  2, batch     5 | loss: 9.6624298Losses:  9.097476959228516 1.897463321685791
CurrentTrain: epoch  2, batch     6 | loss: 9.0974770Losses:  10.320890426635742 2.0562078952789307
CurrentTrain: epoch  2, batch     7 | loss: 10.3208904Losses:  9.228020668029785 1.6654404401779175
CurrentTrain: epoch  2, batch     8 | loss: 9.2280207Losses:  8.704736709594727 1.8441399335861206
CurrentTrain: epoch  2, batch     9 | loss: 8.7047367Losses:  9.208292961120605 1.8309112787246704
CurrentTrain: epoch  2, batch    10 | loss: 9.2082930Losses:  9.238157272338867 1.9586598873138428
CurrentTrain: epoch  2, batch    11 | loss: 9.2381573Losses:  9.140413284301758 1.8902182579040527
CurrentTrain: epoch  2, batch    12 | loss: 9.1404133Losses:  9.24444580078125 1.890105128288269
CurrentTrain: epoch  2, batch    13 | loss: 9.2444458Losses:  9.380538940429688 1.7970350980758667
CurrentTrain: epoch  2, batch    14 | loss: 9.3805389Losses:  8.938285827636719 1.9180033206939697
CurrentTrain: epoch  2, batch    15 | loss: 8.9382858Losses:  9.110822677612305 1.752732515335083
CurrentTrain: epoch  2, batch    16 | loss: 9.1108227Losses:  10.246204376220703 1.8188172578811646
CurrentTrain: epoch  2, batch    17 | loss: 10.2462044Losses:  8.63305950164795 1.7249969244003296
CurrentTrain: epoch  2, batch    18 | loss: 8.6330595Losses:  9.671879768371582 1.990824818611145
CurrentTrain: epoch  2, batch    19 | loss: 9.6718798Losses:  9.632713317871094 1.4363648891448975
CurrentTrain: epoch  2, batch    20 | loss: 9.6327133Losses:  9.999421119689941 2.087216854095459
CurrentTrain: epoch  2, batch    21 | loss: 9.9994211Losses:  8.978734970092773 2.030545473098755
CurrentTrain: epoch  2, batch    22 | loss: 8.9787350Losses:  9.273130416870117 1.8913929462432861
CurrentTrain: epoch  2, batch    23 | loss: 9.2731304Losses:  8.751883506774902 1.898241639137268
CurrentTrain: epoch  2, batch    24 | loss: 8.7518835Losses:  8.263399124145508 1.4791079759597778
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  13.658048629760742 1.8152053356170654
CurrentTrain: epoch  0, batch     0 | loss: 13.6580486Losses:  14.135658264160156 2.084775686264038
CurrentTrain: epoch  0, batch     1 | loss: 14.1356583Losses:  13.935410499572754 1.9284567832946777
CurrentTrain: epoch  0, batch     2 | loss: 13.9354105Losses:  13.578903198242188 1.7870426177978516
CurrentTrain: epoch  0, batch     3 | loss: 13.5789032Losses:  13.228553771972656 1.9896624088287354
CurrentTrain: epoch  0, batch     4 | loss: 13.2285538Losses:  13.832742691040039 1.6988774538040161
CurrentTrain: epoch  0, batch     5 | loss: 13.8327427Losses:  13.42496395111084 1.9350390434265137
CurrentTrain: epoch  0, batch     6 | loss: 13.4249640Losses:  12.718107223510742 1.8829104900360107
CurrentTrain: epoch  0, batch     7 | loss: 12.7181072Losses:  13.004568099975586 1.9475069046020508
CurrentTrain: epoch  0, batch     8 | loss: 13.0045681Losses:  13.599405288696289 1.9413142204284668
CurrentTrain: epoch  0, batch     9 | loss: 13.5994053Losses:  12.523136138916016 1.7766801118850708
CurrentTrain: epoch  0, batch    10 | loss: 12.5231361Losses:  11.946648597717285 1.7518728971481323
CurrentTrain: epoch  0, batch    11 | loss: 11.9466486Losses:  12.728720664978027 1.7979220151901245
CurrentTrain: epoch  0, batch    12 | loss: 12.7287207Losses:  12.93392276763916 1.9537558555603027
CurrentTrain: epoch  0, batch    13 | loss: 12.9339228Losses:  11.903498649597168 1.8731838464736938
CurrentTrain: epoch  0, batch    14 | loss: 11.9034986Losses:  12.969371795654297 2.0658702850341797
CurrentTrain: epoch  0, batch    15 | loss: 12.9693718Losses:  12.451952934265137 1.61127507686615
CurrentTrain: epoch  0, batch    16 | loss: 12.4519529Losses:  13.157547950744629 2.070855140686035
CurrentTrain: epoch  0, batch    17 | loss: 13.1575480Losses:  12.636455535888672 1.8289811611175537
CurrentTrain: epoch  0, batch    18 | loss: 12.6364555Losses:  12.695921897888184 2.0304760932922363
CurrentTrain: epoch  0, batch    19 | loss: 12.6959219Losses:  12.028919219970703 2.003396987915039
CurrentTrain: epoch  0, batch    20 | loss: 12.0289192Losses:  12.210872650146484 2.0093023777008057
CurrentTrain: epoch  0, batch    21 | loss: 12.2108727Losses:  12.665508270263672 1.9600965976715088
CurrentTrain: epoch  0, batch    22 | loss: 12.6655083Losses:  12.045090675354004 1.9494248628616333
CurrentTrain: epoch  0, batch    23 | loss: 12.0450907Losses:  11.80167007446289 2.0188567638397217
CurrentTrain: epoch  0, batch    24 | loss: 11.8016701Losses:  11.846067428588867 1.9156301021575928
CurrentTrain: epoch  0, batch    25 | loss: 11.8460674Losses:  11.887099266052246 1.8347975015640259
CurrentTrain: epoch  0, batch    26 | loss: 11.8870993Losses:  11.868501663208008 2.0617122650146484
CurrentTrain: epoch  0, batch    27 | loss: 11.8685017Losses:  11.99818229675293 1.8991190195083618
CurrentTrain: epoch  0, batch    28 | loss: 11.9981823Losses:  12.154363632202148 2.002124786376953
CurrentTrain: epoch  0, batch    29 | loss: 12.1543636Losses:  12.781850814819336 2.114361524581909
CurrentTrain: epoch  0, batch    30 | loss: 12.7818508Losses:  11.649133682250977 1.8232231140136719
CurrentTrain: epoch  0, batch    31 | loss: 11.6491337Losses:  10.938837051391602 1.4809629917144775
CurrentTrain: epoch  0, batch    32 | loss: 10.9388371Losses:  10.81825065612793 1.683300495147705
CurrentTrain: epoch  0, batch    33 | loss: 10.8182507Losses:  11.587911605834961 1.8528249263763428
CurrentTrain: epoch  0, batch    34 | loss: 11.5879116Losses:  11.52086067199707 1.893585205078125
CurrentTrain: epoch  0, batch    35 | loss: 11.5208607Losses:  11.57608413696289 1.9295761585235596
CurrentTrain: epoch  0, batch    36 | loss: 11.5760841Losses:  11.646147727966309 1.9507185220718384
CurrentTrain: epoch  0, batch    37 | loss: 11.6461477Losses:  12.795304298400879 2.1470561027526855
CurrentTrain: epoch  0, batch    38 | loss: 12.7953043Losses:  12.388534545898438 2.185178279876709
CurrentTrain: epoch  0, batch    39 | loss: 12.3885345Losses:  12.537713050842285 2.0565810203552246
CurrentTrain: epoch  0, batch    40 | loss: 12.5377131Losses:  11.422494888305664 2.0055370330810547
CurrentTrain: epoch  0, batch    41 | loss: 11.4224949Losses:  11.216670989990234 1.9748599529266357
CurrentTrain: epoch  0, batch    42 | loss: 11.2166710Losses:  10.388556480407715 1.6851520538330078
CurrentTrain: epoch  0, batch    43 | loss: 10.3885565Losses:  10.889359474182129 1.773287296295166
CurrentTrain: epoch  0, batch    44 | loss: 10.8893595Losses:  11.437918663024902 2.055168628692627
CurrentTrain: epoch  0, batch    45 | loss: 11.4379187Losses:  11.792146682739258 1.8334534168243408
CurrentTrain: epoch  0, batch    46 | loss: 11.7921467Losses:  10.700630187988281 2.0566840171813965
CurrentTrain: epoch  0, batch    47 | loss: 10.7006302Losses:  11.466564178466797 1.945091724395752
CurrentTrain: epoch  0, batch    48 | loss: 11.4665642Losses:  11.191732406616211 1.9480223655700684
CurrentTrain: epoch  0, batch    49 | loss: 11.1917324Losses:  11.70003890991211 2.008071184158325
CurrentTrain: epoch  0, batch    50 | loss: 11.7000389Losses:  11.469734191894531 2.1224982738494873
CurrentTrain: epoch  0, batch    51 | loss: 11.4697342Losses:  10.866242408752441 1.9469671249389648
CurrentTrain: epoch  0, batch    52 | loss: 10.8662424Losses:  10.988348007202148 1.9207146167755127
CurrentTrain: epoch  0, batch    53 | loss: 10.9883480Losses:  11.87764835357666 2.069917678833008
CurrentTrain: epoch  0, batch    54 | loss: 11.8776484Losses:  10.711790084838867 1.8598437309265137
CurrentTrain: epoch  0, batch    55 | loss: 10.7117901Losses:  10.424843788146973 1.832515001296997
CurrentTrain: epoch  0, batch    56 | loss: 10.4248438Losses:  11.512801170349121 2.0084362030029297
CurrentTrain: epoch  0, batch    57 | loss: 11.5128012Losses:  10.883712768554688 1.7590440511703491
CurrentTrain: epoch  0, batch    58 | loss: 10.8837128Losses:  10.518198013305664 1.741225004196167
CurrentTrain: epoch  0, batch    59 | loss: 10.5181980Losses:  10.463438034057617 1.9696629047393799
CurrentTrain: epoch  0, batch    60 | loss: 10.4634380Losses:  10.986064910888672 1.7755372524261475
CurrentTrain: epoch  0, batch    61 | loss: 10.9860649Losses:  11.876496315002441 1.4757288694381714
CurrentTrain: epoch  0, batch    62 | loss: 11.8764963Losses:  11.00615119934082 1.8054540157318115
CurrentTrain: epoch  1, batch     0 | loss: 11.0061512Losses:  10.809710502624512 1.9071162939071655
CurrentTrain: epoch  1, batch     1 | loss: 10.8097105Losses:  10.336471557617188 1.8037660121917725
CurrentTrain: epoch  1, batch     2 | loss: 10.3364716Losses:  11.127817153930664 2.0839009284973145
CurrentTrain: epoch  1, batch     3 | loss: 11.1278172Losses:  11.356744766235352 2.041707754135132
CurrentTrain: epoch  1, batch     4 | loss: 11.3567448Losses:  10.644865036010742 1.9455293416976929
CurrentTrain: epoch  1, batch     5 | loss: 10.6448650Losses:  10.751540184020996 1.934108853340149
CurrentTrain: epoch  1, batch     6 | loss: 10.7515402Losses:  10.319551467895508 2.080289840698242
CurrentTrain: epoch  1, batch     7 | loss: 10.3195515Losses:  9.826364517211914 1.7551286220550537
CurrentTrain: epoch  1, batch     8 | loss: 9.8263645Losses:  11.016510009765625 2.0044171810150146
CurrentTrain: epoch  1, batch     9 | loss: 11.0165100Losses:  10.829578399658203 1.9565682411193848
CurrentTrain: epoch  1, batch    10 | loss: 10.8295784Losses:  10.484881401062012 1.9733870029449463
CurrentTrain: epoch  1, batch    11 | loss: 10.4848814Losses:  10.869524955749512 2.0335636138916016
CurrentTrain: epoch  1, batch    12 | loss: 10.8695250Losses:  10.44006061553955 1.6632603406906128
CurrentTrain: epoch  1, batch    13 | loss: 10.4400606Losses:  10.18657112121582 1.9753341674804688
CurrentTrain: epoch  1, batch    14 | loss: 10.1865711Losses:  11.667218208312988 1.902620553970337
CurrentTrain: epoch  1, batch    15 | loss: 11.6672182Losses:  10.164722442626953 1.7947072982788086
CurrentTrain: epoch  1, batch    16 | loss: 10.1647224Losses:  9.217344284057617 1.632741928100586
CurrentTrain: epoch  1, batch    17 | loss: 9.2173443Losses:  10.105426788330078 1.8577213287353516
CurrentTrain: epoch  1, batch    18 | loss: 10.1054268Losses:  10.667967796325684 1.9946845769882202
CurrentTrain: epoch  1, batch    19 | loss: 10.6679678Losses:  11.107768058776855 2.084688663482666
CurrentTrain: epoch  1, batch    20 | loss: 11.1077681Losses:  11.093356132507324 2.074587821960449
CurrentTrain: epoch  1, batch    21 | loss: 11.0933561Losses:  10.557735443115234 1.9141008853912354
CurrentTrain: epoch  1, batch    22 | loss: 10.5577354Losses:  10.365035057067871 1.7439560890197754
CurrentTrain: epoch  1, batch    23 | loss: 10.3650351Losses:  10.402629852294922 1.9336299896240234
CurrentTrain: epoch  1, batch    24 | loss: 10.4026299Losses:  9.300516128540039 1.8648028373718262
CurrentTrain: epoch  1, batch    25 | loss: 9.3005161Losses:  10.329456329345703 2.080652952194214
CurrentTrain: epoch  1, batch    26 | loss: 10.3294563Losses:  10.028970718383789 1.9635100364685059
CurrentTrain: epoch  1, batch    27 | loss: 10.0289707Losses:  10.538595199584961 1.9941699504852295
CurrentTrain: epoch  1, batch    28 | loss: 10.5385952Losses:  10.084444046020508 2.074629545211792
CurrentTrain: epoch  1, batch    29 | loss: 10.0844440Losses:  10.127654075622559 1.9160809516906738
CurrentTrain: epoch  1, batch    30 | loss: 10.1276541Losses:  9.918073654174805 2.115823268890381
CurrentTrain: epoch  1, batch    31 | loss: 9.9180737Losses:  9.419921875 1.9824254512786865
CurrentTrain: epoch  1, batch    32 | loss: 9.4199219Losses:  9.400007247924805 1.7466351985931396
CurrentTrain: epoch  1, batch    33 | loss: 9.4000072Losses:  10.792829513549805 2.0415406227111816
CurrentTrain: epoch  1, batch    34 | loss: 10.7928295Losses:  9.455085754394531 1.898605227470398
CurrentTrain: epoch  1, batch    35 | loss: 9.4550858Losses:  10.369172096252441 1.73555326461792
CurrentTrain: epoch  1, batch    36 | loss: 10.3691721Losses:  10.678298950195312 2.019043445587158
CurrentTrain: epoch  1, batch    37 | loss: 10.6782990Losses:  11.23527717590332 2.0786304473876953
CurrentTrain: epoch  1, batch    38 | loss: 11.2352772Losses:  9.584039688110352 1.9918577671051025
CurrentTrain: epoch  1, batch    39 | loss: 9.5840397Losses:  10.417037010192871 1.901558518409729
CurrentTrain: epoch  1, batch    40 | loss: 10.4170370Losses:  10.90762710571289 2.0201289653778076
CurrentTrain: epoch  1, batch    41 | loss: 10.9076271Losses:  10.75309944152832 1.9216382503509521
CurrentTrain: epoch  1, batch    42 | loss: 10.7530994Losses:  11.219287872314453 1.9459196329116821
CurrentTrain: epoch  1, batch    43 | loss: 11.2192879Losses:  10.960738182067871 1.8019181489944458
CurrentTrain: epoch  1, batch    44 | loss: 10.9607382Losses:  9.980695724487305 2.0481581687927246
CurrentTrain: epoch  1, batch    45 | loss: 9.9806957Losses:  9.431447982788086 1.9867926836013794
CurrentTrain: epoch  1, batch    46 | loss: 9.4314480Losses:  10.80199146270752 2.16507625579834
CurrentTrain: epoch  1, batch    47 | loss: 10.8019915Losses:  10.108030319213867 1.9307652711868286
CurrentTrain: epoch  1, batch    48 | loss: 10.1080303Losses:  9.834442138671875 1.9535448551177979
CurrentTrain: epoch  1, batch    49 | loss: 9.8344421Losses:  9.998995780944824 1.8924282789230347
CurrentTrain: epoch  1, batch    50 | loss: 9.9989958Losses:  10.747438430786133 1.7989404201507568
CurrentTrain: epoch  1, batch    51 | loss: 10.7474384Losses:  10.029958724975586 1.8698194026947021
CurrentTrain: epoch  1, batch    52 | loss: 10.0299587Losses:  10.181305885314941 1.9029734134674072
CurrentTrain: epoch  1, batch    53 | loss: 10.1813059Losses:  10.242450714111328 1.9804986715316772
CurrentTrain: epoch  1, batch    54 | loss: 10.2424507Losses:  8.795526504516602 1.7446362972259521
CurrentTrain: epoch  1, batch    55 | loss: 8.7955265Losses:  8.883163452148438 1.7612831592559814
CurrentTrain: epoch  1, batch    56 | loss: 8.8831635Losses:  10.122174263000488 2.00993013381958
CurrentTrain: epoch  1, batch    57 | loss: 10.1221743Losses:  9.12513256072998 1.7954812049865723
CurrentTrain: epoch  1, batch    58 | loss: 9.1251326Losses:  10.823107719421387 2.0801594257354736
CurrentTrain: epoch  1, batch    59 | loss: 10.8231077Losses:  10.32821273803711 2.025230884552002
CurrentTrain: epoch  1, batch    60 | loss: 10.3282127Losses:  9.876555442810059 2.0490670204162598
CurrentTrain: epoch  1, batch    61 | loss: 9.8765554Losses:  7.617546558380127 1.42887282371521
CurrentTrain: epoch  1, batch    62 | loss: 7.6175466Losses:  9.378850936889648 1.6959726810455322
CurrentTrain: epoch  2, batch     0 | loss: 9.3788509Losses:  9.381766319274902 1.7057336568832397
CurrentTrain: epoch  2, batch     1 | loss: 9.3817663Losses:  9.880189895629883 1.982503890991211
CurrentTrain: epoch  2, batch     2 | loss: 9.8801899Losses:  9.048286437988281 1.926525592803955
CurrentTrain: epoch  2, batch     3 | loss: 9.0482864Losses:  9.462017059326172 1.909820318222046
CurrentTrain: epoch  2, batch     4 | loss: 9.4620171Losses:  9.755905151367188 2.1489996910095215
CurrentTrain: epoch  2, batch     5 | loss: 9.7559052Losses:  8.98255443572998 1.7729569673538208
CurrentTrain: epoch  2, batch     6 | loss: 8.9825544Losses:  10.036996841430664 1.9617424011230469
CurrentTrain: epoch  2, batch     7 | loss: 10.0369968Losses:  10.599284172058105 1.9493941068649292
CurrentTrain: epoch  2, batch     8 | loss: 10.5992842Losses:  9.013286590576172 1.8580485582351685
CurrentTrain: epoch  2, batch     9 | loss: 9.0132866Losses:  9.017536163330078 1.9395811557769775
CurrentTrain: epoch  2, batch    10 | loss: 9.0175362Losses:  9.797152519226074 1.8886317014694214
CurrentTrain: epoch  2, batch    11 | loss: 9.7971525Losses:  9.990715026855469 1.837550401687622
CurrentTrain: epoch  2, batch    12 | loss: 9.9907150Losses:  9.520718574523926 1.7872623205184937
CurrentTrain: epoch  2, batch    13 | loss: 9.5207186Losses:  9.664384841918945 1.9970461130142212
CurrentTrain: epoch  2, batch    14 | loss: 9.6643848Losses:  9.455780029296875 2.053041458129883
CurrentTrain: epoch  2, batch    15 | loss: 9.4557800Losses:  8.893370628356934 1.9295824766159058
CurrentTrain: epoch  2, batch    16 | loss: 8.8933706Losses:  7.9936723709106445 1.610083818435669
CurrentTrain: epoch  2, batch    17 | loss: 7.9936724Losses:  10.17605209350586 2.082212448120117
CurrentTrain: epoch  2, batch    18 | loss: 10.1760521Losses:  9.379799842834473 1.9050010442733765
CurrentTrain: epoch  2, batch    19 | loss: 9.3797998Losses:  9.3839693069458 1.9731645584106445
CurrentTrain: epoch  2, batch    20 | loss: 9.3839693Losses:  9.138874053955078 1.9414682388305664
CurrentTrain: epoch  2, batch    21 | loss: 9.1388741Losses:  8.905548095703125 1.9360811710357666
CurrentTrain: epoch  2, batch    22 | loss: 8.9055481Losses:  9.648274421691895 1.9973831176757812
CurrentTrain: epoch  2, batch    23 | loss: 9.6482744Losses:  9.387672424316406 1.8845021724700928
CurrentTrain: epoch  2, batch    24 | loss: 9.3876724Losses:  9.11525821685791 1.94314444065094
CurrentTrain: epoch  2, batch    25 | loss: 9.1152582Losses:  8.863469123840332 1.9279556274414062
CurrentTrain: epoch  2, batch    26 | loss: 8.8634691Losses:  8.662657737731934 1.689427137374878
CurrentTrain: epoch  2, batch    27 | loss: 8.6626577Losses:  8.709653854370117 1.8164374828338623
CurrentTrain: epoch  2, batch    28 | loss: 8.7096539Losses:  9.715156555175781 2.1421074867248535
CurrentTrain: epoch  2, batch    29 | loss: 9.7151566Losses:  9.72634220123291 2.029458522796631
CurrentTrain: epoch  2, batch    30 | loss: 9.7263422Losses:  9.4037446975708 2.1816225051879883
CurrentTrain: epoch  2, batch    31 | loss: 9.4037447Losses:  9.879880905151367 1.787776231765747
CurrentTrain: epoch  2, batch    32 | loss: 9.8798809Losses:  8.530668258666992 1.8562812805175781
CurrentTrain: epoch  2, batch    33 | loss: 8.5306683Losses:  9.564018249511719 2.028674602508545
CurrentTrain: epoch  2, batch    34 | loss: 9.5640182Losses:  9.354131698608398 1.898996114730835
CurrentTrain: epoch  2, batch    35 | loss: 9.3541317Losses:  9.009148597717285 2.0173773765563965
CurrentTrain: epoch  2, batch    36 | loss: 9.0091486Losses:  8.952201843261719 1.7631447315216064
CurrentTrain: epoch  2, batch    37 | loss: 8.9522018Losses:  8.99251937866211 1.8706966638565063
CurrentTrain: epoch  2, batch    38 | loss: 8.9925194Losses:  8.24697494506836 1.7650152444839478
CurrentTrain: epoch  2, batch    39 | loss: 8.2469749Losses:  8.80136775970459 1.6816835403442383
CurrentTrain: epoch  2, batch    40 | loss: 8.8013678Losses:  8.958282470703125 1.5092787742614746
CurrentTrain: epoch  2, batch    41 | loss: 8.9582825Losses:  9.052685737609863 1.9393559694290161
CurrentTrain: epoch  2, batch    42 | loss: 9.0526857Losses:  9.210807800292969 1.9035718441009521
CurrentTrain: epoch  2, batch    43 | loss: 9.2108078Losses:  9.147072792053223 2.0284018516540527
CurrentTrain: epoch  2, batch    44 | loss: 9.1470728Losses:  9.393033981323242 1.9787904024124146
CurrentTrain: epoch  2, batch    45 | loss: 9.3930340Losses:  8.834207534790039 1.9836041927337646
CurrentTrain: epoch  2, batch    46 | loss: 8.8342075Losses:  9.599737167358398 1.9761652946472168
CurrentTrain: epoch  2, batch    47 | loss: 9.5997372Losses:  8.677905082702637 1.8955870866775513
CurrentTrain: epoch  2, batch    48 | loss: 8.6779051Losses:  9.384199142456055 1.9729938507080078
CurrentTrain: epoch  2, batch    49 | loss: 9.3841991Losses:  8.598433494567871 1.777029037475586
CurrentTrain: epoch  2, batch    50 | loss: 8.5984335Losses:  8.848155975341797 2.019977569580078
CurrentTrain: epoch  2, batch    51 | loss: 8.8481560Losses:  8.700761795043945 1.7911955118179321
CurrentTrain: epoch  2, batch    52 | loss: 8.7007618Losses:  9.167439460754395 2.0647153854370117
CurrentTrain: epoch  2, batch    53 | loss: 9.1674395Losses:  8.888906478881836 2.008896589279175
CurrentTrain: epoch  2, batch    54 | loss: 8.8889065Losses:  9.07189655303955 2.0658798217773438
CurrentTrain: epoch  2, batch    55 | loss: 9.0718966Losses:  9.595523834228516 2.135413408279419
CurrentTrain: epoch  2, batch    56 | loss: 9.5955238Losses:  8.98984432220459 1.8528828620910645
CurrentTrain: epoch  2, batch    57 | loss: 8.9898443Losses:  8.668103218078613 1.8154065608978271
CurrentTrain: epoch  2, batch    58 | loss: 8.6681032Losses:  8.779382705688477 1.942810297012329
CurrentTrain: epoch  2, batch    59 | loss: 8.7793827Losses:  8.566644668579102 1.9428074359893799
CurrentTrain: epoch  2, batch    60 | loss: 8.5666447Losses:  8.669975280761719 1.8467285633087158
CurrentTrain: epoch  2, batch    61 | loss: 8.6699753Losses:  8.174745559692383 1.6604325771331787
CurrentTrain: epoch  2, batch    62 | loss: 8.1747456Losses:  9.345770835876465 1.9060474634170532
CurrentTrain: epoch  3, batch     0 | loss: 9.3457708Losses:  9.671205520629883 1.811981439590454
CurrentTrain: epoch  3, batch     1 | loss: 9.6712055Losses:  8.943361282348633 2.007510185241699
CurrentTrain: epoch  3, batch     2 | loss: 8.9433613Losses:  8.89709186553955 1.7648921012878418
CurrentTrain: epoch  3, batch     3 | loss: 8.8970919Losses:  8.96774673461914 1.9021681547164917
CurrentTrain: epoch  3, batch     4 | loss: 8.9677467Losses:  9.087545394897461 1.9060256481170654
CurrentTrain: epoch  3, batch     5 | loss: 9.0875454Losses:  8.737910270690918 1.917704701423645
CurrentTrain: epoch  3, batch     6 | loss: 8.7379103Losses:  8.974287033081055 2.0144033432006836
CurrentTrain: epoch  3, batch     7 | loss: 8.9742870Losses:  9.182612419128418 1.926439642906189
CurrentTrain: epoch  3, batch     8 | loss: 9.1826124Losses:  9.359893798828125 2.0588066577911377
CurrentTrain: epoch  3, batch     9 | loss: 9.3598938Losses:  9.400211334228516 2.1364879608154297
CurrentTrain: epoch  3, batch    10 | loss: 9.4002113Losses:  8.835185050964355 1.912541389465332
CurrentTrain: epoch  3, batch    11 | loss: 8.8351851Losses:  8.261445045471191 1.8721572160720825
CurrentTrain: epoch  3, batch    12 | loss: 8.2614450Losses:  9.309292793273926 2.1339097023010254
CurrentTrain: epoch  3, batch    13 | loss: 9.3092928Losses:  8.729021072387695 1.9792039394378662
CurrentTrain: epoch  3, batch    14 | loss: 8.7290211Losses:  8.713958740234375 2.0094640254974365
CurrentTrain: epoch  3, batch    15 | loss: 8.7139587Losses:  8.932165145874023 1.9608268737792969
CurrentTrain: epoch  3, batch    16 | loss: 8.9321651Losses:  8.765076637268066 1.744612216949463
CurrentTrain: epoch  3, batch    17 | loss: 8.7650766Losses:  8.803165435791016 1.8782217502593994
CurrentTrain: epoch  3, batch    18 | loss: 8.8031654Losses:  8.723587036132812 1.9061248302459717
CurrentTrain: epoch  3, batch    19 | loss: 8.7235870Losses:  8.630387306213379 1.918683648109436
CurrentTrain: epoch  3, batch    20 | loss: 8.6303873Losses:  8.876760482788086 2.045379638671875
CurrentTrain: epoch  3, batch    21 | loss: 8.8767605Losses:  8.407366752624512 1.7694122791290283
CurrentTrain: epoch  3, batch    22 | loss: 8.4073668Losses:  8.790666580200195 2.0439863204956055
CurrentTrain: epoch  3, batch    23 | loss: 8.7906666Losses:  8.04017162322998 1.6720815896987915
CurrentTrain: epoch  3, batch    24 | loss: 8.0401716Losses:  8.845599174499512 2.130293846130371
CurrentTrain: epoch  3, batch    25 | loss: 8.8455992Losses:  9.052338600158691 2.1405491828918457
CurrentTrain: epoch  3, batch    26 | loss: 9.0523386Losses:  8.700323104858398 1.9761104583740234
CurrentTrain: epoch  3, batch    27 | loss: 8.7003231Losses:  9.1575927734375 2.167396306991577
CurrentTrain: epoch  3, batch    28 | loss: 9.1575928Losses:  8.993585586547852 1.8877215385437012
CurrentTrain: epoch  3, batch    29 | loss: 8.9935856Losses:  9.655328750610352 2.091339588165283
CurrentTrain: epoch  3, batch    30 | loss: 9.6553288Losses:  8.291857719421387 1.8951873779296875
CurrentTrain: epoch  3, batch    31 | loss: 8.2918577Losses:  8.219032287597656 1.67802095413208
CurrentTrain: epoch  3, batch    32 | loss: 8.2190323Losses:  8.512140274047852 1.8424912691116333
CurrentTrain: epoch  3, batch    33 | loss: 8.5121403Losses:  8.443891525268555 1.9145448207855225
CurrentTrain: epoch  3, batch    34 | loss: 8.4438915Losses:  8.592813491821289 1.9872848987579346
CurrentTrain: epoch  3, batch    35 | loss: 8.5928135Losses:  9.40396499633789 1.9975793361663818
CurrentTrain: epoch  3, batch    36 | loss: 9.4039650Losses:  8.65148639678955 1.9307337999343872
CurrentTrain: epoch  3, batch    37 | loss: 8.6514864Losses:  8.528413772583008 2.012665033340454
CurrentTrain: epoch  3, batch    38 | loss: 8.5284138Losses:  8.824567794799805 1.6848843097686768
CurrentTrain: epoch  3, batch    39 | loss: 8.8245678Losses:  8.747503280639648 2.047307252883911
CurrentTrain: epoch  3, batch    40 | loss: 8.7475033Losses:  7.96079158782959 1.7577711343765259
CurrentTrain: epoch  3, batch    41 | loss: 7.9607916Losses:  8.560262680053711 1.7506444454193115
CurrentTrain: epoch  3, batch    42 | loss: 8.5602627Losses:  8.401165962219238 1.8366367816925049
CurrentTrain: epoch  3, batch    43 | loss: 8.4011660Losses:  9.021324157714844 2.0494093894958496
CurrentTrain: epoch  3, batch    44 | loss: 9.0213242Losses:  8.53099250793457 1.8026196956634521
CurrentTrain: epoch  3, batch    45 | loss: 8.5309925Losses:  8.44225025177002 2.0130605697631836
CurrentTrain: epoch  3, batch    46 | loss: 8.4422503Losses:  8.447052001953125 1.756083369255066
CurrentTrain: epoch  3, batch    47 | loss: 8.4470520Losses:  7.741095066070557 1.5920774936676025
CurrentTrain: epoch  3, batch    48 | loss: 7.7410951Losses:  7.967398643493652 1.628127098083496
CurrentTrain: epoch  3, batch    49 | loss: 7.9673986Losses:  8.586341857910156 2.013963222503662
CurrentTrain: epoch  3, batch    50 | loss: 8.5863419Losses:  8.38829231262207 1.7794984579086304
CurrentTrain: epoch  3, batch    51 | loss: 8.3882923Losses:  8.302882194519043 1.8835976123809814
CurrentTrain: epoch  3, batch    52 | loss: 8.3028822Losses:  9.044225692749023 1.9556859731674194
CurrentTrain: epoch  3, batch    53 | loss: 9.0442257Losses:  8.410049438476562 1.9580609798431396
CurrentTrain: epoch  3, batch    54 | loss: 8.4100494Losses:  8.885771751403809 2.1325149536132812
CurrentTrain: epoch  3, batch    55 | loss: 8.8857718Losses:  8.17216682434082 1.8621549606323242
CurrentTrain: epoch  3, batch    56 | loss: 8.1721668Losses:  8.602787017822266 1.8798092603683472
CurrentTrain: epoch  3, batch    57 | loss: 8.6027870Losses:  8.693120956420898 1.8568074703216553
CurrentTrain: epoch  3, batch    58 | loss: 8.6931210Losses:  7.886350154876709 1.6578805446624756
CurrentTrain: epoch  3, batch    59 | loss: 7.8863502Losses:  8.671625137329102 2.0700130462646484
CurrentTrain: epoch  3, batch    60 | loss: 8.6716251Losses:  8.30481243133545 1.9398689270019531
CurrentTrain: epoch  3, batch    61 | loss: 8.3048124Losses:  7.377857208251953 1.4740082025527954
CurrentTrain: epoch  3, batch    62 | loss: 7.3778572Losses:  7.469751358032227 1.543897271156311
CurrentTrain: epoch  4, batch     0 | loss: 7.4697514Losses:  8.026288986206055 1.7718714475631714
CurrentTrain: epoch  4, batch     1 | loss: 8.0262890Losses:  8.36121940612793 1.9142338037490845
CurrentTrain: epoch  4, batch     2 | loss: 8.3612194Losses:  7.826651573181152 1.7247743606567383
CurrentTrain: epoch  4, batch     3 | loss: 7.8266516Losses:  8.803766250610352 2.1361284255981445
CurrentTrain: epoch  4, batch     4 | loss: 8.8037663Losses:  8.894861221313477 2.1324315071105957
CurrentTrain: epoch  4, batch     5 | loss: 8.8948612Losses:  8.18510627746582 1.850816011428833
CurrentTrain: epoch  4, batch     6 | loss: 8.1851063Losses:  7.833508491516113 1.7091646194458008
CurrentTrain: epoch  4, batch     7 | loss: 7.8335085Losses:  8.65650749206543 2.119561195373535
CurrentTrain: epoch  4, batch     8 | loss: 8.6565075Losses:  8.199362754821777 1.8843061923980713
CurrentTrain: epoch  4, batch     9 | loss: 8.1993628Losses:  8.181245803833008 1.846603512763977
CurrentTrain: epoch  4, batch    10 | loss: 8.1812458Losses:  8.677152633666992 1.8833335638046265
CurrentTrain: epoch  4, batch    11 | loss: 8.6771526Losses:  8.282144546508789 1.8790606260299683
CurrentTrain: epoch  4, batch    12 | loss: 8.2821445Losses:  8.684531211853027 2.003640651702881
CurrentTrain: epoch  4, batch    13 | loss: 8.6845312Losses:  7.63398551940918 1.7014926671981812
CurrentTrain: epoch  4, batch    14 | loss: 7.6339855Losses:  8.029516220092773 1.8011951446533203
CurrentTrain: epoch  4, batch    15 | loss: 8.0295162Losses:  8.120349884033203 1.8395041227340698
CurrentTrain: epoch  4, batch    16 | loss: 8.1203499Losses:  7.982779502868652 1.7966324090957642
CurrentTrain: epoch  4, batch    17 | loss: 7.9827795Losses:  8.39017105102539 1.9959495067596436
CurrentTrain: epoch  4, batch    18 | loss: 8.3901711Losses:  8.350927352905273 1.968962550163269
CurrentTrain: epoch  4, batch    19 | loss: 8.3509274Losses:  8.370121955871582 1.7680084705352783
CurrentTrain: epoch  4, batch    20 | loss: 8.3701220Losses:  7.965533256530762 1.6518139839172363
CurrentTrain: epoch  4, batch    21 | loss: 7.9655333Losses:  7.850656509399414 1.7967915534973145
CurrentTrain: epoch  4, batch    22 | loss: 7.8506565Losses:  7.328882217407227 1.5397117137908936
CurrentTrain: epoch  4, batch    23 | loss: 7.3288822Losses:  7.666860580444336 1.5653860569000244
CurrentTrain: epoch  4, batch    24 | loss: 7.6668606Losses:  8.48778247833252 2.0391645431518555
CurrentTrain: epoch  4, batch    25 | loss: 8.4877825Losses:  8.11176872253418 1.9161055088043213
CurrentTrain: epoch  4, batch    26 | loss: 8.1117687Losses:  7.992480278015137 1.8235799074172974
CurrentTrain: epoch  4, batch    27 | loss: 7.9924803Losses:  8.107474327087402 1.8152039051055908
CurrentTrain: epoch  4, batch    28 | loss: 8.1074743Losses:  8.02403450012207 1.8122687339782715
CurrentTrain: epoch  4, batch    29 | loss: 8.0240345Losses:  8.020803451538086 1.8775945901870728
CurrentTrain: epoch  4, batch    30 | loss: 8.0208035Losses:  8.089033126831055 1.8338963985443115
CurrentTrain: epoch  4, batch    31 | loss: 8.0890331Losses:  8.262007713317871 1.904849886894226
CurrentTrain: epoch  4, batch    32 | loss: 8.2620077Losses:  8.45760440826416 2.0017781257629395
CurrentTrain: epoch  4, batch    33 | loss: 8.4576044Losses:  8.091544151306152 1.8724571466445923
CurrentTrain: epoch  4, batch    34 | loss: 8.0915442Losses:  8.648193359375 2.0432276725769043
CurrentTrain: epoch  4, batch    35 | loss: 8.6481934Losses:  7.86221981048584 1.7163556814193726
CurrentTrain: epoch  4, batch    36 | loss: 7.8622198Losses:  8.76940631866455 2.002617359161377
CurrentTrain: epoch  4, batch    37 | loss: 8.7694063Losses:  8.134058952331543 1.8394184112548828
CurrentTrain: epoch  4, batch    38 | loss: 8.1340590Losses:  8.071260452270508 1.8812882900238037
CurrentTrain: epoch  4, batch    39 | loss: 8.0712605Losses:  7.902013778686523 1.8164681196212769
CurrentTrain: epoch  4, batch    40 | loss: 7.9020138Losses:  8.132827758789062 1.935495376586914
CurrentTrain: epoch  4, batch    41 | loss: 8.1328278Losses:  8.430822372436523 1.8764128684997559
CurrentTrain: epoch  4, batch    42 | loss: 8.4308224Losses:  8.00313949584961 1.5976731777191162
CurrentTrain: epoch  4, batch    43 | loss: 8.0031395Losses:  7.507382392883301 1.6617745161056519
CurrentTrain: epoch  4, batch    44 | loss: 7.5073824Losses:  8.356066703796387 1.7068369388580322
CurrentTrain: epoch  4, batch    45 | loss: 8.3560667Losses:  8.285426139831543 1.8896539211273193
CurrentTrain: epoch  4, batch    46 | loss: 8.2854261Losses:  8.194398880004883 1.8957631587982178
CurrentTrain: epoch  4, batch    47 | loss: 8.1943989Losses:  8.359916687011719 2.022129774093628
CurrentTrain: epoch  4, batch    48 | loss: 8.3599167Losses:  8.18175983428955 1.927255630493164
CurrentTrain: epoch  4, batch    49 | loss: 8.1817598Losses:  8.628856658935547 2.112811803817749
CurrentTrain: epoch  4, batch    50 | loss: 8.6288567Losses:  7.950878143310547 1.7964929342269897
CurrentTrain: epoch  4, batch    51 | loss: 7.9508781Losses:  7.652780055999756 1.684075117111206
CurrentTrain: epoch  4, batch    52 | loss: 7.6527801Losses:  8.4302339553833 1.8698878288269043
CurrentTrain: epoch  4, batch    53 | loss: 8.4302340Losses:  7.895198345184326 1.7538635730743408
CurrentTrain: epoch  4, batch    54 | loss: 7.8951983Losses:  8.152628898620605 1.9313963651657104
CurrentTrain: epoch  4, batch    55 | loss: 8.1526289Losses:  7.805428981781006 1.7382547855377197
CurrentTrain: epoch  4, batch    56 | loss: 7.8054290Losses:  8.581216812133789 2.11440372467041
CurrentTrain: epoch  4, batch    57 | loss: 8.5812168Losses:  8.054844856262207 1.8615449666976929
CurrentTrain: epoch  4, batch    58 | loss: 8.0548449Losses:  8.031388282775879 1.8671574592590332
CurrentTrain: epoch  4, batch    59 | loss: 8.0313883Losses:  8.003774642944336 1.8547096252441406
CurrentTrain: epoch  4, batch    60 | loss: 8.0037746Losses:  8.250471115112305 1.932570457458496
CurrentTrain: epoch  4, batch    61 | loss: 8.2504711Losses:  7.306697368621826 1.4892799854278564
CurrentTrain: epoch  4, batch    62 | loss: 7.3066974Losses:  8.589536666870117 2.0784900188446045
CurrentTrain: epoch  5, batch     0 | loss: 8.5895367Losses:  8.250736236572266 1.9725042581558228
CurrentTrain: epoch  5, batch     1 | loss: 8.2507362Losses:  7.725954055786133 1.7179237604141235
CurrentTrain: epoch  5, batch     2 | loss: 7.7259541Losses:  8.01600456237793 1.8598768711090088
CurrentTrain: epoch  5, batch     3 | loss: 8.0160046Losses:  8.016040802001953 1.813382863998413
CurrentTrain: epoch  5, batch     4 | loss: 8.0160408Losses:  8.060632705688477 1.849476933479309
CurrentTrain: epoch  5, batch     5 | loss: 8.0606327Losses:  8.233177185058594 1.9458322525024414
CurrentTrain: epoch  5, batch     6 | loss: 8.2331772Losses:  8.182174682617188 1.9716517925262451
CurrentTrain: epoch  5, batch     7 | loss: 8.1821747Losses:  8.255460739135742 1.9530874490737915
CurrentTrain: epoch  5, batch     8 | loss: 8.2554607Losses:  7.588827133178711 1.6754997968673706
CurrentTrain: epoch  5, batch     9 | loss: 7.5888271Losses:  8.591304779052734 2.0088140964508057
CurrentTrain: epoch  5, batch    10 | loss: 8.5913048Losses:  8.337932586669922 2.028653383255005
CurrentTrain: epoch  5, batch    11 | loss: 8.3379326Losses:  7.970306396484375 1.883733868598938
CurrentTrain: epoch  5, batch    12 | loss: 7.9703064Losses:  7.841285228729248 1.8078062534332275
CurrentTrain: epoch  5, batch    13 | loss: 7.8412852Losses:  8.425848960876465 2.1294946670532227
CurrentTrain: epoch  5, batch    14 | loss: 8.4258490Losses:  7.952744007110596 1.860868215560913
CurrentTrain: epoch  5, batch    15 | loss: 7.9527440Losses:  8.45566177368164 2.0735208988189697
CurrentTrain: epoch  5, batch    16 | loss: 8.4556618Losses:  8.225412368774414 2.016531229019165
CurrentTrain: epoch  5, batch    17 | loss: 8.2254124Losses:  8.153939247131348 1.9454654455184937
CurrentTrain: epoch  5, batch    18 | loss: 8.1539392Losses:  7.986449241638184 1.8749414682388306
CurrentTrain: epoch  5, batch    19 | loss: 7.9864492Losses:  7.959043502807617 1.8887752294540405
CurrentTrain: epoch  5, batch    20 | loss: 7.9590435Losses:  7.966202259063721 1.846520185470581
CurrentTrain: epoch  5, batch    21 | loss: 7.9662023Losses:  7.813528537750244 1.7670807838439941
CurrentTrain: epoch  5, batch    22 | loss: 7.8135285Losses:  8.021445274353027 1.8581491708755493
CurrentTrain: epoch  5, batch    23 | loss: 8.0214453Losses:  7.772137641906738 1.7679896354675293
CurrentTrain: epoch  5, batch    24 | loss: 7.7721376Losses:  8.166152954101562 1.9793035984039307
CurrentTrain: epoch  5, batch    25 | loss: 8.1661530Losses:  8.16693115234375 1.9470453262329102
CurrentTrain: epoch  5, batch    26 | loss: 8.1669312Losses:  7.876334190368652 1.8228570222854614
CurrentTrain: epoch  5, batch    27 | loss: 7.8763342Losses:  8.212501525878906 2.0154757499694824
CurrentTrain: epoch  5, batch    28 | loss: 8.2125015Losses:  8.120882034301758 1.9716136455535889
CurrentTrain: epoch  5, batch    29 | loss: 8.1208820Losses:  7.616231918334961 1.72506582736969
CurrentTrain: epoch  5, batch    30 | loss: 7.6162319Losses:  7.778899669647217 1.8028833866119385
CurrentTrain: epoch  5, batch    31 | loss: 7.7788997Losses:  7.859322547912598 1.8019418716430664
CurrentTrain: epoch  5, batch    32 | loss: 7.8593225Losses:  7.303697109222412 1.5923593044281006
CurrentTrain: epoch  5, batch    33 | loss: 7.3036971Losses:  7.913154602050781 1.8395966291427612
CurrentTrain: epoch  5, batch    34 | loss: 7.9131546Losses:  7.561273097991943 1.712378740310669
CurrentTrain: epoch  5, batch    35 | loss: 7.5612731Losses:  7.62266206741333 1.7172057628631592
CurrentTrain: epoch  5, batch    36 | loss: 7.6226621Losses:  7.720986366271973 1.7940195798873901
CurrentTrain: epoch  5, batch    37 | loss: 7.7209864Losses:  7.800017356872559 1.7986429929733276
CurrentTrain: epoch  5, batch    38 | loss: 7.8000174Losses:  8.180856704711914 1.957639217376709
CurrentTrain: epoch  5, batch    39 | loss: 8.1808567Losses:  8.287434577941895 2.082520008087158
CurrentTrain: epoch  5, batch    40 | loss: 8.2874346Losses:  7.824103355407715 1.816422462463379
CurrentTrain: epoch  5, batch    41 | loss: 7.8241034Losses:  8.314579963684082 1.9887949228286743
CurrentTrain: epoch  5, batch    42 | loss: 8.3145800Losses:  8.009066581726074 1.7939740419387817
CurrentTrain: epoch  5, batch    43 | loss: 8.0090666Losses:  7.909056663513184 1.8730846643447876
CurrentTrain: epoch  5, batch    44 | loss: 7.9090567Losses:  7.834597110748291 1.853057622909546
CurrentTrain: epoch  5, batch    45 | loss: 7.8345971Losses:  8.014070510864258 1.9263468980789185
CurrentTrain: epoch  5, batch    46 | loss: 8.0140705Losses:  8.03477668762207 1.9381362199783325
CurrentTrain: epoch  5, batch    47 | loss: 8.0347767Losses:  8.143648147583008 1.9784833192825317
CurrentTrain: epoch  5, batch    48 | loss: 8.1436481Losses:  8.068204879760742 1.9485313892364502
CurrentTrain: epoch  5, batch    49 | loss: 8.0682049Losses:  7.958683013916016 1.9220776557922363
CurrentTrain: epoch  5, batch    50 | loss: 7.9586830Losses:  7.619305610656738 1.7423615455627441
CurrentTrain: epoch  5, batch    51 | loss: 7.6193056Losses:  8.297887802124023 2.0484538078308105
CurrentTrain: epoch  5, batch    52 | loss: 8.2978878Losses:  7.944927215576172 1.879876732826233
CurrentTrain: epoch  5, batch    53 | loss: 7.9449272Losses:  8.228888511657715 2.0521111488342285
CurrentTrain: epoch  5, batch    54 | loss: 8.2288885Losses:  8.1387939453125 1.9420926570892334
CurrentTrain: epoch  5, batch    55 | loss: 8.1387939Losses:  8.26553726196289 2.0502848625183105
CurrentTrain: epoch  5, batch    56 | loss: 8.2655373Losses:  7.7152204513549805 1.7324585914611816
CurrentTrain: epoch  5, batch    57 | loss: 7.7152205Losses:  7.951721668243408 1.790565013885498
CurrentTrain: epoch  5, batch    58 | loss: 7.9517217Losses:  7.693165302276611 1.732654333114624
CurrentTrain: epoch  5, batch    59 | loss: 7.6931653Losses:  7.784187316894531 1.8299399614334106
CurrentTrain: epoch  5, batch    60 | loss: 7.7841873Losses:  7.672671794891357 1.7729780673980713
CurrentTrain: epoch  5, batch    61 | loss: 7.6726718Losses:  7.4816575050354 1.62953519821167
CurrentTrain: epoch  5, batch    62 | loss: 7.4816575Losses:  7.888487815856934 1.8387895822525024
CurrentTrain: epoch  6, batch     0 | loss: 7.8884878Losses:  7.971824645996094 1.930521011352539
CurrentTrain: epoch  6, batch     1 | loss: 7.9718246Losses:  7.6419677734375 1.7835999727249146
CurrentTrain: epoch  6, batch     2 | loss: 7.6419678Losses:  8.535079956054688 2.114126443862915
CurrentTrain: epoch  6, batch     3 | loss: 8.5350800Losses:  7.460193634033203 1.6549409627914429
CurrentTrain: epoch  6, batch     4 | loss: 7.4601936Losses:  8.088338851928711 1.9702162742614746
CurrentTrain: epoch  6, batch     5 | loss: 8.0883389Losses:  8.099505424499512 1.9690234661102295
CurrentTrain: epoch  6, batch     6 | loss: 8.0995054Losses:  7.8188371658325195 1.8289685249328613
CurrentTrain: epoch  6, batch     7 | loss: 7.8188372Losses:  8.081008911132812 1.9870026111602783
CurrentTrain: epoch  6, batch     8 | loss: 8.0810089Losses:  7.315254211425781 1.5973639488220215
CurrentTrain: epoch  6, batch     9 | loss: 7.3152542Losses:  8.049036026000977 1.9557442665100098
CurrentTrain: epoch  6, batch    10 | loss: 8.0490360Losses:  7.452977657318115 1.6453585624694824
CurrentTrain: epoch  6, batch    11 | loss: 7.4529777Losses:  8.300703048706055 2.056011199951172
CurrentTrain: epoch  6, batch    12 | loss: 8.3007030Losses:  7.57359504699707 1.7242636680603027
CurrentTrain: epoch  6, batch    13 | loss: 7.5735950Losses:  8.199491500854492 2.0247201919555664
CurrentTrain: epoch  6, batch    14 | loss: 8.1994915Losses:  8.004877090454102 1.9383097887039185
CurrentTrain: epoch  6, batch    15 | loss: 8.0048771Losses:  8.100079536437988 1.9799408912658691
CurrentTrain: epoch  6, batch    16 | loss: 8.1000795Losses:  8.057191848754883 1.9674317836761475
CurrentTrain: epoch  6, batch    17 | loss: 8.0571918Losses:  6.7869672775268555 1.321852684020996
CurrentTrain: epoch  6, batch    18 | loss: 6.7869673Losses:  8.05747127532959 1.9721571207046509
CurrentTrain: epoch  6, batch    19 | loss: 8.0574713Losses:  8.13966178894043 2.012951612472534
CurrentTrain: epoch  6, batch    20 | loss: 8.1396618Losses:  7.877684116363525 1.847355604171753
CurrentTrain: epoch  6, batch    21 | loss: 7.8776841Losses:  7.6199564933776855 1.7170708179473877
CurrentTrain: epoch  6, batch    22 | loss: 7.6199565Losses:  7.39150333404541 1.5314171314239502
CurrentTrain: epoch  6, batch    23 | loss: 7.3915033Losses:  7.672489643096924 1.7532360553741455
CurrentTrain: epoch  6, batch    24 | loss: 7.6724896Losses:  8.142842292785645 1.9767369031906128
CurrentTrain: epoch  6, batch    25 | loss: 8.1428423Losses:  8.077704429626465 1.9610590934753418
CurrentTrain: epoch  6, batch    26 | loss: 8.0777044Losses:  7.604248046875 1.7557505369186401
CurrentTrain: epoch  6, batch    27 | loss: 7.6042480Losses:  7.885611057281494 1.908992052078247
CurrentTrain: epoch  6, batch    28 | loss: 7.8856111Losses:  8.026481628417969 1.947587013244629
CurrentTrain: epoch  6, batch    29 | loss: 8.0264816Losses:  7.992737770080566 1.951641321182251
CurrentTrain: epoch  6, batch    30 | loss: 7.9927378Losses:  7.165502548217773 1.5667157173156738
CurrentTrain: epoch  6, batch    31 | loss: 7.1655025Losses:  7.658438205718994 1.7521545886993408
CurrentTrain: epoch  6, batch    32 | loss: 7.6584382Losses:  8.065155982971191 1.9631662368774414
CurrentTrain: epoch  6, batch    33 | loss: 8.0651560Losses:  7.735455513000488 1.7986420392990112
CurrentTrain: epoch  6, batch    34 | loss: 7.7354555Losses:  7.326993942260742 1.5894485712051392
CurrentTrain: epoch  6, batch    35 | loss: 7.3269939Losses:  7.090028285980225 1.4542744159698486
CurrentTrain: epoch  6, batch    36 | loss: 7.0900283Losses:  8.055319786071777 1.964362621307373
CurrentTrain: epoch  6, batch    37 | loss: 8.0553198Losses:  8.234986305236816 2.052499294281006
CurrentTrain: epoch  6, batch    38 | loss: 8.2349863Losses:  7.563404560089111 1.7465405464172363
CurrentTrain: epoch  6, batch    39 | loss: 7.5634046Losses:  8.152990341186523 2.0021424293518066
CurrentTrain: epoch  6, batch    40 | loss: 8.1529903Losses:  8.2381591796875 2.068392038345337
CurrentTrain: epoch  6, batch    41 | loss: 8.2381592Losses:  8.040284156799316 1.9802922010421753
CurrentTrain: epoch  6, batch    42 | loss: 8.0402842Losses:  7.517393589019775 1.7391870021820068
CurrentTrain: epoch  6, batch    43 | loss: 7.5173936Losses:  7.721879005432129 1.8204044103622437
CurrentTrain: epoch  6, batch    44 | loss: 7.7218790Losses:  7.958547592163086 1.9166131019592285
CurrentTrain: epoch  6, batch    45 | loss: 7.9585476Losses:  7.627926826477051 1.767606258392334
CurrentTrain: epoch  6, batch    46 | loss: 7.6279268Losses:  7.551825046539307 1.723625898361206
CurrentTrain: epoch  6, batch    47 | loss: 7.5518250Losses:  8.248115539550781 2.079951763153076
CurrentTrain: epoch  6, batch    48 | loss: 8.2481155Losses:  7.402202606201172 1.6546454429626465
CurrentTrain: epoch  6, batch    49 | loss: 7.4022026Losses:  8.164274215698242 2.027137517929077
CurrentTrain: epoch  6, batch    50 | loss: 8.1642742Losses:  8.036669731140137 1.9868971109390259
CurrentTrain: epoch  6, batch    51 | loss: 8.0366697Losses:  8.011974334716797 1.9634149074554443
CurrentTrain: epoch  6, batch    52 | loss: 8.0119743Losses:  6.861875057220459 1.3797941207885742
CurrentTrain: epoch  6, batch    53 | loss: 6.8618751Losses:  7.877560615539551 1.9013503789901733
CurrentTrain: epoch  6, batch    54 | loss: 7.8775606Losses:  7.5171074867248535 1.7328987121582031
CurrentTrain: epoch  6, batch    55 | loss: 7.5171075Losses:  7.310617923736572 1.5885088443756104
CurrentTrain: epoch  6, batch    56 | loss: 7.3106179Losses:  8.020099639892578 1.98374605178833
CurrentTrain: epoch  6, batch    57 | loss: 8.0200996Losses:  7.802275657653809 1.8659319877624512
CurrentTrain: epoch  6, batch    58 | loss: 7.8022757Losses:  7.927606105804443 1.928426742553711
CurrentTrain: epoch  6, batch    59 | loss: 7.9276061Losses:  7.279611110687256 1.5788171291351318
CurrentTrain: epoch  6, batch    60 | loss: 7.2796111Losses:  7.74089241027832 1.8106924295425415
CurrentTrain: epoch  6, batch    61 | loss: 7.7408924Losses:  7.110260009765625 1.5289087295532227
CurrentTrain: epoch  6, batch    62 | loss: 7.1102600Losses:  7.561359882354736 1.7313883304595947
CurrentTrain: epoch  7, batch     0 | loss: 7.5613599Losses:  7.4628190994262695 1.6854618787765503
CurrentTrain: epoch  7, batch     1 | loss: 7.4628191Losses:  7.6940693855285645 1.8087451457977295
CurrentTrain: epoch  7, batch     2 | loss: 7.6940694Losses:  8.077387809753418 1.999576210975647
CurrentTrain: epoch  7, batch     3 | loss: 8.0773878Losses:  7.386113166809082 1.652278184890747
CurrentTrain: epoch  7, batch     4 | loss: 7.3861132Losses:  7.924277305603027 1.9377360343933105
CurrentTrain: epoch  7, batch     5 | loss: 7.9242773Losses:  7.906311511993408 1.928002119064331
CurrentTrain: epoch  7, batch     6 | loss: 7.9063115Losses:  8.056319236755371 1.9582880735397339
CurrentTrain: epoch  7, batch     7 | loss: 8.0563192Losses:  7.357194900512695 1.6732488870620728
CurrentTrain: epoch  7, batch     8 | loss: 7.3571949Losses:  7.547280311584473 1.7549697160720825
CurrentTrain: epoch  7, batch     9 | loss: 7.5472803Losses:  7.713373184204102 1.835038423538208
CurrentTrain: epoch  7, batch    10 | loss: 7.7133732Losses:  7.539790153503418 1.7615275382995605
CurrentTrain: epoch  7, batch    11 | loss: 7.5397902Losses:  7.188241004943848 1.5779266357421875
CurrentTrain: epoch  7, batch    12 | loss: 7.1882410Losses:  7.541355609893799 1.7374956607818604
CurrentTrain: epoch  7, batch    13 | loss: 7.5413556Losses:  7.4832611083984375 1.7296910285949707
CurrentTrain: epoch  7, batch    14 | loss: 7.4832611Losses:  7.623344421386719 1.8120770454406738
CurrentTrain: epoch  7, batch    15 | loss: 7.6233444Losses:  7.671087741851807 1.789475917816162
CurrentTrain: epoch  7, batch    16 | loss: 7.6710877Losses:  7.917524337768555 1.8060952425003052
CurrentTrain: epoch  7, batch    17 | loss: 7.9175243Losses:  8.032209396362305 1.9898619651794434
CurrentTrain: epoch  7, batch    18 | loss: 8.0322094Losses:  7.965461730957031 1.9302796125411987
CurrentTrain: epoch  7, batch    19 | loss: 7.9654617Losses:  7.511340141296387 1.7119402885437012
CurrentTrain: epoch  7, batch    20 | loss: 7.5113401Losses:  7.497334957122803 1.7402474880218506
CurrentTrain: epoch  7, batch    21 | loss: 7.4973350Losses:  7.342019557952881 1.6443521976470947
CurrentTrain: epoch  7, batch    22 | loss: 7.3420196Losses:  7.246327877044678 1.5863640308380127
CurrentTrain: epoch  7, batch    23 | loss: 7.2463279Losses:  7.621086597442627 1.7909367084503174
CurrentTrain: epoch  7, batch    24 | loss: 7.6210866Losses:  7.315319061279297 1.5752116441726685
CurrentTrain: epoch  7, batch    25 | loss: 7.3153191Losses:  7.50452184677124 1.7355387210845947
CurrentTrain: epoch  7, batch    26 | loss: 7.5045218Losses:  7.847799301147461 1.8978794813156128
CurrentTrain: epoch  7, batch    27 | loss: 7.8477993Losses:  7.6942458152771 1.8108396530151367
CurrentTrain: epoch  7, batch    28 | loss: 7.6942458Losses:  7.493342876434326 1.7116799354553223
CurrentTrain: epoch  7, batch    29 | loss: 7.4933429Losses:  7.798111438751221 1.853222131729126
CurrentTrain: epoch  7, batch    30 | loss: 7.7981114Losses:  7.705212593078613 1.8187979459762573
CurrentTrain: epoch  7, batch    31 | loss: 7.7052126Losses:  7.197192668914795 1.563889741897583
CurrentTrain: epoch  7, batch    32 | loss: 7.1971927Losses:  7.564374923706055 1.701846957206726
CurrentTrain: epoch  7, batch    33 | loss: 7.5643749Losses:  7.312732219696045 1.6068737506866455
CurrentTrain: epoch  7, batch    34 | loss: 7.3127322Losses:  7.823059558868408 1.880378246307373
CurrentTrain: epoch  7, batch    35 | loss: 7.8230596Losses:  7.685516357421875 1.7817801237106323
CurrentTrain: epoch  7, batch    36 | loss: 7.6855164Losses:  7.48541784286499 1.7276732921600342
CurrentTrain: epoch  7, batch    37 | loss: 7.4854178Losses:  7.9825825691223145 1.963054895401001
CurrentTrain: epoch  7, batch    38 | loss: 7.9825826Losses:  7.502732276916504 1.739198088645935
CurrentTrain: epoch  7, batch    39 | loss: 7.5027323Losses:  7.877055644989014 1.914376974105835
CurrentTrain: epoch  7, batch    40 | loss: 7.8770556Losses:  7.4108123779296875 1.6965374946594238
CurrentTrain: epoch  7, batch    41 | loss: 7.4108124Losses:  7.611295700073242 1.7889509201049805
CurrentTrain: epoch  7, batch    42 | loss: 7.6112957Losses:  7.593459129333496 1.7692227363586426
CurrentTrain: epoch  7, batch    43 | loss: 7.5934591Losses:  7.788681983947754 1.8721336126327515
CurrentTrain: epoch  7, batch    44 | loss: 7.7886820Losses:  7.844850063323975 1.9250497817993164
CurrentTrain: epoch  7, batch    45 | loss: 7.8448501Losses:  7.51815128326416 1.7333729267120361
CurrentTrain: epoch  7, batch    46 | loss: 7.5181513Losses:  7.764632701873779 1.8633787631988525
CurrentTrain: epoch  7, batch    47 | loss: 7.7646327Losses:  7.776274681091309 1.8728505373001099
CurrentTrain: epoch  7, batch    48 | loss: 7.7762747Losses:  7.818699359893799 1.902961015701294
CurrentTrain: epoch  7, batch    49 | loss: 7.8186994Losses:  7.288734436035156 1.658752202987671
CurrentTrain: epoch  7, batch    50 | loss: 7.2887344Losses:  7.905083656311035 1.9342985153198242
CurrentTrain: epoch  7, batch    51 | loss: 7.9050837Losses:  7.219602108001709 1.5569946765899658
CurrentTrain: epoch  7, batch    52 | loss: 7.2196021Losses:  7.771728515625 1.8485110998153687
CurrentTrain: epoch  7, batch    53 | loss: 7.7717285Losses:  7.582510471343994 1.7543504238128662
CurrentTrain: epoch  7, batch    54 | loss: 7.5825105Losses:  7.400107383728027 1.680819034576416
CurrentTrain: epoch  7, batch    55 | loss: 7.4001074Losses:  7.154548645019531 1.5545392036437988
CurrentTrain: epoch  7, batch    56 | loss: 7.1545486Losses:  7.88411283493042 1.934018850326538
CurrentTrain: epoch  7, batch    57 | loss: 7.8841128Losses:  7.926300525665283 1.9514343738555908
CurrentTrain: epoch  7, batch    58 | loss: 7.9263005Losses:  7.240696907043457 1.5902800559997559
CurrentTrain: epoch  7, batch    59 | loss: 7.2406969Losses:  7.994582176208496 1.9666285514831543
CurrentTrain: epoch  7, batch    60 | loss: 7.9945822Losses:  7.638245582580566 1.7995036840438843
CurrentTrain: epoch  7, batch    61 | loss: 7.6382456Losses:  7.174900054931641 1.5510661602020264
CurrentTrain: epoch  7, batch    62 | loss: 7.1749001Losses:  8.038400650024414 2.006413459777832
CurrentTrain: epoch  8, batch     0 | loss: 8.0384007Losses:  7.717225074768066 1.8242733478546143
CurrentTrain: epoch  8, batch     1 | loss: 7.7172251Losses:  7.710866928100586 1.8323206901550293
CurrentTrain: epoch  8, batch     2 | loss: 7.7108669Losses:  7.408604621887207 1.674210786819458
CurrentTrain: epoch  8, batch     3 | loss: 7.4086046Losses:  7.376961708068848 1.6909074783325195
CurrentTrain: epoch  8, batch     4 | loss: 7.3769617Losses:  7.712235450744629 1.8376259803771973
CurrentTrain: epoch  8, batch     5 | loss: 7.7122355Losses:  7.812985420227051 1.8861579895019531
CurrentTrain: epoch  8, batch     6 | loss: 7.8129854Losses:  7.825782775878906 1.88076651096344
CurrentTrain: epoch  8, batch     7 | loss: 7.8257828Losses:  7.734518527984619 1.849177360534668
CurrentTrain: epoch  8, batch     8 | loss: 7.7345185Losses:  7.855011940002441 1.9064213037490845
CurrentTrain: epoch  8, batch     9 | loss: 7.8550119Losses:  7.665619850158691 1.8304706811904907
CurrentTrain: epoch  8, batch    10 | loss: 7.6656199Losses:  7.572174549102783 1.7762744426727295
CurrentTrain: epoch  8, batch    11 | loss: 7.5721745Losses:  7.751506805419922 1.8479652404785156
CurrentTrain: epoch  8, batch    12 | loss: 7.7515068Losses:  7.560140609741211 1.761802315711975
CurrentTrain: epoch  8, batch    13 | loss: 7.5601406Losses:  7.56560754776001 1.7767159938812256
CurrentTrain: epoch  8, batch    14 | loss: 7.5656075Losses:  7.430145263671875 1.701006531715393
CurrentTrain: epoch  8, batch    15 | loss: 7.4301453Losses:  7.596013069152832 1.8031489849090576
CurrentTrain: epoch  8, batch    16 | loss: 7.5960131Losses:  7.8477654457092285 1.9078869819641113
CurrentTrain: epoch  8, batch    17 | loss: 7.8477654Losses:  7.735261917114258 1.843381404876709
CurrentTrain: epoch  8, batch    18 | loss: 7.7352619Losses:  7.845046043395996 1.8775815963745117
CurrentTrain: epoch  8, batch    19 | loss: 7.8450460Losses:  7.368458271026611 1.710808277130127
CurrentTrain: epoch  8, batch    20 | loss: 7.3684583Losses:  7.654529094696045 1.8205981254577637
CurrentTrain: epoch  8, batch    21 | loss: 7.6545291Losses:  7.0529584884643555 1.5183687210083008
CurrentTrain: epoch  8, batch    22 | loss: 7.0529585Losses:  8.120609283447266 2.042107582092285
CurrentTrain: epoch  8, batch    23 | loss: 8.1206093Losses:  8.165058135986328 2.0583150386810303
CurrentTrain: epoch  8, batch    24 | loss: 8.1650581Losses:  7.206431865692139 1.5947034358978271
CurrentTrain: epoch  8, batch    25 | loss: 7.2064319Losses:  7.3831257820129395 1.6763861179351807
CurrentTrain: epoch  8, batch    26 | loss: 7.3831258Losses:  7.650258541107178 1.784508228302002
CurrentTrain: epoch  8, batch    27 | loss: 7.6502585Losses:  8.045759201049805 2.0135722160339355
CurrentTrain: epoch  8, batch    28 | loss: 8.0457592Losses:  7.7392683029174805 1.8510528802871704
CurrentTrain: epoch  8, batch    29 | loss: 7.7392683Losses:  7.55769157409668 1.7700814008712769
CurrentTrain: epoch  8, batch    30 | loss: 7.5576916Losses:  7.373558044433594 1.6781373023986816
CurrentTrain: epoch  8, batch    31 | loss: 7.3735580Losses:  7.232239723205566 1.5923212766647339
CurrentTrain: epoch  8, batch    32 | loss: 7.2322397Losses:  7.572679042816162 1.7543890476226807
CurrentTrain: epoch  8, batch    33 | loss: 7.5726790Losses:  8.151487350463867 2.05586838722229
CurrentTrain: epoch  8, batch    34 | loss: 8.1514874Losses:  7.603574752807617 1.790560245513916
CurrentTrain: epoch  8, batch    35 | loss: 7.6035748Losses:  7.466392517089844 1.712834358215332
CurrentTrain: epoch  8, batch    36 | loss: 7.4663925Losses:  7.5569000244140625 1.7431851625442505
CurrentTrain: epoch  8, batch    37 | loss: 7.5569000Losses:  7.3437018394470215 1.6786434650421143
CurrentTrain: epoch  8, batch    38 | loss: 7.3437018Losses:  7.738664627075195 1.8386726379394531
CurrentTrain: epoch  8, batch    39 | loss: 7.7386646Losses:  7.956377983093262 1.9332671165466309
CurrentTrain: epoch  8, batch    40 | loss: 7.9563780Losses:  7.930111885070801 1.9461865425109863
CurrentTrain: epoch  8, batch    41 | loss: 7.9301119Losses:  7.771121978759766 1.8622212409973145
CurrentTrain: epoch  8, batch    42 | loss: 7.7711220Losses:  7.8761162757873535 1.8862807750701904
CurrentTrain: epoch  8, batch    43 | loss: 7.8761163Losses:  7.866717338562012 1.911677360534668
CurrentTrain: epoch  8, batch    44 | loss: 7.8667173Losses:  7.395873546600342 1.6588664054870605
CurrentTrain: epoch  8, batch    45 | loss: 7.3958735Losses:  7.251773357391357 1.6304858922958374
CurrentTrain: epoch  8, batch    46 | loss: 7.2517734Losses:  7.072255611419678 1.5148942470550537
CurrentTrain: epoch  8, batch    47 | loss: 7.0722556Losses:  7.241532325744629 1.598038673400879
CurrentTrain: epoch  8, batch    48 | loss: 7.2415323Losses:  7.4468793869018555 1.7059621810913086
CurrentTrain: epoch  8, batch    49 | loss: 7.4468794Losses:  7.682530879974365 1.8331692218780518
CurrentTrain: epoch  8, batch    50 | loss: 7.6825309Losses:  7.019649505615234 1.4892487525939941
CurrentTrain: epoch  8, batch    51 | loss: 7.0196495Losses:  7.820907115936279 1.8943572044372559
CurrentTrain: epoch  8, batch    52 | loss: 7.8209071Losses:  7.641681671142578 1.8023945093154907
CurrentTrain: epoch  8, batch    53 | loss: 7.6416817Losses:  7.9035563468933105 1.946462631225586
CurrentTrain: epoch  8, batch    54 | loss: 7.9035563Losses:  7.27231502532959 1.6340986490249634
CurrentTrain: epoch  8, batch    55 | loss: 7.2723150Losses:  7.531045913696289 1.75129234790802
CurrentTrain: epoch  8, batch    56 | loss: 7.5310459Losses:  8.086685180664062 2.0338683128356934
CurrentTrain: epoch  8, batch    57 | loss: 8.0866852Losses:  7.509315490722656 1.7370681762695312
CurrentTrain: epoch  8, batch    58 | loss: 7.5093155Losses:  7.704167366027832 1.8382116556167603
CurrentTrain: epoch  8, batch    59 | loss: 7.7041674Losses:  7.4004669189453125 1.714064598083496
CurrentTrain: epoch  8, batch    60 | loss: 7.4004669Losses:  7.962860107421875 1.9652704000473022
CurrentTrain: epoch  8, batch    61 | loss: 7.9628601Losses:  6.509883403778076 1.2500426769256592
CurrentTrain: epoch  8, batch    62 | loss: 6.5098834Losses:  7.574257850646973 1.7814499139785767
CurrentTrain: epoch  9, batch     0 | loss: 7.5742579Losses:  7.396117687225342 1.6545498371124268
CurrentTrain: epoch  9, batch     1 | loss: 7.3961177Losses:  7.889619827270508 1.9219650030136108
CurrentTrain: epoch  9, batch     2 | loss: 7.8896198Losses:  7.509800434112549 1.7236342430114746
CurrentTrain: epoch  9, batch     3 | loss: 7.5098004Losses:  7.175389289855957 1.5747421979904175
CurrentTrain: epoch  9, batch     4 | loss: 7.1753893Losses:  7.545877456665039 1.772699236869812
CurrentTrain: epoch  9, batch     5 | loss: 7.5458775Losses:  7.499721527099609 1.7315012216567993
CurrentTrain: epoch  9, batch     6 | loss: 7.4997215Losses:  7.608293533325195 1.789844274520874
CurrentTrain: epoch  9, batch     7 | loss: 7.6082935Losses:  7.176827430725098 1.5957365036010742
CurrentTrain: epoch  9, batch     8 | loss: 7.1768274Losses:  7.635045051574707 1.810699701309204
CurrentTrain: epoch  9, batch     9 | loss: 7.6350451Losses:  7.892005920410156 1.942238450050354
CurrentTrain: epoch  9, batch    10 | loss: 7.8920059Losses:  7.873531818389893 1.9197299480438232
CurrentTrain: epoch  9, batch    11 | loss: 7.8735318Losses:  7.786485195159912 1.878873586654663
CurrentTrain: epoch  9, batch    12 | loss: 7.7864852Losses:  7.765915870666504 1.8718562126159668
CurrentTrain: epoch  9, batch    13 | loss: 7.7659159Losses:  8.106342315673828 2.0461816787719727
CurrentTrain: epoch  9, batch    14 | loss: 8.1063423Losses:  8.022873878479004 1.9894378185272217
CurrentTrain: epoch  9, batch    15 | loss: 8.0228739Losses:  7.721467971801758 1.856631875038147
CurrentTrain: epoch  9, batch    16 | loss: 7.7214680Losses:  7.764852523803711 1.858731746673584
CurrentTrain: epoch  9, batch    17 | loss: 7.7648525Losses:  7.696195602416992 1.8410145044326782
CurrentTrain: epoch  9, batch    18 | loss: 7.6961956Losses:  7.057364463806152 1.5438650846481323
CurrentTrain: epoch  9, batch    19 | loss: 7.0573645Losses:  7.623575210571289 1.7864290475845337
CurrentTrain: epoch  9, batch    20 | loss: 7.6235752Losses:  7.59599494934082 1.7922747135162354
CurrentTrain: epoch  9, batch    21 | loss: 7.5959949Losses:  7.75792932510376 1.8487505912780762
CurrentTrain: epoch  9, batch    22 | loss: 7.7579293Losses:  7.627842426300049 1.8125696182250977
CurrentTrain: epoch  9, batch    23 | loss: 7.6278424Losses:  7.472321033477783 1.7307002544403076
CurrentTrain: epoch  9, batch    24 | loss: 7.4723210Losses:  7.416962146759033 1.6952450275421143
CurrentTrain: epoch  9, batch    25 | loss: 7.4169621Losses:  7.603703498840332 1.8109831809997559
CurrentTrain: epoch  9, batch    26 | loss: 7.6037035Losses:  7.651140213012695 1.822826623916626
CurrentTrain: epoch  9, batch    27 | loss: 7.6511402Losses:  8.01795768737793 2.0086169242858887
CurrentTrain: epoch  9, batch    28 | loss: 8.0179577Losses:  7.416967391967773 1.703924298286438
CurrentTrain: epoch  9, batch    29 | loss: 7.4169674Losses:  7.622544765472412 1.7977898120880127
CurrentTrain: epoch  9, batch    30 | loss: 7.6225448Losses:  7.68255615234375 1.8429468870162964
CurrentTrain: epoch  9, batch    31 | loss: 7.6825562Losses:  7.3945159912109375 1.7064952850341797
CurrentTrain: epoch  9, batch    32 | loss: 7.3945160Losses:  7.102006435394287 1.5651109218597412
CurrentTrain: epoch  9, batch    33 | loss: 7.1020064Losses:  7.966364860534668 1.9764304161071777
CurrentTrain: epoch  9, batch    34 | loss: 7.9663649Losses:  8.004155158996582 2.0012588500976562
CurrentTrain: epoch  9, batch    35 | loss: 8.0041552Losses:  7.046359062194824 1.5220537185668945
CurrentTrain: epoch  9, batch    36 | loss: 7.0463591Losses:  7.821576118469238 1.9144281148910522
CurrentTrain: epoch  9, batch    37 | loss: 7.8215761Losses:  7.640183925628662 1.7966465950012207
CurrentTrain: epoch  9, batch    38 | loss: 7.6401839Losses:  7.797442436218262 1.8714033365249634
CurrentTrain: epoch  9, batch    39 | loss: 7.7974424Losses:  7.3612494468688965 1.6400694847106934
CurrentTrain: epoch  9, batch    40 | loss: 7.3612494Losses:  7.244115352630615 1.624922513961792
CurrentTrain: epoch  9, batch    41 | loss: 7.2441154Losses:  7.499416351318359 1.743452787399292
CurrentTrain: epoch  9, batch    42 | loss: 7.4994164Losses:  7.344991683959961 1.6767017841339111
CurrentTrain: epoch  9, batch    43 | loss: 7.3449917Losses:  7.368358135223389 1.6789534091949463
CurrentTrain: epoch  9, batch    44 | loss: 7.3683581Losses:  7.350151062011719 1.6838618516921997
CurrentTrain: epoch  9, batch    45 | loss: 7.3501511Losses:  7.356907844543457 1.6857582330703735
CurrentTrain: epoch  9, batch    46 | loss: 7.3569078Losses:  7.001450538635254 1.4854109287261963
CurrentTrain: epoch  9, batch    47 | loss: 7.0014505Losses:  7.80058479309082 1.8993794918060303
CurrentTrain: epoch  9, batch    48 | loss: 7.8005848Losses:  7.601003646850586 1.766275405883789
CurrentTrain: epoch  9, batch    49 | loss: 7.6010036Losses:  7.875394821166992 1.9219220876693726
CurrentTrain: epoch  9, batch    50 | loss: 7.8753948Losses:  7.896416664123535 1.9308099746704102
CurrentTrain: epoch  9, batch    51 | loss: 7.8964167Losses:  7.644381523132324 1.8158793449401855
CurrentTrain: epoch  9, batch    52 | loss: 7.6443815Losses:  7.580925464630127 1.7934856414794922
CurrentTrain: epoch  9, batch    53 | loss: 7.5809255Losses:  7.499459266662598 1.748439073562622
CurrentTrain: epoch  9, batch    54 | loss: 7.4994593Losses:  7.489236831665039 1.7501214742660522
CurrentTrain: epoch  9, batch    55 | loss: 7.4892368Losses:  7.828097343444824 1.9270552396774292
CurrentTrain: epoch  9, batch    56 | loss: 7.8280973Losses:  7.780784606933594 1.8826271295547485
CurrentTrain: epoch  9, batch    57 | loss: 7.7807846Losses:  7.876779079437256 1.7712318897247314
CurrentTrain: epoch  9, batch    58 | loss: 7.8767791Losses:  7.705691337585449 1.855662226676941
CurrentTrain: epoch  9, batch    59 | loss: 7.7056913Losses:  7.661745071411133 1.8266286849975586
CurrentTrain: epoch  9, batch    60 | loss: 7.6617451Losses:  7.685558319091797 1.8423885107040405
CurrentTrain: epoch  9, batch    61 | loss: 7.6855583Losses:  7.057554244995117 1.529336929321289
CurrentTrain: epoch  9, batch    62 | loss: 7.0575542
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  10.735280990600586 1.9328808784484863
CurrentTrain: epoch  0, batch     0 | loss: 10.7352810Losses:  10.194183349609375 1.931574821472168
CurrentTrain: epoch  0, batch     1 | loss: 10.1941833Losses:  10.623414993286133 2.027097225189209
CurrentTrain: epoch  0, batch     2 | loss: 10.6234150Losses:  8.10164737701416 0.0
CurrentTrain: epoch  0, batch     3 | loss: 8.1016474Losses:  10.432862281799316 2.0857322216033936
CurrentTrain: epoch  1, batch     0 | loss: 10.4328623Losses:  10.516757011413574 2.220280647277832
CurrentTrain: epoch  1, batch     1 | loss: 10.5167570Losses:  9.640671730041504 2.143796443939209
CurrentTrain: epoch  1, batch     2 | loss: 9.6406717Losses:  4.353968620300293 0.6281446218490601
CurrentTrain: epoch  1, batch     3 | loss: 4.3539686Losses:  9.130863189697266 1.9286816120147705
CurrentTrain: epoch  2, batch     0 | loss: 9.1308632Losses:  8.624467849731445 1.9955575466156006
CurrentTrain: epoch  2, batch     1 | loss: 8.6244678Losses:  8.516645431518555 2.0475409030914307
CurrentTrain: epoch  2, batch     2 | loss: 8.5166454Losses:  8.752391815185547 0.6612573266029358
CurrentTrain: epoch  2, batch     3 | loss: 8.7523918Losses:  9.337640762329102 2.1669561862945557
CurrentTrain: epoch  3, batch     0 | loss: 9.3376408Losses:  7.4611921310424805 1.9793169498443604
CurrentTrain: epoch  3, batch     1 | loss: 7.4611921Losses:  8.37435245513916 2.051332950592041
CurrentTrain: epoch  3, batch     2 | loss: 8.3743525Losses:  4.317739486694336 0.6923146843910217
CurrentTrain: epoch  3, batch     3 | loss: 4.3177395Losses:  7.64048957824707 1.9201090335845947
CurrentTrain: epoch  4, batch     0 | loss: 7.6404896Losses:  7.581148147583008 1.8750752210617065
CurrentTrain: epoch  4, batch     1 | loss: 7.5811481Losses:  7.487496376037598 1.925919532775879
CurrentTrain: epoch  4, batch     2 | loss: 7.4874964Losses:  4.349292755126953 0.6641717553138733
CurrentTrain: epoch  4, batch     3 | loss: 4.3492928Losses:  7.581661224365234 1.9272761344909668
CurrentTrain: epoch  5, batch     0 | loss: 7.5816612Losses:  6.643796443939209 1.927502989768982
CurrentTrain: epoch  5, batch     1 | loss: 6.6437964Losses:  7.358788967132568 2.191920042037964
CurrentTrain: epoch  5, batch     2 | loss: 7.3587890Losses:  6.274565696716309 0.6729563474655151
CurrentTrain: epoch  5, batch     3 | loss: 6.2745657Losses:  7.172778606414795 1.9648675918579102
CurrentTrain: epoch  6, batch     0 | loss: 7.1727786Losses:  6.647866725921631 1.898024320602417
CurrentTrain: epoch  6, batch     1 | loss: 6.6478667Losses:  6.731847763061523 1.9738597869873047
CurrentTrain: epoch  6, batch     2 | loss: 6.7318478Losses:  2.4250762462615967 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4250762Losses:  6.991270542144775 2.052535057067871
CurrentTrain: epoch  7, batch     0 | loss: 6.9912705Losses:  7.039999961853027 2.157447338104248
CurrentTrain: epoch  7, batch     1 | loss: 7.0400000Losses:  6.693315505981445 1.9577295780181885
CurrentTrain: epoch  7, batch     2 | loss: 6.6933155Losses:  1.9964931011199951 0.0
CurrentTrain: epoch  7, batch     3 | loss: 1.9964931Losses:  6.5913920402526855 2.050463914871216
CurrentTrain: epoch  8, batch     0 | loss: 6.5913920Losses:  6.453388214111328 1.7849030494689941
CurrentTrain: epoch  8, batch     1 | loss: 6.4533882Losses:  6.554020881652832 2.0583622455596924
CurrentTrain: epoch  8, batch     2 | loss: 6.5540209Losses:  3.1235923767089844 0.6052545309066772
CurrentTrain: epoch  8, batch     3 | loss: 3.1235924Losses:  6.50153112411499 2.0457353591918945
CurrentTrain: epoch  9, batch     0 | loss: 6.5015311Losses:  6.458038806915283 1.7944233417510986
CurrentTrain: epoch  9, batch     1 | loss: 6.4580388Losses:  6.337759494781494 2.0189967155456543
CurrentTrain: epoch  9, batch     2 | loss: 6.3377595Losses:  3.263673782348633 0.6560940146446228
CurrentTrain: epoch  9, batch     3 | loss: 3.2636738
Losses:  7.833858489990234 2.626809597015381
MemoryTrain:  epoch  0, batch     0 | loss: 7.8338585Losses:  2.5739586353302 1.273844599723816
MemoryTrain:  epoch  0, batch     1 | loss: 2.5739586Losses:  7.1802754402160645 2.6444597244262695
MemoryTrain:  epoch  1, batch     0 | loss: 7.1802754Losses:  3.135164260864258 1.2756105661392212
MemoryTrain:  epoch  1, batch     1 | loss: 3.1351643Losses:  6.706399917602539 2.6190266609191895
MemoryTrain:  epoch  2, batch     0 | loss: 6.7063999Losses:  2.711688995361328 1.317983865737915
MemoryTrain:  epoch  2, batch     1 | loss: 2.7116890Losses:  5.932618618011475 2.628918170928955
MemoryTrain:  epoch  3, batch     0 | loss: 5.9326186Losses:  4.8128662109375 1.3154298067092896
MemoryTrain:  epoch  3, batch     1 | loss: 4.8128662Losses:  6.242497444152832 2.6306114196777344
MemoryTrain:  epoch  4, batch     0 | loss: 6.2424974Losses:  2.6355302333831787 1.2585816383361816
MemoryTrain:  epoch  4, batch     1 | loss: 2.6355302Losses:  6.150842189788818 2.6285312175750732
MemoryTrain:  epoch  5, batch     0 | loss: 6.1508422Losses:  2.5174005031585693 1.2381131649017334
MemoryTrain:  epoch  5, batch     1 | loss: 2.5174005Losses:  6.02955961227417 2.6210861206054688
MemoryTrain:  epoch  6, batch     0 | loss: 6.0295596Losses:  2.638016700744629 1.2838189601898193
MemoryTrain:  epoch  6, batch     1 | loss: 2.6380167Losses:  5.736813545227051 2.6331729888916016
MemoryTrain:  epoch  7, batch     0 | loss: 5.7368135Losses:  4.525055408477783 1.2366974353790283
MemoryTrain:  epoch  7, batch     1 | loss: 4.5250554Losses:  6.085387229919434 2.6254358291625977
MemoryTrain:  epoch  8, batch     0 | loss: 6.0853872Losses:  2.5996134281158447 1.2826156616210938
MemoryTrain:  epoch  8, batch     1 | loss: 2.5996134Losses:  5.974541664123535 2.630784511566162
MemoryTrain:  epoch  9, batch     0 | loss: 5.9745417Losses:  2.6990432739257812 1.2738845348358154
MemoryTrain:  epoch  9, batch     1 | loss: 2.6990433
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 68.09%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.53%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.97%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 93.65%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 93.66%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 93.66%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.66%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 93.66%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.58%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.42%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.26%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 92.95%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.50%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 92.38%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 92.02%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 91.40%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 91.21%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 90.66%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.55%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 90.14%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 90.04%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 89.81%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 89.45%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 89.10%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 88.55%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.09%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.76%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.24%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.93%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 86.08%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.78%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 85.32%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 85.04%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.61%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.17%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.51%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.91%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 82.44%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.93%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.53%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.90%   
cur_acc:  ['0.9435', '0.7153']
his_acc:  ['0.9435', '0.8290']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  11.02564811706543 1.9888768196105957
CurrentTrain: epoch  0, batch     0 | loss: 11.0256481Losses:  10.302666664123535 2.0460290908813477
CurrentTrain: epoch  0, batch     1 | loss: 10.3026667Losses:  11.196624755859375 2.182884931564331
CurrentTrain: epoch  0, batch     2 | loss: 11.1966248Losses:  9.063190460205078 0.6656951904296875
CurrentTrain: epoch  0, batch     3 | loss: 9.0631905Losses:  9.686511993408203 2.205193281173706
CurrentTrain: epoch  1, batch     0 | loss: 9.6865120Losses:  10.38861083984375 1.9926247596740723
CurrentTrain: epoch  1, batch     1 | loss: 10.3886108Losses:  9.585348129272461 2.1659278869628906
CurrentTrain: epoch  1, batch     2 | loss: 9.5853481Losses:  7.222400665283203 0.6830583810806274
CurrentTrain: epoch  1, batch     3 | loss: 7.2224007Losses:  8.139921188354492 1.9313313961029053
CurrentTrain: epoch  2, batch     0 | loss: 8.1399212Losses:  9.5411376953125 2.1527562141418457
CurrentTrain: epoch  2, batch     1 | loss: 9.5411377Losses:  9.909456253051758 2.0189645290374756
CurrentTrain: epoch  2, batch     2 | loss: 9.9094563Losses:  4.448350429534912 0.6713665723800659
CurrentTrain: epoch  2, batch     3 | loss: 4.4483504Losses:  9.006669998168945 2.135617256164551
CurrentTrain: epoch  3, batch     0 | loss: 9.0066700Losses:  7.85163688659668 1.9250907897949219
CurrentTrain: epoch  3, batch     1 | loss: 7.8516369Losses:  9.182621955871582 2.0949394702911377
CurrentTrain: epoch  3, batch     2 | loss: 9.1826220Losses:  6.7128071784973145 0.6764509677886963
CurrentTrain: epoch  3, batch     3 | loss: 6.7128072Losses:  8.989530563354492 2.0136046409606934
CurrentTrain: epoch  4, batch     0 | loss: 8.9895306Losses:  8.59960651397705 2.183741569519043
CurrentTrain: epoch  4, batch     1 | loss: 8.5996065Losses:  6.829916000366211 1.9003965854644775
CurrentTrain: epoch  4, batch     2 | loss: 6.8299160Losses:  6.005183219909668 0.6755242347717285
CurrentTrain: epoch  4, batch     3 | loss: 6.0051832Losses:  8.321617126464844 1.8150825500488281
CurrentTrain: epoch  5, batch     0 | loss: 8.3216171Losses:  7.602458953857422 2.0075411796569824
CurrentTrain: epoch  5, batch     1 | loss: 7.6024590Losses:  7.113409996032715 1.8511205911636353
CurrentTrain: epoch  5, batch     2 | loss: 7.1134100Losses:  8.707151412963867 0.6904040575027466
CurrentTrain: epoch  5, batch     3 | loss: 8.7071514Losses:  8.775203704833984 2.179422616958618
CurrentTrain: epoch  6, batch     0 | loss: 8.7752037Losses:  8.192060470581055 2.1089484691619873
CurrentTrain: epoch  6, batch     1 | loss: 8.1920605Losses:  7.341359615325928 2.0838046073913574
CurrentTrain: epoch  6, batch     2 | loss: 7.3413596Losses:  4.312891960144043 0.6608164310455322
CurrentTrain: epoch  6, batch     3 | loss: 4.3128920Losses:  7.33950138092041 2.027472734451294
CurrentTrain: epoch  7, batch     0 | loss: 7.3395014Losses:  7.694831848144531 2.0212490558624268
CurrentTrain: epoch  7, batch     1 | loss: 7.6948318Losses:  7.586339950561523 2.017195701599121
CurrentTrain: epoch  7, batch     2 | loss: 7.5863400Losses:  6.273076057434082 0.6829781532287598
CurrentTrain: epoch  7, batch     3 | loss: 6.2730761Losses:  7.813140392303467 2.1829423904418945
CurrentTrain: epoch  8, batch     0 | loss: 7.8131404Losses:  7.793573379516602 2.1297433376312256
CurrentTrain: epoch  8, batch     1 | loss: 7.7935734Losses:  7.296213150024414 2.0118956565856934
CurrentTrain: epoch  8, batch     2 | loss: 7.2962132Losses:  4.031426429748535 0.6791430711746216
CurrentTrain: epoch  8, batch     3 | loss: 4.0314264Losses:  6.8755574226379395 1.9151123762130737
CurrentTrain: epoch  9, batch     0 | loss: 6.8755574Losses:  6.3408002853393555 1.8507258892059326
CurrentTrain: epoch  9, batch     1 | loss: 6.3408003Losses:  7.6821088790893555 1.9541186094284058
CurrentTrain: epoch  9, batch     2 | loss: 7.6821089Losses:  6.343104362487793 0.6805154085159302
CurrentTrain: epoch  9, batch     3 | loss: 6.3431044
Losses:  6.626526832580566 2.6681225299835205
MemoryTrain:  epoch  0, batch     0 | loss: 6.6265268Losses:  5.532318115234375 2.521177053451538
MemoryTrain:  epoch  0, batch     1 | loss: 5.5323181Losses:  6.36973762512207 2.682563304901123
MemoryTrain:  epoch  1, batch     0 | loss: 6.3697376Losses:  6.094407558441162 2.512416362762451
MemoryTrain:  epoch  1, batch     1 | loss: 6.0944076Losses:  6.0254364013671875 2.6496543884277344
MemoryTrain:  epoch  2, batch     0 | loss: 6.0254364Losses:  5.629238128662109 2.537433385848999
MemoryTrain:  epoch  2, batch     1 | loss: 5.6292381Losses:  5.772295951843262 2.6448512077331543
MemoryTrain:  epoch  3, batch     0 | loss: 5.7722960Losses:  5.347411155700684 2.5384440422058105
MemoryTrain:  epoch  3, batch     1 | loss: 5.3474112Losses:  5.590239524841309 2.6666455268859863
MemoryTrain:  epoch  4, batch     0 | loss: 5.5902395Losses:  5.2751946449279785 2.523240804672241
MemoryTrain:  epoch  4, batch     1 | loss: 5.2751946Losses:  5.567251682281494 2.690136432647705
MemoryTrain:  epoch  5, batch     0 | loss: 5.5672517Losses:  5.1009650230407715 2.4857168197631836
MemoryTrain:  epoch  5, batch     1 | loss: 5.1009650Losses:  5.409888744354248 2.646669387817383
MemoryTrain:  epoch  6, batch     0 | loss: 5.4098887Losses:  5.131215572357178 2.544131278991699
MemoryTrain:  epoch  6, batch     1 | loss: 5.1312156Losses:  5.4345269203186035 2.665802001953125
MemoryTrain:  epoch  7, batch     0 | loss: 5.4345269Losses:  5.123569488525391 2.524178981781006
MemoryTrain:  epoch  7, batch     1 | loss: 5.1235695Losses:  5.4495344161987305 2.6983189582824707
MemoryTrain:  epoch  8, batch     0 | loss: 5.4495344Losses:  4.988498210906982 2.479391098022461
MemoryTrain:  epoch  8, batch     1 | loss: 4.9884982Losses:  5.441098213195801 2.6927499771118164
MemoryTrain:  epoch  9, batch     0 | loss: 5.4410982Losses:  5.031396865844727 2.496649742126465
MemoryTrain:  epoch  9, batch     1 | loss: 5.0313969
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 62.17%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 70.43%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.83%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 93.22%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.68%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 92.52%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 92.46%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.78%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 92.64%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 92.57%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.43%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.37%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.23%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.56%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 91.23%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 90.74%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 90.25%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 90.07%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 89.83%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 89.15%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 88.92%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 88.76%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 88.54%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 88.52%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 88.37%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 87.83%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 87.24%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 86.59%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 86.15%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 85.59%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.29%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 84.69%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.34%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.13%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.86%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.65%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.31%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 82.83%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 82.12%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 81.48%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 80.80%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.74%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.48%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.46%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.52%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 80.63%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.89%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 80.85%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.90%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.86%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 80.81%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 81.01%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.34%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.66%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.43%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 80.89%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.40%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 79.88%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 79.31%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 78.80%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 78.43%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 79.06%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 78.70%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 78.39%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 78.08%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 77.74%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 77.36%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.35%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.52%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.05%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 78.34%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 78.11%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 78.02%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.91%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.76%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.73%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.71%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.73%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 77.74%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 77.53%   
cur_acc:  ['0.9435', '0.7153', '0.7083']
his_acc:  ['0.9435', '0.8290', '0.7753']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  9.58194637298584 2.0025229454040527
CurrentTrain: epoch  0, batch     0 | loss: 9.5819464Losses:  8.22305679321289 1.9913949966430664
CurrentTrain: epoch  0, batch     1 | loss: 8.2230568Losses:  9.0184326171875 1.8531992435455322
CurrentTrain: epoch  0, batch     2 | loss: 9.0184326Losses:  4.857491493225098 0.6732853651046753
CurrentTrain: epoch  0, batch     3 | loss: 4.8574915Losses:  8.688505172729492 2.1339855194091797
CurrentTrain: epoch  1, batch     0 | loss: 8.6885052Losses:  7.888916015625 2.076186180114746
CurrentTrain: epoch  1, batch     1 | loss: 7.8889160Losses:  8.175209045410156 2.138579845428467
CurrentTrain: epoch  1, batch     2 | loss: 8.1752090Losses:  3.738267660140991 0.6378124952316284
CurrentTrain: epoch  1, batch     3 | loss: 3.7382677Losses:  8.52199935913086 2.0058071613311768
CurrentTrain: epoch  2, batch     0 | loss: 8.5219994Losses:  7.243221282958984 2.0437588691711426
CurrentTrain: epoch  2, batch     1 | loss: 7.2432213Losses:  6.505895614624023 1.974587321281433
CurrentTrain: epoch  2, batch     2 | loss: 6.5058956Losses:  3.4715306758880615 0.6686609983444214
CurrentTrain: epoch  2, batch     3 | loss: 3.4715307Losses:  7.297998905181885 2.0285286903381348
CurrentTrain: epoch  3, batch     0 | loss: 7.2979989Losses:  7.309561729431152 2.1599767208099365
CurrentTrain: epoch  3, batch     1 | loss: 7.3095617Losses:  7.137092113494873 2.0728325843811035
CurrentTrain: epoch  3, batch     2 | loss: 7.1370921Losses:  3.32077956199646 0.6517040729522705
CurrentTrain: epoch  3, batch     3 | loss: 3.3207796Losses:  6.6694416999816895 1.9027886390686035
CurrentTrain: epoch  4, batch     0 | loss: 6.6694417Losses:  6.405662536621094 1.8550840616226196
CurrentTrain: epoch  4, batch     1 | loss: 6.4056625Losses:  6.409595966339111 1.6504871845245361
CurrentTrain: epoch  4, batch     2 | loss: 6.4095960Losses:  3.349000930786133 0.6637866497039795
CurrentTrain: epoch  4, batch     3 | loss: 3.3490009Losses:  7.231152534484863 2.045936107635498
CurrentTrain: epoch  5, batch     0 | loss: 7.2311525Losses:  6.697991847991943 1.9283277988433838
CurrentTrain: epoch  5, batch     1 | loss: 6.6979918Losses:  6.628958702087402 2.1538214683532715
CurrentTrain: epoch  5, batch     2 | loss: 6.6289587Losses:  1.805959939956665 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8059599Losses:  5.854735374450684 1.8892303705215454
CurrentTrain: epoch  6, batch     0 | loss: 5.8547354Losses:  6.512857437133789 1.9176280498504639
CurrentTrain: epoch  6, batch     1 | loss: 6.5128574Losses:  6.916197776794434 2.0266571044921875
CurrentTrain: epoch  6, batch     2 | loss: 6.9161978Losses:  3.6346349716186523 0.6879106760025024
CurrentTrain: epoch  6, batch     3 | loss: 3.6346350Losses:  6.34210205078125 2.0185725688934326
CurrentTrain: epoch  7, batch     0 | loss: 6.3421021Losses:  6.1766252517700195 1.9998440742492676
CurrentTrain: epoch  7, batch     1 | loss: 6.1766253Losses:  6.595070838928223 2.044083833694458
CurrentTrain: epoch  7, batch     2 | loss: 6.5950708Losses:  4.176663398742676 0.6664956212043762
CurrentTrain: epoch  7, batch     3 | loss: 4.1766634Losses:  6.102959156036377 1.8200509548187256
CurrentTrain: epoch  8, batch     0 | loss: 6.1029592Losses:  6.294145584106445 2.048774480819702
CurrentTrain: epoch  8, batch     1 | loss: 6.2941456Losses:  5.959634304046631 2.0232017040252686
CurrentTrain: epoch  8, batch     2 | loss: 5.9596343Losses:  3.691263437271118 0.689958930015564
CurrentTrain: epoch  8, batch     3 | loss: 3.6912634Losses:  6.178985595703125 2.034409761428833
CurrentTrain: epoch  9, batch     0 | loss: 6.1789856Losses:  6.242667198181152 1.9298337697982788
CurrentTrain: epoch  9, batch     1 | loss: 6.2426672Losses:  5.857991695404053 1.9830169677734375
CurrentTrain: epoch  9, batch     2 | loss: 5.8579917Losses:  3.1707656383514404 0.6457745432853699
CurrentTrain: epoch  9, batch     3 | loss: 3.1707656
Losses:  5.6918182373046875 2.693188428878784
MemoryTrain:  epoch  0, batch     0 | loss: 5.6918182Losses:  6.932477951049805 2.6435625553131104
MemoryTrain:  epoch  0, batch     1 | loss: 6.9324780Losses:  5.129250526428223 1.990100622177124
MemoryTrain:  epoch  0, batch     2 | loss: 5.1292505Losses:  6.372860431671143 2.6842074394226074
MemoryTrain:  epoch  1, batch     0 | loss: 6.3728604Losses:  6.649611949920654 2.642148494720459
MemoryTrain:  epoch  1, batch     1 | loss: 6.6496119Losses:  5.949390888214111 2.0189638137817383
MemoryTrain:  epoch  1, batch     2 | loss: 5.9493909Losses:  6.208888530731201 2.6808691024780273
MemoryTrain:  epoch  2, batch     0 | loss: 6.2088885Losses:  6.167722702026367 2.693263530731201
MemoryTrain:  epoch  2, batch     1 | loss: 6.1677227Losses:  4.420825481414795 1.9187917709350586
MemoryTrain:  epoch  2, batch     2 | loss: 4.4208255Losses:  6.211452484130859 2.672872543334961
MemoryTrain:  epoch  3, batch     0 | loss: 6.2114525Losses:  5.783671855926514 2.6615452766418457
MemoryTrain:  epoch  3, batch     1 | loss: 5.7836719Losses:  4.148980140686035 1.9771274328231812
MemoryTrain:  epoch  3, batch     2 | loss: 4.1489801Losses:  5.6315202713012695 2.68172025680542
MemoryTrain:  epoch  4, batch     0 | loss: 5.6315203Losses:  5.7211995124816895 2.6518239974975586
MemoryTrain:  epoch  4, batch     1 | loss: 5.7211995Losses:  5.072230815887451 1.9863719940185547
MemoryTrain:  epoch  4, batch     2 | loss: 5.0722308Losses:  5.903858184814453 2.6456141471862793
MemoryTrain:  epoch  5, batch     0 | loss: 5.9038582Losses:  5.5258049964904785 2.6827621459960938
MemoryTrain:  epoch  5, batch     1 | loss: 5.5258050Losses:  4.067663192749023 1.9872459173202515
MemoryTrain:  epoch  5, batch     2 | loss: 4.0676632Losses:  5.366396903991699 2.637446880340576
MemoryTrain:  epoch  6, batch     0 | loss: 5.3663969Losses:  5.546719551086426 2.678144931793213
MemoryTrain:  epoch  6, batch     1 | loss: 5.5467196Losses:  4.416930675506592 2.014165163040161
MemoryTrain:  epoch  6, batch     2 | loss: 4.4169307Losses:  5.427160263061523 2.654296875
MemoryTrain:  epoch  7, batch     0 | loss: 5.4271603Losses:  5.521036624908447 2.676302909851074
MemoryTrain:  epoch  7, batch     1 | loss: 5.5210366Losses:  4.058705806732178 1.982112169265747
MemoryTrain:  epoch  7, batch     2 | loss: 4.0587058Losses:  5.422363758087158 2.649505138397217
MemoryTrain:  epoch  8, batch     0 | loss: 5.4223638Losses:  5.5037336349487305 2.689176559448242
MemoryTrain:  epoch  8, batch     1 | loss: 5.5037336Losses:  3.983363389968872 1.9526445865631104
MemoryTrain:  epoch  8, batch     2 | loss: 3.9833634Losses:  5.4138336181640625 2.679490566253662
MemoryTrain:  epoch  9, batch     0 | loss: 5.4138336Losses:  5.3425188064575195 2.64312744140625
MemoryTrain:  epoch  9, batch     1 | loss: 5.3425188Losses:  4.086139678955078 1.9895484447479248
MemoryTrain:  epoch  9, batch     2 | loss: 4.0861397
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.50%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.95%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.47%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 90.31%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.06%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 90.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.67%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.46%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.03%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.77%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.74%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.41%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 88.93%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 88.62%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 88.31%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.08%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 87.28%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.01%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 86.81%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 86.56%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 86.04%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 85.53%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 85.03%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 84.66%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 84.31%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 83.35%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.15%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.89%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.69%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.56%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.37%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.89%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 81.19%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 80.50%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.83%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 79.22%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.74%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.48%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.34%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.53%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.04%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.10%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.07%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 80.14%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 80.19%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 80.15%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 80.70%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 80.48%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 79.95%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 79.46%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 78.95%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 78.39%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 77.88%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.56%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.67%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.08%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 78.19%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.71%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 77.29%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 76.95%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 76.57%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.19%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 76.57%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 76.37%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 76.02%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 75.75%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 75.56%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 75.30%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 75.15%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 75.07%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 74.89%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 74.86%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 74.82%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 74.76%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 74.73%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 74.90%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.07%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 75.13%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 75.64%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 75.61%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 75.51%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 75.38%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.94%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 74.85%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.70%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 74.55%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 74.40%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 74.23%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 73.99%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 73.76%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.50%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 74.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 75.86%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.72%   
cur_acc:  ['0.9435', '0.7153', '0.7083', '0.8095']
his_acc:  ['0.9435', '0.8290', '0.7753', '0.7672']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  9.649301528930664 1.9522744417190552
CurrentTrain: epoch  0, batch     0 | loss: 9.6493015Losses:  9.78661823272705 1.9839047193527222
CurrentTrain: epoch  0, batch     1 | loss: 9.7866182Losses:  9.514424324035645 1.9434731006622314
CurrentTrain: epoch  0, batch     2 | loss: 9.5144243Losses:  3.403571605682373 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.4035716Losses:  9.092966079711914 1.9803693294525146
CurrentTrain: epoch  1, batch     0 | loss: 9.0929661Losses:  8.661055564880371 2.081364154815674
CurrentTrain: epoch  1, batch     1 | loss: 8.6610556Losses:  8.436014175415039 2.0011000633239746
CurrentTrain: epoch  1, batch     2 | loss: 8.4360142Losses:  5.864803314208984 0.6769769191741943
CurrentTrain: epoch  1, batch     3 | loss: 5.8648033Losses:  8.394575119018555 2.0972859859466553
CurrentTrain: epoch  2, batch     0 | loss: 8.3945751Losses:  9.14262866973877 2.0889129638671875
CurrentTrain: epoch  2, batch     1 | loss: 9.1426287Losses:  8.149721145629883 2.0774970054626465
CurrentTrain: epoch  2, batch     2 | loss: 8.1497211Losses:  4.195194244384766 0.6782107949256897
CurrentTrain: epoch  2, batch     3 | loss: 4.1951942Losses:  8.672747611999512 2.095231294631958
CurrentTrain: epoch  3, batch     0 | loss: 8.6727476Losses:  7.682501792907715 1.9713857173919678
CurrentTrain: epoch  3, batch     1 | loss: 7.6825018Losses:  7.546069145202637 1.9922888278961182
CurrentTrain: epoch  3, batch     2 | loss: 7.5460691Losses:  5.82260799407959 0.6791386604309082
CurrentTrain: epoch  3, batch     3 | loss: 5.8226080Losses:  7.242846488952637 1.9336289167404175
CurrentTrain: epoch  4, batch     0 | loss: 7.2428465Losses:  7.669931888580322 1.7995731830596924
CurrentTrain: epoch  4, batch     1 | loss: 7.6699319Losses:  7.939676761627197 2.173211097717285
CurrentTrain: epoch  4, batch     2 | loss: 7.9396768Losses:  3.1307811737060547 0.6752674579620361
CurrentTrain: epoch  4, batch     3 | loss: 3.1307812Losses:  7.489738941192627 2.0882344245910645
CurrentTrain: epoch  5, batch     0 | loss: 7.4897389Losses:  7.33579158782959 1.9999912977218628
CurrentTrain: epoch  5, batch     1 | loss: 7.3357916Losses:  7.530926704406738 2.050137519836426
CurrentTrain: epoch  5, batch     2 | loss: 7.5309267Losses:  6.236888885498047 0.6952548027038574
CurrentTrain: epoch  5, batch     3 | loss: 6.2368889Losses:  7.221368789672852 1.9439753293991089
CurrentTrain: epoch  6, batch     0 | loss: 7.2213688Losses:  6.9587907791137695 1.9751954078674316
CurrentTrain: epoch  6, batch     1 | loss: 6.9587908Losses:  7.226778030395508 2.048668384552002
CurrentTrain: epoch  6, batch     2 | loss: 7.2267780Losses:  3.5926249027252197 0.6734827756881714
CurrentTrain: epoch  6, batch     3 | loss: 3.5926249Losses:  7.155531883239746 2.0796542167663574
CurrentTrain: epoch  7, batch     0 | loss: 7.1555319Losses:  7.111804485321045 2.0560178756713867
CurrentTrain: epoch  7, batch     1 | loss: 7.1118045Losses:  6.893744468688965 2.0006513595581055
CurrentTrain: epoch  7, batch     2 | loss: 6.8937445Losses:  4.166029930114746 0.6684774160385132
CurrentTrain: epoch  7, batch     3 | loss: 4.1660299Losses:  7.007338523864746 2.172867774963379
CurrentTrain: epoch  8, batch     0 | loss: 7.0073385Losses:  6.999553203582764 1.9270799160003662
CurrentTrain: epoch  8, batch     1 | loss: 6.9995532Losses:  6.962224006652832 2.0498294830322266
CurrentTrain: epoch  8, batch     2 | loss: 6.9622240Losses:  3.1416730880737305 0.6487172245979309
CurrentTrain: epoch  8, batch     3 | loss: 3.1416731Losses:  7.083294868469238 2.093033790588379
CurrentTrain: epoch  9, batch     0 | loss: 7.0832949Losses:  6.074489116668701 1.9066275358200073
CurrentTrain: epoch  9, batch     1 | loss: 6.0744891Losses:  6.781996726989746 1.9078516960144043
CurrentTrain: epoch  9, batch     2 | loss: 6.7819967Losses:  3.334733486175537 0.6703974008560181
CurrentTrain: epoch  9, batch     3 | loss: 3.3347335
Losses:  5.7201690673828125 2.6407368183135986
MemoryTrain:  epoch  0, batch     0 | loss: 5.7201691Losses:  6.127732276916504 2.702942371368408
MemoryTrain:  epoch  0, batch     1 | loss: 6.1277323Losses:  5.831206798553467 2.6912355422973633
MemoryTrain:  epoch  0, batch     2 | loss: 5.8312068Losses:  2.555896759033203 0.6632242202758789
MemoryTrain:  epoch  0, batch     3 | loss: 2.5558968Losses:  6.6408305168151855 2.6768760681152344
MemoryTrain:  epoch  1, batch     0 | loss: 6.6408305Losses:  6.305210113525391 2.703341484069824
MemoryTrain:  epoch  1, batch     1 | loss: 6.3052101Losses:  5.649398326873779 2.6564226150512695
MemoryTrain:  epoch  1, batch     2 | loss: 5.6493983Losses:  1.3240147829055786 0.6280282735824585
MemoryTrain:  epoch  1, batch     3 | loss: 1.3240148Losses:  6.08859395980835 2.679142475128174
MemoryTrain:  epoch  2, batch     0 | loss: 6.0885940Losses:  5.537319660186768 2.6670572757720947
MemoryTrain:  epoch  2, batch     1 | loss: 5.5373197Losses:  5.853090763092041 2.6942009925842285
MemoryTrain:  epoch  2, batch     2 | loss: 5.8530908Losses:  1.3570224046707153 0.6493760347366333
MemoryTrain:  epoch  2, batch     3 | loss: 1.3570224Losses:  5.712350368499756 2.674055576324463
MemoryTrain:  epoch  3, batch     0 | loss: 5.7123504Losses:  5.520366668701172 2.679051399230957
MemoryTrain:  epoch  3, batch     1 | loss: 5.5203667Losses:  5.708332538604736 2.6709377765655518
MemoryTrain:  epoch  3, batch     2 | loss: 5.7083325Losses:  1.7135672569274902 0.636397123336792
MemoryTrain:  epoch  3, batch     3 | loss: 1.7135673Losses:  5.659878253936768 2.6703031063079834
MemoryTrain:  epoch  4, batch     0 | loss: 5.6598783Losses:  5.5408406257629395 2.692046642303467
MemoryTrain:  epoch  4, batch     1 | loss: 5.5408406Losses:  5.468775272369385 2.662424087524414
MemoryTrain:  epoch  4, batch     2 | loss: 5.4687753Losses:  1.4412827491760254 0.6408900022506714
MemoryTrain:  epoch  4, batch     3 | loss: 1.4412827Losses:  5.468356609344482 2.6618692874908447
MemoryTrain:  epoch  5, batch     0 | loss: 5.4683566Losses:  5.511215686798096 2.668534278869629
MemoryTrain:  epoch  5, batch     1 | loss: 5.5112157Losses:  5.516141891479492 2.699490785598755
MemoryTrain:  epoch  5, batch     2 | loss: 5.5161419Losses:  1.1954370737075806 0.5908154249191284
MemoryTrain:  epoch  5, batch     3 | loss: 1.1954371Losses:  5.4990034103393555 2.661527633666992
MemoryTrain:  epoch  6, batch     0 | loss: 5.4990034Losses:  5.427832126617432 2.6758975982666016
MemoryTrain:  epoch  6, batch     1 | loss: 5.4278321Losses:  5.491357326507568 2.691453456878662
MemoryTrain:  epoch  6, batch     2 | loss: 5.4913573Losses:  1.301992416381836 0.6389015913009644
MemoryTrain:  epoch  6, batch     3 | loss: 1.3019924Losses:  5.462132930755615 2.6862716674804688
MemoryTrain:  epoch  7, batch     0 | loss: 5.4621329Losses:  5.3839569091796875 2.665940761566162
MemoryTrain:  epoch  7, batch     1 | loss: 5.3839569Losses:  5.393765449523926 2.658416986465454
MemoryTrain:  epoch  7, batch     2 | loss: 5.3937654Losses:  1.36176598072052 0.666382372379303
MemoryTrain:  epoch  7, batch     3 | loss: 1.3617660Losses:  5.399574279785156 2.6751132011413574
MemoryTrain:  epoch  8, batch     0 | loss: 5.3995743Losses:  5.3613176345825195 2.651904582977295
MemoryTrain:  epoch  8, batch     1 | loss: 5.3613176Losses:  5.411893844604492 2.679807186126709
MemoryTrain:  epoch  8, batch     2 | loss: 5.4118938Losses:  1.4239263534545898 0.6634473204612732
MemoryTrain:  epoch  8, batch     3 | loss: 1.4239264Losses:  5.391271114349365 2.6625876426696777
MemoryTrain:  epoch  9, batch     0 | loss: 5.3912711Losses:  5.38217306137085 2.66726016998291
MemoryTrain:  epoch  9, batch     1 | loss: 5.3821731Losses:  5.433595657348633 2.6888909339904785
MemoryTrain:  epoch  9, batch     2 | loss: 5.4335957Losses:  1.1565362215042114 0.5713510513305664
MemoryTrain:  epoch  9, batch     3 | loss: 1.1565362
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 63.84%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 60.74%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 59.19%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 57.29%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 55.43%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 57.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.89%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 59.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.45%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.81%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.22%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 89.27%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.04%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.01%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 88.96%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.73%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 88.51%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.96%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.85%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 88.56%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 88.38%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 88.13%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 87.89%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.58%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 87.05%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 86.25%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 85.97%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.13%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 85.01%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 84.83%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 84.89%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 84.54%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 84.11%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 83.55%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 83.01%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 82.67%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 82.08%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 81.82%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 81.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 80.88%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 80.64%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.19%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 79.05%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 78.44%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 77.78%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.55%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.23%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 78.20%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.22%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.97%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 78.71%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 78.15%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 77.63%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 77.13%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.58%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.05%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 75.69%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 76.28%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 75.78%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 75.37%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.04%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 74.16%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.37%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 74.58%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 74.14%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 74.08%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 74.20%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 75.16%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 74.81%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.43%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 74.34%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 73.79%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 73.56%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 73.01%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 72.75%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 72.46%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.10%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.13%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.01%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.66%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.74%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 75.22%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 75.15%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 75.10%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 75.15%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 75.05%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 75.05%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 75.02%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 74.88%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 74.88%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 74.95%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 74.62%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 74.60%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 74.52%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 74.32%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 74.19%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 74.10%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 73.93%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 73.78%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 73.69%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 73.65%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 73.42%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 73.08%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 72.82%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 72.72%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 72.71%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 72.85%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 72.94%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 72.97%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.96%   
cur_acc:  ['0.9435', '0.7153', '0.7083', '0.8095', '0.6845']
his_acc:  ['0.9435', '0.8290', '0.7753', '0.7672', '0.7396']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  10.182798385620117 2.005560874938965
CurrentTrain: epoch  0, batch     0 | loss: 10.1827984Losses:  9.656274795532227 1.7446497678756714
CurrentTrain: epoch  0, batch     1 | loss: 9.6562748Losses:  10.707782745361328 2.0090198516845703
CurrentTrain: epoch  0, batch     2 | loss: 10.7077827Losses:  4.792201995849609 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.7922020Losses:  10.236471176147461 2.1765074729919434
CurrentTrain: epoch  1, batch     0 | loss: 10.2364712Losses:  9.18954849243164 2.0667762756347656
CurrentTrain: epoch  1, batch     1 | loss: 9.1895485Losses:  8.499238967895508 2.0558383464813232
CurrentTrain: epoch  1, batch     2 | loss: 8.4992390Losses:  6.13304328918457 0.6520415544509888
CurrentTrain: epoch  1, batch     3 | loss: 6.1330433Losses:  8.139898300170898 1.920118808746338
CurrentTrain: epoch  2, batch     0 | loss: 8.1398983Losses:  8.928837776184082 2.0319664478302
CurrentTrain: epoch  2, batch     1 | loss: 8.9288378Losses:  8.145532608032227 2.052704334259033
CurrentTrain: epoch  2, batch     2 | loss: 8.1455326Losses:  6.449814796447754 0.6755629777908325
CurrentTrain: epoch  2, batch     3 | loss: 6.4498148Losses:  7.279488563537598 1.9318345785140991
CurrentTrain: epoch  3, batch     0 | loss: 7.2794886Losses:  8.860759735107422 2.123643398284912
CurrentTrain: epoch  3, batch     1 | loss: 8.8607597Losses:  8.704596519470215 2.212470531463623
CurrentTrain: epoch  3, batch     2 | loss: 8.7045965Losses:  5.809854507446289 0.6690260171890259
CurrentTrain: epoch  3, batch     3 | loss: 5.8098545Losses:  6.9394025802612305 1.8513212203979492
CurrentTrain: epoch  4, batch     0 | loss: 6.9394026Losses:  8.692243576049805 1.9313225746154785
CurrentTrain: epoch  4, batch     1 | loss: 8.6922436Losses:  8.21547794342041 2.1260948181152344
CurrentTrain: epoch  4, batch     2 | loss: 8.2154779Losses:  4.511287212371826 0.6819887161254883
CurrentTrain: epoch  4, batch     3 | loss: 4.5112872Losses:  7.490488529205322 2.1157314777374268
CurrentTrain: epoch  5, batch     0 | loss: 7.4904885Losses:  8.108656883239746 1.855749487876892
CurrentTrain: epoch  5, batch     1 | loss: 8.1086569Losses:  7.1436448097229 2.045914888381958
CurrentTrain: epoch  5, batch     2 | loss: 7.1436448Losses:  4.442918300628662 0.6610093116760254
CurrentTrain: epoch  5, batch     3 | loss: 4.4429183Losses:  6.896443843841553 2.052137613296509
CurrentTrain: epoch  6, batch     0 | loss: 6.8964438Losses:  7.902878761291504 2.125761032104492
CurrentTrain: epoch  6, batch     1 | loss: 7.9028788Losses:  7.671808242797852 2.082012176513672
CurrentTrain: epoch  6, batch     2 | loss: 7.6718082Losses:  4.715636253356934 0.6471882462501526
CurrentTrain: epoch  6, batch     3 | loss: 4.7156363Losses:  7.443384170532227 2.051818370819092
CurrentTrain: epoch  7, batch     0 | loss: 7.4433842Losses:  7.308037757873535 2.0846595764160156
CurrentTrain: epoch  7, batch     1 | loss: 7.3080378Losses:  6.643779754638672 1.9126585721969604
CurrentTrain: epoch  7, batch     2 | loss: 6.6437798Losses:  5.412991046905518 0.6633027195930481
CurrentTrain: epoch  7, batch     3 | loss: 5.4129910Losses:  6.589327812194824 1.889479398727417
CurrentTrain: epoch  8, batch     0 | loss: 6.5893278Losses:  6.271638870239258 1.858762502670288
CurrentTrain: epoch  8, batch     1 | loss: 6.2716389Losses:  7.592810153961182 1.956326961517334
CurrentTrain: epoch  8, batch     2 | loss: 7.5928102Losses:  3.168191432952881 0.6710096597671509
CurrentTrain: epoch  8, batch     3 | loss: 3.1681914Losses:  6.489684581756592 1.9977377653121948
CurrentTrain: epoch  9, batch     0 | loss: 6.4896846Losses:  6.385391712188721 1.8604590892791748
CurrentTrain: epoch  9, batch     1 | loss: 6.3853917Losses:  6.675097465515137 1.8440747261047363
CurrentTrain: epoch  9, batch     2 | loss: 6.6750975Losses:  6.768564224243164 0.6737403869628906
CurrentTrain: epoch  9, batch     3 | loss: 6.7685642
Losses:  6.039886951446533 2.703627586364746
MemoryTrain:  epoch  0, batch     0 | loss: 6.0398870Losses:  6.227418422698975 2.6919567584991455
MemoryTrain:  epoch  0, batch     1 | loss: 6.2274184Losses:  5.664304733276367 2.6688942909240723
MemoryTrain:  epoch  0, batch     2 | loss: 5.6643047Losses:  5.26784086227417 2.365067958831787
MemoryTrain:  epoch  0, batch     3 | loss: 5.2678409Losses:  6.779383659362793 2.645031452178955
MemoryTrain:  epoch  1, batch     0 | loss: 6.7793837Losses:  5.9115681648254395 2.6679370403289795
MemoryTrain:  epoch  1, batch     1 | loss: 5.9115682Losses:  6.088488578796387 2.69960355758667
MemoryTrain:  epoch  1, batch     2 | loss: 6.0884886Losses:  5.17919921875 2.4103269577026367
MemoryTrain:  epoch  1, batch     3 | loss: 5.1791992Losses:  5.833137512207031 2.6847786903381348
MemoryTrain:  epoch  2, batch     0 | loss: 5.8331375Losses:  5.70965051651001 2.6683902740478516
MemoryTrain:  epoch  2, batch     1 | loss: 5.7096505Losses:  6.033946514129639 2.6899003982543945
MemoryTrain:  epoch  2, batch     2 | loss: 6.0339465Losses:  4.946197032928467 2.375154495239258
MemoryTrain:  epoch  2, batch     3 | loss: 4.9461970Losses:  5.4766621589660645 2.663581609725952
MemoryTrain:  epoch  3, batch     0 | loss: 5.4766622Losses:  5.4934892654418945 2.68615460395813
MemoryTrain:  epoch  3, batch     1 | loss: 5.4934893Losses:  5.791985511779785 2.65529727935791
MemoryTrain:  epoch  3, batch     2 | loss: 5.7919855Losses:  5.30216646194458 2.414694309234619
MemoryTrain:  epoch  3, batch     3 | loss: 5.3021665Losses:  5.846365928649902 2.6845011711120605
MemoryTrain:  epoch  4, batch     0 | loss: 5.8463659Losses:  5.497509002685547 2.6618576049804688
MemoryTrain:  epoch  4, batch     1 | loss: 5.4975090Losses:  5.386484146118164 2.6557083129882812
MemoryTrain:  epoch  4, batch     2 | loss: 5.3864841Losses:  4.923028945922852 2.419405937194824
MemoryTrain:  epoch  4, batch     3 | loss: 4.9230289Losses:  5.43629264831543 2.678277015686035
MemoryTrain:  epoch  5, batch     0 | loss: 5.4362926Losses:  5.51241397857666 2.6950926780700684
MemoryTrain:  epoch  5, batch     1 | loss: 5.5124140Losses:  5.40924596786499 2.656406879425049
MemoryTrain:  epoch  5, batch     2 | loss: 5.4092460Losses:  5.424962997436523 2.3754079341888428
MemoryTrain:  epoch  5, batch     3 | loss: 5.4249630Losses:  5.498745441436768 2.676349639892578
MemoryTrain:  epoch  6, batch     0 | loss: 5.4987454Losses:  5.412046432495117 2.6817572116851807
MemoryTrain:  epoch  6, batch     1 | loss: 5.4120464Losses:  5.3434062004089355 2.650331974029541
MemoryTrain:  epoch  6, batch     2 | loss: 5.3434062Losses:  4.841914653778076 2.385676383972168
MemoryTrain:  epoch  6, batch     3 | loss: 4.8419147Losses:  5.405525207519531 2.6658740043640137
MemoryTrain:  epoch  7, batch     0 | loss: 5.4055252Losses:  5.4006571769714355 2.673861503601074
MemoryTrain:  epoch  7, batch     1 | loss: 5.4006572Losses:  5.419130325317383 2.680180072784424
MemoryTrain:  epoch  7, batch     2 | loss: 5.4191303Losses:  4.827306747436523 2.3832850456237793
MemoryTrain:  epoch  7, batch     3 | loss: 4.8273067Losses:  5.405162811279297 2.6753735542297363
MemoryTrain:  epoch  8, batch     0 | loss: 5.4051628Losses:  5.354342460632324 2.654991626739502
MemoryTrain:  epoch  8, batch     1 | loss: 5.3543425Losses:  5.38438081741333 2.662971019744873
MemoryTrain:  epoch  8, batch     2 | loss: 5.3843808Losses:  4.862823963165283 2.394845485687256
MemoryTrain:  epoch  8, batch     3 | loss: 4.8628240Losses:  5.3428215980529785 2.654552459716797
MemoryTrain:  epoch  9, batch     0 | loss: 5.3428216Losses:  5.3632659912109375 2.65967059135437
MemoryTrain:  epoch  9, batch     1 | loss: 5.3632660Losses:  5.427168846130371 2.691073417663574
MemoryTrain:  epoch  9, batch     2 | loss: 5.4271688Losses:  4.7933030128479 2.3743183612823486
MemoryTrain:  epoch  9, batch     3 | loss: 4.7933030
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 68.09%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.25%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.50%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 88.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.73%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 86.86%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.56%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 86.07%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.79%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 84.96%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 83.90%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 83.40%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.13%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 82.63%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.45%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 82.28%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 81.78%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 81.33%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 80.80%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 80.16%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 79.45%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.40%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 79.42%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 79.50%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 79.12%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 78.75%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 78.32%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.16%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 77.87%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.29%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.14%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 76.94%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.80%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.73%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.59%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 76.17%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 75.52%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 74.89%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 74.26%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 73.70%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.27%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.06%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.37%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 74.24%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.10%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 74.16%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 73.97%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.20%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 74.77%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 73.75%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.27%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 72.76%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 72.25%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.92%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 72.68%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.20%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 71.81%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 70.63%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.85%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 71.24%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 70.92%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 70.49%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 70.07%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 69.69%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 69.27%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 68.97%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 68.20%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 67.99%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 67.74%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 67.54%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 67.59%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.10%   [EVAL] batch:  194 | acc: 6.25%,  total acc: 68.78%   [EVAL] batch:  195 | acc: 6.25%,  total acc: 68.46%   [EVAL] batch:  196 | acc: 18.75%,  total acc: 68.21%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 67.59%   [EVAL] batch:  199 | acc: 12.50%,  total acc: 67.31%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 66.88%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.68%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 66.51%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 66.22%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 65.77%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 66.59%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 66.50%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 66.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 69.37%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 69.17%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  275 | acc: 31.25%,  total acc: 69.09%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.93%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 68.66%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 68.55%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 68.48%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 68.24%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 68.05%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 67.88%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 67.66%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 67.58%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 67.78%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 68.83%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.67%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 68.53%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 68.33%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 68.26%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.60%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 69.59%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 69.39%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 69.25%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 69.07%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 68.87%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 68.68%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 68.88%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 68.90%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 68.98%   
cur_acc:  ['0.9435', '0.7153', '0.7083', '0.8095', '0.6845', '0.6825']
his_acc:  ['0.9435', '0.8290', '0.7753', '0.7672', '0.7396', '0.6898']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  9.162040710449219 1.7635562419891357
CurrentTrain: epoch  0, batch     0 | loss: 9.1620407Losses:  10.891172409057617 1.8031630516052246
CurrentTrain: epoch  0, batch     1 | loss: 10.8911724Losses:  10.608770370483398 2.0115807056427
CurrentTrain: epoch  0, batch     2 | loss: 10.6087704Losses:  10.247593879699707 0.6697925329208374
CurrentTrain: epoch  0, batch     3 | loss: 10.2475939Losses:  10.09012508392334 2.1020493507385254
CurrentTrain: epoch  1, batch     0 | loss: 10.0901251Losses:  9.559762001037598 1.9296154975891113
CurrentTrain: epoch  1, batch     1 | loss: 9.5597620Losses:  9.051172256469727 2.058065176010132
CurrentTrain: epoch  1, batch     2 | loss: 9.0511723Losses:  8.107722282409668 0.6710420250892639
CurrentTrain: epoch  1, batch     3 | loss: 8.1077223Losses:  8.737987518310547 1.8910651206970215
CurrentTrain: epoch  2, batch     0 | loss: 8.7379875Losses:  8.739909172058105 2.046187400817871
CurrentTrain: epoch  2, batch     1 | loss: 8.7399092Losses:  9.183836936950684 2.2080941200256348
CurrentTrain: epoch  2, batch     2 | loss: 9.1838369Losses:  8.366897583007812 0.676846981048584
CurrentTrain: epoch  2, batch     3 | loss: 8.3668976Losses:  8.263276100158691 1.8403059244155884
CurrentTrain: epoch  3, batch     0 | loss: 8.2632761Losses:  8.798852920532227 2.0776870250701904
CurrentTrain: epoch  3, batch     1 | loss: 8.7988529Losses:  8.42680835723877 2.052067995071411
CurrentTrain: epoch  3, batch     2 | loss: 8.4268084Losses:  5.497851371765137 0.6719831824302673
CurrentTrain: epoch  3, batch     3 | loss: 5.4978514Losses:  8.866314888000488 2.042792797088623
CurrentTrain: epoch  4, batch     0 | loss: 8.8663149Losses:  7.07737398147583 1.8363912105560303
CurrentTrain: epoch  4, batch     1 | loss: 7.0773740Losses:  8.395406723022461 2.119497299194336
CurrentTrain: epoch  4, batch     2 | loss: 8.3954067Losses:  7.519108295440674 0.6937642097473145
CurrentTrain: epoch  4, batch     3 | loss: 7.5191083Losses:  7.543519020080566 1.6427674293518066
CurrentTrain: epoch  5, batch     0 | loss: 7.5435190Losses:  7.723538875579834 2.061183452606201
CurrentTrain: epoch  5, batch     1 | loss: 7.7235389Losses:  7.907819747924805 2.092428684234619
CurrentTrain: epoch  5, batch     2 | loss: 7.9078197Losses:  4.483757019042969 0.6653157472610474
CurrentTrain: epoch  5, batch     3 | loss: 4.4837570Losses:  7.995848655700684 1.9991707801818848
CurrentTrain: epoch  6, batch     0 | loss: 7.9958487Losses:  7.132546901702881 1.7356319427490234
CurrentTrain: epoch  6, batch     1 | loss: 7.1325469Losses:  6.465587615966797 1.8404167890548706
CurrentTrain: epoch  6, batch     2 | loss: 6.4655876Losses:  6.129912853240967 0.6696147322654724
CurrentTrain: epoch  6, batch     3 | loss: 6.1299129Losses:  6.934935569763184 1.9231932163238525
CurrentTrain: epoch  7, batch     0 | loss: 6.9349356Losses:  7.467886447906494 1.8911113739013672
CurrentTrain: epoch  7, batch     1 | loss: 7.4678864Losses:  7.400852203369141 2.0845766067504883
CurrentTrain: epoch  7, batch     2 | loss: 7.4008522Losses:  5.392138481140137 0.6834701895713806
CurrentTrain: epoch  7, batch     3 | loss: 5.3921385Losses:  7.04097843170166 1.9468703269958496
CurrentTrain: epoch  8, batch     0 | loss: 7.0409784Losses:  6.884716987609863 1.9246864318847656
CurrentTrain: epoch  8, batch     1 | loss: 6.8847170Losses:  7.618288040161133 2.2012133598327637
CurrentTrain: epoch  8, batch     2 | loss: 7.6182880Losses:  4.06443977355957 0.6822260618209839
CurrentTrain: epoch  8, batch     3 | loss: 4.0644398Losses:  6.995922088623047 2.1347317695617676
CurrentTrain: epoch  9, batch     0 | loss: 6.9959221Losses:  7.495460510253906 2.06400203704834
CurrentTrain: epoch  9, batch     1 | loss: 7.4954605Losses:  6.837559223175049 1.99351167678833
CurrentTrain: epoch  9, batch     2 | loss: 6.8375592Losses:  3.787189245223999 0.6858135461807251
CurrentTrain: epoch  9, batch     3 | loss: 3.7871892
Losses:  5.803388595581055 2.6743557453155518
MemoryTrain:  epoch  0, batch     0 | loss: 5.8033886Losses:  5.8300604820251465 2.6806087493896484
MemoryTrain:  epoch  0, batch     1 | loss: 5.8300605Losses:  5.996788024902344 2.693089723587036
MemoryTrain:  epoch  0, batch     2 | loss: 5.9967880Losses:  5.568673610687256 2.6939425468444824
MemoryTrain:  epoch  0, batch     3 | loss: 5.5686736Losses:  3.4263899326324463 1.649107813835144
MemoryTrain:  epoch  0, batch     4 | loss: 3.4263899Losses:  5.723322868347168 2.683260440826416
MemoryTrain:  epoch  1, batch     0 | loss: 5.7233229Losses:  5.948800086975098 2.653862953186035
MemoryTrain:  epoch  1, batch     1 | loss: 5.9488001Losses:  5.912118911743164 2.701551914215088
MemoryTrain:  epoch  1, batch     2 | loss: 5.9121189Losses:  5.938689231872559 2.6869754791259766
MemoryTrain:  epoch  1, batch     3 | loss: 5.9386892Losses:  3.5425069332122803 1.6866768598556519
MemoryTrain:  epoch  1, batch     4 | loss: 3.5425069Losses:  5.603172302246094 2.658341646194458
MemoryTrain:  epoch  2, batch     0 | loss: 5.6031723Losses:  5.59985876083374 2.6590752601623535
MemoryTrain:  epoch  2, batch     1 | loss: 5.5998588Losses:  5.487473011016846 2.677000045776367
MemoryTrain:  epoch  2, batch     2 | loss: 5.4874730Losses:  5.785902500152588 2.6926870346069336
MemoryTrain:  epoch  2, batch     3 | loss: 5.7859025Losses:  3.692924737930298 1.7281274795532227
MemoryTrain:  epoch  2, batch     4 | loss: 3.6929247Losses:  5.47975492477417 2.688046455383301
MemoryTrain:  epoch  3, batch     0 | loss: 5.4797549Losses:  5.425491809844971 2.665663242340088
MemoryTrain:  epoch  3, batch     1 | loss: 5.4254918Losses:  5.480551719665527 2.678799629211426
MemoryTrain:  epoch  3, batch     2 | loss: 5.4805517Losses:  5.472671031951904 2.659748077392578
MemoryTrain:  epoch  3, batch     3 | loss: 5.4726710Losses:  3.6593973636627197 1.7081416845321655
MemoryTrain:  epoch  3, batch     4 | loss: 3.6593974Losses:  5.418893814086914 2.6748290061950684
MemoryTrain:  epoch  4, batch     0 | loss: 5.4188938Losses:  5.409892559051514 2.654128074645996
MemoryTrain:  epoch  4, batch     1 | loss: 5.4098926Losses:  5.468265533447266 2.6978063583374023
MemoryTrain:  epoch  4, batch     2 | loss: 5.4682655Losses:  5.561861038208008 2.672060012817383
MemoryTrain:  epoch  4, batch     3 | loss: 5.5618610Losses:  3.4878628253936768 1.6994742155075073
MemoryTrain:  epoch  4, batch     4 | loss: 3.4878628Losses:  5.436939239501953 2.6774253845214844
MemoryTrain:  epoch  5, batch     0 | loss: 5.4369392Losses:  5.428167819976807 2.6757047176361084
MemoryTrain:  epoch  5, batch     1 | loss: 5.4281678Losses:  5.344942092895508 2.6386337280273438
MemoryTrain:  epoch  5, batch     2 | loss: 5.3449421Losses:  5.4521260261535645 2.7008190155029297
MemoryTrain:  epoch  5, batch     3 | loss: 5.4521260Losses:  3.430915117263794 1.7017693519592285
MemoryTrain:  epoch  5, batch     4 | loss: 3.4309151Losses:  5.34325647354126 2.6495931148529053
MemoryTrain:  epoch  6, batch     0 | loss: 5.3432565Losses:  5.414434909820557 2.6880409717559814
MemoryTrain:  epoch  6, batch     1 | loss: 5.4144349Losses:  5.4359564781188965 2.655817985534668
MemoryTrain:  epoch  6, batch     2 | loss: 5.4359565Losses:  5.429685115814209 2.6869330406188965
MemoryTrain:  epoch  6, batch     3 | loss: 5.4296851Losses:  3.4547088146209717 1.7002246379852295
MemoryTrain:  epoch  6, batch     4 | loss: 3.4547088Losses:  5.363950252532959 2.663778305053711
MemoryTrain:  epoch  7, batch     0 | loss: 5.3639503Losses:  5.393545150756836 2.6738688945770264
MemoryTrain:  epoch  7, batch     1 | loss: 5.3935452Losses:  5.4067230224609375 2.6641297340393066
MemoryTrain:  epoch  7, batch     2 | loss: 5.4067230Losses:  5.417388439178467 2.6743221282958984
MemoryTrain:  epoch  7, batch     3 | loss: 5.4173884Losses:  3.4089016914367676 1.6895062923431396
MemoryTrain:  epoch  7, batch     4 | loss: 3.4089017Losses:  5.386521339416504 2.6677021980285645
MemoryTrain:  epoch  8, batch     0 | loss: 5.3865213Losses:  5.340195178985596 2.65036678314209
MemoryTrain:  epoch  8, batch     1 | loss: 5.3401952Losses:  5.412931442260742 2.6809635162353516
MemoryTrain:  epoch  8, batch     2 | loss: 5.4129314Losses:  5.393218040466309 2.6741580963134766
MemoryTrain:  epoch  8, batch     3 | loss: 5.3932180Losses:  3.4215850830078125 1.6777509450912476
MemoryTrain:  epoch  8, batch     4 | loss: 3.4215851Losses:  5.445014476776123 2.6856112480163574
MemoryTrain:  epoch  9, batch     0 | loss: 5.4450145Losses:  5.41279411315918 2.677175283432007
MemoryTrain:  epoch  9, batch     1 | loss: 5.4127941Losses:  5.414516448974609 2.683321952819824
MemoryTrain:  epoch  9, batch     2 | loss: 5.4145164Losses:  5.2551164627075195 2.6131300926208496
MemoryTrain:  epoch  9, batch     3 | loss: 5.2551165Losses:  3.4757909774780273 1.7102928161621094
MemoryTrain:  epoch  9, batch     4 | loss: 3.4757910
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 51.56%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 50.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.56%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.76%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 45.09%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 42.29%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 40.93%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 42.99%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 44.12%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 45.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 47.97%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 49.01%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 49.84%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 51.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 51.98%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 53.78%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 55.71%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 57.53%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 57.60%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 58.02%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 58.48%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 57.87%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 57.42%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 57.29%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 57.16%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 56.45%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.99%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 86.05%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 85.31%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 83.40%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 82.74%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 82.13%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 81.83%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 81.06%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 80.50%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 79.96%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 79.62%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 80.66%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 80.67%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 80.11%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.97%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.61%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.27%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 78.69%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 77.35%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 76.74%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 76.08%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 75.71%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 75.56%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 74.67%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 73.96%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 73.47%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 73.36%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 72.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 72.61%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 72.45%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 72.26%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.17%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 71.79%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 71.18%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 70.58%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 70.00%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 69.43%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 68.97%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 68.81%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 70.44%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 70.41%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 71.30%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 71.04%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 70.62%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 70.17%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 69.67%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 69.19%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 68.88%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 69.70%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.24%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.79%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.38%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.94%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 67.55%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.21%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 67.84%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 67.42%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 67.02%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 66.62%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 66.22%   [EVAL] batch:  168 | acc: 12.50%,  total acc: 65.90%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 65.59%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 65.12%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 64.88%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 64.66%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 64.36%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 65.61%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:  196 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 64.99%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 64.62%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 64.59%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 64.49%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  206 | acc: 6.25%,  total acc: 64.10%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 63.85%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.15%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 62.88%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 62.88%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.05%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 63.81%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 63.77%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 63.76%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 63.78%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 66.93%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 67.12%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 67.09%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 66.71%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 66.78%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.63%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 66.17%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 65.98%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 65.79%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.60%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 65.40%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 65.19%   [EVAL] batch:  288 | acc: 0.00%,  total acc: 64.97%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 64.74%   [EVAL] batch:  290 | acc: 12.50%,  total acc: 64.56%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 64.34%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 63.99%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 63.96%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.38%   #############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  11.842476844787598 1.814839243888855
CurrentTrain: epoch  0, batch     0 | loss: 11.8424768Losses:  12.050819396972656 2.084536075592041
CurrentTrain: epoch  0, batch     1 | loss: 12.0508194Losses:  12.007551193237305 1.928164005279541
CurrentTrain: epoch  0, batch     2 | loss: 12.0075512Losses:  11.792034149169922 1.787132740020752
CurrentTrain: epoch  0, batch     3 | loss: 11.7920341Losses:  11.23896312713623 1.9895966053009033
CurrentTrain: epoch  0, batch     4 | loss: 11.2389631Losses:  12.134610176086426 1.6985269784927368
CurrentTrain: epoch  0, batch     5 | loss: 12.1346102Losses:  11.490281105041504 1.9353147745132446
CurrentTrain: epoch  0, batch     6 | loss: 11.4902811Losses:  10.83636474609375 1.8832135200500488
CurrentTrain: epoch  0, batch     7 | loss: 10.8363647Losses:  11.059054374694824 1.9479436874389648
CurrentTrain: epoch  0, batch     8 | loss: 11.0590544Losses:  11.658085823059082 1.9416007995605469
CurrentTrain: epoch  0, batch     9 | loss: 11.6580858Losses:  10.747320175170898 1.7773652076721191
CurrentTrain: epoch  0, batch    10 | loss: 10.7473202Losses:  10.196151733398438 1.7529921531677246
CurrentTrain: epoch  0, batch    11 | loss: 10.1961517Losses:  10.927364349365234 1.7981369495391846
CurrentTrain: epoch  0, batch    12 | loss: 10.9273643Losses:  10.982017517089844 1.9540138244628906
CurrentTrain: epoch  0, batch    13 | loss: 10.9820175Losses:  10.031285285949707 1.8748456239700317
CurrentTrain: epoch  0, batch    14 | loss: 10.0312853Losses:  10.907901763916016 2.066983938217163
CurrentTrain: epoch  0, batch    15 | loss: 10.9079018Losses:  10.839605331420898 1.612298846244812
CurrentTrain: epoch  0, batch    16 | loss: 10.8396053Losses:  11.08403491973877 2.0727195739746094
CurrentTrain: epoch  0, batch    17 | loss: 11.0840349Losses:  10.805499076843262 1.8302199840545654
CurrentTrain: epoch  0, batch    18 | loss: 10.8054991Losses:  10.667673110961914 2.0316224098205566
CurrentTrain: epoch  0, batch    19 | loss: 10.6676731Losses:  10.025519371032715 2.005660057067871
CurrentTrain: epoch  0, batch    20 | loss: 10.0255194Losses:  10.202306747436523 2.010693073272705
CurrentTrain: epoch  0, batch    21 | loss: 10.2023067Losses:  10.706371307373047 1.9620293378829956
CurrentTrain: epoch  0, batch    22 | loss: 10.7063713Losses:  10.09442138671875 1.9522156715393066
CurrentTrain: epoch  0, batch    23 | loss: 10.0944214Losses:  9.783859252929688 2.0204453468322754
CurrentTrain: epoch  0, batch    24 | loss: 9.7838593Losses:  9.930289268493652 1.9174416065216064
CurrentTrain: epoch  0, batch    25 | loss: 9.9302893Losses:  10.05358600616455 1.8378942012786865
CurrentTrain: epoch  0, batch    26 | loss: 10.0535860Losses:  9.811634063720703 2.0645084381103516
CurrentTrain: epoch  0, batch    27 | loss: 9.8116341Losses:  10.102511405944824 1.9012385606765747
CurrentTrain: epoch  0, batch    28 | loss: 10.1025114Losses:  10.161650657653809 2.0052411556243896
CurrentTrain: epoch  0, batch    29 | loss: 10.1616507Losses:  10.667319297790527 2.1163415908813477
CurrentTrain: epoch  0, batch    30 | loss: 10.6673193Losses:  9.834303855895996 1.826130986213684
CurrentTrain: epoch  0, batch    31 | loss: 9.8343039Losses:  9.462348937988281 1.4833521842956543
CurrentTrain: epoch  0, batch    32 | loss: 9.4623489Losses:  9.139413833618164 1.6853270530700684
CurrentTrain: epoch  0, batch    33 | loss: 9.1394138Losses:  9.739652633666992 1.8553767204284668
CurrentTrain: epoch  0, batch    34 | loss: 9.7396526Losses:  9.62643051147461 1.8952689170837402
CurrentTrain: epoch  0, batch    35 | loss: 9.6264305Losses:  9.649219512939453 1.9321620464324951
CurrentTrain: epoch  0, batch    36 | loss: 9.6492195Losses:  9.701850891113281 1.9541559219360352
CurrentTrain: epoch  0, batch    37 | loss: 9.7018509Losses:  10.653605461120605 2.150104522705078
CurrentTrain: epoch  0, batch    38 | loss: 10.6536055Losses:  10.206162452697754 2.188797950744629
CurrentTrain: epoch  0, batch    39 | loss: 10.2061625Losses:  10.488037109375 2.059664249420166
CurrentTrain: epoch  0, batch    40 | loss: 10.4880371Losses:  9.431059837341309 2.0084950923919678
CurrentTrain: epoch  0, batch    41 | loss: 9.4310598Losses:  9.243856430053711 1.9792513847351074
CurrentTrain: epoch  0, batch    42 | loss: 9.2438564Losses:  8.711710929870605 1.6890480518341064
CurrentTrain: epoch  0, batch    43 | loss: 8.7117109Losses:  9.114785194396973 1.776807188987732
CurrentTrain: epoch  0, batch    44 | loss: 9.1147852Losses:  9.387932777404785 2.05928373336792
CurrentTrain: epoch  0, batch    45 | loss: 9.3879328Losses:  9.960944175720215 1.8364933729171753
CurrentTrain: epoch  0, batch    46 | loss: 9.9609442Losses:  8.65060043334961 2.061201572418213
CurrentTrain: epoch  0, batch    47 | loss: 8.6506004Losses:  9.522762298583984 1.9488353729248047
CurrentTrain: epoch  0, batch    48 | loss: 9.5227623Losses:  9.241673469543457 1.9516613483428955
CurrentTrain: epoch  0, batch    49 | loss: 9.2416735Losses:  9.696640968322754 2.0117361545562744
CurrentTrain: epoch  0, batch    50 | loss: 9.6966410Losses:  9.344442367553711 2.126678705215454
CurrentTrain: epoch  0, batch    51 | loss: 9.3444424Losses:  8.920105934143066 1.9512412548065186
CurrentTrain: epoch  0, batch    52 | loss: 8.9201059Losses:  9.068425178527832 1.9251224994659424
CurrentTrain: epoch  0, batch    53 | loss: 9.0684252Losses:  9.810922622680664 2.073441982269287
CurrentTrain: epoch  0, batch    54 | loss: 9.8109226Losses:  8.861230850219727 1.8647339344024658
CurrentTrain: epoch  0, batch    55 | loss: 8.8612309Losses:  8.597923278808594 1.8384511470794678
CurrentTrain: epoch  0, batch    56 | loss: 8.5979233Losses:  9.51093864440918 2.0129776000976562
CurrentTrain: epoch  0, batch    57 | loss: 9.5109386Losses:  9.131394386291504 1.7639188766479492
CurrentTrain: epoch  0, batch    58 | loss: 9.1313944Losses:  8.77833366394043 1.747494101524353
CurrentTrain: epoch  0, batch    59 | loss: 8.7783337Losses:  8.489391326904297 1.9752180576324463
CurrentTrain: epoch  0, batch    60 | loss: 8.4893913Losses:  9.209113121032715 1.7812919616699219
CurrentTrain: epoch  0, batch    61 | loss: 9.2091131Losses:  10.392874717712402 1.4788389205932617
CurrentTrain: epoch  0, batch    62 | loss: 10.3928747Losses:  9.204902648925781 1.809571623802185
CurrentTrain: epoch  1, batch     0 | loss: 9.2049026Losses:  8.907155990600586 1.9121694564819336
CurrentTrain: epoch  1, batch     1 | loss: 8.9071560Losses:  8.536882400512695 1.8096996545791626
CurrentTrain: epoch  1, batch     2 | loss: 8.5368824Losses:  9.042474746704102 2.0902092456817627
CurrentTrain: epoch  1, batch     3 | loss: 9.0424747Losses:  9.322449684143066 2.04659104347229
CurrentTrain: epoch  1, batch     4 | loss: 9.3224497Losses:  8.700826644897461 1.9509023427963257
CurrentTrain: epoch  1, batch     5 | loss: 8.7008266Losses:  8.827150344848633 1.9396026134490967
CurrentTrain: epoch  1, batch     6 | loss: 8.8271503Losses:  8.244184494018555 2.0871615409851074
CurrentTrain: epoch  1, batch     7 | loss: 8.2441845Losses:  8.07765007019043 1.7620511054992676
CurrentTrain: epoch  1, batch     8 | loss: 8.0776501Losses:  9.020541191101074 2.0111982822418213
CurrentTrain: epoch  1, batch     9 | loss: 9.0205412Losses:  8.880316734313965 1.9629462957382202
CurrentTrain: epoch  1, batch    10 | loss: 8.8803167Losses:  8.522533416748047 1.979333758354187
CurrentTrain: epoch  1, batch    11 | loss: 8.5225334Losses:  8.842191696166992 2.0401625633239746
CurrentTrain: epoch  1, batch    12 | loss: 8.8421917Losses:  8.780844688415527 1.6696535348892212
CurrentTrain: epoch  1, batch    13 | loss: 8.7808447Losses:  8.22461223602295 1.9825774431228638
CurrentTrain: epoch  1, batch    14 | loss: 8.2246122Losses:  9.778413772583008 1.9074379205703735
CurrentTrain: epoch  1, batch    15 | loss: 9.7784138Losses:  8.377867698669434 1.8027313947677612
CurrentTrain: epoch  1, batch    16 | loss: 8.3778677Losses:  7.591700077056885 1.6416335105895996
CurrentTrain: epoch  1, batch    17 | loss: 7.5917001Losses:  8.259974479675293 1.8648788928985596
CurrentTrain: epoch  1, batch    18 | loss: 8.2599745Losses:  8.67973518371582 2.002899408340454
CurrentTrain: epoch  1, batch    19 | loss: 8.6797352Losses:  9.032608985900879 2.0926904678344727
CurrentTrain: epoch  1, batch    20 | loss: 9.0326090Losses:  9.028560638427734 2.0834341049194336
CurrentTrain: epoch  1, batch    21 | loss: 9.0285606Losses:  8.652705192565918 1.9222567081451416
CurrentTrain: epoch  1, batch    22 | loss: 8.6527052Losses:  8.628589630126953 1.751328468322754
CurrentTrain: epoch  1, batch    23 | loss: 8.6285896Losses:  8.48425006866455 1.9413213729858398
CurrentTrain: epoch  1, batch    24 | loss: 8.4842501Losses:  7.444192409515381 1.8738288879394531
CurrentTrain: epoch  1, batch    25 | loss: 7.4441924Losses:  8.253786087036133 2.0903496742248535
CurrentTrain: epoch  1, batch    26 | loss: 8.2537861Losses:  8.072662353515625 1.9716928005218506
CurrentTrain: epoch  1, batch    27 | loss: 8.0726624Losses:  8.546783447265625 2.00192928314209
CurrentTrain: epoch  1, batch    28 | loss: 8.5467834Losses:  8.017921447753906 2.084578514099121
CurrentTrain: epoch  1, batch    29 | loss: 8.0179214Losses:  8.215521812438965 1.9241721630096436
CurrentTrain: epoch  1, batch    30 | loss: 8.2155218Losses:  7.8177385330200195 2.1255698204040527
CurrentTrain: epoch  1, batch    31 | loss: 7.8177385Losses:  7.446661949157715 1.9932500123977661
CurrentTrain: epoch  1, batch    32 | loss: 7.4466619Losses:  7.66696834564209 1.754856824874878
CurrentTrain: epoch  1, batch    33 | loss: 7.6669683Losses:  8.769743919372559 2.0513672828674316
CurrentTrain: epoch  1, batch    34 | loss: 8.7697439Losses:  7.56900691986084 1.9088985919952393
CurrentTrain: epoch  1, batch    35 | loss: 7.5690069Losses:  8.644158363342285 1.743326187133789
CurrentTrain: epoch  1, batch    36 | loss: 8.6441584Losses:  8.662714958190918 2.027820348739624
CurrentTrain: epoch  1, batch    37 | loss: 8.6627150Losses:  9.170806884765625 2.088087558746338
CurrentTrain: epoch  1, batch    38 | loss: 9.1708069Losses:  7.607186794281006 2.0008344650268555
CurrentTrain: epoch  1, batch    39 | loss: 7.6071868Losses:  8.530963897705078 1.910135269165039
CurrentTrain: epoch  1, batch    40 | loss: 8.5309639Losses:  8.900434494018555 2.0279548168182373
CurrentTrain: epoch  1, batch    41 | loss: 8.9004345Losses:  8.83473014831543 1.9309046268463135
CurrentTrain: epoch  1, batch    42 | loss: 8.8347301Losses:  9.28320026397705 1.9554458856582642
CurrentTrain: epoch  1, batch    43 | loss: 9.2832003Losses:  9.173333168029785 1.8107013702392578
CurrentTrain: epoch  1, batch    44 | loss: 9.1733332Losses:  7.9486799240112305 2.059051513671875
CurrentTrain: epoch  1, batch    45 | loss: 7.9486799Losses:  7.461993217468262 1.9985249042510986
CurrentTrain: epoch  1, batch    46 | loss: 7.4619932Losses:  8.651922225952148 2.175816297531128
CurrentTrain: epoch  1, batch    47 | loss: 8.6519222Losses:  8.18465805053711 1.940359115600586
CurrentTrain: epoch  1, batch    48 | loss: 8.1846581Losses:  7.88604211807251 1.9651851654052734
CurrentTrain: epoch  1, batch    49 | loss: 7.8860421Losses:  8.113275527954102 1.9040074348449707
CurrentTrain: epoch  1, batch    50 | loss: 8.1132755Losses:  8.961837768554688 1.8081119060516357
CurrentTrain: epoch  1, batch    51 | loss: 8.9618378Losses:  8.167252540588379 1.882331132888794
CurrentTrain: epoch  1, batch    52 | loss: 8.1672525Losses:  8.294381141662598 1.913996934890747
CurrentTrain: epoch  1, batch    53 | loss: 8.2943811Losses:  8.277998924255371 1.9930942058563232
CurrentTrain: epoch  1, batch    54 | loss: 8.2779989Losses:  7.064650535583496 1.75571870803833
CurrentTrain: epoch  1, batch    55 | loss: 7.0646505Losses:  7.126308917999268 1.774575114250183
CurrentTrain: epoch  1, batch    56 | loss: 7.1263089Losses:  8.122598648071289 2.0221028327941895
CurrentTrain: epoch  1, batch    57 | loss: 8.1225986Losses:  7.327030181884766 1.8067662715911865
CurrentTrain: epoch  1, batch    58 | loss: 7.3270302Losses:  8.74747085571289 2.089717149734497
CurrentTrain: epoch  1, batch    59 | loss: 8.7474709Losses:  8.31513500213623 2.0364267826080322
CurrentTrain: epoch  1, batch    60 | loss: 8.3151350Losses:  7.859498500823975 2.059689998626709
CurrentTrain: epoch  1, batch    61 | loss: 7.8594985Losses:  6.204627990722656 1.4415380954742432
CurrentTrain: epoch  1, batch    62 | loss: 6.2046280Losses:  7.69551420211792 1.707320213317871
CurrentTrain: epoch  2, batch     0 | loss: 7.6955142Losses:  7.702177047729492 1.7160594463348389
CurrentTrain: epoch  2, batch     1 | loss: 7.7021770Losses:  7.912472248077393 1.9954160451889038
CurrentTrain: epoch  2, batch     2 | loss: 7.9124722Losses:  7.1299004554748535 1.939745306968689
CurrentTrain: epoch  2, batch     3 | loss: 7.1299005Losses:  7.565084457397461 1.9245893955230713
CurrentTrain: epoch  2, batch     4 | loss: 7.5650845Losses:  7.621514797210693 2.1624245643615723
CurrentTrain: epoch  2, batch     5 | loss: 7.6215148Losses:  7.226559638977051 1.7864618301391602
CurrentTrain: epoch  2, batch     6 | loss: 7.2265596Losses:  8.103114128112793 1.9737719297409058
CurrentTrain: epoch  2, batch     7 | loss: 8.1031141Losses:  8.661211967468262 1.961651086807251
CurrentTrain: epoch  2, batch     8 | loss: 8.6612120Losses:  7.167694091796875 1.8719890117645264
CurrentTrain: epoch  2, batch     9 | loss: 7.1676941Losses:  7.096608638763428 1.95515775680542
CurrentTrain: epoch  2, batch    10 | loss: 7.0966086Losses:  7.915074348449707 1.9014041423797607
CurrentTrain: epoch  2, batch    11 | loss: 7.9150743Losses:  8.1683988571167 1.8499025106430054
CurrentTrain: epoch  2, batch    12 | loss: 8.1683989Losses:  7.746176242828369 1.7992477416992188
CurrentTrain: epoch  2, batch    13 | loss: 7.7461762Losses:  7.675958633422852 2.0089523792266846
CurrentTrain: epoch  2, batch    14 | loss: 7.6759586Losses:  7.413869380950928 2.065680980682373
CurrentTrain: epoch  2, batch    15 | loss: 7.4138694Losses:  6.985501289367676 1.9459681510925293
CurrentTrain: epoch  2, batch    16 | loss: 6.9855013Losses:  6.397991180419922 1.6253912448883057
CurrentTrain: epoch  2, batch    17 | loss: 6.3979912Losses:  8.100761413574219 2.0958597660064697
CurrentTrain: epoch  2, batch    18 | loss: 8.1007614Losses:  7.484223365783691 1.9178217649459839
CurrentTrain: epoch  2, batch    19 | loss: 7.4842234Losses:  7.4251251220703125 1.9900777339935303
CurrentTrain: epoch  2, batch    20 | loss: 7.4251251Losses:  7.224624156951904 1.957651138305664
CurrentTrain: epoch  2, batch    21 | loss: 7.2246242Losses:  6.993169784545898 1.9534178972244263
CurrentTrain: epoch  2, batch    22 | loss: 6.9931698Losses:  7.672857284545898 2.0116941928863525
CurrentTrain: epoch  2, batch    23 | loss: 7.6728573Losses:  7.508341312408447 1.901741623878479
CurrentTrain: epoch  2, batch    24 | loss: 7.5083413Losses:  7.189576148986816 1.958855152130127
CurrentTrain: epoch  2, batch    25 | loss: 7.1895761Losses:  6.947353363037109 1.9456273317337036
CurrentTrain: epoch  2, batch    26 | loss: 6.9473534Losses:  6.993886947631836 1.7026481628417969
CurrentTrain: epoch  2, batch    27 | loss: 6.9938869Losses:  6.935817718505859 1.8344075679779053
CurrentTrain: epoch  2, batch    28 | loss: 6.9358177Losses:  7.5851593017578125 2.160628318786621
CurrentTrain: epoch  2, batch    29 | loss: 7.5851593Losses:  7.699089527130127 2.046319007873535
CurrentTrain: epoch  2, batch    30 | loss: 7.6990895Losses:  7.231139183044434 2.1992666721343994
CurrentTrain: epoch  2, batch    31 | loss: 7.2311392Losses:  8.096230506896973 1.8013324737548828
CurrentTrain: epoch  2, batch    32 | loss: 8.0962305Losses:  6.700657844543457 1.87388014793396
CurrentTrain: epoch  2, batch    33 | loss: 6.7006578Losses:  7.564215660095215 2.046187162399292
CurrentTrain: epoch  2, batch    34 | loss: 7.5642157Losses:  7.471749782562256 1.914487361907959
CurrentTrain: epoch  2, batch    35 | loss: 7.4717498Losses:  7.009872913360596 2.0374698638916016
CurrentTrain: epoch  2, batch    36 | loss: 7.0098729Losses:  7.196136474609375 1.7786636352539062
CurrentTrain: epoch  2, batch    37 | loss: 7.1961365Losses:  7.1381425857543945 1.8863862752914429
CurrentTrain: epoch  2, batch    38 | loss: 7.1381426Losses:  6.500626564025879 1.7845885753631592
CurrentTrain: epoch  2, batch    39 | loss: 6.5006266Losses:  7.144377708435059 1.700736403465271
CurrentTrain: epoch  2, batch    40 | loss: 7.1443777Losses:  7.482134819030762 1.527037501335144
CurrentTrain: epoch  2, batch    41 | loss: 7.4821348Losses:  7.127743721008301 1.958996057510376
CurrentTrain: epoch  2, batch    42 | loss: 7.1277437Losses:  7.330619812011719 1.9194681644439697
CurrentTrain: epoch  2, batch    43 | loss: 7.3306198Losses:  7.143360137939453 2.0461251735687256
CurrentTrain: epoch  2, batch    44 | loss: 7.1433601Losses:  7.444410800933838 1.9951328039169312
CurrentTrain: epoch  2, batch    45 | loss: 7.4444108Losses:  6.868649482727051 2.001312017440796
CurrentTrain: epoch  2, batch    46 | loss: 6.8686495Losses:  7.635652542114258 1.9923679828643799
CurrentTrain: epoch  2, batch    47 | loss: 7.6356525Losses:  6.797077178955078 1.914792776107788
CurrentTrain: epoch  2, batch    48 | loss: 6.7970772Losses:  7.427921772003174 1.9917031526565552
CurrentTrain: epoch  2, batch    49 | loss: 7.4279218Losses:  6.823642730712891 1.796123743057251
CurrentTrain: epoch  2, batch    50 | loss: 6.8236427Losses:  6.848962783813477 2.0406205654144287
CurrentTrain: epoch  2, batch    51 | loss: 6.8489628Losses:  6.918798446655273 1.8079216480255127
CurrentTrain: epoch  2, batch    52 | loss: 6.9187984Losses:  7.107071876525879 2.0840728282928467
CurrentTrain: epoch  2, batch    53 | loss: 7.1070719Losses:  6.900271415710449 2.0287556648254395
CurrentTrain: epoch  2, batch    54 | loss: 6.9002714Losses:  7.038078308105469 2.086422920227051
CurrentTrain: epoch  2, batch    55 | loss: 7.0380783Losses:  7.478127479553223 2.157115936279297
CurrentTrain: epoch  2, batch    56 | loss: 7.4781275Losses:  7.143101215362549 1.8697242736816406
CurrentTrain: epoch  2, batch    57 | loss: 7.1431012Losses:  6.862771034240723 1.8331019878387451
CurrentTrain: epoch  2, batch    58 | loss: 6.8627710Losses:  6.8508453369140625 1.9633362293243408
CurrentTrain: epoch  2, batch    59 | loss: 6.8508453Losses:  6.6513824462890625 1.9629874229431152
CurrentTrain: epoch  2, batch    60 | loss: 6.6513824Losses:  6.847687721252441 1.8675494194030762
CurrentTrain: epoch  2, batch    61 | loss: 6.8476877Losses:  6.532669544219971 1.6784416437149048
CurrentTrain: epoch  2, batch    62 | loss: 6.5326695Losses:  7.476659774780273 1.9245760440826416
CurrentTrain: epoch  3, batch     0 | loss: 7.4766598Losses:  7.880428314208984 1.830413579940796
CurrentTrain: epoch  3, batch     1 | loss: 7.8804283Losses:  6.966061592102051 2.027545690536499
CurrentTrain: epoch  3, batch     2 | loss: 6.9660616Losses:  7.1449995040893555 1.783807396888733
CurrentTrain: epoch  3, batch     3 | loss: 7.1449995Losses:  7.0928754806518555 1.9233770370483398
CurrentTrain: epoch  3, batch     4 | loss: 7.0928755Losses:  7.200592994689941 1.9249367713928223
CurrentTrain: epoch  3, batch     5 | loss: 7.2005930Losses:  6.854862213134766 1.9392110109329224
CurrentTrain: epoch  3, batch     6 | loss: 6.8548622Losses:  6.9860734939575195 2.0380516052246094
CurrentTrain: epoch  3, batch     7 | loss: 6.9860735Losses:  7.27792501449585 1.9489878416061401
CurrentTrain: epoch  3, batch     8 | loss: 7.2779250Losses:  7.3211517333984375 2.079859972000122
CurrentTrain: epoch  3, batch     9 | loss: 7.3211517Losses:  7.272343635559082 2.157606363296509
CurrentTrain: epoch  3, batch    10 | loss: 7.2723436Losses:  6.941084861755371 1.9347708225250244
CurrentTrain: epoch  3, batch    11 | loss: 6.9410849Losses:  6.413435935974121 1.8952791690826416
CurrentTrain: epoch  3, batch    12 | loss: 6.4134359Losses:  7.209834098815918 2.1573572158813477
CurrentTrain: epoch  3, batch    13 | loss: 7.2098341Losses:  6.777891159057617 2.00089955329895
CurrentTrain: epoch  3, batch    14 | loss: 6.7778912Losses:  6.731293678283691 2.0345704555511475
CurrentTrain: epoch  3, batch    15 | loss: 6.7312937Losses:  7.04965353012085 1.9864487648010254
CurrentTrain: epoch  3, batch    16 | loss: 7.0496535Losses:  7.047328948974609 1.763223648071289
CurrentTrain: epoch  3, batch    17 | loss: 7.0473289Losses:  6.952456474304199 1.9017717838287354
CurrentTrain: epoch  3, batch    18 | loss: 6.9524565Losses:  6.867568016052246 1.9255130290985107
CurrentTrain: epoch  3, batch    19 | loss: 6.8675680Losses:  6.746673107147217 1.939404010772705
CurrentTrain: epoch  3, batch    20 | loss: 6.7466731Losses:  6.860661506652832 2.069870948791504
CurrentTrain: epoch  3, batch    21 | loss: 6.8606615Losses:  6.664542198181152 1.7923612594604492
CurrentTrain: epoch  3, batch    22 | loss: 6.6645422Losses:  6.791345596313477 2.0707995891571045
CurrentTrain: epoch  3, batch    23 | loss: 6.7913456Losses:  6.399397850036621 1.6944739818572998
CurrentTrain: epoch  3, batch    24 | loss: 6.3993979Losses:  6.742654800415039 2.1559720039367676
CurrentTrain: epoch  3, batch    25 | loss: 6.7426548Losses:  6.9619035720825195 2.1642494201660156
CurrentTrain: epoch  3, batch    26 | loss: 6.9619036Losses:  6.746092796325684 2.0004079341888428
CurrentTrain: epoch  3, batch    27 | loss: 6.7460928Losses:  7.025834560394287 2.192406177520752
CurrentTrain: epoch  3, batch    28 | loss: 7.0258346Losses:  7.146549701690674 1.9109253883361816
CurrentTrain: epoch  3, batch    29 | loss: 7.1465497Losses:  7.57086181640625 2.1131601333618164
CurrentTrain: epoch  3, batch    30 | loss: 7.5708618Losses:  6.431690692901611 1.9209285974502563
CurrentTrain: epoch  3, batch    31 | loss: 6.4316907Losses:  6.571225166320801 1.7005081176757812
CurrentTrain: epoch  3, batch    32 | loss: 6.5712252Losses:  6.700929641723633 1.8674650192260742
CurrentTrain: epoch  3, batch    33 | loss: 6.7009296Losses:  6.551083087921143 1.941877841949463
CurrentTrain: epoch  3, batch    34 | loss: 6.5510831Losses:  6.640583038330078 2.013214349746704
CurrentTrain: epoch  3, batch    35 | loss: 6.6405830Losses:  7.4148759841918945 2.0195472240448
CurrentTrain: epoch  3, batch    36 | loss: 7.4148760Losses:  6.744830131530762 1.9550892114639282
CurrentTrain: epoch  3, batch    37 | loss: 6.7448301Losses:  6.539860725402832 2.0391242504119873
CurrentTrain: epoch  3, batch    38 | loss: 6.5398607Losses:  7.113828659057617 1.7073856592178345
CurrentTrain: epoch  3, batch    39 | loss: 7.1138287Losses:  6.715557098388672 2.0748496055603027
CurrentTrain: epoch  3, batch    40 | loss: 6.7155571Losses:  6.235450744628906 1.7871067523956299
CurrentTrain: epoch  3, batch    41 | loss: 6.2354507Losses:  6.837116241455078 1.7715013027191162
CurrentTrain: epoch  3, batch    42 | loss: 6.8371162Losses:  6.578184127807617 1.8611361980438232
CurrentTrain: epoch  3, batch    43 | loss: 6.5781841Losses:  7.016075134277344 2.074944496154785
CurrentTrain: epoch  3, batch    44 | loss: 7.0160751Losses:  6.762202262878418 1.827045202255249
CurrentTrain: epoch  3, batch    45 | loss: 6.7622023Losses:  6.460193157196045 2.0408072471618652
CurrentTrain: epoch  3, batch    46 | loss: 6.4601932Losses:  6.727693557739258 1.7821521759033203
CurrentTrain: epoch  3, batch    47 | loss: 6.7276936Losses:  6.192697048187256 1.6199017763137817
CurrentTrain: epoch  3, batch    48 | loss: 6.1926970Losses:  6.358242988586426 1.6554384231567383
CurrentTrain: epoch  3, batch    49 | loss: 6.3582430Losses:  6.593416213989258 2.0422821044921875
CurrentTrain: epoch  3, batch    50 | loss: 6.5934162Losses:  6.63404655456543 1.8019399642944336
CurrentTrain: epoch  3, batch    51 | loss: 6.6340466Losses:  6.465099334716797 1.9071877002716064
CurrentTrain: epoch  3, batch    52 | loss: 6.4650993Losses:  7.161343574523926 1.983694314956665
CurrentTrain: epoch  3, batch    53 | loss: 7.1613436Losses:  6.482709884643555 1.9851958751678467
CurrentTrain: epoch  3, batch    54 | loss: 6.4827099Losses:  6.791784286499023 2.1612093448638916
CurrentTrain: epoch  3, batch    55 | loss: 6.7917843Losses:  6.356518745422363 1.8909399509429932
CurrentTrain: epoch  3, batch    56 | loss: 6.3565187Losses:  6.774234771728516 1.9055376052856445
CurrentTrain: epoch  3, batch    57 | loss: 6.7742348Losses:  6.846308708190918 1.8815135955810547
CurrentTrain: epoch  3, batch    58 | loss: 6.8463087Losses:  6.263319492340088 1.6881426572799683
CurrentTrain: epoch  3, batch    59 | loss: 6.2633195Losses:  6.645512580871582 2.098146438598633
CurrentTrain: epoch  3, batch    60 | loss: 6.6455126Losses:  6.397573471069336 1.9674348831176758
CurrentTrain: epoch  3, batch    61 | loss: 6.3975735Losses:  5.923160552978516 1.4970190525054932
CurrentTrain: epoch  3, batch    62 | loss: 5.9231606Losses:  5.963683605194092 1.5710057020187378
CurrentTrain: epoch  4, batch     0 | loss: 5.9636836Losses:  6.279517650604248 1.7974505424499512
CurrentTrain: epoch  4, batch     1 | loss: 6.2795177Losses:  6.4772162437438965 1.940548300743103
CurrentTrain: epoch  4, batch     2 | loss: 6.4772162Losses:  6.11856746673584 1.752232551574707
CurrentTrain: epoch  4, batch     3 | loss: 6.1185675Losses:  6.701760292053223 2.164696455001831
CurrentTrain: epoch  4, batch     4 | loss: 6.7017603Losses:  6.805443286895752 2.1628384590148926
CurrentTrain: epoch  4, batch     5 | loss: 6.8054433Losses:  6.383368492126465 1.8850295543670654
CurrentTrain: epoch  4, batch     6 | loss: 6.3833685Losses:  6.165342330932617 1.7392241954803467
CurrentTrain: epoch  4, batch     7 | loss: 6.1653423Losses:  6.566054344177246 2.151340961456299
CurrentTrain: epoch  4, batch     8 | loss: 6.5660543Losses:  6.350749969482422 1.9141690731048584
CurrentTrain: epoch  4, batch     9 | loss: 6.3507500Losses:  6.353532791137695 1.8708646297454834
CurrentTrain: epoch  4, batch    10 | loss: 6.3535328Losses:  6.823908805847168 1.910341739654541
CurrentTrain: epoch  4, batch    11 | loss: 6.8239088Losses:  6.442854881286621 1.9089291095733643
CurrentTrain: epoch  4, batch    12 | loss: 6.4428549Losses:  6.743933200836182 2.033412456512451
CurrentTrain: epoch  4, batch    13 | loss: 6.7439332Losses:  5.9629387855529785 1.73390531539917
CurrentTrain: epoch  4, batch    14 | loss: 5.9629388Losses:  6.259205341339111 1.8303618431091309
CurrentTrain: epoch  4, batch    15 | loss: 6.2592053Losses:  6.326885223388672 1.8705236911773682
CurrentTrain: epoch  4, batch    16 | loss: 6.3268852Losses:  6.206482410430908 1.8275485038757324
CurrentTrain: epoch  4, batch    17 | loss: 6.2064824Losses:  6.433073043823242 2.0273044109344482
CurrentTrain: epoch  4, batch    18 | loss: 6.4330730Losses:  6.426994323730469 2.0038461685180664
CurrentTrain: epoch  4, batch    19 | loss: 6.4269943Losses:  6.655346870422363 1.7982492446899414
CurrentTrain: epoch  4, batch    20 | loss: 6.6553469Losses:  6.3464813232421875 1.6854634284973145
CurrentTrain: epoch  4, batch    21 | loss: 6.3464813Losses:  6.088422775268555 1.825981855392456
CurrentTrain: epoch  4, batch    22 | loss: 6.0884228Losses:  5.81682014465332 1.5716133117675781
CurrentTrain: epoch  4, batch    23 | loss: 5.8168201Losses:  6.132739067077637 1.5913159847259521
CurrentTrain: epoch  4, batch    24 | loss: 6.1327391Losses:  6.485926628112793 2.0734498500823975
CurrentTrain: epoch  4, batch    25 | loss: 6.4859266Losses:  6.232024192810059 1.949669599533081
CurrentTrain: epoch  4, batch    26 | loss: 6.2320242Losses:  6.176937103271484 1.8555314540863037
CurrentTrain: epoch  4, batch    27 | loss: 6.1769371Losses:  6.278855800628662 1.852016568183899
CurrentTrain: epoch  4, batch    28 | loss: 6.2788558Losses:  6.23075008392334 1.840322494506836
CurrentTrain: epoch  4, batch    29 | loss: 6.2307501Losses:  6.1772918701171875 1.9101382493972778
CurrentTrain: epoch  4, batch    30 | loss: 6.1772919Losses:  6.281006813049316 1.8661822080612183
CurrentTrain: epoch  4, batch    31 | loss: 6.2810068Losses:  6.39235258102417 1.938157558441162
CurrentTrain: epoch  4, batch    32 | loss: 6.3923526Losses:  6.486718654632568 2.035372734069824
CurrentTrain: epoch  4, batch    33 | loss: 6.4867187Losses:  6.2415642738342285 1.9034357070922852
CurrentTrain: epoch  4, batch    34 | loss: 6.2415643Losses:  6.660155773162842 2.073944091796875
CurrentTrain: epoch  4, batch    35 | loss: 6.6601558Losses:  6.165974140167236 1.7441619634628296
CurrentTrain: epoch  4, batch    36 | loss: 6.1659741Losses:  6.810032367706299 2.0360398292541504
CurrentTrain: epoch  4, batch    37 | loss: 6.8100324Losses:  6.310111045837402 1.872681736946106
CurrentTrain: epoch  4, batch    38 | loss: 6.3101110Losses:  6.226910591125488 1.9166123867034912
CurrentTrain: epoch  4, batch    39 | loss: 6.2269106Losses:  6.121952533721924 1.8511935472488403
CurrentTrain: epoch  4, batch    40 | loss: 6.1219525Losses:  6.21600341796875 1.971665620803833
CurrentTrain: epoch  4, batch    41 | loss: 6.2160034Losses:  6.626326560974121 1.908435583114624
CurrentTrain: epoch  4, batch    42 | loss: 6.6263266Losses:  6.430699825286865 1.6286611557006836
CurrentTrain: epoch  4, batch    43 | loss: 6.4306998Losses:  5.883249282836914 1.6962058544158936
CurrentTrain: epoch  4, batch    44 | loss: 5.8832493Losses:  6.741084098815918 1.7400130033493042
CurrentTrain: epoch  4, batch    45 | loss: 6.7410841Losses:  6.404560565948486 1.9205611944198608
CurrentTrain: epoch  4, batch    46 | loss: 6.4045606Losses:  6.369440078735352 1.9335510730743408
CurrentTrain: epoch  4, batch    47 | loss: 6.3694401Losses:  6.373127460479736 2.059582233428955
CurrentTrain: epoch  4, batch    48 | loss: 6.3731275Losses:  6.305852890014648 1.9639892578125
CurrentTrain: epoch  4, batch    49 | loss: 6.3058529Losses:  6.5686540603637695 2.148531436920166
CurrentTrain: epoch  4, batch    50 | loss: 6.5686541Losses:  6.189065933227539 1.828552007675171
CurrentTrain: epoch  4, batch    51 | loss: 6.1890659Losses:  6.01529598236084 1.7217930555343628
CurrentTrain: epoch  4, batch    52 | loss: 6.0152960Losses:  6.5785417556762695 1.8992955684661865
CurrentTrain: epoch  4, batch    53 | loss: 6.5785418Losses:  6.176633358001709 1.7845209836959839
CurrentTrain: epoch  4, batch    54 | loss: 6.1766334Losses:  6.262395858764648 1.966766357421875
CurrentTrain: epoch  4, batch    55 | loss: 6.2623959Losses:  6.0953369140625 1.7727845907211304
CurrentTrain: epoch  4, batch    56 | loss: 6.0953369Losses:  6.495029449462891 2.1497602462768555
CurrentTrain: epoch  4, batch    57 | loss: 6.4950294Losses:  6.20175838470459 1.894019365310669
CurrentTrain: epoch  4, batch    58 | loss: 6.2017584Losses:  6.196990013122559 1.9025442600250244
CurrentTrain: epoch  4, batch    59 | loss: 6.1969900Losses:  6.183998107910156 1.8891351222991943
CurrentTrain: epoch  4, batch    60 | loss: 6.1839981Losses:  6.3659586906433105 1.9673336744308472
CurrentTrain: epoch  4, batch    61 | loss: 6.3659587Losses:  5.860469818115234 1.517516851425171
CurrentTrain: epoch  4, batch    62 | loss: 5.8604698Losses:  6.545429706573486 2.1167120933532715
CurrentTrain: epoch  5, batch     0 | loss: 6.5454297Losses:  6.3202667236328125 2.0101797580718994
CurrentTrain: epoch  5, batch     1 | loss: 6.3202667Losses:  6.037833213806152 1.7478303909301758
CurrentTrain: epoch  5, batch     2 | loss: 6.0378332Losses:  6.1747331619262695 1.8942821025848389
CurrentTrain: epoch  5, batch     3 | loss: 6.1747332Losses:  6.233630657196045 1.8431048393249512
CurrentTrain: epoch  5, batch     4 | loss: 6.2336307Losses:  6.2510600090026855 1.8887486457824707
CurrentTrain: epoch  5, batch     5 | loss: 6.2510600Losses:  6.3153977394104 1.9792680740356445
CurrentTrain: epoch  5, batch     6 | loss: 6.3153977Losses:  6.241885185241699 2.0074970722198486
CurrentTrain: epoch  5, batch     7 | loss: 6.2418852Losses:  6.338268280029297 1.9878385066986084
CurrentTrain: epoch  5, batch     8 | loss: 6.3382683Losses:  5.959271430969238 1.7188575267791748
CurrentTrain: epoch  5, batch     9 | loss: 5.9592714Losses:  6.654481887817383 2.051091194152832
CurrentTrain: epoch  5, batch    10 | loss: 6.6544819Losses:  6.355537414550781 2.067058563232422
CurrentTrain: epoch  5, batch    11 | loss: 6.3555374Losses:  6.1373748779296875 1.927170991897583
CurrentTrain: epoch  5, batch    12 | loss: 6.1373749Losses:  6.078627586364746 1.8480825424194336
CurrentTrain: epoch  5, batch    13 | loss: 6.0786276Losses:  6.341872215270996 2.1710357666015625
CurrentTrain: epoch  5, batch    14 | loss: 6.3418722Losses:  6.124692440032959 1.8965907096862793
CurrentTrain: epoch  5, batch    15 | loss: 6.1246924Losses:  6.431621551513672 2.114386558532715
CurrentTrain: epoch  5, batch    16 | loss: 6.4316216Losses:  6.248984336853027 2.0539865493774414
CurrentTrain: epoch  5, batch    17 | loss: 6.2489843Losses:  6.249985694885254 1.9805324077606201
CurrentTrain: epoch  5, batch    18 | loss: 6.2499857Losses:  6.15043306350708 1.9098438024520874
CurrentTrain: epoch  5, batch    19 | loss: 6.1504331Losses:  6.128676414489746 1.9302380084991455
CurrentTrain: epoch  5, batch    20 | loss: 6.1286764Losses:  6.152728080749512 1.8861815929412842
CurrentTrain: epoch  5, batch    21 | loss: 6.1527281Losses:  6.082168102264404 1.8049769401550293
CurrentTrain: epoch  5, batch    22 | loss: 6.0821681Losses:  6.2071404457092285 1.89728581905365
CurrentTrain: epoch  5, batch    23 | loss: 6.2071404Losses:  6.036639213562012 1.8046824932098389
CurrentTrain: epoch  5, batch    24 | loss: 6.0366392Losses:  6.232835292816162 2.022465705871582
CurrentTrain: epoch  5, batch    25 | loss: 6.2328353Losses:  6.255740165710449 1.982576608657837
CurrentTrain: epoch  5, batch    26 | loss: 6.2557402Losses:  6.104483604431152 1.8615938425064087
CurrentTrain: epoch  5, batch    27 | loss: 6.1044836Losses:  6.235892295837402 2.0569684505462646
CurrentTrain: epoch  5, batch    28 | loss: 6.2358923Losses:  6.1950836181640625 2.011227607727051
CurrentTrain: epoch  5, batch    29 | loss: 6.1950836Losses:  5.939106464385986 1.762498378753662
CurrentTrain: epoch  5, batch    30 | loss: 5.9391065Losses:  6.016161918640137 1.840996265411377
CurrentTrain: epoch  5, batch    31 | loss: 6.0161619Losses:  6.099714279174805 1.8433029651641846
CurrentTrain: epoch  5, batch    32 | loss: 6.0997143Losses:  5.755227565765381 1.628519058227539
CurrentTrain: epoch  5, batch    33 | loss: 5.7552276Losses:  6.1145477294921875 1.8841067552566528
CurrentTrain: epoch  5, batch    34 | loss: 6.1145477Losses:  5.899409294128418 1.7581684589385986
CurrentTrain: epoch  5, batch    35 | loss: 5.8994093Losses:  5.939883708953857 1.7582308053970337
CurrentTrain: epoch  5, batch    36 | loss: 5.9398837Losses:  5.967713356018066 1.8381094932556152
CurrentTrain: epoch  5, batch    37 | loss: 5.9677134Losses:  6.041234016418457 1.8353402614593506
CurrentTrain: epoch  5, batch    38 | loss: 6.0412340Losses:  6.269656181335449 1.9978455305099487
CurrentTrain: epoch  5, batch    39 | loss: 6.2696562Losses:  6.250511169433594 2.12785267829895
CurrentTrain: epoch  5, batch    40 | loss: 6.2505112Losses:  6.046882629394531 1.8560590744018555
CurrentTrain: epoch  5, batch    41 | loss: 6.0468826Losses:  6.358898162841797 2.0299272537231445
CurrentTrain: epoch  5, batch    42 | loss: 6.3588982Losses:  6.289681434631348 1.8297778367996216
CurrentTrain: epoch  5, batch    43 | loss: 6.2896814Losses:  6.08758544921875 1.9164248704910278
CurrentTrain: epoch  5, batch    44 | loss: 6.0875854Losses:  6.032378196716309 1.8994252681732178
CurrentTrain: epoch  5, batch    45 | loss: 6.0323782Losses:  6.1249470710754395 1.9700660705566406
CurrentTrain: epoch  5, batch    46 | loss: 6.1249471Losses:  6.143671989440918 1.9856873750686646
CurrentTrain: epoch  5, batch    47 | loss: 6.1436720Losses:  6.211911201477051 2.0243594646453857
CurrentTrain: epoch  5, batch    48 | loss: 6.2119112Losses:  6.165070056915283 1.993371605873108
CurrentTrain: epoch  5, batch    49 | loss: 6.1650701Losses:  6.082417011260986 1.9678711891174316
CurrentTrain: epoch  5, batch    50 | loss: 6.0824170Losses:  5.928217887878418 1.7865676879882812
CurrentTrain: epoch  5, batch    51 | loss: 5.9282179Losses:  6.30780029296875 2.0945394039154053
CurrentTrain: epoch  5, batch    52 | loss: 6.3078003Losses:  6.110118389129639 1.9279676675796509
CurrentTrain: epoch  5, batch    53 | loss: 6.1101184Losses:  6.220355033874512 2.0992095470428467
CurrentTrain: epoch  5, batch    54 | loss: 6.2203550Losses:  6.233237266540527 1.985289216041565
CurrentTrain: epoch  5, batch    55 | loss: 6.2332373Losses:  6.263902187347412 2.098209857940674
CurrentTrain: epoch  5, batch    56 | loss: 6.2639022Losses:  6.032504081726074 1.768798589706421
CurrentTrain: epoch  5, batch    57 | loss: 6.0325041Losses:  6.212115287780762 1.837775468826294
CurrentTrain: epoch  5, batch    58 | loss: 6.2121153Losses:  5.996496200561523 1.7717245817184448
CurrentTrain: epoch  5, batch    59 | loss: 5.9964962Losses:  6.00124979019165 1.877764105796814
CurrentTrain: epoch  5, batch    60 | loss: 6.0012498Losses:  5.954736709594727 1.8255136013031006
CurrentTrain: epoch  5, batch    61 | loss: 5.9547367Losses:  5.898977756500244 1.6695376634597778
CurrentTrain: epoch  5, batch    62 | loss: 5.8989778Losses:  6.097095489501953 1.8783928155899048
CurrentTrain: epoch  6, batch     0 | loss: 6.0970955Losses:  6.081559181213379 1.9723680019378662
CurrentTrain: epoch  6, batch     1 | loss: 6.0815592Losses:  5.9189958572387695 1.8339312076568604
CurrentTrain: epoch  6, batch     2 | loss: 5.9189959Losses:  6.502013206481934 2.1644458770751953
CurrentTrain: epoch  6, batch     3 | loss: 6.5020132Losses:  5.8566203117370605 1.7007756233215332
CurrentTrain: epoch  6, batch     4 | loss: 5.8566203Losses:  6.169061660766602 2.014821767807007
CurrentTrain: epoch  6, batch     5 | loss: 6.1690617Losses:  6.1747846603393555 2.0161561965942383
CurrentTrain: epoch  6, batch     6 | loss: 6.1747847Losses:  6.041707992553711 1.8730669021606445
CurrentTrain: epoch  6, batch     7 | loss: 6.0417080Losses:  6.145252227783203 2.0368525981903076
CurrentTrain: epoch  6, batch     8 | loss: 6.1452522Losses:  5.77689790725708 1.6453676223754883
CurrentTrain: epoch  6, batch     9 | loss: 5.7768979Losses:  6.15043830871582 2.0068576335906982
CurrentTrain: epoch  6, batch    10 | loss: 6.1504383Losses:  5.860518455505371 1.6926852464675903
CurrentTrain: epoch  6, batch    11 | loss: 5.8605185Losses:  6.298365592956543 2.1056156158447266
CurrentTrain: epoch  6, batch    12 | loss: 6.2983656Losses:  5.896660327911377 1.7768640518188477
CurrentTrain: epoch  6, batch    13 | loss: 5.8966603Losses:  6.226143836975098 2.073993682861328
CurrentTrain: epoch  6, batch    14 | loss: 6.2261438Losses:  6.125782012939453 1.9874072074890137
CurrentTrain: epoch  6, batch    15 | loss: 6.1257820Losses:  6.177053451538086 2.0298733711242676
CurrentTrain: epoch  6, batch    16 | loss: 6.1770535Losses:  6.143899440765381 2.014676570892334
CurrentTrain: epoch  6, batch    17 | loss: 6.1438994Losses:  5.512620449066162 1.3622679710388184
CurrentTrain: epoch  6, batch    18 | loss: 5.5126204Losses:  6.134711742401123 2.0190367698669434
CurrentTrain: epoch  6, batch    19 | loss: 6.1347117Losses:  6.182116985321045 2.0641016960144043
CurrentTrain: epoch  6, batch    20 | loss: 6.1821170Losses:  6.077670574188232 1.8920559883117676
CurrentTrain: epoch  6, batch    21 | loss: 6.0776706Losses:  5.943178176879883 1.7580678462982178
CurrentTrain: epoch  6, batch    22 | loss: 5.9431782Losses:  5.968833923339844 1.5745693445205688
CurrentTrain: epoch  6, batch    23 | loss: 5.9688339Losses:  5.968324184417725 1.8030060529708862
CurrentTrain: epoch  6, batch    24 | loss: 5.9683242Losses:  6.227429389953613 2.0204455852508545
CurrentTrain: epoch  6, batch    25 | loss: 6.2274294Losses:  6.170871257781982 2.0099587440490723
CurrentTrain: epoch  6, batch    26 | loss: 6.1708713Losses:  5.897867202758789 1.8045802116394043
CurrentTrain: epoch  6, batch    27 | loss: 5.8978672Losses:  6.038293838500977 1.9588398933410645
CurrentTrain: epoch  6, batch    28 | loss: 6.0382938Losses:  6.13047456741333 2.0005393028259277
CurrentTrain: epoch  6, batch    29 | loss: 6.1304746Losses:  6.093836784362793 2.001671314239502
CurrentTrain: epoch  6, batch    30 | loss: 6.0938368Losses:  5.652715682983398 1.619522213935852
CurrentTrain: epoch  6, batch    31 | loss: 5.6527157Losses:  5.961115837097168 1.8065481185913086
CurrentTrain: epoch  6, batch    32 | loss: 5.9611158Losses:  6.151843070983887 2.010615587234497
CurrentTrain: epoch  6, batch    33 | loss: 6.1518431Losses:  5.989601135253906 1.8491907119750977
CurrentTrain: epoch  6, batch    34 | loss: 5.9896011Losses:  5.820245742797852 1.6349830627441406
CurrentTrain: epoch  6, batch    35 | loss: 5.8202457Losses:  5.678437232971191 1.4949994087219238
CurrentTrain: epoch  6, batch    36 | loss: 5.6784372Losses:  6.154698371887207 2.018407106399536
CurrentTrain: epoch  6, batch    37 | loss: 6.1546984Losses:  6.256526947021484 2.1041769981384277
CurrentTrain: epoch  6, batch    38 | loss: 6.2565269Losses:  5.8782734870910645 1.8028473854064941
CurrentTrain: epoch  6, batch    39 | loss: 5.8782735Losses:  6.216381072998047 2.0528087615966797
CurrentTrain: epoch  6, batch    40 | loss: 6.2163811Losses:  6.228623390197754 2.1190848350524902
CurrentTrain: epoch  6, batch    41 | loss: 6.2286234Losses:  6.12226676940918 2.0335681438446045
CurrentTrain: epoch  6, batch    42 | loss: 6.1222668Losses:  5.839366912841797 1.7944202423095703
CurrentTrain: epoch  6, batch    43 | loss: 5.8393669Losses:  5.963052749633789 1.8689355850219727
CurrentTrain: epoch  6, batch    44 | loss: 5.9630527Losses:  6.099574089050293 1.9636380672454834
CurrentTrain: epoch  6, batch    45 | loss: 6.0995741Losses:  5.924201011657715 1.8191869258880615
CurrentTrain: epoch  6, batch    46 | loss: 5.9242010Losses:  5.894601345062256 1.7713494300842285
CurrentTrain: epoch  6, batch    47 | loss: 5.8946013Losses:  6.233148574829102 2.1311771869659424
CurrentTrain: epoch  6, batch    48 | loss: 6.2331486Losses:  5.80524206161499 1.7025891542434692
CurrentTrain: epoch  6, batch    49 | loss: 5.8052421Losses:  6.200382232666016 2.079198122024536
CurrentTrain: epoch  6, batch    50 | loss: 6.2003822Losses:  6.146166801452637 2.0434722900390625
CurrentTrain: epoch  6, batch    51 | loss: 6.1461668Losses:  6.116153717041016 2.0105364322662354
CurrentTrain: epoch  6, batch    52 | loss: 6.1161537Losses:  5.543699264526367 1.4200563430786133
CurrentTrain: epoch  6, batch    53 | loss: 5.5436993Losses:  6.037080764770508 1.9508154392242432
CurrentTrain: epoch  6, batch    54 | loss: 6.0370808Losses:  5.850000381469727 1.7878073453903198
CurrentTrain: epoch  6, batch    55 | loss: 5.8500004Losses:  5.778687477111816 1.63460373878479
CurrentTrain: epoch  6, batch    56 | loss: 5.7786875Losses:  6.096986293792725 2.0368504524230957
CurrentTrain: epoch  6, batch    57 | loss: 6.0969863Losses:  6.00502872467041 1.9194097518920898
CurrentTrain: epoch  6, batch    58 | loss: 6.0050287Losses:  6.069512844085693 1.989417552947998
CurrentTrain: epoch  6, batch    59 | loss: 6.0695128Losses:  5.75128698348999 1.6229124069213867
CurrentTrain: epoch  6, batch    60 | loss: 5.7512870Losses:  6.0103302001953125 1.8631553649902344
CurrentTrain: epoch  6, batch    61 | loss: 6.0103302Losses:  5.642908573150635 1.574196457862854
CurrentTrain: epoch  6, batch    62 | loss: 5.6429086Losses:  5.892693042755127 1.7844700813293457
CurrentTrain: epoch  7, batch     0 | loss: 5.8926930Losses:  5.844647407531738 1.742202639579773
CurrentTrain: epoch  7, batch     1 | loss: 5.8446474Losses:  5.951763153076172 1.8603146076202393
CurrentTrain: epoch  7, batch     2 | loss: 5.9517632Losses:  6.139969825744629 2.050111770629883
CurrentTrain: epoch  7, batch     3 | loss: 6.1399698Losses:  5.799535751342773 1.6976301670074463
CurrentTrain: epoch  7, batch     4 | loss: 5.7995358Losses:  6.0459160804748535 1.987754464149475
CurrentTrain: epoch  7, batch     5 | loss: 6.0459161Losses:  6.049531936645508 1.9805681705474854
CurrentTrain: epoch  7, batch     6 | loss: 6.0495319Losses:  6.1743879318237305 2.0174403190612793
CurrentTrain: epoch  7, batch     7 | loss: 6.1743879Losses:  5.75605583190918 1.7375001907348633
CurrentTrain: epoch  7, batch     8 | loss: 5.7560558Losses:  5.864658355712891 1.8069632053375244
CurrentTrain: epoch  7, batch     9 | loss: 5.8646584Losses:  5.943428039550781 1.890944480895996
CurrentTrain: epoch  7, batch    10 | loss: 5.9434280Losses:  5.839141845703125 1.8106050491333008
CurrentTrain: epoch  7, batch    11 | loss: 5.8391418Losses:  5.667234897613525 1.6306298971176147
CurrentTrain: epoch  7, batch    12 | loss: 5.6672349Losses:  5.866125106811523 1.7921130657196045
CurrentTrain: epoch  7, batch    13 | loss: 5.8661251Losses:  5.8273115158081055 1.7900911569595337
CurrentTrain: epoch  7, batch    14 | loss: 5.8273115Losses:  5.883145332336426 1.8750700950622559
CurrentTrain: epoch  7, batch    15 | loss: 5.8831453Losses:  5.945916175842285 1.8485381603240967
CurrentTrain: epoch  7, batch    16 | loss: 5.9459162Losses:  6.201873302459717 1.8662447929382324
CurrentTrain: epoch  7, batch    17 | loss: 6.2018733Losses:  6.111073017120361 2.0463290214538574
CurrentTrain: epoch  7, batch    18 | loss: 6.1110730Losses:  6.104266166687012 1.9901907444000244
CurrentTrain: epoch  7, batch    19 | loss: 6.1042662Losses:  5.860480308532715 1.7614699602127075
CurrentTrain: epoch  7, batch    20 | loss: 5.8604803Losses:  5.826798915863037 1.7944717407226562
CurrentTrain: epoch  7, batch    21 | loss: 5.8267989Losses:  5.764168739318848 1.7002112865447998
CurrentTrain: epoch  7, batch    22 | loss: 5.7641687Losses:  5.709629058837891 1.6359601020812988
CurrentTrain: epoch  7, batch    23 | loss: 5.7096291Losses:  5.893901824951172 1.8497211933135986
CurrentTrain: epoch  7, batch    24 | loss: 5.8939018Losses:  5.808868885040283 1.631182074546814
CurrentTrain: epoch  7, batch    25 | loss: 5.8088689Losses:  5.83181095123291 1.7932536602020264
CurrentTrain: epoch  7, batch    26 | loss: 5.8318110Losses:  6.011163234710693 1.956079125404358
CurrentTrain: epoch  7, batch    27 | loss: 6.0111632Losses:  5.943269729614258 1.8713366985321045
CurrentTrain: epoch  7, batch    28 | loss: 5.9432697Losses:  5.843230724334717 1.7698780298233032
CurrentTrain: epoch  7, batch    29 | loss: 5.8432307Losses:  6.011826992034912 1.9132499694824219
CurrentTrain: epoch  7, batch    30 | loss: 6.0118270Losses:  5.9453229904174805 1.871896505355835
CurrentTrain: epoch  7, batch    31 | loss: 5.9453230Losses:  5.693065643310547 1.6182582378387451
CurrentTrain: epoch  7, batch    32 | loss: 5.6930656Losses:  5.936695098876953 1.7640092372894287
CurrentTrain: epoch  7, batch    33 | loss: 5.9366951Losses:  5.770246505737305 1.6620137691497803
CurrentTrain: epoch  7, batch    34 | loss: 5.7702465Losses:  5.997293472290039 1.931796908378601
CurrentTrain: epoch  7, batch    35 | loss: 5.9972935Losses:  5.970593452453613 1.8389966487884521
CurrentTrain: epoch  7, batch    36 | loss: 5.9705935Losses:  5.82274055480957 1.7847905158996582
CurrentTrain: epoch  7, batch    37 | loss: 5.8227406Losses:  6.098864555358887 2.0269787311553955
CurrentTrain: epoch  7, batch    38 | loss: 6.0988646Losses:  5.818409442901611 1.790406584739685
CurrentTrain: epoch  7, batch    39 | loss: 5.8184094Losses:  6.025991439819336 1.971163034439087
CurrentTrain: epoch  7, batch    40 | loss: 6.0259914Losses:  5.781007766723633 1.7585819959640503
CurrentTrain: epoch  7, batch    41 | loss: 5.7810078Losses:  5.880631446838379 1.842020869255066
CurrentTrain: epoch  7, batch    42 | loss: 5.8806314Losses:  5.88400936126709 1.8220386505126953
CurrentTrain: epoch  7, batch    43 | loss: 5.8840094Losses:  5.981253623962402 1.9343364238739014
CurrentTrain: epoch  7, batch    44 | loss: 5.9812536Losses:  5.9830217361450195 1.9842493534088135
CurrentTrain: epoch  7, batch    45 | loss: 5.9830217Losses:  5.8487372398376465 1.7882351875305176
CurrentTrain: epoch  7, batch    46 | loss: 5.8487372Losses:  5.970361709594727 1.9200286865234375
CurrentTrain: epoch  7, batch    47 | loss: 5.9703617Losses:  5.966673851013184 1.93570876121521
CurrentTrain: epoch  7, batch    48 | loss: 5.9666739Losses:  5.998064041137695 1.9670039415359497
CurrentTrain: epoch  7, batch    49 | loss: 5.9980640Losses:  5.688394546508789 1.7115464210510254
CurrentTrain: epoch  7, batch    50 | loss: 5.6883945Losses:  6.038686752319336 1.9933300018310547
CurrentTrain: epoch  7, batch    51 | loss: 6.0386868Losses:  5.719270706176758 1.6113474369049072
CurrentTrain: epoch  7, batch    52 | loss: 5.7192707Losses:  5.989768028259277 1.9083770513534546
CurrentTrain: epoch  7, batch    53 | loss: 5.9897680Losses:  5.893367290496826 1.8133902549743652
CurrentTrain: epoch  7, batch    54 | loss: 5.8933673Losses:  5.782955646514893 1.7396758794784546
CurrentTrain: epoch  7, batch    55 | loss: 5.7829556Losses:  5.674256324768066 1.6179317235946655
CurrentTrain: epoch  7, batch    56 | loss: 5.6742563Losses:  6.017876148223877 1.9949244260787964
CurrentTrain: epoch  7, batch    57 | loss: 6.0178761Losses:  6.043579578399658 2.0175533294677734
CurrentTrain: epoch  7, batch    58 | loss: 6.0435796Losses:  5.710738182067871 1.6440773010253906
CurrentTrain: epoch  7, batch    59 | loss: 5.7107382Losses:  6.092958450317383 2.0265114307403564
CurrentTrain: epoch  7, batch    60 | loss: 6.0929585Losses:  5.907556533813477 1.8576674461364746
CurrentTrain: epoch  7, batch    61 | loss: 5.9075565Losses:  5.695587158203125 1.6161367893218994
CurrentTrain: epoch  7, batch    62 | loss: 5.6955872Losses:  6.108023643493652 2.072824239730835
CurrentTrain: epoch  8, batch     0 | loss: 6.1080236Losses:  5.966718673706055 1.8933790922164917
CurrentTrain: epoch  8, batch     1 | loss: 5.9667187Losses:  5.928930759429932 1.8857693672180176
CurrentTrain: epoch  8, batch     2 | loss: 5.9289308Losses:  5.796014785766602 1.7301775217056274
CurrentTrain: epoch  8, batch     3 | loss: 5.7960148Losses:  5.759546279907227 1.7583023309707642
CurrentTrain: epoch  8, batch     4 | loss: 5.7595463Losses:  5.942347049713135 1.899833083152771
CurrentTrain: epoch  8, batch     5 | loss: 5.9423470Losses:  5.984424591064453 1.945002555847168
CurrentTrain: epoch  8, batch     6 | loss: 5.9844246Losses:  6.020318984985352 1.948843002319336
CurrentTrain: epoch  8, batch     7 | loss: 6.0203190Losses:  5.956347942352295 1.9127116203308105
CurrentTrain: epoch  8, batch     8 | loss: 5.9563479Losses:  6.016129016876221 1.9667972326278687
CurrentTrain: epoch  8, batch     9 | loss: 6.0161290Losses:  5.901317119598389 1.8895573616027832
CurrentTrain: epoch  8, batch    10 | loss: 5.9013171Losses:  5.858994483947754 1.8342735767364502
CurrentTrain: epoch  8, batch    11 | loss: 5.8589945Losses:  5.968235969543457 1.902209997177124
CurrentTrain: epoch  8, batch    12 | loss: 5.9682360Losses:  5.866472244262695 1.8266621828079224
CurrentTrain: epoch  8, batch    13 | loss: 5.8664722Losses:  5.86873197555542 1.8455872535705566
CurrentTrain: epoch  8, batch    14 | loss: 5.8687320Losses:  5.786527633666992 1.7629954814910889
CurrentTrain: epoch  8, batch    15 | loss: 5.7865276Losses:  5.85542106628418 1.8614332675933838
CurrentTrain: epoch  8, batch    16 | loss: 5.8554211Losses:  6.012964725494385 1.9756598472595215
CurrentTrain: epoch  8, batch    17 | loss: 6.0129647Losses:  5.952707290649414 1.9099509716033936
CurrentTrain: epoch  8, batch    18 | loss: 5.9527073Losses:  6.061412811279297 1.9416847229003906
CurrentTrain: epoch  8, batch    19 | loss: 6.0614128Losses:  5.721893787384033 1.7734092473983765
CurrentTrain: epoch  8, batch    20 | loss: 5.7218938Losses:  5.9027419090271 1.88178288936615
CurrentTrain: epoch  8, batch    21 | loss: 5.9027419Losses:  5.612033843994141 1.5893561840057373
CurrentTrain: epoch  8, batch    22 | loss: 5.6120338Losses:  6.144257545471191 2.106449604034424
CurrentTrain: epoch  8, batch    23 | loss: 6.1442575Losses:  6.1853814125061035 2.129761219024658
CurrentTrain: epoch  8, batch    24 | loss: 6.1853814Losses:  5.687659740447998 1.6672958135604858
CurrentTrain: epoch  8, batch    25 | loss: 5.6876597Losses:  5.773590087890625 1.7349507808685303
CurrentTrain: epoch  8, batch    26 | loss: 5.7735901Losses:  5.930915832519531 1.847731351852417
CurrentTrain: epoch  8, batch    27 | loss: 5.9309158Losses:  6.106804370880127 2.0871024131774902
CurrentTrain: epoch  8, batch    28 | loss: 6.1068044Losses:  5.960079669952393 1.92144775390625
CurrentTrain: epoch  8, batch    29 | loss: 5.9600797Losses:  5.855972766876221 1.8286528587341309
CurrentTrain: epoch  8, batch    30 | loss: 5.8559728Losses:  5.770328521728516 1.746812105178833
CurrentTrain: epoch  8, batch    31 | loss: 5.7703285Losses:  5.7015061378479 1.6542450189590454
CurrentTrain: epoch  8, batch    32 | loss: 5.7015061Losses:  5.8944525718688965 1.8175225257873535
CurrentTrain: epoch  8, batch    33 | loss: 5.8944526Losses:  6.171830177307129 2.129251003265381
CurrentTrain: epoch  8, batch    34 | loss: 6.1718302Losses:  5.881980895996094 1.8537657260894775
CurrentTrain: epoch  8, batch    35 | loss: 5.8819809Losses:  5.822766304016113 1.7792043685913086
CurrentTrain: epoch  8, batch    36 | loss: 5.8227663Losses:  5.88409423828125 1.8093225955963135
CurrentTrain: epoch  8, batch    37 | loss: 5.8840942Losses:  5.740993976593018 1.7447431087493896
CurrentTrain: epoch  8, batch    38 | loss: 5.7409940Losses:  5.97249698638916 1.9015531539916992
CurrentTrain: epoch  8, batch    39 | loss: 5.9724970Losses:  6.095495700836182 1.9940953254699707
CurrentTrain: epoch  8, batch    40 | loss: 6.0954957Losses:  6.057497024536133 2.0201950073242188
CurrentTrain: epoch  8, batch    41 | loss: 6.0574970Losses:  5.97524881362915 1.9294828176498413
CurrentTrain: epoch  8, batch    42 | loss: 5.9752488Losses:  6.06716775894165 1.961545467376709
CurrentTrain: epoch  8, batch    43 | loss: 6.0671678Losses:  6.021561145782471 1.9770464897155762
CurrentTrain: epoch  8, batch    44 | loss: 6.0215611Losses:  5.800036430358887 1.7149792909622192
CurrentTrain: epoch  8, batch    45 | loss: 5.8000364Losses:  5.692052841186523 1.6947307586669922
CurrentTrain: epoch  8, batch    46 | loss: 5.6920528Losses:  5.6213483810424805 1.5741654634475708
CurrentTrain: epoch  8, batch    47 | loss: 5.6213484Losses:  5.714659690856934 1.6694135665893555
CurrentTrain: epoch  8, batch    48 | loss: 5.7146597Losses:  5.808020114898682 1.7766231298446655
CurrentTrain: epoch  8, batch    49 | loss: 5.8080201Losses:  5.915012836456299 1.8945765495300293
CurrentTrain: epoch  8, batch    50 | loss: 5.9150128Losses:  5.594542980194092 1.5441360473632812
CurrentTrain: epoch  8, batch    51 | loss: 5.5945430Losses:  6.008007049560547 1.9677925109863281
CurrentTrain: epoch  8, batch    52 | loss: 6.0080070Losses:  5.9141845703125 1.8730467557907104
CurrentTrain: epoch  8, batch    53 | loss: 5.9141846Losses:  6.029583930969238 2.0146830081939697
CurrentTrain: epoch  8, batch    54 | loss: 6.0295839Losses:  5.711159706115723 1.704380989074707
CurrentTrain: epoch  8, batch    55 | loss: 5.7111597Losses:  5.853878974914551 1.8169575929641724
CurrentTrain: epoch  8, batch    56 | loss: 5.8538790Losses:  6.133509159088135 2.101926326751709
CurrentTrain: epoch  8, batch    57 | loss: 6.1335092Losses:  5.843839645385742 1.805985450744629
CurrentTrain: epoch  8, batch    58 | loss: 5.8438396Losses:  5.937838077545166 1.9082841873168945
CurrentTrain: epoch  8, batch    59 | loss: 5.9378381Losses:  5.747079849243164 1.7724621295928955
CurrentTrain: epoch  8, batch    60 | loss: 5.7470798Losses:  6.0678205490112305 2.037647008895874
CurrentTrain: epoch  8, batch    61 | loss: 6.0678205Losses:  5.31475830078125 1.3047246932983398
CurrentTrain: epoch  8, batch    62 | loss: 5.3147583Losses:  5.855969429016113 1.8415918350219727
CurrentTrain: epoch  9, batch     0 | loss: 5.8559694Losses:  5.826216697692871 1.7266924381256104
CurrentTrain: epoch  9, batch     1 | loss: 5.8262167Losses:  6.037049293518066 1.9910023212432861
CurrentTrain: epoch  9, batch     2 | loss: 6.0370493Losses:  5.862120628356934 1.7995846271514893
CurrentTrain: epoch  9, batch     3 | loss: 5.8621206Losses:  5.676983833312988 1.6485202312469482
CurrentTrain: epoch  9, batch     4 | loss: 5.6769838Losses:  5.851317405700684 1.846709966659546
CurrentTrain: epoch  9, batch     5 | loss: 5.8513174Losses:  5.845368385314941 1.8053970336914062
CurrentTrain: epoch  9, batch     6 | loss: 5.8453684Losses:  5.892255783081055 1.8596398830413818
CurrentTrain: epoch  9, batch     7 | loss: 5.8922558Losses:  5.658426761627197 1.6713943481445312
CurrentTrain: epoch  9, batch     8 | loss: 5.6584268Losses:  5.901980400085449 1.8829154968261719
CurrentTrain: epoch  9, batch     9 | loss: 5.9019804Losses:  6.0250444412231445 2.0113420486450195
CurrentTrain: epoch  9, batch    10 | loss: 6.0250444Losses:  6.031399726867676 1.9935452938079834
CurrentTrain: epoch  9, batch    11 | loss: 6.0313997Losses:  5.979738235473633 1.9547064304351807
CurrentTrain: epoch  9, batch    12 | loss: 5.9797382Losses:  5.970128059387207 1.9397437572479248
CurrentTrain: epoch  9, batch    13 | loss: 5.9701281Losses:  6.136920928955078 2.1185314655303955
CurrentTrain: epoch  9, batch    14 | loss: 6.1369209Losses:  6.108495712280273 2.0616209506988525
CurrentTrain: epoch  9, batch    15 | loss: 6.1084957Losses:  5.932126522064209 1.9207969903945923
CurrentTrain: epoch  9, batch    16 | loss: 5.9321265Losses:  5.9862141609191895 1.9327468872070312
CurrentTrain: epoch  9, batch    17 | loss: 5.9862142Losses:  5.922858238220215 1.902637243270874
CurrentTrain: epoch  9, batch    18 | loss: 5.9228582Losses:  5.578884601593018 1.5966973304748535
CurrentTrain: epoch  9, batch    19 | loss: 5.5788846Losses:  5.923088550567627 1.8685083389282227
CurrentTrain: epoch  9, batch    20 | loss: 5.9230886Losses:  5.88591194152832 1.8720934391021729
CurrentTrain: epoch  9, batch    21 | loss: 5.8859119Losses:  5.97959041595459 1.9160761833190918
CurrentTrain: epoch  9, batch    22 | loss: 5.9795904Losses:  5.885906219482422 1.8839547634124756
CurrentTrain: epoch  9, batch    23 | loss: 5.8859062Losses:  5.809395790100098 1.7947072982788086
CurrentTrain: epoch  9, batch    24 | loss: 5.8093958Losses:  5.792511463165283 1.7610325813293457
CurrentTrain: epoch  9, batch    25 | loss: 5.7925115Losses:  5.876645088195801 1.88344144821167
CurrentTrain: epoch  9, batch    26 | loss: 5.8766451Losses:  5.896667957305908 1.8902029991149902
CurrentTrain: epoch  9, batch    27 | loss: 5.8966680Losses:  6.082938194274902 2.08027982711792
CurrentTrain: epoch  9, batch    28 | loss: 6.0829382Losses:  5.785110950469971 1.7770267724990845
CurrentTrain: epoch  9, batch    29 | loss: 5.7851110Losses:  5.9035797119140625 1.869649887084961
CurrentTrain: epoch  9, batch    30 | loss: 5.9035797Losses:  5.904743194580078 1.9110393524169922
CurrentTrain: epoch  9, batch    31 | loss: 5.9047432Losses:  5.763821125030518 1.7738707065582275
CurrentTrain: epoch  9, batch    32 | loss: 5.7638211Losses:  5.596467018127441 1.6229405403137207
CurrentTrain: epoch  9, batch    33 | loss: 5.5964670Losses:  6.064963340759277 2.0488429069519043
CurrentTrain: epoch  9, batch    34 | loss: 6.0649633Losses:  6.088996887207031 2.0815823078155518
CurrentTrain: epoch  9, batch    35 | loss: 6.0889969Losses:  5.594210624694824 1.5889885425567627
CurrentTrain: epoch  9, batch    36 | loss: 5.5942106Losses:  5.988629341125488 1.9889857769012451
CurrentTrain: epoch  9, batch    37 | loss: 5.9886293Losses:  5.911606311798096 1.8620867729187012
CurrentTrain: epoch  9, batch    38 | loss: 5.9116063Losses:  6.060561180114746 1.940246820449829
CurrentTrain: epoch  9, batch    39 | loss: 6.0605612Losses:  5.795216083526611 1.7112383842468262
CurrentTrain: epoch  9, batch    40 | loss: 5.7952161Losses:  5.681449890136719 1.687779426574707
CurrentTrain: epoch  9, batch    41 | loss: 5.6814499Losses:  5.82806396484375 1.807483434677124
CurrentTrain: epoch  9, batch    42 | loss: 5.8280640Losses:  5.738241672515869 1.7433618307113647
CurrentTrain: epoch  9, batch    43 | loss: 5.7382417Losses:  5.768548011779785 1.760238766670227
CurrentTrain: epoch  9, batch    44 | loss: 5.7685480Losses:  5.730981826782227 1.7477128505706787
CurrentTrain: epoch  9, batch    45 | loss: 5.7309818Losses:  5.733249187469482 1.7464741468429565
CurrentTrain: epoch  9, batch    46 | loss: 5.7332492Losses:  5.585115432739258 1.5582035779953003
CurrentTrain: epoch  9, batch    47 | loss: 5.5851154Losses:  5.982312202453613 1.9710675477981567
CurrentTrain: epoch  9, batch    48 | loss: 5.9823122Losses:  5.933303356170654 1.8424032926559448
CurrentTrain: epoch  9, batch    49 | loss: 5.9333034Losses:  6.059342861175537 1.9990683794021606
CurrentTrain: epoch  9, batch    50 | loss: 6.0593429Losses:  6.0505475997924805 2.0043070316314697
CurrentTrain: epoch  9, batch    51 | loss: 6.0505476Losses:  5.903968811035156 1.8862513303756714
CurrentTrain: epoch  9, batch    52 | loss: 5.9039688Losses:  5.868127822875977 1.869929313659668
CurrentTrain: epoch  9, batch    53 | loss: 5.8681278Losses:  5.834624290466309 1.8221461772918701
CurrentTrain: epoch  9, batch    54 | loss: 5.8346243Losses:  5.816628456115723 1.8215243816375732
CurrentTrain: epoch  9, batch    55 | loss: 5.8166285Losses:  6.001521110534668 2.002568244934082
CurrentTrain: epoch  9, batch    56 | loss: 6.0015211Losses:  5.9973063468933105 1.9567413330078125
CurrentTrain: epoch  9, batch    57 | loss: 5.9973063Losses:  6.220588684082031 1.8455405235290527
CurrentTrain: epoch  9, batch    58 | loss: 6.2205887Losses:  5.933320045471191 1.9318068027496338
CurrentTrain: epoch  9, batch    59 | loss: 5.9333200Losses:  5.915719032287598 1.902016282081604
CurrentTrain: epoch  9, batch    60 | loss: 5.9157190Losses:  5.928462505340576 1.9197264909744263
CurrentTrain: epoch  9, batch    61 | loss: 5.9284625Losses:  5.5968451499938965 1.59713876247406
CurrentTrain: epoch  9, batch    62 | loss: 5.5968451
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
cur_acc:  ['0.9444']
his_acc:  ['0.9444']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  8.602017402648926 1.9338634014129639
CurrentTrain: epoch  0, batch     0 | loss: 8.6020174Losses:  8.012639045715332 1.9386194944381714
CurrentTrain: epoch  0, batch     1 | loss: 8.0126390Losses:  8.340108871459961 2.0379562377929688
CurrentTrain: epoch  0, batch     2 | loss: 8.3401089Losses:  7.798684597015381 0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.7986846Losses:  8.145482063293457 2.0909695625305176
CurrentTrain: epoch  1, batch     0 | loss: 8.1454821Losses:  8.135549545288086 2.226297378540039
CurrentTrain: epoch  1, batch     1 | loss: 8.1355495Losses:  7.389369964599609 2.152695894241333
CurrentTrain: epoch  1, batch     2 | loss: 7.3893700Losses:  3.733769416809082 0.6403931379318237
CurrentTrain: epoch  1, batch     3 | loss: 3.7337694Losses:  7.196751594543457 1.9307606220245361
CurrentTrain: epoch  2, batch     0 | loss: 7.1967516Losses:  6.5909953117370605 2.0049691200256348
CurrentTrain: epoch  2, batch     1 | loss: 6.5909953Losses:  6.378236293792725 2.0581326484680176
CurrentTrain: epoch  2, batch     2 | loss: 6.3782363Losses:  7.911147117614746 0.6682556867599487
CurrentTrain: epoch  2, batch     3 | loss: 7.9111471Losses:  7.136124610900879 2.1766138076782227
CurrentTrain: epoch  3, batch     0 | loss: 7.1361246Losses:  5.46268367767334 1.9817569255828857
CurrentTrain: epoch  3, batch     1 | loss: 5.4626837Losses:  6.297710418701172 2.0595030784606934
CurrentTrain: epoch  3, batch     2 | loss: 6.2977104Losses:  3.6370460987091064 0.6913488507270813
CurrentTrain: epoch  3, batch     3 | loss: 3.6370461Losses:  5.675844192504883 1.9249426126480103
CurrentTrain: epoch  4, batch     0 | loss: 5.6758442Losses:  5.686700820922852 1.8828742504119873
CurrentTrain: epoch  4, batch     1 | loss: 5.6867008Losses:  5.556016445159912 1.9398189783096313
CurrentTrain: epoch  4, batch     2 | loss: 5.5560164Losses:  3.800711154937744 0.679085373878479
CurrentTrain: epoch  4, batch     3 | loss: 3.8007112Losses:  5.594285011291504 1.935624599456787
CurrentTrain: epoch  5, batch     0 | loss: 5.5942850Losses:  4.725080490112305 1.9404799938201904
CurrentTrain: epoch  5, batch     1 | loss: 4.7250805Losses:  5.141437530517578 2.2035961151123047
CurrentTrain: epoch  5, batch     2 | loss: 5.1414375Losses:  5.634721755981445 0.6645791530609131
CurrentTrain: epoch  5, batch     3 | loss: 5.6347218Losses:  5.185348033905029 1.9739726781845093
CurrentTrain: epoch  6, batch     0 | loss: 5.1853480Losses:  4.810186386108398 1.9102674722671509
CurrentTrain: epoch  6, batch     1 | loss: 4.8101864Losses:  4.6885271072387695 1.977347731590271
CurrentTrain: epoch  6, batch     2 | loss: 4.6885271Losses:  2.430563449859619 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4305634Losses:  5.069659233093262 2.060898542404175
CurrentTrain: epoch  7, batch     0 | loss: 5.0696592Losses:  4.882115364074707 2.1684927940368652
CurrentTrain: epoch  7, batch     1 | loss: 4.8821154Losses:  4.67896842956543 1.9722565412521362
CurrentTrain: epoch  7, batch     2 | loss: 4.6789684Losses:  2.025303840637207 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.0253038Losses:  4.5657572746276855 2.0573859214782715
CurrentTrain: epoch  8, batch     0 | loss: 4.5657573Losses:  4.670326232910156 1.8014401197433472
CurrentTrain: epoch  8, batch     1 | loss: 4.6703262Losses:  4.577959060668945 2.064054250717163
CurrentTrain: epoch  8, batch     2 | loss: 4.5779591Losses:  2.5834102630615234 0.6242314577102661
CurrentTrain: epoch  8, batch     3 | loss: 2.5834103Losses:  4.439334392547607 2.0548458099365234
CurrentTrain: epoch  9, batch     0 | loss: 4.4393344Losses:  4.7223591804504395 1.8002125024795532
CurrentTrain: epoch  9, batch     1 | loss: 4.7223592Losses:  4.28515625 2.03011155128479
CurrentTrain: epoch  9, batch     2 | loss: 4.2851562Losses:  2.543837070465088 0.659639835357666
CurrentTrain: epoch  9, batch     3 | loss: 2.5438371
Losses:  5.031118392944336 2.6788828372955322
MemoryTrain:  epoch  0, batch     0 | loss: 5.0311184Losses:  1.3759493827819824 1.3063241243362427
MemoryTrain:  epoch  0, batch     1 | loss: 1.3759494Losses:  4.474793434143066 2.6870810985565186
MemoryTrain:  epoch  1, batch     0 | loss: 4.4747934Losses:  2.107025623321533 1.3099368810653687
MemoryTrain:  epoch  1, batch     1 | loss: 2.1070256Losses:  4.267163276672363 2.6699442863464355
MemoryTrain:  epoch  2, batch     0 | loss: 4.2671633Losses:  1.652982473373413 1.3368301391601562
MemoryTrain:  epoch  2, batch     1 | loss: 1.6529825Losses:  3.5532474517822266 2.679088592529297
MemoryTrain:  epoch  3, batch     0 | loss: 3.5532475Losses:  3.3848414421081543 1.3310298919677734
MemoryTrain:  epoch  3, batch     1 | loss: 3.3848414Losses:  3.629802703857422 2.6769094467163086
MemoryTrain:  epoch  4, batch     0 | loss: 3.6298027Losses:  1.5591275691986084 1.305521011352539
MemoryTrain:  epoch  4, batch     1 | loss: 1.5591276Losses:  3.541595935821533 2.6736319065093994
MemoryTrain:  epoch  5, batch     0 | loss: 3.5415959Losses:  1.4222661256790161 1.2871732711791992
MemoryTrain:  epoch  5, batch     1 | loss: 1.4222661Losses:  3.4268624782562256 2.6677825450897217
MemoryTrain:  epoch  6, batch     0 | loss: 3.4268625Losses:  1.3975952863693237 1.3230246305465698
MemoryTrain:  epoch  6, batch     1 | loss: 1.3975953Losses:  3.0636377334594727 2.678421974182129
MemoryTrain:  epoch  7, batch     0 | loss: 3.0636377Losses:  2.5432028770446777 1.2894177436828613
MemoryTrain:  epoch  7, batch     1 | loss: 2.5432029Losses:  3.3011178970336914 2.675053119659424
MemoryTrain:  epoch  8, batch     0 | loss: 3.3011179Losses:  1.3389830589294434 1.312544345855713
MemoryTrain:  epoch  8, batch     1 | loss: 1.3389831Losses:  3.192963123321533 2.678483486175537
MemoryTrain:  epoch  9, batch     0 | loss: 3.1929631Losses:  1.6100876331329346 1.310227632522583
MemoryTrain:  epoch  9, batch     1 | loss: 1.6100876
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 71.81%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 70.96%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.21%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.10%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.92%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.22%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.14%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.98%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.82%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.76%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 92.15%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.93%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.44%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.39%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.19%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.00%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.88%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.77%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.45%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 90.41%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.31%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 90.07%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.04%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 89.88%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 89.72%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 89.43%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 89.08%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 88.74%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 88.34%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 88.01%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 87.82%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.44%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 87.00%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 86.70%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.29%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 86.00%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 85.44%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 85.05%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 84.49%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 83.89%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 83.41%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 82.88%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.59%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.36%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 82.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 82.80%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.84%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.93%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.35%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.70%   
cur_acc:  ['0.9444', '0.7421']
his_acc:  ['0.9444', '0.8370']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  8.95288372039795 1.9874666929244995
CurrentTrain: epoch  0, batch     0 | loss: 8.9528837Losses:  8.165204048156738 2.055220603942871
CurrentTrain: epoch  0, batch     1 | loss: 8.1652040Losses:  9.017908096313477 2.188499927520752
CurrentTrain: epoch  0, batch     2 | loss: 9.0179081Losses:  8.203226089477539 0.6718952655792236
CurrentTrain: epoch  0, batch     3 | loss: 8.2032261Losses:  7.543365478515625 2.2111330032348633
CurrentTrain: epoch  1, batch     0 | loss: 7.5433655Losses:  8.429224014282227 1.992621660232544
CurrentTrain: epoch  1, batch     1 | loss: 8.4292240Losses:  7.3292694091796875 2.1757986545562744
CurrentTrain: epoch  1, batch     2 | loss: 7.3292694Losses:  6.502510070800781 0.6835315227508545
CurrentTrain: epoch  1, batch     3 | loss: 6.5025101Losses:  6.276395797729492 1.9377577304840088
CurrentTrain: epoch  2, batch     0 | loss: 6.2763958Losses:  7.357980728149414 2.158698797225952
CurrentTrain: epoch  2, batch     1 | loss: 7.3579807Losses:  7.803773880004883 2.022040605545044
CurrentTrain: epoch  2, batch     2 | loss: 7.8037739Losses:  4.144563674926758 0.6701592206954956
CurrentTrain: epoch  2, batch     3 | loss: 4.1445637Losses:  6.876725196838379 2.143618583679199
CurrentTrain: epoch  3, batch     0 | loss: 6.8767252Losses:  5.962182998657227 1.9291772842407227
CurrentTrain: epoch  3, batch     1 | loss: 5.9621830Losses:  7.079188346862793 2.100346565246582
CurrentTrain: epoch  3, batch     2 | loss: 7.0791883Losses:  5.810634613037109 0.6770844459533691
CurrentTrain: epoch  3, batch     3 | loss: 5.8106346Losses:  6.889671325683594 2.018483877182007
CurrentTrain: epoch  4, batch     0 | loss: 6.8896713Losses:  6.4606032371521 2.189225673675537
CurrentTrain: epoch  4, batch     1 | loss: 6.4606032Losses:  5.006434440612793 1.9065909385681152
CurrentTrain: epoch  4, batch     2 | loss: 5.0064344Losses:  5.399909973144531 0.6804760694503784
CurrentTrain: epoch  4, batch     3 | loss: 5.3999100Losses:  6.363520622253418 1.8150875568389893
CurrentTrain: epoch  5, batch     0 | loss: 6.3635206Losses:  5.614314556121826 2.013284206390381
CurrentTrain: epoch  5, batch     1 | loss: 5.6143146Losses:  5.390371322631836 1.858652949333191
CurrentTrain: epoch  5, batch     2 | loss: 5.3903713Losses:  8.005704879760742 0.6924453377723694
CurrentTrain: epoch  5, batch     3 | loss: 8.0057049Losses:  6.610865592956543 2.1868302822113037
CurrentTrain: epoch  6, batch     0 | loss: 6.6108656Losses:  6.060786247253418 2.11368465423584
CurrentTrain: epoch  6, batch     1 | loss: 6.0607862Losses:  5.269002914428711 2.0936877727508545
CurrentTrain: epoch  6, batch     2 | loss: 5.2690029Losses:  3.3883745670318604 0.6625759601593018
CurrentTrain: epoch  6, batch     3 | loss: 3.3883746Losses:  5.272055149078369 2.039052724838257
CurrentTrain: epoch  7, batch     0 | loss: 5.2720551Losses:  5.620401382446289 2.026916265487671
CurrentTrain: epoch  7, batch     1 | loss: 5.6204014Losses:  5.569390296936035 2.020265817642212
CurrentTrain: epoch  7, batch     2 | loss: 5.5693903Losses:  5.308255195617676 0.6860535144805908
CurrentTrain: epoch  7, batch     3 | loss: 5.3082552Losses:  5.5813398361206055 2.1893441677093506
CurrentTrain: epoch  8, batch     0 | loss: 5.5813398Losses:  5.689419746398926 2.1384215354919434
CurrentTrain: epoch  8, batch     1 | loss: 5.6894197Losses:  5.195241928100586 2.0201499462127686
CurrentTrain: epoch  8, batch     2 | loss: 5.1952419Losses:  3.178102493286133 0.6796263456344604
CurrentTrain: epoch  8, batch     3 | loss: 3.1781025Losses:  4.976393699645996 1.9244338274002075
CurrentTrain: epoch  9, batch     0 | loss: 4.9763937Losses:  4.376904487609863 1.8566688299179077
CurrentTrain: epoch  9, batch     1 | loss: 4.3769045Losses:  5.638216018676758 1.9590861797332764
CurrentTrain: epoch  9, batch     2 | loss: 5.6382160Losses:  5.677725791931152 0.6841185092926025
CurrentTrain: epoch  9, batch     3 | loss: 5.6777258
Losses:  3.944082260131836 2.7016489505767822
MemoryTrain:  epoch  0, batch     0 | loss: 3.9440823Losses:  3.0114171504974365 2.555938243865967
MemoryTrain:  epoch  0, batch     1 | loss: 3.0114172Losses:  3.7850399017333984 2.7150886058807373
MemoryTrain:  epoch  1, batch     0 | loss: 3.7850399Losses:  3.4205093383789062 2.5510613918304443
MemoryTrain:  epoch  1, batch     1 | loss: 3.4205093Losses:  2.971198797225952 2.685058116912842
MemoryTrain:  epoch  2, batch     0 | loss: 2.9711988Losses:  3.4335567951202393 2.5712990760803223
MemoryTrain:  epoch  2, batch     1 | loss: 3.4335568Losses:  3.089418411254883 2.6928112506866455
MemoryTrain:  epoch  3, batch     0 | loss: 3.0894184Losses:  2.7850115299224854 2.5688586235046387
MemoryTrain:  epoch  3, batch     1 | loss: 2.7850115Losses:  2.987549066543579 2.6979358196258545
MemoryTrain:  epoch  4, batch     0 | loss: 2.9875491Losses:  2.850783586502075 2.568089008331299
MemoryTrain:  epoch  4, batch     1 | loss: 2.8507836Losses:  2.8976354598999023 2.7170674800872803
MemoryTrain:  epoch  5, batch     0 | loss: 2.8976355Losses:  2.6409647464752197 2.536749839782715
MemoryTrain:  epoch  5, batch     1 | loss: 2.6409647Losses:  2.7780513763427734 2.6874492168426514
MemoryTrain:  epoch  6, batch     0 | loss: 2.7780514Losses:  2.6447668075561523 2.57226824760437
MemoryTrain:  epoch  6, batch     1 | loss: 2.6447668Losses:  2.782663106918335 2.701415538787842
MemoryTrain:  epoch  7, batch     0 | loss: 2.7826631Losses:  2.6060597896575928 2.561711072921753
MemoryTrain:  epoch  7, batch     1 | loss: 2.6060598Losses:  2.7660725116729736 2.718395709991455
MemoryTrain:  epoch  8, batch     0 | loss: 2.7660725Losses:  2.5679285526275635 2.535578489303589
MemoryTrain:  epoch  8, batch     1 | loss: 2.5679286Losses:  2.7898261547088623 2.719331741333008
MemoryTrain:  epoch  9, batch     0 | loss: 2.7898262Losses:  2.5665078163146973 2.5462148189544678
MemoryTrain:  epoch  9, batch     1 | loss: 2.5665078
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.33%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.21%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.95%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.10%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.27%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.12%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.94%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 91.59%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 91.30%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.94%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.29%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 89.96%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 89.93%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 89.68%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 89.08%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.92%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.74%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 88.59%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 88.44%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 87.97%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 87.37%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 86.78%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 86.34%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 85.84%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.54%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 84.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.65%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.44%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.10%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.89%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.52%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.94%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 81.31%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 80.80%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.41%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.20%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.37%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.93%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.15%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 81.20%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.30%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 81.30%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 81.30%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 81.39%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.84%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 81.25%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.76%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 80.23%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 79.67%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 79.11%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 78.73%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 79.22%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 78.95%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 78.68%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 78.37%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 78.06%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.67%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.09%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 78.97%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 78.92%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 78.28%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 78.33%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 78.24%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.26%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 77.99%   
cur_acc:  ['0.9444', '0.7421', '0.7133']
his_acc:  ['0.9444', '0.8370', '0.7799']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  7.444147109985352 2.0031652450561523
CurrentTrain: epoch  0, batch     0 | loss: 7.4441471Losses:  6.144460678100586 1.9935033321380615
CurrentTrain: epoch  0, batch     1 | loss: 6.1444607Losses:  7.165569305419922 1.8557038307189941
CurrentTrain: epoch  0, batch     2 | loss: 7.1655693Losses:  4.07697868347168 0.6733936071395874
CurrentTrain: epoch  0, batch     3 | loss: 4.0769787Losses:  6.476941108703613 2.136420965194702
CurrentTrain: epoch  1, batch     0 | loss: 6.4769411Losses:  5.7750725746154785 2.080659866333008
CurrentTrain: epoch  1, batch     1 | loss: 5.7750726Losses:  5.982542991638184 2.1424951553344727
CurrentTrain: epoch  1, batch     2 | loss: 5.9825430Losses:  3.0739331245422363 0.6506415605545044
CurrentTrain: epoch  1, batch     3 | loss: 3.0739331Losses:  6.510862827301025 2.0067124366760254
CurrentTrain: epoch  2, batch     0 | loss: 6.5108628Losses:  5.1826958656311035 2.052088499069214
CurrentTrain: epoch  2, batch     1 | loss: 5.1826959Losses:  4.52361536026001 1.981160283088684
CurrentTrain: epoch  2, batch     2 | loss: 4.5236154Losses:  2.6787824630737305 0.6765023469924927
CurrentTrain: epoch  2, batch     3 | loss: 2.6787825Losses:  5.183114528656006 2.03568172454834
CurrentTrain: epoch  3, batch     0 | loss: 5.1831145Losses:  5.125743865966797 2.1665685176849365
CurrentTrain: epoch  3, batch     1 | loss: 5.1257439Losses:  5.116702556610107 2.078185558319092
CurrentTrain: epoch  3, batch     2 | loss: 5.1167026Losses:  2.6941521167755127 0.6540274024009705
CurrentTrain: epoch  3, batch     3 | loss: 2.6941521Losses:  4.750504493713379 1.9083364009857178
CurrentTrain: epoch  4, batch     0 | loss: 4.7505045Losses:  4.527589797973633 1.865448236465454
CurrentTrain: epoch  4, batch     1 | loss: 4.5275898Losses:  4.850805282592773 1.6594468355178833
CurrentTrain: epoch  4, batch     2 | loss: 4.8508053Losses:  2.669370174407959 0.6682957410812378
CurrentTrain: epoch  4, batch     3 | loss: 2.6693702Losses:  5.207350254058838 2.0528907775878906
CurrentTrain: epoch  5, batch     0 | loss: 5.2073503Losses:  4.685511589050293 1.9383265972137451
CurrentTrain: epoch  5, batch     1 | loss: 4.6855116Losses:  4.496015548706055 2.1653144359588623
CurrentTrain: epoch  5, batch     2 | loss: 4.4960155Losses:  1.8037251234054565 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8037251Losses:  3.989759683609009 1.8991734981536865
CurrentTrain: epoch  6, batch     0 | loss: 3.9897597Losses:  4.686258792877197 1.9279803037643433
CurrentTrain: epoch  6, batch     1 | loss: 4.6862588Losses:  4.838472366333008 2.036111831665039
CurrentTrain: epoch  6, batch     2 | loss: 4.8384724Losses:  2.9797348976135254 0.6851558685302734
CurrentTrain: epoch  6, batch     3 | loss: 2.9797349Losses:  4.325605392456055 2.0282015800476074
CurrentTrain: epoch  7, batch     0 | loss: 4.3256054Losses:  4.188756942749023 2.0106868743896484
CurrentTrain: epoch  7, batch     1 | loss: 4.1887569Losses:  4.553246974945068 2.052304744720459
CurrentTrain: epoch  7, batch     2 | loss: 4.5532470Losses:  3.5762240886688232 0.6759492754936218
CurrentTrain: epoch  7, batch     3 | loss: 3.5762241Losses:  4.273499488830566 1.829344630241394
CurrentTrain: epoch  8, batch     0 | loss: 4.2734995Losses:  4.229998588562012 2.057451009750366
CurrentTrain: epoch  8, batch     1 | loss: 4.2299986Losses:  3.942633628845215 2.0368216037750244
CurrentTrain: epoch  8, batch     2 | loss: 3.9426336Losses:  2.9225709438323975 0.6891334056854248
CurrentTrain: epoch  8, batch     3 | loss: 2.9225709Losses:  4.121760845184326 2.0478062629699707
CurrentTrain: epoch  9, batch     0 | loss: 4.1217608Losses:  4.296494007110596 1.9388401508331299
CurrentTrain: epoch  9, batch     1 | loss: 4.2964940Losses:  3.866396427154541 1.9936232566833496
CurrentTrain: epoch  9, batch     2 | loss: 3.8663964Losses:  2.494372844696045 0.6532977819442749
CurrentTrain: epoch  9, batch     3 | loss: 2.4943728
Losses:  3.0241892337799072 2.7131199836730957
MemoryTrain:  epoch  0, batch     0 | loss: 3.0241892Losses:  4.2824387550354 2.689055919647217
MemoryTrain:  epoch  0, batch     1 | loss: 4.2824388Losses:  3.4086060523986816 2.0170392990112305
MemoryTrain:  epoch  0, batch     2 | loss: 3.4086061Losses:  3.5173399448394775 2.714031934738159
MemoryTrain:  epoch  1, batch     0 | loss: 3.5173399Losses:  3.8655667304992676 2.6835060119628906
MemoryTrain:  epoch  1, batch     1 | loss: 3.8655667Losses:  3.654083728790283 2.034611225128174
MemoryTrain:  epoch  1, batch     2 | loss: 3.6540837Losses:  3.532667636871338 2.7079005241394043
MemoryTrain:  epoch  2, batch     0 | loss: 3.5326676Losses:  3.6652116775512695 2.7143568992614746
MemoryTrain:  epoch  2, batch     1 | loss: 3.6652117Losses:  2.226278066635132 1.9751601219177246
MemoryTrain:  epoch  2, batch     2 | loss: 2.2262781Losses:  3.6482179164886475 2.699282169342041
MemoryTrain:  epoch  3, batch     0 | loss: 3.6482179Losses:  3.1387386322021484 2.7022857666015625
MemoryTrain:  epoch  3, batch     1 | loss: 3.1387386Losses:  2.1784255504608154 2.0076825618743896
MemoryTrain:  epoch  3, batch     2 | loss: 2.1784256Losses:  2.963658094406128 2.706329822540283
MemoryTrain:  epoch  4, batch     0 | loss: 2.9636581Losses:  3.187488317489624 2.6917479038238525
MemoryTrain:  epoch  4, batch     1 | loss: 3.1874883Losses:  3.1760880947113037 2.0215699672698975
MemoryTrain:  epoch  4, batch     2 | loss: 3.1760881Losses:  3.286485195159912 2.6843714714050293
MemoryTrain:  epoch  5, batch     0 | loss: 3.2864852Losses:  2.952451467514038 2.715550422668457
MemoryTrain:  epoch  5, batch     1 | loss: 2.9524515Losses:  2.1940276622772217 2.011679172515869
MemoryTrain:  epoch  5, batch     2 | loss: 2.1940277Losses:  2.823648691177368 2.6809513568878174
MemoryTrain:  epoch  6, batch     0 | loss: 2.8236487Losses:  2.933826208114624 2.7064661979675293
MemoryTrain:  epoch  6, batch     1 | loss: 2.9338262Losses:  2.464785575866699 2.0406038761138916
MemoryTrain:  epoch  6, batch     2 | loss: 2.4647856Losses:  2.9171555042266846 2.6920723915100098
MemoryTrain:  epoch  7, batch     0 | loss: 2.9171555Losses:  2.9260315895080566 2.7073092460632324
MemoryTrain:  epoch  7, batch     1 | loss: 2.9260316Losses:  2.0894529819488525 2.017160177230835
MemoryTrain:  epoch  7, batch     2 | loss: 2.0894530Losses:  2.8359806537628174 2.688962459564209
MemoryTrain:  epoch  8, batch     0 | loss: 2.8359807Losses:  3.0448966026306152 2.7171430587768555
MemoryTrain:  epoch  8, batch     1 | loss: 3.0448966Losses:  2.0742299556732178 1.9907541275024414
MemoryTrain:  epoch  8, batch     2 | loss: 2.0742300Losses:  2.861619710922241 2.7118067741394043
MemoryTrain:  epoch  9, batch     0 | loss: 2.8616197Losses:  2.7496161460876465 2.684359073638916
MemoryTrain:  epoch  9, batch     1 | loss: 2.7496161Losses:  2.1294336318969727 2.0095138549804688
MemoryTrain:  epoch  9, batch     2 | loss: 2.1294336
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.47%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.68%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.90%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 90.41%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 90.25%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.75%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.62%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 89.78%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 89.65%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.61%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.98%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.95%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.97%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.85%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.74%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.10%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 88.55%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 88.10%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 87.72%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 86.78%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 86.65%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 86.59%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 86.53%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 86.55%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 86.42%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 85.97%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 85.59%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 85.03%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 84.73%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 84.28%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 83.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.07%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.87%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.74%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.13%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 81.48%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.91%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 80.28%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 79.67%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.24%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.04%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.87%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 79.99%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.05%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.33%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.49%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.55%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 80.52%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 80.57%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 80.63%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.07%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 80.49%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.00%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 79.48%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 78.92%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 78.37%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 77.99%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.36%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 78.44%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 78.04%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.57%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 77.15%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 76.77%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 76.40%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.43%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 76.76%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 76.64%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 76.44%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 76.24%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 76.09%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 75.86%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 75.81%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 75.65%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.67%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 75.73%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.72%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 75.91%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.03%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 76.31%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 75.98%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 75.79%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 75.63%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 75.47%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 75.44%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.43%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 75.43%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 75.28%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 75.15%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 74.73%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 74.55%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 73.85%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 74.49%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.55%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 74.50%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.84%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.72%   
cur_acc:  ['0.9444', '0.7421', '0.7133', '0.7847']
his_acc:  ['0.9444', '0.8370', '0.7799', '0.7672']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  7.681390762329102 1.9559357166290283
CurrentTrain: epoch  0, batch     0 | loss: 7.6813908Losses:  7.7948222160339355 1.9877458810806274
CurrentTrain: epoch  0, batch     1 | loss: 7.7948222Losses:  7.591696739196777 1.9525597095489502
CurrentTrain: epoch  0, batch     2 | loss: 7.5916967Losses:  3.6581883430480957 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.6581883Losses:  7.127985000610352 1.9871540069580078
CurrentTrain: epoch  1, batch     0 | loss: 7.1279850Losses:  6.669976234436035 2.0879526138305664
CurrentTrain: epoch  1, batch     1 | loss: 6.6699762Losses:  6.359090805053711 2.0080738067626953
CurrentTrain: epoch  1, batch     2 | loss: 6.3590908Losses:  5.039678573608398 0.6791931986808777
CurrentTrain: epoch  1, batch     3 | loss: 5.0396786Losses:  6.291292190551758 2.101283073425293
CurrentTrain: epoch  2, batch     0 | loss: 6.2912922Losses:  7.053469181060791 2.096794605255127
CurrentTrain: epoch  2, batch     1 | loss: 7.0534692Losses:  6.143051624298096 2.085716724395752
CurrentTrain: epoch  2, batch     2 | loss: 6.1430516Losses:  3.4041190147399902 0.683287501335144
CurrentTrain: epoch  2, batch     3 | loss: 3.4041190Losses:  6.539114952087402 2.098642110824585
CurrentTrain: epoch  3, batch     0 | loss: 6.5391150Losses:  5.712915897369385 1.9803318977355957
CurrentTrain: epoch  3, batch     1 | loss: 5.7129159Losses:  5.5044074058532715 2.000476837158203
CurrentTrain: epoch  3, batch     2 | loss: 5.5044074Losses:  5.2663798332214355 0.6808967590332031
CurrentTrain: epoch  3, batch     3 | loss: 5.2663798Losses:  5.3089189529418945 1.9412202835083008
CurrentTrain: epoch  4, batch     0 | loss: 5.3089190Losses:  5.822063446044922 1.806616187095642
CurrentTrain: epoch  4, batch     1 | loss: 5.8220634Losses:  5.7937211990356445 2.182527780532837
CurrentTrain: epoch  4, batch     2 | loss: 5.7937212Losses:  2.430415153503418 0.6772960424423218
CurrentTrain: epoch  4, batch     3 | loss: 2.4304152Losses:  5.3523850440979 2.0942869186401367
CurrentTrain: epoch  5, batch     0 | loss: 5.3523850Losses:  5.397545337677002 2.009153366088867
CurrentTrain: epoch  5, batch     1 | loss: 5.3975453Losses:  5.550967216491699 2.0585379600524902
CurrentTrain: epoch  5, batch     2 | loss: 5.5509672Losses:  5.373936653137207 0.6869351863861084
CurrentTrain: epoch  5, batch     3 | loss: 5.3739367Losses:  5.156842231750488 1.950684905052185
CurrentTrain: epoch  6, batch     0 | loss: 5.1568422Losses:  5.076913356781006 1.9850363731384277
CurrentTrain: epoch  6, batch     1 | loss: 5.0769134Losses:  5.241697311401367 2.0576915740966797
CurrentTrain: epoch  6, batch     2 | loss: 5.2416973Losses:  3.122934341430664 0.6787865161895752
CurrentTrain: epoch  6, batch     3 | loss: 3.1229343Losses:  4.979170799255371 2.0888760089874268
CurrentTrain: epoch  7, batch     0 | loss: 4.9791708Losses:  5.0258989334106445 2.0621390342712402
CurrentTrain: epoch  7, batch     1 | loss: 5.0258989Losses:  5.029323577880859 2.011643886566162
CurrentTrain: epoch  7, batch     2 | loss: 5.0293236Losses:  3.5888876914978027 0.6758540868759155
CurrentTrain: epoch  7, batch     3 | loss: 3.5888877Losses:  4.815380096435547 2.182130813598633
CurrentTrain: epoch  8, batch     0 | loss: 4.8153801Losses:  5.153563022613525 1.9347034692764282
CurrentTrain: epoch  8, batch     1 | loss: 5.1535630Losses:  4.927138328552246 2.057609796524048
CurrentTrain: epoch  8, batch     2 | loss: 4.9271383Losses:  2.490337371826172 0.6505388617515564
CurrentTrain: epoch  8, batch     3 | loss: 2.4903374Losses:  4.968491554260254 2.102259635925293
CurrentTrain: epoch  9, batch     0 | loss: 4.9684916Losses:  4.151896953582764 1.9187774658203125
CurrentTrain: epoch  9, batch     1 | loss: 4.1518970Losses:  4.9347615242004395 1.9178214073181152
CurrentTrain: epoch  9, batch     2 | loss: 4.9347615Losses:  2.7306103706359863 0.6826658844947815
CurrentTrain: epoch  9, batch     3 | loss: 2.7306104
Losses:  3.3011045455932617 2.6874241828918457
MemoryTrain:  epoch  0, batch     0 | loss: 3.3011045Losses:  3.4669790267944336 2.720465660095215
MemoryTrain:  epoch  0, batch     1 | loss: 3.4669790Losses:  3.1711971759796143 2.7213950157165527
MemoryTrain:  epoch  0, batch     2 | loss: 3.1711972Losses:  2.1567351818084717 0.669235348701477
MemoryTrain:  epoch  0, batch     3 | loss: 2.1567352Losses:  4.080597400665283 2.7106165885925293
MemoryTrain:  epoch  1, batch     0 | loss: 4.0805974Losses:  3.7845702171325684 2.7219350337982178
MemoryTrain:  epoch  1, batch     1 | loss: 3.7845702Losses:  3.0310447216033936 2.695587635040283
MemoryTrain:  epoch  1, batch     2 | loss: 3.0310447Losses:  0.6900374293327332 0.6431914567947388
MemoryTrain:  epoch  1, batch     3 | loss: 0.6900374Losses:  3.49212908744812 2.708366870880127
MemoryTrain:  epoch  2, batch     0 | loss: 3.4921291Losses:  3.2941651344299316 2.700211763381958
MemoryTrain:  epoch  2, batch     1 | loss: 3.2941651Losses:  3.314399480819702 2.7157461643218994
MemoryTrain:  epoch  2, batch     2 | loss: 3.3143995Losses:  0.7402620911598206 0.6600285768508911
MemoryTrain:  epoch  2, batch     3 | loss: 0.7402621Losses:  3.095475912094116 2.709311008453369
MemoryTrain:  epoch  3, batch     0 | loss: 3.0954759Losses:  2.902496337890625 2.7036192417144775
MemoryTrain:  epoch  3, batch     1 | loss: 2.9024963Losses:  3.2253899574279785 2.698660135269165
MemoryTrain:  epoch  3, batch     2 | loss: 3.2253900Losses:  1.4947547912597656 0.6614631414413452
MemoryTrain:  epoch  3, batch     3 | loss: 1.4947548Losses:  3.0459914207458496 2.7032032012939453
MemoryTrain:  epoch  4, batch     0 | loss: 3.0459914Losses:  2.8801119327545166 2.7173538208007812
MemoryTrain:  epoch  4, batch     1 | loss: 2.8801119Losses:  3.011963367462158 2.6909894943237305
MemoryTrain:  epoch  4, batch     2 | loss: 3.0119634Losses:  0.9256952404975891 0.6545052528381348
MemoryTrain:  epoch  4, batch     3 | loss: 0.9256952Losses:  2.87052583694458 2.696896553039551
MemoryTrain:  epoch  5, batch     0 | loss: 2.8705258Losses:  2.8601083755493164 2.6958975791931152
MemoryTrain:  epoch  5, batch     1 | loss: 2.8601084Losses:  2.8109285831451416 2.722705841064453
MemoryTrain:  epoch  5, batch     2 | loss: 2.8109286Losses:  0.6386803388595581 0.6248139142990112
MemoryTrain:  epoch  5, batch     3 | loss: 0.6386803Losses:  2.8535821437835693 2.693225860595703
MemoryTrain:  epoch  6, batch     0 | loss: 2.8535821Losses:  2.8062660694122314 2.706810474395752
MemoryTrain:  epoch  6, batch     1 | loss: 2.8062661Losses:  2.826202154159546 2.7116661071777344
MemoryTrain:  epoch  6, batch     2 | loss: 2.8262022Losses:  0.681691586971283 0.6558663845062256
MemoryTrain:  epoch  6, batch     3 | loss: 0.6816916Losses:  2.8229353427886963 2.7066750526428223
MemoryTrain:  epoch  7, batch     0 | loss: 2.8229353Losses:  2.753765106201172 2.6966617107391357
MemoryTrain:  epoch  7, batch     1 | loss: 2.7537651Losses:  2.8267111778259277 2.7007579803466797
MemoryTrain:  epoch  7, batch     2 | loss: 2.8267112Losses:  0.7008808255195618 0.6721318960189819
MemoryTrain:  epoch  7, batch     3 | loss: 0.7008808Losses:  2.7585346698760986 2.7077488899230957
MemoryTrain:  epoch  8, batch     0 | loss: 2.7585347Losses:  2.7592551708221436 2.6946301460266113
MemoryTrain:  epoch  8, batch     1 | loss: 2.7592552Losses:  2.7737767696380615 2.703883647918701
MemoryTrain:  epoch  8, batch     2 | loss: 2.7737768Losses:  0.8263682126998901 0.6773188710212708
MemoryTrain:  epoch  8, batch     3 | loss: 0.8263682Losses:  2.746131658554077 2.692763328552246
MemoryTrain:  epoch  9, batch     0 | loss: 2.7461317Losses:  2.777331829071045 2.7047042846679688
MemoryTrain:  epoch  9, batch     1 | loss: 2.7773318Losses:  2.787883996963501 2.7128360271453857
MemoryTrain:  epoch  9, batch     2 | loss: 2.7878840Losses:  0.6224912405014038 0.6074901819229126
MemoryTrain:  epoch  9, batch     3 | loss: 0.6224912
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.02%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.80%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.44%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.30%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.17%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.77%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 88.73%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 88.98%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 88.73%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 88.31%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 87.82%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.58%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 87.03%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 86.81%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 86.43%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 85.84%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 85.42%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 85.00%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 84.59%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 83.84%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 83.66%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 83.50%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 83.45%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 83.36%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 83.13%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.65%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 82.17%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 81.71%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 81.44%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 80.99%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 80.74%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 80.25%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 79.39%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 79.29%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 79.13%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.74%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.12%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 77.52%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.93%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 76.35%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.95%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.72%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 77.07%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 77.32%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 77.49%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 77.72%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.31%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 77.74%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 77.23%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 76.73%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.19%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 75.66%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 75.30%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.72%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 75.79%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.33%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 74.96%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 74.51%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 74.15%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.72%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 73.77%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 74.16%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 73.84%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 73.70%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.59%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 73.59%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 73.60%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 73.67%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 73.79%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 73.77%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.92%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 74.03%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.45%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 74.42%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 74.33%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 74.21%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 73.96%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 73.78%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 73.66%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.60%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 73.61%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 73.22%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 72.99%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 72.85%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 72.59%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 72.42%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 72.11%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 72.75%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 74.56%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.72%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.10%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 75.02%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 75.02%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 74.98%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.98%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 74.95%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 74.74%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 74.62%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 74.48%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 74.44%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 74.37%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 74.42%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 74.71%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 74.57%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 74.53%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 74.40%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 74.27%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 73.99%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 73.88%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 73.69%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 73.48%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 73.43%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 73.55%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 73.60%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 73.61%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 74.68%   
cur_acc:  ['0.9444', '0.7421', '0.7133', '0.7847', '0.7302']
his_acc:  ['0.9444', '0.8370', '0.7799', '0.7672', '0.7468']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  8.248279571533203 2.011155366897583
CurrentTrain: epoch  0, batch     0 | loss: 8.2482796Losses:  7.88410758972168 1.7520580291748047
CurrentTrain: epoch  0, batch     1 | loss: 7.8841076Losses:  8.45148754119873 2.0168001651763916
CurrentTrain: epoch  0, batch     2 | loss: 8.4514875Losses:  4.7982282638549805 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.7982283Losses:  8.033239364624023 2.186549186706543
CurrentTrain: epoch  1, batch     0 | loss: 8.0332394Losses:  7.183836936950684 2.073793411254883
CurrentTrain: epoch  1, batch     1 | loss: 7.1838369Losses:  6.438623428344727 2.0660619735717773
CurrentTrain: epoch  1, batch     2 | loss: 6.4386234Losses:  5.798211097717285 0.6552590727806091
CurrentTrain: epoch  1, batch     3 | loss: 5.7982111Losses:  6.3704328536987305 1.9279382228851318
CurrentTrain: epoch  2, batch     0 | loss: 6.3704329Losses:  6.837769508361816 2.041961669921875
CurrentTrain: epoch  2, batch     1 | loss: 6.8377695Losses:  6.076431751251221 2.0623717308044434
CurrentTrain: epoch  2, batch     2 | loss: 6.0764318Losses:  5.573938369750977 0.6793947219848633
CurrentTrain: epoch  2, batch     3 | loss: 5.5739384Losses:  5.323427200317383 1.9424185752868652
CurrentTrain: epoch  3, batch     0 | loss: 5.3234272Losses:  6.629598140716553 2.1320881843566895
CurrentTrain: epoch  3, batch     1 | loss: 6.6295981Losses:  6.519914627075195 2.224773406982422
CurrentTrain: epoch  3, batch     2 | loss: 6.5199146Losses:  4.973742485046387 0.6691626310348511
CurrentTrain: epoch  3, batch     3 | loss: 4.9737425Losses:  5.084664344787598 1.8590657711029053
CurrentTrain: epoch  4, batch     0 | loss: 5.0846643Losses:  6.701711654663086 1.944466471672058
CurrentTrain: epoch  4, batch     1 | loss: 6.7017117Losses:  6.1209516525268555 2.1344525814056396
CurrentTrain: epoch  4, batch     2 | loss: 6.1209517Losses:  3.9603629112243652 0.686848521232605
CurrentTrain: epoch  4, batch     3 | loss: 3.9603629Losses:  5.359151363372803 2.126664638519287
CurrentTrain: epoch  5, batch     0 | loss: 5.3591514Losses:  6.240839958190918 1.865140676498413
CurrentTrain: epoch  5, batch     1 | loss: 6.2408400Losses:  5.018373489379883 2.0570616722106934
CurrentTrain: epoch  5, batch     2 | loss: 5.0183735Losses:  3.8483290672302246 0.6739077568054199
CurrentTrain: epoch  5, batch     3 | loss: 3.8483291Losses:  4.841188907623291 2.0619726181030273
CurrentTrain: epoch  6, batch     0 | loss: 4.8411889Losses:  5.833443641662598 2.1352081298828125
CurrentTrain: epoch  6, batch     1 | loss: 5.8334436Losses:  5.605460166931152 2.092737913131714
CurrentTrain: epoch  6, batch     2 | loss: 5.6054602Losses:  3.947207450866699 0.6594953536987305
CurrentTrain: epoch  6, batch     3 | loss: 3.9472075Losses:  5.3778839111328125 2.0634639263153076
CurrentTrain: epoch  7, batch     0 | loss: 5.3778839Losses:  5.194543838500977 2.0957260131835938
CurrentTrain: epoch  7, batch     1 | loss: 5.1945438Losses:  4.7099456787109375 1.924092411994934
CurrentTrain: epoch  7, batch     2 | loss: 4.7099457Losses:  4.496423244476318 0.6700190305709839
CurrentTrain: epoch  7, batch     3 | loss: 4.4964232Losses:  4.633677959442139 1.901344895362854
CurrentTrain: epoch  8, batch     0 | loss: 4.6336780Losses:  4.324661731719971 1.869564414024353
CurrentTrain: epoch  8, batch     1 | loss: 4.3246617Losses:  5.574987411499023 1.9690577983856201
CurrentTrain: epoch  8, batch     2 | loss: 5.5749874Losses:  2.439509391784668 0.6809536218643188
CurrentTrain: epoch  8, batch     3 | loss: 2.4395094Losses:  4.4530792236328125 2.010283946990967
CurrentTrain: epoch  9, batch     0 | loss: 4.4530792Losses:  4.457533836364746 1.873582124710083
CurrentTrain: epoch  9, batch     1 | loss: 4.4575338Losses:  4.736190319061279 1.8545377254486084
CurrentTrain: epoch  9, batch     2 | loss: 4.7361903Losses:  5.9477386474609375 0.6785179376602173
CurrentTrain: epoch  9, batch     3 | loss: 5.9477386
Losses:  3.4015064239501953 2.7235774993896484
MemoryTrain:  epoch  0, batch     0 | loss: 3.4015064Losses:  3.3916542530059814 2.7190284729003906
MemoryTrain:  epoch  0, batch     1 | loss: 3.3916543Losses:  3.1789536476135254 2.7036540508270264
MemoryTrain:  epoch  0, batch     2 | loss: 3.1789536Losses:  2.785382032394409 2.4052178859710693
MemoryTrain:  epoch  0, batch     3 | loss: 2.7853820Losses:  3.9325881004333496 2.686948776245117
MemoryTrain:  epoch  1, batch     0 | loss: 3.9325881Losses:  3.2527365684509277 2.703237771987915
MemoryTrain:  epoch  1, batch     1 | loss: 3.2527366Losses:  3.372201681137085 2.7208704948425293
MemoryTrain:  epoch  1, batch     2 | loss: 3.3722017Losses:  2.8449573516845703 2.4301674365997314
MemoryTrain:  epoch  1, batch     3 | loss: 2.8449574Losses:  3.124138832092285 2.7099967002868652
MemoryTrain:  epoch  2, batch     0 | loss: 3.1241388Losses:  3.059291124343872 2.705003499984741
MemoryTrain:  epoch  2, batch     1 | loss: 3.0592911Losses:  2.9689981937408447 2.7137069702148438
MemoryTrain:  epoch  2, batch     2 | loss: 2.9689982Losses:  2.5913162231445312 2.4051332473754883
MemoryTrain:  epoch  2, batch     3 | loss: 2.5913162Losses:  2.8688242435455322 2.692101001739502
MemoryTrain:  epoch  3, batch     0 | loss: 2.8688242Losses:  2.84112548828125 2.705142021179199
MemoryTrain:  epoch  3, batch     1 | loss: 2.8411255Losses:  2.932668685913086 2.699591636657715
MemoryTrain:  epoch  3, batch     2 | loss: 2.9326687Losses:  2.7109057903289795 2.437922477722168
MemoryTrain:  epoch  3, batch     3 | loss: 2.7109058Losses:  3.021930694580078 2.7060706615448
MemoryTrain:  epoch  4, batch     0 | loss: 3.0219307Losses:  2.8075125217437744 2.7040603160858154
MemoryTrain:  epoch  4, batch     1 | loss: 2.8075125Losses:  2.759805917739868 2.6894564628601074
MemoryTrain:  epoch  4, batch     2 | loss: 2.7598059Losses:  2.535823345184326 2.4356606006622314
MemoryTrain:  epoch  4, batch     3 | loss: 2.5358233Losses:  2.757189989089966 2.7048001289367676
MemoryTrain:  epoch  5, batch     0 | loss: 2.7571900Losses:  2.7978999614715576 2.720700740814209
MemoryTrain:  epoch  5, batch     1 | loss: 2.7979000Losses:  2.7602672576904297 2.6969869136810303
MemoryTrain:  epoch  5, batch     2 | loss: 2.7602673Losses:  3.0072357654571533 2.40936279296875
MemoryTrain:  epoch  5, batch     3 | loss: 3.0072358Losses:  2.8393378257751465 2.704732656478882
MemoryTrain:  epoch  6, batch     0 | loss: 2.8393378Losses:  2.7565276622772217 2.7057924270629883
MemoryTrain:  epoch  6, batch     1 | loss: 2.7565277Losses:  2.739840507507324 2.689809560775757
MemoryTrain:  epoch  6, batch     2 | loss: 2.7398405Losses:  2.478421211242676 2.4227726459503174
MemoryTrain:  epoch  6, batch     3 | loss: 2.4784212Losses:  2.754075288772583 2.7008488178253174
MemoryTrain:  epoch  7, batch     0 | loss: 2.7540753Losses:  2.771439790725708 2.7080013751983643
MemoryTrain:  epoch  7, batch     1 | loss: 2.7714398Losses:  2.752711534500122 2.7002854347229004
MemoryTrain:  epoch  7, batch     2 | loss: 2.7527115Losses:  2.4640533924102783 2.4145259857177734
MemoryTrain:  epoch  7, batch     3 | loss: 2.4640534Losses:  2.743947744369507 2.702700614929199
MemoryTrain:  epoch  8, batch     0 | loss: 2.7439477Losses:  2.739949941635132 2.688584566116333
MemoryTrain:  epoch  8, batch     1 | loss: 2.7399499Losses:  2.742203950881958 2.701270580291748
MemoryTrain:  epoch  8, batch     2 | loss: 2.7422040Losses:  2.5651416778564453 2.4221177101135254
MemoryTrain:  epoch  8, batch     3 | loss: 2.5651417Losses:  2.7320640087127686 2.690487861633301
MemoryTrain:  epoch  9, batch     0 | loss: 2.7320640Losses:  2.7389421463012695 2.6993408203125
MemoryTrain:  epoch  9, batch     1 | loss: 2.7389421Losses:  2.7627456188201904 2.714421272277832
MemoryTrain:  epoch  9, batch     2 | loss: 2.7627456Losses:  2.446002960205078 2.4038777351379395
MemoryTrain:  epoch  9, batch     3 | loss: 2.4460030
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.27%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 66.81%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.96%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.70%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.40%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 86.62%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 85.80%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 85.35%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 84.83%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 84.51%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.73%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.98%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.25%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 84.87%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 84.17%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 83.73%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 83.47%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 83.05%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 82.87%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 82.55%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 82.00%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 81.62%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 80.24%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 80.50%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 80.31%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 79.61%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 78.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.85%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.70%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 77.43%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.28%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 77.20%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 76.10%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 75.52%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 74.89%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 74.32%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.94%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.73%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 74.95%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 74.95%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 74.85%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 74.76%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.09%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 75.63%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 75.09%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 74.11%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 73.59%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 73.08%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 72.74%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.86%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 72.47%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 72.04%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 71.65%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 71.21%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 70.78%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 70.43%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 69.71%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 69.59%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 69.30%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 69.08%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 68.61%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.76%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.94%   [EVAL] batch:  194 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:  195 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  196 | acc: 6.25%,  total acc: 68.97%   [EVAL] batch:  197 | acc: 6.25%,  total acc: 68.66%   [EVAL] batch:  198 | acc: 6.25%,  total acc: 68.34%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 68.00%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 67.31%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 67.05%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 66.62%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.33%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 67.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 69.19%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 70.04%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 70.01%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 70.01%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 69.81%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 69.64%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 69.54%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 69.95%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 69.80%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 69.68%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 69.63%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 69.43%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 69.30%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 69.12%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 68.90%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 68.82%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 70.12%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 69.80%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 69.52%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 69.42%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 69.50%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 69.34%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.55%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 70.60%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 70.40%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 70.25%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 70.07%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 69.88%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 69.68%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 69.91%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.87%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 69.80%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.88%   
cur_acc:  ['0.9444', '0.7421', '0.7133', '0.7847', '0.7302', '0.6696']
his_acc:  ['0.9444', '0.8370', '0.7799', '0.7672', '0.7468', '0.6988']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  7.332282543182373 1.7705460786819458
CurrentTrain: epoch  0, batch     0 | loss: 7.3322825Losses:  9.04189682006836 1.8096766471862793
CurrentTrain: epoch  0, batch     1 | loss: 9.0418968Losses:  8.62722110748291 2.0165703296661377
CurrentTrain: epoch  0, batch     2 | loss: 8.6272211Losses:  9.548233032226562 0.6731585264205933
CurrentTrain: epoch  0, batch     3 | loss: 9.5482330Losses:  7.881441116333008 2.1061758995056152
CurrentTrain: epoch  1, batch     0 | loss: 7.8814411Losses:  7.49050235748291 1.9394325017929077
CurrentTrain: epoch  1, batch     1 | loss: 7.4905024Losses:  6.7821245193481445 2.0622215270996094
CurrentTrain: epoch  1, batch     2 | loss: 6.7821245Losses:  7.181744575500488 0.6754329204559326
CurrentTrain: epoch  1, batch     3 | loss: 7.1817446Losses:  6.745645999908447 1.8956241607666016
CurrentTrain: epoch  2, batch     0 | loss: 6.7456460Losses:  6.565125942230225 2.0565056800842285
CurrentTrain: epoch  2, batch     1 | loss: 6.5651259Losses:  6.896392822265625 2.2158401012420654
CurrentTrain: epoch  2, batch     2 | loss: 6.8963928Losses:  7.615157127380371 0.6828241348266602
CurrentTrain: epoch  2, batch     3 | loss: 7.6151571Losses:  6.465629577636719 1.8454582691192627
CurrentTrain: epoch  3, batch     0 | loss: 6.4656296Losses:  6.615838050842285 2.0854616165161133
CurrentTrain: epoch  3, batch     1 | loss: 6.6158381Losses:  6.302486419677734 2.061544418334961
CurrentTrain: epoch  3, batch     2 | loss: 6.3024864Losses:  4.688533782958984 0.6749144792556763
CurrentTrain: epoch  3, batch     3 | loss: 4.6885338Losses:  6.723210334777832 2.050041913986206
CurrentTrain: epoch  4, batch     0 | loss: 6.7232103Losses:  5.226898193359375 1.8449997901916504
CurrentTrain: epoch  4, batch     1 | loss: 5.2268982Losses:  6.166757106781006 2.1267342567443848
CurrentTrain: epoch  4, batch     2 | loss: 6.1667571Losses:  6.450340270996094 0.6920045018196106
CurrentTrain: epoch  4, batch     3 | loss: 6.4503403Losses:  5.783828258514404 1.651877999305725
CurrentTrain: epoch  5, batch     0 | loss: 5.7838283Losses:  5.615703582763672 2.067845582962036
CurrentTrain: epoch  5, batch     1 | loss: 5.6157036Losses:  5.773853778839111 2.0985825061798096
CurrentTrain: epoch  5, batch     2 | loss: 5.7738538Losses:  3.7553935050964355 0.6656471490859985
CurrentTrain: epoch  5, batch     3 | loss: 3.7553935Losses:  5.971897125244141 2.008026361465454
CurrentTrain: epoch  6, batch     0 | loss: 5.9718971Losses:  5.469171047210693 1.742191195487976
CurrentTrain: epoch  6, batch     1 | loss: 5.4691710Losses:  4.679581642150879 1.8496489524841309
CurrentTrain: epoch  6, batch     2 | loss: 4.6795816Losses:  5.4713850021362305 0.669743537902832
CurrentTrain: epoch  6, batch     3 | loss: 5.4713850Losses:  4.955711841583252 1.933494210243225
CurrentTrain: epoch  7, batch     0 | loss: 4.9557118Losses:  5.717231750488281 1.8977704048156738
CurrentTrain: epoch  7, batch     1 | loss: 5.7172318Losses:  5.430798530578613 2.093654155731201
CurrentTrain: epoch  7, batch     2 | loss: 5.4307985Losses:  4.56195068359375 0.6835654973983765
CurrentTrain: epoch  7, batch     3 | loss: 4.5619507Losses:  5.144589900970459 1.95050847530365
CurrentTrain: epoch  8, batch     0 | loss: 5.1445899Losses:  5.043187618255615 1.9374750852584839
CurrentTrain: epoch  8, batch     1 | loss: 5.0431876Losses:  5.478687286376953 2.212479591369629
CurrentTrain: epoch  8, batch     2 | loss: 5.4786873Losses:  3.1565513610839844 0.6842354536056519
CurrentTrain: epoch  8, batch     3 | loss: 3.1565514Losses:  4.880703449249268 2.1431329250335693
CurrentTrain: epoch  9, batch     0 | loss: 4.8807034Losses:  5.470035552978516 2.0700535774230957
CurrentTrain: epoch  9, batch     1 | loss: 5.4700356Losses:  4.832130432128906 2.002690076828003
CurrentTrain: epoch  9, batch     2 | loss: 4.8321304Losses:  2.839725971221924 0.689496636390686
CurrentTrain: epoch  9, batch     3 | loss: 2.8397260
Losses:  3.155675172805786 2.7112131118774414
MemoryTrain:  epoch  0, batch     0 | loss: 3.1556752Losses:  3.2556915283203125 2.7113990783691406
MemoryTrain:  epoch  0, batch     1 | loss: 3.2556915Losses:  3.3047070503234863 2.7128653526306152
MemoryTrain:  epoch  0, batch     2 | loss: 3.3047071Losses:  3.0324008464813232 2.7226498126983643
MemoryTrain:  epoch  0, batch     3 | loss: 3.0324008Losses:  1.80569589138031 1.683081030845642
MemoryTrain:  epoch  0, batch     4 | loss: 1.8056959Losses:  3.224853992462158 2.71004056930542
MemoryTrain:  epoch  1, batch     0 | loss: 3.2248540Losses:  3.5128049850463867 2.6917524337768555
MemoryTrain:  epoch  1, batch     1 | loss: 3.5128050Losses:  3.10905122756958 2.724402666091919
MemoryTrain:  epoch  1, batch     2 | loss: 3.1090512Losses:  3.4554431438446045 2.7115750312805176
MemoryTrain:  epoch  1, batch     3 | loss: 3.4554431Losses:  1.9861607551574707 1.7244863510131836
MemoryTrain:  epoch  1, batch     4 | loss: 1.9861608Losses:  2.9449756145477295 2.6959068775177
MemoryTrain:  epoch  2, batch     0 | loss: 2.9449756Losses:  2.9659931659698486 2.6937978267669678
MemoryTrain:  epoch  2, batch     1 | loss: 2.9659932Losses:  2.8906922340393066 2.704965353012085
MemoryTrain:  epoch  2, batch     2 | loss: 2.8906922Losses:  3.42206072807312 2.715466260910034
MemoryTrain:  epoch  2, batch     3 | loss: 3.4220607Losses:  1.94443678855896 1.7587285041809082
MemoryTrain:  epoch  2, batch     4 | loss: 1.9444368Losses:  2.9040980339050293 2.708644390106201
MemoryTrain:  epoch  3, batch     0 | loss: 2.9040980Losses:  2.8993678092956543 2.693084716796875
MemoryTrain:  epoch  3, batch     1 | loss: 2.8993678Losses:  2.8399157524108887 2.709287405014038
MemoryTrain:  epoch  3, batch     2 | loss: 2.8399158Losses:  2.9726102352142334 2.706650495529175
MemoryTrain:  epoch  3, batch     3 | loss: 2.9726102Losses:  2.0461044311523438 1.7309424877166748
MemoryTrain:  epoch  3, batch     4 | loss: 2.0461044Losses:  2.7610390186309814 2.705315351486206
MemoryTrain:  epoch  4, batch     0 | loss: 2.7610390Losses:  2.8415489196777344 2.6978821754455566
MemoryTrain:  epoch  4, batch     1 | loss: 2.8415489Losses:  2.859672784805298 2.7148005962371826
MemoryTrain:  epoch  4, batch     2 | loss: 2.8596728Losses:  3.0384299755096436 2.701279640197754
MemoryTrain:  epoch  4, batch     3 | loss: 3.0384300Losses:  1.791120171546936 1.7290875911712646
MemoryTrain:  epoch  4, batch     4 | loss: 1.7911202Losses:  2.8343117237091064 2.7045609951019287
MemoryTrain:  epoch  5, batch     0 | loss: 2.8343117Losses:  2.7833797931671143 2.704181432723999
MemoryTrain:  epoch  5, batch     1 | loss: 2.7833798Losses:  2.8285439014434814 2.685581922531128
MemoryTrain:  epoch  5, batch     2 | loss: 2.8285439Losses:  2.8076677322387695 2.7219369411468506
MemoryTrain:  epoch  5, batch     3 | loss: 2.8076677Losses:  1.8028684854507446 1.7278633117675781
MemoryTrain:  epoch  5, batch     4 | loss: 1.8028685Losses:  2.745128631591797 2.6938118934631348
MemoryTrain:  epoch  6, batch     0 | loss: 2.7451286Losses:  2.769728183746338 2.7157416343688965
MemoryTrain:  epoch  6, batch     1 | loss: 2.7697282Losses:  2.8183982372283936 2.689739465713501
MemoryTrain:  epoch  6, batch     2 | loss: 2.8183982Losses:  2.786747932434082 2.7117278575897217
MemoryTrain:  epoch  6, batch     3 | loss: 2.7867479Losses:  1.7587380409240723 1.7222288846969604
MemoryTrain:  epoch  6, batch     4 | loss: 1.7587380Losses:  2.749181032180786 2.69846773147583
MemoryTrain:  epoch  7, batch     0 | loss: 2.7491810Losses:  2.767709732055664 2.704035758972168
MemoryTrain:  epoch  7, batch     1 | loss: 2.7677097Losses:  2.764126777648926 2.7021901607513428
MemoryTrain:  epoch  7, batch     2 | loss: 2.7641268Losses:  2.7827587127685547 2.7020552158355713
MemoryTrain:  epoch  7, batch     3 | loss: 2.7827587Losses:  1.7847545146942139 1.7196201086044312
MemoryTrain:  epoch  7, batch     4 | loss: 1.7847545Losses:  2.7634198665618896 2.704869270324707
MemoryTrain:  epoch  8, batch     0 | loss: 2.7634199Losses:  2.734548807144165 2.6883015632629395
MemoryTrain:  epoch  8, batch     1 | loss: 2.7345488Losses:  2.768176317214966 2.710958480834961
MemoryTrain:  epoch  8, batch     2 | loss: 2.7681763Losses:  2.7654757499694824 2.707979679107666
MemoryTrain:  epoch  8, batch     3 | loss: 2.7654757Losses:  1.7913947105407715 1.712045431137085
MemoryTrain:  epoch  8, batch     4 | loss: 1.7913947Losses:  2.8021633625030518 2.7116336822509766
MemoryTrain:  epoch  9, batch     0 | loss: 2.8021634Losses:  2.761786460876465 2.7041475772857666
MemoryTrain:  epoch  9, batch     1 | loss: 2.7617865Losses:  2.7569618225097656 2.7147204875946045
MemoryTrain:  epoch  9, batch     2 | loss: 2.7569618Losses:  2.7021844387054443 2.6709489822387695
MemoryTrain:  epoch  9, batch     3 | loss: 2.7021844Losses:  1.8058598041534424 1.73256516456604
MemoryTrain:  epoch  9, batch     4 | loss: 1.8058598
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 54.83%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 52.86%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 51.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 49.76%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 47.92%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 46.21%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 44.61%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 43.12%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 41.94%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 42.38%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 45.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 46.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 47.57%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 48.48%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 49.67%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 50.64%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 51.88%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 52.74%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 53.42%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 54.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.40%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 55.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 56.66%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 57.18%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 57.94%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 58.55%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 59.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 58.58%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 58.77%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 59.49%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 59.05%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 58.69%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 58.65%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 58.30%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 58.37%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 57.74%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   #############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.0, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  9.215940475463867 1.8152083158493042
CurrentTrain: epoch  0, batch     0 | loss: 9.2159405Losses:  9.47031021118164 2.08428955078125
CurrentTrain: epoch  0, batch     1 | loss: 9.4703102Losses:  9.293124198913574 1.9275076389312744
CurrentTrain: epoch  0, batch     2 | loss: 9.2931242Losses:  9.160496711730957 1.7856714725494385
CurrentTrain: epoch  0, batch     3 | loss: 9.1604967Losses:  9.279772758483887 1.9881689548492432
CurrentTrain: epoch  0, batch     4 | loss: 9.2797728Losses:  9.064017295837402 1.6976587772369385
CurrentTrain: epoch  0, batch     5 | loss: 9.0640173Losses:  9.21583366394043 1.9329516887664795
CurrentTrain: epoch  0, batch     6 | loss: 9.2158337Losses:  9.080413818359375 1.8803858757019043
CurrentTrain: epoch  0, batch     7 | loss: 9.0804138Losses:  9.164777755737305 1.944213628768921
CurrentTrain: epoch  0, batch     8 | loss: 9.1647778Losses:  9.201143264770508 1.9387493133544922
CurrentTrain: epoch  0, batch     9 | loss: 9.2011433Losses:  8.965120315551758 1.7725341320037842
CurrentTrain: epoch  0, batch    10 | loss: 8.9651203Losses:  8.858241081237793 1.7483980655670166
CurrentTrain: epoch  0, batch    11 | loss: 8.8582411Losses:  8.987069129943848 1.7934958934783936
CurrentTrain: epoch  0, batch    12 | loss: 8.9870691Losses:  9.101889610290527 1.948432207107544
CurrentTrain: epoch  0, batch    13 | loss: 9.1018896Losses:  8.895774841308594 1.8688175678253174
CurrentTrain: epoch  0, batch    14 | loss: 8.8957748Losses:  9.170968055725098 2.061832904815674
CurrentTrain: epoch  0, batch    15 | loss: 9.1709681Losses:  8.717938423156738 1.6075481176376343
CurrentTrain: epoch  0, batch    16 | loss: 8.7179384Losses:  9.134491920471191 2.06813383102417
CurrentTrain: epoch  0, batch    17 | loss: 9.1344919Losses:  8.903813362121582 1.8263142108917236
CurrentTrain: epoch  0, batch    18 | loss: 8.9038134Losses:  9.063835144042969 2.026698112487793
CurrentTrain: epoch  0, batch    19 | loss: 9.0638351Losses:  8.96020793914795 2.001276969909668
CurrentTrain: epoch  0, batch    20 | loss: 8.9602079Losses:  9.003429412841797 2.007176399230957
CurrentTrain: epoch  0, batch    21 | loss: 9.0034294Losses:  8.964492797851562 1.956791639328003
CurrentTrain: epoch  0, batch    22 | loss: 8.9644928Losses:  8.925716400146484 1.950577735900879
CurrentTrain: epoch  0, batch    23 | loss: 8.9257164Losses:  8.98415756225586 2.0140013694763184
CurrentTrain: epoch  0, batch    24 | loss: 8.9841576Losses:  8.9099760055542 1.9137221574783325
CurrentTrain: epoch  0, batch    25 | loss: 8.9099760Losses:  8.795507431030273 1.827035903930664
CurrentTrain: epoch  0, batch    26 | loss: 8.7955074Losses:  9.01641845703125 2.0569162368774414
CurrentTrain: epoch  0, batch    27 | loss: 9.0164185Losses:  8.885605812072754 1.893450379371643
CurrentTrain: epoch  0, batch    28 | loss: 8.8856058Losses:  8.958245277404785 1.9982045888900757
CurrentTrain: epoch  0, batch    29 | loss: 8.9582453Losses:  9.103620529174805 2.103074550628662
CurrentTrain: epoch  0, batch    30 | loss: 9.1036205Losses:  8.735355377197266 1.8135924339294434
CurrentTrain: epoch  0, batch    31 | loss: 8.7353554Losses:  8.476085662841797 1.4732286930084229
CurrentTrain: epoch  0, batch    32 | loss: 8.4760857Losses:  8.597665786743164 1.668342113494873
CurrentTrain: epoch  0, batch    33 | loss: 8.5976658Losses:  8.775960922241211 1.8387105464935303
CurrentTrain: epoch  0, batch    34 | loss: 8.7759609Losses:  8.821285247802734 1.8827476501464844
CurrentTrain: epoch  0, batch    35 | loss: 8.8212852Losses:  8.857649803161621 1.917436122894287
CurrentTrain: epoch  0, batch    36 | loss: 8.8576498Losses:  8.838216781616211 1.9307037591934204
CurrentTrain: epoch  0, batch    37 | loss: 8.8382168Losses:  9.087953567504883 2.1280555725097656
CurrentTrain: epoch  0, batch    38 | loss: 9.0879536Losses:  9.074871063232422 2.1687493324279785
CurrentTrain: epoch  0, batch    39 | loss: 9.0748711Losses:  8.97833251953125 2.0389175415039062
CurrentTrain: epoch  0, batch    40 | loss: 8.9783325Losses:  8.860401153564453 1.98246431350708
CurrentTrain: epoch  0, batch    41 | loss: 8.8604012Losses:  8.779280662536621 1.9511315822601318
CurrentTrain: epoch  0, batch    42 | loss: 8.7792807Losses:  8.474935531616211 1.6649541854858398
CurrentTrain: epoch  0, batch    43 | loss: 8.4749355Losses:  8.60382080078125 1.7528069019317627
CurrentTrain: epoch  0, batch    44 | loss: 8.6038208Losses:  8.83399772644043 2.027284860610962
CurrentTrain: epoch  0, batch    45 | loss: 8.8339977Losses:  8.722561836242676 1.8112455606460571
CurrentTrain: epoch  0, batch    46 | loss: 8.7225618Losses:  8.769768714904785 2.0282142162323
CurrentTrain: epoch  0, batch    47 | loss: 8.7697687Losses:  8.755707740783691 1.9184930324554443
CurrentTrain: epoch  0, batch    48 | loss: 8.7557077Losses:  8.720393180847168 1.9221986532211304
CurrentTrain: epoch  0, batch    49 | loss: 8.7203932Losses:  8.803797721862793 1.9857933521270752
CurrentTrain: epoch  0, batch    50 | loss: 8.8037977Losses:  8.886613845825195 2.0971972942352295
CurrentTrain: epoch  0, batch    51 | loss: 8.8866138Losses:  8.66159439086914 1.9198341369628906
CurrentTrain: epoch  0, batch    52 | loss: 8.6615944Losses:  8.637876510620117 1.8908882141113281
CurrentTrain: epoch  0, batch    53 | loss: 8.6378765Losses:  8.858247756958008 2.041311264038086
CurrentTrain: epoch  0, batch    54 | loss: 8.8582478Losses:  8.513547897338867 1.825044870376587
CurrentTrain: epoch  0, batch    55 | loss: 8.5135479Losses:  8.439898490905762 1.8009610176086426
CurrentTrain: epoch  0, batch    56 | loss: 8.4398985Losses:  8.740630149841309 1.97548508644104
CurrentTrain: epoch  0, batch    57 | loss: 8.7406301Losses:  8.429112434387207 1.7307416200637817
CurrentTrain: epoch  0, batch    58 | loss: 8.4291124Losses:  8.368651390075684 1.703507661819458
CurrentTrain: epoch  0, batch    59 | loss: 8.3686514Losses:  8.559268951416016 1.9324570894241333
CurrentTrain: epoch  0, batch    60 | loss: 8.5592690Losses:  8.446965217590332 1.7462742328643799
CurrentTrain: epoch  0, batch    61 | loss: 8.4469652Losses:  8.322917938232422 1.4515635967254639
CurrentTrain: epoch  0, batch    62 | loss: 8.3229179Losses:  8.521244049072266 1.780718207359314
CurrentTrain: epoch  1, batch     0 | loss: 8.5212440Losses:  8.539259910583496 1.8767627477645874
CurrentTrain: epoch  1, batch     1 | loss: 8.5392599Losses:  8.421278953552246 1.765610933303833
CurrentTrain: epoch  1, batch     2 | loss: 8.4212790Losses:  8.638872146606445 2.035128116607666
CurrentTrain: epoch  1, batch     3 | loss: 8.6388721Losses:  8.74472427368164 2.0044028759002686
CurrentTrain: epoch  1, batch     4 | loss: 8.7447243Losses:  8.5450439453125 1.91034996509552
CurrentTrain: epoch  1, batch     5 | loss: 8.5450439Losses:  8.514202117919922 1.8999085426330566
CurrentTrain: epoch  1, batch     6 | loss: 8.5142021Losses:  8.5807523727417 2.0344443321228027
CurrentTrain: epoch  1, batch     7 | loss: 8.5807524Losses:  8.242182731628418 1.7083089351654053
CurrentTrain: epoch  1, batch     8 | loss: 8.2421827Losses:  8.585029602050781 1.9546163082122803
CurrentTrain: epoch  1, batch     9 | loss: 8.5850296Losses:  8.56478214263916 1.9218329191207886
CurrentTrain: epoch  1, batch    10 | loss: 8.5647821Losses:  8.53391170501709 1.9339277744293213
CurrentTrain: epoch  1, batch    11 | loss: 8.5339117Losses:  8.599905967712402 1.9879224300384521
CurrentTrain: epoch  1, batch    12 | loss: 8.5999060Losses:  8.213630676269531 1.6104587316513062
CurrentTrain: epoch  1, batch    13 | loss: 8.2136307Losses:  8.468624114990234 1.929634690284729
CurrentTrain: epoch  1, batch    14 | loss: 8.4686241Losses:  8.604000091552734 1.8688950538635254
CurrentTrain: epoch  1, batch    15 | loss: 8.6040001Losses:  8.29659652709961 1.747414231300354
CurrentTrain: epoch  1, batch    16 | loss: 8.2965965Losses:  8.049410820007324 1.5814517736434937
CurrentTrain: epoch  1, batch    17 | loss: 8.0494108Losses:  8.31374454498291 1.806696891784668
CurrentTrain: epoch  1, batch    18 | loss: 8.3137445Losses:  8.475903511047363 1.947744607925415
CurrentTrain: epoch  1, batch    19 | loss: 8.4759035Losses:  8.564727783203125 2.023860216140747
CurrentTrain: epoch  1, batch    20 | loss: 8.5647278Losses:  8.555699348449707 2.017669916152954
CurrentTrain: epoch  1, batch    21 | loss: 8.5556993Losses:  8.387996673583984 1.867599368095398
CurrentTrain: epoch  1, batch    22 | loss: 8.3879967Losses:  8.276382446289062 1.7012534141540527
CurrentTrain: epoch  1, batch    23 | loss: 8.2763824Losses:  8.436363220214844 1.8824107646942139
CurrentTrain: epoch  1, batch    24 | loss: 8.4363632Losses:  8.178678512573242 1.7995576858520508
CurrentTrain: epoch  1, batch    25 | loss: 8.1786785Losses:  8.467829704284668 2.010953187942505
CurrentTrain: epoch  1, batch    26 | loss: 8.4678297Losses:  8.418512344360352 1.9193713665008545
CurrentTrain: epoch  1, batch    27 | loss: 8.4185123Losses:  8.393128395080566 1.940231204032898
CurrentTrain: epoch  1, batch    28 | loss: 8.3931284Losses:  8.39781379699707 2.0088491439819336
CurrentTrain: epoch  1, batch    29 | loss: 8.3978138Losses:  8.311339378356934 1.858909010887146
CurrentTrain: epoch  1, batch    30 | loss: 8.3113394Losses:  8.423377990722656 2.0561556816101074
CurrentTrain: epoch  1, batch    31 | loss: 8.4233780Losses:  8.203279495239258 1.9134321212768555
CurrentTrain: epoch  1, batch    32 | loss: 8.2032795Losses:  8.089404106140137 1.692233920097351
CurrentTrain: epoch  1, batch    33 | loss: 8.0894041Losses:  8.426608085632324 1.9737322330474854
CurrentTrain: epoch  1, batch    34 | loss: 8.4266081Losses:  8.138259887695312 1.8341221809387207
CurrentTrain: epoch  1, batch    35 | loss: 8.1382599Losses:  8.193883895874023 1.6732197999954224
CurrentTrain: epoch  1, batch    36 | loss: 8.1938839Losses:  8.443573951721191 1.9567352533340454
CurrentTrain: epoch  1, batch    37 | loss: 8.4435740Losses:  8.414285659790039 2.0074753761291504
CurrentTrain: epoch  1, batch    38 | loss: 8.4142857Losses:  8.26486587524414 1.9321314096450806
CurrentTrain: epoch  1, batch    39 | loss: 8.2648659Losses:  8.31114387512207 1.8466973304748535
CurrentTrain: epoch  1, batch    40 | loss: 8.3111439Losses:  8.439986228942871 1.9594358205795288
CurrentTrain: epoch  1, batch    41 | loss: 8.4399862Losses:  8.21432876586914 1.861634612083435
CurrentTrain: epoch  1, batch    42 | loss: 8.2143288Losses:  8.307807922363281 1.8863930702209473
CurrentTrain: epoch  1, batch    43 | loss: 8.3078079Losses:  8.211328506469727 1.732479214668274
CurrentTrain: epoch  1, batch    44 | loss: 8.2113285Losses:  8.288504600524902 1.9791380167007446
CurrentTrain: epoch  1, batch    45 | loss: 8.2885046Losses:  8.160569190979004 1.9176337718963623
CurrentTrain: epoch  1, batch    46 | loss: 8.1605692Losses:  8.463130950927734 2.096311092376709
CurrentTrain: epoch  1, batch    47 | loss: 8.4631310Losses:  8.245525360107422 1.8687481880187988
CurrentTrain: epoch  1, batch    48 | loss: 8.2455254Losses:  8.174339294433594 1.8769712448120117
CurrentTrain: epoch  1, batch    49 | loss: 8.1743393Losses:  8.101580619812012 1.8273671865463257
CurrentTrain: epoch  1, batch    50 | loss: 8.1015806Losses:  8.216115951538086 1.7430338859558105
CurrentTrain: epoch  1, batch    51 | loss: 8.2161160Losses:  8.048179626464844 1.7964129447937012
CurrentTrain: epoch  1, batch    52 | loss: 8.0481796Losses:  8.101933479309082 1.8213012218475342
CurrentTrain: epoch  1, batch    53 | loss: 8.1019335Losses:  8.15945816040039 1.9027169942855835
CurrentTrain: epoch  1, batch    54 | loss: 8.1594582Losses:  7.816411972045898 1.6688525676727295
CurrentTrain: epoch  1, batch    55 | loss: 7.8164120Losses:  7.808350086212158 1.6833654642105103
CurrentTrain: epoch  1, batch    56 | loss: 7.8083501Losses:  8.14867877960205 1.926191806793213
CurrentTrain: epoch  1, batch    57 | loss: 8.1486788Losses:  7.979426383972168 1.737472414970398
CurrentTrain: epoch  1, batch    58 | loss: 7.9794264Losses:  8.354248046875 2.010094165802002
CurrentTrain: epoch  1, batch    59 | loss: 8.3542480Losses:  8.233420372009277 1.9558714628219604
CurrentTrain: epoch  1, batch    60 | loss: 8.2334204Losses:  8.202290534973145 1.9762855768203735
CurrentTrain: epoch  1, batch    61 | loss: 8.2022905Losses:  7.368002891540527 1.361570119857788
CurrentTrain: epoch  1, batch    62 | loss: 7.3680029Losses:  7.874234676361084 1.633506178855896
CurrentTrain: epoch  2, batch     0 | loss: 7.8742347Losses:  7.807908535003662 1.6162829399108887
CurrentTrain: epoch  2, batch     1 | loss: 7.8079085Losses:  8.09338092803955 1.9088857173919678
CurrentTrain: epoch  2, batch     2 | loss: 8.0933809Losses:  7.946387767791748 1.8538841009140015
CurrentTrain: epoch  2, batch     3 | loss: 7.9463878Losses:  7.960203170776367 1.8150044679641724
CurrentTrain: epoch  2, batch     4 | loss: 7.9602032Losses:  8.191937446594238 2.063897132873535
CurrentTrain: epoch  2, batch     5 | loss: 8.1919374Losses:  7.775148868560791 1.685762882232666
CurrentTrain: epoch  2, batch     6 | loss: 7.7751489Losses:  8.11390495300293 1.8826584815979004
CurrentTrain: epoch  2, batch     7 | loss: 8.1139050Losses:  8.174409866333008 1.8829067945480347
CurrentTrain: epoch  2, batch     8 | loss: 8.1744099Losses:  7.906970977783203 1.7782399654388428
CurrentTrain: epoch  2, batch     9 | loss: 7.9069710Losses:  7.934440612792969 1.860764503479004
CurrentTrain: epoch  2, batch    10 | loss: 7.9344406Losses:  7.996356010437012 1.8101099729537964
CurrentTrain: epoch  2, batch    11 | loss: 7.9963560Losses:  7.987283229827881 1.752532958984375
CurrentTrain: epoch  2, batch    12 | loss: 7.9872832Losses:  7.94603157043457 1.706106185913086
CurrentTrain: epoch  2, batch    13 | loss: 7.9460316Losses:  8.186970710754395 1.9200712442398071
CurrentTrain: epoch  2, batch    14 | loss: 8.1869707Losses:  8.147948265075684 1.9738558530807495
CurrentTrain: epoch  2, batch    15 | loss: 8.1479483Losses:  7.897994518280029 1.8446918725967407
CurrentTrain: epoch  2, batch    16 | loss: 7.8979945Losses:  7.5545854568481445 1.543471097946167
CurrentTrain: epoch  2, batch    17 | loss: 7.5545855Losses:  8.252738952636719 2.006132125854492
CurrentTrain: epoch  2, batch    18 | loss: 8.2527390Losses:  8.012626647949219 1.8206274509429932
CurrentTrain: epoch  2, batch    19 | loss: 8.0126266Losses:  7.975551605224609 1.8842628002166748
CurrentTrain: epoch  2, batch    20 | loss: 7.9755516Losses:  7.947116374969482 1.856587290763855
CurrentTrain: epoch  2, batch    21 | loss: 7.9471164Losses:  7.867985725402832 1.8321971893310547
CurrentTrain: epoch  2, batch    22 | loss: 7.8679857Losses:  8.128788948059082 1.9255253076553345
CurrentTrain: epoch  2, batch    23 | loss: 8.1287889Losses:  7.888887405395508 1.8063743114471436
CurrentTrain: epoch  2, batch    24 | loss: 7.8888874Losses:  7.956377029418945 1.8646023273468018
CurrentTrain: epoch  2, batch    25 | loss: 7.9563770Losses:  7.923252105712891 1.8452181816101074
CurrentTrain: epoch  2, batch    26 | loss: 7.9232521Losses:  7.738015174865723 1.6173276901245117
CurrentTrain: epoch  2, batch    27 | loss: 7.7380152Losses:  7.793405532836914 1.7425585985183716
CurrentTrain: epoch  2, batch    28 | loss: 7.7934055Losses:  8.115653991699219 2.041402816772461
CurrentTrain: epoch  2, batch    29 | loss: 8.1156540Losses:  8.088713645935059 1.944817304611206
CurrentTrain: epoch  2, batch    30 | loss: 8.0887136Losses:  8.152765274047852 2.0920844078063965
CurrentTrain: epoch  2, batch    31 | loss: 8.1527653Losses:  7.99155855178833 1.718135952949524
CurrentTrain: epoch  2, batch    32 | loss: 7.9915586Losses:  7.766305923461914 1.7582474946975708
CurrentTrain: epoch  2, batch    33 | loss: 7.7663059Losses:  8.061817169189453 1.9409259557724
CurrentTrain: epoch  2, batch    34 | loss: 8.0618172Losses:  7.967186450958252 1.8071752786636353
CurrentTrain: epoch  2, batch    35 | loss: 7.9671865Losses:  7.917501449584961 1.9127652645111084
CurrentTrain: epoch  2, batch    36 | loss: 7.9175014Losses:  7.751631736755371 1.6760951280593872
CurrentTrain: epoch  2, batch    37 | loss: 7.7516317Losses:  7.900460243225098 1.7913901805877686
CurrentTrain: epoch  2, batch    38 | loss: 7.9004602Losses:  7.617704391479492 1.6780707836151123
CurrentTrain: epoch  2, batch    39 | loss: 7.6177044Losses:  7.555904388427734 1.5840040445327759
CurrentTrain: epoch  2, batch    40 | loss: 7.5559044Losses:  7.597879886627197 1.4239181280136108
CurrentTrain: epoch  2, batch    41 | loss: 7.5978799Losses:  7.856935024261475 1.84518301486969
CurrentTrain: epoch  2, batch    42 | loss: 7.8569350Losses:  7.958351135253906 1.8148157596588135
CurrentTrain: epoch  2, batch    43 | loss: 7.9583511Losses:  7.99800443649292 1.9418226480484009
CurrentTrain: epoch  2, batch    44 | loss: 7.9980044Losses:  8.006143569946289 1.9019477367401123
CurrentTrain: epoch  2, batch    45 | loss: 8.0061436Losses:  7.889585494995117 1.8933331966400146
CurrentTrain: epoch  2, batch    46 | loss: 7.8895855Losses:  7.954111576080322 1.8844043016433716
CurrentTrain: epoch  2, batch    47 | loss: 7.9541116Losses:  7.812585830688477 1.811469316482544
CurrentTrain: epoch  2, batch    48 | loss: 7.8125858Losses:  7.928012847900391 1.8708903789520264
CurrentTrain: epoch  2, batch    49 | loss: 7.9280128Losses:  7.668425559997559 1.685137152671814
CurrentTrain: epoch  2, batch    50 | loss: 7.6684256Losses:  7.900478839874268 1.9227644205093384
CurrentTrain: epoch  2, batch    51 | loss: 7.9004788Losses:  7.814445972442627 1.7057756185531616
CurrentTrain: epoch  2, batch    52 | loss: 7.8144460Losses:  7.941551208496094 1.962478756904602
CurrentTrain: epoch  2, batch    53 | loss: 7.9415512Losses:  7.9103899002075195 1.908111572265625
CurrentTrain: epoch  2, batch    54 | loss: 7.9103899Losses:  7.96959924697876 1.9714425802230835
CurrentTrain: epoch  2, batch    55 | loss: 7.9695992Losses:  8.055656433105469 2.041088581085205
CurrentTrain: epoch  2, batch    56 | loss: 8.0556564Losses:  7.831817626953125 1.7605007886886597
CurrentTrain: epoch  2, batch    57 | loss: 7.8318176Losses:  7.760377883911133 1.733447790145874
CurrentTrain: epoch  2, batch    58 | loss: 7.7603779Losses:  7.827659606933594 1.8473215103149414
CurrentTrain: epoch  2, batch    59 | loss: 7.8276596Losses:  7.81395959854126 1.855729579925537
CurrentTrain: epoch  2, batch    60 | loss: 7.8139596Losses:  7.713909149169922 1.7343997955322266
CurrentTrain: epoch  2, batch    61 | loss: 7.7139091Losses:  7.529751777648926 1.5833561420440674
CurrentTrain: epoch  2, batch    62 | loss: 7.5297518Losses:  7.816219329833984 1.812464952468872
CurrentTrain: epoch  3, batch     0 | loss: 7.8162193Losses:  7.7638726234436035 1.7014579772949219
CurrentTrain: epoch  3, batch     1 | loss: 7.7638726Losses:  7.869985580444336 1.898012399673462
CurrentTrain: epoch  3, batch     2 | loss: 7.8699856Losses:  7.619704723358154 1.6676182746887207
CurrentTrain: epoch  3, batch     3 | loss: 7.6197047Losses:  7.730976104736328 1.7950257062911987
CurrentTrain: epoch  3, batch     4 | loss: 7.7309761Losses:  7.830931663513184 1.8214980363845825
CurrentTrain: epoch  3, batch     5 | loss: 7.8309317Losses:  7.739802837371826 1.8002935647964478
CurrentTrain: epoch  3, batch     6 | loss: 7.7398028Losses:  7.837015151977539 1.9127006530761719
CurrentTrain: epoch  3, batch     7 | loss: 7.8370152Losses:  7.8296685218811035 1.8233332633972168
CurrentTrain: epoch  3, batch     8 | loss: 7.8296685Losses:  7.934107303619385 1.9525823593139648
CurrentTrain: epoch  3, batch     9 | loss: 7.9341073Losses:  8.029431343078613 2.041961193084717
CurrentTrain: epoch  3, batch    10 | loss: 8.0294313Losses:  7.806103229522705 1.807753086090088
CurrentTrain: epoch  3, batch    11 | loss: 7.8061032Losses:  7.677926063537598 1.7892320156097412
CurrentTrain: epoch  3, batch    12 | loss: 7.6779261Losses:  8.008804321289062 2.0324976444244385
CurrentTrain: epoch  3, batch    13 | loss: 8.0088043Losses:  7.795617580413818 1.885040283203125
CurrentTrain: epoch  3, batch    14 | loss: 7.7956176Losses:  7.80617618560791 1.91255784034729
CurrentTrain: epoch  3, batch    15 | loss: 7.8061762Losses:  7.761064529418945 1.8564326763153076
CurrentTrain: epoch  3, batch    16 | loss: 7.7610645Losses:  7.761554718017578 1.6485240459442139
CurrentTrain: epoch  3, batch    17 | loss: 7.7615547Losses:  7.669217109680176 1.7644321918487549
CurrentTrain: epoch  3, batch    18 | loss: 7.6692171Losses:  7.815807342529297 1.8130148649215698
CurrentTrain: epoch  3, batch    19 | loss: 7.8158073Losses:  7.786350727081299 1.821651577949524
CurrentTrain: epoch  3, batch    20 | loss: 7.7863507Losses:  7.859013557434082 1.9353255033493042
CurrentTrain: epoch  3, batch    21 | loss: 7.8590136Losses:  7.581076622009277 1.6604732275009155
CurrentTrain: epoch  3, batch    22 | loss: 7.5810766Losses:  7.85278844833374 1.9326785802841187
CurrentTrain: epoch  3, batch    23 | loss: 7.8527884Losses:  7.488168716430664 1.5805786848068237
CurrentTrain: epoch  3, batch    24 | loss: 7.4881687Losses:  7.919390678405762 2.0277583599090576
CurrentTrain: epoch  3, batch    25 | loss: 7.9193907Losses:  7.953098297119141 2.0192205905914307
CurrentTrain: epoch  3, batch    26 | loss: 7.9530983Losses:  7.763510704040527 1.8645329475402832
CurrentTrain: epoch  3, batch    27 | loss: 7.7635107Losses:  7.965132236480713 2.0519399642944336
CurrentTrain: epoch  3, batch    28 | loss: 7.9651322Losses:  7.73423433303833 1.7768502235412598
CurrentTrain: epoch  3, batch    29 | loss: 7.7342343Losses:  8.015039443969727 1.987630844116211
CurrentTrain: epoch  3, batch    30 | loss: 8.0150394Losses:  7.7224578857421875 1.7960803508758545
CurrentTrain: epoch  3, batch    31 | loss: 7.7224579Losses:  7.5501627922058105 1.5934463739395142
CurrentTrain: epoch  3, batch    32 | loss: 7.5501628Losses:  7.681938648223877 1.7390308380126953
CurrentTrain: epoch  3, batch    33 | loss: 7.6819386Losses:  7.7191362380981445 1.8154501914978027
CurrentTrain: epoch  3, batch    34 | loss: 7.7191362Losses:  7.770109176635742 1.8710613250732422
CurrentTrain: epoch  3, batch    35 | loss: 7.7701092Losses:  7.905587196350098 1.8881347179412842
CurrentTrain: epoch  3, batch    36 | loss: 7.9055872Losses:  7.824831962585449 1.8404591083526611
CurrentTrain: epoch  3, batch    37 | loss: 7.8248320Losses:  7.801338195800781 1.910949468612671
CurrentTrain: epoch  3, batch    38 | loss: 7.8013382Losses:  7.591158866882324 1.5835294723510742
CurrentTrain: epoch  3, batch    39 | loss: 7.5911589Losses:  7.82962703704834 1.9401302337646484
CurrentTrain: epoch  3, batch    40 | loss: 7.8296270Losses:  7.519046783447266 1.6687252521514893
CurrentTrain: epoch  3, batch    41 | loss: 7.5190468Losses:  7.57725715637207 1.6383640766143799
CurrentTrain: epoch  3, batch    42 | loss: 7.5772572Losses:  7.669458389282227 1.7518309354782104
CurrentTrain: epoch  3, batch    43 | loss: 7.6694584Losses:  7.948873996734619 1.9403409957885742
CurrentTrain: epoch  3, batch    44 | loss: 7.9488740Losses:  7.634491920471191 1.6818554401397705
CurrentTrain: epoch  3, batch    45 | loss: 7.6344919Losses:  7.771993637084961 1.9023773670196533
CurrentTrain: epoch  3, batch    46 | loss: 7.7719936Losses:  7.509280204772949 1.6187684535980225
CurrentTrain: epoch  3, batch    47 | loss: 7.5092802Losses:  7.301515579223633 1.464651346206665
CurrentTrain: epoch  3, batch    48 | loss: 7.3015156Losses:  7.428536891937256 1.5097330808639526
CurrentTrain: epoch  3, batch    49 | loss: 7.4285369Losses:  7.787073135375977 1.908774733543396
CurrentTrain: epoch  3, batch    50 | loss: 7.7870731Losses:  7.545641899108887 1.6496045589447021
CurrentTrain: epoch  3, batch    51 | loss: 7.5456419Losses:  7.628204822540283 1.7593172788619995
CurrentTrain: epoch  3, batch    52 | loss: 7.6282048Losses:  7.83034610748291 1.8482791185379028
CurrentTrain: epoch  3, batch    53 | loss: 7.8303461Losses:  7.711430072784424 1.8572536706924438
CurrentTrain: epoch  3, batch    54 | loss: 7.7114301Losses:  7.899038791656494 2.0169906616210938
CurrentTrain: epoch  3, batch    55 | loss: 7.8990388Losses:  7.664148807525635 1.7602704763412476
CurrentTrain: epoch  3, batch    56 | loss: 7.6641488Losses:  7.680001258850098 1.7701337337493896
CurrentTrain: epoch  3, batch    57 | loss: 7.6800013Losses:  7.710557460784912 1.7599340677261353
CurrentTrain: epoch  3, batch    58 | loss: 7.7105575Losses:  7.405165195465088 1.5407514572143555
CurrentTrain: epoch  3, batch    59 | loss: 7.4051652Losses:  7.832888126373291 1.9543547630310059
CurrentTrain: epoch  3, batch    60 | loss: 7.8328881Losses:  7.673114776611328 1.8244467973709106
CurrentTrain: epoch  3, batch    61 | loss: 7.6731148Losses:  7.255343437194824 1.3986361026763916
CurrentTrain: epoch  3, batch    62 | loss: 7.2553434Losses:  7.273717880249023 1.453137755393982
CurrentTrain: epoch  4, batch     0 | loss: 7.2737179Losses:  7.574868679046631 1.6600645780563354
CurrentTrain: epoch  4, batch     1 | loss: 7.5748687Losses:  7.667245864868164 1.7995539903640747
CurrentTrain: epoch  4, batch     2 | loss: 7.6672459Losses:  7.461297512054443 1.629056453704834
CurrentTrain: epoch  4, batch     3 | loss: 7.4612975Losses:  7.885220527648926 2.0174472332000732
CurrentTrain: epoch  4, batch     4 | loss: 7.8852205Losses:  7.8649210929870605 2.01173734664917
CurrentTrain: epoch  4, batch     5 | loss: 7.8649211Losses:  7.577775955200195 1.7334883213043213
CurrentTrain: epoch  4, batch     6 | loss: 7.5777760Losses:  7.439878463745117 1.6013882160186768
CurrentTrain: epoch  4, batch     7 | loss: 7.4398785Losses:  7.859203815460205 2.0086398124694824
CurrentTrain: epoch  4, batch     8 | loss: 7.8592038Losses:  7.6253342628479 1.7736531496047974
CurrentTrain: epoch  4, batch     9 | loss: 7.6253343Losses:  7.610283851623535 1.7597181797027588
CurrentTrain: epoch  4, batch    10 | loss: 7.6102839Losses:  7.643609046936035 1.7740861177444458
CurrentTrain: epoch  4, batch    11 | loss: 7.6436090Losses:  7.628831386566162 1.7630829811096191
CurrentTrain: epoch  4, batch    12 | loss: 7.6288314Losses:  7.8069658279418945 1.9003264904022217
CurrentTrain: epoch  4, batch    13 | loss: 7.8069658Losses:  7.433888912200928 1.6107429265975952
CurrentTrain: epoch  4, batch    14 | loss: 7.4338889Losses:  7.569948673248291 1.7069811820983887
CurrentTrain: epoch  4, batch    15 | loss: 7.5699487Losses:  7.556178092956543 1.707012414932251
CurrentTrain: epoch  4, batch    16 | loss: 7.5561781Losses:  7.526128768920898 1.6982085704803467
CurrentTrain: epoch  4, batch    17 | loss: 7.5261288Losses:  7.723936080932617 1.8863834142684937
CurrentTrain: epoch  4, batch    18 | loss: 7.7239361Losses:  7.659821510314941 1.8381588459014893
CurrentTrain: epoch  4, batch    19 | loss: 7.6598215Losses:  7.599467754364014 1.633634090423584
CurrentTrain: epoch  4, batch    20 | loss: 7.5994678Losses:  7.367870330810547 1.5095088481903076
CurrentTrain: epoch  4, batch    21 | loss: 7.3678703Losses:  7.496875762939453 1.6918087005615234
CurrentTrain: epoch  4, batch    22 | loss: 7.4968758Losses:  7.253655433654785 1.4506418704986572
CurrentTrain: epoch  4, batch    23 | loss: 7.2536554Losses:  7.324012279510498 1.4430665969848633
CurrentTrain: epoch  4, batch    24 | loss: 7.3240123Losses:  7.746216297149658 1.9097943305969238
CurrentTrain: epoch  4, batch    25 | loss: 7.7462163Losses:  7.60402774810791 1.7939221858978271
CurrentTrain: epoch  4, batch    26 | loss: 7.6040277Losses:  7.575690746307373 1.7339591979980469
CurrentTrain: epoch  4, batch    27 | loss: 7.5756907Losses:  7.578733921051025 1.6944804191589355
CurrentTrain: epoch  4, batch    28 | loss: 7.5787339Losses:  7.572957515716553 1.717654824256897
CurrentTrain: epoch  4, batch    29 | loss: 7.5729575Losses:  7.603270053863525 1.7807585000991821
CurrentTrain: epoch  4, batch    30 | loss: 7.6032701Losses:  7.595670223236084 1.7083684206008911
CurrentTrain: epoch  4, batch    31 | loss: 7.5956702Losses:  7.6399431228637695 1.7945650815963745
CurrentTrain: epoch  4, batch    32 | loss: 7.6399431Losses:  7.744653701782227 1.8861740827560425
CurrentTrain: epoch  4, batch    33 | loss: 7.7446537Losses:  7.603076934814453 1.7751718759536743
CurrentTrain: epoch  4, batch    34 | loss: 7.6030769Losses:  7.822518825531006 1.9264435768127441
CurrentTrain: epoch  4, batch    35 | loss: 7.8225188Losses:  7.479101181030273 1.608292818069458
CurrentTrain: epoch  4, batch    36 | loss: 7.4791012Losses:  7.734119892120361 1.8689765930175781
CurrentTrain: epoch  4, batch    37 | loss: 7.7341199Losses:  7.5267133712768555 1.7061364650726318
CurrentTrain: epoch  4, batch    38 | loss: 7.5267134Losses:  7.58114767074585 1.7716635465621948
CurrentTrain: epoch  4, batch    39 | loss: 7.5811477Losses:  7.530250072479248 1.7184062004089355
CurrentTrain: epoch  4, batch    40 | loss: 7.5302501Losses:  7.639762878417969 1.827005386352539
CurrentTrain: epoch  4, batch    41 | loss: 7.6397629Losses:  7.712087631225586 1.7530592679977417
CurrentTrain: epoch  4, batch    42 | loss: 7.7120876Losses:  7.434518337249756 1.5053778886795044
CurrentTrain: epoch  4, batch    43 | loss: 7.4345183Losses:  7.363696098327637 1.557283639907837
CurrentTrain: epoch  4, batch    44 | loss: 7.3636961Losses:  7.469653606414795 1.5784125328063965
CurrentTrain: epoch  4, batch    45 | loss: 7.4696536Losses:  7.599406719207764 1.7596269845962524
CurrentTrain: epoch  4, batch    46 | loss: 7.5994067Losses:  7.580620765686035 1.768315076828003
CurrentTrain: epoch  4, batch    47 | loss: 7.5806208Losses:  7.7206196784973145 1.9025837182998657
CurrentTrain: epoch  4, batch    48 | loss: 7.7206197Losses:  7.647001266479492 1.822495698928833
CurrentTrain: epoch  4, batch    49 | loss: 7.6470013Losses:  7.805603981018066 1.9780850410461426
CurrentTrain: epoch  4, batch    50 | loss: 7.8056040Losses:  7.538705825805664 1.6971533298492432
CurrentTrain: epoch  4, batch    51 | loss: 7.5387058Losses:  7.346640110015869 1.5534042119979858
CurrentTrain: epoch  4, batch    52 | loss: 7.3466401Losses:  7.603872776031494 1.7525668144226074
CurrentTrain: epoch  4, batch    53 | loss: 7.6038728Losses:  7.450490951538086 1.6189645528793335
CurrentTrain: epoch  4, batch    54 | loss: 7.4504910Losses:  7.627561569213867 1.8146390914916992
CurrentTrain: epoch  4, batch    55 | loss: 7.6275616Losses:  7.421863555908203 1.621759295463562
CurrentTrain: epoch  4, batch    56 | loss: 7.4218636Losses:  7.821428298950195 1.9970324039459229
CurrentTrain: epoch  4, batch    57 | loss: 7.8214283Losses:  7.573413848876953 1.7565444707870483
CurrentTrain: epoch  4, batch    58 | loss: 7.5734138Losses:  7.579801559448242 1.760696291923523
CurrentTrain: epoch  4, batch    59 | loss: 7.5798016Losses:  7.548410892486572 1.7390241622924805
CurrentTrain: epoch  4, batch    60 | loss: 7.5484109Losses:  7.653306484222412 1.82317054271698
CurrentTrain: epoch  4, batch    61 | loss: 7.6533065Losses:  7.408995151519775 1.4136590957641602
CurrentTrain: epoch  4, batch    62 | loss: 7.4089952Losses:  7.771792888641357 1.9523615837097168
CurrentTrain: epoch  5, batch     0 | loss: 7.7717929Losses:  7.658390998840332 1.8541450500488281
CurrentTrain: epoch  5, batch     1 | loss: 7.6583910Losses:  7.45601224899292 1.6275014877319336
CurrentTrain: epoch  5, batch     2 | loss: 7.4560122Losses:  7.56193733215332 1.7585535049438477
CurrentTrain: epoch  5, batch     3 | loss: 7.5619373Losses:  7.636707305908203 1.7083604335784912
CurrentTrain: epoch  5, batch     4 | loss: 7.6367073Losses:  7.538875579833984 1.7198436260223389
CurrentTrain: epoch  5, batch     5 | loss: 7.5388756Losses:  7.6542558670043945 1.830908179283142
CurrentTrain: epoch  5, batch     6 | loss: 7.6542559Losses:  7.676692962646484 1.857966423034668
CurrentTrain: epoch  5, batch     7 | loss: 7.6766930#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  8.299175262451172 1.8152083158493042
CurrentTrain: epoch  0, batch     0 | loss: 8.2991753Losses:  8.565143585205078 2.0843472480773926
CurrentTrain: epoch  0, batch     1 | loss: 8.5651436Losses:  8.402494430541992 1.9276912212371826
CurrentTrain: epoch  0, batch     2 | loss: 8.4024944Losses:  8.275490760803223 1.7858058214187622
CurrentTrain: epoch  0, batch     3 | loss: 8.2754908Losses:  8.407060623168945 1.9882845878601074
CurrentTrain: epoch  0, batch     4 | loss: 8.4070606Losses:  8.200249671936035 1.698082685470581
CurrentTrain: epoch  0, batch     5 | loss: 8.2002497Losses:  8.35329532623291 1.9329583644866943
CurrentTrain: epoch  0, batch     6 | loss: 8.3532953Losses:  8.253837585449219 1.8804433345794678
CurrentTrain: epoch  0, batch     7 | loss: 8.2538376Losses:  8.330718040466309 1.9444189071655273
CurrentTrain: epoch  0, batch     8 | loss: 8.3307180Losses:  8.390596389770508 1.9388748407363892
CurrentTrain: epoch  0, batch     9 | loss: 8.3905964Losses:  8.14656925201416 1.7729367017745972
CurrentTrain: epoch  0, batch    10 | loss: 8.1465693Losses:  8.055900573730469 1.746852159500122
CurrentTrain: epoch  0, batch    11 | loss: 8.0559006Losses:  8.198514938354492 1.7933361530303955
CurrentTrain: epoch  0, batch    12 | loss: 8.1985149Losses:  8.327463150024414 1.948751449584961
CurrentTrain: epoch  0, batch    13 | loss: 8.3274632Losses:  8.148519515991211 1.8665175437927246
CurrentTrain: epoch  0, batch    14 | loss: 8.1485195Losses:  8.415699005126953 2.0582211017608643
CurrentTrain: epoch  0, batch    15 | loss: 8.4156990Losses:  8.016707420349121 1.605133295059204
CurrentTrain: epoch  0, batch    16 | loss: 8.0167074Losses:  8.417896270751953 2.0634546279907227
CurrentTrain: epoch  0, batch    17 | loss: 8.4178963Losses:  8.194138526916504 1.8216769695281982
CurrentTrain: epoch  0, batch    18 | loss: 8.1941385Losses:  8.345352172851562 2.0217316150665283
CurrentTrain: epoch  0, batch    19 | loss: 8.3453522Losses:  8.228422164916992 1.9908232688903809
CurrentTrain: epoch  0, batch    20 | loss: 8.2284222Losses:  8.263181686401367 2.000094175338745
CurrentTrain: epoch  0, batch    21 | loss: 8.2631817Losses:  8.237164497375488 1.947885274887085
CurrentTrain: epoch  0, batch    22 | loss: 8.2371645Losses:  8.184846878051758 1.9367783069610596
CurrentTrain: epoch  0, batch    23 | loss: 8.1848469Losses:  8.240459442138672 2.005939483642578
CurrentTrain: epoch  0, batch    24 | loss: 8.2404594Losses:  8.15278434753418 1.9030497074127197
CurrentTrain: epoch  0, batch    25 | loss: 8.1527843Losses:  8.051032066345215 1.8203601837158203
CurrentTrain: epoch  0, batch    26 | loss: 8.0510321Losses:  8.23648738861084 2.0462732315063477
CurrentTrain: epoch  0, batch    27 | loss: 8.2364874Losses:  8.112916946411133 1.8820481300354004
CurrentTrain: epoch  0, batch    28 | loss: 8.1129169Losses:  8.195828437805176 1.9839308261871338
CurrentTrain: epoch  0, batch    29 | loss: 8.1958284Losses:  8.357759475708008 2.098872661590576
CurrentTrain: epoch  0, batch    30 | loss: 8.3577595Losses:  7.987374305725098 1.8052282333374023
CurrentTrain: epoch  0, batch    31 | loss: 7.9873743Losses:  7.695796489715576 1.463900089263916
CurrentTrain: epoch  0, batch    32 | loss: 7.6957965Losses:  7.8252763748168945 1.662717580795288
CurrentTrain: epoch  0, batch    33 | loss: 7.8252764Losses:  8.017763137817383 1.827895164489746
CurrentTrain: epoch  0, batch    34 | loss: 8.0177631Losses:  8.074307441711426 1.877494215965271
CurrentTrain: epoch  0, batch    35 | loss: 8.0743074Losses:  8.112038612365723 1.9132065773010254
CurrentTrain: epoch  0, batch    36 | loss: 8.1120386Losses:  8.083212852478027 1.92600417137146
CurrentTrain: epoch  0, batch    37 | loss: 8.0832129Losses:  8.341758728027344 2.1235461235046387
CurrentTrain: epoch  0, batch    38 | loss: 8.3417587Losses:  8.319787979125977 2.161433696746826
CurrentTrain: epoch  0, batch    39 | loss: 8.3197880Losses:  8.23261833190918 2.0350093841552734
CurrentTrain: epoch  0, batch    40 | loss: 8.2326183Losses:  8.096360206604004 1.978270411491394
CurrentTrain: epoch  0, batch    41 | loss: 8.0963602Losses:  8.004024505615234 1.9424480199813843
CurrentTrain: epoch  0, batch    42 | loss: 8.0040245Losses:  7.703477382659912 1.6607050895690918
CurrentTrain: epoch  0, batch    43 | loss: 7.7034774Losses:  7.831560134887695 1.7461683750152588
CurrentTrain: epoch  0, batch    44 | loss: 7.8315601Losses:  8.080696105957031 2.0245361328125
CurrentTrain: epoch  0, batch    45 | loss: 8.0806961Losses:  7.976883888244629 1.8055379390716553
CurrentTrain: epoch  0, batch    46 | loss: 7.9768839Losses:  7.992776393890381 2.022376537322998
CurrentTrain: epoch  0, batch    47 | loss: 7.9927764Losses:  8.003765106201172 1.9171596765518188
CurrentTrain: epoch  0, batch    48 | loss: 8.0037651Losses:  7.959151744842529 1.9170503616333008
CurrentTrain: epoch  0, batch    49 | loss: 7.9591517Losses:  8.0563383102417 1.9785882234573364
CurrentTrain: epoch  0, batch    50 | loss: 8.0563383Losses:  8.135039329528809 2.0914406776428223
CurrentTrain: epoch  0, batch    51 | loss: 8.1350393Losses:  7.9060235023498535 1.9147268533706665
CurrentTrain: epoch  0, batch    52 | loss: 7.9060235Losses:  7.890392303466797 1.8849546909332275
CurrentTrain: epoch  0, batch    53 | loss: 7.8903923Losses:  8.106283187866211 2.0367822647094727
CurrentTrain: epoch  0, batch    54 | loss: 8.1062832Losses:  7.757842063903809 1.8230009078979492
CurrentTrain: epoch  0, batch    55 | loss: 7.7578421Losses:  7.68559455871582 1.7940740585327148
CurrentTrain: epoch  0, batch    56 | loss: 7.6855946Losses:  7.978975296020508 1.9683071374893188
CurrentTrain: epoch  0, batch    57 | loss: 7.9789753Losses:  7.681393623352051 1.7235667705535889
CurrentTrain: epoch  0, batch    58 | loss: 7.6813936Losses:  7.608220100402832 1.6990082263946533
CurrentTrain: epoch  0, batch    59 | loss: 7.6082201Losses:  7.792187213897705 1.9254059791564941
CurrentTrain: epoch  0, batch    60 | loss: 7.7921872Losses:  7.696767807006836 1.7417434453964233
CurrentTrain: epoch  0, batch    61 | loss: 7.6967678Losses:  7.589293956756592 1.4494694471359253
CurrentTrain: epoch  0, batch    62 | loss: 7.5892940Losses:  7.767336368560791 1.7750563621520996
CurrentTrain: epoch  1, batch     0 | loss: 7.7673364Losses:  7.775488376617432 1.8718634843826294
CurrentTrain: epoch  1, batch     1 | loss: 7.7754884Losses:  7.665948390960693 1.7617888450622559
CurrentTrain: epoch  1, batch     2 | loss: 7.6659484Losses:  7.884520530700684 2.0304434299468994
CurrentTrain: epoch  1, batch     3 | loss: 7.8845205Losses:  7.986656665802002 1.9981416463851929
CurrentTrain: epoch  1, batch     4 | loss: 7.9866567Losses:  7.7725419998168945 1.9040594100952148
CurrentTrain: epoch  1, batch     5 | loss: 7.7725420Losses:  7.752192974090576 1.8937491178512573
CurrentTrain: epoch  1, batch     6 | loss: 7.7521930Losses:  7.826515197753906 2.029167652130127
CurrentTrain: epoch  1, batch     7 | loss: 7.8265152Losses:  7.499688148498535 1.703554630279541
CurrentTrain: epoch  1, batch     8 | loss: 7.4996881Losses:  7.837303638458252 1.9491448402404785
CurrentTrain: epoch  1, batch     9 | loss: 7.8373036Losses:  7.799102306365967 1.916643500328064
CurrentTrain: epoch  1, batch    10 | loss: 7.7991023Losses:  7.767876625061035 1.9292874336242676
CurrentTrain: epoch  1, batch    11 | loss: 7.7678766Losses:  7.846700668334961 1.9830113649368286
CurrentTrain: epoch  1, batch    12 | loss: 7.8467007Losses:  7.470297813415527 1.6063563823699951
CurrentTrain: epoch  1, batch    13 | loss: 7.4702978Losses:  7.711233615875244 1.9220515489578247
CurrentTrain: epoch  1, batch    14 | loss: 7.7112336Losses:  7.852657794952393 1.8634482622146606
CurrentTrain: epoch  1, batch    15 | loss: 7.8526578Losses:  7.550822734832764 1.7404848337173462
CurrentTrain: epoch  1, batch    16 | loss: 7.5508227Losses:  7.292445659637451 1.5774255990982056
CurrentTrain: epoch  1, batch    17 | loss: 7.2924457Losses:  7.555105209350586 1.803072214126587
CurrentTrain: epoch  1, batch    18 | loss: 7.5551052Losses:  7.723586082458496 1.939582109451294
CurrentTrain: epoch  1, batch    19 | loss: 7.7235861Losses:  7.81293249130249 2.021512508392334
CurrentTrain: epoch  1, batch    20 | loss: 7.8129325Losses:  7.812859535217285 2.0142505168914795
CurrentTrain: epoch  1, batch    21 | loss: 7.8128595Losses:  7.643496513366699 1.8626035451889038
CurrentTrain: epoch  1, batch    22 | loss: 7.6434965Losses:  7.541487693786621 1.694451093673706
CurrentTrain: epoch  1, batch    23 | loss: 7.5414877Losses:  7.689979553222656 1.8778061866760254
CurrentTrain: epoch  1, batch    24 | loss: 7.6899796Losses:  7.429861545562744 1.7967333793640137
CurrentTrain: epoch  1, batch    25 | loss: 7.4298615Losses:  7.72023344039917 2.009131908416748
CurrentTrain: epoch  1, batch    26 | loss: 7.7202334Losses:  7.6608405113220215 1.9111148118972778
CurrentTrain: epoch  1, batch    27 | loss: 7.6608405Losses:  7.632641792297363 1.9362610578536987
CurrentTrain: epoch  1, batch    28 | loss: 7.6326418Losses:  7.65667724609375 2.005124807357788
CurrentTrain: epoch  1, batch    29 | loss: 7.6566772Losses:  7.563410758972168 1.8564188480377197
CurrentTrain: epoch  1, batch    30 | loss: 7.5634108Losses:  7.688185691833496 2.050321340560913
CurrentTrain: epoch  1, batch    31 | loss: 7.6881857Losses:  7.468106269836426 1.9117307662963867
CurrentTrain: epoch  1, batch    32 | loss: 7.4681063Losses:  7.342299461364746 1.6870085000991821
CurrentTrain: epoch  1, batch    33 | loss: 7.3422995Losses:  7.696247577667236 1.9707297086715698
CurrentTrain: epoch  1, batch    34 | loss: 7.6962476Losses:  7.406291961669922 1.8315298557281494
CurrentTrain: epoch  1, batch    35 | loss: 7.4062920Losses:  7.446124076843262 1.672325849533081
CurrentTrain: epoch  1, batch    36 | loss: 7.4461241Losses:  7.693542957305908 1.9531207084655762
CurrentTrain: epoch  1, batch    37 | loss: 7.6935430Losses:  7.674501419067383 2.0043580532073975
CurrentTrain: epoch  1, batch    38 | loss: 7.6745014Losses:  7.524825096130371 1.928894281387329
CurrentTrain: epoch  1, batch    39 | loss: 7.5248251Losses:  7.57466983795166 1.8411340713500977
CurrentTrain: epoch  1, batch    40 | loss: 7.5746698Losses:  7.696073055267334 1.958596110343933
CurrentTrain: epoch  1, batch    41 | loss: 7.6960731Losses:  7.482576370239258 1.8576629161834717
CurrentTrain: epoch  1, batch    42 | loss: 7.4825764Losses:  7.567070007324219 1.8828747272491455
CurrentTrain: epoch  1, batch    43 | loss: 7.5670700Losses:  7.466463088989258 1.7311573028564453
CurrentTrain: epoch  1, batch    44 | loss: 7.4664631Losses:  7.542440414428711 1.9726874828338623
CurrentTrain: epoch  1, batch    45 | loss: 7.5424404Losses:  7.429584980010986 1.9136043787002563
CurrentTrain: epoch  1, batch    46 | loss: 7.4295850Losses:  7.729898452758789 2.0924718379974365
CurrentTrain: epoch  1, batch    47 | loss: 7.7298985Losses:  7.502351760864258 1.8663263320922852
CurrentTrain: epoch  1, batch    48 | loss: 7.5023518Losses:  7.448178291320801 1.8737201690673828
CurrentTrain: epoch  1, batch    49 | loss: 7.4481783Losses:  7.383052825927734 1.8252503871917725
CurrentTrain: epoch  1, batch    50 | loss: 7.3830528Losses:  7.4769206047058105 1.7399035692214966
CurrentTrain: epoch  1, batch    51 | loss: 7.4769206Losses:  7.30898904800415 1.792555809020996
CurrentTrain: epoch  1, batch    52 | loss: 7.3089890Losses:  7.374553680419922 1.8205920457839966
CurrentTrain: epoch  1, batch    53 | loss: 7.3745537Losses:  7.444765090942383 1.8984081745147705
CurrentTrain: epoch  1, batch    54 | loss: 7.4447651Losses:  7.097273349761963 1.6671222448349
CurrentTrain: epoch  1, batch    55 | loss: 7.0972733Losses:  7.091900825500488 1.6813316345214844
CurrentTrain: epoch  1, batch    56 | loss: 7.0919008Losses:  7.417316913604736 1.9235568046569824
CurrentTrain: epoch  1, batch    57 | loss: 7.4173169Losses:  7.235143661499023 1.7323976755142212
CurrentTrain: epoch  1, batch    58 | loss: 7.2351437Losses:  7.620277404785156 2.0085813999176025
CurrentTrain: epoch  1, batch    59 | loss: 7.6202774Losses:  7.509623050689697 1.9519699811935425
CurrentTrain: epoch  1, batch    60 | loss: 7.5096231Losses:  7.4660563468933105 1.9736019372940063
CurrentTrain: epoch  1, batch    61 | loss: 7.4660563Losses:  6.669650077819824 1.3585669994354248
CurrentTrain: epoch  1, batch    62 | loss: 6.6696501Losses:  7.148426532745361 1.631283164024353
CurrentTrain: epoch  2, batch     0 | loss: 7.1484265Losses:  7.096377372741699 1.6164984703063965
CurrentTrain: epoch  2, batch     1 | loss: 7.0963774Losses:  7.382265090942383 1.9052646160125732
CurrentTrain: epoch  2, batch     2 | loss: 7.3822651Losses:  7.23093318939209 1.8505170345306396
CurrentTrain: epoch  2, batch     3 | loss: 7.2309332Losses:  7.241733551025391 1.8101069927215576
CurrentTrain: epoch  2, batch     4 | loss: 7.2417336Losses:  7.4728498458862305 2.062613010406494
CurrentTrain: epoch  2, batch     5 | loss: 7.4728498Losses:  7.057027339935303 1.684952735900879
CurrentTrain: epoch  2, batch     6 | loss: 7.0570273Losses:  7.38051700592041 1.8802368640899658
CurrentTrain: epoch  2, batch     7 | loss: 7.3805170Losses:  7.430966377258301 1.8774802684783936
CurrentTrain: epoch  2, batch     8 | loss: 7.4309664Losses:  7.177586555480957 1.7753322124481201
CurrentTrain: epoch  2, batch     9 | loss: 7.1775866Losses:  7.221182823181152 1.8570175170898438
CurrentTrain: epoch  2, batch    10 | loss: 7.2211828Losses:  7.266408443450928 1.8077454566955566
CurrentTrain: epoch  2, batch    11 | loss: 7.2664084Losses:  7.292469024658203 1.754082202911377
CurrentTrain: epoch  2, batch    12 | loss: 7.2924690Losses:  7.212883949279785 1.703540325164795
CurrentTrain: epoch  2, batch    13 | loss: 7.2128839Losses:  7.440313339233398 1.9178953170776367
CurrentTrain: epoch  2, batch    14 | loss: 7.4403133Losses:  7.407041549682617 1.9712364673614502
CurrentTrain: epoch  2, batch    15 | loss: 7.4070415Losses:  7.1867547035217285 1.8421988487243652
CurrentTrain: epoch  2, batch    16 | loss: 7.1867547Losses:  6.849954128265381 1.5386260747909546
CurrentTrain: epoch  2, batch    17 | loss: 6.8499541Losses:  7.526322841644287 2.002664089202881
CurrentTrain: epoch  2, batch    18 | loss: 7.5263228Losses:  7.290676593780518 1.8188824653625488
CurrentTrain: epoch  2, batch    19 | loss: 7.2906766Losses:  7.262351989746094 1.8786661624908447
CurrentTrain: epoch  2, batch    20 | loss: 7.2623520Losses:  7.232572078704834 1.855198860168457
CurrentTrain: epoch  2, batch    21 | loss: 7.2325721Losses:  7.163105010986328 1.8301870822906494
CurrentTrain: epoch  2, batch    22 | loss: 7.1631050Losses:  7.396084785461426 1.9222934246063232
CurrentTrain: epoch  2, batch    23 | loss: 7.3960848Losses:  7.1906633377075195 1.8038221597671509
CurrentTrain: epoch  2, batch    24 | loss: 7.1906633Losses:  7.255115985870361 1.8649004697799683
CurrentTrain: epoch  2, batch    25 | loss: 7.2551160Losses:  7.212116241455078 1.8431751728057861
CurrentTrain: epoch  2, batch    26 | loss: 7.2121162Losses:  7.0305681228637695 1.6144564151763916
CurrentTrain: epoch  2, batch    27 | loss: 7.0305681Losses:  7.095460891723633 1.7412397861480713
CurrentTrain: epoch  2, batch    28 | loss: 7.0954609Losses:  7.420621871948242 2.0412817001342773
CurrentTrain: epoch  2, batch    29 | loss: 7.4206219Losses:  7.360760688781738 1.9392728805541992
CurrentTrain: epoch  2, batch    30 | loss: 7.3607607Losses:  7.444118499755859 2.0901682376861572
CurrentTrain: epoch  2, batch    31 | loss: 7.4441185Losses:  7.249847412109375 1.7173370122909546
CurrentTrain: epoch  2, batch    32 | loss: 7.2498474Losses:  7.0587687492370605 1.7546437978744507
CurrentTrain: epoch  2, batch    33 | loss: 7.0587687Losses:  7.353092193603516 1.9374630451202393
CurrentTrain: epoch  2, batch    34 | loss: 7.3530922Losses:  7.249220371246338 1.806490421295166
CurrentTrain: epoch  2, batch    35 | loss: 7.2492204Losses:  7.227288246154785 1.9100475311279297
CurrentTrain: epoch  2, batch    36 | loss: 7.2272882Losses:  7.039488792419434 1.6740312576293945
CurrentTrain: epoch  2, batch    37 | loss: 7.0394888Losses:  7.182188034057617 1.79115891456604
CurrentTrain: epoch  2, batch    38 | loss: 7.1821880Losses:  6.9274983406066895 1.6762232780456543
CurrentTrain: epoch  2, batch    39 | loss: 6.9274983Losses:  6.859944820404053 1.5803536176681519
CurrentTrain: epoch  2, batch    40 | loss: 6.8599448Losses:  6.869518756866455 1.4226021766662598
CurrentTrain: epoch  2, batch    41 | loss: 6.8695188Losses:  7.163254737854004 1.8417547941207886
CurrentTrain: epoch  2, batch    42 | loss: 7.1632547Losses:  7.213408470153809 1.8116310834884644
CurrentTrain: epoch  2, batch    43 | loss: 7.2134085Losses:  7.282874584197998 1.9387803077697754
CurrentTrain: epoch  2, batch    44 | loss: 7.2828746Losses:  7.288091659545898 1.9007766246795654
CurrentTrain: epoch  2, batch    45 | loss: 7.2880917Losses:  7.195436954498291 1.891497015953064
CurrentTrain: epoch  2, batch    46 | loss: 7.1954370Losses:  7.240356922149658 1.8805185556411743
CurrentTrain: epoch  2, batch    47 | loss: 7.2403569Losses:  7.11026668548584 1.8088778257369995
CurrentTrain: epoch  2, batch    48 | loss: 7.1102667Losses:  7.218789577484131 1.8679909706115723
CurrentTrain: epoch  2, batch    49 | loss: 7.2187896Losses:  6.971756458282471 1.6839066743850708
CurrentTrain: epoch  2, batch    50 | loss: 6.9717565Losses:  7.2048187255859375 1.9210145473480225
CurrentTrain: epoch  2, batch    51 | loss: 7.2048187Losses:  7.100334644317627 1.7035194635391235
CurrentTrain: epoch  2, batch    52 | loss: 7.1003346Losses:  7.250015735626221 1.9609826803207397
CurrentTrain: epoch  2, batch    53 | loss: 7.2500157Losses:  7.213324546813965 1.9079911708831787
CurrentTrain: epoch  2, batch    54 | loss: 7.2133245Losses:  7.278164386749268 1.9692167043685913
CurrentTrain: epoch  2, batch    55 | loss: 7.2781644Losses:  7.347955703735352 2.0382113456726074
CurrentTrain: epoch  2, batch    56 | loss: 7.3479557Losses:  7.115601062774658 1.7585893869400024
CurrentTrain: epoch  2, batch    57 | loss: 7.1156011Losses:  7.0534257888793945 1.7309796810150146
CurrentTrain: epoch  2, batch    58 | loss: 7.0534258Losses:  7.140085697174072 1.8456597328186035
CurrentTrain: epoch  2, batch    59 | loss: 7.1400857Losses:  7.106828689575195 1.8517179489135742
CurrentTrain: epoch  2, batch    60 | loss: 7.1068287Losses:  7.00916862487793 1.7321836948394775
CurrentTrain: epoch  2, batch    61 | loss: 7.0091686Losses:  6.85453987121582 1.5814316272735596
CurrentTrain: epoch  2, batch    62 | loss: 6.8545399Losses:  7.124758720397949 1.8094866275787354
CurrentTrain: epoch  3, batch     0 | loss: 7.1247587Losses:  7.060722351074219 1.700728178024292
CurrentTrain: epoch  3, batch     1 | loss: 7.0607224Losses:  7.176009178161621 1.8965853452682495
CurrentTrain: epoch  3, batch     2 | loss: 7.1760092Losses:  6.9186320304870605 1.6633163690567017
CurrentTrain: epoch  3, batch     3 | loss: 6.9186320Losses:  7.051907062530518 1.7929720878601074
CurrentTrain: epoch  3, batch     4 | loss: 7.0519071Losses:  7.123450756072998 1.8189630508422852
CurrentTrain: epoch  3, batch     5 | loss: 7.1234508Losses:  7.057257175445557 1.7977596521377563
CurrentTrain: epoch  3, batch     6 | loss: 7.0572572Losses:  7.147831916809082 1.9080177545547485
CurrentTrain: epoch  3, batch     7 | loss: 7.1478319Losses:  7.143139839172363 1.8192788362503052
CurrentTrain: epoch  3, batch     8 | loss: 7.1431398Losses:  7.242259979248047 1.950607180595398
CurrentTrain: epoch  3, batch     9 | loss: 7.2422600Losses:  7.3365020751953125 2.0390164852142334
CurrentTrain: epoch  3, batch    10 | loss: 7.3365021Losses:  7.1066789627075195 1.8051044940948486
CurrentTrain: epoch  3, batch    11 | loss: 7.1066790Losses:  6.997708320617676 1.7855165004730225
CurrentTrain: epoch  3, batch    12 | loss: 6.9977083Losses:  7.326022148132324 2.0296709537506104
CurrentTrain: epoch  3, batch    13 | loss: 7.3260221Losses:  7.1093854904174805 1.8830050230026245
CurrentTrain: epoch  3, batch    14 | loss: 7.1093855Losses:  7.1219377517700195 1.9086915254592896
CurrentTrain: epoch  3, batch    15 | loss: 7.1219378Losses:  7.078991889953613 1.854111671447754
CurrentTrain: epoch  3, batch    16 | loss: 7.0789919Losses:  7.069301605224609 1.6499369144439697
CurrentTrain: epoch  3, batch    17 | loss: 7.0693016Losses:  6.9866132736206055 1.7606351375579834
CurrentTrain: epoch  3, batch    18 | loss: 6.9866133Losses:  7.125239849090576 1.8117985725402832
CurrentTrain: epoch  3, batch    19 | loss: 7.1252398Losses:  7.095818996429443 1.8202482461929321
CurrentTrain: epoch  3, batch    20 | loss: 7.0958190Losses:  7.170423984527588 1.9318122863769531
CurrentTrain: epoch  3, batch    21 | loss: 7.1704240Losses:  6.896331787109375 1.6586604118347168
CurrentTrain: epoch  3, batch    22 | loss: 6.8963318Losses:  7.169902801513672 1.9296132326126099
CurrentTrain: epoch  3, batch    23 | loss: 7.1699028Losses:  6.814488410949707 1.5774744749069214
CurrentTrain: epoch  3, batch    24 | loss: 6.8144884Losses:  7.23880672454834 2.0243351459503174
CurrentTrain: epoch  3, batch    25 | loss: 7.2388067Losses:  7.266597270965576 2.016763210296631
CurrentTrain: epoch  3, batch    26 | loss: 7.2665973Losses:  7.088438034057617 1.8624885082244873
CurrentTrain: epoch  3, batch    27 | loss: 7.0884380Losses:  7.285067558288574 2.050387144088745
CurrentTrain: epoch  3, batch    28 | loss: 7.2850676Losses:  7.047796249389648 1.775380253791809
CurrentTrain: epoch  3, batch    29 | loss: 7.0477962Losses:  7.338388442993164 1.986952781677246
CurrentTrain: epoch  3, batch    30 | loss: 7.3383884Losses:  7.03291130065918 1.792954444885254
CurrentTrain: epoch  3, batch    31 | loss: 7.0329113Losses:  6.858571529388428 1.5910873413085938
CurrentTrain: epoch  3, batch    32 | loss: 6.8585715Losses:  7.0007781982421875 1.7378895282745361
CurrentTrain: epoch  3, batch    33 | loss: 7.0007782Losses:  7.026492118835449 1.8121726512908936
CurrentTrain: epoch  3, batch    34 | loss: 7.0264921Losses:  7.081063270568848 1.8682196140289307
CurrentTrain: epoch  3, batch    35 | loss: 7.0810633Losses:  7.2048540115356445 1.8845051527023315
CurrentTrain: epoch  3, batch    36 | loss: 7.2048540Losses:  7.11663818359375 1.8364235162734985
CurrentTrain: epoch  3, batch    37 | loss: 7.1166382Losses:  7.125689506530762 1.9096953868865967
CurrentTrain: epoch  3, batch    38 | loss: 7.1256895Losses:  6.897900581359863 1.5829182863235474
CurrentTrain: epoch  3, batch    39 | loss: 6.8979006Losses:  7.1490020751953125 1.936901569366455
CurrentTrain: epoch  3, batch    40 | loss: 7.1490021Losses:  6.840885639190674 1.6657835245132446
CurrentTrain: epoch  3, batch    41 | loss: 6.8408856Losses:  6.910245895385742 1.6396400928497314
CurrentTrain: epoch  3, batch    42 | loss: 6.9102459Losses:  6.991739273071289 1.7485448122024536
CurrentTrain: epoch  3, batch    43 | loss: 6.9917393Losses:  7.251933574676514 1.9385520219802856
CurrentTrain: epoch  3, batch    44 | loss: 7.2519336Losses:  6.939404010772705 1.6816086769104004
CurrentTrain: epoch  3, batch    45 | loss: 6.9394040Losses:  7.093914985656738 1.8983383178710938
CurrentTrain: epoch  3, batch    46 | loss: 7.0939150Losses:  6.832787036895752 1.6168417930603027
CurrentTrain: epoch  3, batch    47 | loss: 6.8327870Losses:  6.634618759155273 1.4611575603485107
CurrentTrain: epoch  3, batch    48 | loss: 6.6346188Losses:  6.730858325958252 1.5059820413589478
CurrentTrain: epoch  3, batch    49 | loss: 6.7308583Losses:  7.1100311279296875 1.9060986042022705
CurrentTrain: epoch  3, batch    50 | loss: 7.1100311Losses:  6.869633197784424 1.6461272239685059
CurrentTrain: epoch  3, batch    51 | loss: 6.8696332Losses:  6.957456588745117 1.7574721574783325
CurrentTrain: epoch  3, batch    52 | loss: 6.9574566Losses:  7.144641399383545 1.8452916145324707
CurrentTrain: epoch  3, batch    53 | loss: 7.1446414Losses:  7.038743019104004 1.8533885478973389
CurrentTrain: epoch  3, batch    54 | loss: 7.0387430Losses:  7.215575218200684 2.013814926147461
CurrentTrain: epoch  3, batch    55 | loss: 7.2155752Losses:  6.9669013023376465 1.7547177076339722
CurrentTrain: epoch  3, batch    56 | loss: 6.9669013Losses:  6.9855756759643555 1.7639645338058472
CurrentTrain: epoch  3, batch    57 | loss: 6.9855757Losses:  7.018932342529297 1.7559843063354492
CurrentTrain: epoch  3, batch    58 | loss: 7.0189323Losses:  6.717266082763672 1.5372360944747925
CurrentTrain: epoch  3, batch    59 | loss: 6.7172661Losses:  7.157952308654785 1.9517699480056763
CurrentTrain: epoch  3, batch    60 | loss: 7.1579523Losses:  6.999465465545654 1.819707989692688
CurrentTrain: epoch  3, batch    61 | loss: 6.9994655Losses:  6.592772960662842 1.3961501121520996
CurrentTrain: epoch  3, batch    62 | loss: 6.5927730Losses:  6.595199108123779 1.4477181434631348
CurrentTrain: epoch  4, batch     0 | loss: 6.5951991Losses:  6.850643157958984 1.6546636819839478
CurrentTrain: epoch  4, batch     1 | loss: 6.8506432Losses:  6.988191604614258 1.7967031002044678
CurrentTrain: epoch  4, batch     2 | loss: 6.9881916Losses:  6.785837173461914 1.626480221748352
CurrentTrain: epoch  4, batch     3 | loss: 6.7858372Losses:  7.2166852951049805 2.0162785053253174
CurrentTrain: epoch  4, batch     4 | loss: 7.2166853Losses:  7.191155433654785 2.0087735652923584
CurrentTrain: epoch  4, batch     5 | loss: 7.1911554Losses:  6.908137798309326 1.7304357290267944
CurrentTrain: epoch  4, batch     6 | loss: 6.9081378Losses:  6.767983436584473 1.6003220081329346
CurrentTrain: epoch  4, batch     7 | loss: 6.7679834Losses:  7.187560081481934 2.0050299167633057
CurrentTrain: epoch  4, batch     8 | loss: 7.1875601Losses:  6.957976341247559 1.7712810039520264
CurrentTrain: epoch  4, batch     9 | loss: 6.9579763Losses:  6.940491676330566 1.7572243213653564
CurrentTrain: epoch  4, batch    10 | loss: 6.9404917Losses:  6.979323387145996 1.7707698345184326
CurrentTrain: epoch  4, batch    11 | loss: 6.9793234Losses:  6.943370342254639 1.7590166330337524
CurrentTrain: epoch  4, batch    12 | loss: 6.9433703Losses:  7.119112014770508 1.8959530591964722
CurrentTrain: epoch  4, batch    13 | loss: 7.1191120Losses:  6.757826805114746 1.606726884841919
CurrentTrain: epoch  4, batch    14 | loss: 6.7578268Losses:  6.896357536315918 1.704716444015503
CurrentTrain: epoch  4, batch    15 | loss: 6.8963575Losses:  6.882875442504883 1.7037782669067383
CurrentTrain: epoch  4, batch    16 | loss: 6.8828754Losses:  6.863323211669922 1.6940300464630127
CurrentTrain: epoch  4, batch    17 | loss: 6.8633232Losses:  7.054174423217773 1.8823497295379639
CurrentTrain: epoch  4, batch    18 | loss: 7.0541744Losses:  6.992138862609863 1.8328392505645752
CurrentTrain: epoch  4, batch    19 | loss: 6.9921389Losses:  6.918910503387451 1.6312888860702515
CurrentTrain: epoch  4, batch    20 | loss: 6.9189105Losses:  6.694633483886719 1.5065873861312866
CurrentTrain: epoch  4, batch    21 | loss: 6.6946335Losses:  6.829547882080078 1.68562912940979
CurrentTrain: epoch  4, batch    22 | loss: 6.8295479Losses:  6.586411476135254 1.4456796646118164
CurrentTrain: epoch  4, batch    23 | loss: 6.5864115Losses:  6.655128479003906 1.4423017501831055
CurrentTrain: epoch  4, batch    24 | loss: 6.6551285Losses:  7.08119010925293 1.9073712825775146
CurrentTrain: epoch  4, batch    25 | loss: 7.0811901Losses:  6.93618106842041 1.789475440979004
CurrentTrain: epoch  4, batch    26 | loss: 6.9361811Losses:  6.895841598510742 1.72939133644104
CurrentTrain: epoch  4, batch    27 | loss: 6.8958416Losses:  6.875552654266357 1.6872472763061523
CurrentTrain: epoch  4, batch    28 | loss: 6.8755527Losses:  6.907625675201416 1.7151771783828735
CurrentTrain: epoch  4, batch    29 | loss: 6.9076257Losses:  6.927669048309326 1.7773919105529785
CurrentTrain: epoch  4, batch    30 | loss: 6.9276690Losses:  6.926197052001953 1.7065532207489014
CurrentTrain: epoch  4, batch    31 | loss: 6.9261971Losses:  6.967524528503418 1.7906664609909058
CurrentTrain: epoch  4, batch    32 | loss: 6.9675245Losses:  7.068689346313477 1.882495403289795
CurrentTrain: epoch  4, batch    33 | loss: 7.0686893Losses:  6.936526298522949 1.771613359451294
CurrentTrain: epoch  4, batch    34 | loss: 6.9365263Losses:  7.145651817321777 1.9247143268585205
CurrentTrain: epoch  4, batch    35 | loss: 7.1456518Losses:  6.795633316040039 1.6056387424468994
CurrentTrain: epoch  4, batch    36 | loss: 6.7956333Losses:  7.03862190246582 1.863501787185669
CurrentTrain: epoch  4, batch    37 | loss: 7.0386219Losses:  6.853422164916992 1.7011792659759521
CurrentTrain: epoch  4, batch    38 | loss: 6.8534222Losses:  6.916286945343018 1.7673673629760742
CurrentTrain: epoch  4, batch    39 | loss: 6.9162869Losses:  6.8602705001831055 1.714621901512146
CurrentTrain: epoch  4, batch    40 | loss: 6.8602705Losses:  6.966235637664795 1.8222556114196777
CurrentTrain: epoch  4, batch    41 | loss: 6.9662356Losses:  7.0308732986450195 1.7509386539459229
CurrentTrain: epoch  4, batch    42 | loss: 7.0308733Losses:  6.760673522949219 1.504347801208496
CurrentTrain: epoch  4, batch    43 | loss: 6.7606735Losses:  6.695654392242432 1.553614616394043
CurrentTrain: epoch  4, batch    44 | loss: 6.6956544Losses:  6.765083312988281 1.5711251497268677
CurrentTrain: epoch  4, batch    45 | loss: 6.7650833Losses:  6.927523136138916 1.758057951927185
CurrentTrain: epoch  4, batch    46 | loss: 6.9275231Losses:  6.9024763107299805 1.7641228437423706
CurrentTrain: epoch  4, batch    47 | loss: 6.9024763Losses:  7.054325580596924 1.8998312950134277
CurrentTrain: epoch  4, batch    48 | loss: 7.0543256Losses:  6.972339153289795 1.8197706937789917
CurrentTrain: epoch  4, batch    49 | loss: 6.9723392Losses:  7.13680362701416 1.9750332832336426
CurrentTrain: epoch  4, batch    50 | loss: 7.1368036Losses:  6.8643693923950195 1.6952686309814453
CurrentTrain: epoch  4, batch    51 | loss: 6.8643694Losses:  6.675228118896484 1.5482661724090576
CurrentTrain: epoch  4, batch    52 | loss: 6.6752281Losses:  6.92418909072876 1.7507209777832031
CurrentTrain: epoch  4, batch    53 | loss: 6.9241891Losses:  6.785145282745361 1.616492748260498
CurrentTrain: epoch  4, batch    54 | loss: 6.7851453Losses:  6.9542951583862305 1.8110483884811401
CurrentTrain: epoch  4, batch    55 | loss: 6.9542952Losses:  6.755624771118164 1.616470217704773
CurrentTrain: epoch  4, batch    56 | loss: 6.7556248Losses:  7.1567511558532715 1.9937797784805298
CurrentTrain: epoch  4, batch    57 | loss: 7.1567512Losses:  6.906835556030273 1.751659631729126
CurrentTrain: epoch  4, batch    58 | loss: 6.9068356Losses:  6.917882919311523 1.757267713546753
CurrentTrain: epoch  4, batch    59 | loss: 6.9178829Losses:  6.879889488220215 1.7354462146759033
CurrentTrain: epoch  4, batch    60 | loss: 6.8798895Losses:  6.986437797546387 1.8201367855072021
CurrentTrain: epoch  4, batch    61 | loss: 6.9864378Losses:  6.786502838134766 1.414304494857788
CurrentTrain: epoch  4, batch    62 | loss: 6.7865028Losses:  7.1055474281311035 1.947765827178955
CurrentTrain: epoch  5, batch     0 | loss: 7.1055474Losses:  6.9996795654296875 1.8507033586502075
CurrentTrain: epoch  5, batch     1 | loss: 6.9996796Losses:  6.791479587554932 1.6256762742996216
CurrentTrain: epoch  5, batch     2 | loss: 6.7914796Losses:  6.900083541870117 1.7548835277557373
CurrentTrain: epoch  5, batch     3 | loss: 6.9000835Losses:  6.943264961242676 1.7037074565887451
CurrentTrain: epoch  5, batch     4 | loss: 6.9432650Losses:  6.8657026290893555 1.7151780128479004
CurrentTrain: epoch  5, batch     5 | loss: 6.8657026Losses:  6.990721225738525 1.8285861015319824
CurrentTrain: epoch  5, batch     6 | loss: 6.9907212Losses:  7.00087308883667 1.853148102760315
CurrentTrain: epoch  5, batch     7 | loss: 7.0008731Losses:  7.005253314971924 1.831743836402893
CurrentTrain: epoch  5, batch     8 | loss: 7.0052533Losses:  6.694509983062744 1.5621004104614258
CurrentTrain: epoch  5, batch     9 | loss: 6.6945100Losses:  7.020002365112305 1.88005530834198
CurrentTrain: epoch  5, batch    10 | loss: 7.0200024Losses:  7.062085151672363 1.9057648181915283
CurrentTrain: epoch  5, batch    11 | loss: 7.0620852Losses:  6.888524055480957 1.7579963207244873
CurrentTrain: epoch  5, batch    12 | loss: 6.8885241Losses:  6.820918083190918 1.681260108947754
CurrentTrain: epoch  5, batch    13 | loss: 6.8209181Losses:  7.135826587677002 2.0026140213012695
CurrentTrain: epoch  5, batch    14 | loss: 7.1358266Losses:  6.896200180053711 1.742013692855835
CurrentTrain: epoch  5, batch    15 | loss: 6.8962002Losses:  7.084468364715576 1.9403038024902344
CurrentTrain: epoch  5, batch    16 | loss: 7.0844684Losses:  7.031089782714844 1.8958243131637573
CurrentTrain: epoch  5, batch    17 | loss: 7.0310898Losses:  6.982326507568359 1.8211441040039062
CurrentTrain: epoch  5, batch    18 | loss: 6.9823265Losses:  6.91599178314209 1.763411521911621
CurrentTrain: epoch  5, batch    19 | loss: 6.9159918Losses:  6.910939693450928 1.7716106176376343
CurrentTrain: epoch  5, batch    20 | loss: 6.9109397Losses:  6.84567928314209 1.7093961238861084
CurrentTrain: epoch  5, batch    21 | loss: 6.8456793Losses:  6.789436340332031 1.6566355228424072
CurrentTrain: epoch  5, batch    22 | loss: 6.7894363Losses:  6.867682456970215 1.715238332748413
CurrentTrain: epoch  5, batch    23 | loss: 6.8676825Losses:  6.810678482055664 1.677702784538269
CurrentTrain: epoch  5, batch    24 | loss: 6.8106785Losses:  6.98350715637207 1.8392058610916138
CurrentTrain: epoch  5, batch    25 | loss: 6.9835072Losses:  6.97243595123291 1.8265635967254639
CurrentTrain: epoch  5, batch    26 | loss: 6.9724360Losses:  6.852373123168945 1.7077593803405762
CurrentTrain: epoch  5, batch    27 | loss: 6.8523731Losses:  7.026216983795166 1.8882765769958496
CurrentTrain: epoch  5, batch    28 | loss: 7.0262170Losses:  6.98319673538208 1.84568452835083
CurrentTrain: epoch  5, batch    29 | loss: 6.9831967Losses:  6.761719703674316 1.6260398626327515
CurrentTrain: epoch  5, batch    30 | loss: 6.7617197Losses:  6.82478141784668 1.6954079866409302
CurrentTrain: epoch  5, batch    31 | loss: 6.8247814Losses:  6.830777168273926 1.6839263439178467
CurrentTrain: epoch  5, batch    32 | loss: 6.8307772Losses:  6.599447250366211 1.4939348697662354
CurrentTrain: epoch  5, batch    33 | loss: 6.5994473Losses:  6.848001480102539 1.7167989015579224
CurrentTrain: epoch  5, batch    34 | loss: 6.8480015Losses:  6.833454132080078 1.5942020416259766
CurrentTrain: epoch  5, batch    35 | loss: 6.8334541Losses:  6.744787693023682 1.6103744506835938
CurrentTrain: epoch  5, batch    36 | loss: 6.7447877Losses:  6.80495023727417 1.6802268028259277
CurrentTrain: epoch  5, batch    37 | loss: 6.8049502Losses:  6.829090118408203 1.687000036239624
CurrentTrain: epoch  5, batch    38 | loss: 6.8290901Losses:  6.979332447052002 1.8475332260131836
CurrentTrain: epoch  5, batch    39 | loss: 6.9793324Losses:  7.089101314544678 1.9632697105407715
CurrentTrain: epoch  5, batch    40 | loss: 7.0891013Losses:  6.826608657836914 1.6995799541473389
CurrentTrain: epoch  5, batch    41 | loss: 6.8266087Losses:  7.003673076629639 1.85585355758667
CurrentTrain: epoch  5, batch    42 | loss: 7.0036731Losses:  6.805992603302002 1.6669397354125977
CurrentTrain: epoch  5, batch    43 | loss: 6.8059926Losses:  6.880326271057129 1.75593101978302
CurrentTrain: epoch  5, batch    44 | loss: 6.8803263Losses:  6.842730522155762 1.7272557020187378
CurrentTrain: epoch  5, batch    45 | loss: 6.8427305Losses:  6.920467376708984 1.797224998474121
CurrentTrain: epoch  5, batch    46 | loss: 6.9204674Losses:  6.935087203979492 1.8126459121704102
CurrentTrain: epoch  5, batch    47 | loss: 6.9350872Losses:  6.967988014221191 1.850008487701416
CurrentTrain: epoch  5, batch    48 | loss: 6.9679880Losses:  6.942480564117432 1.8235087394714355
CurrentTrain: epoch  5, batch    49 | loss: 6.9424806Losses:  6.907886505126953 1.794885516166687
CurrentTrain: epoch  5, batch    50 | loss: 6.9078865Losses:  6.727028846740723 1.6120061874389648
CurrentTrain: epoch  5, batch    51 | loss: 6.7270288Losses:  7.054845809936523 1.9301390647888184
CurrentTrain: epoch  5, batch    52 | loss: 7.0548458Losses:  6.864846706390381 1.7466917037963867
CurrentTrain: epoch  5, batch    53 | loss: 6.8648467Losses:  7.034699440002441 1.924556851387024
CurrentTrain: epoch  5, batch    54 | loss: 7.0346994Losses:  6.933104991912842 1.817378044128418
CurrentTrain: epoch  5, batch    55 | loss: 6.9331050Losses:  7.045031547546387 1.93681800365448
CurrentTrain: epoch  5, batch    56 | loss: 7.0450315Losses:  6.83178186416626 1.624717354774475
CurrentTrain: epoch  5, batch    57 | loss: 6.8317819Losses:  6.783202171325684 1.6706626415252686
CurrentTrain: epoch  5, batch    58 | loss: 6.7832022Losses:  6.736786842346191 1.6101655960083008
CurrentTrain: epoch  5, batch    59 | loss: 6.7367868Losses:  6.834814071655273 1.7177486419677734
CurrentTrain: epoch  5, batch    60 | loss: 6.8348141Losses:  6.748764991760254 1.637526273727417
CurrentTrain: epoch  5, batch    61 | loss: 6.7487650Losses:  6.659785747528076 1.5157569646835327
CurrentTrain: epoch  5, batch    62 | loss: 6.6597857Losses:  6.850887298583984 1.7240822315216064
CurrentTrain: epoch  6, batch     0 | loss: 6.8508873Losses:  6.928806304931641 1.8109028339385986
CurrentTrain: epoch  6, batch     1 | loss: 6.9288063Losses:  6.783238887786865 1.6781569719314575
CurrentTrain: epoch  6, batch     2 | loss: 6.7832389Losses:  7.2190022468566895 1.9901014566421509
CurrentTrain: epoch  6, batch     3 | loss: 7.2190022Losses:  6.659823417663574 1.5473334789276123
CurrentTrain: epoch  6, batch     4 | loss: 6.6598234Losses:  6.9738054275512695 1.852876901626587
CurrentTrain: epoch  6, batch     5 | loss: 6.9738054Losses:  6.958198070526123 1.8311638832092285
CurrentTrain: epoch  6, batch     6 | loss: 6.9581981Losses:  6.826354026794434 1.7055068016052246
CurrentTrain: epoch  6, batch     7 | loss: 6.8263540Losses:  6.982399940490723 1.8667182922363281
CurrentTrain: epoch  6, batch     8 | loss: 6.9823999Losses:  6.57452917098999 1.4579280614852905
CurrentTrain: epoch  6, batch     9 | loss: 6.5745292Losses:  6.941367149353027 1.8311643600463867
CurrentTrain: epoch  6, batch    10 | loss: 6.9413671Losses:  6.656355857849121 1.5183038711547852
CurrentTrain: epoch  6, batch    11 | loss: 6.6563559Losses:  7.053343296051025 1.929030418395996
CurrentTrain: epoch  6, batch    12 | loss: 7.0533433Losses:  6.7186784744262695 1.601684808731079
CurrentTrain: epoch  6, batch    13 | loss: 6.7186785Losses:  7.020994186401367 1.9030991792678833
CurrentTrain: epoch  6, batch    14 | loss: 7.0209942Losses:  6.902224063873291 1.801120638847351
CurrentTrain: epoch  6, batch    15 | loss: 6.9022241Losses:  6.976491928100586 1.8628625869750977
CurrentTrain: epoch  6, batch    16 | loss: 6.9764919Losses:  6.959611892700195 1.8422496318817139
CurrentTrain: epoch  6, batch    17 | loss: 6.9596119Losses:  6.348024368286133 1.2224605083465576
CurrentTrain: epoch  6, batch    18 | loss: 6.3480244Losses:  6.961305141448975 1.847425937652588
CurrentTrain: epoch  6, batch    19 | loss: 6.9613051Losses:  7.007114410400391 1.889756202697754
CurrentTrain: epoch  6, batch    20 | loss: 7.0071144Losses:  6.860583305358887 1.7334964275360107
CurrentTrain: epoch  6, batch    21 | loss: 6.8605833Losses:  6.725824356079102 1.6005213260650635
CurrentTrain: epoch  6, batch    22 | loss: 6.7258244Losses:  6.536381244659424 1.4174952507019043
CurrentTrain: epoch  6, batch    23 | loss: 6.5363812Losses:  6.745082378387451 1.62251615524292
CurrentTrain: epoch  6, batch    24 | loss: 6.7450824Losses:  6.9896368980407715 1.8611980676651
CurrentTrain: epoch  6, batch    25 | loss: 6.9896369Losses:  6.948142051696777 1.830672264099121
CurrentTrain: epoch  6, batch    26 | loss: 6.9481421Losses:  6.74747896194458 1.6341605186462402
CurrentTrain: epoch  6, batch    27 | loss: 6.7474790Losses:  6.900725364685059 1.7958678007125854
CurrentTrain: epoch  6, batch    28 | loss: 6.9007254Losses:  6.933684349060059 1.8252745866775513
CurrentTrain: epoch  6, batch    29 | loss: 6.9336843Losses:  6.952553749084473 1.8333261013031006
CurrentTrain: epoch  6, batch    30 | loss: 6.9525537Losses:  6.547223091125488 1.4546096324920654
CurrentTrain: epoch  6, batch    31 | loss: 6.5472231Losses:  6.743640899658203 1.6290695667266846
CurrentTrain: epoch  6, batch    32 | loss: 6.7436409Losses:  6.974857330322266 1.8503339290618896
CurrentTrain: epoch  6, batch    33 | loss: 6.9748573Losses:  6.815449237823486 1.6958374977111816
CurrentTrain: epoch  6, batch    34 | loss: 6.8154492Losses:  6.605578899383545 1.4794392585754395
CurrentTrain: epoch  6, batch    35 | loss: 6.6055789Losses:  6.485055446624756 1.3637059926986694
CurrentTrain: epoch  6, batch    36 | loss: 6.4850554Losses:  6.932814121246338 1.8240474462509155
CurrentTrain: epoch  6, batch    37 | loss: 6.9328141Losses:  7.036553382873535 1.917418122291565
CurrentTrain: epoch  6, batch    38 | loss: 7.0365534Losses:  6.722672462463379 1.6231482028961182
CurrentTrain: epoch  6, batch    39 | loss: 6.7226725Losses:  6.992995262145996 1.8799412250518799
CurrentTrain: epoch  6, batch    40 | loss: 6.9929953Losses:  7.063320159912109 1.948203206062317
CurrentTrain: epoch  6, batch    41 | loss: 7.0633202Losses:  6.950988292694092 1.8439728021621704
CurrentTrain: epoch  6, batch    42 | loss: 6.9509883Losses:  6.718264579772949 1.6221134662628174
CurrentTrain: epoch  6, batch    43 | loss: 6.7182646Losses:  6.814148902893066 1.7065635919570923
CurrentTrain: epoch  6, batch    44 | loss: 6.8141489Losses:  6.9163103103637695 1.8003489971160889
CurrentTrain: epoch  6, batch    45 | loss: 6.9163103Losses:  6.761480331420898 1.6577808856964111
CurrentTrain: epoch  6, batch    46 | loss: 6.7614803Losses:  6.725028038024902 1.612586259841919
CurrentTrain: epoch  6, batch    47 | loss: 6.7250280Losses:  7.070436477661133 1.9598228931427002
CurrentTrain: epoch  6, batch    48 | loss: 7.0704365Losses:  6.668476104736328 1.5605075359344482
CurrentTrain: epoch  6, batch    49 | loss: 6.6684761Losses:  7.0144243240356445 1.9066987037658691
CurrentTrain: epoch  6, batch    50 | loss: 7.0144243Losses:  6.9651055335998535 1.857388973236084
CurrentTrain: epoch  6, batch    51 | loss: 6.9651055Losses:  6.965301513671875 1.8525655269622803
CurrentTrain: epoch  6, batch    52 | loss: 6.9653015Losses:  6.400297164916992 1.2910568714141846
CurrentTrain: epoch  6, batch    53 | loss: 6.4002972Losses:  6.899216651916504 1.795243501663208
CurrentTrain: epoch  6, batch    54 | loss: 6.8992167Losses:  6.709885597229004 1.6109578609466553
CurrentTrain: epoch  6, batch    55 | loss: 6.7098856Losses:  6.613987445831299 1.4950037002563477
CurrentTrain: epoch  6, batch    56 | loss: 6.6139874Losses:  6.979646682739258 1.8688220977783203
CurrentTrain: epoch  6, batch    57 | loss: 6.9796467Losses:  6.847070217132568 1.7439970970153809
CurrentTrain: epoch  6, batch    58 | loss: 6.8470702Losses:  6.911526203155518 1.8060351610183716
CurrentTrain: epoch  6, batch    59 | loss: 6.9115262Losses:  6.584909439086914 1.4681123495101929
CurrentTrain: epoch  6, batch    60 | loss: 6.5849094Losses:  6.81057071685791 1.702239751815796
CurrentTrain: epoch  6, batch    61 | loss: 6.8105707Losses:  6.5152506828308105 1.4138227701187134
CurrentTrain: epoch  6, batch    62 | loss: 6.5152507Losses:  6.7419610023498535 1.6254545450210571
CurrentTrain: epoch  7, batch     0 | loss: 6.7419610Losses:  6.67907190322876 1.5640512704849243
CurrentTrain: epoch  7, batch     1 | loss: 6.6790719Losses:  6.802373886108398 1.7024455070495605
CurrentTrain: epoch  7, batch     2 | loss: 6.8023739Losses:  6.987114906311035 1.878982663154602
CurrentTrain: epoch  7, batch     3 | loss: 6.9871149Losses:  6.6610260009765625 1.553870439529419
CurrentTrain: epoch  7, batch     4 | loss: 6.6610260Losses:  6.927140235900879 1.8299379348754883
CurrentTrain: epoch  7, batch     5 | loss: 6.9271402Losses:  6.9161882400512695 1.8059858083724976
CurrentTrain: epoch  7, batch     6 | loss: 6.9161882Losses:  6.92939567565918 1.8175477981567383
CurrentTrain: epoch  7, batch     7 | loss: 6.9293957Losses:  6.647548675537109 1.5574486255645752
CurrentTrain: epoch  7, batch     8 | loss: 6.6475487Losses:  6.7519450187683105 1.6544116735458374
CurrentTrain: epoch  7, batch     9 | loss: 6.7519450Losses:  6.804656028747559 1.7131378650665283
CurrentTrain: epoch  7, batch    10 | loss: 6.8046560Losses:  6.748476982116699 1.6614203453063965
CurrentTrain: epoch  7, batch    11 | loss: 6.7484770Losses:  6.566120147705078 1.48418128490448
CurrentTrain: epoch  7, batch    12 | loss: 6.5661201Losses:  6.724166393280029 1.6248966455459595
CurrentTrain: epoch  7, batch    13 | loss: 6.7241664Losses:  6.702928066253662 1.6182969808578491
CurrentTrain: epoch  7, batch    14 | loss: 6.7029281Losses:  6.7718353271484375 1.6917517185211182
CurrentTrain: epoch  7, batch    15 | loss: 6.7718353Losses:  6.782267093658447 1.6751818656921387
CurrentTrain: epoch  7, batch    16 | loss: 6.7822671Losses:  6.790413856506348 1.6932837963104248
CurrentTrain: epoch  7, batch    17 | loss: 6.7904139Losses:  6.978072166442871 1.876145601272583
CurrentTrain: epoch  7, batch    18 | loss: 6.9780722Losses:  6.918080806732178 1.8018407821655273
CurrentTrain: epoch  7, batch    19 | loss: 6.9180808Losses:  6.720470428466797 1.5992175340652466
CurrentTrain: epoch  7, batch    20 | loss: 6.7204704Losses:  6.839122772216797 1.631126046180725
CurrentTrain: epoch  7, batch    21 | loss: 6.8391228Losses:  6.628564834594727 1.5201895236968994
CurrentTrain: epoch  7, batch    22 | loss: 6.6285648Losses:  6.599996566772461 1.4880043268203735
CurrentTrain: epoch  7, batch    23 | loss: 6.5999966Losses:  6.7833452224731445 1.6750746965408325
CurrentTrain: epoch  7, batch    24 | loss: 6.7833452Losses:  6.576240539550781 1.4605616331100464
CurrentTrain: epoch  7, batch    25 | loss: 6.5762405Losses:  6.731568336486816 1.6332707405090332
CurrentTrain: epoch  7, batch    26 | loss: 6.7315683Losses:  6.874497413635254 1.772279977798462
CurrentTrain: epoch  7, batch    27 | loss: 6.8744974Losses:  6.803858757019043 1.7076396942138672
CurrentTrain: epoch  7, batch    28 | loss: 6.8038588Losses:  6.716008186340332 1.608985185623169
CurrentTrain: epoch  7, batch    29 | loss: 6.7160082Losses:  6.841772079467773 1.7315021753311157
CurrentTrain: epoch  7, batch    30 | loss: 6.8417721Losses:  6.804839611053467 1.7034096717834473
CurrentTrain: epoch  7, batch    31 | loss: 6.8048396Losses:  6.569957733154297 1.4617681503295898
CurrentTrain: epoch  7, batch    32 | loss: 6.5699577Losses:  6.6860833168029785 1.569754958152771
CurrentTrain: epoch  7, batch    33 | loss: 6.6860833Losses:  6.608726501464844 1.4920942783355713
CurrentTrain: epoch  7, batch    34 | loss: 6.6087265Losses:  6.86691951751709 1.766149878501892
CurrentTrain: epoch  7, batch    35 | loss: 6.8669195Losses:  6.767665863037109 1.6528942584991455
CurrentTrain: epoch  7, batch    36 | loss: 6.7676659Losses:  6.714949131011963 1.6237435340881348
CurrentTrain: epoch  7, batch    37 | loss: 6.7149491Losses:  6.942363739013672 1.8391705751419067
CurrentTrain: epoch  7, batch    38 | loss: 6.9423637Losses:  6.739790439605713 1.646252155303955
CurrentTrain: epoch  7, batch    39 | loss: 6.7397904Losses:  6.8843584060668945 1.7902729511260986
CurrentTrain: epoch  7, batch    40 | loss: 6.8843584Losses:  6.683887481689453 1.5934317111968994
CurrentTrain: epoch  7, batch    41 | loss: 6.6838875Losses:  6.7776899337768555 1.6731507778167725
CurrentTrain: epoch  7, batch    42 | loss: 6.7776899Losses:  6.76423454284668 1.6636993885040283
CurrentTrain: epoch  7, batch    43 | loss: 6.7642345Losses:  6.859055519104004 1.7658360004425049
CurrentTrain: epoch  7, batch    44 | loss: 6.8590555Losses:  6.9196271896362305 1.8257477283477783
CurrentTrain: epoch  7, batch    45 | loss: 6.9196272Losses:  6.731807708740234 1.630544662475586
CurrentTrain: epoch  7, batch    46 | loss: 6.7318077Losses:  6.8548903465271 1.7572664022445679
CurrentTrain: epoch  7, batch    47 | loss: 6.8548903Losses:  6.847397804260254 1.7565159797668457
CurrentTrain: epoch  7, batch    48 | loss: 6.8473978Losses:  6.867290496826172 1.7718794345855713
CurrentTrain: epoch  7, batch    49 | loss: 6.8672905Losses:  6.64967679977417 1.5635828971862793
CurrentTrain: epoch  7, batch    50 | loss: 6.6496768Losses:  6.9283528327941895 1.8292752504348755
CurrentTrain: epoch  7, batch    51 | loss: 6.9283528Losses:  6.558619499206543 1.4490442276000977
CurrentTrain: epoch  7, batch    52 | loss: 6.5586195Losses:  6.838240146636963 1.7331042289733887
CurrentTrain: epoch  7, batch    53 | loss: 6.8382401Losses:  6.745144844055176 1.6386349201202393
CurrentTrain: epoch  7, batch    54 | loss: 6.7451448Losses:  6.671392440795898 1.5710766315460205
CurrentTrain: epoch  7, batch    55 | loss: 6.6713924Losses:  6.529589653015137 1.4360015392303467
CurrentTrain: epoch  7, batch    56 | loss: 6.5295897Losses:  6.906854152679443 1.8180794715881348
CurrentTrain: epoch  7, batch    57 | loss: 6.9068542Losses:  6.924168109893799 1.8377631902694702
CurrentTrain: epoch  7, batch    58 | loss: 6.9241681Losses:  6.59375524520874 1.4936118125915527
CurrentTrain: epoch  7, batch    59 | loss: 6.5937552Losses:  6.959022521972656 1.8538235425949097
CurrentTrain: epoch  7, batch    60 | loss: 6.9590225Losses:  6.798521041870117 1.7022104263305664
CurrentTrain: epoch  7, batch    61 | loss: 6.7985210Losses:  6.562081813812256 1.4564976692199707
CurrentTrain: epoch  7, batch    62 | loss: 6.5620818Losses:  6.982002258300781 1.8833744525909424
CurrentTrain: epoch  8, batch     0 | loss: 6.9820023Losses:  6.813506126403809 1.7108584642410278
CurrentTrain: epoch  8, batch     1 | loss: 6.8135061Losses:  6.831025123596191 1.7349634170532227
CurrentTrain: epoch  8, batch     2 | loss: 6.8310251Losses:  6.687419891357422 1.5882666110992432
CurrentTrain: epoch  8, batch     3 | loss: 6.6874199Losses:  6.6743621826171875 1.5883255004882812
CurrentTrain: epoch  8, batch     4 | loss: 6.6743622Losses:  6.816319465637207 1.726812720298767
CurrentTrain: epoch  8, batch     5 | loss: 6.8163195Losses:  6.885461807250977 1.778761625289917
CurrentTrain: epoch  8, batch     6 | loss: 6.8854618Losses:  6.8562164306640625 1.7569630146026611
CurrentTrain: epoch  8, batch     7 | loss: 6.8562164Losses:  6.824620723724365 1.7290388345718384
CurrentTrain: epoch  8, batch     8 | loss: 6.8246207Losses:  6.891216278076172 1.798980951309204
CurrentTrain: epoch  8, batch     9 | loss: 6.8912163Losses:  6.812727928161621 1.7167384624481201
CurrentTrain: epoch  8, batch    10 | loss: 6.8127279Losses:  6.7509989738464355 1.6597830057144165
CurrentTrain: epoch  8, batch    11 | loss: 6.7509990Losses:  6.833110809326172 1.735386610031128
CurrentTrain: epoch  8, batch    12 | loss: 6.8331108Losses:  6.739651679992676 1.638962984085083
CurrentTrain: epoch  8, batch    13 | loss: 6.7396517Losses:  6.745362281799316 1.6625187397003174
CurrentTrain: epoch  8, batch    14 | loss: 6.7453623Losses:  6.687735080718994 1.5978866815567017
CurrentTrain: epoch  8, batch    15 | loss: 6.6877351Losses:  6.797530174255371 1.7063283920288086
CurrentTrain: epoch  8, batch    16 | loss: 6.7975302Losses:  6.901278018951416 1.8014293909072876
CurrentTrain: epoch  8, batch    17 | loss: 6.9012780Losses:  6.822916507720947 1.7255401611328125
CurrentTrain: epoch  8, batch    18 | loss: 6.8229165Losses:  6.876372814178467 1.7752900123596191
CurrentTrain: epoch  8, batch    19 | loss: 6.8763728Losses:  6.692998886108398 1.6133038997650146
CurrentTrain: epoch  8, batch    20 | loss: 6.6929989Losses:  6.813737869262695 1.7146222591400146
CurrentTrain: epoch  8, batch    21 | loss: 6.8137379Losses:  6.507299423217773 1.4157025814056396
CurrentTrain: epoch  8, batch    22 | loss: 6.5072994Losses:  7.030385971069336 1.9325960874557495
CurrentTrain: epoch  8, batch    23 | loss: 7.0303860Losses:  7.03791618347168 1.9415571689605713
CurrentTrain: epoch  8, batch    24 | loss: 7.0379162Losses:  6.571571350097656 1.4801182746887207
CurrentTrain: epoch  8, batch    25 | loss: 6.5715714Losses:  6.669429779052734 1.5784645080566406
CurrentTrain: epoch  8, batch    26 | loss: 6.6694298Losses:  6.78477668762207 1.6808271408081055
CurrentTrain: epoch  8, batch    27 | loss: 6.7847767Losses:  7.1202073097229 1.926401972770691
CurrentTrain: epoch  8, batch    28 | loss: 7.1202073Losses:  6.840005874633789 1.7473278045654297
CurrentTrain: epoch  8, batch    29 | loss: 6.8400059Losses:  6.761380195617676 1.670609951019287
CurrentTrain: epoch  8, batch    30 | loss: 6.7613802Losses:  6.670749664306641 1.5768091678619385
CurrentTrain: epoch  8, batch    31 | loss: 6.6707497Losses:  6.599287986755371 1.5052576065063477
CurrentTrain: epoch  8, batch    32 | loss: 6.5992880Losses:  6.735673904418945 1.6412079334259033
CurrentTrain: epoch  8, batch    33 | loss: 6.7356739Losses:  7.0367302894592285 1.9450030326843262
CurrentTrain: epoch  8, batch    34 | loss: 7.0367303Losses:  6.783702373504639 1.6998977661132812
CurrentTrain: epoch  8, batch    35 | loss: 6.7837024Losses:  6.706127166748047 1.6090630292892456
CurrentTrain: epoch  8, batch    36 | loss: 6.7061272Losses:  6.735569000244141 1.6389310359954834
CurrentTrain: epoch  8, batch    37 | loss: 6.7355690Losses:  6.6608052253723145 1.578752040863037
CurrentTrain: epoch  8, batch    38 | loss: 6.6608052Losses:  6.829784393310547 1.7345564365386963
CurrentTrain: epoch  8, batch    39 | loss: 6.8297844Losses:  6.923429489135742 1.8263981342315674
CurrentTrain: epoch  8, batch    40 | loss: 6.9234295Losses:  6.930234909057617 1.835683822631836
CurrentTrain: epoch  8, batch    41 | loss: 6.9302349Losses:  6.84727668762207 1.750204086303711
CurrentTrain: epoch  8, batch    42 | loss: 6.8472767Losses:  6.869951248168945 1.7699730396270752
CurrentTrain: epoch  8, batch    43 | loss: 6.8699512Losses:  6.906819820404053 1.8055790662765503
CurrentTrain: epoch  8, batch    44 | loss: 6.9068198Losses:  6.672482490539551 1.5652281045913696
CurrentTrain: epoch  8, batch    45 | loss: 6.6724825Losses:  6.616396903991699 1.5308036804199219
CurrentTrain: epoch  8, batch    46 | loss: 6.6163969Losses:  6.516601085662842 1.4272836446762085
CurrentTrain: epoch  8, batch    47 | loss: 6.5166011Losses:  6.603156566619873 1.5027570724487305
CurrentTrain: epoch  8, batch    48 | loss: 6.6031566Losses:  6.693393707275391 1.5969182252883911
CurrentTrain: epoch  8, batch    49 | loss: 6.6933937Losses:  6.827525615692139 1.7306408882141113
CurrentTrain: epoch  8, batch    50 | loss: 6.8275256Losses:  6.489951133728027 1.4003392457962036
CurrentTrain: epoch  8, batch    51 | loss: 6.4899511Losses:  6.884884834289551 1.786167025566101
CurrentTrain: epoch  8, batch    52 | loss: 6.8848848Losses:  6.783057689666748 1.6820520162582397
CurrentTrain: epoch  8, batch    53 | loss: 6.7830577Losses:  6.931334495544434 1.841585397720337
CurrentTrain: epoch  8, batch    54 | loss: 6.9313345Losses:  6.632577896118164 1.5433294773101807
CurrentTrain: epoch  8, batch    55 | loss: 6.6325779Losses:  6.7346110343933105 1.6376533508300781
CurrentTrain: epoch  8, batch    56 | loss: 6.7346110Losses:  7.022482395172119 1.9267454147338867
CurrentTrain: epoch  8, batch    57 | loss: 7.0224824Losses:  6.733158588409424 1.6351372003555298
CurrentTrain: epoch  8, batch    58 | loss: 6.7331586Losses:  6.849864959716797 1.7535812854766846
CurrentTrain: epoch  8, batch    59 | loss: 6.8498650Losses:  6.692264556884766 1.6139206886291504
CurrentTrain: epoch  8, batch    60 | loss: 6.6922646Losses:  6.958215236663818 1.859824299812317
CurrentTrain: epoch  8, batch    61 | loss: 6.9582152Losses:  6.248932838439941 1.1485469341278076
CurrentTrain: epoch  8, batch    62 | loss: 6.2489328Losses:  6.787320137023926 1.6960704326629639
CurrentTrain: epoch  9, batch     0 | loss: 6.7873201Losses:  6.6486897468566895 1.5519317388534546
CurrentTrain: epoch  9, batch     1 | loss: 6.6486897Losses:  6.914978981018066 1.815891981124878
CurrentTrain: epoch  9, batch     2 | loss: 6.9149790Losses:  6.716430187225342 1.6146597862243652
CurrentTrain: epoch  9, batch     3 | loss: 6.7164302Losses:  6.564242362976074 1.4737799167633057
CurrentTrain: epoch  9, batch     4 | loss: 6.5642424Losses:  6.765356063842773 1.670928955078125
CurrentTrain: epoch  9, batch     5 | loss: 6.7653561Losses:  6.722394943237305 1.6344573497772217
CurrentTrain: epoch  9, batch     6 | loss: 6.7223949Losses:  6.781617164611816 1.6899831295013428
CurrentTrain: epoch  9, batch     7 | loss: 6.7816172Losses:  6.587650775909424 1.5046319961547852
CurrentTrain: epoch  9, batch     8 | loss: 6.5876508Losses:  6.7861528396606445 1.6990387439727783
CurrentTrain: epoch  9, batch     9 | loss: 6.7861528Losses:  6.926246166229248 1.8357906341552734
CurrentTrain: epoch  9, batch    10 | loss: 6.9262462Losses:  6.919468879699707 1.8210883140563965
CurrentTrain: epoch  9, batch    11 | loss: 6.9194689Losses:  6.870228290557861 1.7786468267440796
CurrentTrain: epoch  9, batch    12 | loss: 6.8702283Losses:  6.8702239990234375 1.7783410549163818
CurrentTrain: epoch  9, batch    13 | loss: 6.8702240Losses:  7.035273551940918 1.9396226406097412
CurrentTrain: epoch  9, batch    14 | loss: 7.0352736Losses:  6.975183963775635 1.8811370134353638
CurrentTrain: epoch  9, batch    15 | loss: 6.9751840Losses:  6.850895404815674 1.758960247039795
CurrentTrain: epoch  9, batch    16 | loss: 6.8508954Losses:  6.851829528808594 1.7509071826934814
CurrentTrain: epoch  9, batch    17 | loss: 6.8518295Losses:  6.837031364440918 1.7465459108352661
CurrentTrain: epoch  9, batch    18 | loss: 6.8370314Losses:  6.536246299743652 1.4496238231658936
CurrentTrain: epoch  9, batch    19 | loss: 6.5362463Losses:  6.775391578674316 1.6768763065338135
CurrentTrain: epoch  9, batch    20 | loss: 6.7753916Losses:  6.778298377990723 1.6954964399337769
CurrentTrain: epoch  9, batch    21 | loss: 6.7782984Losses:  6.861027717590332 1.758568525314331
CurrentTrain: epoch  9, batch    22 | loss: 6.8610277Losses:  6.80714225769043 1.7147738933563232
CurrentTrain: epoch  9, batch    23 | loss: 6.8071423Losses:  6.736359596252441 1.6457624435424805
CurrentTrain: epoch  9, batch    24 | loss: 6.7363596Losses:  6.708542823791504 1.6161561012268066
CurrentTrain: epoch  9, batch    25 | loss: 6.7085428Losses:  6.792136192321777 1.7097526788711548
CurrentTrain: epoch  9, batch    26 | loss: 6.7921362Losses:  6.823774337768555 1.7333239316940308
CurrentTrain: epoch  9, batch    27 | loss: 6.8237743Losses:  6.998492240905762 1.9090105295181274
CurrentTrain: epoch  9, batch    28 | loss: 6.9984922Losses:  6.678586006164551 1.593793511390686
CurrentTrain: epoch  9, batch    29 | loss: 6.6785860Losses:  6.794878005981445 1.7030564546585083
CurrentTrain: epoch  9, batch    30 | loss: 6.7948780Losses:  6.8304033279418945 1.7503364086151123
CurrentTrain: epoch  9, batch    31 | loss: 6.8304033Losses:  6.684624671936035 1.6068352460861206
CurrentTrain: epoch  9, batch    32 | loss: 6.6846247Losses:  6.5643205642700195 1.477526307106018
CurrentTrain: epoch  9, batch    33 | loss: 6.5643206Losses:  6.971558570861816 1.885772705078125
CurrentTrain: epoch  9, batch    34 | loss: 6.9715586Losses:  6.983620643615723 1.8940578699111938
CurrentTrain: epoch  9, batch    35 | loss: 6.9836206Losses:  6.5222883224487305 1.4340088367462158
CurrentTrain: epoch  9, batch    36 | loss: 6.5222883Losses:  6.899660587310791 1.8070030212402344
CurrentTrain: epoch  9, batch    37 | loss: 6.8996606Losses:  6.807575702667236 1.7068723440170288
CurrentTrain: epoch  9, batch    38 | loss: 6.8075757Losses:  6.863691329956055 1.7765108346939087
CurrentTrain: epoch  9, batch    39 | loss: 6.8636913Losses:  6.661531925201416 1.545827865600586
CurrentTrain: epoch  9, batch    40 | loss: 6.6615319Losses:  6.61983585357666 1.5354077816009521
CurrentTrain: epoch  9, batch    41 | loss: 6.6198359Losses:  6.747732639312744 1.6536617279052734
CurrentTrain: epoch  9, batch    42 | loss: 6.7477326Losses:  6.681031227111816 1.592979073524475
CurrentTrain: epoch  9, batch    43 | loss: 6.6810312Losses:  6.681365966796875 1.5873897075653076
CurrentTrain: epoch  9, batch    44 | loss: 6.6813660Losses:  6.681175231933594 1.598433256149292
CurrentTrain: epoch  9, batch    45 | loss: 6.6811752Losses:  6.684293270111084 1.6053293943405151
CurrentTrain: epoch  9, batch    46 | loss: 6.6842933Losses:  6.487764835357666 1.3965059518814087
CurrentTrain: epoch  9, batch    47 | loss: 6.4877648Losses:  6.887156009674072 1.805207371711731
CurrentTrain: epoch  9, batch    48 | loss: 6.8871560Losses:  6.771612167358398 1.6708581447601318
CurrentTrain: epoch  9, batch    49 | loss: 6.7716122Losses:  6.906069755554199 1.8171870708465576
CurrentTrain: epoch  9, batch    50 | loss: 6.9060698Losses:  6.929131031036377 1.8295456171035767
CurrentTrain: epoch  9, batch    51 | loss: 6.9291310Losses:  6.816469192504883 1.7224314212799072
CurrentTrain: epoch  9, batch    52 | loss: 6.8164692Losses:  6.772480487823486 1.6962518692016602
CurrentTrain: epoch  9, batch    53 | loss: 6.7724805Losses:  6.734879493713379 1.6485382318496704
CurrentTrain: epoch  9, batch    54 | loss: 6.7348795Losses:  6.7328996658325195 1.64650297164917
CurrentTrain: epoch  9, batch    55 | loss: 6.7328997Losses:  6.903140068054199 1.8263726234436035
CurrentTrain: epoch  9, batch    56 | loss: 6.9031401Losses:  6.9035234451293945 1.7916343212127686
CurrentTrain: epoch  9, batch    57 | loss: 6.9035234Losses:  6.734473705291748 1.657620906829834
CurrentTrain: epoch  9, batch    58 | loss: 6.7344737Losses:  6.8449506759643555 1.7593393325805664
CurrentTrain: epoch  9, batch    59 | loss: 6.8449507Losses:  6.828915596008301 1.7402093410491943
CurrentTrain: epoch  9, batch    60 | loss: 6.8289156Losses:  6.836762428283691 1.751622200012207
CurrentTrain: epoch  9, batch    61 | loss: 6.8367624Losses:  6.555516242980957 1.4641005992889404
CurrentTrain: epoch  9, batch    62 | loss: 6.5555162
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 1 0 2 3 1 0 0 0]
Losses:  8.278576850891113 1.794609785079956
CurrentTrain: epoch  0, batch     0 | loss: 8.2785769Losses:  8.538261413574219 2.057354688644409
CurrentTrain: epoch  0, batch     1 | loss: 8.5382614Losses:  8.368913650512695 1.894228219985962
CurrentTrain: epoch  0, batch     2 | loss: 8.3689137Losses:  8.254069328308105 1.7646706104278564
CurrentTrain: epoch  0, batch     3 | loss: 8.2540693Losses:  8.375592231750488 1.9569275379180908
CurrentTrain: epoch  0, batch     4 | loss: 8.3755922Losses:  8.182735443115234 1.6796832084655762
CurrentTrain: epoch  0, batch     5 | loss: 8.1827354Losses:  8.325416564941406 1.904523253440857
CurrentTrain: epoch  0, batch     6 | loss: 8.3254166Losses:  8.220006942749023 1.846757411956787
CurrentTrain: epoch  0, batch     7 | loss: 8.2200069Losses:  8.290410041809082 1.9051082134246826
CurrentTrain: epoch  0, batch     8 | loss: 8.2904100Losses:  8.364420890808105 1.9127084016799927
CurrentTrain: epoch  0, batch     9 | loss: 8.3644209Losses:  8.113361358642578 1.7416144609451294
CurrentTrain: epoch  0, batch    10 | loss: 8.1133614Losses:  8.014458656311035 1.7076066732406616
CurrentTrain: epoch  0, batch    11 | loss: 8.0144587Losses:  8.169574737548828 1.7652196884155273
CurrentTrain: epoch  0, batch    12 | loss: 8.1695747Losses:  8.295910835266113 1.917572259902954
CurrentTrain: epoch  0, batch    13 | loss: 8.2959108Losses:  8.111461639404297 1.8309473991394043
CurrentTrain: epoch  0, batch    14 | loss: 8.1114616Losses:  8.378549575805664 2.023789882659912
CurrentTrain: epoch  0, batch    15 | loss: 8.3785496Losses:  7.985801696777344 1.5718863010406494
CurrentTrain: epoch  0, batch    16 | loss: 7.9858017Losses:  8.394712448120117 2.040560245513916
CurrentTrain: epoch  0, batch    17 | loss: 8.3947124Losses:  8.178656578063965 1.8058907985687256
CurrentTrain: epoch  0, batch    18 | loss: 8.1786566Losses:  8.321581840515137 1.9996039867401123
CurrentTrain: epoch  0, batch    19 | loss: 8.3215818Losses:  8.179697036743164 1.944390058517456
CurrentTrain: epoch  0, batch    20 | loss: 8.1796970Losses:  8.227855682373047 1.9679746627807617
CurrentTrain: epoch  0, batch    21 | loss: 8.2278557Losses:  8.206998825073242 1.9191443920135498
CurrentTrain: epoch  0, batch    22 | loss: 8.2069988Losses:  8.146333694458008 1.9015566110610962
CurrentTrain: epoch  0, batch    23 | loss: 8.1463337Losses:  8.201024055480957 1.9676083326339722
CurrentTrain: epoch  0, batch    24 | loss: 8.2010241Losses:  8.115307807922363 1.8674652576446533
CurrentTrain: epoch  0, batch    25 | loss: 8.1153078Losses:  8.01218318939209 1.7843725681304932
CurrentTrain: epoch  0, batch    26 | loss: 8.0121832Losses:  8.195365905761719 2.009192943572998
CurrentTrain: epoch  0, batch    27 | loss: 8.1953659Losses:  8.086216926574707 1.8541203737258911
CurrentTrain: epoch  0, batch    28 | loss: 8.0862169Losses:  8.148395538330078 1.943374514579773
CurrentTrain: epoch  0, batch    29 | loss: 8.1483955Losses:  8.321823120117188 2.0612995624542236
CurrentTrain: epoch  0, batch    30 | loss: 8.3218231Losses:  7.935477256774902 1.7573859691619873
CurrentTrain: epoch  0, batch    31 | loss: 7.9354773Losses:  7.676399230957031 1.445694923400879
CurrentTrain: epoch  0, batch    32 | loss: 7.6763992Losses:  7.78671407699585 1.6301045417785645
CurrentTrain: epoch  0, batch    33 | loss: 7.7867141Losses:  7.978155136108398 1.7933018207550049
CurrentTrain: epoch  0, batch    34 | loss: 7.9781551Losses:  8.040026664733887 1.8480055332183838
CurrentTrain: epoch  0, batch    35 | loss: 8.0400267Losses:  8.078634262084961 1.8802740573883057
CurrentTrain: epoch  0, batch    36 | loss: 8.0786343Losses:  8.043211936950684 1.8893611431121826
CurrentTrain: epoch  0, batch    37 | loss: 8.0432119Losses:  8.309701919555664 2.090874671936035
CurrentTrain: epoch  0, batch    38 | loss: 8.3097019Losses:  8.280906677246094 2.1248064041137695
CurrentTrain: epoch  0, batch    39 | loss: 8.2809067Losses:  8.201379776000977 2.0100135803222656
CurrentTrain: epoch  0, batch    40 | loss: 8.2013798Losses:  8.058591842651367 1.9413563013076782
CurrentTrain: epoch  0, batch    41 | loss: 8.0585918Losses:  7.961825370788574 1.905065655708313
CurrentTrain: epoch  0, batch    42 | loss: 7.9618254Losses:  7.65370512008667 1.6156033277511597
CurrentTrain: epoch  0, batch    43 | loss: 7.6537051Losses:  7.7913737297058105 1.711260437965393
CurrentTrain: epoch  0, batch    44 | loss: 7.7913737Losses:  8.037900924682617 1.98820161819458
CurrentTrain: epoch  0, batch    45 | loss: 8.0379009Losses:  7.927371025085449 1.7617719173431396
CurrentTrain: epoch  0, batch    46 | loss: 7.9273710Losses:  7.941732406616211 1.9766921997070312
CurrentTrain: epoch  0, batch    47 | loss: 7.9417324Losses:  7.967031478881836 1.885664939880371
CurrentTrain: epoch  0, batch    48 | loss: 7.9670315Losses:  7.923669338226318 1.8879998922348022
CurrentTrain: epoch  0, batch    49 | loss: 7.9236693Losses:  8.012958526611328 1.938699722290039
CurrentTrain: epoch  0, batch    50 | loss: 8.0129585Losses:  8.088849067687988 2.051797866821289
CurrentTrain: epoch  0, batch    51 | loss: 8.0888491Losses:  7.863009452819824 1.8754527568817139
CurrentTrain: epoch  0, batch    52 | loss: 7.8630095Losses:  7.846285343170166 1.8493232727050781
CurrentTrain: epoch  0, batch    53 | loss: 7.8462853Losses:  8.082691192626953 2.0132806301116943
CurrentTrain: epoch  0, batch    54 | loss: 8.0826912Losses:  7.715917110443115 1.7924696207046509
CurrentTrain: epoch  0, batch    55 | loss: 7.7159171Losses:  7.631577968597412 1.74891996383667
CurrentTrain: epoch  0, batch    56 | loss: 7.6315780Losses:  7.938819885253906 1.9374170303344727
CurrentTrain: epoch  0, batch    57 | loss: 7.9388199Losses:  7.6355695724487305 1.683288335800171
CurrentTrain: epoch  0, batch    58 | loss: 7.6355696Losses:  7.580199718475342 1.671980381011963
CurrentTrain: epoch  0, batch    59 | loss: 7.5801997Losses:  7.753963470458984 1.886918067932129
CurrentTrain: epoch  0, batch    60 | loss: 7.7539635Losses:  7.660689353942871 1.7081773281097412
CurrentTrain: epoch  0, batch    61 | loss: 7.6606894Losses:  7.572140216827393 1.4379639625549316
CurrentTrain: epoch  0, batch    62 | loss: 7.5721402Losses:  7.728011131286621 1.7433944940567017
CurrentTrain: epoch  1, batch     0 | loss: 7.7280111Losses:  7.73112678527832 1.8306224346160889
CurrentTrain: epoch  1, batch     1 | loss: 7.7311268Losses:  7.6380181312561035 1.7409915924072266
CurrentTrain: epoch  1, batch     2 | loss: 7.6380181Losses:  7.845799446105957 1.9945194721221924
CurrentTrain: epoch  1, batch     3 | loss: 7.8457994Losses:  7.967294692993164 1.9774683713912964
CurrentTrain: epoch  1, batch     4 | loss: 7.9672947Losses:  7.739072799682617 1.8691885471343994
CurrentTrain: epoch  1, batch     5 | loss: 7.7390728Losses:  7.709527015686035 1.8540880680084229
CurrentTrain: epoch  1, batch     6 | loss: 7.7095270Losses:  7.778566837310791 1.9861392974853516
CurrentTrain: epoch  1, batch     7 | loss: 7.7785668Losses:  7.45347261428833 1.6665549278259277
CurrentTrain: epoch  1, batch     8 | loss: 7.4534726Losses:  7.801273345947266 1.9182183742523193
CurrentTrain: epoch  1, batch     9 | loss: 7.8012733Losses:  7.779123783111572 1.892250657081604
CurrentTrain: epoch  1, batch    10 | loss: 7.7791238Losses:  7.736492156982422 1.9033441543579102
CurrentTrain: epoch  1, batch    11 | loss: 7.7364922Losses:  7.820601940155029 1.9602537155151367
CurrentTrain: epoch  1, batch    12 | loss: 7.8206019Losses:  7.4463043212890625 1.5853699445724487
CurrentTrain: epoch  1, batch    13 | loss: 7.4463043Losses:  7.690723896026611 1.900238037109375
CurrentTrain: epoch  1, batch    14 | loss: 7.6907239Losses:  7.84114933013916 1.8453314304351807
CurrentTrain: epoch  1, batch    15 | loss: 7.8411493Losses:  7.523887634277344 1.7162100076675415
CurrentTrain: epoch  1, batch    16 | loss: 7.5238876Losses:  7.261039733886719 1.5489555597305298
CurrentTrain: epoch  1, batch    17 | loss: 7.2610397Losses:  7.534901142120361 1.7801051139831543
CurrentTrain: epoch  1, batch    18 | loss: 7.5349011Losses:  7.677830696105957 1.9017269611358643
CurrentTrain: epoch  1, batch    19 | loss: 7.6778307Losses:  7.797208786010742 2.002408742904663
CurrentTrain: epoch  1, batch    20 | loss: 7.7972088Losses:  7.787486553192139 1.9935706853866577
CurrentTrain: epoch  1, batch    21 | loss: 7.7874866Losses:  7.605133056640625 1.8303200006484985
CurrentTrain: epoch  1, batch    22 | loss: 7.6051331Losses:  7.510763168334961 1.6689226627349854
CurrentTrain: epoch  1, batch    23 | loss: 7.5107632Losses:  7.65790319442749 1.8474758863449097
CurrentTrain: epoch  1, batch    24 | loss: 7.6579032Losses:  7.391627311706543 1.7633965015411377
CurrentTrain: epoch  1, batch    25 | loss: 7.3916273Losses:  7.705831527709961 1.9943938255310059
CurrentTrain: epoch  1, batch    26 | loss: 7.7058315Losses:  7.622855186462402 1.8797736167907715
CurrentTrain: epoch  1, batch    27 | loss: 7.6228552Losses:  7.607585906982422 1.9090321063995361
CurrentTrain: epoch  1, batch    28 | loss: 7.6075859Losses:  7.629985332489014 1.9779038429260254
CurrentTrain: epoch  1, batch    29 | loss: 7.6299853Losses:  7.5407915115356445 1.8368470668792725
CurrentTrain: epoch  1, batch    30 | loss: 7.5407915Losses:  7.643730640411377 2.0135202407836914
CurrentTrain: epoch  1, batch    31 | loss: 7.6437306Losses:  7.430889129638672 1.8772804737091064
CurrentTrain: epoch  1, batch    32 | loss: 7.4308891Losses:  7.29850435256958 1.655098557472229
CurrentTrain: epoch  1, batch    33 | loss: 7.2985044Losses:  7.659939289093018 1.9482296705245972
CurrentTrain: epoch  1, batch    34 | loss: 7.6599393Losses:  7.369531631469727 1.8007481098175049
CurrentTrain: epoch  1, batch    35 | loss: 7.3695316Losses:  7.438024044036865 1.6670422554016113
CurrentTrain: epoch  1, batch    36 | loss: 7.4380240Losses:  7.67229700088501 1.941250205039978
CurrentTrain: epoch  1, batch    37 | loss: 7.6722970Losses:  7.654269695281982 1.98472261428833
CurrentTrain: epoch  1, batch    38 | loss: 7.6542697Losses:  7.499538421630859 1.9033031463623047
CurrentTrain: epoch  1, batch    39 | loss: 7.4995384Losses:  7.5467729568481445 1.814394235610962
CurrentTrain: epoch  1, batch    40 | loss: 7.5467730Losses:  7.682919025421143 1.9452309608459473
CurrentTrain: epoch  1, batch    41 | loss: 7.6829190Losses:  7.451076507568359 1.828845739364624
CurrentTrain: epoch  1, batch    42 | loss: 7.4510765Losses:  7.542154312133789 1.855363130569458
CurrentTrain: epoch  1, batch    43 | loss: 7.5421543Losses:  7.463569164276123 1.732568383216858
CurrentTrain: epoch  1, batch    44 | loss: 7.4635692Losses:  7.512567520141602 1.9494614601135254
CurrentTrain: epoch  1, batch    45 | loss: 7.5125675Losses:  7.393475532531738 1.8874096870422363
CurrentTrain: epoch  1, batch    46 | loss: 7.3934755Losses:  7.702186107635498 2.0698561668395996
CurrentTrain: epoch  1, batch    47 | loss: 7.7021861Losses:  7.483014106750488 1.8476502895355225
CurrentTrain: epoch  1, batch    48 | loss: 7.4830141Losses:  7.422074317932129 1.8492662906646729
CurrentTrain: epoch  1, batch    49 | loss: 7.4220743Losses:  7.353542327880859 1.802445650100708
CurrentTrain: epoch  1, batch    50 | loss: 7.3535423Losses:  7.442397594451904 1.715701937675476
CurrentTrain: epoch  1, batch    51 | loss: 7.4423976Losses:  7.281596660614014 1.770642876625061
CurrentTrain: epoch  1, batch    52 | loss: 7.2815967Losses:  7.354910373687744 1.800943374633789
CurrentTrain: epoch  1, batch    53 | loss: 7.3549104Losses:  7.433892250061035 1.887157678604126
CurrentTrain: epoch  1, batch    54 | loss: 7.4338923Losses:  7.064896583557129 1.6398611068725586
CurrentTrain: epoch  1, batch    55 | loss: 7.0648966Losses:  7.074714660644531 1.6628897190093994
CurrentTrain: epoch  1, batch    56 | loss: 7.0747147Losses:  7.421936988830566 1.9240434169769287
CurrentTrain: epoch  1, batch    57 | loss: 7.4219370Losses:  7.209801197052002 1.7078646421432495
CurrentTrain: epoch  1, batch    58 | loss: 7.2098012Losses:  7.591663837432861 1.9897493124008179
CurrentTrain: epoch  1, batch    59 | loss: 7.5916638Losses:  7.469587326049805 1.9263163805007935
CurrentTrain: epoch  1, batch    60 | loss: 7.4695873Losses:  7.443845748901367 1.956899881362915
CurrentTrain: epoch  1, batch    61 | loss: 7.4438457Losses:  6.6185688972473145 1.3162007331848145
CurrentTrain: epoch  1, batch    62 | loss: 6.6185689Losses:  7.12942361831665 1.611222743988037
CurrentTrain: epoch  2, batch     0 | loss: 7.1294236Losses:  7.10290002822876 1.6190714836120605
CurrentTrain: epoch  2, batch     1 | loss: 7.1029000Losses:  7.355459213256836 1.8833847045898438
CurrentTrain: epoch  2, batch     2 | loss: 7.3554592Losses:  7.188835144042969 1.814229130744934
CurrentTrain: epoch  2, batch     3 | loss: 7.1888351Losses:  7.21505880355835 1.7962565422058105
CurrentTrain: epoch  2, batch     4 | loss: 7.2150588Losses:  7.448056221008301 2.040980577468872
CurrentTrain: epoch  2, batch     5 | loss: 7.4480562Losses:  7.041850566864014 1.674558162689209
CurrentTrain: epoch  2, batch     6 | loss: 7.0418506Losses:  7.374361991882324 1.874974012374878
CurrentTrain: epoch  2, batch     7 | loss: 7.3743620Losses:  7.417815685272217 1.8577170372009277
CurrentTrain: epoch  2, batch     8 | loss: 7.4178157Losses:  7.144340515136719 1.7525354623794556
CurrentTrain: epoch  2, batch     9 | loss: 7.1443405Losses:  7.194009304046631 1.8330849409103394
CurrentTrain: epoch  2, batch    10 | loss: 7.1940093Losses:  7.273528575897217 1.8026509284973145
CurrentTrain: epoch  2, batch    11 | loss: 7.2735286Losses:  7.277190685272217 1.7466120719909668
CurrentTrain: epoch  2, batch    12 | loss: 7.2771907Losses:  7.247550010681152 1.718845248222351
CurrentTrain: epoch  2, batch    13 | loss: 7.2475500Losses:  7.454195022583008 1.9175673723220825
CurrentTrain: epoch  2, batch    14 | loss: 7.4541950Losses:  7.398769855499268 1.9580999612808228
CurrentTrain: epoch  2, batch    15 | loss: 7.3987699Losses:  7.164066314697266 1.8243436813354492
CurrentTrain: epoch  2, batch    16 | loss: 7.1640663Losses:  6.799415588378906 1.5015065670013428
CurrentTrain: epoch  2, batch    17 | loss: 6.7994156Losses:  7.51677131652832 1.9888560771942139
CurrentTrain: epoch  2, batch    18 | loss: 7.5167713Losses:  7.264969825744629 1.800868272781372
CurrentTrain: epoch  2, batch    19 | loss: 7.2649698Losses:  7.234368801116943 1.8649266958236694
CurrentTrain: epoch  2, batch    20 | loss: 7.2343688Losses:  7.202714920043945 1.8322646617889404
CurrentTrain: epoch  2, batch    21 | loss: 7.2027149Losses:  7.146045684814453 1.8212814331054688
CurrentTrain: epoch  2, batch    22 | loss: 7.1460457Losses:  7.35638427734375 1.896066427230835
CurrentTrain: epoch  2, batch    23 | loss: 7.3563843Losses:  7.160923957824707 1.7773010730743408
CurrentTrain: epoch  2, batch    24 | loss: 7.1609240Losses:  7.232682228088379 1.8441839218139648
CurrentTrain: epoch  2, batch    25 | loss: 7.2326822Losses:  7.202213287353516 1.8267967700958252
CurrentTrain: epoch  2, batch    26 | loss: 7.2022133Losses:  6.997183799743652 1.595862627029419
CurrentTrain: epoch  2, batch    27 | loss: 6.9971838Losses:  7.065775394439697 1.7132114171981812
CurrentTrain: epoch  2, batch    28 | loss: 7.0657754Losses:  7.416635513305664 2.0359983444213867
CurrentTrain: epoch  2, batch    29 | loss: 7.4166355Losses:  7.340271949768066 1.9241454601287842
CurrentTrain: epoch  2, batch    30 | loss: 7.3402719Losses:  7.423377990722656 2.074521780014038
CurrentTrain: epoch  2, batch    31 | loss: 7.4233780Losses:  7.236627578735352 1.707930326461792
CurrentTrain: epoch  2, batch    32 | loss: 7.2366276Losses:  7.034887313842773 1.7402334213256836
CurrentTrain: epoch  2, batch    33 | loss: 7.0348873Losses:  7.329444885253906 1.916982889175415
CurrentTrain: epoch  2, batch    34 | loss: 7.3294449Losses:  7.258903980255127 1.8102411031723022
CurrentTrain: epoch  2, batch    35 | loss: 7.2589040Losses:  7.210635662078857 1.8982681035995483
CurrentTrain: epoch  2, batch    36 | loss: 7.2106357Losses:  7.028956890106201 1.6666512489318848
CurrentTrain: epoch  2, batch    37 | loss: 7.0289569Losses:  7.17100715637207 1.78733229637146
CurrentTrain: epoch  2, batch    38 | loss: 7.1710072Losses:  6.898194789886475 1.6548341512680054
CurrentTrain: epoch  2, batch    39 | loss: 6.8981948Losses:  6.85075044631958 1.5729436874389648
CurrentTrain: epoch  2, batch    40 | loss: 6.8507504Losses:  6.876145362854004 1.4193646907806396
CurrentTrain: epoch  2, batch    41 | loss: 6.8761454Losses:  7.136858940124512 1.8242565393447876
CurrentTrain: epoch  2, batch    42 | loss: 7.1368589Losses:  7.215668678283691 1.8101454973220825
CurrentTrain: epoch  2, batch    43 | loss: 7.2156687Losses:  7.250529766082764 1.9171934127807617
CurrentTrain: epoch  2, batch    44 | loss: 7.2505298Losses:  7.290235996246338 1.8898940086364746
CurrentTrain: epoch  2, batch    45 | loss: 7.2902360Losses:  7.172328948974609 1.8792202472686768
CurrentTrain: epoch  2, batch    46 | loss: 7.1723289Losses:  7.227807521820068 1.8730802536010742
CurrentTrain: epoch  2, batch    47 | loss: 7.2278075Losses:  7.088098526000977 1.7906482219696045
CurrentTrain: epoch  2, batch    48 | loss: 7.0880985Losses:  7.206182956695557 1.8619227409362793
CurrentTrain: epoch  2, batch    49 | loss: 7.2061830Losses:  6.977434158325195 1.6880931854248047
CurrentTrain: epoch  2, batch    50 | loss: 6.9774342Losses:  7.192403793334961 1.9098615646362305
CurrentTrain: epoch  2, batch    51 | loss: 7.1924038Losses:  7.079900741577148 1.6869157552719116
CurrentTrain: epoch  2, batch    52 | loss: 7.0799007Losses:  7.247498512268066 1.963180661201477
CurrentTrain: epoch  2, batch    53 | loss: 7.2474985Losses:  7.1887125968933105 1.893486499786377
CurrentTrain: epoch  2, batch    54 | loss: 7.1887126Losses:  7.271259307861328 1.9648220539093018
CurrentTrain: epoch  2, batch    55 | loss: 7.2712593Losses:  7.345383644104004 2.0309362411499023
CurrentTrain: epoch  2, batch    56 | loss: 7.3453836Losses:  7.108100414276123 1.763992428779602
CurrentTrain: epoch  2, batch    57 | loss: 7.1081004Losses:  7.015528678894043 1.702113389968872
CurrentTrain: epoch  2, batch    58 | loss: 7.0155287Losses:  7.146144866943359 1.8484556674957275
CurrentTrain: epoch  2, batch    59 | loss: 7.1461449Losses:  7.090646743774414 1.8362760543823242
CurrentTrain: epoch  2, batch    60 | loss: 7.0906467Losses:  7.041828155517578 1.7581701278686523
CurrentTrain: epoch  2, batch    61 | loss: 7.0418282Losses:  6.819507598876953 1.5654144287109375
CurrentTrain: epoch  2, batch    62 | loss: 6.8195076Losses:  7.134303092956543 1.8066930770874023
CurrentTrain: epoch  3, batch     0 | loss: 7.1343031Losses:  7.079293251037598 1.7141683101654053
CurrentTrain: epoch  3, batch     1 | loss: 7.0792933Losses:  7.175845146179199 1.900441288948059
CurrentTrain: epoch  3, batch     2 | loss: 7.1758451Losses:  6.894315719604492 1.6506330966949463
CurrentTrain: epoch  3, batch     3 | loss: 6.8943157Losses:  7.056159973144531 1.8005281686782837
CurrentTrain: epoch  3, batch     4 | loss: 7.0561600Losses:  7.096538543701172 1.80009126663208
CurrentTrain: epoch  3, batch     5 | loss: 7.0965385Losses:  7.057330131530762 1.8073054552078247
CurrentTrain: epoch  3, batch     6 | loss: 7.0573301Losses:  7.135176181793213 1.9014192819595337
CurrentTrain: epoch  3, batch     7 | loss: 7.1351762Losses:  7.108642578125 1.8087031841278076
CurrentTrain: epoch  3, batch     8 | loss: 7.1086426Losses:  7.231924533843994 1.9486699104309082
CurrentTrain: epoch  3, batch     9 | loss: 7.2319245Losses:  7.3167314529418945 2.031604290008545
CurrentTrain: epoch  3, batch    10 | loss: 7.3167315Losses:  7.098804950714111 1.807228446006775
CurrentTrain: epoch  3, batch    11 | loss: 7.0988050Losses:  6.981630325317383 1.7708356380462646
CurrentTrain: epoch  3, batch    12 | loss: 6.9816303Losses:  7.311953544616699 2.019744634628296
CurrentTrain: epoch  3, batch    13 | loss: 7.3119535Losses:  7.1038079261779785 1.8799715042114258
CurrentTrain: epoch  3, batch    14 | loss: 7.1038079Losses:  7.10137939453125 1.8936983346939087
CurrentTrain: epoch  3, batch    15 | loss: 7.1013794Losses:  7.094298362731934 1.8502954244613647
CurrentTrain: epoch  3, batch    16 | loss: 7.0942984Losses:  7.0935959815979 1.6609355211257935
CurrentTrain: epoch  3, batch    17 | loss: 7.0935960Losses:  6.97955322265625 1.7591477632522583
CurrentTrain: epoch  3, batch    18 | loss: 6.9795532Losses:  7.095880031585693 1.7991262674331665
CurrentTrain: epoch  3, batch    19 | loss: 7.0958800Losses:  7.071569442749023 1.8081738948822021
CurrentTrain: epoch  3, batch    20 | loss: 7.0715694Losses:  7.171245098114014 1.9353326559066772
CurrentTrain: epoch  3, batch    21 | loss: 7.1712451Losses:  6.903177738189697 1.6651597023010254
CurrentTrain: epoch  3, batch    22 | loss: 6.9031777Losses:  7.171332359313965 1.931168556213379
CurrentTrain: epoch  3, batch    23 | loss: 7.1713324Losses:  6.8041582107543945 1.5697498321533203
CurrentTrain: epoch  3, batch    24 | loss: 6.8041582Losses:  7.221782684326172 2.0144197940826416
CurrentTrain: epoch  3, batch    25 | loss: 7.2217827Losses:  7.264566898345947 2.0202083587646484
CurrentTrain: epoch  3, batch    26 | loss: 7.2645669Losses:  7.095039367675781 1.8700835704803467
CurrentTrain: epoch  3, batch    27 | loss: 7.0950394Losses:  7.282811641693115 2.0508828163146973
CurrentTrain: epoch  3, batch    28 | loss: 7.2828116Losses:  7.069638252258301 1.7822978496551514
CurrentTrain: epoch  3, batch    29 | loss: 7.0696383Losses:  7.365760326385498 1.9949365854263306
CurrentTrain: epoch  3, batch    30 | loss: 7.3657603Losses:  7.028862476348877 1.7911916971206665
CurrentTrain: epoch  3, batch    31 | loss: 7.0288625Losses:  6.84222412109375 1.5728847980499268
CurrentTrain: epoch  3, batch    32 | loss: 6.8422241Losses:  6.997617721557617 1.737267017364502
CurrentTrain: epoch  3, batch    33 | loss: 6.9976177Losses:  7.008229732513428 1.8020139932632446
CurrentTrain: epoch  3, batch    34 | loss: 7.0082297Losses:  7.065997123718262 1.8628761768341064
CurrentTrain: epoch  3, batch    35 | loss: 7.0659971Losses:  7.2009477615356445 1.8837831020355225
CurrentTrain: epoch  3, batch    36 | loss: 7.2009478Losses:  7.096241474151611 1.8315925598144531
CurrentTrain: epoch  3, batch    37 | loss: 7.0962415Losses:  7.123579978942871 1.9115372896194458
CurrentTrain: epoch  3, batch    38 | loss: 7.1235800Losses:  6.918725967407227 1.594024419784546
CurrentTrain: epoch  3, batch    39 | loss: 6.9187260Losses:  7.133255481719971 1.9298092126846313
CurrentTrain: epoch  3, batch    40 | loss: 7.1332555Losses:  6.816287040710449 1.6466896533966064
CurrentTrain: epoch  3, batch    41 | loss: 6.8162870Losses:  6.923431873321533 1.6592332124710083
CurrentTrain: epoch  3, batch    42 | loss: 6.9234319Losses:  6.953346252441406 1.7277761697769165
CurrentTrain: epoch  3, batch    43 | loss: 6.9533463Losses:  7.2328901290893555 1.9395718574523926
CurrentTrain: epoch  3, batch    44 | loss: 7.2328901Losses:  6.96067476272583 1.7103772163391113
CurrentTrain: epoch  3, batch    45 | loss: 6.9606748Losses:  7.094964027404785 1.9064395427703857
CurrentTrain: epoch  3, batch    46 | loss: 7.0949640Losses:  6.844118595123291 1.6368093490600586
CurrentTrain: epoch  3, batch    47 | loss: 6.8441186Losses:  6.632068634033203 1.4674127101898193
CurrentTrain: epoch  3, batch    48 | loss: 6.6320686Losses:  6.726903915405273 1.5093472003936768
CurrentTrain: epoch  3, batch    49 | loss: 6.7269039Losses:  7.099105358123779 1.9057999849319458
CurrentTrain: epoch  3, batch    50 | loss: 7.0991054Losses:  6.872111797332764 1.6639350652694702
CurrentTrain: epoch  3, batch    51 | loss: 6.8721118Losses:  6.955690383911133 1.7629566192626953
CurrentTrain: epoch  3, batch    52 | loss: 6.9556904Losses:  7.130733489990234 1.8439143896102905
CurrentTrain: epoch  3, batch    53 | loss: 7.1307335Losses:  7.020416736602783 1.8408961296081543
CurrentTrain: epoch  3, batch    54 | loss: 7.0204167Losses:  7.210763454437256 2.0185751914978027
CurrentTrain: epoch  3, batch    55 | loss: 7.2107635Losses:  6.936923980712891 1.7472693920135498
CurrentTrain: epoch  3, batch    56 | loss: 6.9369240Losses:  6.969720840454102 1.759739875793457
CurrentTrain: epoch  3, batch    57 | loss: 6.9697208Losses:  6.9978485107421875 1.7563114166259766
CurrentTrain: epoch  3, batch    58 | loss: 6.9978485Losses:  6.7081732749938965 1.5384163856506348
CurrentTrain: epoch  3, batch    59 | loss: 6.7081733Losses:  7.146878242492676 1.949460744857788
CurrentTrain: epoch  3, batch    60 | loss: 7.1468782Losses:  6.991832733154297 1.82126784324646
CurrentTrain: epoch  3, batch    61 | loss: 6.9918327Losses:  6.548374176025391 1.3642021417617798
CurrentTrain: epoch  3, batch    62 | loss: 6.5483742Losses:  6.597777366638184 1.4519152641296387
CurrentTrain: epoch  4, batch     0 | loss: 6.5977774Losses:  6.8459601402282715 1.6710615158081055
CurrentTrain: epoch  4, batch     1 | loss: 6.8459601Losses:  6.986151695251465 1.8042488098144531
CurrentTrain: epoch  4, batch     2 | loss: 6.9861517Losses:  6.76578426361084 1.6144038438796997
CurrentTrain: epoch  4, batch     3 | loss: 6.7657843Losses:  7.210113048553467 2.019911289215088
CurrentTrain: epoch  4, batch     4 | loss: 7.2101130Losses:  7.193479537963867 2.018864393234253
CurrentTrain: epoch  4, batch     5 | loss: 7.1934795Losses:  6.911550521850586 1.7379601001739502
CurrentTrain: epoch  4, batch     6 | loss: 6.9115505Losses:  6.774474143981934 1.6038954257965088
CurrentTrain: epoch  4, batch     7 | loss: 6.7744741Losses:  7.178509712219238 2.0053834915161133
CurrentTrain: epoch  4, batch     8 | loss: 7.1785097Losses:  6.9465413093566895 1.7709431648254395
CurrentTrain: epoch  4, batch     9 | loss: 6.9465413Losses:  6.921760559082031 1.7439818382263184
CurrentTrain: epoch  4, batch    10 | loss: 6.9217606Losses:  6.964560031890869 1.7664533853530884
CurrentTrain: epoch  4, batch    11 | loss: 6.9645600Losses:  6.957395553588867 1.7800452709197998
CurrentTrain: epoch  4, batch    12 | loss: 6.9573956Losses:  7.098392486572266 1.889171838760376
CurrentTrain: epoch  4, batch    13 | loss: 7.0983925Losses:  6.731462478637695 1.5902621746063232
CurrentTrain: epoch  4, batch    14 | loss: 6.7314625Losses:  6.883195877075195 1.698333978652954
CurrentTrain: epoch  4, batch    15 | loss: 6.8831959Losses:  6.90672492980957 1.731619954109192
CurrentTrain: epoch  4, batch    16 | loss: 6.9067249Losses:  6.854378700256348 1.6925488710403442
CurrentTrain: epoch  4, batch    17 | loss: 6.8543787Losses:  7.035831928253174 1.8757719993591309
CurrentTrain: epoch  4, batch    18 | loss: 7.0358319Losses:  6.994660377502441 1.844911813735962
CurrentTrain: epoch  4, batch    19 | loss: 6.9946604Losses:  6.934884548187256 1.6574406623840332
CurrentTrain: epoch  4, batch    20 | loss: 6.9348845Losses:  6.726485252380371 1.5458087921142578
CurrentTrain: epoch  4, batch    21 | loss: 6.7264853Losses:  6.815058708190918 1.6792619228363037
CurrentTrain: epoch  4, batch    22 | loss: 6.8150587Losses:  6.573014736175537 1.4390462636947632
CurrentTrain: epoch  4, batch    23 | loss: 6.5730147Losses:  6.6822404861450195 1.4729139804840088
CurrentTrain: epoch  4, batch    24 | loss: 6.6822405Losses:  7.097609519958496 1.9261608123779297
CurrentTrain: epoch  4, batch    25 | loss: 7.0976095Losses:  6.948738098144531 1.8098386526107788
CurrentTrain: epoch  4, batch    26 | loss: 6.9487381Losses:  6.856954574584961 1.7154788970947266
CurrentTrain: epoch  4, batch    27 | loss: 6.8569546Losses:  6.87772274017334 1.6973364353179932
CurrentTrain: epoch  4, batch    28 | loss: 6.8777227Losses:  6.891473770141602 1.7122125625610352
CurrentTrain: epoch  4, batch    29 | loss: 6.8914738Losses:  6.907559394836426 1.7663748264312744
CurrentTrain: epoch  4, batch    30 | loss: 6.9075594Losses:  6.9416399002075195 1.7223827838897705
CurrentTrain: epoch  4, batch    31 | loss: 6.9416399Losses:  6.948775291442871 1.785477876663208
CurrentTrain: epoch  4, batch    32 | loss: 6.9487753Losses:  7.063676357269287 1.8829666376113892
CurrentTrain: epoch  4, batch    33 | loss: 7.0636764Losses:  6.925637722015381 1.768168568611145
CurrentTrain: epoch  4, batch    34 | loss: 6.9256377Losses:  7.138843536376953 1.939777135848999
CurrentTrain: epoch  4, batch    35 | loss: 7.1388435Losses:  6.7954912185668945 1.6149226427078247
CurrentTrain: epoch  4, batch    36 | loss: 6.7954912Losses:  7.070377349853516 1.8980340957641602
CurrentTrain: epoch  4, batch    37 | loss: 7.0703773Losses:  6.85727596282959 1.714996099472046
CurrentTrain: epoch  4, batch    38 | loss: 6.8572760Losses:  6.90083646774292 1.7619377374649048
CurrentTrain: epoch  4, batch    39 | loss: 6.9008365Losses:  6.847954273223877 1.71400785446167
CurrentTrain: epoch  4, batch    40 | loss: 6.8479543Losses:  6.952023506164551 1.8232085704803467
CurrentTrain: epoch  4, batch    41 | loss: 6.9520235Losses:  7.054406642913818 1.7803349494934082
CurrentTrain: epoch  4, batch    42 | loss: 7.0544066Losses:  6.768246650695801 1.511962652206421
CurrentTrain: epoch  4, batch    43 | loss: 6.7682467Losses:  6.6905198097229 1.5573978424072266
CurrentTrain: epoch  4, batch    44 | loss: 6.6905198Losses:  6.803697109222412 1.5950398445129395
CurrentTrain: epoch  4, batch    45 | loss: 6.8036971Losses:  6.948338031768799 1.7840172052383423
CurrentTrain: epoch  4, batch    46 | loss: 6.9483380Losses:  6.924385070800781 1.7933712005615234
CurrentTrain: epoch  4, batch    47 | loss: 6.9243851Losses:  7.070046424865723 1.9168004989624023
CurrentTrain: epoch  4, batch    48 | loss: 7.0700464Losses:  6.970523834228516 1.8262816667556763
CurrentTrain: epoch  4, batch    49 | loss: 6.9705238Losses:  7.146706581115723 1.9892919063568115
CurrentTrain: epoch  4, batch    50 | loss: 7.1467066Losses:  6.862458229064941 1.6977746486663818
CurrentTrain: epoch  4, batch    51 | loss: 6.8624582Losses:  6.683990001678467 1.564016342163086
CurrentTrain: epoch  4, batch    52 | loss: 6.6839900Losses:  6.926967620849609 1.7629812955856323
CurrentTrain: epoch  4, batch    53 | loss: 6.9269676Losses:  6.802631378173828 1.6430472135543823
CurrentTrain: epoch  4, batch    54 | loss: 6.8026314Losses:  6.954665184020996 1.8173506259918213
CurrentTrain: epoch  4, batch    55 | loss: 6.9546652Losses:  6.770146369934082 1.6313382387161255
CurrentTrain: epoch  4, batch    56 | loss: 6.7701464Losses:  7.148307800292969 1.9972445964813232
CurrentTrain: epoch  4, batch    57 | loss: 7.1483078Losses:  6.904686450958252 1.754906177520752
CurrentTrain: epoch  4, batch    58 | loss: 6.9046865Losses:  6.921932697296143 1.7671197652816772
CurrentTrain: epoch  4, batch    59 | loss: 6.9219327Losses:  6.878485202789307 1.7431039810180664
CurrentTrain: epoch  4, batch    60 | loss: 6.8784852Losses:  6.993892669677734 1.8349659442901611
CurrentTrain: epoch  4, batch    61 | loss: 6.9938927Losses:  6.7678751945495605 1.4114089012145996
CurrentTrain: epoch  4, batch    62 | loss: 6.7678752Losses:  7.112340927124023 1.9669039249420166
CurrentTrain: epoch  5, batch     0 | loss: 7.1123409Losses:  7.0103535652160645 1.8658868074417114
CurrentTrain: epoch  5, batch     1 | loss: 7.0103536Losses:  6.757828712463379 1.6087483167648315
CurrentTrain: epoch  5, batch     2 | loss: 6.7578287Losses:  6.894230842590332 1.758702039718628
CurrentTrain: epoch  5, batch     3 | loss: 6.8942308Losses:  6.944636344909668 1.714601755142212
CurrentTrain: epoch  5, batch     4 | loss: 6.9446363Losses:  6.88955020904541 1.7436448335647583
CurrentTrain: epoch  5, batch     5 | loss: 6.8895502Losses:  6.993100166320801 1.8400688171386719
CurrentTrain: epoch  5, batch     6 | loss: 6.9931002Losses:  7.003903865814209 1.858163833618164
CurrentTrain: epoch  5, batch     7 | loss: 7.0039039Losses:  7.0199360847473145 1.8566218614578247
CurrentTrain: epoch  5, batch     8 | loss: 7.0199361Losses:  6.703895568847656 1.5778758525848389
CurrentTrain: epoch  5, batch     9 | loss: 6.7038956Losses:  7.026002883911133 1.8934963941574097
CurrentTrain: epoch  5, batch    10 | loss: 7.0260029Losses:  7.082347393035889 1.929416298866272
CurrentTrain: epoch  5, batch    11 | loss: 7.0823474Losses:  6.893099784851074 1.7712185382843018
CurrentTrain: epoch  5, batch    12 | loss: 6.8930998Losses:  6.846226692199707 1.7125003337860107
CurrentTrain: epoch  5, batch    13 | loss: 6.8462267Losses:  7.141303062438965 2.015915632247925
CurrentTrain: epoch  5, batch    14 | loss: 7.1413031Losses:  6.903114318847656 1.7587687969207764
CurrentTrain: epoch  5, batch    15 | loss: 6.9031143Losses:  7.103353500366211 1.9665330648422241
CurrentTrain: epoch  5, batch    16 | loss: 7.1033535Losses:  7.0201802253723145 1.8950210809707642
CurrentTrain: epoch  5, batch    17 | loss: 7.0201802Losses:  6.996828079223633 1.8402422666549683
CurrentTrain: epoch  5, batch    18 | loss: 6.9968281Losses:  6.924266338348389 1.7772741317749023
CurrentTrain: epoch  5, batch    19 | loss: 6.9242663Losses:  6.918428897857666 1.7832914590835571
CurrentTrain: epoch  5, batch    20 | loss: 6.9184289Losses:  6.853477954864502 1.7296233177185059
CurrentTrain: epoch  5, batch    21 | loss: 6.8534780Losses:  6.784942626953125 1.6588919162750244
CurrentTrain: epoch  5, batch    22 | loss: 6.7849426Losses:  6.878782272338867 1.7422363758087158
CurrentTrain: epoch  5, batch    23 | loss: 6.8787823Losses:  6.78104305267334 1.658352017402649
CurrentTrain: epoch  5, batch    24 | loss: 6.7810431Losses:  7.006236553192139 1.8649272918701172
CurrentTrain: epoch  5, batch    25 | loss: 7.0062366Losses:  6.974437713623047 1.8363311290740967
CurrentTrain: epoch  5, batch    26 | loss: 6.9744377Losses:  6.855135440826416 1.722485065460205
CurrentTrain: epoch  5, batch    27 | loss: 6.8551354Losses:  7.056121349334717 1.9239946603775024
CurrentTrain: epoch  5, batch    28 | loss: 7.0561213Losses:  6.999796390533447 1.867026925086975
CurrentTrain: epoch  5, batch    29 | loss: 6.9997964Losses:  6.748070240020752 1.6177462339401245
CurrentTrain: epoch  5, batch    30 | loss: 6.7480702Losses:  6.820433616638184 1.6967345476150513
CurrentTrain: epoch  5, batch    31 | loss: 6.8204336Losses:  6.856425762176514 1.7114238739013672
CurrentTrain: epoch  5, batch    32 | loss: 6.8564258Losses:  6.574090957641602 1.4771974086761475
CurrentTrain: epoch  5, batch    33 | loss: 6.5740910Losses:  6.86305570602417 1.7387133836746216
CurrentTrain: epoch  5, batch    34 | loss: 6.8630557Losses:  6.854124546051025 1.624387264251709
CurrentTrain: epoch  5, batch    35 | loss: 6.8541245Losses:  6.747445583343506 1.6209235191345215
CurrentTrain: epoch  5, batch    36 | loss: 6.7474456Losses:  6.821667671203613 1.701522707939148
CurrentTrain: epoch  5, batch    37 | loss: 6.8216677Losses:  6.843190670013428 1.7102152109146118
CurrentTrain: epoch  5, batch    38 | loss: 6.8431907Losses:  6.970377445220947 1.8475823402404785
CurrentTrain: epoch  5, batch    39 | loss: 6.9703774Losses:  7.09504508972168 1.9773261547088623
CurrentTrain: epoch  5, batch    40 | loss: 7.0950451Losses:  6.835277557373047 1.7140898704528809
CurrentTrain: epoch  5, batch    41 | loss: 6.8352776Losses:  7.016685485839844 1.8711826801300049
CurrentTrain: epoch  5, batch    42 | loss: 7.0166855Losses:  6.854447364807129 1.7060811519622803
CurrentTrain: epoch  5, batch    43 | loss: 6.8544474Losses:  6.879842281341553 1.7680745124816895
CurrentTrain: epoch  5, batch    44 | loss: 6.8798423Losses:  6.860105037689209 1.7498594522476196
CurrentTrain: epoch  5, batch    45 | loss: 6.8601050Losses:  6.932247638702393 1.8169870376586914
CurrentTrain: epoch  5, batch    46 | loss: 6.9322476Losses:  6.943166255950928 1.8297882080078125
CurrentTrain: epoch  5, batch    47 | loss: 6.9431663Losses:  6.972032070159912 1.863685131072998
CurrentTrain: epoch  5, batch    48 | loss: 6.9720321Losses:  6.956110954284668 1.841017723083496
CurrentTrain: epoch  5, batch    49 | loss: 6.9561110Losses:  6.9304633140563965 1.8232289552688599
CurrentTrain: epoch  5, batch    50 | loss: 6.9304633Losses:  6.744353771209717 1.6386985778808594
CurrentTrain: epoch  5, batch    51 | loss: 6.7443538Losses:  7.065443515777588 1.9461932182312012
CurrentTrain: epoch  5, batch    52 | loss: 7.0654435Losses:  6.885249137878418 1.7720391750335693
CurrentTrain: epoch  5, batch    53 | loss: 6.8852491Losses:  7.075340747833252 1.94900381565094
CurrentTrain: epoch  5, batch    54 | loss: 7.0753407Losses:  6.938950538635254 1.8314814567565918
CurrentTrain: epoch  5, batch    55 | loss: 6.9389505Losses:  7.053816795349121 1.9518184661865234
CurrentTrain: epoch  5, batch    56 | loss: 7.0538168Losses:  6.878118515014648 1.6520445346832275
CurrentTrain: epoch  5, batch    57 | loss: 6.8781185Losses:  6.792881965637207 1.6899011135101318
CurrentTrain: epoch  5, batch    58 | loss: 6.7928820Losses:  6.749096870422363 1.6293714046478271
CurrentTrain: epoch  5, batch    59 | loss: 6.7490969Losses:  6.837874889373779 1.7297320365905762
CurrentTrain: epoch  5, batch    60 | loss: 6.8378749Losses:  6.776322364807129 1.6705673933029175
CurrentTrain: epoch  5, batch    61 | loss: 6.7763224Losses:  6.6780686378479 1.538360595703125
CurrentTrain: epoch  5, batch    62 | loss: 6.6780686Losses:  6.846297264099121 1.7279927730560303
CurrentTrain: epoch  6, batch     0 | loss: 6.8462973Losses:  6.942788124084473 1.8311747312545776
CurrentTrain: epoch  6, batch     1 | loss: 6.9427881Losses:  6.784644603729248 1.6882801055908203
CurrentTrain: epoch  6, batch     2 | loss: 6.7846446Losses:  7.239535331726074 2.0184125900268555
CurrentTrain: epoch  6, batch     3 | loss: 7.2395353Losses:  6.660095691680908 1.5554423332214355
CurrentTrain: epoch  6, batch     4 | loss: 6.6600957Losses:  6.986191749572754 1.8726117610931396
CurrentTrain: epoch  6, batch     5 | loss: 6.9861917Losses:  6.989383697509766 1.8673917055130005
CurrentTrain: epoch  6, batch     6 | loss: 6.9893837Losses:  6.841540336608887 1.7288024425506592
CurrentTrain: epoch  6, batch     7 | loss: 6.8415403Losses:  6.9846601486206055 1.8773497343063354
CurrentTrain: epoch  6, batch     8 | loss: 6.9846601Losses:  6.617050647735596 1.5053682327270508
CurrentTrain: epoch  6, batch     9 | loss: 6.6170506Losses:  6.951582908630371 1.850679636001587
CurrentTrain: epoch  6, batch    10 | loss: 6.9515829Losses:  6.675997734069824 1.5473308563232422
CurrentTrain: epoch  6, batch    11 | loss: 6.6759977Losses:  7.07977819442749 1.959394931793213
CurrentTrain: epoch  6, batch    12 | loss: 7.0797782Losses:  6.759843826293945 1.6445971727371216
CurrentTrain: epoch  6, batch    13 | loss: 6.7598438Losses:  7.033230781555176 1.923821210861206
CurrentTrain: epoch  6, batch    14 | loss: 7.0332308Losses:  6.9401068687438965 1.8431715965270996
CurrentTrain: epoch  6, batch    15 | loss: 6.9401069Losses:  6.985215187072754 1.880746603012085
CurrentTrain: epoch  6, batch    16 | loss: 6.9852152Losses:  6.973355293273926 1.863761067390442
CurrentTrain: epoch  6, batch    17 | loss: 6.9733553Losses:  6.384911060333252 1.2637590169906616
CurrentTrain: epoch  6, batch    18 | loss: 6.3849111Losses:  6.967601299285889 1.8632303476333618
CurrentTrain: epoch  6, batch    19 | loss: 6.9676013Losses:  7.0189104080200195 1.9101725816726685
CurrentTrain: epoch  6, batch    20 | loss: 7.0189104Losses:  6.882070541381836 1.7579195499420166
CurrentTrain: epoch  6, batch    21 | loss: 6.8820705Losses:  6.737098693847656 1.6173334121704102
CurrentTrain: epoch  6, batch    22 | loss: 6.7370987Losses:  6.556636810302734 1.4413166046142578
CurrentTrain: epoch  6, batch    23 | loss: 6.5566368Losses:  6.782131195068359 1.666689157485962
CurrentTrain: epoch  6, batch    24 | loss: 6.7821312Losses:  6.9972639083862305 1.8729617595672607
CurrentTrain: epoch  6, batch    25 | loss: 6.9972639Losses:  6.974991321563721 1.8632383346557617
CurrentTrain: epoch  6, batch    26 | loss: 6.9749913Losses:  6.773808002471924 1.6705107688903809
CurrentTrain: epoch  6, batch    27 | loss: 6.7738080Losses:  6.914568901062012 1.8157451152801514
CurrentTrain: epoch  6, batch    28 | loss: 6.9145689Losses:  6.946632385253906 1.8448961973190308
CurrentTrain: epoch  6, batch    29 | loss: 6.9466324Losses:  6.959247589111328 1.8524086475372314
CurrentTrain: epoch  6, batch    30 | loss: 6.9592476Losses:  6.558459281921387 1.4713945388793945
CurrentTrain: epoch  6, batch    31 | loss: 6.5584593Losses:  6.774715423583984 1.6683056354522705
CurrentTrain: epoch  6, batch    32 | loss: 6.7747154Losses:  6.9676618576049805 1.8556557893753052
CurrentTrain: epoch  6, batch    33 | loss: 6.9676619Losses:  6.80856990814209 1.6985888481140137
CurrentTrain: epoch  6, batch    34 | loss: 6.8085699Losses:  6.626723766326904 1.5043292045593262
CurrentTrain: epoch  6, batch    35 | loss: 6.6267238Losses:  6.472381114959717 1.362410068511963
CurrentTrain: epoch  6, batch    36 | loss: 6.4723811Losses:  6.97786283493042 1.8751827478408813
CurrentTrain: epoch  6, batch    37 | loss: 6.9778628Losses:  7.067514896392822 1.9436888694763184
CurrentTrain: epoch  6, batch    38 | loss: 7.0675149Losses:  6.742495536804199 1.6514701843261719
CurrentTrain: epoch  6, batch    39 | loss: 6.7424955Losses:  7.013507843017578 1.9075655937194824
CurrentTrain: epoch  6, batch    40 | loss: 7.0135078Losses:  7.076967239379883 1.9709844589233398
CurrentTrain: epoch  6, batch    41 | loss: 7.0769672Losses:  6.9859466552734375 1.884818196296692
CurrentTrain: epoch  6, batch    42 | loss: 6.9859467Losses:  6.740818500518799 1.6513768434524536
CurrentTrain: epoch  6, batch    43 | loss: 6.7408185Losses:  6.810848236083984 1.7144193649291992
CurrentTrain: epoch  6, batch    44 | loss: 6.8108482Losses:  6.936495780944824 1.827952265739441
CurrentTrain: epoch  6, batch    45 | loss: 6.9364958Losses:  6.769376754760742 1.6727980375289917
CurrentTrain: epoch  6, batch    46 | loss: 6.7693768Losses:  6.730428695678711 1.629812240600586
CurrentTrain: epoch  6, batch    47 | loss: 6.7304287Losses:  7.077617645263672 1.976318359375
CurrentTrain: epoch  6, batch    48 | loss: 7.0776176Losses:  6.655587673187256 1.5613867044448853
CurrentTrain: epoch  6, batch    49 | loss: 6.6555877Losses:  7.035449028015137 1.9364371299743652
CurrentTrain: epoch  6, batch    50 | loss: 7.0354490Losses:  6.9872941970825195 1.8914986848831177
CurrentTrain: epoch  6, batch    51 | loss: 6.9872942Losses:  6.968451499938965 1.8655744791030884
CurrentTrain: epoch  6, batch    52 | loss: 6.9684515Losses:  6.39105749130249 1.289656162261963
CurrentTrain: epoch  6, batch    53 | loss: 6.3910575Losses:  6.8917436599731445 1.799926996231079
CurrentTrain: epoch  6, batch    54 | loss: 6.8917437Losses:  6.736302375793457 1.6434953212738037
CurrentTrain: epoch  6, batch    55 | loss: 6.7363024Losses:  6.608110427856445 1.4955778121948242
CurrentTrain: epoch  6, batch    56 | loss: 6.6081104Losses:  6.9850921630859375 1.885148048400879
CurrentTrain: epoch  6, batch    57 | loss: 6.9850922Losses:  6.8445725440979 1.7566243410110474
CurrentTrain: epoch  6, batch    58 | loss: 6.8445725Losses:  6.927669048309326 1.83319890499115
CurrentTrain: epoch  6, batch    59 | loss: 6.9276690Losses:  6.5869855880737305 1.4784080982208252
CurrentTrain: epoch  6, batch    60 | loss: 6.5869856Losses:  6.824712753295898 1.7204363346099854
CurrentTrain: epoch  6, batch    61 | loss: 6.8247128Losses:  6.53187370300293 1.4394252300262451
CurrentTrain: epoch  6, batch    62 | loss: 6.5318737Losses:  6.7520294189453125 1.6463968753814697
CurrentTrain: epoch  7, batch     0 | loss: 6.7520294Losses:  6.705802917480469 1.596486210823059
CurrentTrain: epoch  7, batch     1 | loss: 6.7058029Losses:  6.811933994293213 1.7239465713500977
CurrentTrain: epoch  7, batch     2 | loss: 6.8119340Losses:  7.00532341003418 1.900575876235962
CurrentTrain: epoch  7, batch     3 | loss: 7.0053234Losses:  6.66098690032959 1.5613048076629639
CurrentTrain: epoch  7, batch     4 | loss: 6.6609869Losses:  6.931127071380615 1.8412107229232788
CurrentTrain: epoch  7, batch     5 | loss: 6.9311271Losses:  6.940367698669434 1.8380941152572632
CurrentTrain: epoch  7, batch     6 | loss: 6.9403677Losses:  6.979442119598389 1.8695778846740723
CurrentTrain: epoch  7, batch     7 | loss: 6.9794421Losses:  6.673214435577393 1.5923824310302734
CurrentTrain: epoch  7, batch     8 | loss: 6.6732144Losses:  6.74969482421875 1.6593074798583984
CurrentTrain: epoch  7, batch     9 | loss: 6.7496948Losses:  6.831340789794922 1.743296504020691
CurrentTrain: epoch  7, batch    10 | loss: 6.8313408Losses:  6.738918304443359 1.6602790355682373
CurrentTrain: epoch  7, batch    11 | loss: 6.7389183Losses:  6.563094139099121 1.4895098209381104
CurrentTrain: epoch  7, batch    12 | loss: 6.5630941Losses:  6.754098415374756 1.6574015617370605
CurrentTrain: epoch  7, batch    13 | loss: 6.7540984Losses:  6.729127883911133 1.6499273777008057
CurrentTrain: epoch  7, batch    14 | loss: 6.7291279Losses:  6.793109893798828 1.7225558757781982
CurrentTrain: epoch  7, batch    15 | loss: 6.7931099Losses:  6.790828227996826 1.6926121711730957
CurrentTrain: epoch  7, batch    16 | loss: 6.7908282Losses:  6.79839563369751 1.71060311794281
CurrentTrain: epoch  7, batch    17 | loss: 6.7983956Losses:  6.986313343048096 1.894707202911377
CurrentTrain: epoch  7, batch    18 | loss: 6.9863133Losses:  6.949946403503418 1.843422770500183
CurrentTrain: epoch  7, batch    19 | loss: 6.9499464Losses:  6.750027656555176 1.6358774900436401
CurrentTrain: epoch  7, batch    20 | loss: 6.7500277Losses:  6.862774848937988 1.666344165802002
CurrentTrain: epoch  7, batch    21 | loss: 6.8627748Losses:  6.654656410217285 1.5575339794158936
CurrentTrain: epoch  7, batch    22 | loss: 6.6546564Losses:  6.594354629516602 1.4969693422317505
CurrentTrain: epoch  7, batch    23 | loss: 6.5943546Losses:  6.801883220672607 1.702959656715393
CurrentTrain: epoch  7, batch    24 | loss: 6.8018832Losses:  6.606447219848633 1.4974344968795776
CurrentTrain: epoch  7, batch    25 | loss: 6.6064472Losses:  6.754894256591797 1.6632561683654785
CurrentTrain: epoch  7, batch    26 | loss: 6.7548943Losses:  6.910681247711182 1.8143491744995117
CurrentTrain: epoch  7, batch    27 | loss: 6.9106812Losses:  6.812749862670898 1.721954584121704
CurrentTrain: epoch  7, batch    28 | loss: 6.8127499Losses:  6.734381675720215 1.6362836360931396
CurrentTrain: epoch  7, batch    29 | loss: 6.7343817Losses:  6.876575469970703 1.776306390762329
CurrentTrain: epoch  7, batch    30 | loss: 6.8765755Losses:  6.834170341491699 1.736572504043579
CurrentTrain: epoch  7, batch    31 | loss: 6.8341703Losses:  6.593282699584961 1.496751308441162
CurrentTrain: epoch  7, batch    32 | loss: 6.5932827Losses:  6.737371444702148 1.6273376941680908
CurrentTrain: epoch  7, batch    33 | loss: 6.7373714Losses:  6.6327409744262695 1.527531385421753
CurrentTrain: epoch  7, batch    34 | loss: 6.6327410Losses:  6.878827095031738 1.7888634204864502
CurrentTrain: epoch  7, batch    35 | loss: 6.8788271Losses:  6.808073997497559 1.7025506496429443
CurrentTrain: epoch  7, batch    36 | loss: 6.8080740Losses:  6.717504978179932 1.6357358694076538
CurrentTrain: epoch  7, batch    37 | loss: 6.7175050Losses:  6.971369743347168 1.877506971359253
CurrentTrain: epoch  7, batch    38 | loss: 6.9713697Losses:  6.734187602996826 1.6510533094406128
CurrentTrain: epoch  7, batch    39 | loss: 6.7341876Losses:  6.917873859405518 1.8294559717178345
CurrentTrain: epoch  7, batch    40 | loss: 6.9178739Losses:  6.691743850708008 1.611165165901184
CurrentTrain: epoch  7, batch    41 | loss: 6.6917439Losses:  6.798861503601074 1.7043020725250244
CurrentTrain: epoch  7, batch    42 | loss: 6.7988615Losses:  6.77017879486084 1.6817371845245361
CurrentTrain: epoch  7, batch    43 | loss: 6.7701788Losses:  6.868685722351074 1.7850728034973145
CurrentTrain: epoch  7, batch    44 | loss: 6.8686857Losses:  6.9211602210998535 1.8416577577590942
CurrentTrain: epoch  7, batch    45 | loss: 6.9211602Losses:  6.730363845825195 1.6414319276809692
CurrentTrain: epoch  7, batch    46 | loss: 6.7303638Losses:  6.864887714385986 1.7779163122177124
CurrentTrain: epoch  7, batch    47 | loss: 6.8648877Losses:  6.869060516357422 1.7831907272338867
CurrentTrain: epoch  7, batch    48 | loss: 6.8690605Losses:  6.902139186859131 1.8139595985412598
CurrentTrain: epoch  7, batch    49 | loss: 6.9021392Losses:  6.644639015197754 1.567713737487793
CurrentTrain: epoch  7, batch    50 | loss: 6.6446390Losses:  6.935698509216309 1.8478591442108154
CurrentTrain: epoch  7, batch    51 | loss: 6.9356985Losses:  6.562516212463379 1.460320234298706
CurrentTrain: epoch  7, batch    52 | loss: 6.5625162Losses:  6.862803936004639 1.7649654150009155
CurrentTrain: epoch  7, batch    53 | loss: 6.8628039Losses:  6.7632904052734375 1.6662118434906006
CurrentTrain: epoch  7, batch    54 | loss: 6.7632904Losses:  6.699675559997559 1.6088659763336182
CurrentTrain: epoch  7, batch    55 | loss: 6.6996756Losses:  6.568476676940918 1.4838823080062866
CurrentTrain: epoch  7, batch    56 | loss: 6.5684767Losses:  6.927035808563232 1.8459882736206055
CurrentTrain: epoch  7, batch    57 | loss: 6.9270358Losses:  6.938282012939453 1.8614709377288818
CurrentTrain: epoch  7, batch    58 | loss: 6.9382820Losses:  6.589297771453857 1.500706672668457
CurrentTrain: epoch  7, batch    59 | loss: 6.5892978Losses:  6.9680962562561035 1.8722400665283203
CurrentTrain: epoch  7, batch    60 | loss: 6.9680963Losses:  6.810059547424316 1.7214243412017822
CurrentTrain: epoch  7, batch    61 | loss: 6.8100595Losses:  6.588416576385498 1.4922876358032227
CurrentTrain: epoch  7, batch    62 | loss: 6.5884166Losses:  7.014613151550293 1.9249612092971802
CurrentTrain: epoch  8, batch     0 | loss: 7.0146132Losses:  6.851893424987793 1.7593424320220947
CurrentTrain: epoch  8, batch     1 | loss: 6.8518934Losses:  6.827856063842773 1.741184949874878
CurrentTrain: epoch  8, batch     2 | loss: 6.8278561Losses:  6.679055213928223 1.5913046598434448
CurrentTrain: epoch  8, batch     3 | loss: 6.6790552Losses:  6.697751045227051 1.6209441423416138
CurrentTrain: epoch  8, batch     4 | loss: 6.6977510Losses:  6.827940464019775 1.7477226257324219
CurrentTrain: epoch  8, batch     5 | loss: 6.8279405Losses:  6.905205726623535 1.8052129745483398
CurrentTrain: epoch  8, batch     6 | loss: 6.9052057Losses:  6.894410133361816 1.804198980331421
CurrentTrain: epoch  8, batch     7 | loss: 6.8944101Losses:  6.865540504455566 1.780388355255127
CurrentTrain: epoch  8, batch     8 | loss: 6.8655405Losses:  6.912020206451416 1.8254510164260864
CurrentTrain: epoch  8, batch     9 | loss: 6.9120202Losses:  6.837431907653809 1.7482712268829346
CurrentTrain: epoch  8, batch    10 | loss: 6.8374319Losses:  6.77839469909668 1.6982405185699463
CurrentTrain: epoch  8, batch    11 | loss: 6.7783947Losses:  6.845273494720459 1.7614588737487793
CurrentTrain: epoch  8, batch    12 | loss: 6.8452735Losses:  6.791260719299316 1.6989824771881104
CurrentTrain: epoch  8, batch    13 | loss: 6.7912607Losses:  6.765356063842773 1.6893318891525269
CurrentTrain: epoch  8, batch    14 | loss: 6.7653561Losses:  6.687564373016357 1.6083216667175293
CurrentTrain: epoch  8, batch    15 | loss: 6.6875644Losses:  6.80608606338501 1.7242003679275513
CurrentTrain: epoch  8, batch    16 | loss: 6.8060861Losses:  6.915821075439453 1.8294203281402588
CurrentTrain: epoch  8, batch    17 | loss: 6.9158211Losses:  6.861540794372559 1.7679797410964966
CurrentTrain: epoch  8, batch    18 | loss: 6.8615408Losses:  6.886192321777344 1.7948644161224365
CurrentTrain: epoch  8, batch    19 | loss: 6.8861923Losses:  6.707286357879639 1.6387100219726562
CurrentTrain: epoch  8, batch    20 | loss: 6.7072864Losses:  6.824762344360352 1.7402559518814087
CurrentTrain: epoch  8, batch    21 | loss: 6.8247623Losses:  6.53978157043457 1.4588031768798828
CurrentTrain: epoch  8, batch    22 | loss: 6.5397816Losses:  7.040105819702148 1.9559803009033203
CurrentTrain: epoch  8, batch    23 | loss: 7.0401058Losses:  7.064411163330078 1.9795682430267334
CurrentTrain: epoch  8, batch    24 | loss: 7.0644112Losses:  6.619750022888184 1.5325607061386108
CurrentTrain: epoch  8, batch    25 | loss: 6.6197500Losses:  6.691281795501709 1.6040910482406616
CurrentTrain: epoch  8, batch    26 | loss: 6.6912818Losses:  6.796924591064453 1.704493761062622
CurrentTrain: epoch  8, batch    27 | loss: 6.7969246Losses:  7.131200790405273 1.9502604007720947
CurrentTrain: epoch  8, batch    28 | loss: 7.1312008Losses:  6.864826202392578 1.7820751667022705
CurrentTrain: epoch  8, batch    29 | loss: 6.8648262Losses:  6.759731292724609 1.682685136795044
CurrentTrain: epoch  8, batch    30 | loss: 6.7597313Losses:  6.6912312507629395 1.6112446784973145
CurrentTrain: epoch  8, batch    31 | loss: 6.6912313Losses:  6.606265068054199 1.5227363109588623
CurrentTrain: epoch  8, batch    32 | loss: 6.6062651Losses:  6.78228759765625 1.6957893371582031
CurrentTrain: epoch  8, batch    33 | loss: 6.7822876Losses:  7.061685562133789 1.9804599285125732
CurrentTrain: epoch  8, batch    34 | loss: 7.0616856Losses:  6.780769348144531 1.7058240175247192
CurrentTrain: epoch  8, batch    35 | loss: 6.7807693Losses:  6.726115703582764 1.6450810432434082
CurrentTrain: epoch  8, batch    36 | loss: 6.7261157Losses:  6.7564473152160645 1.6673550605773926
CurrentTrain: epoch  8, batch    37 | loss: 6.7564473Losses:  6.67371129989624 1.5998255014419556
CurrentTrain: epoch  8, batch    38 | loss: 6.6737113Losses:  6.83681583404541 1.7496483325958252
CurrentTrain: epoch  8, batch    39 | loss: 6.8368158Losses:  6.936254978179932 1.8484305143356323
CurrentTrain: epoch  8, batch    40 | loss: 6.9362550Losses:  6.966861724853516 1.8817791938781738
CurrentTrain: epoch  8, batch    41 | loss: 6.9668617Losses:  6.889055252075195 1.8008965253829956
CurrentTrain: epoch  8, batch    42 | loss: 6.8890553Losses:  6.894699573516846 1.8094085454940796
CurrentTrain: epoch  8, batch    43 | loss: 6.8946996Losses:  6.9228997230529785 1.8341822624206543
CurrentTrain: epoch  8, batch    44 | loss: 6.9228997Losses:  6.6801557540893555 1.582076072692871
CurrentTrain: epoch  8, batch    45 | loss: 6.6801558Losses:  6.644089698791504 1.5635621547698975
CurrentTrain: epoch  8, batch    46 | loss: 6.6440897Losses:  6.514113426208496 1.4343690872192383
CurrentTrain: epoch  8, batch    47 | loss: 6.5141134Losses:  6.638844966888428 1.5451363325119019
CurrentTrain: epoch  8, batch    48 | loss: 6.6388450Losses:  6.724942207336426 1.6366018056869507
CurrentTrain: epoch  8, batch    49 | loss: 6.7249422Losses:  6.832589149475098 1.7481844425201416
CurrentTrain: epoch  8, batch    50 | loss: 6.8325891Losses:  6.496072292327881 1.4130654335021973
CurrentTrain: epoch  8, batch    51 | loss: 6.4960723Losses:  6.9097900390625 1.8202427625656128
CurrentTrain: epoch  8, batch    52 | loss: 6.9097900Losses:  6.826393127441406 1.732651710510254
CurrentTrain: epoch  8, batch    53 | loss: 6.8263931Losses:  6.942870140075684 1.8672184944152832
CurrentTrain: epoch  8, batch    54 | loss: 6.9428701Losses:  6.652308464050293 1.5735816955566406
CurrentTrain: epoch  8, batch    55 | loss: 6.6523085Losses:  6.748554229736328 1.668560266494751
CurrentTrain: epoch  8, batch    56 | loss: 6.7485542Losses:  7.032968044281006 1.950939655303955
CurrentTrain: epoch  8, batch    57 | loss: 7.0329680Losses:  6.763509750366211 1.674992322921753
CurrentTrain: epoch  8, batch    58 | loss: 6.7635098Losses:  6.85735559463501 1.7724248170852661
CurrentTrain: epoch  8, batch    59 | loss: 6.8573556Losses:  6.70046854019165 1.6349300146102905
CurrentTrain: epoch  8, batch    60 | loss: 6.7004685Losses:  6.981956958770752 1.8994035720825195
CurrentTrain: epoch  8, batch    61 | loss: 6.9819570Losses:  6.272678375244141 1.1875903606414795
CurrentTrain: epoch  8, batch    62 | loss: 6.2726784Losses:  6.784987449645996 1.7069318294525146
CurrentTrain: epoch  9, batch     0 | loss: 6.7849874Losses:  6.686384677886963 1.5973830223083496
CurrentTrain: epoch  9, batch     1 | loss: 6.6863847Losses:  6.9410295486450195 1.8529560565948486
CurrentTrain: epoch  9, batch     2 | loss: 6.9410295Losses:  6.766135215759277 1.6725828647613525
CurrentTrain: epoch  9, batch     3 | loss: 6.7661352Losses:  6.604197978973389 1.523092269897461
CurrentTrain: epoch  9, batch     4 | loss: 6.6041980Losses:  6.793423175811768 1.7080392837524414
CurrentTrain: epoch  9, batch     5 | loss: 6.7934232Losses:  6.73805046081543 1.6602270603179932
CurrentTrain: epoch  9, batch     6 | loss: 6.7380505Losses:  6.7903900146484375 1.7138968706130981
CurrentTrain: epoch  9, batch     7 | loss: 6.7903900Losses:  6.610116004943848 1.5373084545135498
CurrentTrain: epoch  9, batch     8 | loss: 6.6101160Losses:  6.826250076293945 1.7483291625976562
CurrentTrain: epoch  9, batch     9 | loss: 6.8262501Losses:  6.949840068817139 1.8707677125930786
CurrentTrain: epoch  9, batch    10 | loss: 6.9498401Losses:  6.941037654876709 1.8516631126403809
CurrentTrain: epoch  9, batch    11 | loss: 6.9410377Losses:  6.899261474609375 1.8145430088043213
CurrentTrain: epoch  9, batch    12 | loss: 6.8992615Losses:  6.870947360992432 1.7945013046264648
CurrentTrain: epoch  9, batch    13 | loss: 6.8709474Losses:  7.054722785949707 1.9687676429748535
CurrentTrain: epoch  9, batch    14 | loss: 7.0547228Losses:  7.010176658630371 1.9263896942138672
CurrentTrain: epoch  9, batch    15 | loss: 7.0101767Losses:  6.867786884307861 1.7839980125427246
CurrentTrain: epoch  9, batch    16 | loss: 6.8677869Losses:  6.885584354400635 1.795844554901123
CurrentTrain: epoch  9, batch    17 | loss: 6.8855844Losses:  6.838312149047852 1.758495807647705
CurrentTrain: epoch  9, batch    18 | loss: 6.8383121Losses:  6.554975509643555 1.4737229347229004
CurrentTrain: epoch  9, batch    19 | loss: 6.5549755Losses:  6.832588195800781 1.7428877353668213
CurrentTrain: epoch  9, batch    20 | loss: 6.8325882Losses:  6.815143585205078 1.7410564422607422
CurrentTrain: epoch  9, batch    21 | loss: 6.8151436Losses:  6.864720344543457 1.7752807140350342
CurrentTrain: epoch  9, batch    22 | loss: 6.8647203Losses:  6.827605247497559 1.7451125383377075
CurrentTrain: epoch  9, batch    23 | loss: 6.8276052Losses:  6.741927623748779 1.6619105339050293
CurrentTrain: epoch  9, batch    24 | loss: 6.7419276Losses:  6.721601963043213 1.636940598487854
CurrentTrain: epoch  9, batch    25 | loss: 6.7216020Losses:  6.819164276123047 1.7459166049957275
CurrentTrain: epoch  9, batch    26 | loss: 6.8191643Losses:  6.8263044357299805 1.7493922710418701
CurrentTrain: epoch  9, batch    27 | loss: 6.8263044Losses:  7.018749237060547 1.938685417175293
CurrentTrain: epoch  9, batch    28 | loss: 7.0187492Losses:  6.726586818695068 1.650688648223877
CurrentTrain: epoch  9, batch    29 | loss: 6.7265868Losses:  6.816889762878418 1.735407829284668
CurrentTrain: epoch  9, batch    30 | loss: 6.8168898Losses:  6.840600967407227 1.770470380783081
CurrentTrain: epoch  9, batch    31 | loss: 6.8406010Losses:  6.711752891540527 1.6441266536712646
CurrentTrain: epoch  9, batch    32 | loss: 6.7117529Losses:  6.561130523681641 1.4858787059783936
CurrentTrain: epoch  9, batch    33 | loss: 6.5611305Losses:  6.982151508331299 1.907275676727295
CurrentTrain: epoch  9, batch    34 | loss: 6.9821515Losses:  7.025232315063477 1.9433457851409912
CurrentTrain: epoch  9, batch    35 | loss: 7.0252323Losses:  6.540482997894287 1.4620755910873413
CurrentTrain: epoch  9, batch    36 | loss: 6.5404830Losses:  6.937621116638184 1.8543730974197388
CurrentTrain: epoch  9, batch    37 | loss: 6.9376211Losses:  6.8154191970825195 1.7244371175765991
CurrentTrain: epoch  9, batch    38 | loss: 6.8154192Losses:  6.880952835083008 1.801191806793213
CurrentTrain: epoch  9, batch    39 | loss: 6.8809528Losses:  6.7013983726501465 1.5962510108947754
CurrentTrain: epoch  9, batch    40 | loss: 6.7013984Losses:  6.640072822570801 1.5646848678588867
CurrentTrain: epoch  9, batch    41 | loss: 6.6400728Losses:  6.765371322631836 1.676911473274231
CurrentTrain: epoch  9, batch    42 | loss: 6.7653713Losses:  6.673911094665527 1.5995776653289795
CurrentTrain: epoch  9, batch    43 | loss: 6.6739111Losses:  6.722084045410156 1.6403281688690186
CurrentTrain: epoch  9, batch    44 | loss: 6.7220840Losses:  6.677594184875488 1.606399655342102
CurrentTrain: epoch  9, batch    45 | loss: 6.6775942Losses:  6.672163009643555 1.6051642894744873
CurrentTrain: epoch  9, batch    46 | loss: 6.6721630Losses:  6.5230021476745605 1.4387264251708984
CurrentTrain: epoch  9, batch    47 | loss: 6.5230021Losses:  6.912128448486328 1.8377667665481567
CurrentTrain: epoch  9, batch    48 | loss: 6.9121284Losses:  6.804942607879639 1.7099641561508179
CurrentTrain: epoch  9, batch    49 | loss: 6.8049426Losses:  6.932077407836914 1.8555495738983154
CurrentTrain: epoch  9, batch    50 | loss: 6.9320774Losses:  6.947514533996582 1.8617780208587646
CurrentTrain: epoch  9, batch    51 | loss: 6.9475145Losses:  6.830267429351807 1.7506146430969238
CurrentTrain: epoch  9, batch    52 | loss: 6.8302674Losses:  6.807254791259766 1.7401243448257446
CurrentTrain: epoch  9, batch    53 | loss: 6.8072548Losses:  6.769824028015137 1.692840337753296
CurrentTrain: epoch  9, batch    54 | loss: 6.7698240Losses:  6.7676920890808105 1.6868280172348022
CurrentTrain: epoch  9, batch    55 | loss: 6.7676921Losses:  6.921292304992676 1.854616641998291
CurrentTrain: epoch  9, batch    56 | loss: 6.9212923Losses:  6.941558361053467 1.8247160911560059
CurrentTrain: epoch  9, batch    57 | loss: 6.9415584Losses:  6.789556980133057 1.7201656103134155
CurrentTrain: epoch  9, batch    58 | loss: 6.7895570Losses:  6.867732524871826 1.7949694395065308
CurrentTrain: epoch  9, batch    59 | loss: 6.8677325Losses:  6.847098350524902 1.7675963640213013
CurrentTrain: epoch  9, batch    60 | loss: 6.8470984Losses:  6.86376953125 1.7873852252960205
CurrentTrain: epoch  9, batch    61 | loss: 6.8637695Losses:  6.552504062652588 1.4732352495193481
CurrentTrain: epoch  9, batch    62 | loss: 6.5525041
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 91.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 91.92%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.47%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 91.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 91.92%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.47%   
cur_acc:  ['0.9147']
his_acc:  ['0.9147']
Clustering into  9  clusters
Clusters:  [1 4 2 1 5 7 2 1 0 0 2 0 0 4 6 3 0 4 8 3]
Losses:  5.44035005569458 1.886225938796997
CurrentTrain: epoch  0, batch     0 | loss: 5.4403501Losses:  5.527460098266602 1.9137694835662842
CurrentTrain: epoch  0, batch     1 | loss: 5.5274601Losses:  5.53176736831665 1.9916319847106934
CurrentTrain: epoch  0, batch     2 | loss: 5.5317674Losses:  4.250000953674316 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.2500010Losses:  5.607746124267578 2.041168451309204
CurrentTrain: epoch  1, batch     0 | loss: 5.6077461Losses:  5.636807441711426 2.1696770191192627
CurrentTrain: epoch  1, batch     1 | loss: 5.6368074Losses:  5.612290382385254 2.1128275394439697
CurrentTrain: epoch  1, batch     2 | loss: 5.6122904Losses:  3.607428550720215 0.5701538324356079
CurrentTrain: epoch  1, batch     3 | loss: 3.6074286Losses:  5.147463321685791 1.8581727743148804
CurrentTrain: epoch  2, batch     0 | loss: 5.1474633Losses:  5.330099105834961 1.958770513534546
CurrentTrain: epoch  2, batch     1 | loss: 5.3300991Losses:  5.282737731933594 2.017465114593506
CurrentTrain: epoch  2, batch     2 | loss: 5.2827377Losses:  4.273162841796875 0.6533429622650146
CurrentTrain: epoch  2, batch     3 | loss: 4.2731628Losses:  5.392845153808594 2.1180076599121094
CurrentTrain: epoch  3, batch     0 | loss: 5.3928452Losses:  5.110647201538086 1.9194519519805908
CurrentTrain: epoch  3, batch     1 | loss: 5.1106472Losses:  5.320774078369141 2.0107338428497314
CurrentTrain: epoch  3, batch     2 | loss: 5.3207741Losses:  4.019294738769531 0.6893192529678345
CurrentTrain: epoch  3, batch     3 | loss: 4.0192947Losses:  5.031649589538574 1.883812665939331
CurrentTrain: epoch  4, batch     0 | loss: 5.0316496Losses:  4.953415870666504 1.8285152912139893
CurrentTrain: epoch  4, batch     1 | loss: 4.9534159Losses:  5.176041603088379 1.8758121728897095
CurrentTrain: epoch  4, batch     2 | loss: 5.1760416Losses:  3.6881227493286133 0.6223745346069336
CurrentTrain: epoch  4, batch     3 | loss: 3.6881227Losses:  5.079230308532715 1.8832314014434814
CurrentTrain: epoch  5, batch     0 | loss: 5.0792303Losses:  5.00980806350708 1.888382077217102
CurrentTrain: epoch  5, batch     1 | loss: 5.0098081Losses:  5.203575611114502 2.1440248489379883
CurrentTrain: epoch  5, batch     2 | loss: 5.2035756Losses:  3.788139581680298 0.622257649898529
CurrentTrain: epoch  5, batch     3 | loss: 3.7881396Losses:  5.051686763763428 1.9373739957809448
CurrentTrain: epoch  6, batch     0 | loss: 5.0516868Losses:  4.851065158843994 1.8309693336486816
CurrentTrain: epoch  6, batch     1 | loss: 4.8510652Losses:  4.992198944091797 1.9167604446411133
CurrentTrain: epoch  6, batch     2 | loss: 4.9921989Losses:  3.1902241706848145 0.0
CurrentTrain: epoch  6, batch     3 | loss: 3.1902242Losses:  5.067854881286621 2.0155553817749023
CurrentTrain: epoch  7, batch     0 | loss: 5.0678549Losses:  5.1499128341674805 2.114394426345825
CurrentTrain: epoch  7, batch     1 | loss: 5.1499128Losses:  4.894372940063477 1.8926117420196533
CurrentTrain: epoch  7, batch     2 | loss: 4.8943729Losses:  2.8080925941467285 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.8080926Losses:  5.042679309844971 1.9938873052597046
CurrentTrain: epoch  8, batch     0 | loss: 5.0426793Losses:  4.709986209869385 1.7622790336608887
CurrentTrain: epoch  8, batch     1 | loss: 4.7099862Losses:  4.955014228820801 1.9817581176757812
CurrentTrain: epoch  8, batch     2 | loss: 4.9550142Losses:  3.4612698554992676 0.5935525894165039
CurrentTrain: epoch  8, batch     3 | loss: 3.4612699Losses:  5.001903533935547 1.9734327793121338
CurrentTrain: epoch  9, batch     0 | loss: 5.0019035Losses:  4.760941505432129 1.758780837059021
CurrentTrain: epoch  9, batch     1 | loss: 4.7609415Losses:  4.860327243804932 1.9570540189743042
CurrentTrain: epoch  9, batch     2 | loss: 4.8603272Losses:  3.46925687789917 0.571123480796814
CurrentTrain: epoch  9, batch     3 | loss: 3.4692569
Losses:  4.566384315490723 2.569150686264038
MemoryTrain:  epoch  0, batch     0 | loss: 4.5663843Losses:  3.0924222469329834 1.2362257242202759
MemoryTrain:  epoch  0, batch     1 | loss: 3.0924222Losses:  4.642080307006836 2.5815885066986084
MemoryTrain:  epoch  1, batch     0 | loss: 4.6420803Losses:  3.2185583114624023 1.2300691604614258
MemoryTrain:  epoch  1, batch     1 | loss: 3.2185583Losses:  4.608336448669434 2.568232536315918
MemoryTrain:  epoch  2, batch     0 | loss: 4.6083364Losses:  3.2706315517425537 1.2568438053131104
MemoryTrain:  epoch  2, batch     1 | loss: 3.2706316Losses:  4.591011047363281 2.5784661769866943
MemoryTrain:  epoch  3, batch     0 | loss: 4.5910110Losses:  3.232006311416626 1.245617389678955
MemoryTrain:  epoch  3, batch     1 | loss: 3.2320063Losses:  4.5806403160095215 2.5728206634521484
MemoryTrain:  epoch  4, batch     0 | loss: 4.5806403Losses:  3.156722068786621 1.2014847993850708
MemoryTrain:  epoch  4, batch     1 | loss: 3.1567221Losses:  4.578038692474365 2.579876661300659
MemoryTrain:  epoch  5, batch     0 | loss: 4.5780387Losses:  3.123203992843628 1.1725308895111084
MemoryTrain:  epoch  5, batch     1 | loss: 3.1232040Losses:  4.546466827392578 2.5602409839630127
MemoryTrain:  epoch  6, batch     0 | loss: 4.5464668Losses:  3.1823697090148926 1.2329508066177368
MemoryTrain:  epoch  6, batch     1 | loss: 3.1823697Losses:  4.539319038391113 2.56366229057312
MemoryTrain:  epoch  7, batch     0 | loss: 4.5393190Losses:  3.206281900405884 1.2090494632720947
MemoryTrain:  epoch  7, batch     1 | loss: 3.2062819Losses:  4.526773452758789 2.554309129714966
MemoryTrain:  epoch  8, batch     0 | loss: 4.5267735Losses:  3.266476631164551 1.2556509971618652
MemoryTrain:  epoch  8, batch     1 | loss: 3.2664766Losses:  4.538047790527344 2.5623457431793213
MemoryTrain:  epoch  9, batch     0 | loss: 4.5380478Losses:  3.157711982727051 1.1983263492584229
MemoryTrain:  epoch  9, batch     1 | loss: 3.1577120
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 61.29%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 61.13%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 60.98%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 60.71%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 60.14%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 60.47%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 60.12%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 59.10%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 57.78%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 58.05%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.37%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.68%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 59.04%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 59.10%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.05%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 58.85%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 58.91%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 58.97%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.43%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 90.43%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.83%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.99%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 90.88%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 90.53%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 90.44%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 90.30%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 90.17%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 90.32%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 89.81%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 89.53%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 89.14%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 88.47%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 87.90%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 86.80%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 86.50%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 85.90%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 85.62%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 85.34%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 84.71%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 84.52%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 84.05%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 83.50%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 82.92%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 82.42%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.07%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 81.45%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 80.85%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 80.66%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 80.40%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.15%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 79.85%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 79.25%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 78.82%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.45%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 77.60%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 77.24%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 76.88%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.46%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 76.11%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 76.03%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 75.68%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 75.20%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 74.90%   
cur_acc:  ['0.9147', '0.5843']
his_acc:  ['0.9147', '0.7490']
Clustering into  14  clusters
Clusters:  [11  9 10  2 13  0 10  2  2  2 10  1  1  0 12  6  1  0  8  6  2  6  4  7
  2  9  4  2  3  5]
Losses:  5.567246913909912 1.9481693506240845
CurrentTrain: epoch  0, batch     0 | loss: 5.5672469Losses:  5.561956405639648 2.0185091495513916
CurrentTrain: epoch  0, batch     1 | loss: 5.5619564Losses:  5.838109016418457 2.1325185298919678
CurrentTrain: epoch  0, batch     2 | loss: 5.8381090Losses:  4.491990089416504 0.6522161960601807
CurrentTrain: epoch  0, batch     3 | loss: 4.4919901Losses:  5.696290493011475 2.1636199951171875
CurrentTrain: epoch  1, batch     0 | loss: 5.6962905Losses:  5.695446968078613 1.955557107925415
CurrentTrain: epoch  1, batch     1 | loss: 5.6954470Losses:  5.605661869049072 2.1128339767456055
CurrentTrain: epoch  1, batch     2 | loss: 5.6056619Losses:  4.277910232543945 0.6596677303314209
CurrentTrain: epoch  1, batch     3 | loss: 4.2779102Losses:  5.159655570983887 1.8715403079986572
CurrentTrain: epoch  2, batch     0 | loss: 5.1596556Losses:  5.531192779541016 2.0965607166290283
CurrentTrain: epoch  2, batch     1 | loss: 5.5311928Losses:  5.624391555786133 1.9858590364456177
CurrentTrain: epoch  2, batch     2 | loss: 5.6243916Losses:  3.7541561126708984 0.6418654918670654
CurrentTrain: epoch  2, batch     3 | loss: 3.7541561Losses:  5.438647270202637 2.0866129398345947
CurrentTrain: epoch  3, batch     0 | loss: 5.4386473Losses:  5.161300182342529 1.8777729272842407
CurrentTrain: epoch  3, batch     1 | loss: 5.1613002Losses:  5.4996795654296875 2.037058115005493
CurrentTrain: epoch  3, batch     2 | loss: 5.4996796Losses:  3.925288200378418 0.6387201547622681
CurrentTrain: epoch  3, batch     3 | loss: 3.9252882Losses:  5.369169235229492 1.9648603200912476
CurrentTrain: epoch  4, batch     0 | loss: 5.3691692Losses:  5.473426342010498 2.133857250213623
CurrentTrain: epoch  4, batch     1 | loss: 5.4734263Losses:  4.993836879730225 1.8386998176574707
CurrentTrain: epoch  4, batch     2 | loss: 4.9938369Losses:  3.807007074356079 0.6214378476142883
CurrentTrain: epoch  4, batch     3 | loss: 3.8070071Losses:  5.137660980224609 1.7677948474884033
CurrentTrain: epoch  5, batch     0 | loss: 5.1376610Losses:  5.121803283691406 1.9472332000732422
CurrentTrain: epoch  5, batch     1 | loss: 5.1218033Losses:  4.968544960021973 1.7868657112121582
CurrentTrain: epoch  5, batch     2 | loss: 4.9685450Losses:  4.27487850189209 0.6378074884414673
CurrentTrain: epoch  5, batch     3 | loss: 4.2748785Losses:  5.360119342803955 2.1205296516418457
CurrentTrain: epoch  6, batch     0 | loss: 5.3601193Losses:  5.358945846557617 2.058488607406616
CurrentTrain: epoch  6, batch     1 | loss: 5.3589458Losses:  5.136927127838135 2.016202211380005
CurrentTrain: epoch  6, batch     2 | loss: 5.1369271Losses:  3.6058013439178467 0.6163108348846436
CurrentTrain: epoch  6, batch     3 | loss: 3.6058013Losses:  5.046511650085449 1.9536670446395874
CurrentTrain: epoch  7, batch     0 | loss: 5.0465117Losses:  5.150896072387695 1.9661388397216797
CurrentTrain: epoch  7, batch     1 | loss: 5.1508961Losses:  5.176743507385254 1.9570510387420654
CurrentTrain: epoch  7, batch     2 | loss: 5.1767435Losses:  4.109016418457031 0.673681378364563
CurrentTrain: epoch  7, batch     3 | loss: 4.1090164Losses:  5.257369041442871 2.110743522644043
CurrentTrain: epoch  8, batch     0 | loss: 5.2573690Losses:  5.226694107055664 2.0642056465148926
CurrentTrain: epoch  8, batch     1 | loss: 5.2266941Losses:  5.119141101837158 1.9368342161178589
CurrentTrain: epoch  8, batch     2 | loss: 5.1191411Losses:  3.680893898010254 0.6617439985275269
CurrentTrain: epoch  8, batch     3 | loss: 3.6808939Losses:  4.945284366607666 1.8560062646865845
CurrentTrain: epoch  9, batch     0 | loss: 4.9452844Losses:  4.868184566497803 1.7827247381210327
CurrentTrain: epoch  9, batch     1 | loss: 4.8681846Losses:  5.103717803955078 1.8901653289794922
CurrentTrain: epoch  9, batch     2 | loss: 5.1037178Losses:  4.1129150390625 0.6640341281890869
CurrentTrain: epoch  9, batch     3 | loss: 4.1129150
Losses:  4.9829487800598145 2.6034324169158936
MemoryTrain:  epoch  0, batch     0 | loss: 4.9829488Losses:  4.827259063720703 2.4527556896209717
MemoryTrain:  epoch  0, batch     1 | loss: 4.8272591Losses:  5.083918571472168 2.6085166931152344
MemoryTrain:  epoch  1, batch     0 | loss: 5.0839186Losses:  4.899338722229004 2.4474003314971924
MemoryTrain:  epoch  1, batch     1 | loss: 4.8993387Losses:  4.9787421226501465 2.577091693878174
MemoryTrain:  epoch  2, batch     0 | loss: 4.9787421Losses:  4.889232158660889 2.4692649841308594
MemoryTrain:  epoch  2, batch     1 | loss: 4.8892322Losses:  4.960301399230957 2.5753393173217773
MemoryTrain:  epoch  3, batch     0 | loss: 4.9603014Losses:  4.86732292175293 2.454951524734497
MemoryTrain:  epoch  3, batch     1 | loss: 4.8673229Losses:  5.00780725479126 2.599180221557617
MemoryTrain:  epoch  4, batch     0 | loss: 5.0078073Losses:  4.814538955688477 2.432281494140625
MemoryTrain:  epoch  4, batch     1 | loss: 4.8145390Losses:  4.9864397048950195 2.5967226028442383
MemoryTrain:  epoch  5, batch     0 | loss: 4.9864397Losses:  4.802306175231934 2.425313711166382
MemoryTrain:  epoch  5, batch     1 | loss: 4.8023062Losses:  4.922545433044434 2.561833381652832
MemoryTrain:  epoch  6, batch     0 | loss: 4.9225454Losses:  4.850553512573242 2.4623966217041016
MemoryTrain:  epoch  6, batch     1 | loss: 4.8505535Losses:  4.921873092651367 2.575554847717285
MemoryTrain:  epoch  7, batch     0 | loss: 4.9218731Losses:  4.840193748474121 2.447669267654419
MemoryTrain:  epoch  7, batch     1 | loss: 4.8401937Losses:  4.990938186645508 2.6121304035186768
MemoryTrain:  epoch  8, batch     0 | loss: 4.9909382Losses:  4.741885185241699 2.398279905319214
MemoryTrain:  epoch  8, batch     1 | loss: 4.7418852Losses:  4.968386650085449 2.6100223064422607
MemoryTrain:  epoch  9, batch     0 | loss: 4.9683867Losses:  4.784400939941406 2.4192728996276855
MemoryTrain:  epoch  9, batch     1 | loss: 4.7844009
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 63.84%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 60.00%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 58.27%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 58.01%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 58.14%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 59.03%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 60.36%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 61.97%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 60.71%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 60.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 60.22%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 60.14%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 59.95%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.82%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 59.48%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 59.64%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 59.02%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 58.87%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.33%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 88.24%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.51%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.30%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.22%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 89.96%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 90.13%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 90.23%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.98%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 89.78%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 89.56%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 88.88%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 88.14%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 87.66%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 87.03%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 86.65%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 86.20%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 85.69%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 85.12%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 84.78%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 84.38%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 83.91%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 83.74%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 83.15%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 82.64%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 81.73%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 81.18%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.65%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 80.07%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 79.62%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 79.25%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 78.76%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 78.41%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 77.75%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.39%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 77.06%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 76.80%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 76.67%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 75.81%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 75.63%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 75.34%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 74.94%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 74.67%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 74.51%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 74.16%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 74.49%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 74.55%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 74.40%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 74.26%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 74.46%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 74.15%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.76%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 73.59%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 73.09%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  145 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 73.51%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 73.03%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 72.59%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 72.20%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.73%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.31%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 71.18%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 71.12%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 71.11%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 71.65%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 71.42%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 71.11%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 70.15%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 69.81%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 69.77%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 69.59%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 69.46%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 69.12%   
cur_acc:  ['0.9147', '0.5843', '0.5833']
his_acc:  ['0.9147', '0.7490', '0.6912']
Clustering into  19  clusters
Clusters:  [ 6 18  2  1  9  0  2  5  3  3  2  3 14  0 17  4  3  0 11  4  8  4  1 15
  1 18  8  1 13 16  2  8  8  1 12  6 10  7  5 13]
Losses:  5.438786506652832 1.9441790580749512
CurrentTrain: epoch  0, batch     0 | loss: 5.4387865Losses:  5.277298450469971 1.930227518081665
CurrentTrain: epoch  0, batch     1 | loss: 5.2772985Losses:  5.159860610961914 1.7623302936553955
CurrentTrain: epoch  0, batch     2 | loss: 5.1598606Losses:  3.8011131286621094 0.60486900806427
CurrentTrain: epoch  0, batch     3 | loss: 3.8011131Losses:  5.427696228027344 2.0485708713531494
CurrentTrain: epoch  1, batch     0 | loss: 5.4276962Losses:  5.2928385734558105 1.9905617237091064
CurrentTrain: epoch  1, batch     1 | loss: 5.2928386Losses:  5.44412899017334 2.0587809085845947
CurrentTrain: epoch  1, batch     2 | loss: 5.4441290Losses:  3.851506233215332 0.5793813467025757
CurrentTrain: epoch  1, batch     3 | loss: 3.8515062Losses:  5.311012268066406 1.9424021244049072
CurrentTrain: epoch  2, batch     0 | loss: 5.3110123Losses:  5.185530662536621 1.9444630146026611
CurrentTrain: epoch  2, batch     1 | loss: 5.1855307Losses:  5.0117387771606445 1.8672583103179932
CurrentTrain: epoch  2, batch     2 | loss: 5.0117388Losses:  3.6987192630767822 0.6013538837432861
CurrentTrain: epoch  2, batch     3 | loss: 3.6987193Losses:  5.133048057556152 1.9548804759979248
CurrentTrain: epoch  3, batch     0 | loss: 5.1330481Losses:  5.31455135345459 2.0514912605285645
CurrentTrain: epoch  3, batch     1 | loss: 5.3145514Losses:  5.122143268585205 1.9675025939941406
CurrentTrain: epoch  3, batch     2 | loss: 5.1221433Losses:  3.6752426624298096 0.6377936005592346
CurrentTrain: epoch  3, batch     3 | loss: 3.6752427Losses:  4.995837688446045 1.7969591617584229
CurrentTrain: epoch  4, batch     0 | loss: 4.9958377Losses:  4.8301239013671875 1.7548584938049316
CurrentTrain: epoch  4, batch     1 | loss: 4.8301239Losses:  4.782577037811279 1.5931193828582764
CurrentTrain: epoch  4, batch     2 | loss: 4.7825770Losses:  3.5707359313964844 0.5876361131668091
CurrentTrain: epoch  4, batch     3 | loss: 3.5707359Losses:  5.185579299926758 1.9509360790252686
CurrentTrain: epoch  5, batch     0 | loss: 5.1855793Losses:  4.907360076904297 1.8090598583221436
CurrentTrain: epoch  5, batch     1 | loss: 4.9073601Losses:  5.106459140777588 2.055567741394043
CurrentTrain: epoch  5, batch     2 | loss: 5.1064591Losses:  2.9408140182495117 0.0
CurrentTrain: epoch  5, batch     3 | loss: 2.9408140Losses:  4.804659843444824 1.804452896118164
CurrentTrain: epoch  6, batch     0 | loss: 4.8046598Losses:  4.918007850646973 1.7938323020935059
CurrentTrain: epoch  6, batch     1 | loss: 4.9180079Losses:  5.071929454803467 1.9191570281982422
CurrentTrain: epoch  6, batch     2 | loss: 5.0719295Losses:  3.778158664703369 0.6718000173568726
CurrentTrain: epoch  6, batch     3 | loss: 3.7781587Losses:  4.956155300140381 1.913120150566101
CurrentTrain: epoch  7, batch     0 | loss: 4.9561553Losses:  4.925806999206543 1.8882148265838623
CurrentTrain: epoch  7, batch     1 | loss: 4.9258070Losses:  4.9878058433532715 1.9303532838821411
CurrentTrain: epoch  7, batch     2 | loss: 4.9878058Losses:  4.051086902618408 0.62132328748703
CurrentTrain: epoch  7, batch     3 | loss: 4.0510869Losses:  4.795559406280518 1.7119171619415283
CurrentTrain: epoch  8, batch     0 | loss: 4.7955594Losses:  4.995813846588135 1.9490035772323608
CurrentTrain: epoch  8, batch     1 | loss: 4.9958138Losses:  4.900428295135498 1.9066966772079468
CurrentTrain: epoch  8, batch     2 | loss: 4.9004283Losses:  3.702411413192749 0.6080217361450195
CurrentTrain: epoch  8, batch     3 | loss: 3.7024114Losses:  4.9037628173828125 1.9240586757659912
CurrentTrain: epoch  9, batch     0 | loss: 4.9037628Losses:  4.905669212341309 1.81756591796875
CurrentTrain: epoch  9, batch     1 | loss: 4.9056692Losses:  4.848366737365723 1.8715431690216064
CurrentTrain: epoch  9, batch     2 | loss: 4.8483667Losses:  3.5977203845977783 0.550797164440155
CurrentTrain: epoch  9, batch     3 | loss: 3.5977204
Losses:  5.17446231842041 2.580890417098999
MemoryTrain:  epoch  0, batch     0 | loss: 5.1744623Losses:  5.1743268966674805 2.5750913619995117
MemoryTrain:  epoch  0, batch     1 | loss: 5.1743269Losses:  4.438413619995117 1.8692861795425415
MemoryTrain:  epoch  0, batch     2 | loss: 4.4384136Losses:  5.2732110023498535 2.5909366607666016
MemoryTrain:  epoch  1, batch     0 | loss: 5.2732110Losses:  5.238615036010742 2.5461483001708984
MemoryTrain:  epoch  1, batch     1 | loss: 5.2386150Losses:  4.561254024505615 1.9035085439682007
MemoryTrain:  epoch  1, batch     2 | loss: 4.5612540Losses:  5.198687553405762 2.56087064743042
MemoryTrain:  epoch  2, batch     0 | loss: 5.1986876Losses:  5.258415699005127 2.6005616188049316
MemoryTrain:  epoch  2, batch     1 | loss: 5.2584157Losses:  4.462978363037109 1.8473551273345947
MemoryTrain:  epoch  2, batch     2 | loss: 4.4629784Losses:  5.211906909942627 2.580906867980957
MemoryTrain:  epoch  3, batch     0 | loss: 5.2119069Losses:  5.201098442077637 2.5554237365722656
MemoryTrain:  epoch  3, batch     1 | loss: 5.2010984Losses:  4.482038974761963 1.8738728761672974
MemoryTrain:  epoch  3, batch     2 | loss: 4.4820390Losses:  5.182262420654297 2.5673961639404297
MemoryTrain:  epoch  4, batch     0 | loss: 5.1822624Losses:  5.180228233337402 2.5561163425445557
MemoryTrain:  epoch  4, batch     1 | loss: 5.1802282Losses:  4.559409141540527 1.9015882015228271
MemoryTrain:  epoch  4, batch     2 | loss: 4.5594091Losses:  5.176051616668701 2.5501904487609863
MemoryTrain:  epoch  5, batch     0 | loss: 5.1760516Losses:  5.192617416381836 2.5732622146606445
MemoryTrain:  epoch  5, batch     1 | loss: 5.1926174Losses:  4.51908016204834 1.8970880508422852
MemoryTrain:  epoch  5, batch     2 | loss: 4.5190802Losses:  5.148679733276367 2.5353786945343018
MemoryTrain:  epoch  6, batch     0 | loss: 5.1486797Losses:  5.188016414642334 2.5677857398986816
MemoryTrain:  epoch  6, batch     1 | loss: 5.1880164Losses:  4.556098461151123 1.9363778829574585
MemoryTrain:  epoch  6, batch     2 | loss: 4.5560985Losses:  5.156763076782227 2.5588841438293457
MemoryTrain:  epoch  7, batch     0 | loss: 5.1567631Losses:  5.206301212310791 2.575833797454834
MemoryTrain:  epoch  7, batch     1 | loss: 5.2063012Losses:  4.500703811645508 1.8839237689971924
MemoryTrain:  epoch  7, batch     2 | loss: 4.5007038Losses:  5.149754047393799 2.546908140182495
MemoryTrain:  epoch  8, batch     0 | loss: 5.1497540Losses:  5.200151443481445 2.5860447883605957
MemoryTrain:  epoch  8, batch     1 | loss: 5.2001514Losses:  4.467653751373291 1.853505253791809
MemoryTrain:  epoch  8, batch     2 | loss: 4.4676538Losses:  5.197723865509033 2.58852481842041
MemoryTrain:  epoch  9, batch     0 | loss: 5.1977239Losses:  5.127917289733887 2.533987522125244
MemoryTrain:  epoch  9, batch     1 | loss: 5.1279173Losses:  4.47611141204834 1.8650648593902588
MemoryTrain:  epoch  9, batch     2 | loss: 4.4761114
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 25.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 76.16%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 75.56%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 75.77%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.17%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.93%   #############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.8347806930542 1.807142972946167
CurrentTrain: epoch  0, batch     0 | loss: 11.8347807Losses:  12.049310684204102 2.0834856033325195
CurrentTrain: epoch  0, batch     1 | loss: 12.0493107Losses:  11.989362716674805 1.911325216293335
CurrentTrain: epoch  0, batch     2 | loss: 11.9893627Losses:  11.795045852661133 1.7906537055969238
CurrentTrain: epoch  0, batch     3 | loss: 11.7950459Losses:  11.211152076721191 1.962788462638855
CurrentTrain: epoch  0, batch     4 | loss: 11.2111521Losses:  12.120055198669434 1.6870828866958618
CurrentTrain: epoch  0, batch     5 | loss: 12.1200552Losses:  11.48250961303711 1.9306451082229614
CurrentTrain: epoch  0, batch     6 | loss: 11.4825096Losses:  10.81885814666748 1.86490797996521
CurrentTrain: epoch  0, batch     7 | loss: 10.8188581Losses:  11.042916297912598 1.9308388233184814
CurrentTrain: epoch  0, batch     8 | loss: 11.0429163Losses:  11.643489837646484 1.9287242889404297
CurrentTrain: epoch  0, batch     9 | loss: 11.6434898Losses:  10.736745834350586 1.7657628059387207
CurrentTrain: epoch  0, batch    10 | loss: 10.7367458Losses:  10.181541442871094 1.7369763851165771
CurrentTrain: epoch  0, batch    11 | loss: 10.1815414Losses:  10.912748336791992 1.788586139678955
CurrentTrain: epoch  0, batch    12 | loss: 10.9127483Losses:  10.954297065734863 1.9289531707763672
CurrentTrain: epoch  0, batch    13 | loss: 10.9542971Losses:  10.018232345581055 1.862654209136963
CurrentTrain: epoch  0, batch    14 | loss: 10.0182323Losses:  10.899341583251953 2.059267044067383
CurrentTrain: epoch  0, batch    15 | loss: 10.8993416Losses:  10.827293395996094 1.599959135055542
CurrentTrain: epoch  0, batch    16 | loss: 10.8272934Losses:  11.076066017150879 2.063431739807129
CurrentTrain: epoch  0, batch    17 | loss: 11.0760660Losses:  10.781877517700195 1.8164480924606323
CurrentTrain: epoch  0, batch    18 | loss: 10.7818775Losses:  10.673437118530273 2.033536195755005
CurrentTrain: epoch  0, batch    19 | loss: 10.6734371Losses:  9.995133399963379 1.9716236591339111
CurrentTrain: epoch  0, batch    20 | loss: 9.9951334Losses:  10.182242393493652 1.9901416301727295
CurrentTrain: epoch  0, batch    21 | loss: 10.1822424Losses:  10.686077117919922 1.9401936531066895
CurrentTrain: epoch  0, batch    22 | loss: 10.6860771Losses:  10.050192832946777 1.9092646837234497
CurrentTrain: epoch  0, batch    23 | loss: 10.0501928Losses:  9.752341270446777 1.9891676902770996
CurrentTrain: epoch  0, batch    24 | loss: 9.7523413Losses:  9.892712593078613 1.8826268911361694
CurrentTrain: epoch  0, batch    25 | loss: 9.8927126Losses:  10.023848533630371 1.8119895458221436
CurrentTrain: epoch  0, batch    26 | loss: 10.0238485Losses:  9.782342910766602 2.041661262512207
CurrentTrain: epoch  0, batch    27 | loss: 9.7823429Losses:  10.077502250671387 1.8738480806350708
CurrentTrain: epoch  0, batch    28 | loss: 10.0775023Losses:  10.118365287780762 1.9675111770629883
CurrentTrain: epoch  0, batch    29 | loss: 10.1183653Losses:  10.622550964355469 2.077596426010132
CurrentTrain: epoch  0, batch    30 | loss: 10.6225510Losses:  9.79157829284668 1.7900623083114624
CurrentTrain: epoch  0, batch    31 | loss: 9.7915783Losses:  9.44528579711914 1.4651117324829102
CurrentTrain: epoch  0, batch    32 | loss: 9.4452858Losses:  9.124256134033203 1.6696841716766357
CurrentTrain: epoch  0, batch    33 | loss: 9.1242561Losses:  9.696988105773926 1.8191089630126953
CurrentTrain: epoch  0, batch    34 | loss: 9.6969881Losses:  9.588913917541504 1.8482297658920288
CurrentTrain: epoch  0, batch    35 | loss: 9.5889139Losses:  9.62702751159668 1.9139900207519531
CurrentTrain: epoch  0, batch    36 | loss: 9.6270275Losses:  9.66705322265625 1.9175831079483032
CurrentTrain: epoch  0, batch    37 | loss: 9.6670532Losses:  10.622628211975098 2.122753858566284
CurrentTrain: epoch  0, batch    38 | loss: 10.6226282Losses:  10.161784172058105 2.13344669342041
CurrentTrain: epoch  0, batch    39 | loss: 10.1617842Losses:  10.46462631225586 2.033449172973633
CurrentTrain: epoch  0, batch    40 | loss: 10.4646263Losses:  9.391698837280273 1.9715718030929565
CurrentTrain: epoch  0, batch    41 | loss: 9.3916988Losses:  9.212141036987305 1.9485212564468384
CurrentTrain: epoch  0, batch    42 | loss: 9.2121410Losses:  8.667978286743164 1.6493502855300903
CurrentTrain: epoch  0, batch    43 | loss: 8.6679783Losses:  9.071480751037598 1.7395411729812622
CurrentTrain: epoch  0, batch    44 | loss: 9.0714808Losses:  9.362805366516113 2.037419080734253
CurrentTrain: epoch  0, batch    45 | loss: 9.3628054Losses:  9.891868591308594 1.775052547454834
CurrentTrain: epoch  0, batch    46 | loss: 9.8918686Losses:  8.624404907226562 2.034243106842041
CurrentTrain: epoch  0, batch    47 | loss: 8.6244049Losses:  9.510567665100098 1.945023775100708
CurrentTrain: epoch  0, batch    48 | loss: 9.5105677Losses:  9.206707000732422 1.9313900470733643
CurrentTrain: epoch  0, batch    49 | loss: 9.2067070Losses:  9.655362129211426 1.9820886850357056
CurrentTrain: epoch  0, batch    50 | loss: 9.6553621Losses:  9.296676635742188 2.0921883583068848
CurrentTrain: epoch  0, batch    51 | loss: 9.2966766Losses:  8.87683391571045 1.9130134582519531
CurrentTrain: epoch  0, batch    52 | loss: 8.8768339Losses:  9.048752784729004 1.909952998161316
CurrentTrain: epoch  0, batch    53 | loss: 9.0487528Losses:  9.791742324829102 2.055269718170166
CurrentTrain: epoch  0, batch    54 | loss: 9.7917423Losses:  8.817909240722656 1.8428311347961426
CurrentTrain: epoch  0, batch    55 | loss: 8.8179092Losses:  8.55390739440918 1.789391040802002
CurrentTrain: epoch  0, batch    56 | loss: 8.5539074Losses:  9.470672607421875 1.9717702865600586
CurrentTrain: epoch  0, batch    57 | loss: 9.4706726Losses:  9.099682807922363 1.7259143590927124
CurrentTrain: epoch  0, batch    58 | loss: 9.0996828Losses:  8.72774887084961 1.7001365423202515
CurrentTrain: epoch  0, batch    59 | loss: 8.7277489Losses:  8.44964599609375 1.936100721359253
CurrentTrain: epoch  0, batch    60 | loss: 8.4496460Losses:  9.161876678466797 1.7254681587219238
CurrentTrain: epoch  0, batch    61 | loss: 9.1618767Losses:  10.372476577758789 1.4367696046829224
CurrentTrain: epoch  0, batch    62 | loss: 10.3724766Losses:  9.167893409729004 1.7679860591888428
CurrentTrain: epoch  1, batch     0 | loss: 9.1678934Losses:  8.848360061645508 1.8577138185501099
CurrentTrain: epoch  1, batch     1 | loss: 8.8483601Losses:  8.528350830078125 1.795277714729309
CurrentTrain: epoch  1, batch     2 | loss: 8.5283508Losses:  9.026357650756836 2.0458221435546875
CurrentTrain: epoch  1, batch     3 | loss: 9.0263577Losses:  9.283323287963867 2.0051350593566895
CurrentTrain: epoch  1, batch     4 | loss: 9.2833233Losses:  8.6563720703125 1.8948824405670166
CurrentTrain: epoch  1, batch     5 | loss: 8.6563721Losses:  8.773059844970703 1.897310733795166
CurrentTrain: epoch  1, batch     6 | loss: 8.7730598Losses:  8.194128036499023 2.033466339111328
CurrentTrain: epoch  1, batch     7 | loss: 8.1941280Losses:  8.048453330993652 1.7202637195587158
CurrentTrain: epoch  1, batch     8 | loss: 8.0484533Losses:  8.99484634399414 1.979018211364746
CurrentTrain: epoch  1, batch     9 | loss: 8.9948463Losses:  8.837393760681152 1.921390175819397
CurrentTrain: epoch  1, batch    10 | loss: 8.8373938Losses:  8.508017539978027 1.965043544769287
CurrentTrain: epoch  1, batch    11 | loss: 8.5080175Losses:  8.813987731933594 2.0072755813598633
CurrentTrain: epoch  1, batch    12 | loss: 8.8139877Losses:  8.760385513305664 1.6378755569458008
CurrentTrain: epoch  1, batch    13 | loss: 8.7603855Losses:  8.181336402893066 1.9358127117156982
CurrentTrain: epoch  1, batch    14 | loss: 8.1813364Losses:  9.708335876464844 1.8559534549713135
CurrentTrain: epoch  1, batch    15 | loss: 9.7083359Losses:  8.353912353515625 1.7762757539749146
CurrentTrain: epoch  1, batch    16 | loss: 8.3539124Losses:  7.54079008102417 1.5917161703109741
CurrentTrain: epoch  1, batch    17 | loss: 7.5407901Losses:  8.21278190612793 1.813019037246704
CurrentTrain: epoch  1, batch    18 | loss: 8.2127819Losses:  8.616769790649414 1.9516477584838867
CurrentTrain: epoch  1, batch    19 | loss: 8.6167698Losses:  9.009464263916016 2.053952693939209
CurrentTrain: epoch  1, batch    20 | loss: 9.0094643Losses:  9.000665664672852 2.0581021308898926
CurrentTrain: epoch  1, batch    21 | loss: 9.0006657Losses:  8.616004943847656 1.8913288116455078
CurrentTrain: epoch  1, batch    22 | loss: 8.6160049Losses:  8.580886840820312 1.7080273628234863
CurrentTrain: epoch  1, batch    23 | loss: 8.5808868Losses:  8.44156551361084 1.900972843170166
CurrentTrain: epoch  1, batch    24 | loss: 8.4415655Losses:  7.403064250946045 1.8408869504928589
CurrentTrain: epoch  1, batch    25 | loss: 7.4030643Losses:  8.214532852172852 2.0687336921691895
CurrentTrain: epoch  1, batch    26 | loss: 8.2145329Losses:  8.020332336425781 1.9333271980285645
CurrentTrain: epoch  1, batch    27 | loss: 8.0203323Losses:  8.48284912109375 1.9655678272247314
CurrentTrain: epoch  1, batch    28 | loss: 8.4828491Losses:  7.982632160186768 2.0477099418640137
CurrentTrain: epoch  1, batch    29 | loss: 7.9826322Losses:  8.155085563659668 1.8567211627960205
CurrentTrain: epoch  1, batch    30 | loss: 8.1550856Losses:  7.759891510009766 2.076432466506958
CurrentTrain: epoch  1, batch    31 | loss: 7.7598915Losses:  7.420502662658691 1.9612797498703003
CurrentTrain: epoch  1, batch    32 | loss: 7.4205027Losses:  7.6136555671691895 1.7096039056777954
CurrentTrain: epoch  1, batch    33 | loss: 7.6136556Losses:  8.729541778564453 2.0178818702697754
CurrentTrain: epoch  1, batch    34 | loss: 8.7295418Losses:  7.536523818969727 1.8839778900146484
CurrentTrain: epoch  1, batch    35 | loss: 7.5365238Losses:  8.59455394744873 1.704442024230957
CurrentTrain: epoch  1, batch    36 | loss: 8.5945539Losses:  8.617238998413086 1.9943571090698242
CurrentTrain: epoch  1, batch    37 | loss: 8.6172390Losses:  9.127508163452148 2.0533909797668457
CurrentTrain: epoch  1, batch    38 | loss: 9.1275082Losses:  7.548943996429443 1.9455653429031372
CurrentTrain: epoch  1, batch    39 | loss: 7.5489440Losses:  8.45988941192627 1.8444557189941406
CurrentTrain: epoch  1, batch    40 | loss: 8.4598894Losses:  8.874801635742188 2.011385202407837
CurrentTrain: epoch  1, batch    41 | loss: 8.8748016Losses:  8.793611526489258 1.8990802764892578
CurrentTrain: epoch  1, batch    42 | loss: 8.7936115Losses:  9.248186111450195 1.9240748882293701
CurrentTrain: epoch  1, batch    43 | loss: 9.2481861Losses:  9.117823600769043 1.7641384601593018
CurrentTrain: epoch  1, batch    44 | loss: 9.1178236Losses:  7.893847465515137 2.007449150085449
CurrentTrain: epoch  1, batch    45 | loss: 7.8938475Losses:  7.431491851806641 1.9733073711395264
CurrentTrain: epoch  1, batch    46 | loss: 7.4314919Losses:  8.608933448791504 2.1375224590301514
CurrentTrain: epoch  1, batch    47 | loss: 8.6089334Losses:  8.153494834899902 1.9206916093826294
CurrentTrain: epoch  1, batch    48 | loss: 8.1534948Losses:  7.816290378570557 1.9213967323303223
CurrentTrain: epoch  1, batch    49 | loss: 7.8162904Losses:  8.053777694702148 1.8640248775482178
CurrentTrain: epoch  1, batch    50 | loss: 8.0537777Losses:  8.906341552734375 1.7783393859863281
CurrentTrain: epoch  1, batch    51 | loss: 8.9063416Losses:  8.115616798400879 1.852580189704895
CurrentTrain: epoch  1, batch    52 | loss: 8.1156168Losses:  8.197590827941895 1.8648688793182373
CurrentTrain: epoch  1, batch    53 | loss: 8.1975908Losses:  8.262133598327637 1.9725478887557983
CurrentTrain: epoch  1, batch    54 | loss: 8.2621336Losses:  6.975861072540283 1.6953319311141968
CurrentTrain: epoch  1, batch    55 | loss: 6.9758611Losses:  7.064100742340088 1.7337017059326172
CurrentTrain: epoch  1, batch    56 | loss: 7.0641007Losses:  8.056532859802246 1.9776813983917236
CurrentTrain: epoch  1, batch    57 | loss: 8.0565329Losses:  7.289645195007324 1.7606788873672485
CurrentTrain: epoch  1, batch    58 | loss: 7.2896452Losses:  8.706506729125977 2.0449132919311523
CurrentTrain: epoch  1, batch    59 | loss: 8.7065067Losses:  8.278082847595215 1.9958584308624268
CurrentTrain: epoch  1, batch    60 | loss: 8.2780828Losses:  7.801552772521973 2.010098934173584
CurrentTrain: epoch  1, batch    61 | loss: 7.8015528Losses:  6.157940864562988 1.382426381111145
CurrentTrain: epoch  1, batch    62 | loss: 6.1579409Losses:  7.6956939697265625 1.6787160634994507
CurrentTrain: epoch  2, batch     0 | loss: 7.6956940Losses:  7.627875804901123 1.6715235710144043
CurrentTrain: epoch  2, batch     1 | loss: 7.6278758Losses:  7.888949394226074 1.9367387294769287
CurrentTrain: epoch  2, batch     2 | loss: 7.8889494Losses:  7.086757659912109 1.8989975452423096
CurrentTrain: epoch  2, batch     3 | loss: 7.0867577Losses:  7.519344329833984 1.8749314546585083
CurrentTrain: epoch  2, batch     4 | loss: 7.5193443Losses:  7.550556659698486 2.113466262817383
CurrentTrain: epoch  2, batch     5 | loss: 7.5505567Losses:  7.196444511413574 1.7513301372528076
CurrentTrain: epoch  2, batch     6 | loss: 7.1964445Losses:  8.019209861755371 1.9228038787841797
CurrentTrain: epoch  2, batch     7 | loss: 8.0192099Losses:  8.585969924926758 1.8989050388336182
CurrentTrain: epoch  2, batch     8 | loss: 8.5859699Losses:  7.083681106567383 1.809436559677124
CurrentTrain: epoch  2, batch     9 | loss: 7.0836811Losses:  7.081018447875977 1.9265141487121582
CurrentTrain: epoch  2, batch    10 | loss: 7.0810184Losses:  7.852605819702148 1.8583602905273438
CurrentTrain: epoch  2, batch    11 | loss: 7.8526058Losses:  8.091216087341309 1.7960398197174072
CurrentTrain: epoch  2, batch    12 | loss: 8.0912161Losses:  7.673099517822266 1.7461440563201904
CurrentTrain: epoch  2, batch    13 | loss: 7.6730995Losses:  7.613888263702393 1.9551321268081665
CurrentTrain: epoch  2, batch    14 | loss: 7.6138883Losses:  7.328579902648926 1.984054684638977
CurrentTrain: epoch  2, batch    15 | loss: 7.3285799Losses:  6.901256561279297 1.872894048690796
CurrentTrain: epoch  2, batch    16 | loss: 6.9012566Losses:  6.347546577453613 1.5871121883392334
CurrentTrain: epoch  2, batch    17 | loss: 6.3475466Losses:  7.995899200439453 2.0199427604675293
CurrentTrain: epoch  2, batch    18 | loss: 7.9958992Losses:  7.412525177001953 1.8295477628707886
CurrentTrain: epoch  2, batch    19 | loss: 7.4125252Losses:  7.39222526550293 1.9495353698730469
CurrentTrain: epoch  2, batch    20 | loss: 7.3922253Losses:  7.154382705688477 1.895841121673584
CurrentTrain: epoch  2, batch    21 | loss: 7.1543827Losses:  6.942634582519531 1.901516079902649
CurrentTrain: epoch  2, batch    22 | loss: 6.9426346Losses:  7.5940141677856445 1.9385442733764648
CurrentTrain: epoch  2, batch    23 | loss: 7.5940142Losses:  7.4724931716918945 1.8768222332000732
CurrentTrain: epoch  2, batch    24 | loss: 7.4724932Losses:  7.126766204833984 1.8925118446350098
CurrentTrain: epoch  2, batch    25 | loss: 7.1267662Losses:  6.889431953430176 1.8849972486495972
CurrentTrain: epoch  2, batch    26 | loss: 6.8894320Losses:  6.931153297424316 1.6511132717132568
CurrentTrain: epoch  2, batch    27 | loss: 6.9311533Losses:  6.89119815826416 1.797348976135254
CurrentTrain: epoch  2, batch    28 | loss: 6.8911982Losses:  7.523976802825928 2.109252452850342
CurrentTrain: epoch  2, batch    29 | loss: 7.5239768Losses:  7.659815311431885 2.016658306121826
CurrentTrain: epoch  2, batch    30 | loss: 7.6598153Losses:  7.189210891723633 2.1243302822113037
CurrentTrain: epoch  2, batch    31 | loss: 7.1892109Losses:  8.0498046875 1.7506239414215088
CurrentTrain: epoch  2, batch    32 | loss: 8.0498047Losses:  6.67594575881958 1.8324761390686035
CurrentTrain: epoch  2, batch    33 | loss: 6.6759458Losses:  7.518912315368652 1.9910109043121338
CurrentTrain: epoch  2, batch    34 | loss: 7.5189123Losses:  7.391968727111816 1.8492372035980225
CurrentTrain: epoch  2, batch    35 | loss: 7.3919687Losses:  6.95416259765625 1.977764368057251
CurrentTrain: epoch  2, batch    36 | loss: 6.9541626Losses:  7.15048360824585 1.7370028495788574
CurrentTrain: epoch  2, batch    37 | loss: 7.1504836Losses:  7.0616984367370605 1.8180241584777832
CurrentTrain: epoch  2, batch    38 | loss: 7.0616984Losses:  6.45560359954834 1.7530231475830078
CurrentTrain: epoch  2, batch    39 | loss: 6.4556036Losses:  7.1090288162231445 1.6605647802352905
CurrentTrain: epoch  2, batch    40 | loss: 7.1090288Losses:  7.435111999511719 1.4933648109436035
CurrentTrain: epoch  2, batch    41 | loss: 7.4351120Losses:  7.088935852050781 1.9242916107177734
CurrentTrain: epoch  2, batch    42 | loss: 7.0889359Losses:  7.25316047668457 1.863337755203247
CurrentTrain: epoch  2, batch    43 | loss: 7.2531605Losses:  7.121077537536621 1.9937794208526611
CurrentTrain: epoch  2, batch    44 | loss: 7.1210775Losses:  7.397496700286865 1.926802158355713
CurrentTrain: epoch  2, batch    45 | loss: 7.3974967Losses:  6.777483940124512 1.9183586835861206
CurrentTrain: epoch  2, batch    46 | loss: 6.7774839Losses:  7.6095170974731445 1.92945396900177
CurrentTrain: epoch  2, batch    47 | loss: 7.6095171Losses:  6.721611976623535 1.8564481735229492
CurrentTrain: epoch  2, batch    48 | loss: 6.7216120Losses:  7.3729119300842285 1.9402118921279907
CurrentTrain: epoch  2, batch    49 | loss: 7.3729119Losses:  6.806717872619629 1.7356622219085693
CurrentTrain: epoch  2, batch    50 | loss: 6.8067179Losses:  6.764054298400879 1.9663031101226807
CurrentTrain: epoch  2, batch    51 | loss: 6.7640543Losses:  6.821037292480469 1.7227816581726074
CurrentTrain: epoch  2, batch    52 | loss: 6.8210373Losses:  7.056102752685547 2.0191822052001953
CurrentTrain: epoch  2, batch    53 | loss: 7.0561028Losses:  6.832781791687012 1.9654321670532227
CurrentTrain: epoch  2, batch    54 | loss: 6.8327818Losses:  6.992189407348633 2.0173308849334717
CurrentTrain: epoch  2, batch    55 | loss: 6.9921894Losses:  7.43240213394165 2.111173152923584
CurrentTrain: epoch  2, batch    56 | loss: 7.4324021Losses:  7.050651550292969 1.8071022033691406
CurrentTrain: epoch  2, batch    57 | loss: 7.0506516Losses:  6.769798278808594 1.7651947736740112
CurrentTrain: epoch  2, batch    58 | loss: 6.7697983Losses:  6.789426803588867 1.905542254447937
CurrentTrain: epoch  2, batch    59 | loss: 6.7894268Losses:  6.576893329620361 1.9160107374191284
CurrentTrain: epoch  2, batch    60 | loss: 6.5768933Losses:  6.81281852722168 1.8237429857254028
CurrentTrain: epoch  2, batch    61 | loss: 6.8128185Losses:  6.4242682456970215 1.5707510709762573
CurrentTrain: epoch  2, batch    62 | loss: 6.4242682Losses:  7.372417449951172 1.8499820232391357
CurrentTrain: epoch  3, batch     0 | loss: 7.3724174Losses:  7.818521499633789 1.7732179164886475
CurrentTrain: epoch  3, batch     1 | loss: 7.8185215Losses:  6.8902363777160645 1.96041738986969
CurrentTrain: epoch  3, batch     2 | loss: 6.8902364Losses:  7.097917556762695 1.7196426391601562
CurrentTrain: epoch  3, batch     3 | loss: 7.0979176Losses:  7.036540985107422 1.8564306497573853
CurrentTrain: epoch  3, batch     4 | loss: 7.0365410Losses:  7.135041236877441 1.8470910787582397
CurrentTrain: epoch  3, batch     5 | loss: 7.1350412Losses:  6.793939590454102 1.8728188276290894
CurrentTrain: epoch  3, batch     6 | loss: 6.7939396Losses:  6.902417182922363 1.9557744264602661
CurrentTrain: epoch  3, batch     7 | loss: 6.9024172Losses:  7.237414360046387 1.8979483842849731
CurrentTrain: epoch  3, batch     8 | loss: 7.2374144Losses:  7.254872798919678 2.008115291595459
CurrentTrain: epoch  3, batch     9 | loss: 7.2548728Losses:  7.174227237701416 2.094836711883545
CurrentTrain: epoch  3, batch    10 | loss: 7.1742272Losses:  6.877377510070801 1.8754606246948242
CurrentTrain: epoch  3, batch    11 | loss: 6.8773775Losses:  6.352175235748291 1.8219882249832153
CurrentTrain: epoch  3, batch    12 | loss: 6.3521752Losses:  7.154833793640137 2.1047258377075195
CurrentTrain: epoch  3, batch    13 | loss: 7.1548338Losses:  6.693943977355957 1.927542805671692
CurrentTrain: epoch  3, batch    14 | loss: 6.6939440Losses:  6.670161724090576 1.9761015176773071
CurrentTrain: epoch  3, batch    15 | loss: 6.6701617Losses:  7.018455982208252 1.9324069023132324
CurrentTrain: epoch  3, batch    16 | loss: 7.0184560Losses:  6.976114749908447 1.6980727910995483
CurrentTrain: epoch  3, batch    17 | loss: 6.9761147Losses:  6.881618499755859 1.8327008485794067
CurrentTrain: epoch  3, batch    18 | loss: 6.8816185Losses:  6.809058666229248 1.8662962913513184
CurrentTrain: epoch  3, batch    19 | loss: 6.8090587Losses:  6.652817726135254 1.846906065940857
CurrentTrain: epoch  3, batch    20 | loss: 6.6528177Losses:  6.771056652069092 1.9929332733154297
CurrentTrain: epoch  3, batch    21 | loss: 6.7710567Losses:  6.600425720214844 1.7305347919464111
CurrentTrain: epoch  3, batch    22 | loss: 6.6004257Losses:  6.726922988891602 2.0181427001953125
CurrentTrain: epoch  3, batch    23 | loss: 6.7269230Losses:  6.344959259033203 1.6293270587921143
CurrentTrain: epoch  3, batch    24 | loss: 6.3449593Losses:  6.669261455535889 2.0775651931762695
CurrentTrain: epoch  3, batch    25 | loss: 6.6692615Losses:  6.870509147644043 2.081315040588379
CurrentTrain: epoch  3, batch    26 | loss: 6.8705091Losses:  6.6817307472229 1.9439805746078491
CurrentTrain: epoch  3, batch    27 | loss: 6.6817307Losses:  6.935171604156494 2.11594295501709
CurrentTrain: epoch  3, batch    28 | loss: 6.9351716Losses:  7.041574001312256 1.8559404611587524
CurrentTrain: epoch  3, batch    29 | loss: 7.0415740Losses:  7.464765548706055 2.027383804321289
CurrentTrain: epoch  3, batch    30 | loss: 7.4647655Losses:  6.317728042602539 1.843097448348999
CurrentTrain: epoch  3, batch    31 | loss: 6.3177280Losses:  6.46510648727417 1.6254944801330566
CurrentTrain: epoch  3, batch    32 | loss: 6.4651065Losses:  6.646875858306885 1.8099675178527832
CurrentTrain: epoch  3, batch    33 | loss: 6.6468759Losses:  6.509832859039307 1.8943172693252563
CurrentTrain: epoch  3, batch    34 | loss: 6.5098329Losses:  6.559349536895752 1.942275047302246
CurrentTrain: epoch  3, batch    35 | loss: 6.5593495Losses:  7.27908992767334 1.9523460865020752
CurrentTrain: epoch  3, batch    36 | loss: 7.2790899Losses:  6.63856840133667 1.8668251037597656
CurrentTrain: epoch  3, batch    37 | loss: 6.6385684Losses:  6.448235511779785 1.9483805894851685
CurrentTrain: epoch  3, batch    38 | loss: 6.4482355Losses:  7.016648769378662 1.6147536039352417
CurrentTrain: epoch  3, batch    39 | loss: 7.0166488Losses:  6.611709117889404 1.9923176765441895
CurrentTrain: epoch  3, batch    40 | loss: 6.6117091Losses:  6.173413276672363 1.7372515201568604
CurrentTrain: epoch  3, batch    41 | loss: 6.1734133Losses:  6.688069820404053 1.6958712339401245
CurrentTrain: epoch  3, batch    42 | loss: 6.6880698Losses:  6.511295318603516 1.793018102645874
CurrentTrain: epoch  3, batch    43 | loss: 6.5112953Losses:  6.914636611938477 2.002749443054199
CurrentTrain: epoch  3, batch    44 | loss: 6.9146366Losses:  6.631893157958984 1.743890643119812
CurrentTrain: epoch  3, batch    45 | loss: 6.6318932Losses:  6.363622665405273 1.948359727859497
CurrentTrain: epoch  3, batch    46 | loss: 6.3636227Losses:  6.5718278884887695 1.713171362876892
CurrentTrain: epoch  3, batch    47 | loss: 6.5718279Losses:  6.062001705169678 1.5492005348205566
CurrentTrain: epoch  3, batch    48 | loss: 6.0620017Losses:  6.272857666015625 1.5924465656280518
CurrentTrain: epoch  3, batch    49 | loss: 6.2728577Losses:  6.530195236206055 1.9683442115783691
CurrentTrain: epoch  3, batch    50 | loss: 6.5301952Losses:  6.554971694946289 1.726094365119934
CurrentTrain: epoch  3, batch    51 | loss: 6.5549717Losses:  6.394126892089844 1.8427753448486328
CurrentTrain: epoch  3, batch    52 | loss: 6.3941269Losses:  7.042172431945801 1.9025371074676514
CurrentTrain: epoch  3, batch    53 | loss: 7.0421724Losses:  6.40342378616333 1.913938045501709
CurrentTrain: epoch  3, batch    54 | loss: 6.4034238Losses:  6.681221008300781 2.077346086502075
CurrentTrain: epoch  3, batch    55 | loss: 6.6812210Losses:  6.2807111740112305 1.8300249576568604
CurrentTrain: epoch  3, batch    56 | loss: 6.2807112Losses:  6.695739269256592 1.8251910209655762
CurrentTrain: epoch  3, batch    57 | loss: 6.6957393Losses:  6.703760623931885 1.7887206077575684
CurrentTrain: epoch  3, batch    58 | loss: 6.7037606Losses:  6.168327808380127 1.5896495580673218
CurrentTrain: epoch  3, batch    59 | loss: 6.1683278Losses:  6.595422744750977 2.0313720703125
CurrentTrain: epoch  3, batch    60 | loss: 6.5954227Losses:  6.310705661773682 1.8707150220870972
CurrentTrain: epoch  3, batch    61 | loss: 6.3107057Losses:  5.811225891113281 1.385522484779358
CurrentTrain: epoch  3, batch    62 | loss: 5.8112259Losses:  5.887216567993164 1.5151805877685547
CurrentTrain: epoch  4, batch     0 | loss: 5.8872166Losses:  6.21807336807251 1.7362499237060547
CurrentTrain: epoch  4, batch     1 | loss: 6.2180734Losses:  6.37276029586792 1.848575234413147
CurrentTrain: epoch  4, batch     2 | loss: 6.3727603Losses:  6.004370212554932 1.6455574035644531
CurrentTrain: epoch  4, batch     3 | loss: 6.0043702Losses:  6.570563793182373 2.058382511138916
CurrentTrain: epoch  4, batch     4 | loss: 6.5705638Losses:  6.706182479858398 2.0801327228546143
CurrentTrain: epoch  4, batch     5 | loss: 6.7061825Losses:  6.3281025886535645 1.8155938386917114
CurrentTrain: epoch  4, batch     6 | loss: 6.3281026Losses:  6.083657264709473 1.661320686340332
CurrentTrain: epoch  4, batch     7 | loss: 6.0836573Losses:  6.458643913269043 2.049830436706543
CurrentTrain: epoch  4, batch     8 | loss: 6.4586439Losses:  6.24931526184082 1.816737413406372
CurrentTrain: epoch  4, batch     9 | loss: 6.2493153Losses:  6.252498626708984 1.7642029523849487
CurrentTrain: epoch  4, batch    10 | loss: 6.2524986Losses:  6.751798152923584 1.83653724193573
CurrentTrain: epoch  4, batch    11 | loss: 6.7517982Losses:  6.320676803588867 1.8231018781661987
CurrentTrain: epoch  4, batch    12 | loss: 6.3206768Losses:  6.678449630737305 1.962327241897583
CurrentTrain: epoch  4, batch    13 | loss: 6.6784496Losses:  5.884018898010254 1.6560159921646118
CurrentTrain: epoch  4, batch    14 | loss: 5.8840189Losses:  6.191773891448975 1.7666198015213013
CurrentTrain: epoch  4, batch    15 | loss: 6.1917739Losses:  6.259452819824219 1.8060429096221924
CurrentTrain: epoch  4, batch    16 | loss: 6.2594528Losses:  6.057002544403076 1.6961225271224976
CurrentTrain: epoch  4, batch    17 | loss: 6.0570025Losses:  6.346680641174316 1.9384005069732666
CurrentTrain: epoch  4, batch    18 | loss: 6.3466806Losses:  6.365659713745117 1.9355099201202393
CurrentTrain: epoch  4, batch    19 | loss: 6.3656597Losses:  6.568331718444824 1.7011882066726685
CurrentTrain: epoch  4, batch    20 | loss: 6.5683317Losses:  6.256993293762207 1.5869853496551514
CurrentTrain: epoch  4, batch    21 | loss: 6.2569933Losses:  6.024092197418213 1.759958267211914
CurrentTrain: epoch  4, batch    22 | loss: 6.0240922Losses:  5.708113193511963 1.4676443338394165
CurrentTrain: epoch  4, batch    23 | loss: 5.7081132Losses:  6.078693866729736 1.5428900718688965
CurrentTrain: epoch  4, batch    24 | loss: 6.0786939Losses:  6.393426895141602 1.9749717712402344
CurrentTrain: epoch  4, batch    25 | loss: 6.3934269Losses:  6.1579413414001465 1.8674216270446777
CurrentTrain: epoch  4, batch    26 | loss: 6.1579413Losses:  6.053882122039795 1.7549290657043457
CurrentTrain: epoch  4, batch    27 | loss: 6.0538821Losses:  6.204530715942383 1.790034294128418
CurrentTrain: epoch  4, batch    28 | loss: 6.2045307Losses:  6.135475158691406 1.7462832927703857
CurrentTrain: epoch  4, batch    29 | loss: 6.1354752Losses:  6.058748722076416 1.8011832237243652
CurrentTrain: epoch  4, batch    30 | loss: 6.0587487Losses:  6.184203624725342 1.7888789176940918
CurrentTrain: epoch  4, batch    31 | loss: 6.1842036Losses:  6.441059589385986 1.8658437728881836
CurrentTrain: epoch  4, batch    32 | loss: 6.4410596Losses:  6.420662879943848 1.9751943349838257
CurrentTrain: epoch  4, batch    33 | loss: 6.4206629Losses:  6.1280059814453125 1.7866218090057373
CurrentTrain: epoch  4, batch    34 | loss: 6.1280060Losses:  6.550311088562012 1.9747283458709717
CurrentTrain: epoch  4, batch    35 | loss: 6.5503111Losses:  6.08544397354126 1.6738405227661133
CurrentTrain: epoch  4, batch    36 | loss: 6.0854440Losses:  6.670385360717773 1.9538068771362305
CurrentTrain: epoch  4, batch    37 | loss: 6.6703854Losses:  6.169447422027588 1.7709859609603882
CurrentTrain: epoch  4, batch    38 | loss: 6.1694474Losses:  6.1557111740112305 1.8477363586425781
CurrentTrain: epoch  4, batch    39 | loss: 6.1557112Losses:  6.019868850708008 1.7500522136688232
CurrentTrain: epoch  4, batch    40 | loss: 6.0198689Losses:  6.120864391326904 1.8913378715515137
CurrentTrain: epoch  4, batch    41 | loss: 6.1208644Losses:  6.7112956047058105 1.8145767450332642
CurrentTrain: epoch  4, batch    42 | loss: 6.7112956Losses:  6.298699378967285 1.5646069049835205
CurrentTrain: epoch  4, batch    43 | loss: 6.2986994Losses:  5.771997451782227 1.5887691974639893
CurrentTrain: epoch  4, batch    44 | loss: 5.7719975Losses:  6.575344085693359 1.665632963180542
CurrentTrain: epoch  4, batch    45 | loss: 6.5753441Losses:  6.354480266571045 1.8243274688720703
CurrentTrain: epoch  4, batch    46 | loss: 6.3544803Losses:  6.238836288452148 1.8315269947052002
CurrentTrain: epoch  4, batch    47 | loss: 6.2388363Losses:  6.283285617828369 1.9714179039001465
CurrentTrain: epoch  4, batch    48 | loss: 6.2832856Losses:  6.2238264083862305 1.882507562637329
CurrentTrain: epoch  4, batch    49 | loss: 6.2238264Losses:  6.535932540893555 2.078216791152954
CurrentTrain: epoch  4, batch    50 | loss: 6.5359325Losses:  6.0893449783325195 1.7156949043273926
CurrentTrain: epoch  4, batch    51 | loss: 6.0893450Losses:  5.940363883972168 1.6606719493865967
CurrentTrain: epoch  4, batch    52 | loss: 5.9403639Losses:  6.403765678405762 1.8171360492706299
CurrentTrain: epoch  4, batch    53 | loss: 6.4037657Losses:  6.081075668334961 1.688472032546997
CurrentTrain: epoch  4, batch    54 | loss: 6.0810757Losses:  6.156402587890625 1.8810455799102783
CurrentTrain: epoch  4, batch    55 | loss: 6.1564026Losses:  5.977061748504639 1.66011381149292
CurrentTrain: epoch  4, batch    56 | loss: 5.9770617Losses:  6.339573383331299 2.032688617706299
CurrentTrain: epoch  4, batch    57 | loss: 6.3395734Losses:  6.054948329925537 1.7786136865615845
CurrentTrain: epoch  4, batch    58 | loss: 6.0549483Losses:  6.076724052429199 1.785834550857544
CurrentTrain: epoch  4, batch    59 | loss: 6.0767241Losses:  6.04503059387207 1.7795592546463013
CurrentTrain: epoch  4, batch    60 | loss: 6.0450306Losses:  6.248202323913574 1.843491792678833
CurrentTrain: epoch  4, batch    61 | loss: 6.2482023Losses:  5.72178840637207 1.4036433696746826
CurrentTrain: epoch  4, batch    62 | loss: 5.7217884Losses:  6.431474208831787 2.0145912170410156
CurrentTrain: epoch  5, batch     0 | loss: 6.4314742Losses:  6.279140472412109 1.946413278579712
CurrentTrain: epoch  5, batch     1 | loss: 6.2791405Losses:  5.912048816680908 1.63261079788208
CurrentTrain: epoch  5, batch     2 | loss: 5.9120488Losses:  6.054065227508545 1.7963333129882812
CurrentTrain: epoch  5, batch     3 | loss: 6.0540652Losses:  6.08433198928833 1.711991310119629
CurrentTrain: epoch  5, batch     4 | loss: 6.0843320Losses:  6.1606221199035645 1.8032938241958618
CurrentTrain: epoch  5, batch     5 | loss: 6.1606221Losses:  6.224041938781738 1.8695688247680664
CurrentTrain: epoch  5, batch     6 | loss: 6.2240419Losses:  6.102867603302002 1.8993479013442993
CurrentTrain: epoch  5, batch     7 | loss: 6.1028676Losses:  6.204242706298828 1.86326003074646
CurrentTrain: epoch  5, batch     8 | loss: 6.2042427Losses:  5.901163578033447 1.659401774406433
CurrentTrain: epoch  5, batch     9 | loss: 5.9011636Losses:  6.576297760009766 1.9726605415344238
CurrentTrain: epoch  5, batch    10 | loss: 6.5762978Losses:  6.229799747467041 1.9502476453781128
CurrentTrain: epoch  5, batch    11 | loss: 6.2297997Losses:  6.0528974533081055 1.842571496963501
CurrentTrain: epoch  5, batch    12 | loss: 6.0528975Losses:  5.96127462387085 1.7325042486190796
CurrentTrain: epoch  5, batch    13 | loss: 5.9612746Losses:  6.218916893005371 2.0544846057891846
CurrentTrain: epoch  5, batch    14 | loss: 6.2189169Losses:  6.0038018226623535 1.7849324941635132
CurrentTrain: epoch  5, batch    15 | loss: 6.0038018Losses:  6.296217441558838 1.99726140499115
CurrentTrain: epoch  5, batch    16 | loss: 6.2962174Losses:  6.115744590759277 1.9318711757659912
CurrentTrain: epoch  5, batch    17 | loss: 6.1157446Losses:  6.138819694519043 1.856137990951538
CurrentTrain: epoch  5, batch    18 | loss: 6.1388197Losses:  6.03074836730957 1.7927215099334717
CurrentTrain: epoch  5, batch    19 | loss: 6.0307484Losses:  6.016819953918457 1.8159419298171997
CurrentTrain: epoch  5, batch    20 | loss: 6.0168200Losses:  6.036807060241699 1.7734439373016357
CurrentTrain: epoch  5, batch    21 | loss: 6.0368071Losses:  5.970710754394531 1.6994761228561401
CurrentTrain: epoch  5, batch    22 | loss: 5.9707108Losses:  6.090654373168945 1.7944729328155518
CurrentTrain: epoch  5, batch    23 | loss: 6.0906544Losses:  5.909414291381836 1.691051721572876
CurrentTrain: epoch  5, batch    24 | loss: 5.9094143Losses:  6.118859767913818 1.9099597930908203
CurrentTrain: epoch  5, batch    25 | loss: 6.1188598Losses:  6.138209342956543 1.8677031993865967
CurrentTrain: epoch  5, batch    26 | loss: 6.1382093Losses:  5.976914405822754 1.7581627368927002
CurrentTrain: epoch  5, batch    27 | loss: 5.9769144Losses:  6.121678352355957 1.9392017126083374
CurrentTrain: epoch  5, batch    28 | loss: 6.1216784Losses:  6.095919132232666 1.9222407341003418
CurrentTrain: epoch  5, batch    29 | loss: 6.0959191Losses:  5.813635349273682 1.6410924196243286
CurrentTrain: epoch  5, batch    30 | loss: 5.8136353Losses:  5.889769554138184 1.7230956554412842
CurrentTrain: epoch  5, batch    31 | loss: 5.8897696Losses:  6.007246494293213 1.7398791313171387
CurrentTrain: epoch  5, batch    32 | loss: 6.0072465Losses:  5.69090461730957 1.5690391063690186
CurrentTrain: epoch  5, batch    33 | loss: 5.6909046Losses:  6.03814697265625 1.7985715866088867
CurrentTrain: epoch  5, batch    34 | loss: 6.0381470Losses:  5.8147077560424805 1.6727712154388428
CurrentTrain: epoch  5, batch    35 | loss: 5.8147078Losses:  5.861965179443359 1.686461091041565
CurrentTrain: epoch  5, batch    36 | loss: 5.8619652Losses:  5.887089729309082 1.757961392402649
CurrentTrain: epoch  5, batch    37 | loss: 5.8870897Losses:  5.933443546295166 1.730741024017334
CurrentTrain: epoch  5, batch    38 | loss: 5.9334435Losses:  6.193759441375732 1.89797842502594
CurrentTrain: epoch  5, batch    39 | loss: 6.1937594Losses:  6.1308913230896 2.0137877464294434
CurrentTrain: epoch  5, batch    40 | loss: 6.1308913Losses:  5.950004577636719 1.7556734085083008
CurrentTrain: epoch  5, batch    41 | loss: 5.9500046Losses:  6.225228309631348 1.9028618335723877
CurrentTrain: epoch  5, batch    42 | loss: 6.2252283Losses:  6.155001163482666 1.744866967201233
CurrentTrain: epoch  5, batch    43 | loss: 6.1550012Losses:  5.981536865234375 1.8186542987823486
CurrentTrain: epoch  5, batch    44 | loss: 5.9815369Losses:  5.952571868896484 1.8179274797439575
CurrentTrain: epoch  5, batch    45 | loss: 5.9525719Losses:  6.037843704223633 1.8846065998077393
CurrentTrain: epoch  5, batch    46 | loss: 6.0378437Losses:  6.041765213012695 1.888474702835083
CurrentTrain: epoch  5, batch    47 | loss: 6.0417652Losses:  6.08902645111084 1.9190070629119873
CurrentTrain: epoch  5, batch    48 | loss: 6.0890265Losses:  6.055604934692383 1.8859779834747314
CurrentTrain: epoch  5, batch    49 | loss: 6.0556049Losses:  5.971236228942871 1.8522014617919922
CurrentTrain: epoch  5, batch    50 | loss: 5.9712362Losses:  5.797361373901367 1.6709363460540771
CurrentTrain: epoch  5, batch    51 | loss: 5.7973614Losses:  6.217571258544922 1.9920637607574463
CurrentTrain: epoch  5, batch    52 | loss: 6.2175713Losses:  6.015458106994629 1.830359697341919
CurrentTrain: epoch  5, batch    53 | loss: 6.0154581Losses:  6.117312431335449 2.002272844314575
CurrentTrain: epoch  5, batch    54 | loss: 6.1173124Losses:  6.154756546020508 1.8968431949615479
CurrentTrain: epoch  5, batch    55 | loss: 6.1547565Losses:  6.151462554931641 1.9931952953338623
CurrentTrain: epoch  5, batch    56 | loss: 6.1514626Losses:  5.900897979736328 1.6693570613861084
CurrentTrain: epoch  5, batch    57 | loss: 5.9008980Losses:  6.066648483276367 1.7240586280822754
CurrentTrain: epoch  5, batch    58 | loss: 6.0666485Losses:  5.900876045227051 1.6911427974700928
CurrentTrain: epoch  5, batch    59 | loss: 5.9008760Losses:  5.889172554016113 1.7735700607299805
CurrentTrain: epoch  5, batch    60 | loss: 5.8891726Losses:  5.839207172393799 1.7168890237808228
CurrentTrain: epoch  5, batch    61 | loss: 5.8392072Losses:  5.750743389129639 1.54616117477417
CurrentTrain: epoch  5, batch    62 | loss: 5.7507434Losses:  5.999195575714111 1.7791523933410645
CurrentTrain: epoch  6, batch     0 | loss: 5.9991956Losses:  5.921285629272461 1.8182144165039062
CurrentTrain: epoch  6, batch     1 | loss: 5.9212856Losses:  5.813108444213867 1.7283458709716797
CurrentTrain: epoch  6, batch     2 | loss: 5.8131084Losses:  6.308619499206543 2.0491409301757812
CurrentTrain: epoch  6, batch     3 | loss: 6.3086195Losses:  5.763883590698242 1.602524995803833
CurrentTrain: epoch  6, batch     4 | loss: 5.7638836Losses:  6.03411340713501 1.8788894414901733
CurrentTrain: epoch  6, batch     5 | loss: 6.0341134Losses:  6.067920684814453 1.9143617153167725
CurrentTrain: epoch  6, batch     6 | loss: 6.0679207Losses:  5.884965419769287 1.7206931114196777
CurrentTrain: epoch  6, batch     7 | loss: 5.8849654Losses:  6.039163589477539 1.934828758239746
CurrentTrain: epoch  6, batch     8 | loss: 6.0391636Losses:  5.669493675231934 1.5329101085662842
CurrentTrain: epoch  6, batch     9 | loss: 5.6694937Losses:  6.050229072570801 1.9151967763900757
CurrentTrain: epoch  6, batch    10 | loss: 6.0502291Losses:  5.7336530685424805 1.5632760524749756
CurrentTrain: epoch  6, batch    11 | loss: 5.7336531Losses:  6.199094295501709 2.0042948722839355
CurrentTrain: epoch  6, batch    12 | loss: 6.1990943Losses:  5.793767929077148 1.6828410625457764
CurrentTrain: epoch  6, batch    13 | loss: 5.7937679Losses:  6.112416744232178 1.960150122642517
CurrentTrain: epoch  6, batch    14 | loss: 6.1124167Losses:  6.005505084991455 1.8599276542663574
CurrentTrain: epoch  6, batch    15 | loss: 6.0055051Losses:  6.0550737380981445 1.9258891344070435
CurrentTrain: epoch  6, batch    16 | loss: 6.0550737Losses:  6.0119194984436035 1.8831177949905396
CurrentTrain: epoch  6, batch    17 | loss: 6.0119195Losses:  5.401279449462891 1.2537200450897217
CurrentTrain: epoch  6, batch    18 | loss: 5.4012794Losses:  6.023820877075195 1.9037220478057861
CurrentTrain: epoch  6, batch    19 | loss: 6.0238209Losses:  6.084575176239014 1.9716229438781738
CurrentTrain: epoch  6, batch    20 | loss: 6.0845752Losses:  5.941834926605225 1.7501254081726074
CurrentTrain: epoch  6, batch    21 | loss: 5.9418349Losses:  5.850827217102051 1.6624372005462646
CurrentTrain: epoch  6, batch    22 | loss: 5.8508272Losses:  5.943393707275391 1.4871199131011963
CurrentTrain: epoch  6, batch    23 | loss: 5.9433937Losses:  5.872702598571777 1.709871530532837
CurrentTrain: epoch  6, batch    24 | loss: 5.8727026Losses:  6.111442565917969 1.8934504985809326
CurrentTrain: epoch  6, batch    25 | loss: 6.1114426Losses:  6.048969268798828 1.8875455856323242
CurrentTrain: epoch  6, batch    26 | loss: 6.0489693Losses:  5.785801887512207 1.6751925945281982
CurrentTrain: epoch  6, batch    27 | loss: 5.7858019Losses:  5.937338829040527 1.850017786026001
CurrentTrain: epoch  6, batch    28 | loss: 5.9373388Losses:  6.019501686096191 1.8951056003570557
CurrentTrain: epoch  6, batch    29 | loss: 6.0195017Losses:  5.963530540466309 1.8689781427383423
CurrentTrain: epoch  6, batch    30 | loss: 5.9635305Losses:  5.547761917114258 1.5078356266021729
CurrentTrain: epoch  6, batch    31 | loss: 5.5477619Losses:  5.887838363647461 1.728567123413086
CurrentTrain: epoch  6, batch    32 | loss: 5.8878384Losses:  6.042941093444824 1.9112833738327026
CurrentTrain: epoch  6, batch    33 | loss: 6.0429411Losses:  5.926675796508789 1.7854974269866943
CurrentTrain: epoch  6, batch    34 | loss: 5.9266758Losses:  5.688220024108887 1.5601598024368286
CurrentTrain: epoch  6, batch    35 | loss: 5.6882200Losses:  5.598615646362305 1.4057118892669678
CurrentTrain: epoch  6, batch    36 | loss: 5.5986156Losses:  6.034342288970947 1.899011254310608
CurrentTrain: epoch  6, batch    37 | loss: 6.0343423Losses:  6.140940189361572 1.9851495027542114
CurrentTrain: epoch  6, batch    38 | loss: 6.1409402Losses:  5.742857933044434 1.667099118232727
CurrentTrain: epoch  6, batch    39 | loss: 5.7428579Losses:  6.080081939697266 1.9250727891921997
CurrentTrain: epoch  6, batch    40 | loss: 6.0800819Losses:  6.133876800537109 2.0113630294799805
CurrentTrain: epoch  6, batch    41 | loss: 6.1338768Losses:  6.005999565124512 1.9261444807052612
CurrentTrain: epoch  6, batch    42 | loss: 6.0059996Losses:  5.742436408996582 1.6874825954437256
CurrentTrain: epoch  6, batch    43 | loss: 5.7424364Losses:  5.813397407531738 1.7323766946792603
CurrentTrain: epoch  6, batch    44 | loss: 5.8133974Losses:  5.9632415771484375 1.83478844165802
CurrentTrain: epoch  6, batch    45 | loss: 5.9632416Losses:  5.830024719238281 1.7147526741027832
CurrentTrain: epoch  6, batch    46 | loss: 5.8300247Losses:  5.7106804847717285 1.615735411643982
CurrentTrain: epoch  6, batch    47 | loss: 5.7106805Losses:  6.076376914978027 1.9916725158691406
CurrentTrain: epoch  6, batch    48 | loss: 6.0763769Losses:  5.645927906036377 1.5571235418319702
CurrentTrain: epoch  6, batch    49 | loss: 5.6459279Losses:  6.0650763511657715 1.9438294172286987
CurrentTrain: epoch  6, batch    50 | loss: 6.0650764Losses:  5.9612603187561035 1.8986588716506958
CurrentTrain: epoch  6, batch    51 | loss: 5.9612603Losses:  5.971375465393066 1.8821852207183838
CurrentTrain: epoch  6, batch    52 | loss: 5.9713755Losses:  5.405048847198486 1.2966617345809937
CurrentTrain: epoch  6, batch    53 | loss: 5.4050488Losses:  5.907800197601318 1.8288313150405884
CurrentTrain: epoch  6, batch    54 | loss: 5.9078002Losses:  5.764235973358154 1.7011303901672363
CurrentTrain: epoch  6, batch    55 | loss: 5.7642360Losses:  5.673863887786865 1.5410161018371582
CurrentTrain: epoch  6, batch    56 | loss: 5.6738639Losses:  5.968149185180664 1.9128819704055786
CurrentTrain: epoch  6, batch    57 | loss: 5.9681492Losses:  5.844308853149414 1.7748793363571167
CurrentTrain: epoch  6, batch    58 | loss: 5.8443089Losses:  5.978575706481934 1.9034671783447266
CurrentTrain: epoch  6, batch    59 | loss: 5.9785757Losses:  5.630920886993408 1.5052722692489624
CurrentTrain: epoch  6, batch    60 | loss: 5.6309209Losses:  5.913186073303223 1.763723373413086
CurrentTrain: epoch  6, batch    61 | loss: 5.9131861Losses:  5.593138694763184 1.5347011089324951
CurrentTrain: epoch  6, batch    62 | loss: 5.5931387Losses:  5.761138439178467 1.656265139579773
CurrentTrain: epoch  7, batch     0 | loss: 5.7611384Losses:  5.7466535568237305 1.6499626636505127
CurrentTrain: epoch  7, batch     1 | loss: 5.7466536Losses:  5.849307537078857 1.757723331451416
CurrentTrain: epoch  7, batch     2 | loss: 5.8493075Losses:  5.97685432434082 1.9061274528503418
CurrentTrain: epoch  7, batch     3 | loss: 5.9768543Losses:  5.672511577606201 1.5846649408340454
CurrentTrain: epoch  7, batch     4 | loss: 5.6725116Losses:  5.867257595062256 1.814987063407898
CurrentTrain: epoch  7, batch     5 | loss: 5.8672576Losses:  5.90126371383667 1.8384122848510742
CurrentTrain: epoch  7, batch     6 | loss: 5.9012637Losses:  6.014708995819092 1.8818411827087402
CurrentTrain: epoch  7, batch     7 | loss: 6.0147090Losses:  5.622808933258057 1.6139140129089355
CurrentTrain: epoch  7, batch     8 | loss: 5.6228089Losses:  5.786567687988281 1.726954698562622
CurrentTrain: epoch  7, batch     9 | loss: 5.7865677Losses:  5.845644950866699 1.7906453609466553
CurrentTrain: epoch  7, batch    10 | loss: 5.8456450Losses:  5.697584629058838 1.677901268005371
CurrentTrain: epoch  7, batch    11 | loss: 5.6975846Losses:  5.560731887817383 1.5232733488082886
CurrentTrain: epoch  7, batch    12 | loss: 5.5607319Losses:  5.738338470458984 1.668191909790039
CurrentTrain: epoch  7, batch    13 | loss: 5.7383385Losses:  5.723446846008301 1.6910830736160278
CurrentTrain: epoch  7, batch    14 | loss: 5.7234468Losses:  5.793457984924316 1.785485863685608
CurrentTrain: epoch  7, batch    15 | loss: 5.7934580Losses:  5.882172107696533 1.7842631340026855
CurrentTrain: epoch  7, batch    16 | loss: 5.8821721Losses:  6.159634590148926 1.8054592609405518
CurrentTrain: epoch  7, batch    17 | loss: 6.1596346Losses:  5.957415580749512 1.8949737548828125
CurrentTrain: epoch  7, batch    18 | loss: 5.9574156Losses:  5.968825340270996 1.864095687866211
CurrentTrain: epoch  7, batch    19 | loss: 5.9688253Losses:  5.737784385681152 1.6414034366607666
CurrentTrain: epoch  7, batch    20 | loss: 5.7377844Losses:  5.7198710441589355 1.6809347867965698
CurrentTrain: epoch  7, batch    21 | loss: 5.7198710Losses:  5.609534740447998 1.5525918006896973
CurrentTrain: epoch  7, batch    22 | loss: 5.6095347Losses:  5.614365577697754 1.5358107089996338
CurrentTrain: epoch  7, batch    23 | loss: 5.6143656Losses:  5.821514129638672 1.7746350765228271
CurrentTrain: epoch  7, batch    24 | loss: 5.8215141Losses:  5.718264579772949 1.5432243347167969
CurrentTrain: epoch  7, batch    25 | loss: 5.7182646Losses:  5.703431129455566 1.6702312231063843
CurrentTrain: epoch  7, batch    26 | loss: 5.7034311Losses:  5.867399215698242 1.8164902925491333
CurrentTrain: epoch  7, batch    27 | loss: 5.8673992Losses:  5.840989112854004 1.771601915359497
CurrentTrain: epoch  7, batch    28 | loss: 5.8409891Losses:  5.694326400756836 1.6206881999969482
CurrentTrain: epoch  7, batch    29 | loss: 5.6943264Losses:  5.8713154792785645 1.7790507078170776
CurrentTrain: epoch  7, batch    30 | loss: 5.8713155Losses:  5.807224273681641 1.7358314990997314
CurrentTrain: epoch  7, batch    31 | loss: 5.8072243Losses:  5.541536331176758 1.4718689918518066
CurrentTrain: epoch  7, batch    32 | loss: 5.5415363Losses:  5.841276168823242 1.6652190685272217
CurrentTrain: epoch  7, batch    33 | loss: 5.8412762Losses:  5.616631507873535 1.5079108476638794
CurrentTrain: epoch  7, batch    34 | loss: 5.6166315Losses:  5.820728302001953 1.7750084400177002
CurrentTrain: epoch  7, batch    35 | loss: 5.8207283Losses:  5.813960075378418 1.6910921335220337
CurrentTrain: epoch  7, batch    36 | loss: 5.8139601Losses:  5.688661575317383 1.6562254428863525
CurrentTrain: epoch  7, batch    37 | loss: 5.6886616Losses:  5.972941875457764 1.9095563888549805
CurrentTrain: epoch  7, batch    38 | loss: 5.9729419Losses:  5.685430526733398 1.660951852798462
CurrentTrain: epoch  7, batch    39 | loss: 5.6854305Losses:  5.8902997970581055 1.835410237312317
CurrentTrain: epoch  7, batch    40 | loss: 5.8902998Losses:  5.678589344024658 1.6492255926132202
CurrentTrain: epoch  7, batch    41 | loss: 5.6785893Losses:  5.746446132659912 1.7104653120040894
CurrentTrain: epoch  7, batch    42 | loss: 5.7464461Losses:  5.733550071716309 1.6807631254196167
CurrentTrain: epoch  7, batch    43 | loss: 5.7335501Losses:  5.827446937561035 1.7840063571929932
CurrentTrain: epoch  7, batch    44 | loss: 5.8274469Losses:  5.826574802398682 1.838968276977539
CurrentTrain: epoch  7, batch    45 | loss: 5.8265748Losses:  5.676600456237793 1.626535415649414
CurrentTrain: epoch  7, batch    46 | loss: 5.6766005Losses:  5.823692321777344 1.7821635007858276
CurrentTrain: epoch  7, batch    47 | loss: 5.8236923Losses:  5.885182857513428 1.8508726358413696
CurrentTrain: epoch  7, batch    48 | loss: 5.8851829Losses:  5.887425422668457 1.8671352863311768
CurrentTrain: epoch  7, batch    49 | loss: 5.8874254Losses:  5.545012950897217 1.5790867805480957
CurrentTrain: epoch  7, batch    50 | loss: 5.5450130Losses:  5.901548862457275 1.8650661706924438
CurrentTrain: epoch  7, batch    51 | loss: 5.9015489Losses:  5.675905704498291 1.569841742515564
CurrentTrain: epoch  7, batch    52 | loss: 5.6759057Losses:  5.856046676635742 1.7776892185211182
CurrentTrain: epoch  7, batch    53 | loss: 5.8560467Losses:  5.75360107421875 1.680849313735962
CurrentTrain: epoch  7, batch    54 | loss: 5.7536011Losses:  5.6724982261657715 1.6285597085952759
CurrentTrain: epoch  7, batch    55 | loss: 5.6724982Losses:  5.526017189025879 1.4732273817062378
CurrentTrain: epoch  7, batch    56 | loss: 5.5260172Losses:  5.853886604309082 1.8377220630645752
CurrentTrain: epoch  7, batch    57 | loss: 5.8538866Losses:  5.902971267700195 1.8825054168701172
CurrentTrain: epoch  7, batch    58 | loss: 5.9029713Losses:  5.571748733520508 1.5149567127227783
CurrentTrain: epoch  7, batch    59 | loss: 5.5717487Losses:  5.9360575675964355 1.8695926666259766
CurrentTrain: epoch  7, batch    60 | loss: 5.9360576Losses:  5.776501655578613 1.7203786373138428
CurrentTrain: epoch  7, batch    61 | loss: 5.7765017Losses:  5.5692830085754395 1.4980195760726929
CurrentTrain: epoch  7, batch    62 | loss: 5.5692830Losses:  5.996407508850098 1.9533427953720093
CurrentTrain: epoch  8, batch     0 | loss: 5.9964075Losses:  5.846763610839844 1.778738021850586
CurrentTrain: epoch  8, batch     1 | loss: 5.8467636Losses:  5.808831214904785 1.7519810199737549
CurrentTrain: epoch  8, batch     2 | loss: 5.8088312Losses:  5.628342628479004 1.5763423442840576
CurrentTrain: epoch  8, batch     3 | loss: 5.6283426Losses:  5.630580902099609 1.6349756717681885
CurrentTrain: epoch  8, batch     4 | loss: 5.6305809Losses:  5.789929389953613 1.747631311416626
CurrentTrain: epoch  8, batch     5 | loss: 5.7899294Losses:  5.852343559265137 1.8082911968231201
CurrentTrain: epoch  8, batch     6 | loss: 5.8523436Losses:  5.876844882965088 1.8140250444412231
CurrentTrain: epoch  8, batch     7 | loss: 5.8768449Losses:  5.819612503051758 1.7754292488098145
CurrentTrain: epoch  8, batch     8 | loss: 5.8196125Losses:  5.883080005645752 1.8372424840927124
CurrentTrain: epoch  8, batch     9 | loss: 5.8830800Losses:  5.731878280639648 1.724956750869751
CurrentTrain: epoch  8, batch    10 | loss: 5.7318783Losses:  5.701326370239258 1.6667234897613525
CurrentTrain: epoch  8, batch    11 | loss: 5.7013264Losses:  5.808415412902832 1.7354583740234375
CurrentTrain: epoch  8, batch    12 | loss: 5.8084154Losses:  5.715977668762207 1.6687943935394287
CurrentTrain: epoch  8, batch    13 | loss: 5.7159777Losses:  5.777222633361816 1.7546355724334717
CurrentTrain: epoch  8, batch    14 | loss: 5.7772226Losses:  5.661332130432129 1.6402301788330078
CurrentTrain: epoch  8, batch    15 | loss: 5.6613321Losses:  5.685176849365234 1.6822054386138916
CurrentTrain: epoch  8, batch    16 | loss: 5.6851768Losses:  5.863493919372559 1.8232136964797974
CurrentTrain: epoch  8, batch    17 | loss: 5.8634939Losses:  5.832529544830322 1.7789264917373657
CurrentTrain: epoch  8, batch    18 | loss: 5.8325295Losses:  5.840590476989746 1.7755651473999023
CurrentTrain: epoch  8, batch    19 | loss: 5.8405905Losses:  5.54635763168335 1.5987354516983032
CurrentTrain: epoch  8, batch    20 | loss: 5.5463576Losses:  5.7601728439331055 1.7354624271392822
CurrentTrain: epoch  8, batch    21 | loss: 5.7601728Losses:  5.52962589263916 1.5118658542633057
CurrentTrain: epoch  8, batch    22 | loss: 5.5296259Losses:  5.997030258178711 1.9565889835357666
CurrentTrain: epoch  8, batch    23 | loss: 5.9970303Losses:  6.041498184204102 1.993729591369629
CurrentTrain: epoch  8, batch    24 | loss: 6.0414982Losses:  5.54979944229126 1.5359200239181519
CurrentTrain: epoch  8, batch    25 | loss: 5.5497994Losses:  5.668606758117676 1.618224859237671
CurrentTrain: epoch  8, batch    26 | loss: 5.6686068Losses:  5.8070783615112305 1.7230770587921143
CurrentTrain: epoch  8, batch    27 | loss: 5.8070784Losses:  5.9728498458862305 1.9525089263916016
CurrentTrain: epoch  8, batch    28 | loss: 5.9728498Losses:  5.831330299377441 1.7866132259368896
CurrentTrain: epoch  8, batch    29 | loss: 5.8313303Losses:  5.702966690063477 1.6848167181015015
CurrentTrain: epoch  8, batch    30 | loss: 5.7029667Losses:  5.627455711364746 1.6002247333526611
CurrentTrain: epoch  8, batch    31 | loss: 5.6274557Losses:  5.590368747711182 1.5457301139831543
CurrentTrain: epoch  8, batch    32 | loss: 5.5903687Losses:  5.708993434906006 1.6590142250061035
CurrentTrain: epoch  8, batch    33 | loss: 5.7089934Losses:  6.014440536499023 1.9777557849884033
CurrentTrain: epoch  8, batch    34 | loss: 6.0144405Losses:  5.7557692527771 1.72809636592865
CurrentTrain: epoch  8, batch    35 | loss: 5.7557693Losses:  5.65716552734375 1.614382028579712
CurrentTrain: epoch  8, batch    36 | loss: 5.6571655Losses:  5.785551071166992 1.7164937257766724
CurrentTrain: epoch  8, batch    37 | loss: 5.7855511Losses:  5.648684501647949 1.661008596420288
CurrentTrain: epoch  8, batch    38 | loss: 5.6486845Losses:  5.844386100769043 1.7705812454223633
CurrentTrain: epoch  8, batch    39 | loss: 5.8443861Losses:  5.955636024475098 1.863647699356079
CurrentTrain: epoch  8, batch    40 | loss: 5.9556360Losses:  5.898650169372559 1.8653712272644043
CurrentTrain: epoch  8, batch    41 | loss: 5.8986502Losses:  5.81341552734375 1.7771949768066406
CurrentTrain: epoch  8, batch    42 | loss: 5.8134155Losses:  5.907005310058594 1.8245298862457275
CurrentTrain: epoch  8, batch    43 | loss: 5.9070053Losses:  5.875073432922363 1.8318312168121338
CurrentTrain: epoch  8, batch    44 | loss: 5.8750734Losses:  5.639093399047852 1.5666911602020264
CurrentTrain: epoch  8, batch    45 | loss: 5.6390934Losses:  5.552741527557373 1.5674951076507568
CurrentTrain: epoch  8, batch    46 | loss: 5.5527415Losses:  5.544931411743164 1.5072782039642334
CurrentTrain: epoch  8, batch    47 | loss: 5.5449314Losses:  5.559290885925293 1.5144875049591064
CurrentTrain: epoch  8, batch    48 | loss: 5.5592909Losses:  5.693362236022949 1.6734237670898438
CurrentTrain: epoch  8, batch    49 | loss: 5.6933622Losses:  5.752157211303711 1.7406485080718994
CurrentTrain: epoch  8, batch    50 | loss: 5.7521572Losses:  5.482007026672363 1.4384806156158447
CurrentTrain: epoch  8, batch    51 | loss: 5.4820070Losses:  5.9194793701171875 1.87935209274292
CurrentTrain: epoch  8, batch    52 | loss: 5.9194794Losses:  5.800753116607666 1.7583519220352173
CurrentTrain: epoch  8, batch    53 | loss: 5.8007531Losses:  5.889340877532959 1.8809250593185425
CurrentTrain: epoch  8, batch    54 | loss: 5.8893409Losses:  5.595772743225098 1.5938894748687744
CurrentTrain: epoch  8, batch    55 | loss: 5.5957727Losses:  5.675286293029785 1.6423861980438232
CurrentTrain: epoch  8, batch    56 | loss: 5.6752863Losses:  5.945577621459961 1.9258278608322144
CurrentTrain: epoch  8, batch    57 | loss: 5.9455776Losses:  5.6869354248046875 1.6486091613769531
CurrentTrain: epoch  8, batch    58 | loss: 5.6869354Losses:  5.831634521484375 1.8036773204803467
CurrentTrain: epoch  8, batch    59 | loss: 5.8316345Losses:  5.657118797302246 1.6842677593231201
CurrentTrain: epoch  8, batch    60 | loss: 5.6571188Losses:  5.89684534072876 1.8663253784179688
CurrentTrain: epoch  8, batch    61 | loss: 5.8968453Losses:  5.152054786682129 1.1504002809524536
CurrentTrain: epoch  8, batch    62 | loss: 5.1520548Losses:  5.6801581382751465 1.676510214805603
CurrentTrain: epoch  9, batch     0 | loss: 5.6801581Losses:  5.684943675994873 1.5940632820129395
CurrentTrain: epoch  9, batch     1 | loss: 5.6849437Losses:  5.87335205078125 1.8333882093429565
CurrentTrain: epoch  9, batch     2 | loss: 5.8733521Losses:  5.737220287322998 1.6765742301940918
CurrentTrain: epoch  9, batch     3 | loss: 5.7372203Losses:  5.543339729309082 1.522071361541748
CurrentTrain: epoch  9, batch     4 | loss: 5.5433397Losses:  5.701650619506836 1.6956377029418945
CurrentTrain: epoch  9, batch     5 | loss: 5.7016506Losses:  5.69639253616333 1.6617369651794434
CurrentTrain: epoch  9, batch     6 | loss: 5.6963925Losses:  5.687779426574707 1.6754281520843506
CurrentTrain: epoch  9, batch     7 | loss: 5.6877794Losses:  5.54680871963501 1.5624465942382812
CurrentTrain: epoch  9, batch     8 | loss: 5.5468087Losses:  5.776904106140137 1.760451078414917
CurrentTrain: epoch  9, batch     9 | loss: 5.7769041Losses:  5.867351531982422 1.8589800596237183
CurrentTrain: epoch  9, batch    10 | loss: 5.8673515Losses:  5.917065620422363 1.8758833408355713
CurrentTrain: epoch  9, batch    11 | loss: 5.9170656Losses:  5.850765228271484 1.8267226219177246
CurrentTrain: epoch  9, batch    12 | loss: 5.8507652Losses:  5.780649662017822 1.7454582452774048
CurrentTrain: epoch  9, batch    13 | loss: 5.7806497Losses:  6.004261016845703 1.9843838214874268
CurrentTrain: epoch  9, batch    14 | loss: 6.0042610Losses:  5.951753616333008 1.9005156755447388
CurrentTrain: epoch  9, batch    15 | loss: 5.9517536Losses:  5.800929069519043 1.7973358631134033
CurrentTrain: epoch  9, batch    16 | loss: 5.8009291Losses:  5.831592559814453 1.7801820039749146
CurrentTrain: epoch  9, batch    17 | loss: 5.8315926Losses:  5.724567413330078 1.7156598567962646
CurrentTrain: epoch  9, batch    18 | loss: 5.7245674Losses:  5.412812232971191 1.4479644298553467
CurrentTrain: epoch  9, batch    19 | loss: 5.4128122Losses:  5.759761333465576 1.7053524255752563
CurrentTrain: epoch  9, batch    20 | loss: 5.7597613Losses:  5.767669677734375 1.7573914527893066
CurrentTrain: epoch  9, batch    21 | loss: 5.7676697Losses:  5.8235626220703125 1.7665166854858398
CurrentTrain: epoch  9, batch    22 | loss: 5.8235626Losses:  5.733364105224609 1.7349467277526855
CurrentTrain: epoch  9, batch    23 | loss: 5.7333641Losses:  5.647809028625488 1.6326029300689697
CurrentTrain: epoch  9, batch    24 | loss: 5.6478090Losses:  5.667008876800537 1.6376363039016724
CurrentTrain: epoch  9, batch    25 | loss: 5.6670089Losses:  5.756350994110107 1.7733960151672363
CurrentTrain: epoch  9, batch    26 | loss: 5.7563510Losses:  5.712581634521484 1.7094242572784424
CurrentTrain: epoch  9, batch    27 | loss: 5.7125816Losses:  5.925958633422852 1.9274744987487793
CurrentTrain: epoch  9, batch    28 | loss: 5.9259586Losses:  5.663699150085449 1.6583095788955688
CurrentTrain: epoch  9, batch    29 | loss: 5.6636992Losses:  5.747724533081055 1.716193437576294
CurrentTrain: epoch  9, batch    30 | loss: 5.7477245Losses:  5.79749059677124 1.8052197694778442
CurrentTrain: epoch  9, batch    31 | loss: 5.7974906Losses:  5.581841945648193 1.5998319387435913
CurrentTrain: epoch  9, batch    32 | loss: 5.5818419Losses:  5.397763252258301 1.4263869524002075
CurrentTrain: epoch  9, batch    33 | loss: 5.3977633Losses:  5.896771430969238 1.8827569484710693
CurrentTrain: epoch  9, batch    34 | loss: 5.8967714Losses:  5.960094451904297 1.9527838230133057
CurrentTrain: epoch  9, batch    35 | loss: 5.9600945Losses:  5.433592319488525 1.427777647972107
CurrentTrain: epoch  9, batch    36 | loss: 5.4335923Losses:  5.815108299255371 1.8207767009735107
CurrentTrain: epoch  9, batch    37 | loss: 5.8151083Losses:  5.731247425079346 1.6857080459594727
CurrentTrain: epoch  9, batch    38 | loss: 5.7312474Losses:  6.069668292999268 1.8187241554260254
CurrentTrain: epoch  9, batch    39 | loss: 6.0696683Losses:  5.675967216491699 1.596346139907837
CurrentTrain: epoch  9, batch    40 | loss: 5.6759672Losses:  5.509510040283203 1.515486240386963
CurrentTrain: epoch  9, batch    41 | loss: 5.5095100Losses:  5.663585186004639 1.640812873840332
CurrentTrain: epoch  9, batch    42 | loss: 5.6635852Losses:  5.554538249969482 1.5607774257659912
CurrentTrain: epoch  9, batch    43 | loss: 5.5545382Losses:  5.622271537780762 1.5968801975250244
CurrentTrain: epoch  9, batch    44 | loss: 5.6222715Losses:  5.593988418579102 1.6087034940719604
CurrentTrain: epoch  9, batch    45 | loss: 5.5939884Losses:  5.561023712158203 1.5806832313537598
CurrentTrain: epoch  9, batch    46 | loss: 5.5610237Losses:  5.483013153076172 1.4616496562957764
CurrentTrain: epoch  9, batch    47 | loss: 5.4830132Losses:  5.826303005218506 1.8170099258422852
CurrentTrain: epoch  9, batch    48 | loss: 5.8263030Losses:  6.0820136070251465 1.7559056282043457
CurrentTrain: epoch  9, batch    49 | loss: 6.0820136Losses:  5.968902587890625 1.8500099182128906
CurrentTrain: epoch  9, batch    50 | loss: 5.9689026Losses:  5.882071495056152 1.828902006149292
CurrentTrain: epoch  9, batch    51 | loss: 5.8820715Losses:  5.715506076812744 1.7032103538513184
CurrentTrain: epoch  9, batch    52 | loss: 5.7155061Losses:  5.735096454620361 1.7413091659545898
CurrentTrain: epoch  9, batch    53 | loss: 5.7350965Losses:  5.680941581726074 1.6701794862747192
CurrentTrain: epoch  9, batch    54 | loss: 5.6809416Losses:  5.642739295959473 1.6368029117584229
CurrentTrain: epoch  9, batch    55 | loss: 5.6427393Losses:  5.806835174560547 1.8322546482086182
CurrentTrain: epoch  9, batch    56 | loss: 5.8068352Losses:  5.8537750244140625 1.8341237306594849
CurrentTrain: epoch  9, batch    57 | loss: 5.8537750Losses:  5.713357925415039 1.6988502740859985
CurrentTrain: epoch  9, batch    58 | loss: 5.7133579Losses:  5.785406112670898 1.7741880416870117
CurrentTrain: epoch  9, batch    59 | loss: 5.7854061Losses:  5.769946098327637 1.7477343082427979
CurrentTrain: epoch  9, batch    60 | loss: 5.7699461Losses:  5.778013229370117 1.7486789226531982
CurrentTrain: epoch  9, batch    61 | loss: 5.7780132Losses:  5.422593593597412 1.42364501953125
CurrentTrain: epoch  9, batch    62 | loss: 5.4225936
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  8.623163223266602 1.872035264968872
CurrentTrain: epoch  0, batch     0 | loss: 8.6231632Losses:  7.833947658538818 1.8610239028930664
CurrentTrain: epoch  0, batch     1 | loss: 7.8339477Losses:  8.419319152832031 1.9923200607299805
CurrentTrain: epoch  0, batch     2 | loss: 8.4193192Losses:  7.562911033630371 0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.5629110Losses:  8.024663925170898 2.0250039100646973
CurrentTrain: epoch  1, batch     0 | loss: 8.0246639Losses:  8.051559448242188 2.1436688899993896
CurrentTrain: epoch  1, batch     1 | loss: 8.0515594Losses:  7.332094669342041 2.082486629486084
CurrentTrain: epoch  1, batch     2 | loss: 7.3320947Losses:  3.911308765411377 0.6734749674797058
CurrentTrain: epoch  1, batch     3 | loss: 3.9113088Losses:  7.230628967285156 1.867722988128662
CurrentTrain: epoch  2, batch     0 | loss: 7.2306290Losses:  6.521164894104004 1.9493143558502197
CurrentTrain: epoch  2, batch     1 | loss: 6.5211649Losses:  6.418659210205078 1.9740175008773804
CurrentTrain: epoch  2, batch     2 | loss: 6.4186592Losses:  8.21723747253418 0.6096987724304199
CurrentTrain: epoch  2, batch     3 | loss: 8.2172375Losses:  7.3845391273498535 2.107517719268799
CurrentTrain: epoch  3, batch     0 | loss: 7.3845391Losses:  5.625767707824707 1.8968620300292969
CurrentTrain: epoch  3, batch     1 | loss: 5.6257677Losses:  6.344304084777832 2.023402452468872
CurrentTrain: epoch  3, batch     2 | loss: 6.3443041Losses:  3.6356348991394043 0.6559277176856995
CurrentTrain: epoch  3, batch     3 | loss: 3.6356349Losses:  5.592145919799805 1.8434467315673828
CurrentTrain: epoch  4, batch     0 | loss: 5.5921459Losses:  5.53101921081543 1.8209228515625
CurrentTrain: epoch  4, batch     1 | loss: 5.5310192Losses:  6.110214710235596 1.8856725692749023
CurrentTrain: epoch  4, batch     2 | loss: 6.1102147Losses:  4.375772476196289 0.6206262111663818
CurrentTrain: epoch  4, batch     3 | loss: 4.3757725Losses:  5.825558662414551 1.8591002225875854
CurrentTrain: epoch  5, batch     0 | loss: 5.8255587Losses:  4.690373420715332 1.8922984600067139
CurrentTrain: epoch  5, batch     1 | loss: 4.6903734Losses:  5.331231594085693 2.1391243934631348
CurrentTrain: epoch  5, batch     2 | loss: 5.3312316Losses:  5.718584060668945 0.6079392433166504
CurrentTrain: epoch  5, batch     3 | loss: 5.7185841Losses:  5.275996208190918 1.903883695602417
CurrentTrain: epoch  6, batch     0 | loss: 5.2759962Losses:  5.107780456542969 1.8575241565704346
CurrentTrain: epoch  6, batch     1 | loss: 5.1077805Losses:  4.803203582763672 1.9323503971099854
CurrentTrain: epoch  6, batch     2 | loss: 4.8032036Losses:  2.554631471633911 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.5546315Losses:  5.249080181121826 1.9657328128814697
CurrentTrain: epoch  7, batch     0 | loss: 5.2490802Losses:  4.806909084320068 2.1026644706726074
CurrentTrain: epoch  7, batch     1 | loss: 4.8069091Losses:  4.609440803527832 1.9258301258087158
CurrentTrain: epoch  7, batch     2 | loss: 4.6094408Losses:  2.028940439224243 0.0
CurrentTrain: epoch  7, batch     3 | loss: 2.0289404Losses:  4.683770179748535 1.9864013195037842
CurrentTrain: epoch  8, batch     0 | loss: 4.6837702Losses:  4.560812950134277 1.7442816495895386
CurrentTrain: epoch  8, batch     1 | loss: 4.5608130Losses:  4.709131717681885 1.985190987586975
CurrentTrain: epoch  8, batch     2 | loss: 4.7091317Losses:  2.4860036373138428 0.5647199153900146
CurrentTrain: epoch  8, batch     3 | loss: 2.4860036Losses:  4.429564476013184 2.0025033950805664
CurrentTrain: epoch  9, batch     0 | loss: 4.4295645Losses:  4.870170593261719 1.7207043170928955
CurrentTrain: epoch  9, batch     1 | loss: 4.8701706Losses:  4.264191627502441 1.9544661045074463
CurrentTrain: epoch  9, batch     2 | loss: 4.2641916Losses:  2.5259127616882324 0.6437159180641174
CurrentTrain: epoch  9, batch     3 | loss: 2.5259128
Losses:  4.692081928253174 2.5632035732269287
MemoryTrain:  epoch  0, batch     0 | loss: 4.6920819Losses:  1.66709303855896 1.1644420623779297
MemoryTrain:  epoch  0, batch     1 | loss: 1.6670930Losses:  4.289588451385498 2.5554440021514893
MemoryTrain:  epoch  1, batch     0 | loss: 4.2895885Losses:  2.6441478729248047 1.3057100772857666
MemoryTrain:  epoch  1, batch     1 | loss: 2.6441479Losses:  4.082562446594238 2.571507692337036
MemoryTrain:  epoch  2, batch     0 | loss: 4.0825624Losses:  1.325804591178894 1.2069587707519531
MemoryTrain:  epoch  2, batch     1 | loss: 1.3258046Losses:  3.0995898246765137 2.545067071914673
MemoryTrain:  epoch  3, batch     0 | loss: 3.0995898Losses:  3.1356024742126465 1.3152258396148682
MemoryTrain:  epoch  3, batch     1 | loss: 3.1356025Losses:  3.473689079284668 2.5752549171447754
MemoryTrain:  epoch  4, batch     0 | loss: 3.4736891Losses:  1.257462739944458 1.208268165588379
MemoryTrain:  epoch  4, batch     1 | loss: 1.2574627Losses:  3.248852252960205 2.5849974155426025
MemoryTrain:  epoch  5, batch     0 | loss: 3.2488523Losses:  1.2369577884674072 1.1414496898651123
MemoryTrain:  epoch  5, batch     1 | loss: 1.2369578Losses:  3.1509952545166016 2.554110050201416
MemoryTrain:  epoch  6, batch     0 | loss: 3.1509953Losses:  1.4304633140563965 1.2797731161117554
MemoryTrain:  epoch  6, batch     1 | loss: 1.4304633Losses:  2.859924793243408 2.575047731399536
MemoryTrain:  epoch  7, batch     0 | loss: 2.8599248Losses:  2.181330680847168 1.1923036575317383
MemoryTrain:  epoch  7, batch     1 | loss: 2.1813307Losses:  3.048234224319458 2.572906017303467
MemoryTrain:  epoch  8, batch     0 | loss: 3.0482342Losses:  1.2326925992965698 1.1974945068359375
MemoryTrain:  epoch  8, batch     1 | loss: 1.2326926Losses:  2.914975166320801 2.5599353313446045
MemoryTrain:  epoch  9, batch     0 | loss: 2.9149752Losses:  2.008770704269409 1.2399585247039795
MemoryTrain:  epoch  9, batch     1 | loss: 2.0087707
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.31%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.10%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.92%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.22%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 92.97%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.89%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.74%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.68%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 92.15%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.93%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.46%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.27%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.18%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 91.13%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.80%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.73%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 90.49%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 90.42%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 90.26%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 89.76%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.21%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 88.67%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 88.27%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.76%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 87.44%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 86.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 86.51%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 86.21%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 85.80%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.58%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.36%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 85.08%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.70%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 84.20%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.66%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.24%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.77%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.48%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.18%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.32%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 83.75%   
cur_acc:  ['0.9454', '0.7431']
his_acc:  ['0.9454', '0.8375']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  8.872294425964355 1.9670209884643555
CurrentTrain: epoch  0, batch     0 | loss: 8.8722944Losses:  8.145380020141602 2.044318437576294
CurrentTrain: epoch  0, batch     1 | loss: 8.1453800Losses:  8.8912353515625 2.174560785293579
CurrentTrain: epoch  0, batch     2 | loss: 8.8912354Losses:  8.430651664733887 0.6622560620307922
CurrentTrain: epoch  0, batch     3 | loss: 8.4306517Losses:  7.426477432250977 2.189173698425293
CurrentTrain: epoch  1, batch     0 | loss: 7.4264774Losses:  8.589923858642578 1.978393316268921
CurrentTrain: epoch  1, batch     1 | loss: 8.5899239Losses:  7.178692817687988 2.1549978256225586
CurrentTrain: epoch  1, batch     2 | loss: 7.1786928Losses:  6.446920394897461 0.6454722881317139
CurrentTrain: epoch  1, batch     3 | loss: 6.4469204Losses:  6.362582683563232 1.915064811706543
CurrentTrain: epoch  2, batch     0 | loss: 6.3625827Losses:  7.2937517166137695 2.1319808959960938
CurrentTrain: epoch  2, batch     1 | loss: 7.2937517Losses:  7.962325572967529 2.0081467628479004
CurrentTrain: epoch  2, batch     2 | loss: 7.9623256Losses:  3.763970375061035 0.6253968477249146
CurrentTrain: epoch  2, batch     3 | loss: 3.7639704Losses:  6.79901647567749 2.12846040725708
CurrentTrain: epoch  3, batch     0 | loss: 6.7990165Losses:  6.201653480529785 1.8964084386825562
CurrentTrain: epoch  3, batch     1 | loss: 6.2016535Losses:  7.041945934295654 2.0782246589660645
CurrentTrain: epoch  3, batch     2 | loss: 7.0419459Losses:  5.945024490356445 0.6391963958740234
CurrentTrain: epoch  3, batch     3 | loss: 5.9450245Losses:  6.970549583435059 1.9917066097259521
CurrentTrain: epoch  4, batch     0 | loss: 6.9705496Losses:  6.564241409301758 2.157348155975342
CurrentTrain: epoch  4, batch     1 | loss: 6.5642414Losses:  5.012172698974609 1.8802433013916016
CurrentTrain: epoch  4, batch     2 | loss: 5.0121727Losses:  5.266132831573486 0.6754122376441956
CurrentTrain: epoch  4, batch     3 | loss: 5.2661328Losses:  6.465988636016846 1.799943447113037
CurrentTrain: epoch  5, batch     0 | loss: 6.4659886Losses:  5.63127326965332 1.972503900527954
CurrentTrain: epoch  5, batch     1 | loss: 5.6312733Losses:  5.440245628356934 1.82462477684021
CurrentTrain: epoch  5, batch     2 | loss: 5.4402456Losses:  7.911949157714844 0.6956679821014404
CurrentTrain: epoch  5, batch     3 | loss: 7.9119492Losses:  6.591831207275391 2.1689188480377197
CurrentTrain: epoch  6, batch     0 | loss: 6.5918312Losses:  6.195993423461914 2.0694503784179688
CurrentTrain: epoch  6, batch     1 | loss: 6.1959934Losses:  5.233071327209473 2.0567545890808105
CurrentTrain: epoch  6, batch     2 | loss: 5.2330713Losses:  3.543123722076416 0.6334962844848633
CurrentTrain: epoch  6, batch     3 | loss: 3.5431237Losses:  5.286084175109863 2.016610622406006
CurrentTrain: epoch  7, batch     0 | loss: 5.2860842Losses:  5.624858856201172 1.9810898303985596
CurrentTrain: epoch  7, batch     1 | loss: 5.6248589Losses:  5.618491172790527 1.9801604747772217
CurrentTrain: epoch  7, batch     2 | loss: 5.6184912Losses:  5.746906280517578 0.6838133931159973
CurrentTrain: epoch  7, batch     3 | loss: 5.7469063Losses:  5.615809440612793 2.150379180908203
CurrentTrain: epoch  8, batch     0 | loss: 5.6158094Losses:  5.713345527648926 2.1020119190216064
CurrentTrain: epoch  8, batch     1 | loss: 5.7133455Losses:  5.205569744110107 1.9677319526672363
CurrentTrain: epoch  8, batch     2 | loss: 5.2055697Losses:  3.0249085426330566 0.6230436563491821
CurrentTrain: epoch  8, batch     3 | loss: 3.0249085Losses:  4.901918411254883 1.889866828918457
CurrentTrain: epoch  9, batch     0 | loss: 4.9019184Losses:  4.42585563659668 1.8263843059539795
CurrentTrain: epoch  9, batch     1 | loss: 4.4258556Losses:  5.657747268676758 1.9155094623565674
CurrentTrain: epoch  9, batch     2 | loss: 5.6577473Losses:  5.689383029937744 0.6951766014099121
CurrentTrain: epoch  9, batch     3 | loss: 5.6893830
Losses:  3.8345019817352295 2.611485004425049
MemoryTrain:  epoch  0, batch     0 | loss: 3.8345020Losses:  2.8041670322418213 2.492389678955078
MemoryTrain:  epoch  0, batch     1 | loss: 2.8041670Losses:  3.537459373474121 2.6190786361694336
MemoryTrain:  epoch  1, batch     0 | loss: 3.5374594Losses:  3.417311668395996 2.4596705436706543
MemoryTrain:  epoch  1, batch     1 | loss: 3.4173117Losses:  2.9017410278320312 2.5916595458984375
MemoryTrain:  epoch  2, batch     0 | loss: 2.9017410Losses:  3.1627230644226074 2.487637519836426
MemoryTrain:  epoch  2, batch     1 | loss: 3.1627231Losses:  2.8958301544189453 2.595393180847168
MemoryTrain:  epoch  3, batch     0 | loss: 2.8958302Losses:  2.6865811347961426 2.4934470653533936
MemoryTrain:  epoch  3, batch     1 | loss: 2.6865811Losses:  2.843172073364258 2.63838267326355
MemoryTrain:  epoch  4, batch     0 | loss: 2.8431721Losses:  2.6097309589385986 2.448260545730591
MemoryTrain:  epoch  4, batch     1 | loss: 2.6097310Losses:  2.742499589920044 2.625380754470825
MemoryTrain:  epoch  5, batch     0 | loss: 2.7424996Losses:  2.539079427719116 2.4570939540863037
MemoryTrain:  epoch  5, batch     1 | loss: 2.5390794Losses:  2.6866202354431152 2.6134603023529053
MemoryTrain:  epoch  6, batch     0 | loss: 2.6866202Losses:  2.5176448822021484 2.4653007984161377
MemoryTrain:  epoch  6, batch     1 | loss: 2.5176449Losses:  2.679997444152832 2.6049633026123047
MemoryTrain:  epoch  7, batch     0 | loss: 2.6799974Losses:  2.5051403045654297 2.4588463306427
MemoryTrain:  epoch  7, batch     1 | loss: 2.5051403Losses:  2.6759424209594727 2.6333670616149902
MemoryTrain:  epoch  8, batch     0 | loss: 2.6759424Losses:  2.4649689197540283 2.4399943351745605
MemoryTrain:  epoch  8, batch     1 | loss: 2.4649689Losses:  2.6885900497436523 2.6288034915924072
MemoryTrain:  epoch  9, batch     0 | loss: 2.6885900Losses:  2.4669055938720703 2.4456193447113037
MemoryTrain:  epoch  9, batch     1 | loss: 2.4669056
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 62.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 63.26%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 71.44%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 71.46%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 92.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 91.70%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.53%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.47%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.31%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.79%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 91.75%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 91.70%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 91.55%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.45%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 91.31%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 90.87%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 90.59%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 90.16%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 90.12%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.09%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 89.76%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.51%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 89.26%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 88.65%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.57%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 88.48%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 88.38%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 88.24%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.83%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 87.24%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 86.52%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 86.08%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 85.46%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 84.50%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.22%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.01%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.74%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.53%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 83.39%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 83.14%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.71%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.12%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.54%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 81.08%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 80.57%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.98%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.10%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 81.05%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 80.86%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.81%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 80.77%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 81.30%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 80.71%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.22%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 79.70%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 79.14%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 78.58%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 78.21%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 78.73%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 78.45%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 78.19%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 77.96%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 77.66%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 77.28%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.58%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.74%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.14%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 78.67%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 78.56%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.64%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.39%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 78.27%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 78.11%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.09%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 78.04%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 78.09%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 77.97%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 77.99%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.94%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 77.73%   
cur_acc:  ['0.9454', '0.7431', '0.7103']
his_acc:  ['0.9454', '0.8375', '0.7773']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  7.6279802322387695 1.9830541610717773
CurrentTrain: epoch  0, batch     0 | loss: 7.6279802Losses:  6.260134696960449 1.9777967929840088
CurrentTrain: epoch  0, batch     1 | loss: 6.2601347Losses:  7.201015472412109 1.8570277690887451
CurrentTrain: epoch  0, batch     2 | loss: 7.2010155Losses:  4.177406311035156 0.6803406476974487
CurrentTrain: epoch  0, batch     3 | loss: 4.1774063Losses:  6.560042858123779 2.1044468879699707
CurrentTrain: epoch  1, batch     0 | loss: 6.5600429Losses:  5.871218681335449 2.0488882064819336
CurrentTrain: epoch  1, batch     1 | loss: 5.8712187Losses:  6.013411521911621 2.1448981761932373
CurrentTrain: epoch  1, batch     2 | loss: 6.0134115Losses:  3.306891441345215 0.6491252183914185
CurrentTrain: epoch  1, batch     3 | loss: 3.3068914Losses:  6.572291374206543 1.997713327407837
CurrentTrain: epoch  2, batch     0 | loss: 6.5722914Losses:  5.306026458740234 2.0444018840789795
CurrentTrain: epoch  2, batch     1 | loss: 5.3060265Losses:  4.534348487854004 1.9548566341400146
CurrentTrain: epoch  2, batch     2 | loss: 4.5343485Losses:  2.6410040855407715 0.6482867002487183
CurrentTrain: epoch  2, batch     3 | loss: 2.6410041Losses:  5.209024429321289 2.005711078643799
CurrentTrain: epoch  3, batch     0 | loss: 5.2090244Losses:  5.20259952545166 2.15879487991333
CurrentTrain: epoch  3, batch     1 | loss: 5.2025995Losses:  5.251468658447266 2.054703712463379
CurrentTrain: epoch  3, batch     2 | loss: 5.2514687Losses:  2.654395580291748 0.6051279306411743
CurrentTrain: epoch  3, batch     3 | loss: 2.6543956Losses:  4.718522071838379 1.8595502376556396
CurrentTrain: epoch  4, batch     0 | loss: 4.7185221Losses:  4.645233154296875 1.8708237409591675
CurrentTrain: epoch  4, batch     1 | loss: 4.6452332Losses:  4.988109111785889 1.6388311386108398
CurrentTrain: epoch  4, batch     2 | loss: 4.9881091Losses:  2.6956281661987305 0.6622109413146973
CurrentTrain: epoch  4, batch     3 | loss: 2.6956282Losses:  5.1206464767456055 2.019374370574951
CurrentTrain: epoch  5, batch     0 | loss: 5.1206465Losses:  4.979781627655029 1.9096518754959106
CurrentTrain: epoch  5, batch     1 | loss: 4.9797816Losses:  4.4762396812438965 2.146787166595459
CurrentTrain: epoch  5, batch     2 | loss: 4.4762397Losses:  1.8925402164459229 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8925402Losses:  4.011414051055908 1.8713494539260864
CurrentTrain: epoch  6, batch     0 | loss: 4.0114141Losses:  4.794020652770996 1.8808417320251465
CurrentTrain: epoch  6, batch     1 | loss: 4.7940207Losses:  4.84441614151001 2.0363364219665527
CurrentTrain: epoch  6, batch     2 | loss: 4.8444161Losses:  2.8590288162231445 0.6530847549438477
CurrentTrain: epoch  6, batch     3 | loss: 2.8590288Losses:  4.370702266693115 1.9673500061035156
CurrentTrain: epoch  7, batch     0 | loss: 4.3707023Losses:  4.238698482513428 2.011922836303711
CurrentTrain: epoch  7, batch     1 | loss: 4.2386985Losses:  4.5782470703125 2.042172908782959
CurrentTrain: epoch  7, batch     2 | loss: 4.5782471Losses:  3.9699277877807617 0.6761109232902527
CurrentTrain: epoch  7, batch     3 | loss: 3.9699278Losses:  4.4339189529418945 1.8275859355926514
CurrentTrain: epoch  8, batch     0 | loss: 4.4339190Losses:  4.208008766174316 2.0006892681121826
CurrentTrain: epoch  8, batch     1 | loss: 4.2080088Losses:  3.9538893699645996 2.0101747512817383
CurrentTrain: epoch  8, batch     2 | loss: 3.9538894Losses:  2.8291938304901123 0.6565685868263245
CurrentTrain: epoch  8, batch     3 | loss: 2.8291938Losses:  4.196990966796875 2.0353310108184814
CurrentTrain: epoch  9, batch     0 | loss: 4.1969910Losses:  4.451181411743164 1.8946247100830078
CurrentTrain: epoch  9, batch     1 | loss: 4.4511814Losses:  3.8064019680023193 1.9382914304733276
CurrentTrain: epoch  9, batch     2 | loss: 3.8064020Losses:  2.609133005142212 0.6834359169006348
CurrentTrain: epoch  9, batch     3 | loss: 2.6091330
Losses:  2.8940253257751465 2.6482253074645996
MemoryTrain:  epoch  0, batch     0 | loss: 2.8940253Losses:  4.027132034301758 2.598893880844116
MemoryTrain:  epoch  0, batch     1 | loss: 4.0271320Losses:  2.821223258972168 1.9448013305664062
MemoryTrain:  epoch  0, batch     2 | loss: 2.8212233Losses:  3.637324333190918 2.618922710418701
MemoryTrain:  epoch  1, batch     0 | loss: 3.6373243Losses:  3.7673163414001465 2.591984987258911
MemoryTrain:  epoch  1, batch     1 | loss: 3.7673163Losses:  3.40514874458313 2.0052709579467773
MemoryTrain:  epoch  1, batch     2 | loss: 3.4051487Losses:  3.394897937774658 2.663952112197876
MemoryTrain:  epoch  2, batch     0 | loss: 3.3948979Losses:  3.5401558876037598 2.6164519786834717
MemoryTrain:  epoch  2, batch     1 | loss: 3.5401559Losses:  2.569476366043091 1.9051105976104736
MemoryTrain:  epoch  2, batch     2 | loss: 2.5694764Losses:  3.3493998050689697 2.6588380336761475
MemoryTrain:  epoch  3, batch     0 | loss: 3.3493998Losses:  3.1730706691741943 2.5774002075195312
MemoryTrain:  epoch  3, batch     1 | loss: 3.1730707Losses:  2.1369411945343018 1.9643359184265137
MemoryTrain:  epoch  3, batch     2 | loss: 2.1369412Losses:  2.8572516441345215 2.6396098136901855
MemoryTrain:  epoch  4, batch     0 | loss: 2.8572516Losses:  3.117237091064453 2.584232807159424
MemoryTrain:  epoch  4, batch     1 | loss: 3.1172371Losses:  3.294894218444824 1.9800939559936523
MemoryTrain:  epoch  4, batch     2 | loss: 3.2948942Losses:  2.9804766178131104 2.594724416732788
MemoryTrain:  epoch  5, batch     0 | loss: 2.9804766Losses:  2.7777600288391113 2.6457290649414062
MemoryTrain:  epoch  5, batch     1 | loss: 2.7777600Losses:  2.026973009109497 1.935861349105835
MemoryTrain:  epoch  5, batch     2 | loss: 2.0269730Losses:  2.646482229232788 2.5960493087768555
MemoryTrain:  epoch  6, batch     0 | loss: 2.6464822Losses:  2.725342273712158 2.614539861679077
MemoryTrain:  epoch  6, batch     1 | loss: 2.7253423Losses:  2.0865564346313477 1.9933183193206787
MemoryTrain:  epoch  6, batch     2 | loss: 2.0865564Losses:  2.6881179809570312 2.6114325523376465
MemoryTrain:  epoch  7, batch     0 | loss: 2.6881180Losses:  2.7045609951019287 2.62490177154541
MemoryTrain:  epoch  7, batch     1 | loss: 2.7045610Losses:  1.9882481098175049 1.9387558698654175
MemoryTrain:  epoch  7, batch     2 | loss: 1.9882481Losses:  2.6344501972198486 2.5835390090942383
MemoryTrain:  epoch  8, batch     0 | loss: 2.6344502Losses:  2.7110114097595215 2.6546266078948975
MemoryTrain:  epoch  8, batch     1 | loss: 2.7110114Losses:  2.039780616760254 1.9624831676483154
MemoryTrain:  epoch  8, batch     2 | loss: 2.0397806Losses:  2.6970245838165283 2.650421142578125
MemoryTrain:  epoch  9, batch     0 | loss: 2.6970246Losses:  2.6171512603759766 2.5821943283081055
MemoryTrain:  epoch  9, batch     1 | loss: 2.6171513Losses:  1.9932687282562256 1.938150405883789
MemoryTrain:  epoch  9, batch     2 | loss: 1.9932687
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.86%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.52%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.23%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.90%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 90.41%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.75%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.62%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.68%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 89.65%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 90.02%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.27%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.98%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.86%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.80%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.61%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.42%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.32%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 88.91%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 88.49%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.10%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.80%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 87.28%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 86.49%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 86.43%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 86.31%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 85.82%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.31%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 84.74%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 84.11%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 83.83%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 83.29%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 83.02%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 82.44%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 82.12%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.92%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.67%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.84%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.21%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 78.49%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.82%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 79.49%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 79.28%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 79.25%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 79.10%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 79.57%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 79.00%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 78.53%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 78.01%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 77.46%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.92%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.56%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.68%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 77.03%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 76.60%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 76.31%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.73%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.40%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.77%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 75.30%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 74.85%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 74.44%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 74.03%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 73.66%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 73.34%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 73.43%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 73.27%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 73.28%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 73.37%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 73.31%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 73.43%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 73.45%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 73.43%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 74.20%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 74.48%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.42%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.25%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 74.10%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 73.89%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 73.69%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 73.40%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.20%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.18%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 73.95%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.99%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 74.00%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 73.92%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 75.54%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 76.10%   
cur_acc:  ['0.9454', '0.7431', '0.7103', '0.8323']
his_acc:  ['0.9454', '0.8375', '0.7773', '0.7610']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  7.611728668212891 1.9267246723175049
CurrentTrain: epoch  0, batch     0 | loss: 7.6117287Losses:  7.855983734130859 1.9904234409332275
CurrentTrain: epoch  0, batch     1 | loss: 7.8559837Losses:  7.63676643371582 1.9227977991104126
CurrentTrain: epoch  0, batch     2 | loss: 7.6367664Losses:  3.8853085041046143 0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.8853085Losses:  7.1746826171875 1.9716601371765137
CurrentTrain: epoch  1, batch     0 | loss: 7.1746826Losses:  6.796050548553467 2.0682215690612793
CurrentTrain: epoch  1, batch     1 | loss: 6.7960505Losses:  6.391605854034424 1.9858102798461914
CurrentTrain: epoch  1, batch     2 | loss: 6.3916059Losses:  4.886598587036133 0.6837795972824097
CurrentTrain: epoch  1, batch     3 | loss: 4.8865986Losses:  6.250321388244629 2.099604845046997
CurrentTrain: epoch  2, batch     0 | loss: 6.2503214Losses:  6.933999538421631 2.078057289123535
CurrentTrain: epoch  2, batch     1 | loss: 6.9339995Losses:  6.2544264793396 2.0613884925842285
CurrentTrain: epoch  2, batch     2 | loss: 6.2544265Losses:  3.5473923683166504 0.5890768766403198
CurrentTrain: epoch  2, batch     3 | loss: 3.5473924Losses:  6.63829231262207 2.0846598148345947
CurrentTrain: epoch  3, batch     0 | loss: 6.6382923Losses:  5.797167778015137 1.9433348178863525
CurrentTrain: epoch  3, batch     1 | loss: 5.7971678Losses:  5.603713035583496 1.982886791229248
CurrentTrain: epoch  3, batch     2 | loss: 5.6037130Losses:  5.0705647468566895 0.6745993494987488
CurrentTrain: epoch  3, batch     3 | loss: 5.0705647Losses:  5.372016906738281 1.9354053735733032
CurrentTrain: epoch  4, batch     0 | loss: 5.3720169Losses:  5.939250469207764 1.7674098014831543
CurrentTrain: epoch  4, batch     1 | loss: 5.9392505Losses:  5.8944902420043945 2.1548359394073486
CurrentTrain: epoch  4, batch     2 | loss: 5.8944902Losses:  2.52303147315979 0.6605644226074219
CurrentTrain: epoch  4, batch     3 | loss: 2.5230315Losses:  5.465447902679443 2.0917468070983887
CurrentTrain: epoch  5, batch     0 | loss: 5.4654479Losses:  5.546605110168457 1.978053331375122
CurrentTrain: epoch  5, batch     1 | loss: 5.5466051Losses:  5.661459922790527 2.026811122894287
CurrentTrain: epoch  5, batch     2 | loss: 5.6614599Losses:  5.5318779945373535 0.6853863596916199
CurrentTrain: epoch  5, batch     3 | loss: 5.5318780Losses:  5.338888168334961 1.9210352897644043
CurrentTrain: epoch  6, batch     0 | loss: 5.3388882Losses:  5.119424819946289 1.9393271207809448
CurrentTrain: epoch  6, batch     1 | loss: 5.1194248Losses:  5.318964958190918 2.053251028060913
CurrentTrain: epoch  6, batch     2 | loss: 5.3189650Losses:  3.365663528442383 0.6661092638969421
CurrentTrain: epoch  6, batch     3 | loss: 3.3656635Losses:  5.063790321350098 2.063030958175659
CurrentTrain: epoch  7, batch     0 | loss: 5.0637903Losses:  5.1148834228515625 2.0387773513793945
CurrentTrain: epoch  7, batch     1 | loss: 5.1148834Losses:  5.045296669006348 1.985854148864746
CurrentTrain: epoch  7, batch     2 | loss: 5.0452967Losses:  3.574134588241577 0.6850845813751221
CurrentTrain: epoch  7, batch     3 | loss: 3.5741346Losses:  4.830618858337402 2.1602160930633545
CurrentTrain: epoch  8, batch     0 | loss: 4.8306189Losses:  5.147220611572266 1.9094129800796509
CurrentTrain: epoch  8, batch     1 | loss: 5.1472206Losses:  5.027282238006592 2.0271310806274414
CurrentTrain: epoch  8, batch     2 | loss: 5.0272822Losses:  2.494335174560547 0.63138747215271
CurrentTrain: epoch  8, batch     3 | loss: 2.4943352Losses:  5.030186653137207 2.083930492401123
CurrentTrain: epoch  9, batch     0 | loss: 5.0301867Losses:  4.202216148376465 1.8905190229415894
CurrentTrain: epoch  9, batch     1 | loss: 4.2022161Losses:  4.981438636779785 1.8963968753814697
CurrentTrain: epoch  9, batch     2 | loss: 4.9814386Losses:  2.828293800354004 0.6741570234298706
CurrentTrain: epoch  9, batch     3 | loss: 2.8282938
Losses:  3.130133867263794 2.620431423187256
MemoryTrain:  epoch  0, batch     0 | loss: 3.1301339Losses:  3.1970267295837402 2.673413038253784
MemoryTrain:  epoch  0, batch     1 | loss: 3.1970267Losses:  3.0882797241210938 2.615327835083008
MemoryTrain:  epoch  0, batch     2 | loss: 3.0882797Losses:  1.7870196104049683 0.6430176496505737
MemoryTrain:  epoch  0, batch     3 | loss: 1.7870196Losses:  3.7811179161071777 2.6211485862731934
MemoryTrain:  epoch  1, batch     0 | loss: 3.7811179Losses:  3.3151326179504395 2.6760826110839844
MemoryTrain:  epoch  1, batch     1 | loss: 3.3151326Losses:  2.8960158824920654 2.6212730407714844
MemoryTrain:  epoch  1, batch     2 | loss: 2.8960159Losses:  0.6146783232688904 0.558026909828186
MemoryTrain:  epoch  1, batch     3 | loss: 0.6146783Losses:  3.2075419425964355 2.633465528488159
MemoryTrain:  epoch  2, batch     0 | loss: 3.2075419Losses:  2.749358654022217 2.624941110610962
MemoryTrain:  epoch  2, batch     1 | loss: 2.7493587Losses:  3.089700222015381 2.6389784812927246
MemoryTrain:  epoch  2, batch     2 | loss: 3.0897002Losses:  0.7561889886856079 0.688165545463562
MemoryTrain:  epoch  2, batch     3 | loss: 0.7561890Losses:  2.938840627670288 2.609363555908203
MemoryTrain:  epoch  3, batch     0 | loss: 2.9388406Losses:  2.7660508155822754 2.6613290309906006
MemoryTrain:  epoch  3, batch     1 | loss: 2.7660508Losses:  2.9925363063812256 2.637218952178955
MemoryTrain:  epoch  3, batch     2 | loss: 2.9925363Losses:  0.8580575585365295 0.6755611896514893
MemoryTrain:  epoch  3, batch     3 | loss: 0.8580576Losses:  2.891366481781006 2.627847671508789
MemoryTrain:  epoch  4, batch     0 | loss: 2.8913665Losses:  2.790762424468994 2.630375385284424
MemoryTrain:  epoch  4, batch     1 | loss: 2.7907624Losses:  2.6947672367095947 2.6334891319274902
MemoryTrain:  epoch  4, batch     2 | loss: 2.6947672Losses:  0.7669731378555298 0.6453418135643005
MemoryTrain:  epoch  4, batch     3 | loss: 0.7669731Losses:  2.7594475746154785 2.605919122695923
MemoryTrain:  epoch  5, batch     0 | loss: 2.7594476Losses:  2.794726848602295 2.6248481273651123
MemoryTrain:  epoch  5, batch     1 | loss: 2.7947268Losses:  2.7364723682403564 2.6722664833068848
MemoryTrain:  epoch  5, batch     2 | loss: 2.7364724Losses:  0.5737921595573425 0.5619412660598755
MemoryTrain:  epoch  5, batch     3 | loss: 0.5737922Losses:  2.742994546890259 2.6096715927124023
MemoryTrain:  epoch  6, batch     0 | loss: 2.7429945Losses:  2.7091758251190186 2.627110481262207
MemoryTrain:  epoch  6, batch     1 | loss: 2.7091758Losses:  2.714021921157837 2.6678671836853027
MemoryTrain:  epoch  6, batch     2 | loss: 2.7140219Losses:  0.587917149066925 0.5700068473815918
MemoryTrain:  epoch  6, batch     3 | loss: 0.5879171Losses:  2.71256685256958 2.6351094245910645
MemoryTrain:  epoch  7, batch     0 | loss: 2.7125669Losses:  2.6816320419311523 2.6221253871917725
MemoryTrain:  epoch  7, batch     1 | loss: 2.6816320Losses:  2.7758021354675293 2.6339683532714844
MemoryTrain:  epoch  7, batch     2 | loss: 2.7758021Losses:  0.5770326852798462 0.5438055396080017
MemoryTrain:  epoch  7, batch     3 | loss: 0.5770327Losses:  2.708125352859497 2.662991762161255
MemoryTrain:  epoch  8, batch     0 | loss: 2.7081254Losses:  2.6450324058532715 2.599741220474243
MemoryTrain:  epoch  8, batch     1 | loss: 2.6450324Losses:  2.6686384677886963 2.621962308883667
MemoryTrain:  epoch  8, batch     2 | loss: 2.6686385Losses:  0.7114467620849609 0.6625069379806519
MemoryTrain:  epoch  8, batch     3 | loss: 0.7114468Losses:  2.64880108833313 2.605959892272949
MemoryTrain:  epoch  9, batch     0 | loss: 2.6488011Losses:  2.6970913410186768 2.652649164199829
MemoryTrain:  epoch  9, batch     1 | loss: 2.6970913Losses:  2.6826624870300293 2.6357028484344482
MemoryTrain:  epoch  9, batch     2 | loss: 2.6826625Losses:  0.5299707651138306 0.5062723755836487
MemoryTrain:  epoch  9, batch     3 | loss: 0.5299708
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 61.79%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 59.29%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.01%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 61.48%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 61.79%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.83%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.08%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.57%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 90.39%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.91%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 89.16%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 89.27%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 89.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.61%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 89.41%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.30%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 88.64%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 88.46%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.29%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.80%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 87.35%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 86.76%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 86.56%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 85.78%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 85.65%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 85.53%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 85.46%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 85.22%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 84.77%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 84.21%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 83.46%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 83.12%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 82.59%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.32%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 81.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.56%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.37%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.13%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.66%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.26%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.63%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.07%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.52%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.93%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.51%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.32%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 79.02%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 78.74%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 78.61%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 78.46%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 77.95%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 77.44%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.35%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.00%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 76.49%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 76.07%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 75.78%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 75.41%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.96%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.04%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 74.54%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 74.09%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 73.64%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 73.24%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 72.81%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 72.52%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 72.71%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 72.95%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.13%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  193 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  194 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 74.42%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 74.01%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 73.77%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 73.57%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 73.34%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.14%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 73.79%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.74%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 75.92%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 75.81%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 75.77%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 75.21%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 75.16%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 75.34%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 75.41%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 75.18%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 75.07%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 74.98%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 74.80%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 74.67%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 74.60%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 74.34%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 74.23%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 74.02%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 73.82%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 73.77%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 73.88%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 73.97%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 74.94%   
cur_acc:  ['0.9454', '0.7431', '0.7103', '0.8323', '0.7083']
his_acc:  ['0.9454', '0.8375', '0.7773', '0.7610', '0.7494']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  7.969415664672852 1.9800140857696533
CurrentTrain: epoch  0, batch     0 | loss: 7.9694157Losses:  7.780282020568848 1.7279369831085205
CurrentTrain: epoch  0, batch     1 | loss: 7.7802820Losses:  8.829413414001465 1.9926514625549316
CurrentTrain: epoch  0, batch     2 | loss: 8.8294134Losses:  5.067866325378418 0.0
CurrentTrain: epoch  0, batch     3 | loss: 5.0678663Losses:  7.884866714477539 2.1646194458007812
CurrentTrain: epoch  1, batch     0 | loss: 7.8848667Losses:  7.074182510375977 2.036226272583008
CurrentTrain: epoch  1, batch     1 | loss: 7.0741825Losses:  6.385501384735107 2.020517349243164
CurrentTrain: epoch  1, batch     2 | loss: 6.3855014Losses:  5.58651065826416 0.6647331714630127
CurrentTrain: epoch  1, batch     3 | loss: 5.5865107Losses:  6.3521246910095215 1.9059834480285645
CurrentTrain: epoch  2, batch     0 | loss: 6.3521247Losses:  6.761656761169434 2.0147039890289307
CurrentTrain: epoch  2, batch     1 | loss: 6.7616568Losses:  6.070496082305908 2.0228519439697266
CurrentTrain: epoch  2, batch     2 | loss: 6.0704961Losses:  5.252896308898926 0.6591920852661133
CurrentTrain: epoch  2, batch     3 | loss: 5.2528963Losses:  5.423470497131348 1.916408896446228
CurrentTrain: epoch  3, batch     0 | loss: 5.4234705Losses:  6.648848056793213 2.113192081451416
CurrentTrain: epoch  3, batch     1 | loss: 6.6488481Losses:  6.423023700714111 2.194559097290039
CurrentTrain: epoch  3, batch     2 | loss: 6.4230237Losses:  5.123709678649902 0.6558238863945007
CurrentTrain: epoch  3, batch     3 | loss: 5.1237097Losses:  5.188938140869141 1.8348803520202637
CurrentTrain: epoch  4, batch     0 | loss: 5.1889381Losses:  6.602265357971191 1.9147471189498901
CurrentTrain: epoch  4, batch     1 | loss: 6.6022654Losses:  5.97866153717041 2.1050100326538086
CurrentTrain: epoch  4, batch     2 | loss: 5.9786615Losses:  3.7543959617614746 0.6765395402908325
CurrentTrain: epoch  4, batch     3 | loss: 3.7543960Losses:  5.302987098693848 2.0973358154296875
CurrentTrain: epoch  5, batch     0 | loss: 5.3029871Losses:  6.0994062423706055 1.8275625705718994
CurrentTrain: epoch  5, batch     1 | loss: 6.0994062Losses:  5.138988494873047 2.0290422439575195
CurrentTrain: epoch  5, batch     2 | loss: 5.1389885Losses:  3.439056873321533 0.6427903175354004
CurrentTrain: epoch  5, batch     3 | loss: 3.4390569Losses:  4.6769232749938965 2.020219326019287
CurrentTrain: epoch  6, batch     0 | loss: 4.6769233Losses:  5.729607582092285 2.08479380607605
CurrentTrain: epoch  6, batch     1 | loss: 5.7296076Losses:  5.711930274963379 2.081148862838745
CurrentTrain: epoch  6, batch     2 | loss: 5.7119303Losses:  4.012186050415039 0.5868179798126221
CurrentTrain: epoch  6, batch     3 | loss: 4.0121861Losses:  5.460995197296143 2.0344719886779785
CurrentTrain: epoch  7, batch     0 | loss: 5.4609952Losses:  5.178670406341553 2.055957794189453
CurrentTrain: epoch  7, batch     1 | loss: 5.1786704Losses:  4.781086444854736 1.8759751319885254
CurrentTrain: epoch  7, batch     2 | loss: 4.7810864Losses:  4.638295650482178 0.6570422053337097
CurrentTrain: epoch  7, batch     3 | loss: 4.6382957Losses:  4.685639381408691 1.8404014110565186
CurrentTrain: epoch  8, batch     0 | loss: 4.6856394Losses:  4.421494483947754 1.8284447193145752
CurrentTrain: epoch  8, batch     1 | loss: 4.4214945Losses:  5.670742034912109 1.9557137489318848
CurrentTrain: epoch  8, batch     2 | loss: 5.6707420Losses:  2.4479267597198486 0.6252568960189819
CurrentTrain: epoch  8, batch     3 | loss: 2.4479268Losses:  4.4101972579956055 1.9441108703613281
CurrentTrain: epoch  9, batch     0 | loss: 4.4101973Losses:  4.582083702087402 1.8437244892120361
CurrentTrain: epoch  9, batch     1 | loss: 4.5820837Losses:  4.865971088409424 1.8301701545715332
CurrentTrain: epoch  9, batch     2 | loss: 4.8659711Losses:  5.693268299102783 0.6765793561935425
CurrentTrain: epoch  9, batch     3 | loss: 5.6932683
Losses:  3.168109655380249 2.649728298187256
MemoryTrain:  epoch  0, batch     0 | loss: 3.1681097Losses:  3.4054131507873535 2.6550331115722656
MemoryTrain:  epoch  0, batch     1 | loss: 3.4054132Losses:  2.9229013919830322 2.6361923217773438
MemoryTrain:  epoch  0, batch     2 | loss: 2.9229014Losses:  2.961869478225708 2.3485660552978516
MemoryTrain:  epoch  0, batch     3 | loss: 2.9618695Losses:  3.674335479736328 2.593778371810913
MemoryTrain:  epoch  1, batch     0 | loss: 3.6743355Losses:  3.025519609451294 2.630861759185791
MemoryTrain:  epoch  1, batch     1 | loss: 3.0255196Losses:  3.330470085144043 2.667269229888916
MemoryTrain:  epoch  1, batch     2 | loss: 3.3304701Losses:  2.865086555480957 2.39215350151062
MemoryTrain:  epoch  1, batch     3 | loss: 2.8650866Losses:  3.1001269817352295 2.63338565826416
MemoryTrain:  epoch  2, batch     0 | loss: 3.1001270Losses:  2.9002373218536377 2.652372360229492
MemoryTrain:  epoch  2, batch     1 | loss: 2.9002373Losses:  3.2131097316741943 2.6126656532287598
MemoryTrain:  epoch  2, batch     2 | loss: 3.2131097Losses:  2.5946459770202637 2.3322362899780273
MemoryTrain:  epoch  2, batch     3 | loss: 2.5946460Losses:  2.758028268814087 2.626760721206665
MemoryTrain:  epoch  3, batch     0 | loss: 2.7580283Losses:  2.887885332107544 2.642406940460205
MemoryTrain:  epoch  3, batch     1 | loss: 2.8878853Losses:  2.994234561920166 2.589285373687744
MemoryTrain:  epoch  3, batch     2 | loss: 2.9942346Losses:  2.979104995727539 2.405940294265747
MemoryTrain:  epoch  3, batch     3 | loss: 2.9791050Losses:  3.1422250270843506 2.659817695617676
MemoryTrain:  epoch  4, batch     0 | loss: 3.1422250Losses:  2.836749792098999 2.59903621673584
MemoryTrain:  epoch  4, batch     1 | loss: 2.8367498Losses:  2.711850643157959 2.6181488037109375
MemoryTrain:  epoch  4, batch     2 | loss: 2.7118506Losses:  2.5568363666534424 2.367006778717041
MemoryTrain:  epoch  4, batch     3 | loss: 2.5568364Losses:  2.8568334579467773 2.626150608062744
MemoryTrain:  epoch  5, batch     0 | loss: 2.8568335Losses:  2.775223731994629 2.6771461963653564
MemoryTrain:  epoch  5, batch     1 | loss: 2.7752237Losses:  2.668022871017456 2.6114025115966797
MemoryTrain:  epoch  5, batch     2 | loss: 2.6680229Losses:  2.8901119232177734 2.3293557167053223
MemoryTrain:  epoch  5, batch     3 | loss: 2.8901119Losses:  2.7561793327331543 2.631669521331787
MemoryTrain:  epoch  6, batch     0 | loss: 2.7561793Losses:  2.7302024364471436 2.658299446105957
MemoryTrain:  epoch  6, batch     1 | loss: 2.7302024Losses:  2.654569625854492 2.6052181720733643
MemoryTrain:  epoch  6, batch     2 | loss: 2.6545696Losses:  2.388633966445923 2.3409526348114014
MemoryTrain:  epoch  6, batch     3 | loss: 2.3886340Losses:  2.7353434562683105 2.6272778511047363
MemoryTrain:  epoch  7, batch     0 | loss: 2.7353435Losses:  2.671509027481079 2.6214025020599365
MemoryTrain:  epoch  7, batch     1 | loss: 2.6715090Losses:  2.6838438510894775 2.636030673980713
MemoryTrain:  epoch  7, batch     2 | loss: 2.6838439Losses:  2.393455982208252 2.347841262817383
MemoryTrain:  epoch  7, batch     3 | loss: 2.3934560Losses:  2.6707794666290283 2.6222550868988037
MemoryTrain:  epoch  8, batch     0 | loss: 2.6707795Losses:  2.6636171340942383 2.610873222351074
MemoryTrain:  epoch  8, batch     1 | loss: 2.6636171Losses:  2.6735966205596924 2.632906913757324
MemoryTrain:  epoch  8, batch     2 | loss: 2.6735966Losses:  2.4976067543029785 2.3770384788513184
MemoryTrain:  epoch  8, batch     3 | loss: 2.4976068Losses:  2.626814842224121 2.5930869579315186
MemoryTrain:  epoch  9, batch     0 | loss: 2.6268148Losses:  2.683222770690918 2.6426758766174316
MemoryTrain:  epoch  9, batch     1 | loss: 2.6832228Losses:  2.714714288711548 2.668513298034668
MemoryTrain:  epoch  9, batch     2 | loss: 2.7147143Losses:  2.362563371658325 2.3191628456115723
MemoryTrain:  epoch  9, batch     3 | loss: 2.3625634
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 75.32%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 74.22%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 73.32%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 72.80%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 70.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.53%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.03%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 89.77%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 89.04%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.25%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 87.71%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 86.99%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.69%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 86.41%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 85.55%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 84.94%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 84.42%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 83.82%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 83.56%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 83.47%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 82.95%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.77%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 82.59%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 82.42%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 82.25%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 82.09%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 81.70%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 80.39%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 80.31%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 79.99%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 79.47%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 78.91%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 78.19%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 78.09%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 77.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.35%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.00%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.86%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.65%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.29%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.69%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 75.17%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 74.72%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 74.21%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.77%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.90%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 74.85%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 74.46%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.09%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 74.91%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 74.37%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 73.88%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.40%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 72.89%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 72.38%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 72.72%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 72.33%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 71.63%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 70.79%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 71.00%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 70.57%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 70.14%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 69.72%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 69.31%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 69.01%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 68.60%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 68.32%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 70.35%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 70.34%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 70.24%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 69.95%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 69.58%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 69.40%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 69.97%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 69.99%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 69.81%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 72.13%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 72.53%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 72.47%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.39%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 72.28%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 71.94%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 72.22%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 72.01%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 71.91%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 71.81%   [EVAL] batch:  282 | acc: 18.75%,  total acc: 71.62%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.39%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 71.18%   [EVAL] batch:  285 | acc: 0.00%,  total acc: 70.94%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 70.71%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 70.63%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 70.70%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 70.81%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 71.77%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 71.70%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 71.61%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 71.47%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 71.66%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 71.40%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 71.33%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 71.27%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.79%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 72.38%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 72.25%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 72.13%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 71.92%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 71.84%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 72.25%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 72.18%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 71.93%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 71.77%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 71.72%   
cur_acc:  ['0.9454', '0.7431', '0.7103', '0.8323', '0.7083', '0.6994']
his_acc:  ['0.9454', '0.8375', '0.7773', '0.7610', '0.7494', '0.7172']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  7.736265182495117 1.7621304988861084
CurrentTrain: epoch  0, batch     0 | loss: 7.7362652Losses:  9.063741683959961 1.816163420677185
CurrentTrain: epoch  0, batch     1 | loss: 9.0637417Losses:  8.819499015808105 2.0127973556518555
CurrentTrain: epoch  0, batch     2 | loss: 8.8194990Losses:  9.545244216918945 0.6662139296531677
CurrentTrain: epoch  0, batch     3 | loss: 9.5452442Losses:  8.241759300231934 2.1074187755584717
CurrentTrain: epoch  1, batch     0 | loss: 8.2417593Losses:  7.7825775146484375 1.9343767166137695
CurrentTrain: epoch  1, batch     1 | loss: 7.7825775Losses:  7.188149452209473 2.0648715496063232
CurrentTrain: epoch  1, batch     2 | loss: 7.1881495Losses:  7.667057514190674 0.6622518301010132
CurrentTrain: epoch  1, batch     3 | loss: 7.6670575Losses:  6.911450386047363 1.9000251293182373
CurrentTrain: epoch  2, batch     0 | loss: 6.9114504Losses:  6.8690876960754395 2.045006275177002
CurrentTrain: epoch  2, batch     1 | loss: 6.8690877Losses:  7.26375675201416 2.2240986824035645
CurrentTrain: epoch  2, batch     2 | loss: 7.2637568Losses:  7.932554244995117 0.6673461198806763
CurrentTrain: epoch  2, batch     3 | loss: 7.9325542Losses:  6.657203674316406 1.849461317062378
CurrentTrain: epoch  3, batch     0 | loss: 6.6572037Losses:  7.051414489746094 2.0889368057250977
CurrentTrain: epoch  3, batch     1 | loss: 7.0514145Losses:  6.596492290496826 2.0471553802490234
CurrentTrain: epoch  3, batch     2 | loss: 6.5964923Losses:  5.090185165405273 0.6913139224052429
CurrentTrain: epoch  3, batch     3 | loss: 5.0901852Losses:  6.851978778839111 2.0518507957458496
CurrentTrain: epoch  4, batch     0 | loss: 6.8519788Losses:  5.623542785644531 1.8443241119384766
CurrentTrain: epoch  4, batch     1 | loss: 5.6235428Losses:  6.450100421905518 2.1330084800720215
CurrentTrain: epoch  4, batch     2 | loss: 6.4501004Losses:  6.70556640625 0.6998571157455444
CurrentTrain: epoch  4, batch     3 | loss: 6.7055664Losses:  6.104826927185059 1.6606380939483643
CurrentTrain: epoch  5, batch     0 | loss: 6.1048269Losses:  5.856219291687012 2.0573534965515137
CurrentTrain: epoch  5, batch     1 | loss: 5.8562193Losses:  6.074110984802246 2.0907864570617676
CurrentTrain: epoch  5, batch     2 | loss: 6.0741110Losses:  4.441627025604248 0.6836830377578735
CurrentTrain: epoch  5, batch     3 | loss: 4.4416270Losses:  6.253216743469238 2.0031814575195312
CurrentTrain: epoch  6, batch     0 | loss: 6.2532167Losses:  5.658881187438965 1.7413889169692993
CurrentTrain: epoch  6, batch     1 | loss: 5.6588812Losses:  4.903497695922852 1.8333206176757812
CurrentTrain: epoch  6, batch     2 | loss: 4.9034977Losses:  5.190273761749268 0.6640774011611938
CurrentTrain: epoch  6, batch     3 | loss: 5.1902738Losses:  5.278767108917236 1.9202075004577637
CurrentTrain: epoch  7, batch     0 | loss: 5.2787671Losses:  5.679513931274414 1.8924604654312134
CurrentTrain: epoch  7, batch     1 | loss: 5.6795139Losses:  5.48330020904541 2.0867624282836914
CurrentTrain: epoch  7, batch     2 | loss: 5.4833002Losses:  4.939920425415039 0.6878456473350525
CurrentTrain: epoch  7, batch     3 | loss: 4.9399204Losses:  5.415288925170898 1.9447144269943237
CurrentTrain: epoch  8, batch     0 | loss: 5.4152889Losses:  5.139814853668213 1.9212182760238647
CurrentTrain: epoch  8, batch     1 | loss: 5.1398149Losses:  5.568027496337891 2.204176425933838
CurrentTrain: epoch  8, batch     2 | loss: 5.5680275Losses:  3.7786707878112793 0.7005548477172852
CurrentTrain: epoch  8, batch     3 | loss: 3.7786708Losses:  5.056783199310303 2.1379880905151367
CurrentTrain: epoch  9, batch     0 | loss: 5.0567832Losses:  5.4644646644592285 2.0546698570251465
CurrentTrain: epoch  9, batch     1 | loss: 5.4644647Losses:  4.936878204345703 1.9954028129577637
CurrentTrain: epoch  9, batch     2 | loss: 4.9368782Losses:  3.0555360317230225 0.6925427317619324
CurrentTrain: epoch  9, batch     3 | loss: 3.0555360
Losses:  3.1395609378814697 2.6486706733703613
MemoryTrain:  epoch  0, batch     0 | loss: 3.1395609Losses:  2.895401954650879 2.6742444038391113
MemoryTrain:  epoch  0, batch     1 | loss: 2.8954020Losses:  2.977280378341675 2.667306661605835
MemoryTrain:  epoch  0, batch     2 | loss: 2.9772804Losses:  2.890695095062256 2.6283211708068848
MemoryTrain:  epoch  0, batch     3 | loss: 2.8906951Losses:  1.656650424003601 1.612821340560913
MemoryTrain:  epoch  0, batch     4 | loss: 1.6566504Losses:  2.948150634765625 2.606840133666992
MemoryTrain:  epoch  1, batch     0 | loss: 2.9481506Losses:  3.1245827674865723 2.616593360900879
MemoryTrain:  epoch  1, batch     1 | loss: 3.1245828Losses:  3.1934103965759277 2.6707143783569336
MemoryTrain:  epoch  1, batch     2 | loss: 3.1934104Losses:  3.3342299461364746 2.6503095626831055
MemoryTrain:  epoch  1, batch     3 | loss: 3.3342299Losses:  1.9114165306091309 1.7130050659179688
MemoryTrain:  epoch  1, batch     4 | loss: 1.9114165Losses:  2.761775016784668 2.610055446624756
MemoryTrain:  epoch  2, batch     0 | loss: 2.7617750Losses:  2.8021931648254395 2.606290817260742
MemoryTrain:  epoch  2, batch     1 | loss: 2.8021932Losses:  2.8165383338928223 2.6656787395477295
MemoryTrain:  epoch  2, batch     2 | loss: 2.8165383Losses:  3.1110591888427734 2.6688060760498047
MemoryTrain:  epoch  2, batch     3 | loss: 3.1110592Losses:  1.86371648311615 1.7370328903198242
MemoryTrain:  epoch  2, batch     4 | loss: 1.8637165Losses:  2.8223841190338135 2.6681766510009766
MemoryTrain:  epoch  3, batch     0 | loss: 2.8223841Losses:  2.7155303955078125 2.626760482788086
MemoryTrain:  epoch  3, batch     1 | loss: 2.7155304Losses:  2.7554166316986084 2.639713764190674
MemoryTrain:  epoch  3, batch     2 | loss: 2.7554166Losses:  2.875770092010498 2.6315276622772217
MemoryTrain:  epoch  3, batch     3 | loss: 2.8757701Losses:  1.9142639636993408 1.663553237915039
MemoryTrain:  epoch  3, batch     4 | loss: 1.9142640Losses:  2.677157163619995 2.6154308319091797
MemoryTrain:  epoch  4, batch     0 | loss: 2.6771572Losses:  2.7573344707489014 2.6343092918395996
MemoryTrain:  epoch  4, batch     1 | loss: 2.7573345Losses:  2.71694016456604 2.626901149749756
MemoryTrain:  epoch  4, batch     2 | loss: 2.7169402Losses:  2.886183500289917 2.6721980571746826
MemoryTrain:  epoch  4, batch     3 | loss: 2.8861835Losses:  1.8263049125671387 1.7175452709197998
MemoryTrain:  epoch  4, batch     4 | loss: 1.8263049Losses:  2.770137071609497 2.640871524810791
MemoryTrain:  epoch  5, batch     0 | loss: 2.7701371Losses:  2.784311294555664 2.6745057106018066
MemoryTrain:  epoch  5, batch     1 | loss: 2.7843113Losses:  2.6909866333007812 2.6120002269744873
MemoryTrain:  epoch  5, batch     2 | loss: 2.6909866Losses:  2.7308478355407715 2.643278121948242
MemoryTrain:  epoch  5, batch     3 | loss: 2.7308478Losses:  1.6269835233688354 1.6017627716064453
MemoryTrain:  epoch  5, batch     4 | loss: 1.6269835Losses:  2.6809146404266357 2.645883083343506
MemoryTrain:  epoch  6, batch     0 | loss: 2.6809146Losses:  2.674478054046631 2.617990016937256
MemoryTrain:  epoch  6, batch     1 | loss: 2.6744781Losses:  2.7826690673828125 2.623124122619629
MemoryTrain:  epoch  6, batch     2 | loss: 2.7826691Losses:  2.7268664836883545 2.6382460594177246
MemoryTrain:  epoch  6, batch     3 | loss: 2.7268665Losses:  1.7652491331100464 1.7236692905426025
MemoryTrain:  epoch  6, batch     4 | loss: 1.7652491Losses:  2.6925947666168213 2.6422786712646484
MemoryTrain:  epoch  7, batch     0 | loss: 2.6925948Losses:  2.735398054122925 2.6520543098449707
MemoryTrain:  epoch  7, batch     1 | loss: 2.7353981Losses:  2.6889452934265137 2.634864568710327
MemoryTrain:  epoch  7, batch     2 | loss: 2.6889453Losses:  2.6784188747406006 2.6055502891540527
MemoryTrain:  epoch  7, batch     3 | loss: 2.6784189Losses:  1.6779447793960571 1.6493252515792847
MemoryTrain:  epoch  7, batch     4 | loss: 1.6779448Losses:  2.6811106204986572 2.621769428253174
MemoryTrain:  epoch  8, batch     0 | loss: 2.6811106Losses:  2.7022972106933594 2.6489200592041016
MemoryTrain:  epoch  8, batch     1 | loss: 2.7022972Losses:  2.6927671432495117 2.647467613220215
MemoryTrain:  epoch  8, batch     2 | loss: 2.6927671Losses:  2.682065963745117 2.613227367401123
MemoryTrain:  epoch  8, batch     3 | loss: 2.6820660Losses:  1.779040813446045 1.6723459959030151
MemoryTrain:  epoch  8, batch     4 | loss: 1.7790408Losses:  2.7226665019989014 2.658379554748535
MemoryTrain:  epoch  9, batch     0 | loss: 2.7226665Losses:  2.6485490798950195 2.5928521156311035
MemoryTrain:  epoch  9, batch     1 | loss: 2.6485491Losses:  2.6846141815185547 2.641207695007324
MemoryTrain:  epoch  9, batch     2 | loss: 2.6846142Losses:  2.6520395278930664 2.6149661540985107
MemoryTrain:  epoch  9, batch     3 | loss: 2.6520395Losses:  1.749420166015625 1.6940619945526123
MemoryTrain:  epoch  9, batch     4 | loss: 1.7494202
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 55.71%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 52.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.72%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 48.84%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.10%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 45.47%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 44.38%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 42.94%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 43.36%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 44.89%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 45.77%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 48.26%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 49.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 50.82%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 51.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.81%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 53.66%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 54.32%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 55.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 57.85%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 59.18%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 59.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 59.93%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 60.73%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 61.23%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 61.62%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 60.99%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 60.49%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 60.21%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 59.13%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 88.48%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 86.18%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 85.45%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 85.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 83.30%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 83.37%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 82.77%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 82.28%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 81.80%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.34%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 81.87%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 81.49%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.41%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.33%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 81.10%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 80.79%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 80.12%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 79.39%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 78.82%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 78.27%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 77.51%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 77.20%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 77.18%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 76.95%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 76.53%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 76.05%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 75.46%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 75.13%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 74.74%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 74.62%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 74.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 73.89%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 73.60%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 73.45%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.35%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 72.96%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.40%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 71.90%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.36%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 70.78%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 70.42%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 71.82%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.51%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 71.67%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 70.74%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.25%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 69.76%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 70.16%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.74%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 69.07%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 68.31%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 68.31%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 67.99%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 67.58%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 66.78%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 66.28%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 65.84%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 67.45%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 67.00%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 66.25%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 66.00%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 65.71%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 65.67%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.66%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.65%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 69.57%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 69.66%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 69.61%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 69.73%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.61%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 69.29%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 69.19%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 69.26%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 69.65%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.60%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.90%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.51%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 68.29%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 68.08%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 67.86%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 67.40%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  292 | acc: 18.75%,  total acc: 67.02%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 66.86%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 66.90%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 67.94%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.84%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 67.90%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 67.93%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 67.72%   [EVAL] batch:  328 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 68.44%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 68.91%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 68.80%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 68.70%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 68.49%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 68.40%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 68.66%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 68.48%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 68.38%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 68.30%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  374 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 68.05%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 67.89%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 67.72%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 67.60%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 67.31%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.57%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 67.75%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 67.55%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 67.48%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 67.34%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 67.07%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 66.90%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.73%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 66.57%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 66.44%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 67.03%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 67.21%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 67.26%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 67.23%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 67.15%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 67.10%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 67.04%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 66.89%   
cur_acc:  ['0.9454', '0.7431', '0.7103', '0.8323', '0.7083', '0.6994', '0.5913']
his_acc:  ['0.9454', '0.8375', '0.7773', '0.7610', '0.7494', '0.7172', '0.6689']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  8.20167350769043 2.138392448425293
CurrentTrain: epoch  0, batch     0 | loss: 8.2016735Losses:  8.581806182861328 1.9773874282836914
CurrentTrain: epoch  0, batch     1 | loss: 8.5818062Losses:  7.6639275550842285 2.0365262031555176
CurrentTrain: epoch  0, batch     2 | loss: 7.6639276Losses:  6.650808334350586 0.655063807964325
CurrentTrain: epoch  0, batch     3 | loss: 6.6508083Losses:  6.8998870849609375 1.8443976640701294
CurrentTrain: epoch  1, batch     0 | loss: 6.8998871Losses:  7.097245216369629 1.9446003437042236
CurrentTrain: epoch  1, batch     1 | loss: 7.0972452Losses:  7.085082530975342 1.8761264085769653
CurrentTrain: epoch  1, batch     2 | loss: 7.0850825Losses:  2.8906090259552 0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.8906090Losses:  6.598345756530762 1.9290237426757812
CurrentTrain: epoch  2, batch     0 | loss: 6.5983458Losses:  6.8717827796936035 1.9155546426773071
CurrentTrain: epoch  2, batch     1 | loss: 6.8717828Losses:  6.458708763122559 1.9210444688796997
CurrentTrain: epoch  2, batch     2 | loss: 6.4587088Losses:  3.8500237464904785 0.667228639125824
CurrentTrain: epoch  2, batch     3 | loss: 3.8500237Losses:  6.469228267669678 2.078561305999756
CurrentTrain: epoch  3, batch     0 | loss: 6.4692283Losses:  6.2251386642456055 1.9298121929168701
CurrentTrain: epoch  3, batch     1 | loss: 6.2251387Losses:  6.113655090332031 2.063927412033081
CurrentTrain: epoch  3, batch     2 | loss: 6.1136551Losses:  6.34404993057251 0.7212164998054504
CurrentTrain: epoch  3, batch     3 | loss: 6.3440499Losses:  5.985630989074707 2.063725471496582
CurrentTrain: epoch  4, batch     0 | loss: 5.9856310Losses:  5.467472553253174 2.066929817199707
CurrentTrain: epoch  4, batch     1 | loss: 5.4674726Losses:  6.622389316558838 2.03108549118042
CurrentTrain: epoch  4, batch     2 | loss: 6.6223893Losses:  7.1435723304748535 0.6650224328041077
CurrentTrain: epoch  4, batch     3 | loss: 7.1435723Losses:  6.288135528564453 2.109102487564087
CurrentTrain: epoch  5, batch     0 | loss: 6.2881355Losses:  6.017274379730225 1.8378582000732422
CurrentTrain: epoch  5, batch     1 | loss: 6.0172744Losses:  5.320125579833984 1.8049125671386719
CurrentTrain: epoch  5, batch     2 | loss: 5.3201256Losses:  2.733975410461426 0.6614969968795776
CurrentTrain: epoch  5, batch     3 | loss: 2.7339754Losses:  6.331214904785156 2.169266939163208
CurrentTrain: epoch  6, batch     0 | loss: 6.3312149Losses:  5.57444953918457 1.995833396911621
CurrentTrain: epoch  6, batch     1 | loss: 5.5744495Losses:  4.935825824737549 1.9282785654067993
CurrentTrain: epoch  6, batch     2 | loss: 4.9358258Losses:  3.2801480293273926 0.6654462814331055
CurrentTrain: epoch  6, batch     3 | loss: 3.2801480Losses:  5.623030662536621 1.9605748653411865
CurrentTrain: epoch  7, batch     0 | loss: 5.6230307Losses:  5.117297172546387 1.942490577697754
CurrentTrain: epoch  7, batch     1 | loss: 5.1172972Losses:  5.264336109161377 2.159412145614624
CurrentTrain: epoch  7, batch     2 | loss: 5.2643361Losses:  3.6592986583709717 0.6378765106201172
CurrentTrain: epoch  7, batch     3 | loss: 3.6592987Losses:  5.325125694274902 1.9266879558563232
CurrentTrain: epoch  8, batch     0 | loss: 5.3251257Losses:  4.989717483520508 1.8856632709503174
CurrentTrain: epoch  8, batch     1 | loss: 4.9897175Losses:  4.742820739746094 2.0580039024353027
CurrentTrain: epoch  8, batch     2 | loss: 4.7428207Losses:  4.192627906799316 0.6872358918190002
CurrentTrain: epoch  8, batch     3 | loss: 4.1926279Losses:  4.445409774780273 1.9130029678344727
CurrentTrain: epoch  9, batch     0 | loss: 4.4454098Losses:  5.073773384094238 2.1678380966186523
CurrentTrain: epoch  9, batch     1 | loss: 5.0737734Losses:  4.737084865570068 1.9011369943618774
CurrentTrain: epoch  9, batch     2 | loss: 4.7370849Losses:  2.8674936294555664 0.6654214859008789
CurrentTrain: epoch  9, batch     3 | loss: 2.8674936
Losses:  2.698474407196045 2.6171820163726807
MemoryTrain:  epoch  0, batch     0 | loss: 2.6984744Losses:  3.5225887298583984 2.644191026687622
MemoryTrain:  epoch  0, batch     1 | loss: 3.5225887Losses:  3.100003480911255 2.6769609451293945
MemoryTrain:  epoch  0, batch     2 | loss: 3.1000035Losses:  2.9568915367126465 2.6339762210845947
MemoryTrain:  epoch  0, batch     3 | loss: 2.9568915Losses:  3.317054033279419 2.644200325012207
MemoryTrain:  epoch  0, batch     4 | loss: 3.3170540Losses:  3.9220306873321533 2.61472225189209
MemoryTrain:  epoch  1, batch     0 | loss: 3.9220307Losses:  2.820476770401001 2.6182708740234375
MemoryTrain:  epoch  1, batch     1 | loss: 2.8204768Losses:  3.1197502613067627 2.661857843399048
MemoryTrain:  epoch  1, batch     2 | loss: 3.1197503Losses:  3.167562246322632 2.680907726287842
MemoryTrain:  epoch  1, batch     3 | loss: 3.1675622Losses:  3.1160991191864014 2.623891830444336
MemoryTrain:  epoch  1, batch     4 | loss: 3.1160991Losses:  2.9155080318450928 2.6118266582489014
MemoryTrain:  epoch  2, batch     0 | loss: 2.9155080Losses:  3.1107115745544434 2.672926187515259
MemoryTrain:  epoch  2, batch     1 | loss: 3.1107116Losses:  2.9577057361602783 2.6431548595428467
MemoryTrain:  epoch  2, batch     2 | loss: 2.9577057Losses:  2.7582614421844482 2.6568715572357178
MemoryTrain:  epoch  2, batch     3 | loss: 2.7582614Losses:  2.8182737827301025 2.6082241535186768
MemoryTrain:  epoch  2, batch     4 | loss: 2.8182738Losses:  2.7640914916992188 2.6515793800354004
MemoryTrain:  epoch  3, batch     0 | loss: 2.7640915Losses:  2.871220111846924 2.6368377208709717
MemoryTrain:  epoch  3, batch     1 | loss: 2.8712201Losses:  2.8423855304718018 2.6353697776794434
MemoryTrain:  epoch  3, batch     2 | loss: 2.8423855Losses:  2.7608463764190674 2.6256401538848877
MemoryTrain:  epoch  3, batch     3 | loss: 2.7608464Losses:  2.884295701980591 2.628478765487671
MemoryTrain:  epoch  3, batch     4 | loss: 2.8842957Losses:  2.8006699085235596 2.65106463432312
MemoryTrain:  epoch  4, batch     0 | loss: 2.8006699Losses:  2.7180163860321045 2.6262478828430176
MemoryTrain:  epoch  4, batch     1 | loss: 2.7180164Losses:  2.78413462638855 2.6196889877319336
MemoryTrain:  epoch  4, batch     2 | loss: 2.7841346Losses:  2.7969858646392822 2.6421854496002197
MemoryTrain:  epoch  4, batch     3 | loss: 2.7969859Losses:  2.7506113052368164 2.6357297897338867
MemoryTrain:  epoch  4, batch     4 | loss: 2.7506113Losses:  2.6355674266815186 2.574227809906006
MemoryTrain:  epoch  5, batch     0 | loss: 2.6355674Losses:  2.688863515853882 2.6517233848571777
MemoryTrain:  epoch  5, batch     1 | loss: 2.6888635Losses:  2.755129814147949 2.6435070037841797
MemoryTrain:  epoch  5, batch     2 | loss: 2.7551298Losses:  2.7587780952453613 2.6747355461120605
MemoryTrain:  epoch  5, batch     3 | loss: 2.7587781Losses:  2.7271296977996826 2.6249208450317383
MemoryTrain:  epoch  5, batch     4 | loss: 2.7271297Losses:  2.684737205505371 2.5953168869018555
MemoryTrain:  epoch  6, batch     0 | loss: 2.6847372Losses:  2.660403251647949 2.6140012741088867
MemoryTrain:  epoch  6, batch     1 | loss: 2.6604033Losses:  2.6919050216674805 2.6294949054718018
MemoryTrain:  epoch  6, batch     2 | loss: 2.6919050Losses:  2.749744415283203 2.6800215244293213
MemoryTrain:  epoch  6, batch     3 | loss: 2.7497444Losses:  2.674107313156128 2.629258155822754
MemoryTrain:  epoch  6, batch     4 | loss: 2.6741073Losses:  2.7055087089538574 2.6538233757019043
MemoryTrain:  epoch  7, batch     0 | loss: 2.7055087Losses:  2.700406312942505 2.6437244415283203
MemoryTrain:  epoch  7, batch     1 | loss: 2.7004063Losses:  2.664915084838867 2.619579792022705
MemoryTrain:  epoch  7, batch     2 | loss: 2.6649151Losses:  2.6999359130859375 2.6484813690185547
MemoryTrain:  epoch  7, batch     3 | loss: 2.6999359Losses:  2.6632490158081055 2.5986061096191406
MemoryTrain:  epoch  7, batch     4 | loss: 2.6632490Losses:  2.682512044906616 2.6283974647521973
MemoryTrain:  epoch  8, batch     0 | loss: 2.6825120Losses:  2.64664626121521 2.614295244216919
MemoryTrain:  epoch  8, batch     1 | loss: 2.6466463Losses:  2.741724967956543 2.6396610736846924
MemoryTrain:  epoch  8, batch     2 | loss: 2.7417250Losses:  2.6942665576934814 2.6558289527893066
MemoryTrain:  epoch  8, batch     3 | loss: 2.6942666Losses:  2.7136824131011963 2.641005516052246
MemoryTrain:  epoch  8, batch     4 | loss: 2.7136824Losses:  2.6858201026916504 2.6477532386779785
MemoryTrain:  epoch  9, batch     0 | loss: 2.6858201Losses:  2.6719768047332764 2.6108920574188232
MemoryTrain:  epoch  9, batch     1 | loss: 2.6719768Losses:  2.7409613132476807 2.667685031890869
MemoryTrain:  epoch  9, batch     2 | loss: 2.7409613Losses:  2.632232427597046 2.5853471755981445
MemoryTrain:  epoch  9, batch     3 | loss: 2.6322324Losses:  2.66750168800354 2.627166748046875
MemoryTrain:  epoch  9, batch     4 | loss: 2.6675017
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 74.58%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 71.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 73.14%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 71.15%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 71.00%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 69.44%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.78%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 87.38%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 86.46%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 85.27%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 83.37%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 83.02%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.58%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.36%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 81.75%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 80.57%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 79.62%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 78.50%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 77.43%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 76.38%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 75.72%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 76.54%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 76.60%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 76.62%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 76.68%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 76.66%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 76.72%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 75.75%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 75.07%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 73.91%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 73.20%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 72.89%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 71.91%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 71.42%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 71.13%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 70.73%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 70.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 69.52%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 69.16%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 68.63%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 68.18%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 67.67%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 67.12%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 66.74%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 68.60%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 69.16%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 67.77%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 67.30%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 66.83%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.54%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 67.38%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 66.98%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 66.27%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 65.89%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 65.50%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 65.70%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 64.91%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 64.52%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 64.14%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 63.87%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 63.93%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 63.72%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 63.71%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 63.49%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 63.20%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 63.09%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 62.95%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 63.41%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 64.33%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.70%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 64.83%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 64.61%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 64.59%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 64.34%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 64.15%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 63.91%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.60%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 63.36%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 63.09%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 64.16%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.26%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 67.14%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.21%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 67.10%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 67.09%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 66.73%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 66.62%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 66.82%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 66.82%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  282 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  283 | acc: 0.00%,  total acc: 66.11%   [EVAL] batch:  284 | acc: 0.00%,  total acc: 65.88%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 65.67%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 65.23%   [EVAL] batch:  288 | acc: 6.25%,  total acc: 65.03%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 64.81%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 64.58%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  292 | acc: 18.75%,  total acc: 64.23%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 64.07%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 64.05%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 64.09%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 64.08%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 65.09%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 64.84%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.82%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 64.95%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 64.79%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 64.65%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 64.51%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 64.38%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 65.76%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 65.59%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 65.29%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 65.11%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 64.96%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.43%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 65.37%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 65.27%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 65.20%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 65.04%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 64.98%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:  373 | acc: 50.00%,  total acc: 64.76%   [EVAL] batch:  374 | acc: 25.00%,  total acc: 64.65%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 64.53%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 64.46%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 64.32%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 64.22%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 64.10%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 63.96%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 64.21%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  394 | acc: 0.00%,  total acc: 64.51%   [EVAL] batch:  395 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 64.07%   [EVAL] batch:  398 | acc: 0.00%,  total acc: 63.91%   [EVAL] batch:  399 | acc: 0.00%,  total acc: 63.75%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.61%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 63.45%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 63.29%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 63.13%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 62.82%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 62.93%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 63.22%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.65%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 63.81%   [EVAL] batch:  426 | acc: 43.75%,  total acc: 63.76%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 63.76%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.74%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  431 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 63.51%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 63.46%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 63.40%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 63.40%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 63.42%   [EVAL] batch:  440 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 63.45%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  445 | acc: 87.50%,  total acc: 63.58%   [EVAL] batch:  446 | acc: 50.00%,  total acc: 63.55%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 63.63%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.84%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 63.91%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 63.92%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 63.83%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 63.79%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 63.69%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 63.65%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 63.91%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 64.52%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  478 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  481 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 64.35%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 64.23%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 64.54%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 64.48%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 64.40%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 64.35%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:  498 | acc: 37.50%,  total acc: 64.24%   [EVAL] batch:  499 | acc: 37.50%,  total acc: 64.19%   
cur_acc:  ['0.9454', '0.7431', '0.7103', '0.8323', '0.7083', '0.6994', '0.5913', '0.6944']
his_acc:  ['0.9454', '0.8375', '0.7773', '0.7610', '0.7494', '0.7172', '0.6689', '0.6419']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  12.343778610229492 1.9482017755508423
CurrentTrain: epoch  0, batch     0 | loss: 12.3437786Losses:  12.067928314208984 2.113302707672119
CurrentTrain: epoch  0, batch     1 | loss: 12.0679283Losses:  11.542136192321777 1.8062869310379028
CurrentTrain: epoch  0, batch     2 | loss: 11.5421362Losses:  12.239473342895508 2.2177109718322754
CurrentTrain: epoch  0, batch     3 | loss: 12.2394733Losses:  11.846378326416016 1.92684805393219
CurrentTrain: epoch  0, batch     4 | loss: 11.8463783Losses:  11.196611404418945 2.015629291534424
CurrentTrain: epoch  0, batch     5 | loss: 11.1966114Losses:  11.491458892822266 1.9109437465667725
CurrentTrain: epoch  0, batch     6 | loss: 11.4914589Losses:  11.098770141601562 1.8468444347381592
CurrentTrain: epoch  0, batch     7 | loss: 11.0987701Losses:  11.55198860168457 2.0467445850372314
CurrentTrain: epoch  0, batch     8 | loss: 11.5519886Losses:  10.99558162689209 2.044947862625122
CurrentTrain: epoch  0, batch     9 | loss: 10.9955816Losses:  11.57468032836914 2.0788378715515137
CurrentTrain: epoch  0, batch    10 | loss: 11.5746803Losses:  10.69123649597168 2.0515918731689453
CurrentTrain: epoch  0, batch    11 | loss: 10.6912365Losses:  10.600387573242188 1.8179306983947754
CurrentTrain: epoch  0, batch    12 | loss: 10.6003876Losses:  10.725119590759277 2.0767602920532227
CurrentTrain: epoch  0, batch    13 | loss: 10.7251196Losses:  10.920589447021484 1.8319292068481445
CurrentTrain: epoch  0, batch    14 | loss: 10.9205894Losses:  10.184432983398438 2.030120372772217
CurrentTrain: epoch  0, batch    15 | loss: 10.1844330Losses:  10.647799491882324 2.0751922130584717
CurrentTrain: epoch  0, batch    16 | loss: 10.6477995Losses:  10.26740837097168 1.8426437377929688
CurrentTrain: epoch  0, batch    17 | loss: 10.2674084Losses:  10.612695693969727 1.7981013059616089
CurrentTrain: epoch  0, batch    18 | loss: 10.6126957Losses:  9.93425464630127 2.0388376712799072
CurrentTrain: epoch  0, batch    19 | loss: 9.9342546Losses:  9.876797676086426 1.8505715131759644
CurrentTrain: epoch  0, batch    20 | loss: 9.8767977Losses:  10.319005966186523 1.8602479696273804
CurrentTrain: epoch  0, batch    21 | loss: 10.3190060Losses:  9.585050582885742 1.6963273286819458
CurrentTrain: epoch  0, batch    22 | loss: 9.5850506Losses:  9.975484848022461 1.9776064157485962
CurrentTrain: epoch  0, batch    23 | loss: 9.9754848Losses:  10.659469604492188 2.073615074157715
CurrentTrain: epoch  0, batch    24 | loss: 10.6594696Losses:  9.729762077331543 1.7461227178573608
CurrentTrain: epoch  0, batch    25 | loss: 9.7297621Losses:  10.476093292236328 2.019522190093994
CurrentTrain: epoch  0, batch    26 | loss: 10.4760933Losses:  10.63039779663086 2.091306686401367
CurrentTrain: epoch  0, batch    27 | loss: 10.6303978Losses:  10.13553237915039 1.9098718166351318
CurrentTrain: epoch  0, batch    28 | loss: 10.1355324Losses:  9.84688663482666 1.7842085361480713
CurrentTrain: epoch  0, batch    29 | loss: 9.8468866Losses:  9.920289993286133 2.0327353477478027
CurrentTrain: epoch  0, batch    30 | loss: 9.9202900Losses:  10.281780242919922 2.2020184993743896
CurrentTrain: epoch  0, batch    31 | loss: 10.2817802Losses:  10.488983154296875 2.078526258468628
CurrentTrain: epoch  0, batch    32 | loss: 10.4889832Losses:  10.034122467041016 2.094635486602783
CurrentTrain: epoch  0, batch    33 | loss: 10.0341225Losses:  9.69485092163086 2.052069664001465
CurrentTrain: epoch  0, batch    34 | loss: 9.6948509Losses:  9.110071182250977 1.895444631576538
CurrentTrain: epoch  0, batch    35 | loss: 9.1100712Losses:  9.788747787475586 1.6351451873779297
CurrentTrain: epoch  0, batch    36 | loss: 9.7887478Losses:  9.54921817779541 2.019352674484253
CurrentTrain: epoch  0, batch    37 | loss: 9.5492182Losses:  9.885238647460938 2.0496809482574463
CurrentTrain: epoch  0, batch    38 | loss: 9.8852386Losses:  10.054744720458984 2.067352056503296
CurrentTrain: epoch  0, batch    39 | loss: 10.0547447Losses:  9.670461654663086 2.032020092010498
CurrentTrain: epoch  0, batch    40 | loss: 9.6704617Losses:  9.568376541137695 1.977310061454773
CurrentTrain: epoch  0, batch    41 | loss: 9.5683765Losses:  9.352277755737305 1.8862531185150146
CurrentTrain: epoch  0, batch    42 | loss: 9.3522778Losses:  9.587262153625488 1.8507672548294067
CurrentTrain: epoch  0, batch    43 | loss: 9.5872622Losses:  9.217850685119629 1.860947608947754
CurrentTrain: epoch  0, batch    44 | loss: 9.2178507Losses:  9.53514289855957 1.8608380556106567
CurrentTrain: epoch  0, batch    45 | loss: 9.5351429Losses:  9.700761795043945 1.9327938556671143
CurrentTrain: epoch  0, batch    46 | loss: 9.7007618Losses:  9.15255355834961 1.9906251430511475
CurrentTrain: epoch  0, batch    47 | loss: 9.1525536Losses:  9.767289161682129 1.9445239305496216
CurrentTrain: epoch  0, batch    48 | loss: 9.7672892Losses:  9.102655410766602 2.044905185699463
CurrentTrain: epoch  0, batch    49 | loss: 9.1026554Losses:  10.107337951660156 1.5130431652069092
CurrentTrain: epoch  0, batch    50 | loss: 10.1073380Losses:  8.516828536987305 1.820495367050171
CurrentTrain: epoch  0, batch    51 | loss: 8.5168285Losses:  9.395195960998535 2.0243072509765625
CurrentTrain: epoch  0, batch    52 | loss: 9.3951960Losses:  9.170160293579102 1.6832680702209473
CurrentTrain: epoch  0, batch    53 | loss: 9.1701603Losses:  10.64584732055664 1.9370170831680298
CurrentTrain: epoch  0, batch    54 | loss: 10.6458473Losses:  10.308616638183594 1.9577633142471313
CurrentTrain: epoch  0, batch    55 | loss: 10.3086166Losses:  8.589263916015625 1.924239158630371
CurrentTrain: epoch  0, batch    56 | loss: 8.5892639Losses:  8.518747329711914 1.7505518198013306
CurrentTrain: epoch  0, batch    57 | loss: 8.5187473Losses:  9.257864952087402 1.9731695652008057
CurrentTrain: epoch  0, batch    58 | loss: 9.2578650Losses:  9.451591491699219 1.6635711193084717
CurrentTrain: epoch  0, batch    59 | loss: 9.4515915Losses:  9.344850540161133 1.9427762031555176
CurrentTrain: epoch  0, batch    60 | loss: 9.3448505Losses:  8.871343612670898 1.863511323928833
CurrentTrain: epoch  0, batch    61 | loss: 8.8713436Losses:  8.314085960388184 1.6951968669891357
CurrentTrain: epoch  0, batch    62 | loss: 8.3140860Losses:  10.30987548828125 1.8601104021072388
CurrentTrain: epoch  1, batch     0 | loss: 10.3098755Losses:  8.305455207824707 1.9268081188201904
CurrentTrain: epoch  1, batch     1 | loss: 8.3054552Losses:  9.246252059936523 2.0398292541503906
CurrentTrain: epoch  1, batch     2 | loss: 9.2462521Losses:  8.555122375488281 1.7540922164916992
CurrentTrain: epoch  1, batch     3 | loss: 8.5551224Losses:  8.261148452758789 1.9920613765716553
CurrentTrain: epoch  1, batch     4 | loss: 8.2611485Losses:  8.004410743713379 1.741041660308838
CurrentTrain: epoch  1, batch     5 | loss: 8.0044107Losses:  9.011466026306152 1.7621791362762451
CurrentTrain: epoch  1, batch     6 | loss: 9.0114660Losses:  9.021825790405273 1.9550516605377197
CurrentTrain: epoch  1, batch     7 | loss: 9.0218258Losses:  9.03009033203125 2.0412909984588623
CurrentTrain: epoch  1, batch     8 | loss: 9.0300903Losses:  7.9401350021362305 1.4845879077911377
CurrentTrain: epoch  1, batch     9 | loss: 7.9401350Losses:  8.70650863647461 1.8973959684371948
CurrentTrain: epoch  1, batch    10 | loss: 8.7065086Losses:  9.623783111572266 1.9716846942901611
CurrentTrain: epoch  1, batch    11 | loss: 9.6237831Losses:  8.724165916442871 1.896661400794983
CurrentTrain: epoch  1, batch    12 | loss: 8.7241659Losses:  8.808993339538574 1.921963095664978
CurrentTrain: epoch  1, batch    13 | loss: 8.8089933Losses:  9.248820304870605 2.0591421127319336
CurrentTrain: epoch  1, batch    14 | loss: 9.2488203Losses:  8.724985122680664 1.8189573287963867
CurrentTrain: epoch  1, batch    15 | loss: 8.7249851Losses:  8.37277603149414 1.9926416873931885
CurrentTrain: epoch  1, batch    16 | loss: 8.3727760Losses:  9.663084983825684 1.8960273265838623
CurrentTrain: epoch  1, batch    17 | loss: 9.6630850Losses:  8.592767715454102 2.0506961345672607
CurrentTrain: epoch  1, batch    18 | loss: 8.5927677Losses:  8.537111282348633 1.9975850582122803
CurrentTrain: epoch  1, batch    19 | loss: 8.5371113Losses:  8.789770126342773 2.054192543029785
CurrentTrain: epoch  1, batch    20 | loss: 8.7897701Losses:  8.687911987304688 2.0321171283721924
CurrentTrain: epoch  1, batch    21 | loss: 8.6879120Losses:  8.372207641601562 2.029203414916992
CurrentTrain: epoch  1, batch    22 | loss: 8.3722076Losses:  8.70322036743164 1.9421021938323975
CurrentTrain: epoch  1, batch    23 | loss: 8.7032204Losses:  8.876174926757812 1.964189052581787
CurrentTrain: epoch  1, batch    24 | loss: 8.8761749Losses:  8.154404640197754 2.025557279586792
CurrentTrain: epoch  1, batch    25 | loss: 8.1544046Losses:  9.152242660522461 1.926950454711914
CurrentTrain: epoch  1, batch    26 | loss: 9.1522427Losses:  7.657878398895264 1.717928409576416
CurrentTrain: epoch  1, batch    27 | loss: 7.6578784Losses:  7.56291389465332 1.7314064502716064
CurrentTrain: epoch  1, batch    28 | loss: 7.5629139Losses:  8.363508224487305 2.050593852996826
CurrentTrain: epoch  1, batch    29 | loss: 8.3635082Losses:  8.47647762298584 2.029507875442505
CurrentTrain: epoch  1, batch    30 | loss: 8.4764776Losses:  8.560948371887207 1.8425049781799316
CurrentTrain: epoch  1, batch    31 | loss: 8.5609484Losses:  7.78736686706543 1.6484317779541016
CurrentTrain: epoch  1, batch    32 | loss: 7.7873669Losses:  8.197352409362793 1.972311019897461
CurrentTrain: epoch  1, batch    33 | loss: 8.1973524Losses:  8.378104209899902 1.9144338369369507
CurrentTrain: epoch  1, batch    34 | loss: 8.3781042Losses:  8.6229829788208 1.9814871549606323
CurrentTrain: epoch  1, batch    35 | loss: 8.6229830Losses:  8.33286190032959 2.1334969997406006
CurrentTrain: epoch  1, batch    36 | loss: 8.3328619Losses:  8.19246768951416 2.059222936630249
CurrentTrain: epoch  1, batch    37 | loss: 8.1924677Losses:  8.832767486572266 2.1364383697509766
CurrentTrain: epoch  1, batch    38 | loss: 8.8327675Losses:  6.775245666503906 1.4311797618865967
CurrentTrain: epoch  1, batch    39 | loss: 6.7752457Losses:  8.544690132141113 1.881777048110962
CurrentTrain: epoch  1, batch    40 | loss: 8.5446901Losses:  8.951615333557129 2.0265820026397705
CurrentTrain: epoch  1, batch    41 | loss: 8.9516153Losses:  7.7112345695495605 2.029059410095215
CurrentTrain: epoch  1, batch    42 | loss: 7.7112346Losses:  8.381996154785156 2.0462193489074707
CurrentTrain: epoch  1, batch    43 | loss: 8.3819962Losses:  9.186874389648438 1.9443137645721436
CurrentTrain: epoch  1, batch    44 | loss: 9.1868744Losses:  8.04073715209961 2.0430819988250732
CurrentTrain: epoch  1, batch    45 | loss: 8.0407372Losses:  7.760169982910156 1.7177343368530273
CurrentTrain: epoch  1, batch    46 | loss: 7.7601700Losses:  7.931588172912598 1.8621184825897217
CurrentTrain: epoch  1, batch    47 | loss: 7.9315882Losses:  8.917364120483398 1.7996548414230347
CurrentTrain: epoch  1, batch    48 | loss: 8.9173641Losses:  7.769337177276611 1.941880226135254
CurrentTrain: epoch  1, batch    49 | loss: 7.7693372Losses:  7.715029239654541 1.9159358739852905
CurrentTrain: epoch  1, batch    50 | loss: 7.7150292Losses:  6.782688140869141 1.5769795179367065
CurrentTrain: epoch  1, batch    51 | loss: 6.7826881Losses:  7.872899055480957 1.7648941278457642
CurrentTrain: epoch  1, batch    52 | loss: 7.8728991Losses:  7.119684219360352 1.8713948726654053
CurrentTrain: epoch  1, batch    53 | loss: 7.1196842Losses:  8.589997291564941 1.9695836305618286
CurrentTrain: epoch  1, batch    54 | loss: 8.5899973Losses:  7.217329978942871 1.6478347778320312
CurrentTrain: epoch  1, batch    55 | loss: 7.2173300Losses:  8.16705322265625 1.926077961921692
CurrentTrain: epoch  1, batch    56 | loss: 8.1670532Losses:  8.132011413574219 1.821436882019043
CurrentTrain: epoch  1, batch    57 | loss: 8.1320114Losses:  7.872897624969482 1.891726016998291
CurrentTrain: epoch  1, batch    58 | loss: 7.8728976Losses:  7.493589401245117 1.9206945896148682
CurrentTrain: epoch  1, batch    59 | loss: 7.4935894Losses:  7.410125255584717 1.9420784711837769
CurrentTrain: epoch  1, batch    60 | loss: 7.4101253Losses:  8.223381042480469 1.8488956689834595
CurrentTrain: epoch  1, batch    61 | loss: 8.2233810Losses:  8.154610633850098 1.4125244617462158
CurrentTrain: epoch  1, batch    62 | loss: 8.1546106Losses:  7.586678981781006 1.8704109191894531
CurrentTrain: epoch  2, batch     0 | loss: 7.5866790Losses:  7.190379619598389 1.9712854623794556
CurrentTrain: epoch  2, batch     1 | loss: 7.1903796Losses:  7.05579948425293 1.9151471853256226
CurrentTrain: epoch  2, batch     2 | loss: 7.0557995Losses:  7.13678503036499 1.869410514831543
CurrentTrain: epoch  2, batch     3 | loss: 7.1367850Losses:  7.144771575927734 1.8735113143920898
CurrentTrain: epoch  2, batch     4 | loss: 7.1447716Losses:  7.821406841278076 1.8251851797103882
CurrentTrain: epoch  2, batch     5 | loss: 7.8214068Losses:  7.246535778045654 1.9115214347839355
CurrentTrain: epoch  2, batch     6 | loss: 7.2465358Losses:  8.227170944213867 2.0465078353881836
CurrentTrain: epoch  2, batch     7 | loss: 8.2271709Losses:  7.5898308753967285 1.6865581274032593
CurrentTrain: epoch  2, batch     8 | loss: 7.5898309Losses:  6.868569850921631 1.8477773666381836
CurrentTrain: epoch  2, batch     9 | loss: 6.8685699Losses:  7.336089134216309 1.808278203010559
CurrentTrain: epoch  2, batch    10 | loss: 7.3360891Losses:  7.263426780700684 1.940899133682251
CurrentTrain: epoch  2, batch    11 | loss: 7.2634268Losses:  7.2429609298706055 1.861199140548706
CurrentTrain: epoch  2, batch    12 | loss: 7.2429609Losses:  7.3586931228637695 1.8937398195266724
CurrentTrain: epoch  2, batch    13 | loss: 7.3586931Losses:  7.562839508056641 1.761803388595581
CurrentTrain: epoch  2, batch    14 | loss: 7.5628395Losses:  7.009748458862305 1.898857593536377
CurrentTrain: epoch  2, batch    15 | loss: 7.0097485Losses:  7.310145378112793 1.727249264717102
CurrentTrain: epoch  2, batch    16 | loss: 7.3101454Losses:  8.373982429504395 1.7888649702072144
CurrentTrain: epoch  2, batch    17 | loss: 8.3739824Losses:  6.9622578620910645 1.7511463165283203
CurrentTrain: epoch  2, batch    18 | loss: 6.9622579Losses:  7.62244987487793 1.970997929573059
CurrentTrain: epoch  2, batch    19 | loss: 7.6224499Losses:  8.150130271911621 1.4169690608978271
CurrentTrain: epoch  2, batch    20 | loss: 8.1501303Losses:  7.919677734375 2.070563316345215
CurrentTrain: epoch  2, batch    21 | loss: 7.9196777Losses:  6.93160343170166 2.016139268875122
CurrentTrain: epoch  2, batch    22 | loss: 6.9316034Losses:  7.3910112380981445 1.8777084350585938
CurrentTrain: epoch  2, batch    23 | loss: 7.3910112Losses:  6.891592025756836 1.9218318462371826
CurrentTrain: epoch  2, batch    24 | loss: 6.8915920Losses:  6.759468078613281 1.467688798904419
CurrentTrain: epoch  2, batch    25 | loss: 6.7594681Losses:  6.7257795333862305 1.9611190557479858
CurrentTrain: epoch  2, batch    26 | loss: 6.7257795Losses:  7.1999640464782715 1.9854990243911743
CurrentTrain: epoch  2, batch    27 | loss: 7.1999640Losses:  6.582314491271973 1.634159803390503
CurrentTrain: epoch  2, batch    28 | loss: 6.5823145Losses:  7.76318359375 2.1492066383361816
CurrentTrain: epoch  2, batch    29 | loss: 7.7631836Losses:  7.301263332366943 1.9777644872665405
CurrentTrain: epoch  2, batch    30 | loss: 7.3012633Losses:  7.690032005310059 2.005032539367676
CurrentTrain: epoch  2, batch    31 | loss: 7.6900320Losses:  7.069736957550049 1.7316166162490845
CurrentTrain: epoch  2, batch    32 | loss: 7.0697370Losses:  7.2596659660339355 1.8324661254882812
CurrentTrain: epoch  2, batch    33 | loss: 7.2596660Losses:  8.070470809936523 1.903275966644287
CurrentTrain: epoch  2, batch    34 | loss: 8.0704708Losses:  7.128165245056152 1.7913777828216553
CurrentTrain: epoch  2, batch    35 | loss: 7.1281652Losses:  8.026586532592773 2.1269497871398926
CurrentTrain: epoch  2, batch    36 | loss: 8.0265865Losses:  7.2429399490356445 2.009648323059082
CurrentTrain: epoch  2, batch    37 | loss: 7.2429399Losses:  7.418753147125244 1.9061150550842285
CurrentTrain: epoch  2, batch    38 | loss: 7.4187531Losses:  7.715585708618164 2.110973358154297
CurrentTrain: epoch  2, batch    39 | loss: 7.7155857Losses:  8.126447677612305 1.9938130378723145
CurrentTrain: epoch  2, batch    40 | loss: 8.1264477Losses:  6.917896270751953 1.9186122417449951
CurrentTrain: epoch  2, batch    41 | loss: 6.9178963Losses:  7.076056480407715 1.8607654571533203
CurrentTrain: epoch  2, batch    42 | loss: 7.0760565Losses:  7.507135391235352 1.9420119524002075
CurrentTrain: epoch  2, batch    43 | loss: 7.5071354Losses:  6.8636345863342285 1.9098601341247559
CurrentTrain: epoch  2, batch    44 | loss: 6.8636346Losses:  6.414050579071045 1.5541529655456543
CurrentTrain: epoch  2, batch    45 | loss: 6.4140506Losses:  7.4456987380981445 1.9206740856170654
CurrentTrain: epoch  2, batch    46 | loss: 7.4456987Losses:  6.719315052032471 1.6995148658752441
CurrentTrain: epoch  2, batch    47 | loss: 6.7193151Losses:  7.369589805603027 1.7999794483184814
CurrentTrain: epoch  2, batch    48 | loss: 7.3695898Losses:  7.131180286407471 1.6700873374938965
CurrentTrain: epoch  2, batch    49 | loss: 7.1311803Losses:  6.923896312713623 1.733235478401184
CurrentTrain: epoch  2, batch    50 | loss: 6.9238963Losses:  7.198053359985352 1.8162806034088135
CurrentTrain: epoch  2, batch    51 | loss: 7.1980534Losses:  7.073640823364258 1.7094335556030273
CurrentTrain: epoch  2, batch    52 | loss: 7.0736408Losses:  7.059026718139648 1.781411051750183
CurrentTrain: epoch  2, batch    53 | loss: 7.0590267Losses:  7.2501044273376465 1.6599363088607788
CurrentTrain: epoch  2, batch    54 | loss: 7.2501044Losses:  7.409919738769531 1.800018310546875
CurrentTrain: epoch  2, batch    55 | loss: 7.4099197Losses:  7.0240678787231445 1.8229868412017822
CurrentTrain: epoch  2, batch    56 | loss: 7.0240679Losses:  7.310114860534668 1.8365612030029297
CurrentTrain: epoch  2, batch    57 | loss: 7.3101149Losses:  6.673549175262451 1.8290905952453613
CurrentTrain: epoch  2, batch    58 | loss: 6.6735492Losses:  6.974358558654785 2.039329767227173
CurrentTrain: epoch  2, batch    59 | loss: 6.9743586Losses:  6.589890956878662 1.8267778158187866
CurrentTrain: epoch  2, batch    60 | loss: 6.5898910Losses:  6.422277450561523 1.7513434886932373
CurrentTrain: epoch  2, batch    61 | loss: 6.4222775Losses:  6.201473236083984 1.3985114097595215
CurrentTrain: epoch  2, batch    62 | loss: 6.2014732Losses:  6.693202495574951 1.8282952308654785
CurrentTrain: epoch  3, batch     0 | loss: 6.6932025Losses:  6.564216136932373 1.755972981452942
CurrentTrain: epoch  3, batch     1 | loss: 6.5642161Losses:  6.60786247253418 1.7818214893341064
CurrentTrain: epoch  3, batch     2 | loss: 6.6078625Losses:  6.408631801605225 1.841437816619873
CurrentTrain: epoch  3, batch     3 | loss: 6.4086318Losses:  6.674161434173584 1.798013687133789
CurrentTrain: epoch  3, batch     4 | loss: 6.6741614Losses:  7.334814548492432 2.050915241241455
CurrentTrain: epoch  3, batch     5 | loss: 7.3348145Losses:  6.834324836730957 2.0495944023132324
CurrentTrain: epoch  3, batch     6 | loss: 6.8343248Losses:  6.829753875732422 1.826901912689209
CurrentTrain: epoch  3, batch     7 | loss: 6.8297539Losses:  6.750337600708008 1.7664563655853271
CurrentTrain: epoch  3, batch     8 | loss: 6.7503376Losses:  6.901293754577637 1.8715484142303467
CurrentTrain: epoch  3, batch     9 | loss: 6.9012938Losses:  7.06273078918457 1.751396894454956
CurrentTrain: epoch  3, batch    10 | loss: 7.0627308Losses:  6.5412211418151855 1.7704787254333496
CurrentTrain: epoch  3, batch    11 | loss: 6.5412211Losses:  7.061985969543457 2.0554590225219727
CurrentTrain: epoch  3, batch    12 | loss: 7.0619860Losses:  6.422822952270508 1.7573318481445312
CurrentTrain: epoch  3, batch    13 | loss: 6.4228230Losses:  7.666291236877441 1.9437496662139893
CurrentTrain: epoch  3, batch    14 | loss: 7.6662912Losses:  7.336915969848633 1.9012365341186523
CurrentTrain: epoch  3, batch    15 | loss: 7.3369160Losses:  7.235947608947754 1.9711236953735352
CurrentTrain: epoch  3, batch    16 | loss: 7.2359476Losses:  6.509828567504883 1.6185472011566162
CurrentTrain: epoch  3, batch    17 | loss: 6.5098286Losses:  6.680267810821533 1.7667044401168823
CurrentTrain: epoch  3, batch    18 | loss: 6.6802678Losses:  6.584855079650879 1.8926677703857422
CurrentTrain: epoch  3, batch    19 | loss: 6.5848551Losses:  6.628793716430664 1.9676705598831177
CurrentTrain: epoch  3, batch    20 | loss: 6.6287937Losses:  7.050405025482178 1.8375935554504395
CurrentTrain: epoch  3, batch    21 | loss: 7.0504050Losses:  6.738137722015381 1.9695916175842285
CurrentTrain: epoch  3, batch    22 | loss: 6.7381377Losses:  6.720686912536621 1.5124431848526
CurrentTrain: epoch  3, batch    23 | loss: 6.7206869Losses:  6.49708890914917 1.7074637413024902
CurrentTrain: epoch  3, batch    24 | loss: 6.4970889Losses:  6.756147861480713 1.8763518333435059
CurrentTrain: epoch  3, batch    25 | loss: 6.7561479Losses:  6.554394245147705 1.8770467042922974
CurrentTrain: epoch  3, batch    26 | loss: 6.5543942Losses:  6.827706813812256 1.961878776550293
CurrentTrain: epoch  3, batch    27 | loss: 6.8277068Losses:  6.918636322021484 1.538572072982788
CurrentTrain: epoch  3, batch    28 | loss: 6.9186363Losses:  6.713320732116699 1.9623031616210938
CurrentTrain: epoch  3, batch    29 | loss: 6.7133207Losses:  6.459712982177734 1.903275728225708
CurrentTrain: epoch  3, batch    30 | loss: 6.4597130Losses:  7.430525779724121 1.8459444046020508
CurrentTrain: epoch  3, batch    31 | loss: 7.4305258Losses:  6.551152229309082 1.852083683013916
CurrentTrain: epoch  3, batch    32 | loss: 6.5511522Losses:  6.7284698486328125 1.7190043926239014
CurrentTrain: epoch  3, batch    33 | loss: 6.7284698Losses:  7.335453987121582 2.003711223602295
CurrentTrain: epoch  3, batch    34 | loss: 7.3354540Losses:  6.239592552185059 1.7777544260025024
CurrentTrain: epoch  3, batch    35 | loss: 6.2395926Losses:  6.537793159484863 1.807086706161499
CurrentTrain: epoch  3, batch    36 | loss: 6.5377932Losses:  6.316208839416504 1.8058116436004639
CurrentTrain: epoch  3, batch    37 | loss: 6.3162088Losses:  6.623887538909912 1.855014681816101
CurrentTrain: epoch  3, batch    38 | loss: 6.6238875Losses:  5.934019565582275 1.474976658821106
CurrentTrain: epoch  3, batch    39 | loss: 5.9340196Losses:  6.7921061515808105 2.005180835723877
CurrentTrain: epoch  3, batch    40 | loss: 6.7921062Losses:  6.80666446685791 1.7014005184173584
CurrentTrain: epoch  3, batch    41 | loss: 6.8066645Losses:  6.793271064758301 1.7248892784118652
CurrentTrain: epoch  3, batch    42 | loss: 6.7932711Losses:  6.248721599578857 1.689070224761963
CurrentTrain: epoch  3, batch    43 | loss: 6.2487216Losses:  6.410754203796387 1.8309917449951172
CurrentTrain: epoch  3, batch    44 | loss: 6.4107542Losses:  7.138333797454834 2.0887856483459473
CurrentTrain: epoch  3, batch    45 | loss: 7.1383338Losses:  6.222707748413086 1.6678286790847778
CurrentTrain: epoch  3, batch    46 | loss: 6.2227077Losses:  6.5970282554626465 1.8507227897644043
CurrentTrain: epoch  3, batch    47 | loss: 6.5970283Losses:  6.717693328857422 1.990722894668579
CurrentTrain: epoch  3, batch    48 | loss: 6.7176933Losses:  6.061624050140381 1.7570405006408691
CurrentTrain: epoch  3, batch    49 | loss: 6.0616241Losses:  7.156014442443848 1.9910225868225098
CurrentTrain: epoch  3, batch    50 | loss: 7.1560144Losses:  6.168079376220703 1.7193031311035156
CurrentTrain: epoch  3, batch    51 | loss: 6.1680794Losses:  6.5026679039001465 1.9259886741638184
CurrentTrain: epoch  3, batch    52 | loss: 6.5026679Losses:  6.326762676239014 1.8036408424377441
CurrentTrain: epoch  3, batch    53 | loss: 6.3267627Losses:  6.533020973205566 1.9005730152130127
CurrentTrain: epoch  3, batch    54 | loss: 6.5330210Losses:  6.295418739318848 1.682096004486084
CurrentTrain: epoch  3, batch    55 | loss: 6.2954187Losses:  6.395705223083496 1.7411067485809326
CurrentTrain: epoch  3, batch    56 | loss: 6.3957052Losses:  5.973027229309082 1.6618220806121826
CurrentTrain: epoch  3, batch    57 | loss: 5.9730272Losses:  6.484967231750488 1.8456966876983643
CurrentTrain: epoch  3, batch    58 | loss: 6.4849672Losses:  6.48297643661499 1.813942313194275
CurrentTrain: epoch  3, batch    59 | loss: 6.4829764Losses:  6.830297470092773 1.5823613405227661
CurrentTrain: epoch  3, batch    60 | loss: 6.8302975Losses:  6.6551055908203125 1.9504398107528687
CurrentTrain: epoch  3, batch    61 | loss: 6.6551056Losses:  6.643426418304443 1.6094013452529907
CurrentTrain: epoch  3, batch    62 | loss: 6.6434264Losses:  6.615222454071045 2.068909168243408
CurrentTrain: epoch  4, batch     0 | loss: 6.6152225Losses:  6.2020344734191895 1.8114361763000488
CurrentTrain: epoch  4, batch     1 | loss: 6.2020345Losses:  6.853827476501465 1.7682253122329712
CurrentTrain: epoch  4, batch     2 | loss: 6.8538275Losses:  6.786942481994629 1.7985596656799316
CurrentTrain: epoch  4, batch     3 | loss: 6.7869425Losses:  6.324965953826904 1.7260394096374512
CurrentTrain: epoch  4, batch     4 | loss: 6.3249660Losses:  6.379493236541748 1.6854983568191528
CurrentTrain: epoch  4, batch     5 | loss: 6.3794932Losses:  6.352478981018066 1.833685278892517
CurrentTrain: epoch  4, batch     6 | loss: 6.3524790Losses:  6.448347568511963 1.8410687446594238
CurrentTrain: epoch  4, batch     7 | loss: 6.4483476Losses:  6.6669511795043945 1.9431298971176147
CurrentTrain: epoch  4, batch     8 | loss: 6.6669512Losses:  6.672992706298828 2.029303789138794
CurrentTrain: epoch  4, batch     9 | loss: 6.6729927Losses:  6.32523250579834 1.8415873050689697
CurrentTrain: epoch  4, batch    10 | loss: 6.3252325Losses:  5.970597267150879 1.5491206645965576
CurrentTrain: epoch  4, batch    11 | loss: 5.9705973Losses:  6.278164386749268 1.8586187362670898
CurrentTrain: epoch  4, batch    12 | loss: 6.2781644Losses:  6.696871757507324 2.034419059753418
CurrentTrain: epoch  4, batch    13 | loss: 6.6968718Losses:  6.246876239776611 1.7061396837234497
CurrentTrain: epoch  4, batch    14 | loss: 6.2468762Losses:  6.375704765319824 1.8441903591156006
CurrentTrain: epoch  4, batch    15 | loss: 6.3757048Losses:  6.446721076965332 2.0231971740722656
CurrentTrain: epoch  4, batch    16 | loss: 6.4467211Losses:  6.489641189575195 1.9769337177276611
CurrentTrain: epoch  4, batch    17 | loss: 6.4896412Losses:  6.489593982696533 2.056217670440674
CurrentTrain: epoch  4, batch    18 | loss: 6.4895940Losses:  6.267138957977295 1.7415032386779785
CurrentTrain: epoch  4, batch    19 | loss: 6.2671390Losses:  6.26828670501709 1.9285109043121338
CurrentTrain: epoch  4, batch    20 | loss: 6.2682867Losses:  5.914189338684082 1.6629390716552734
CurrentTrain: epoch  4, batch    21 | loss: 5.9141893Losses:  6.689828872680664 1.8308303356170654
CurrentTrain: epoch  4, batch    22 | loss: 6.6898289Losses:  6.589385509490967 2.072618007659912
CurrentTrain: epoch  4, batch    23 | loss: 6.5893855Losses:  6.447165489196777 2.0693695545196533
CurrentTrain: epoch  4, batch    24 | loss: 6.4471655Losses:  6.423354625701904 1.96481454372406
CurrentTrain: epoch  4, batch    25 | loss: 6.4233546Losses:  6.134597301483154 1.7877243757247925
CurrentTrain: epoch  4, batch    26 | loss: 6.1345973Losses:  6.468539237976074 2.0550220012664795
CurrentTrain: epoch  4, batch    27 | loss: 6.4685392Losses:  6.453831672668457 1.9891798496246338
CurrentTrain: epoch  4, batch    28 | loss: 6.4538317Losses:  6.0235772132873535 1.658921718597412
CurrentTrain: epoch  4, batch    29 | loss: 6.0235772Losses:  6.2754292488098145 1.811544418334961
CurrentTrain: epoch  4, batch    30 | loss: 6.2754292Losses:  5.967463493347168 1.5743777751922607
CurrentTrain: epoch  4, batch    31 | loss: 5.9674635Losses:  6.35285758972168 1.8036185503005981
CurrentTrain: epoch  4, batch    32 | loss: 6.3528576Losses:  6.165457725524902 1.6805994510650635
CurrentTrain: epoch  4, batch    33 | loss: 6.1654577Losses:  5.983719348907471 1.5640244483947754
CurrentTrain: epoch  4, batch    34 | loss: 5.9837193Losses:  6.0857343673706055 1.6884267330169678
CurrentTrain: epoch  4, batch    35 | loss: 6.0857344Losses:  6.064001083374023 1.8189115524291992
CurrentTrain: epoch  4, batch    36 | loss: 6.0640011Losses:  6.327208995819092 1.745924472808838
CurrentTrain: epoch  4, batch    37 | loss: 6.3272090Losses:  6.123066425323486 1.8286253213882446
CurrentTrain: epoch  4, batch    38 | loss: 6.1230664Losses:  6.341329574584961 1.714270830154419
CurrentTrain: epoch  4, batch    39 | loss: 6.3413296Losses:  6.2614216804504395 1.9075331687927246
CurrentTrain: epoch  4, batch    40 | loss: 6.2614217Losses:  6.232330322265625 1.8795549869537354
CurrentTrain: epoch  4, batch    41 | loss: 6.2323303Losses:  6.361413478851318 1.9029849767684937
CurrentTrain: epoch  4, batch    42 | loss: 6.3614135Losses:  6.297964096069336 1.895599126815796
CurrentTrain: epoch  4, batch    43 | loss: 6.2979641Losses:  5.989634037017822 1.6651301383972168
CurrentTrain: epoch  4, batch    44 | loss: 5.9896340Losses:  6.244875907897949 1.767153024673462
CurrentTrain: epoch  4, batch    45 | loss: 6.2448759Losses:  6.219260215759277 1.817582368850708
CurrentTrain: epoch  4, batch    46 | loss: 6.2192602Losses:  5.864282608032227 1.6023976802825928
CurrentTrain: epoch  4, batch    47 | loss: 5.8642826Losses:  6.102557182312012 1.752835988998413
CurrentTrain: epoch  4, batch    48 | loss: 6.1025572Losses:  6.108665943145752 1.721377968788147
CurrentTrain: epoch  4, batch    49 | loss: 6.1086659Losses:  6.114605903625488 1.8535133600234985
CurrentTrain: epoch  4, batch    50 | loss: 6.1146059Losses:  6.209855079650879 1.9069240093231201
CurrentTrain: epoch  4, batch    51 | loss: 6.2098551Losses:  6.191436290740967 1.8328232765197754
CurrentTrain: epoch  4, batch    52 | loss: 6.1914363Losses:  6.190127849578857 1.8277592658996582
CurrentTrain: epoch  4, batch    53 | loss: 6.1901278Losses:  6.5175604820251465 1.8272544145584106
CurrentTrain: epoch  4, batch    54 | loss: 6.5175605Losses:  6.293787956237793 1.9439685344696045
CurrentTrain: epoch  4, batch    55 | loss: 6.2937880Losses:  6.235259056091309 1.9242908954620361
CurrentTrain: epoch  4, batch    56 | loss: 6.2352591Losses:  6.095254898071289 1.891624927520752
CurrentTrain: epoch  4, batch    57 | loss: 6.0952549Losses:  6.496401786804199 1.9138935804367065
CurrentTrain: epoch  4, batch    58 | loss: 6.4964018Losses:  6.219524383544922 1.9562842845916748
CurrentTrain: epoch  4, batch    59 | loss: 6.2195244Losses:  6.024761199951172 1.5630629062652588
CurrentTrain: epoch  4, batch    60 | loss: 6.0247612Losses:  6.114239692687988 1.9078853130340576
CurrentTrain: epoch  4, batch    61 | loss: 6.1142397Losses:  5.605606555938721 1.3868483304977417
CurrentTrain: epoch  4, batch    62 | loss: 5.6056066Losses:  6.146353721618652 1.8459328413009644
CurrentTrain: epoch  5, batch     0 | loss: 6.1463537Losses:  6.183119773864746 1.930276870727539
CurrentTrain: epoch  5, batch     1 | loss: 6.1831198Losses:  6.065261363983154 1.798883080482483
CurrentTrain: epoch  5, batch     2 | loss: 6.0652614Losses:  6.252978324890137 1.9823617935180664
CurrentTrain: epoch  5, batch     3 | loss: 6.2529783Losses:  6.158390998840332 1.8004765510559082
CurrentTrain: epoch  5, batch     4 | loss: 6.1583910