#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 1 0 3 3 1 0 2 2]
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.926849365234375 1.7984235286712646
CurrentTrain: epoch  0, batch     0 | loss: 10.9268494x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.036380767822266 2.139348030090332
CurrentTrain: epoch  0, batch     1 | loss: 11.0363808x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.066969871520996 1.9709371328353882
CurrentTrain: epoch  0, batch     2 | loss: 11.0669699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.887831687927246 1.7607920169830322
CurrentTrain: epoch  0, batch     3 | loss: 10.8878317x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.255082130432129 2.0063090324401855
CurrentTrain: epoch  0, batch     4 | loss: 10.2550821x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.279458999633789 1.6876899003982544
CurrentTrain: epoch  0, batch     5 | loss: 11.2794590x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.48698616027832 1.8612170219421387
CurrentTrain: epoch  0, batch     6 | loss: 10.4869862x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.94703483581543 1.973750114440918
CurrentTrain: epoch  0, batch     7 | loss: 9.9470348x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.128488540649414 2.01865816116333
CurrentTrain: epoch  0, batch     8 | loss: 10.1284885x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.697672843933105 1.9570696353912354
CurrentTrain: epoch  0, batch     9 | loss: 10.6976728x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.838467597961426 1.7362463474273682
CurrentTrain: epoch  0, batch    10 | loss: 9.8384676x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.33670425415039 1.7744444608688354
CurrentTrain: epoch  0, batch    11 | loss: 9.3367043x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.044028282165527 1.8078560829162598
CurrentTrain: epoch  0, batch    12 | loss: 10.0440283x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.975881576538086 1.8939635753631592
CurrentTrain: epoch  0, batch    13 | loss: 9.9758816x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.111963272094727 1.8994615077972412
CurrentTrain: epoch  0, batch    14 | loss: 9.1119633x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.866656303405762 2.021256923675537
CurrentTrain: epoch  0, batch    15 | loss: 9.8666563x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.058426856994629 1.6424639225006104
CurrentTrain: epoch  0, batch    16 | loss: 10.0584269x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.066779136657715 2.1173763275146484
CurrentTrain: epoch  0, batch    17 | loss: 10.0667791x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.88123607635498 1.8144304752349854
CurrentTrain: epoch  0, batch    18 | loss: 9.8812361x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.649572372436523 2.014930248260498
CurrentTrain: epoch  0, batch    19 | loss: 9.6495724x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.003922462463379 1.9514461755752563
CurrentTrain: epoch  0, batch    20 | loss: 9.0039225x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.172370910644531 1.9768781661987305
CurrentTrain: epoch  0, batch    21 | loss: 9.1723709x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.688657760620117 1.8792906999588013
CurrentTrain: epoch  0, batch    22 | loss: 9.6886578x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.10012149810791 1.8889232873916626
CurrentTrain: epoch  0, batch    23 | loss: 9.1001215x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.770186424255371 1.9979017972946167
CurrentTrain: epoch  0, batch    24 | loss: 8.7701864x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.946695327758789 1.8629238605499268
CurrentTrain: epoch  0, batch    25 | loss: 8.9466953x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.109234809875488 1.7668402194976807
CurrentTrain: epoch  0, batch    26 | loss: 9.1092348x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.778518676757812 2.047266960144043
CurrentTrain: epoch  0, batch    27 | loss: 8.7785187x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.098732948303223 1.7841252088546753
CurrentTrain: epoch  0, batch    28 | loss: 9.0987329x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.13674545288086 1.9178639650344849
CurrentTrain: epoch  0, batch    29 | loss: 9.1367455x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.567536354064941 2.0098025798797607
CurrentTrain: epoch  0, batch    30 | loss: 9.5675364x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.879860877990723 1.7162644863128662
CurrentTrain: epoch  0, batch    31 | loss: 8.8798609x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.670268058776855 1.3618415594100952
CurrentTrain: epoch  0, batch    32 | loss: 8.6702681x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.318636894226074 1.7121176719665527
CurrentTrain: epoch  0, batch    33 | loss: 8.3186369x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.825912475585938 1.8363399505615234
CurrentTrain: epoch  0, batch    34 | loss: 8.8259125x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.652521133422852 1.8380666971206665
CurrentTrain: epoch  0, batch    35 | loss: 8.6525211x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.657340049743652 1.8205554485321045
CurrentTrain: epoch  0, batch    36 | loss: 8.6573400x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.692634582519531 1.864047646522522
CurrentTrain: epoch  0, batch    37 | loss: 8.6926346x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.539445877075195 2.0489768981933594
CurrentTrain: epoch  0, batch    38 | loss: 9.5394459x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.06056022644043 2.041947603225708
CurrentTrain: epoch  0, batch    39 | loss: 9.0605602x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.387600898742676 1.8561205863952637
CurrentTrain: epoch  0, batch    40 | loss: 9.3876009x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.434120178222656 1.9399871826171875
CurrentTrain: epoch  0, batch    41 | loss: 8.4341202x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.22739315032959 1.8954997062683105
CurrentTrain: epoch  0, batch    42 | loss: 8.2273932x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815733909606934 1.5591213703155518
CurrentTrain: epoch  0, batch    43 | loss: 7.8157339x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.182122230529785 1.6677024364471436
CurrentTrain: epoch  0, batch    44 | loss: 8.1821222x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.323400497436523 1.9396532773971558
CurrentTrain: epoch  0, batch    45 | loss: 8.3234005x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.007547378540039 1.7199137210845947
CurrentTrain: epoch  0, batch    46 | loss: 9.0075474x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.5302653312683105 1.8195229768753052
CurrentTrain: epoch  0, batch    47 | loss: 7.5302653x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.532403945922852 1.8521592617034912
CurrentTrain: epoch  0, batch    48 | loss: 8.5324039x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.20888614654541 1.8072999715805054
CurrentTrain: epoch  0, batch    49 | loss: 8.2088861x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.609411239624023 1.8396973609924316
CurrentTrain: epoch  0, batch    50 | loss: 8.6094112x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.199667930603027 1.9371323585510254
CurrentTrain: epoch  0, batch    51 | loss: 8.1996679x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.848418712615967 1.7412703037261963
CurrentTrain: epoch  0, batch    52 | loss: 7.8484187x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.000316619873047 1.7041115760803223
CurrentTrain: epoch  0, batch    53 | loss: 8.0003166x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.699665069580078 1.9070736169815063
CurrentTrain: epoch  0, batch    54 | loss: 8.6996651x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.839630126953125 1.6547417640686035
CurrentTrain: epoch  0, batch    55 | loss: 7.8396301x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.614195823669434 1.6612800359725952
CurrentTrain: epoch  0, batch    56 | loss: 7.6141958x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.442203521728516 1.8620173931121826
CurrentTrain: epoch  0, batch    57 | loss: 8.4422035x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.174666404724121 1.540844440460205
CurrentTrain: epoch  0, batch    58 | loss: 8.1746664x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.778033256530762 1.4610964059829712
CurrentTrain: epoch  0, batch    59 | loss: 7.7780333x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.397575855255127 1.7511435747146606
CurrentTrain: epoch  0, batch    60 | loss: 7.3975759x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.202156066894531 1.5253829956054688
CurrentTrain: epoch  0, batch    61 | loss: 8.2021561x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.528881072998047 1.2764627933502197
CurrentTrain: epoch  0, batch    62 | loss: 9.5288811x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.230393409729004 1.6139895915985107
CurrentTrain: epoch  1, batch     0 | loss: 8.2303934x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.833333969116211 1.6495968103408813
CurrentTrain: epoch  1, batch     1 | loss: 7.8333340x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.499612808227539 1.5285990238189697
CurrentTrain: epoch  1, batch     2 | loss: 7.4996128x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.852048873901367 1.7880139350891113
CurrentTrain: epoch  1, batch     3 | loss: 7.8520489x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.197295188903809 1.8135771751403809
CurrentTrain: epoch  1, batch     4 | loss: 8.1972952x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.601391315460205 1.651627779006958
CurrentTrain: epoch  1, batch     5 | loss: 7.6013913x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.739384651184082 1.6118459701538086
CurrentTrain: epoch  1, batch     6 | loss: 7.7393847x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.071622848510742 1.8014861345291138
CurrentTrain: epoch  1, batch     7 | loss: 7.0716228x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.06693696975708 1.4502551555633545
CurrentTrain: epoch  1, batch     8 | loss: 7.0669370x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.887425422668457 1.7192975282669067
CurrentTrain: epoch  1, batch     9 | loss: 7.8874254x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.787710189819336 1.708604335784912
CurrentTrain: epoch  1, batch    10 | loss: 7.7877102x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.386960983276367 1.689343810081482
CurrentTrain: epoch  1, batch    11 | loss: 7.3869610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.725907325744629 1.8044259548187256
CurrentTrain: epoch  1, batch    12 | loss: 7.7259073x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.805411338806152 1.3810582160949707
CurrentTrain: epoch  1, batch    13 | loss: 7.8054113x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.030904293060303 1.5862092971801758
CurrentTrain: epoch  1, batch    14 | loss: 7.0309043x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.75450611114502 1.7045416831970215
CurrentTrain: epoch  1, batch    15 | loss: 8.7545061x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.320767402648926 1.4552063941955566
CurrentTrain: epoch  1, batch    16 | loss: 7.3207674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.6597700119018555 1.3610494136810303
CurrentTrain: epoch  1, batch    17 | loss: 6.6597700x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.124475002288818 1.5135514736175537
CurrentTrain: epoch  1, batch    18 | loss: 7.1244750x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.509670734405518 1.6222670078277588
CurrentTrain: epoch  1, batch    19 | loss: 7.5096707x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815581321716309 1.7129367589950562
CurrentTrain: epoch  1, batch    20 | loss: 7.8155813x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.81373405456543 1.6730904579162598
CurrentTrain: epoch  1, batch    21 | loss: 7.8137341x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.510550022125244 1.5836284160614014
CurrentTrain: epoch  1, batch    22 | loss: 7.5105500x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.604043960571289 1.4094659090042114
CurrentTrain: epoch  1, batch    23 | loss: 7.6040440x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.351528644561768 1.5248346328735352
CurrentTrain: epoch  1, batch    24 | loss: 7.3515286x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.34840726852417 1.4831191301345825
CurrentTrain: epoch  1, batch    25 | loss: 6.3484073x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.978714466094971 1.6536893844604492
CurrentTrain: epoch  1, batch    26 | loss: 6.9787145x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.979429721832275 1.6979093551635742
CurrentTrain: epoch  1, batch    27 | loss: 6.9794297x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.359614372253418 1.6020715236663818
CurrentTrain: epoch  1, batch    28 | loss: 7.3596144x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8036370277404785 1.6879304647445679
CurrentTrain: epoch  1, batch    29 | loss: 6.8036370x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.0389251708984375 1.4688650369644165
CurrentTrain: epoch  1, batch    30 | loss: 7.0389252x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.539873123168945 1.6757961511611938
CurrentTrain: epoch  1, batch    31 | loss: 6.5398731x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2656354904174805 1.565091848373413
CurrentTrain: epoch  1, batch    32 | loss: 6.2656355x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.601977825164795 1.3745038509368896
CurrentTrain: epoch  1, batch    33 | loss: 6.6019778x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.520873546600342 1.6367700099945068
CurrentTrain: epoch  1, batch    34 | loss: 7.5208735x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.396836757659912 1.3968578577041626
CurrentTrain: epoch  1, batch    35 | loss: 6.3968368x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.622469902038574 1.3556203842163086
CurrentTrain: epoch  1, batch    36 | loss: 7.6224699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.46106481552124 1.668927788734436
CurrentTrain: epoch  1, batch    37 | loss: 7.4610648x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.874467372894287 1.647559404373169
CurrentTrain: epoch  1, batch    38 | loss: 7.8744674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349011421203613 1.5208356380462646
CurrentTrain: epoch  1, batch    39 | loss: 6.3490114x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.380049228668213 1.4435378313064575
CurrentTrain: epoch  1, batch    40 | loss: 7.3800492x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.665403366088867 1.6588380336761475
CurrentTrain: epoch  1, batch    41 | loss: 7.6654034x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.623701095581055 1.4333010911941528
CurrentTrain: epoch  1, batch    42 | loss: 7.6237011x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.052209854125977 1.5580213069915771
CurrentTrain: epoch  1, batch    43 | loss: 8.0522099x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.080000877380371 1.4077578783035278
CurrentTrain: epoch  1, batch    44 | loss: 8.0800009x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.70178747177124 1.5435359477996826
CurrentTrain: epoch  1, batch    45 | loss: 6.7017875x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.237035751342773 1.4853355884552002
CurrentTrain: epoch  1, batch    46 | loss: 6.2370358x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.285773754119873 1.6266601085662842
CurrentTrain: epoch  1, batch    47 | loss: 7.2857738x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.972940921783447 1.4674501419067383
CurrentTrain: epoch  1, batch    48 | loss: 6.9729409x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.655514717102051 1.4708733558654785
CurrentTrain: epoch  1, batch    49 | loss: 6.6555147x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.886979103088379 1.332857608795166
CurrentTrain: epoch  1, batch    50 | loss: 6.8869791x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.892170429229736 1.4267454147338867
CurrentTrain: epoch  1, batch    51 | loss: 7.8921704x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.9680633544921875 1.4318666458129883
CurrentTrain: epoch  1, batch    52 | loss: 6.9680634x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.060792446136475 1.3945385217666626
CurrentTrain: epoch  1, batch    53 | loss: 7.0607924x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.015600681304932 1.4445074796676636
CurrentTrain: epoch  1, batch    54 | loss: 7.0156007x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.888180255889893 1.2253177165985107
CurrentTrain: epoch  1, batch    55 | loss: 5.8881803x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.040467739105225 1.2677106857299805
CurrentTrain: epoch  1, batch    56 | loss: 6.0404677x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.834133148193359 1.453202247619629
CurrentTrain: epoch  1, batch    57 | loss: 6.8341331x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.212436676025391 1.2806034088134766
CurrentTrain: epoch  1, batch    58 | loss: 6.2124367x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.405393123626709 1.5608692169189453
CurrentTrain: epoch  1, batch    59 | loss: 7.4053931x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.056479454040527 1.482541561126709
CurrentTrain: epoch  1, batch    60 | loss: 7.0564795x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.567456245422363 1.5728840827941895
CurrentTrain: epoch  1, batch    61 | loss: 6.5674562x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2067670822143555 0.931165874004364
CurrentTrain: epoch  1, batch    62 | loss: 5.2067671x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.630093097686768 1.2052849531173706
CurrentTrain: epoch  2, batch     0 | loss: 6.6300931x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.508365631103516 1.1735543012619019
CurrentTrain: epoch  2, batch     1 | loss: 6.5083656x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.681255340576172 1.4644107818603516
CurrentTrain: epoch  2, batch     2 | loss: 6.6812553x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.850689888000488 1.2834796905517578
CurrentTrain: epoch  2, batch     3 | loss: 5.8506899x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.26637077331543 1.259701132774353
CurrentTrain: epoch  2, batch     4 | loss: 6.2663708x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.18607759475708 1.469761610031128
CurrentTrain: epoch  2, batch     5 | loss: 6.1860776x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.061710357666016 1.180121898651123
CurrentTrain: epoch  2, batch     6 | loss: 6.0617104x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.816301345825195 1.377725601196289
CurrentTrain: epoch  2, batch     7 | loss: 6.8163013x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.329916954040527 1.3377785682678223
CurrentTrain: epoch  2, batch     8 | loss: 7.3299170x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.899871349334717 1.2408268451690674
CurrentTrain: epoch  2, batch     9 | loss: 5.8998713x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.774296760559082 1.2740153074264526
CurrentTrain: epoch  2, batch    10 | loss: 5.7742968x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.730959415435791 1.3716809749603271
CurrentTrain: epoch  2, batch    11 | loss: 6.7309594x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.932214736938477 1.2796344757080078
CurrentTrain: epoch  2, batch    12 | loss: 6.9322147x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.55429744720459 1.2464255094528198
CurrentTrain: epoch  2, batch    13 | loss: 6.5542974x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.390684127807617 1.4167113304138184
CurrentTrain: epoch  2, batch    14 | loss: 6.3906841x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.026276111602783 1.3728628158569336
CurrentTrain: epoch  2, batch    15 | loss: 6.0262761x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.680749893188477 1.2630494832992554
CurrentTrain: epoch  2, batch    16 | loss: 5.6807499x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.244289875030518 0.9192905426025391
CurrentTrain: epoch  2, batch    17 | loss: 5.2442899x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.768978118896484 1.442166805267334
CurrentTrain: epoch  2, batch    18 | loss: 6.7689781x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.147119045257568 1.249469518661499
CurrentTrain: epoch  2, batch    19 | loss: 6.1471190x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.05328893661499 1.2401154041290283
CurrentTrain: epoch  2, batch    20 | loss: 6.0532889x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.860775947570801 1.2457081079483032
CurrentTrain: epoch  2, batch    21 | loss: 5.8607759x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.675922393798828 1.2711093425750732
CurrentTrain: epoch  2, batch    22 | loss: 5.6759224x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.320955753326416 1.3323872089385986
CurrentTrain: epoch  2, batch    23 | loss: 6.3209558x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.205848693847656 1.1694908142089844
CurrentTrain: epoch  2, batch    24 | loss: 6.2058487x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.807610511779785 1.2032126188278198
CurrentTrain: epoch  2, batch    25 | loss: 5.8076105x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.581456184387207 1.1734309196472168
CurrentTrain: epoch  2, batch    26 | loss: 5.5814562x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.789383888244629 1.007904052734375
CurrentTrain: epoch  2, batch    27 | loss: 5.7893839x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.644453525543213 1.0599491596221924
CurrentTrain: epoch  2, batch    28 | loss: 5.6444535x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.141595840454102 1.3762619495391846
CurrentTrain: epoch  2, batch    29 | loss: 6.1415958x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349484920501709 1.2934733629226685
CurrentTrain: epoch  2, batch    30 | loss: 6.3494849x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.732242584228516 1.3867745399475098
CurrentTrain: epoch  2, batch    31 | loss: 5.7322426x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8030781745910645 1.1243202686309814
CurrentTrain: epoch  2, batch    32 | loss: 6.8030782x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.371408462524414 1.06284499168396
CurrentTrain: epoch  2, batch    33 | loss: 5.3714085x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.11379337310791 1.235999584197998
CurrentTrain: epoch  2, batch    34 | loss: 6.1137934x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.1160430908203125 1.1560214757919312
CurrentTrain: epoch  2, batch    35 | loss: 6.1160431x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.563666820526123 1.1961088180541992
CurrentTrain: epoch  2, batch    36 | loss: 5.5636668x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.901392936706543 1.0547535419464111
CurrentTrain: epoch  2, batch    37 | loss: 5.9013929x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.816005706787109 1.1190227270126343
CurrentTrain: epoch  2, batch    38 | loss: 5.8160057x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.200374126434326 0.9636704921722412
CurrentTrain: epoch  2, batch    39 | loss: 5.2003741x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.949509143829346 0.9317396283149719
CurrentTrain: epoch  2, batch    40 | loss: 5.9495091x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.409481048583984 0.8464484810829163
CurrentTrain: epoch  2, batch    41 | loss: 6.4094810x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.743829250335693 1.0953044891357422
CurrentTrain: epoch  2, batch    42 | loss: 5.7438293x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.965728759765625 1.1844062805175781
CurrentTrain: epoch  2, batch    43 | loss: 5.9657288x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.747776508331299 1.249800443649292
CurrentTrain: epoch  2, batch    44 | loss: 5.7477765x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.041460990905762 1.1863003969192505
CurrentTrain: epoch  2, batch    45 | loss: 6.0414610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.448573112487793 1.1486191749572754
CurrentTrain: epoch  2, batch    46 | loss: 5.4485731x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2633867263793945 1.1743338108062744
CurrentTrain: epoch  2, batch    47 | loss: 6.2633867x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.380342483520508 1.0493706464767456
CurrentTrain: epoch  2, batch    48 | loss: 5.3803425x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.993921756744385 1.2152526378631592
CurrentTrain: epoch  2, batch    49 | loss: 5.9939218x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.56021785736084 0.9582090377807617
CurrentTrain: epoch  2, batch    50 | loss: 5.5602179x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.392078399658203 1.1478691101074219
CurrentTrain: epoch  2, batch    51 | loss: 5.3920784x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.622655391693115 1.0182898044586182
CurrentTrain: epoch  2, batch    52 | loss: 5.6226554x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626033306121826 1.2189699411392212
CurrentTrain: epoch  2, batch    53 | loss: 5.6260333x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.478575229644775 1.1590144634246826
CurrentTrain: epoch  2, batch    54 | loss: 5.4785752x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.522172451019287 1.1833986043930054
CurrentTrain: epoch  2, batch    55 | loss: 5.5221725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.971188545227051 1.2475519180297852
CurrentTrain: epoch  2, batch    56 | loss: 5.9711885x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.804933547973633 1.1557669639587402
CurrentTrain: epoch  2, batch    57 | loss: 5.8049335x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5172319412231445 1.027930498123169
CurrentTrain: epoch  2, batch    58 | loss: 5.5172319x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.411221981048584 1.138274908065796
CurrentTrain: epoch  2, batch    59 | loss: 5.4112220x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.255485534667969 1.0583497285842896
CurrentTrain: epoch  2, batch    60 | loss: 5.2554855x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.477558135986328 1.049267292022705
CurrentTrain: epoch  2, batch    61 | loss: 5.4775581x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.209966659545898 0.7732165455818176
CurrentTrain: epoch  2, batch    62 | loss: 5.2099667x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.035955905914307 1.081830620765686
CurrentTrain: epoch  3, batch     0 | loss: 6.0359559x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.456923007965088 1.0081608295440674
CurrentTrain: epoch  3, batch     1 | loss: 6.4569230x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.480226039886475 1.078663945198059
CurrentTrain: epoch  3, batch     2 | loss: 5.4802260x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.740518093109131 0.898904025554657
CurrentTrain: epoch  3, batch     3 | loss: 5.7405181x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626705169677734 0.9309966564178467
CurrentTrain: epoch  3, batch     4 | loss: 5.6267052x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.738065242767334 0.9585984945297241
CurrentTrain: epoch  3, batch     5 | loss: 5.7380652x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.444523811340332 1.0503966808319092
CurrentTrain: epoch  3, batch     6 | loss: 5.4445238x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.4213995933532715 1.037995457649231
CurrentTrain: epoch  3, batch     7 | loss: 5.4213996x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7687273025512695 0.9488410353660583
CurrentTrain: epoch  3, batch     8 | loss: 5.7687273x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7205610275268555 1.0622001886367798
CurrentTrain: epoch  3, batch     9 | loss: 5.7205610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.6544694900512695 1.1499388217926025
CurrentTrain: epoch  3, batch    10 | loss: 5.6544695x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.486059188842773 0.986730694770813
CurrentTrain: epoch  3, batch    11 | loss: 5.4860592x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.953603744506836 0.8693920373916626
CurrentTrain: epoch  3, batch    12 | loss: 4.9536037x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.584843158721924 1.0983784198760986
CurrentTrain: epoch  3, batch    13 | loss: 5.5848432x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.262392997741699 1.0211337804794312
CurrentTrain: epoch  3, batch    14 | loss: 5.2623930x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140032768249512 0.9961339235305786
CurrentTrain: epoch  3, batch    15 | loss: 5.1400328x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.498325824737549 0.9570016860961914
CurrentTrain: epoch  3, batch    16 | loss: 5.4983258x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.739621162414551 0.9503822326660156
CurrentTrain: epoch  3, batch    17 | loss: 5.7396212x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.441994667053223 0.8477289080619812
CurrentTrain: epoch  3, batch    18 | loss: 5.4419947x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.407026767730713 0.9850529432296753
CurrentTrain: epoch  3, batch    19 | loss: 5.4070268x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.29840612411499 1.0056352615356445
CurrentTrain: epoch  3, batch    20 | loss: 5.2984061x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2915849685668945 1.0394574403762817
CurrentTrain: epoch  3, batch    21 | loss: 5.2915850x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.343301296234131 0.9191257953643799
CurrentTrain: epoch  3, batch    22 | loss: 5.3433013x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.226625919342041 0.9959335923194885
CurrentTrain: epoch  3, batch    23 | loss: 5.2266259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.03607702255249 0.6870988607406616
CurrentTrain: epoch  3, batch    24 | loss: 5.0360770x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.109294891357422 1.0195376873016357
CurrentTrain: epoch  3, batch    25 | loss: 5.1092949x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.272613048553467 1.0991891622543335
CurrentTrain: epoch  3, batch    26 | loss: 5.2726130x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.175928115844727 0.9497132301330566
CurrentTrain: epoch  3, batch    27 | loss: 5.1759281x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.34478235244751 1.0801821947097778
CurrentTrain: epoch  3, batch    28 | loss: 5.3447824x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.681675434112549 0.9446146488189697
CurrentTrain: epoch  3, batch    29 | loss: 5.6816754x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.160543441772461 1.1458017826080322
CurrentTrain: epoch  3, batch    30 | loss: 6.1605434x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.885409832000732 0.8732045292854309
CurrentTrain: epoch  3, batch    31 | loss: 4.8854098x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.374021530151367 0.8337904214859009
CurrentTrain: epoch  3, batch    32 | loss: 5.3740215x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.220224857330322 0.829306960105896
CurrentTrain: epoch  3, batch    33 | loss: 5.2202249x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0848708152771 0.9158856868743896
CurrentTrain: epoch  3, batch    34 | loss: 5.0848708x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0665106773376465 0.9240090847015381
CurrentTrain: epoch  3, batch    35 | loss: 5.0665107x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.8889312744140625 1.0663390159606934
CurrentTrain: epoch  3, batch    36 | loss: 5.8889313x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.190898418426514 0.8566985726356506
CurrentTrain: epoch  3, batch    37 | loss: 5.1908984x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959601879119873 0.9252779483795166
CurrentTrain: epoch  3, batch    38 | loss: 4.9596019x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.827635765075684 0.8134810924530029
CurrentTrain: epoch  3, batch    39 | loss: 5.8276358x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140291213989258 0.9769730567932129
CurrentTrain: epoch  3, batch    40 | loss: 5.1402912x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.766822338104248 0.6842474341392517
CurrentTrain: epoch  3, batch    41 | loss: 4.7668223x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.570883750915527 0.8117127418518066
CurrentTrain: epoch  3, batch    42 | loss: 5.5708838x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.099788188934326 0.729293942451477
CurrentTrain: epoch  3, batch    43 | loss: 5.0997882x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.410670280456543 0.9232392311096191
CurrentTrain: epoch  3, batch    44 | loss: 5.4106703x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.390532493591309 0.8209837675094604
CurrentTrain: epoch  3, batch    45 | loss: 5.3905325x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.863620758056641 0.8677620887756348
CurrentTrain: epoch  3, batch    46 | loss: 4.8636208x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.13870906829834 0.7726187705993652
CurrentTrain: epoch  3, batch    47 | loss: 5.1387091x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8672943115234375 0.6362746953964233
CurrentTrain: epoch  3, batch    48 | loss: 4.8672943x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0335259437561035 0.6836175918579102
CurrentTrain: epoch  3, batch    49 | loss: 5.0335259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.008811950683594 0.8716031908988953
CurrentTrain: epoch  3, batch    50 | loss: 5.0088120x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.192744731903076 0.7628194093704224
CurrentTrain: epoch  3, batch    51 | loss: 5.1927447x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.968265533447266 0.8424209952354431
CurrentTrain: epoch  3, batch    52 | loss: 4.9682655x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.672567844390869 0.9232137203216553
CurrentTrain: epoch  3, batch    53 | loss: 5.6725678x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.920748710632324 0.8501842021942139
CurrentTrain: epoch  3, batch    54 | loss: 4.9207487x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.090141296386719 0.957904577255249
CurrentTrain: epoch  3, batch    55 | loss: 5.0901413x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.886087417602539 0.816415548324585
CurrentTrain: epoch  3, batch    56 | loss: 4.8860874x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.327167510986328 0.8300053477287292
CurrentTrain: epoch  3, batch    57 | loss: 5.3271675x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2497477531433105 0.8219953179359436
CurrentTrain: epoch  3, batch    58 | loss: 5.2497478x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.9105000495910645 0.6527562737464905
CurrentTrain: epoch  3, batch    59 | loss: 4.9105000x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.984633445739746 0.9194714426994324
CurrentTrain: epoch  3, batch    60 | loss: 4.9846334x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.813289642333984 0.7433576583862305
CurrentTrain: epoch  3, batch    61 | loss: 4.8132896x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691621780395508 0.5168606042861938
CurrentTrain: epoch  3, batch    62 | loss: 4.6916218x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.693078517913818 0.5873658061027527
CurrentTrain: epoch  4, batch     0 | loss: 4.6930785x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.806028366088867 0.745362401008606
CurrentTrain: epoch  4, batch     1 | loss: 4.8060284x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.904834747314453 0.7988278269767761
CurrentTrain: epoch  4, batch     2 | loss: 4.9048347x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.719254970550537 0.658371090888977
CurrentTrain: epoch  4, batch     3 | loss: 4.7192550x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959964752197266 0.9529321193695068
CurrentTrain: epoch  4, batch     4 | loss: 4.9599648x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.135072708129883 0.9262544512748718
CurrentTrain: epoch  4, batch     5 | loss: 5.1350727x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.84869909286499 0.7296959161758423
CurrentTrain: epoch  4, batch     6 | loss: 4.8486991x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.733144760131836 0.6090331077575684
CurrentTrain: epoch  4, batch     7 | loss: 4.7331448x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.841223239898682 0.8645848035812378
CurrentTrain: epoch  4, batch     8 | loss: 4.8412232x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7772746086120605 0.7030757069587708
CurrentTrain: epoch  4, batch     9 | loss: 4.7772746x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.814172744750977 0.7269054651260376
CurrentTrain: epoch  4, batch    10 | loss: 4.8141727x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.166572570800781 0.7654482126235962
CurrentTrain: epoch  4, batch    11 | loss: 5.1665726x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.869943618774414 0.7695276141166687
CurrentTrain: epoch  4, batch    12 | loss: 4.8699436x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.181846618652344 0.8651489615440369
CurrentTrain: epoch  4, batch    13 | loss: 5.1818466x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.493844985961914 0.5341933965682983
CurrentTrain: epoch  4, batch    14 | loss: 4.4938450x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.749185562133789 0.6812607049942017
CurrentTrain: epoch  4, batch    15 | loss: 4.7491856x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.824125289916992 0.7251666784286499
CurrentTrain: epoch  4, batch    16 | loss: 4.8241253x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.799384593963623 0.6490915417671204
CurrentTrain: epoch  4, batch    17 | loss: 4.7993846x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.801788330078125 0.8030322790145874
CurrentTrain: epoch  4, batch    18 | loss: 4.8017883x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.796889781951904 0.7754301428794861
CurrentTrain: epoch  4, batch    19 | loss: 4.7968898x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.153695106506348 0.6834776997566223
CurrentTrain: epoch  4, batch    20 | loss: 5.1536951x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.915266990661621 0.5711497664451599
CurrentTrain: epoch  4, batch    21 | loss: 4.9152670x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.570249557495117 0.6395601630210876
CurrentTrain: epoch  4, batch    22 | loss: 4.5702496x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.478219032287598 0.44560980796813965
CurrentTrain: epoch  4, batch    23 | loss: 4.4782190x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.817839622497559 0.5910068154335022
CurrentTrain: epoch  4, batch    24 | loss: 4.8178396x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.768563270568848 0.7635782957077026
CurrentTrain: epoch  4, batch    25 | loss: 4.7685633x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.609344959259033 0.6901866793632507
CurrentTrain: epoch  4, batch    26 | loss: 4.6093450x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.637467384338379 0.6230311989784241
CurrentTrain: epoch  4, batch    27 | loss: 4.6374674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.767701625823975 0.6288499236106873
CurrentTrain: epoch  4, batch    28 | loss: 4.7677016x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.700007915496826 0.6359813809394836
CurrentTrain: epoch  4, batch    29 | loss: 4.7000079x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.588162422180176 0.6554694175720215
CurrentTrain: epoch  4, batch    30 | loss: 4.5881624x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.732369899749756 0.6765909194946289
CurrentTrain: epoch  4, batch    31 | loss: 4.7323699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.666533470153809 0.6684315204620361
CurrentTrain: epoch  4, batch    32 | loss: 4.6665335x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7807159423828125 0.7347530126571655
CurrentTrain: epoch  4, batch    33 | loss: 4.7807159x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6732892990112305 0.6531981229782104
CurrentTrain: epoch  4, batch    34 | loss: 4.6732893x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.888833045959473 0.7925335764884949
CurrentTrain: epoch  4, batch    35 | loss: 4.8888330x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.699143409729004 0.6564288139343262
CurrentTrain: epoch  4, batch    36 | loss: 4.6991434x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.110682964324951 0.7736770510673523
CurrentTrain: epoch  4, batch    37 | loss: 5.1106830x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704012393951416 0.6345605254173279
CurrentTrain: epoch  4, batch    38 | loss: 4.7040124x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.62897253036499 0.6374919414520264
CurrentTrain: epoch  4, batch    39 | loss: 4.6289725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.552116870880127 0.6075010299682617
CurrentTrain: epoch  4, batch    40 | loss: 4.5521169x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.595989227294922 0.6946349143981934
CurrentTrain: epoch  4, batch    41 | loss: 4.5959892x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.858814239501953 0.7200260162353516
CurrentTrain: epoch  4, batch    42 | loss: 4.8588142x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.929131984710693 0.5336486101150513
CurrentTrain: epoch  4, batch    43 | loss: 4.9291320x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.455041885375977 0.5244512557983398
CurrentTrain: epoch  4, batch    44 | loss: 4.4550419x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.24995231628418 0.6656460762023926
CurrentTrain: epoch  4, batch    45 | loss: 5.2499523x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.745295524597168 0.7060742378234863
CurrentTrain: epoch  4, batch    46 | loss: 4.7452955x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.660812854766846 0.6329832673072815
CurrentTrain: epoch  4, batch    47 | loss: 4.6608129x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.638529300689697 0.6848247051239014
CurrentTrain: epoch  4, batch    48 | loss: 4.6385293x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.644412040710449 0.6397827863693237
CurrentTrain: epoch  4, batch    49 | loss: 4.6444120x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.774701118469238 0.8059102892875671
CurrentTrain: epoch  4, batch    50 | loss: 4.7747011x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.615450859069824 0.5835493206977844
CurrentTrain: epoch  4, batch    51 | loss: 4.6154509x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.524040222167969 0.5134026408195496
CurrentTrain: epoch  4, batch    52 | loss: 4.5240402x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8538665771484375 0.6754355430603027
CurrentTrain: epoch  4, batch    53 | loss: 4.8538666x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.653186321258545 0.5864333510398865
CurrentTrain: epoch  4, batch    54 | loss: 4.6531863x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.649025917053223 0.6997280120849609
CurrentTrain: epoch  4, batch    55 | loss: 4.6490259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.565957546234131 0.5201479196548462
CurrentTrain: epoch  4, batch    56 | loss: 4.5659575x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691095352172852 0.7682797908782959
CurrentTrain: epoch  4, batch    57 | loss: 4.6910954x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590443134307861 0.582393229007721
CurrentTrain: epoch  4, batch    58 | loss: 4.5904431x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.584754467010498 0.6168481111526489
CurrentTrain: epoch  4, batch    59 | loss: 4.5847545x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.592937469482422 0.6070008277893066
CurrentTrain: epoch  4, batch    60 | loss: 4.5929375x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704367637634277 0.6315898895263672
CurrentTrain: epoch  4, batch    61 | loss: 4.7043676x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.513129234313965 0.45638883113861084
CurrentTrain: epoch  4, batch    62 | loss: 4.5131292x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.75014066696167 0.7058000564575195
CurrentTrain: epoch  5, batch     0 | loss: 4.7501407x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.630569934844971 0.6423497796058655
CurrentTrain: epoch  5, batch     1 | loss: 4.6305699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.550356388092041 0.5766547322273254
CurrentTrain: epoch  5, batch     2 | loss: 4.5503564x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.533287048339844 0.5767020583152771
CurrentTrain: epoch  5, batch     3 | loss: 4.5332870x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.652379035949707 0.6052058339118958
CurrentTrain: epoch  5, batch     4 | loss: 4.6523790x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.605034351348877 0.5880939960479736
CurrentTrain: epoch  5, batch     5 | loss: 4.6050344x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.602302074432373 0.6390889883041382
CurrentTrain: epoch  5, batch     6 | loss: 4.6023021x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.482685089111328 0.6640385389328003
CurrentTrain: epoch  5, batch     7 | loss: 4.4826851x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.64866304397583 0.6475061774253845
CurrentTrain: epoch  5, batch     8 | loss: 4.6486630x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.44582986831665 0.4502357840538025
CurrentTrain: epoch  5, batch     9 | loss: 4.4458299x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.843441009521484 0.6541036367416382
CurrentTrain: epoch  5, batch    10 | loss: 4.8434410x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590672492980957 0.6501652002334595
CurrentTrain: epoch  5, batch    11 | loss: 4.5906725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.445680141448975 0.5148342847824097
CurrentTrain: epoch  5, batch    12 | loss: 4.4456801x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.491142749786377 0.5643436908721924
CurrentTrain: epoch  5, batch    13 | loss: 4.4911427x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.483101844787598 0.6623929142951965
CurrentTrain: epoch  5, batch    14 | loss: 4.4831018x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.50734806060791 0.5970326066017151
CurrentTrain: epoch  5, batch    15 | loss: 4.5073481x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.620008945465088 0.6581331491470337
CurrentTrain: epoch  5, batch    16 | loss: 4.6200089x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485630035400391 0.6099047660827637
CurrentTrain: epoch  5, batch    17 | loss: 4.4856300x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.618782043457031 0.6864808797836304
CurrentTrain: epoch  5, batch    18 | loss: 4.6187820x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.487261772155762 0.547090470790863
CurrentTrain: epoch  5, batch    19 | loss: 4.4872618x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416949272155762 0.5254210829734802
CurrentTrain: epoch  5, batch    20 | loss: 4.4169493x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.467197418212891 0.5430623888969421
CurrentTrain: epoch  5, batch    21 | loss: 4.4671974x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485051155090332 0.4818846583366394
CurrentTrain: epoch  5, batch    22 | loss: 4.4850512x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.551313877105713 0.5901062488555908
CurrentTrain: epoch  5, batch    23 | loss: 4.5513139x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.453506946563721 0.498520165681839
CurrentTrain: epoch  5, batch    24 | loss: 4.4535069x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463562488555908 0.5646237134933472
CurrentTrain: epoch  5, batch    25 | loss: 4.4635625x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5455098152160645 0.5900653600692749
CurrentTrain: epoch  5, batch    26 | loss: 4.5455098x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4906439781188965 0.5196026563644409
CurrentTrain: epoch  5, batch    27 | loss: 4.4906440x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463355541229248 0.6276993155479431
CurrentTrain: epoch  5, batch    28 | loss: 4.4633555x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466972351074219 0.6047004461288452
CurrentTrain: epoch  5, batch    29 | loss: 4.4669724x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400462627410889 0.4878422021865845
CurrentTrain: epoch  5, batch    30 | loss: 4.4004626x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.408786296844482 0.5057677030563354
CurrentTrain: epoch  5, batch    31 | loss: 4.4087863x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466117858886719 0.48728930950164795
CurrentTrain: epoch  5, batch    32 | loss: 4.4661179x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.321932792663574 0.44563430547714233
CurrentTrain: epoch  5, batch    33 | loss: 4.3219328x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.451994895935059 0.4971400499343872
CurrentTrain: epoch  5, batch    34 | loss: 4.4519949#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 1 0 3 3 1 0 2 2]
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.926849365234375 1.7984235286712646
CurrentTrain: epoch  0, batch     0 | loss: 10.9268494x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.036380767822266 2.139348030090332
CurrentTrain: epoch  0, batch     1 | loss: 11.0363808x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.066969871520996 1.9709371328353882
CurrentTrain: epoch  0, batch     2 | loss: 11.0669699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.887831687927246 1.7607920169830322
CurrentTrain: epoch  0, batch     3 | loss: 10.8878317x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.255082130432129 2.0063090324401855
CurrentTrain: epoch  0, batch     4 | loss: 10.2550821x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.279458999633789 1.6876899003982544
CurrentTrain: epoch  0, batch     5 | loss: 11.2794590x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.48698616027832 1.8612170219421387
CurrentTrain: epoch  0, batch     6 | loss: 10.4869862x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.94703483581543 1.973750114440918
CurrentTrain: epoch  0, batch     7 | loss: 9.9470348x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.128488540649414 2.01865816116333
CurrentTrain: epoch  0, batch     8 | loss: 10.1284885x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.697672843933105 1.9570696353912354
CurrentTrain: epoch  0, batch     9 | loss: 10.6976728x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.838467597961426 1.7362463474273682
CurrentTrain: epoch  0, batch    10 | loss: 9.8384676x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.33670425415039 1.7744444608688354
CurrentTrain: epoch  0, batch    11 | loss: 9.3367043x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.044028282165527 1.8078560829162598
CurrentTrain: epoch  0, batch    12 | loss: 10.0440283x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.975881576538086 1.8939635753631592
CurrentTrain: epoch  0, batch    13 | loss: 9.9758816x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.111963272094727 1.8994615077972412
CurrentTrain: epoch  0, batch    14 | loss: 9.1119633x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.866656303405762 2.021256923675537
CurrentTrain: epoch  0, batch    15 | loss: 9.8666563x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.058426856994629 1.6424639225006104
CurrentTrain: epoch  0, batch    16 | loss: 10.0584269x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.066779136657715 2.1173763275146484
CurrentTrain: epoch  0, batch    17 | loss: 10.0667791x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.88123607635498 1.8144304752349854
CurrentTrain: epoch  0, batch    18 | loss: 9.8812361x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.649572372436523 2.014930248260498
CurrentTrain: epoch  0, batch    19 | loss: 9.6495724x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.003922462463379 1.9514461755752563
CurrentTrain: epoch  0, batch    20 | loss: 9.0039225x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.172370910644531 1.9768781661987305
CurrentTrain: epoch  0, batch    21 | loss: 9.1723709x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.688657760620117 1.8792906999588013
CurrentTrain: epoch  0, batch    22 | loss: 9.6886578x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.10012149810791 1.8889232873916626
CurrentTrain: epoch  0, batch    23 | loss: 9.1001215x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.770186424255371 1.9979017972946167
CurrentTrain: epoch  0, batch    24 | loss: 8.7701864x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.946695327758789 1.8629238605499268
CurrentTrain: epoch  0, batch    25 | loss: 8.9466953x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.109234809875488 1.7668402194976807
CurrentTrain: epoch  0, batch    26 | loss: 9.1092348x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.778518676757812 2.047266960144043
CurrentTrain: epoch  0, batch    27 | loss: 8.7785187x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.098732948303223 1.7841252088546753
CurrentTrain: epoch  0, batch    28 | loss: 9.0987329x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.13674545288086 1.9178639650344849
CurrentTrain: epoch  0, batch    29 | loss: 9.1367455x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.567536354064941 2.0098025798797607
CurrentTrain: epoch  0, batch    30 | loss: 9.5675364x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.879860877990723 1.7162644863128662
CurrentTrain: epoch  0, batch    31 | loss: 8.8798609x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.670268058776855 1.3618415594100952
CurrentTrain: epoch  0, batch    32 | loss: 8.6702681x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.318636894226074 1.7121176719665527
CurrentTrain: epoch  0, batch    33 | loss: 8.3186369x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.825912475585938 1.8363399505615234
CurrentTrain: epoch  0, batch    34 | loss: 8.8259125x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.652521133422852 1.8380666971206665
CurrentTrain: epoch  0, batch    35 | loss: 8.6525211x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.657340049743652 1.8205554485321045
CurrentTrain: epoch  0, batch    36 | loss: 8.6573400x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.692634582519531 1.864047646522522
CurrentTrain: epoch  0, batch    37 | loss: 8.6926346x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.539445877075195 2.0489768981933594
CurrentTrain: epoch  0, batch    38 | loss: 9.5394459x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.06056022644043 2.041947603225708
CurrentTrain: epoch  0, batch    39 | loss: 9.0605602x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.387600898742676 1.8561205863952637
CurrentTrain: epoch  0, batch    40 | loss: 9.3876009x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.434120178222656 1.9399871826171875
CurrentTrain: epoch  0, batch    41 | loss: 8.4341202x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.22739315032959 1.8954997062683105
CurrentTrain: epoch  0, batch    42 | loss: 8.2273932x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815733909606934 1.5591213703155518
CurrentTrain: epoch  0, batch    43 | loss: 7.8157339x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.182122230529785 1.6677024364471436
CurrentTrain: epoch  0, batch    44 | loss: 8.1821222x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.323400497436523 1.9396532773971558
CurrentTrain: epoch  0, batch    45 | loss: 8.3234005x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.007547378540039 1.7199137210845947
CurrentTrain: epoch  0, batch    46 | loss: 9.0075474x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.5302653312683105 1.8195229768753052
CurrentTrain: epoch  0, batch    47 | loss: 7.5302653x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.532403945922852 1.8521592617034912
CurrentTrain: epoch  0, batch    48 | loss: 8.5324039x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.20888614654541 1.8072999715805054
CurrentTrain: epoch  0, batch    49 | loss: 8.2088861x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.609411239624023 1.8396973609924316
CurrentTrain: epoch  0, batch    50 | loss: 8.6094112x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.199667930603027 1.9371323585510254
CurrentTrain: epoch  0, batch    51 | loss: 8.1996679x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.848418712615967 1.7412703037261963
CurrentTrain: epoch  0, batch    52 | loss: 7.8484187x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.000316619873047 1.7041115760803223
CurrentTrain: epoch  0, batch    53 | loss: 8.0003166x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.699665069580078 1.9070736169815063
CurrentTrain: epoch  0, batch    54 | loss: 8.6996651x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.839630126953125 1.6547417640686035
CurrentTrain: epoch  0, batch    55 | loss: 7.8396301x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.614195823669434 1.6612800359725952
CurrentTrain: epoch  0, batch    56 | loss: 7.6141958x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.442203521728516 1.8620173931121826
CurrentTrain: epoch  0, batch    57 | loss: 8.4422035x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.174666404724121 1.540844440460205
CurrentTrain: epoch  0, batch    58 | loss: 8.1746664x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.778033256530762 1.4610964059829712
CurrentTrain: epoch  0, batch    59 | loss: 7.7780333x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.397575855255127 1.7511435747146606
CurrentTrain: epoch  0, batch    60 | loss: 7.3975759x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.202156066894531 1.5253829956054688
CurrentTrain: epoch  0, batch    61 | loss: 8.2021561x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.528881072998047 1.2764627933502197
CurrentTrain: epoch  0, batch    62 | loss: 9.5288811x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.230393409729004 1.6139895915985107
CurrentTrain: epoch  1, batch     0 | loss: 8.2303934x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.833333969116211 1.6495968103408813
CurrentTrain: epoch  1, batch     1 | loss: 7.8333340x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.499612808227539 1.5285990238189697
CurrentTrain: epoch  1, batch     2 | loss: 7.4996128x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.852048873901367 1.7880139350891113
CurrentTrain: epoch  1, batch     3 | loss: 7.8520489x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.197295188903809 1.8135771751403809
CurrentTrain: epoch  1, batch     4 | loss: 8.1972952x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.601391315460205 1.651627779006958
CurrentTrain: epoch  1, batch     5 | loss: 7.6013913x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.739384651184082 1.6118459701538086
CurrentTrain: epoch  1, batch     6 | loss: 7.7393847x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.071622848510742 1.8014861345291138
CurrentTrain: epoch  1, batch     7 | loss: 7.0716228x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.06693696975708 1.4502551555633545
CurrentTrain: epoch  1, batch     8 | loss: 7.0669370x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.887425422668457 1.7192975282669067
CurrentTrain: epoch  1, batch     9 | loss: 7.8874254x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.787710189819336 1.708604335784912
CurrentTrain: epoch  1, batch    10 | loss: 7.7877102x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.386960983276367 1.689343810081482
CurrentTrain: epoch  1, batch    11 | loss: 7.3869610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.725907325744629 1.8044259548187256
CurrentTrain: epoch  1, batch    12 | loss: 7.7259073x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.805411338806152 1.3810582160949707
CurrentTrain: epoch  1, batch    13 | loss: 7.8054113x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.030904293060303 1.5862092971801758
CurrentTrain: epoch  1, batch    14 | loss: 7.0309043x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.75450611114502 1.7045416831970215
CurrentTrain: epoch  1, batch    15 | loss: 8.7545061x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.320767402648926 1.4552063941955566
CurrentTrain: epoch  1, batch    16 | loss: 7.3207674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.6597700119018555 1.3610494136810303
CurrentTrain: epoch  1, batch    17 | loss: 6.6597700x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.124475002288818 1.5135514736175537
CurrentTrain: epoch  1, batch    18 | loss: 7.1244750x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.509670734405518 1.6222670078277588
CurrentTrain: epoch  1, batch    19 | loss: 7.5096707x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815581321716309 1.7129367589950562
CurrentTrain: epoch  1, batch    20 | loss: 7.8155813x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.81373405456543 1.6730904579162598
CurrentTrain: epoch  1, batch    21 | loss: 7.8137341x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.510550022125244 1.5836284160614014
CurrentTrain: epoch  1, batch    22 | loss: 7.5105500x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.604043960571289 1.4094659090042114
CurrentTrain: epoch  1, batch    23 | loss: 7.6040440x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.351528644561768 1.5248346328735352
CurrentTrain: epoch  1, batch    24 | loss: 7.3515286x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.34840726852417 1.4831191301345825
CurrentTrain: epoch  1, batch    25 | loss: 6.3484073x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.978714466094971 1.6536893844604492
CurrentTrain: epoch  1, batch    26 | loss: 6.9787145x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.979429721832275 1.6979093551635742
CurrentTrain: epoch  1, batch    27 | loss: 6.9794297x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.359614372253418 1.6020715236663818
CurrentTrain: epoch  1, batch    28 | loss: 7.3596144x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8036370277404785 1.6879304647445679
CurrentTrain: epoch  1, batch    29 | loss: 6.8036370x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.0389251708984375 1.4688650369644165
CurrentTrain: epoch  1, batch    30 | loss: 7.0389252x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.539873123168945 1.6757961511611938
CurrentTrain: epoch  1, batch    31 | loss: 6.5398731x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2656354904174805 1.565091848373413
CurrentTrain: epoch  1, batch    32 | loss: 6.2656355x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.601977825164795 1.3745038509368896
CurrentTrain: epoch  1, batch    33 | loss: 6.6019778x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.520873546600342 1.6367700099945068
CurrentTrain: epoch  1, batch    34 | loss: 7.5208735x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.396836757659912 1.3968578577041626
CurrentTrain: epoch  1, batch    35 | loss: 6.3968368x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.622469902038574 1.3556203842163086
CurrentTrain: epoch  1, batch    36 | loss: 7.6224699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.46106481552124 1.668927788734436
CurrentTrain: epoch  1, batch    37 | loss: 7.4610648x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.874467372894287 1.647559404373169
CurrentTrain: epoch  1, batch    38 | loss: 7.8744674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349011421203613 1.5208356380462646
CurrentTrain: epoch  1, batch    39 | loss: 6.3490114x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.380049228668213 1.4435378313064575
CurrentTrain: epoch  1, batch    40 | loss: 7.3800492x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.665403366088867 1.6588380336761475
CurrentTrain: epoch  1, batch    41 | loss: 7.6654034x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.623701095581055 1.4333010911941528
CurrentTrain: epoch  1, batch    42 | loss: 7.6237011x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.052209854125977 1.5580213069915771
CurrentTrain: epoch  1, batch    43 | loss: 8.0522099x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.080000877380371 1.4077578783035278
CurrentTrain: epoch  1, batch    44 | loss: 8.0800009x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.70178747177124 1.5435359477996826
CurrentTrain: epoch  1, batch    45 | loss: 6.7017875x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.237035751342773 1.4853355884552002
CurrentTrain: epoch  1, batch    46 | loss: 6.2370358x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.285773754119873 1.6266601085662842
CurrentTrain: epoch  1, batch    47 | loss: 7.2857738x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.972940921783447 1.4674501419067383
CurrentTrain: epoch  1, batch    48 | loss: 6.9729409x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.655514717102051 1.4708733558654785
CurrentTrain: epoch  1, batch    49 | loss: 6.6555147x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.886979103088379 1.332857608795166
CurrentTrain: epoch  1, batch    50 | loss: 6.8869791x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.892170429229736 1.4267454147338867
CurrentTrain: epoch  1, batch    51 | loss: 7.8921704x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.9680633544921875 1.4318666458129883
CurrentTrain: epoch  1, batch    52 | loss: 6.9680634x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.060792446136475 1.3945385217666626
CurrentTrain: epoch  1, batch    53 | loss: 7.0607924x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.015600681304932 1.4445074796676636
CurrentTrain: epoch  1, batch    54 | loss: 7.0156007x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.888180255889893 1.2253177165985107
CurrentTrain: epoch  1, batch    55 | loss: 5.8881803x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.040467739105225 1.2677106857299805
CurrentTrain: epoch  1, batch    56 | loss: 6.0404677x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.834133148193359 1.453202247619629
CurrentTrain: epoch  1, batch    57 | loss: 6.8341331x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.212436676025391 1.2806034088134766
CurrentTrain: epoch  1, batch    58 | loss: 6.2124367x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.405393123626709 1.5608692169189453
CurrentTrain: epoch  1, batch    59 | loss: 7.4053931x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.056479454040527 1.482541561126709
CurrentTrain: epoch  1, batch    60 | loss: 7.0564795x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.567456245422363 1.5728840827941895
CurrentTrain: epoch  1, batch    61 | loss: 6.5674562x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2067670822143555 0.931165874004364
CurrentTrain: epoch  1, batch    62 | loss: 5.2067671x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.630093097686768 1.2052849531173706
CurrentTrain: epoch  2, batch     0 | loss: 6.6300931x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.508365631103516 1.1735543012619019
CurrentTrain: epoch  2, batch     1 | loss: 6.5083656x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.681255340576172 1.4644107818603516
CurrentTrain: epoch  2, batch     2 | loss: 6.6812553x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.850689888000488 1.2834796905517578
CurrentTrain: epoch  2, batch     3 | loss: 5.8506899x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.26637077331543 1.259701132774353
CurrentTrain: epoch  2, batch     4 | loss: 6.2663708x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.18607759475708 1.469761610031128
CurrentTrain: epoch  2, batch     5 | loss: 6.1860776x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.061710357666016 1.180121898651123
CurrentTrain: epoch  2, batch     6 | loss: 6.0617104x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.816301345825195 1.377725601196289
CurrentTrain: epoch  2, batch     7 | loss: 6.8163013x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.329916954040527 1.3377785682678223
CurrentTrain: epoch  2, batch     8 | loss: 7.3299170x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.899871349334717 1.2408268451690674
CurrentTrain: epoch  2, batch     9 | loss: 5.8998713x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.774296760559082 1.2740153074264526
CurrentTrain: epoch  2, batch    10 | loss: 5.7742968x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.730959415435791 1.3716809749603271
CurrentTrain: epoch  2, batch    11 | loss: 6.7309594x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.932214736938477 1.2796344757080078
CurrentTrain: epoch  2, batch    12 | loss: 6.9322147x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.55429744720459 1.2464255094528198
CurrentTrain: epoch  2, batch    13 | loss: 6.5542974x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.390684127807617 1.4167113304138184
CurrentTrain: epoch  2, batch    14 | loss: 6.3906841x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.026276111602783 1.3728628158569336
CurrentTrain: epoch  2, batch    15 | loss: 6.0262761x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.680749893188477 1.2630494832992554
CurrentTrain: epoch  2, batch    16 | loss: 5.6807499x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.244289875030518 0.9192905426025391
CurrentTrain: epoch  2, batch    17 | loss: 5.2442899x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.768978118896484 1.442166805267334
CurrentTrain: epoch  2, batch    18 | loss: 6.7689781x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.147119045257568 1.249469518661499
CurrentTrain: epoch  2, batch    19 | loss: 6.1471190x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.05328893661499 1.2401154041290283
CurrentTrain: epoch  2, batch    20 | loss: 6.0532889x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.860775947570801 1.2457081079483032
CurrentTrain: epoch  2, batch    21 | loss: 5.8607759x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.675922393798828 1.2711093425750732
CurrentTrain: epoch  2, batch    22 | loss: 5.6759224x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.320955753326416 1.3323872089385986
CurrentTrain: epoch  2, batch    23 | loss: 6.3209558x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.205848693847656 1.1694908142089844
CurrentTrain: epoch  2, batch    24 | loss: 6.2058487x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.807610511779785 1.2032126188278198
CurrentTrain: epoch  2, batch    25 | loss: 5.8076105x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.581456184387207 1.1734309196472168
CurrentTrain: epoch  2, batch    26 | loss: 5.5814562x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.789383888244629 1.007904052734375
CurrentTrain: epoch  2, batch    27 | loss: 5.7893839x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.644453525543213 1.0599491596221924
CurrentTrain: epoch  2, batch    28 | loss: 5.6444535x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.141595840454102 1.3762619495391846
CurrentTrain: epoch  2, batch    29 | loss: 6.1415958x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349484920501709 1.2934733629226685
CurrentTrain: epoch  2, batch    30 | loss: 6.3494849x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.732242584228516 1.3867745399475098
CurrentTrain: epoch  2, batch    31 | loss: 5.7322426x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8030781745910645 1.1243202686309814
CurrentTrain: epoch  2, batch    32 | loss: 6.8030782x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.371408462524414 1.06284499168396
CurrentTrain: epoch  2, batch    33 | loss: 5.3714085x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.11379337310791 1.235999584197998
CurrentTrain: epoch  2, batch    34 | loss: 6.1137934x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.1160430908203125 1.1560214757919312
CurrentTrain: epoch  2, batch    35 | loss: 6.1160431x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.563666820526123 1.1961088180541992
CurrentTrain: epoch  2, batch    36 | loss: 5.5636668x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.901392936706543 1.0547535419464111
CurrentTrain: epoch  2, batch    37 | loss: 5.9013929x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.816005706787109 1.1190227270126343
CurrentTrain: epoch  2, batch    38 | loss: 5.8160057x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.200374126434326 0.9636704921722412
CurrentTrain: epoch  2, batch    39 | loss: 5.2003741x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.949509143829346 0.9317396283149719
CurrentTrain: epoch  2, batch    40 | loss: 5.9495091x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.409481048583984 0.8464484810829163
CurrentTrain: epoch  2, batch    41 | loss: 6.4094810x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.743829250335693 1.0953044891357422
CurrentTrain: epoch  2, batch    42 | loss: 5.7438293x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.965728759765625 1.1844062805175781
CurrentTrain: epoch  2, batch    43 | loss: 5.9657288x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.747776508331299 1.249800443649292
CurrentTrain: epoch  2, batch    44 | loss: 5.7477765x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.041460990905762 1.1863003969192505
CurrentTrain: epoch  2, batch    45 | loss: 6.0414610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.448573112487793 1.1486191749572754
CurrentTrain: epoch  2, batch    46 | loss: 5.4485731x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2633867263793945 1.1743338108062744
CurrentTrain: epoch  2, batch    47 | loss: 6.2633867x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.380342483520508 1.0493706464767456
CurrentTrain: epoch  2, batch    48 | loss: 5.3803425x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.993921756744385 1.2152526378631592
CurrentTrain: epoch  2, batch    49 | loss: 5.9939218x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.56021785736084 0.9582090377807617
CurrentTrain: epoch  2, batch    50 | loss: 5.5602179x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.392078399658203 1.1478691101074219
CurrentTrain: epoch  2, batch    51 | loss: 5.3920784x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.622655391693115 1.0182898044586182
CurrentTrain: epoch  2, batch    52 | loss: 5.6226554x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626033306121826 1.2189699411392212
CurrentTrain: epoch  2, batch    53 | loss: 5.6260333x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.478575229644775 1.1590144634246826
CurrentTrain: epoch  2, batch    54 | loss: 5.4785752x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.522172451019287 1.1833986043930054
CurrentTrain: epoch  2, batch    55 | loss: 5.5221725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.971188545227051 1.2475519180297852
CurrentTrain: epoch  2, batch    56 | loss: 5.9711885x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.804933547973633 1.1557669639587402
CurrentTrain: epoch  2, batch    57 | loss: 5.8049335x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5172319412231445 1.027930498123169
CurrentTrain: epoch  2, batch    58 | loss: 5.5172319x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.411221981048584 1.138274908065796
CurrentTrain: epoch  2, batch    59 | loss: 5.4112220x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.255485534667969 1.0583497285842896
CurrentTrain: epoch  2, batch    60 | loss: 5.2554855x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.477558135986328 1.049267292022705
CurrentTrain: epoch  2, batch    61 | loss: 5.4775581x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.209966659545898 0.7732165455818176
CurrentTrain: epoch  2, batch    62 | loss: 5.2099667x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.035955905914307 1.081830620765686
CurrentTrain: epoch  3, batch     0 | loss: 6.0359559x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.456923007965088 1.0081608295440674
CurrentTrain: epoch  3, batch     1 | loss: 6.4569230x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.480226039886475 1.078663945198059
CurrentTrain: epoch  3, batch     2 | loss: 5.4802260x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.740518093109131 0.898904025554657
CurrentTrain: epoch  3, batch     3 | loss: 5.7405181x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626705169677734 0.9309966564178467
CurrentTrain: epoch  3, batch     4 | loss: 5.6267052x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.738065242767334 0.9585984945297241
CurrentTrain: epoch  3, batch     5 | loss: 5.7380652x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.444523811340332 1.0503966808319092
CurrentTrain: epoch  3, batch     6 | loss: 5.4445238x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.4213995933532715 1.037995457649231
CurrentTrain: epoch  3, batch     7 | loss: 5.4213996x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7687273025512695 0.9488410353660583
CurrentTrain: epoch  3, batch     8 | loss: 5.7687273x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7205610275268555 1.0622001886367798
CurrentTrain: epoch  3, batch     9 | loss: 5.7205610x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.6544694900512695 1.1499388217926025
CurrentTrain: epoch  3, batch    10 | loss: 5.6544695x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.486059188842773 0.986730694770813
CurrentTrain: epoch  3, batch    11 | loss: 5.4860592x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.953603744506836 0.8693920373916626
CurrentTrain: epoch  3, batch    12 | loss: 4.9536037x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.584843158721924 1.0983784198760986
CurrentTrain: epoch  3, batch    13 | loss: 5.5848432x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.262392997741699 1.0211337804794312
CurrentTrain: epoch  3, batch    14 | loss: 5.2623930x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140032768249512 0.9961339235305786
CurrentTrain: epoch  3, batch    15 | loss: 5.1400328x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.498325824737549 0.9570016860961914
CurrentTrain: epoch  3, batch    16 | loss: 5.4983258x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.739621162414551 0.9503822326660156
CurrentTrain: epoch  3, batch    17 | loss: 5.7396212x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.441994667053223 0.8477289080619812
CurrentTrain: epoch  3, batch    18 | loss: 5.4419947x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.407026767730713 0.9850529432296753
CurrentTrain: epoch  3, batch    19 | loss: 5.4070268x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.29840612411499 1.0056352615356445
CurrentTrain: epoch  3, batch    20 | loss: 5.2984061x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2915849685668945 1.0394574403762817
CurrentTrain: epoch  3, batch    21 | loss: 5.2915850x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.343301296234131 0.9191257953643799
CurrentTrain: epoch  3, batch    22 | loss: 5.3433013x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.226625919342041 0.9959335923194885
CurrentTrain: epoch  3, batch    23 | loss: 5.2266259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.03607702255249 0.6870988607406616
CurrentTrain: epoch  3, batch    24 | loss: 5.0360770x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.109294891357422 1.0195376873016357
CurrentTrain: epoch  3, batch    25 | loss: 5.1092949x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.272613048553467 1.0991891622543335
CurrentTrain: epoch  3, batch    26 | loss: 5.2726130x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.175928115844727 0.9497132301330566
CurrentTrain: epoch  3, batch    27 | loss: 5.1759281x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.34478235244751 1.0801821947097778
CurrentTrain: epoch  3, batch    28 | loss: 5.3447824x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.681675434112549 0.9446146488189697
CurrentTrain: epoch  3, batch    29 | loss: 5.6816754x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.160543441772461 1.1458017826080322
CurrentTrain: epoch  3, batch    30 | loss: 6.1605434x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.885409832000732 0.8732045292854309
CurrentTrain: epoch  3, batch    31 | loss: 4.8854098x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.374021530151367 0.8337904214859009
CurrentTrain: epoch  3, batch    32 | loss: 5.3740215x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.220224857330322 0.829306960105896
CurrentTrain: epoch  3, batch    33 | loss: 5.2202249x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0848708152771 0.9158856868743896
CurrentTrain: epoch  3, batch    34 | loss: 5.0848708x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0665106773376465 0.9240090847015381
CurrentTrain: epoch  3, batch    35 | loss: 5.0665107x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.8889312744140625 1.0663390159606934
CurrentTrain: epoch  3, batch    36 | loss: 5.8889313x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.190898418426514 0.8566985726356506
CurrentTrain: epoch  3, batch    37 | loss: 5.1908984x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959601879119873 0.9252779483795166
CurrentTrain: epoch  3, batch    38 | loss: 4.9596019x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.827635765075684 0.8134810924530029
CurrentTrain: epoch  3, batch    39 | loss: 5.8276358x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140291213989258 0.9769730567932129
CurrentTrain: epoch  3, batch    40 | loss: 5.1402912x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.766822338104248 0.6842474341392517
CurrentTrain: epoch  3, batch    41 | loss: 4.7668223x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.570883750915527 0.8117127418518066
CurrentTrain: epoch  3, batch    42 | loss: 5.5708838x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.099788188934326 0.729293942451477
CurrentTrain: epoch  3, batch    43 | loss: 5.0997882x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.410670280456543 0.9232392311096191
CurrentTrain: epoch  3, batch    44 | loss: 5.4106703x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.390532493591309 0.8209837675094604
CurrentTrain: epoch  3, batch    45 | loss: 5.3905325x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.863620758056641 0.8677620887756348
CurrentTrain: epoch  3, batch    46 | loss: 4.8636208x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.13870906829834 0.7726187705993652
CurrentTrain: epoch  3, batch    47 | loss: 5.1387091x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8672943115234375 0.6362746953964233
CurrentTrain: epoch  3, batch    48 | loss: 4.8672943x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0335259437561035 0.6836175918579102
CurrentTrain: epoch  3, batch    49 | loss: 5.0335259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.008811950683594 0.8716031908988953
CurrentTrain: epoch  3, batch    50 | loss: 5.0088120x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.192744731903076 0.7628194093704224
CurrentTrain: epoch  3, batch    51 | loss: 5.1927447x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.968265533447266 0.8424209952354431
CurrentTrain: epoch  3, batch    52 | loss: 4.9682655x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.672567844390869 0.9232137203216553
CurrentTrain: epoch  3, batch    53 | loss: 5.6725678x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.920748710632324 0.8501842021942139
CurrentTrain: epoch  3, batch    54 | loss: 4.9207487x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.090141296386719 0.957904577255249
CurrentTrain: epoch  3, batch    55 | loss: 5.0901413x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.886087417602539 0.816415548324585
CurrentTrain: epoch  3, batch    56 | loss: 4.8860874x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.327167510986328 0.8300053477287292
CurrentTrain: epoch  3, batch    57 | loss: 5.3271675x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2497477531433105 0.8219953179359436
CurrentTrain: epoch  3, batch    58 | loss: 5.2497478x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.9105000495910645 0.6527562737464905
CurrentTrain: epoch  3, batch    59 | loss: 4.9105000x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.984633445739746 0.9194714426994324
CurrentTrain: epoch  3, batch    60 | loss: 4.9846334x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.813289642333984 0.7433576583862305
CurrentTrain: epoch  3, batch    61 | loss: 4.8132896x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691621780395508 0.5168606042861938
CurrentTrain: epoch  3, batch    62 | loss: 4.6916218x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.693078517913818 0.5873658061027527
CurrentTrain: epoch  4, batch     0 | loss: 4.6930785x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.806028366088867 0.745362401008606
CurrentTrain: epoch  4, batch     1 | loss: 4.8060284x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.904834747314453 0.7988278269767761
CurrentTrain: epoch  4, batch     2 | loss: 4.9048347x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.719254970550537 0.658371090888977
CurrentTrain: epoch  4, batch     3 | loss: 4.7192550x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959964752197266 0.9529321193695068
CurrentTrain: epoch  4, batch     4 | loss: 4.9599648x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.135072708129883 0.9262544512748718
CurrentTrain: epoch  4, batch     5 | loss: 5.1350727x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.84869909286499 0.7296959161758423
CurrentTrain: epoch  4, batch     6 | loss: 4.8486991x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.733144760131836 0.6090331077575684
CurrentTrain: epoch  4, batch     7 | loss: 4.7331448x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.841223239898682 0.8645848035812378
CurrentTrain: epoch  4, batch     8 | loss: 4.8412232x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7772746086120605 0.7030757069587708
CurrentTrain: epoch  4, batch     9 | loss: 4.7772746x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.814172744750977 0.7269054651260376
CurrentTrain: epoch  4, batch    10 | loss: 4.8141727x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.166572570800781 0.7654482126235962
CurrentTrain: epoch  4, batch    11 | loss: 5.1665726x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.869943618774414 0.7695276141166687
CurrentTrain: epoch  4, batch    12 | loss: 4.8699436x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.181846618652344 0.8651489615440369
CurrentTrain: epoch  4, batch    13 | loss: 5.1818466x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.493844985961914 0.5341933965682983
CurrentTrain: epoch  4, batch    14 | loss: 4.4938450x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.749185562133789 0.6812607049942017
CurrentTrain: epoch  4, batch    15 | loss: 4.7491856x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.824125289916992 0.7251666784286499
CurrentTrain: epoch  4, batch    16 | loss: 4.8241253x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.799384593963623 0.6490915417671204
CurrentTrain: epoch  4, batch    17 | loss: 4.7993846x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.801788330078125 0.8030322790145874
CurrentTrain: epoch  4, batch    18 | loss: 4.8017883x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.796889781951904 0.7754301428794861
CurrentTrain: epoch  4, batch    19 | loss: 4.7968898x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.153695106506348 0.6834776997566223
CurrentTrain: epoch  4, batch    20 | loss: 5.1536951x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.915266990661621 0.5711497664451599
CurrentTrain: epoch  4, batch    21 | loss: 4.9152670x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.570249557495117 0.6395601630210876
CurrentTrain: epoch  4, batch    22 | loss: 4.5702496x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.478219032287598 0.44560980796813965
CurrentTrain: epoch  4, batch    23 | loss: 4.4782190x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.817839622497559 0.5910068154335022
CurrentTrain: epoch  4, batch    24 | loss: 4.8178396x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.768563270568848 0.7635782957077026
CurrentTrain: epoch  4, batch    25 | loss: 4.7685633x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.609344959259033 0.6901866793632507
CurrentTrain: epoch  4, batch    26 | loss: 4.6093450x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.637467384338379 0.6230311989784241
CurrentTrain: epoch  4, batch    27 | loss: 4.6374674x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.767701625823975 0.6288499236106873
CurrentTrain: epoch  4, batch    28 | loss: 4.7677016x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.700007915496826 0.6359813809394836
CurrentTrain: epoch  4, batch    29 | loss: 4.7000079x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.588162422180176 0.6554694175720215
CurrentTrain: epoch  4, batch    30 | loss: 4.5881624x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.732369899749756 0.6765909194946289
CurrentTrain: epoch  4, batch    31 | loss: 4.7323699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.666533470153809 0.6684315204620361
CurrentTrain: epoch  4, batch    32 | loss: 4.6665335x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7807159423828125 0.7347530126571655
CurrentTrain: epoch  4, batch    33 | loss: 4.7807159x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6732892990112305 0.6531981229782104
CurrentTrain: epoch  4, batch    34 | loss: 4.6732893x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.888833045959473 0.7925335764884949
CurrentTrain: epoch  4, batch    35 | loss: 4.8888330x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.699143409729004 0.6564288139343262
CurrentTrain: epoch  4, batch    36 | loss: 4.6991434x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.110682964324951 0.7736770510673523
CurrentTrain: epoch  4, batch    37 | loss: 5.1106830x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704012393951416 0.6345605254173279
CurrentTrain: epoch  4, batch    38 | loss: 4.7040124x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.62897253036499 0.6374919414520264
CurrentTrain: epoch  4, batch    39 | loss: 4.6289725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.552116870880127 0.6075010299682617
CurrentTrain: epoch  4, batch    40 | loss: 4.5521169x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.595989227294922 0.6946349143981934
CurrentTrain: epoch  4, batch    41 | loss: 4.5959892x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.858814239501953 0.7200260162353516
CurrentTrain: epoch  4, batch    42 | loss: 4.8588142x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.929131984710693 0.5336486101150513
CurrentTrain: epoch  4, batch    43 | loss: 4.9291320x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.455041885375977 0.5244512557983398
CurrentTrain: epoch  4, batch    44 | loss: 4.4550419x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.24995231628418 0.6656460762023926
CurrentTrain: epoch  4, batch    45 | loss: 5.2499523x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.745295524597168 0.7060742378234863
CurrentTrain: epoch  4, batch    46 | loss: 4.7452955x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.660812854766846 0.6329832673072815
CurrentTrain: epoch  4, batch    47 | loss: 4.6608129x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.638529300689697 0.6848247051239014
CurrentTrain: epoch  4, batch    48 | loss: 4.6385293x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.644412040710449 0.6397827863693237
CurrentTrain: epoch  4, batch    49 | loss: 4.6444120x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.774701118469238 0.8059102892875671
CurrentTrain: epoch  4, batch    50 | loss: 4.7747011x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.615450859069824 0.5835493206977844
CurrentTrain: epoch  4, batch    51 | loss: 4.6154509x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.524040222167969 0.5134026408195496
CurrentTrain: epoch  4, batch    52 | loss: 4.5240402x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8538665771484375 0.6754355430603027
CurrentTrain: epoch  4, batch    53 | loss: 4.8538666x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.653186321258545 0.5864333510398865
CurrentTrain: epoch  4, batch    54 | loss: 4.6531863x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.649025917053223 0.6997280120849609
CurrentTrain: epoch  4, batch    55 | loss: 4.6490259x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.565957546234131 0.5201479196548462
CurrentTrain: epoch  4, batch    56 | loss: 4.5659575x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691095352172852 0.7682797908782959
CurrentTrain: epoch  4, batch    57 | loss: 4.6910954x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590443134307861 0.582393229007721
CurrentTrain: epoch  4, batch    58 | loss: 4.5904431x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.584754467010498 0.6168481111526489
CurrentTrain: epoch  4, batch    59 | loss: 4.5847545x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.592937469482422 0.6070008277893066
CurrentTrain: epoch  4, batch    60 | loss: 4.5929375x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704367637634277 0.6315898895263672
CurrentTrain: epoch  4, batch    61 | loss: 4.7043676x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.513129234313965 0.45638883113861084
CurrentTrain: epoch  4, batch    62 | loss: 4.5131292x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.75014066696167 0.7058000564575195
CurrentTrain: epoch  5, batch     0 | loss: 4.7501407x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.630569934844971 0.6423497796058655
CurrentTrain: epoch  5, batch     1 | loss: 4.6305699x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.550356388092041 0.5766547322273254
CurrentTrain: epoch  5, batch     2 | loss: 4.5503564x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.533287048339844 0.5767020583152771
CurrentTrain: epoch  5, batch     3 | loss: 4.5332870x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.652379035949707 0.6052058339118958
CurrentTrain: epoch  5, batch     4 | loss: 4.6523790x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.605034351348877 0.5880939960479736
CurrentTrain: epoch  5, batch     5 | loss: 4.6050344x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.602302074432373 0.6390889883041382
CurrentTrain: epoch  5, batch     6 | loss: 4.6023021x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.482685089111328 0.6640385389328003
CurrentTrain: epoch  5, batch     7 | loss: 4.4826851x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.64866304397583 0.6475061774253845
CurrentTrain: epoch  5, batch     8 | loss: 4.6486630x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.44582986831665 0.4502357840538025
CurrentTrain: epoch  5, batch     9 | loss: 4.4458299x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.843441009521484 0.6541036367416382
CurrentTrain: epoch  5, batch    10 | loss: 4.8434410x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590672492980957 0.6501652002334595
CurrentTrain: epoch  5, batch    11 | loss: 4.5906725x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.445680141448975 0.5148342847824097
CurrentTrain: epoch  5, batch    12 | loss: 4.4456801x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.491142749786377 0.5643436908721924
CurrentTrain: epoch  5, batch    13 | loss: 4.4911427x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.483101844787598 0.6623929142951965
CurrentTrain: epoch  5, batch    14 | loss: 4.4831018x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.50734806060791 0.5970326066017151
CurrentTrain: epoch  5, batch    15 | loss: 4.5073481x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.620008945465088 0.6581331491470337
CurrentTrain: epoch  5, batch    16 | loss: 4.6200089x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485630035400391 0.6099047660827637
CurrentTrain: epoch  5, batch    17 | loss: 4.4856300x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.618782043457031 0.6864808797836304
CurrentTrain: epoch  5, batch    18 | loss: 4.6187820x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.487261772155762 0.547090470790863
CurrentTrain: epoch  5, batch    19 | loss: 4.4872618x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416949272155762 0.5254210829734802
CurrentTrain: epoch  5, batch    20 | loss: 4.4169493x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.467197418212891 0.5430623888969421
CurrentTrain: epoch  5, batch    21 | loss: 4.4671974x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485051155090332 0.4818846583366394
CurrentTrain: epoch  5, batch    22 | loss: 4.4850512x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.551313877105713 0.5901062488555908
CurrentTrain: epoch  5, batch    23 | loss: 4.5513139x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.453506946563721 0.498520165681839
CurrentTrain: epoch  5, batch    24 | loss: 4.4535069x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463562488555908 0.5646237134933472
CurrentTrain: epoch  5, batch    25 | loss: 4.4635625x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5455098152160645 0.5900653600692749
CurrentTrain: epoch  5, batch    26 | loss: 4.5455098x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4906439781188965 0.5196026563644409
CurrentTrain: epoch  5, batch    27 | loss: 4.4906440x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463355541229248 0.6276993155479431
CurrentTrain: epoch  5, batch    28 | loss: 4.4633555x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466972351074219 0.6047004461288452
CurrentTrain: epoch  5, batch    29 | loss: 4.4669724x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400462627410889 0.4878422021865845
CurrentTrain: epoch  5, batch    30 | loss: 4.4004626x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.408786296844482 0.5057677030563354
CurrentTrain: epoch  5, batch    31 | loss: 4.4087863x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466117858886719 0.48728930950164795
CurrentTrain: epoch  5, batch    32 | loss: 4.4661179x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.321932792663574 0.44563430547714233
CurrentTrain: epoch  5, batch    33 | loss: 4.3219328x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.451994895935059 0.4971400499343872
CurrentTrain: epoch  5, batch    34 | loss: 4.4519949x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.360955715179443 0.4419785439968109
CurrentTrain: epoch  5, batch    35 | loss: 4.3609557x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3710784912109375 0.430392861366272
CurrentTrain: epoch  5, batch    36 | loss: 4.3710785x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.347805976867676 0.4565986096858978
CurrentTrain: epoch  5, batch    37 | loss: 4.3478060x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.444753646850586 0.5379966497421265
CurrentTrain: epoch  5, batch    38 | loss: 4.4447536x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.534544944763184 0.5813654661178589
CurrentTrain: epoch  5, batch    39 | loss: 4.5345449x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.415805339813232 0.6031494140625
CurrentTrain: epoch  5, batch    40 | loss: 4.4158053x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.398653507232666 0.4548051953315735
CurrentTrain: epoch  5, batch    41 | loss: 4.3986535x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.596743106842041 0.6089962124824524
CurrentTrain: epoch  5, batch    42 | loss: 4.5967431x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.757709503173828 0.6084247827529907
CurrentTrain: epoch  5, batch    43 | loss: 4.7577095x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.414988040924072 0.5211746692657471
CurrentTrain: epoch  5, batch    44 | loss: 4.4149880x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.372142314910889 0.4713820517063141
CurrentTrain: epoch  5, batch    45 | loss: 4.3721423x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.401346683502197 0.5249283313751221
CurrentTrain: epoch  5, batch    46 | loss: 4.4013467x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3771162033081055 0.5075411796569824
CurrentTrain: epoch  5, batch    47 | loss: 4.3771162x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.41010856628418 0.5496524572372437
CurrentTrain: epoch  5, batch    48 | loss: 4.4101086x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416181564331055 0.49380460381507874
CurrentTrain: epoch  5, batch    49 | loss: 4.4161816x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.341184139251709 0.46353790163993835
CurrentTrain: epoch  5, batch    50 | loss: 4.3411841x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.352419853210449 0.4754869043827057
CurrentTrain: epoch  5, batch    51 | loss: 4.3524199x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.464930057525635 0.5858806371688843
CurrentTrain: epoch  5, batch    52 | loss: 4.4649301x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.403540134429932 0.4749881625175476
CurrentTrain: epoch  5, batch    53 | loss: 4.4035401x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.404925346374512 0.6019455194473267
CurrentTrain: epoch  5, batch    54 | loss: 4.4049253x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.451889514923096 0.5230963230133057
CurrentTrain: epoch  5, batch    55 | loss: 4.4518895x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.415914535522461 0.5216556787490845
CurrentTrain: epoch  5, batch    56 | loss: 4.4159145x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.686596870422363 0.47879573702812195
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 1 0 3 3 1 0 2 2]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.926849365234375 1.7984235286712646
CurrentTrain: epoch  0, batch     0 | loss: 10.9268494des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.036380767822266 2.139348030090332
CurrentTrain: epoch  0, batch     1 | loss: 11.0363808des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.066969871520996 1.9709371328353882
CurrentTrain: epoch  0, batch     2 | loss: 11.0669699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.887831687927246 1.7607920169830322
CurrentTrain: epoch  0, batch     3 | loss: 10.8878317des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.255082130432129 2.0063090324401855
CurrentTrain: epoch  0, batch     4 | loss: 10.2550821des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.279458999633789 1.6876899003982544
CurrentTrain: epoch  0, batch     5 | loss: 11.2794590des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.48698616027832 1.8612170219421387
CurrentTrain: epoch  0, batch     6 | loss: 10.4869862des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.94703483581543 1.973750114440918
CurrentTrain: epoch  0, batch     7 | loss: 9.9470348des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.128488540649414 2.01865816116333
CurrentTrain: epoch  0, batch     8 | loss: 10.1284885des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.697672843933105 1.9570696353912354
CurrentTrain: epoch  0, batch     9 | loss: 10.6976728des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.838467597961426 1.7362463474273682
CurrentTrain: epoch  0, batch    10 | loss: 9.8384676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.33670425415039 1.7744444608688354
CurrentTrain: epoch  0, batch    11 | loss: 9.3367043des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.044028282165527 1.8078560829162598
CurrentTrain: epoch  0, batch    12 | loss: 10.0440283des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.975881576538086 1.8939635753631592
CurrentTrain: epoch  0, batch    13 | loss: 9.9758816des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.111963272094727 1.8994615077972412
CurrentTrain: epoch  0, batch    14 | loss: 9.1119633des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.866656303405762 2.021256923675537
CurrentTrain: epoch  0, batch    15 | loss: 9.8666563des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.058426856994629 1.6424639225006104
CurrentTrain: epoch  0, batch    16 | loss: 10.0584269des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.066779136657715 2.1173763275146484
CurrentTrain: epoch  0, batch    17 | loss: 10.0667791des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.88123607635498 1.8144304752349854
CurrentTrain: epoch  0, batch    18 | loss: 9.8812361des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.649572372436523 2.014930248260498
CurrentTrain: epoch  0, batch    19 | loss: 9.6495724des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.003922462463379 1.9514461755752563
CurrentTrain: epoch  0, batch    20 | loss: 9.0039225des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.172370910644531 1.9768781661987305
CurrentTrain: epoch  0, batch    21 | loss: 9.1723709des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.688657760620117 1.8792906999588013
CurrentTrain: epoch  0, batch    22 | loss: 9.6886578des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.10012149810791 1.8889232873916626
CurrentTrain: epoch  0, batch    23 | loss: 9.1001215des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.770186424255371 1.9979017972946167
CurrentTrain: epoch  0, batch    24 | loss: 8.7701864des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.946695327758789 1.8629238605499268
CurrentTrain: epoch  0, batch    25 | loss: 8.9466953des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.109234809875488 1.7668402194976807
CurrentTrain: epoch  0, batch    26 | loss: 9.1092348des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.778518676757812 2.047266960144043
CurrentTrain: epoch  0, batch    27 | loss: 8.7785187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.098732948303223 1.7841252088546753
CurrentTrain: epoch  0, batch    28 | loss: 9.0987329des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.13674545288086 1.9178639650344849
CurrentTrain: epoch  0, batch    29 | loss: 9.1367455des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.567536354064941 2.0098025798797607
CurrentTrain: epoch  0, batch    30 | loss: 9.5675364des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.879860877990723 1.7162644863128662
CurrentTrain: epoch  0, batch    31 | loss: 8.8798609des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.670268058776855 1.3618415594100952
CurrentTrain: epoch  0, batch    32 | loss: 8.6702681des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.318636894226074 1.7121176719665527
CurrentTrain: epoch  0, batch    33 | loss: 8.3186369des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.825912475585938 1.8363399505615234
CurrentTrain: epoch  0, batch    34 | loss: 8.8259125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.652521133422852 1.8380666971206665
CurrentTrain: epoch  0, batch    35 | loss: 8.6525211des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.657340049743652 1.8205554485321045
CurrentTrain: epoch  0, batch    36 | loss: 8.6573400des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.692634582519531 1.864047646522522
CurrentTrain: epoch  0, batch    37 | loss: 8.6926346des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.539445877075195 2.0489768981933594
CurrentTrain: epoch  0, batch    38 | loss: 9.5394459des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.06056022644043 2.041947603225708
CurrentTrain: epoch  0, batch    39 | loss: 9.0605602des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.387600898742676 1.8561205863952637
CurrentTrain: epoch  0, batch    40 | loss: 9.3876009des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.434120178222656 1.9399871826171875
CurrentTrain: epoch  0, batch    41 | loss: 8.4341202des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.22739315032959 1.8954997062683105
CurrentTrain: epoch  0, batch    42 | loss: 8.2273932des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815733909606934 1.5591213703155518
CurrentTrain: epoch  0, batch    43 | loss: 7.8157339des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.182122230529785 1.6677024364471436
CurrentTrain: epoch  0, batch    44 | loss: 8.1821222des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.323400497436523 1.9396532773971558
CurrentTrain: epoch  0, batch    45 | loss: 8.3234005des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.007547378540039 1.7199137210845947
CurrentTrain: epoch  0, batch    46 | loss: 9.0075474des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.5302653312683105 1.8195229768753052
CurrentTrain: epoch  0, batch    47 | loss: 7.5302653des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.532403945922852 1.8521592617034912
CurrentTrain: epoch  0, batch    48 | loss: 8.5324039des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.20888614654541 1.8072999715805054
CurrentTrain: epoch  0, batch    49 | loss: 8.2088861des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.609411239624023 1.8396973609924316
CurrentTrain: epoch  0, batch    50 | loss: 8.6094112des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.199667930603027 1.9371323585510254
CurrentTrain: epoch  0, batch    51 | loss: 8.1996679des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.848418712615967 1.7412703037261963
CurrentTrain: epoch  0, batch    52 | loss: 7.8484187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.000316619873047 1.7041115760803223
CurrentTrain: epoch  0, batch    53 | loss: 8.0003166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.699665069580078 1.9070736169815063
CurrentTrain: epoch  0, batch    54 | loss: 8.6996651des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.839630126953125 1.6547417640686035
CurrentTrain: epoch  0, batch    55 | loss: 7.8396301des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.614195823669434 1.6612800359725952
CurrentTrain: epoch  0, batch    56 | loss: 7.6141958des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.442203521728516 1.8620173931121826
CurrentTrain: epoch  0, batch    57 | loss: 8.4422035des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.174666404724121 1.540844440460205
CurrentTrain: epoch  0, batch    58 | loss: 8.1746664des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.778033256530762 1.4610964059829712
CurrentTrain: epoch  0, batch    59 | loss: 7.7780333des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.397575855255127 1.7511435747146606
CurrentTrain: epoch  0, batch    60 | loss: 7.3975759des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.202156066894531 1.5253829956054688
CurrentTrain: epoch  0, batch    61 | loss: 8.2021561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.528881072998047 1.2764627933502197
CurrentTrain: epoch  0, batch    62 | loss: 9.5288811des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.230393409729004 1.6139895915985107
CurrentTrain: epoch  1, batch     0 | loss: 8.2303934des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.833333969116211 1.6495968103408813
CurrentTrain: epoch  1, batch     1 | loss: 7.8333340des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.499612808227539 1.5285990238189697
CurrentTrain: epoch  1, batch     2 | loss: 7.4996128des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.852048873901367 1.7880139350891113
CurrentTrain: epoch  1, batch     3 | loss: 7.8520489des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.197295188903809 1.8135771751403809
CurrentTrain: epoch  1, batch     4 | loss: 8.1972952des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.601391315460205 1.651627779006958
CurrentTrain: epoch  1, batch     5 | loss: 7.6013913des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.739384651184082 1.6118459701538086
CurrentTrain: epoch  1, batch     6 | loss: 7.7393847des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.071622848510742 1.8014861345291138
CurrentTrain: epoch  1, batch     7 | loss: 7.0716228des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.06693696975708 1.4502551555633545
CurrentTrain: epoch  1, batch     8 | loss: 7.0669370des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.887425422668457 1.7192975282669067
CurrentTrain: epoch  1, batch     9 | loss: 7.8874254des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.787710189819336 1.708604335784912
CurrentTrain: epoch  1, batch    10 | loss: 7.7877102des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.386960983276367 1.689343810081482
CurrentTrain: epoch  1, batch    11 | loss: 7.3869610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.725907325744629 1.8044259548187256
CurrentTrain: epoch  1, batch    12 | loss: 7.7259073des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.805411338806152 1.3810582160949707
CurrentTrain: epoch  1, batch    13 | loss: 7.8054113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.030904293060303 1.5862092971801758
CurrentTrain: epoch  1, batch    14 | loss: 7.0309043des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.75450611114502 1.7045416831970215
CurrentTrain: epoch  1, batch    15 | loss: 8.7545061des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.320767402648926 1.4552063941955566
CurrentTrain: epoch  1, batch    16 | loss: 7.3207674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.6597700119018555 1.3610494136810303
CurrentTrain: epoch  1, batch    17 | loss: 6.6597700des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.124475002288818 1.5135514736175537
CurrentTrain: epoch  1, batch    18 | loss: 7.1244750des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.509670734405518 1.6222670078277588
CurrentTrain: epoch  1, batch    19 | loss: 7.5096707des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.815581321716309 1.7129367589950562
CurrentTrain: epoch  1, batch    20 | loss: 7.8155813des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.81373405456543 1.6730904579162598
CurrentTrain: epoch  1, batch    21 | loss: 7.8137341des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.510550022125244 1.5836284160614014
CurrentTrain: epoch  1, batch    22 | loss: 7.5105500des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.604043960571289 1.4094659090042114
CurrentTrain: epoch  1, batch    23 | loss: 7.6040440des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.351528644561768 1.5248346328735352
CurrentTrain: epoch  1, batch    24 | loss: 7.3515286des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.34840726852417 1.4831191301345825
CurrentTrain: epoch  1, batch    25 | loss: 6.3484073des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.978714466094971 1.6536893844604492
CurrentTrain: epoch  1, batch    26 | loss: 6.9787145des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.979429721832275 1.6979093551635742
CurrentTrain: epoch  1, batch    27 | loss: 6.9794297des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.359614372253418 1.6020715236663818
CurrentTrain: epoch  1, batch    28 | loss: 7.3596144des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8036370277404785 1.6879304647445679
CurrentTrain: epoch  1, batch    29 | loss: 6.8036370des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.0389251708984375 1.4688650369644165
CurrentTrain: epoch  1, batch    30 | loss: 7.0389252des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.539873123168945 1.6757961511611938
CurrentTrain: epoch  1, batch    31 | loss: 6.5398731des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2656354904174805 1.565091848373413
CurrentTrain: epoch  1, batch    32 | loss: 6.2656355des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.601977825164795 1.3745038509368896
CurrentTrain: epoch  1, batch    33 | loss: 6.6019778des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.520873546600342 1.6367700099945068
CurrentTrain: epoch  1, batch    34 | loss: 7.5208735des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.396836757659912 1.3968578577041626
CurrentTrain: epoch  1, batch    35 | loss: 6.3968368des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.622469902038574 1.3556203842163086
CurrentTrain: epoch  1, batch    36 | loss: 7.6224699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.46106481552124 1.668927788734436
CurrentTrain: epoch  1, batch    37 | loss: 7.4610648des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.874467372894287 1.647559404373169
CurrentTrain: epoch  1, batch    38 | loss: 7.8744674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349011421203613 1.5208356380462646
CurrentTrain: epoch  1, batch    39 | loss: 6.3490114des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.380049228668213 1.4435378313064575
CurrentTrain: epoch  1, batch    40 | loss: 7.3800492des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.665403366088867 1.6588380336761475
CurrentTrain: epoch  1, batch    41 | loss: 7.6654034des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.623701095581055 1.4333010911941528
CurrentTrain: epoch  1, batch    42 | loss: 7.6237011des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.052209854125977 1.5580213069915771
CurrentTrain: epoch  1, batch    43 | loss: 8.0522099des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.080000877380371 1.4077578783035278
CurrentTrain: epoch  1, batch    44 | loss: 8.0800009des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.70178747177124 1.5435359477996826
CurrentTrain: epoch  1, batch    45 | loss: 6.7017875des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.237035751342773 1.4853355884552002
CurrentTrain: epoch  1, batch    46 | loss: 6.2370358des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.285773754119873 1.6266601085662842
CurrentTrain: epoch  1, batch    47 | loss: 7.2857738des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.972940921783447 1.4674501419067383
CurrentTrain: epoch  1, batch    48 | loss: 6.9729409des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.655514717102051 1.4708733558654785
CurrentTrain: epoch  1, batch    49 | loss: 6.6555147des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.886979103088379 1.332857608795166
CurrentTrain: epoch  1, batch    50 | loss: 6.8869791des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.892170429229736 1.4267454147338867
CurrentTrain: epoch  1, batch    51 | loss: 7.8921704des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.9680633544921875 1.4318666458129883
CurrentTrain: epoch  1, batch    52 | loss: 6.9680634des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.060792446136475 1.3945385217666626
CurrentTrain: epoch  1, batch    53 | loss: 7.0607924des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.015600681304932 1.4445074796676636
CurrentTrain: epoch  1, batch    54 | loss: 7.0156007des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.888180255889893 1.2253177165985107
CurrentTrain: epoch  1, batch    55 | loss: 5.8881803des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.040467739105225 1.2677106857299805
CurrentTrain: epoch  1, batch    56 | loss: 6.0404677des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.834133148193359 1.453202247619629
CurrentTrain: epoch  1, batch    57 | loss: 6.8341331des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.212436676025391 1.2806034088134766
CurrentTrain: epoch  1, batch    58 | loss: 6.2124367des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.405393123626709 1.5608692169189453
CurrentTrain: epoch  1, batch    59 | loss: 7.4053931des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.056479454040527 1.482541561126709
CurrentTrain: epoch  1, batch    60 | loss: 7.0564795des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.567456245422363 1.5728840827941895
CurrentTrain: epoch  1, batch    61 | loss: 6.5674562des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2067670822143555 0.931165874004364
CurrentTrain: epoch  1, batch    62 | loss: 5.2067671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.630093097686768 1.2052849531173706
CurrentTrain: epoch  2, batch     0 | loss: 6.6300931des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.508365631103516 1.1735543012619019
CurrentTrain: epoch  2, batch     1 | loss: 6.5083656des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.681255340576172 1.4644107818603516
CurrentTrain: epoch  2, batch     2 | loss: 6.6812553des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.850689888000488 1.2834796905517578
CurrentTrain: epoch  2, batch     3 | loss: 5.8506899des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.26637077331543 1.259701132774353
CurrentTrain: epoch  2, batch     4 | loss: 6.2663708des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.18607759475708 1.469761610031128
CurrentTrain: epoch  2, batch     5 | loss: 6.1860776des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.061710357666016 1.180121898651123
CurrentTrain: epoch  2, batch     6 | loss: 6.0617104des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.816301345825195 1.377725601196289
CurrentTrain: epoch  2, batch     7 | loss: 6.8163013des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.329916954040527 1.3377785682678223
CurrentTrain: epoch  2, batch     8 | loss: 7.3299170des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.899871349334717 1.2408268451690674
CurrentTrain: epoch  2, batch     9 | loss: 5.8998713des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.774296760559082 1.2740153074264526
CurrentTrain: epoch  2, batch    10 | loss: 5.7742968des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.730959415435791 1.3716809749603271
CurrentTrain: epoch  2, batch    11 | loss: 6.7309594des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.932214736938477 1.2796344757080078
CurrentTrain: epoch  2, batch    12 | loss: 6.9322147des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.55429744720459 1.2464255094528198
CurrentTrain: epoch  2, batch    13 | loss: 6.5542974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.390684127807617 1.4167113304138184
CurrentTrain: epoch  2, batch    14 | loss: 6.3906841des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.026276111602783 1.3728628158569336
CurrentTrain: epoch  2, batch    15 | loss: 6.0262761des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.680749893188477 1.2630494832992554
CurrentTrain: epoch  2, batch    16 | loss: 5.6807499des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.244289875030518 0.9192905426025391
CurrentTrain: epoch  2, batch    17 | loss: 5.2442899des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.768978118896484 1.442166805267334
CurrentTrain: epoch  2, batch    18 | loss: 6.7689781des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.147119045257568 1.249469518661499
CurrentTrain: epoch  2, batch    19 | loss: 6.1471190des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.05328893661499 1.2401154041290283
CurrentTrain: epoch  2, batch    20 | loss: 6.0532889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.860775947570801 1.2457081079483032
CurrentTrain: epoch  2, batch    21 | loss: 5.8607759des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.675922393798828 1.2711093425750732
CurrentTrain: epoch  2, batch    22 | loss: 5.6759224des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.320955753326416 1.3323872089385986
CurrentTrain: epoch  2, batch    23 | loss: 6.3209558des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.205848693847656 1.1694908142089844
CurrentTrain: epoch  2, batch    24 | loss: 6.2058487des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.807610511779785 1.2032126188278198
CurrentTrain: epoch  2, batch    25 | loss: 5.8076105des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.581456184387207 1.1734309196472168
CurrentTrain: epoch  2, batch    26 | loss: 5.5814562des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.789383888244629 1.007904052734375
CurrentTrain: epoch  2, batch    27 | loss: 5.7893839des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.644453525543213 1.0599491596221924
CurrentTrain: epoch  2, batch    28 | loss: 5.6444535des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.141595840454102 1.3762619495391846
CurrentTrain: epoch  2, batch    29 | loss: 6.1415958des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.349484920501709 1.2934733629226685
CurrentTrain: epoch  2, batch    30 | loss: 6.3494849des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.732242584228516 1.3867745399475098
CurrentTrain: epoch  2, batch    31 | loss: 5.7322426des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.8030781745910645 1.1243202686309814
CurrentTrain: epoch  2, batch    32 | loss: 6.8030782des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.371408462524414 1.06284499168396
CurrentTrain: epoch  2, batch    33 | loss: 5.3714085des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.11379337310791 1.235999584197998
CurrentTrain: epoch  2, batch    34 | loss: 6.1137934des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.1160430908203125 1.1560214757919312
CurrentTrain: epoch  2, batch    35 | loss: 6.1160431des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.563666820526123 1.1961088180541992
CurrentTrain: epoch  2, batch    36 | loss: 5.5636668des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.901392936706543 1.0547535419464111
CurrentTrain: epoch  2, batch    37 | loss: 5.9013929des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.816005706787109 1.1190227270126343
CurrentTrain: epoch  2, batch    38 | loss: 5.8160057des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.200374126434326 0.9636704921722412
CurrentTrain: epoch  2, batch    39 | loss: 5.2003741des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.949509143829346 0.9317396283149719
CurrentTrain: epoch  2, batch    40 | loss: 5.9495091des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.409481048583984 0.8464484810829163
CurrentTrain: epoch  2, batch    41 | loss: 6.4094810des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.743829250335693 1.0953044891357422
CurrentTrain: epoch  2, batch    42 | loss: 5.7438293des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.965728759765625 1.1844062805175781
CurrentTrain: epoch  2, batch    43 | loss: 5.9657288des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.747776508331299 1.249800443649292
CurrentTrain: epoch  2, batch    44 | loss: 5.7477765des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.041460990905762 1.1863003969192505
CurrentTrain: epoch  2, batch    45 | loss: 6.0414610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.448573112487793 1.1486191749572754
CurrentTrain: epoch  2, batch    46 | loss: 5.4485731des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2633867263793945 1.1743338108062744
CurrentTrain: epoch  2, batch    47 | loss: 6.2633867des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.380342483520508 1.0493706464767456
CurrentTrain: epoch  2, batch    48 | loss: 5.3803425des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.993921756744385 1.2152526378631592
CurrentTrain: epoch  2, batch    49 | loss: 5.9939218des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.56021785736084 0.9582090377807617
CurrentTrain: epoch  2, batch    50 | loss: 5.5602179des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.392078399658203 1.1478691101074219
CurrentTrain: epoch  2, batch    51 | loss: 5.3920784des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.622655391693115 1.0182898044586182
CurrentTrain: epoch  2, batch    52 | loss: 5.6226554des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626033306121826 1.2189699411392212
CurrentTrain: epoch  2, batch    53 | loss: 5.6260333des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.478575229644775 1.1590144634246826
CurrentTrain: epoch  2, batch    54 | loss: 5.4785752des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.522172451019287 1.1833986043930054
CurrentTrain: epoch  2, batch    55 | loss: 5.5221725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.971188545227051 1.2475519180297852
CurrentTrain: epoch  2, batch    56 | loss: 5.9711885des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.804933547973633 1.1557669639587402
CurrentTrain: epoch  2, batch    57 | loss: 5.8049335des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5172319412231445 1.027930498123169
CurrentTrain: epoch  2, batch    58 | loss: 5.5172319des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.411221981048584 1.138274908065796
CurrentTrain: epoch  2, batch    59 | loss: 5.4112220des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.255485534667969 1.0583497285842896
CurrentTrain: epoch  2, batch    60 | loss: 5.2554855des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.477558135986328 1.049267292022705
CurrentTrain: epoch  2, batch    61 | loss: 5.4775581des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.209966659545898 0.7732165455818176
CurrentTrain: epoch  2, batch    62 | loss: 5.2099667des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.035955905914307 1.081830620765686
CurrentTrain: epoch  3, batch     0 | loss: 6.0359559des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.456923007965088 1.0081608295440674
CurrentTrain: epoch  3, batch     1 | loss: 6.4569230des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.480226039886475 1.078663945198059
CurrentTrain: epoch  3, batch     2 | loss: 5.4802260des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.740518093109131 0.898904025554657
CurrentTrain: epoch  3, batch     3 | loss: 5.7405181des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626705169677734 0.9309966564178467
CurrentTrain: epoch  3, batch     4 | loss: 5.6267052des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.738065242767334 0.9585984945297241
CurrentTrain: epoch  3, batch     5 | loss: 5.7380652des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.444523811340332 1.0503966808319092
CurrentTrain: epoch  3, batch     6 | loss: 5.4445238des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.4213995933532715 1.037995457649231
CurrentTrain: epoch  3, batch     7 | loss: 5.4213996des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7687273025512695 0.9488410353660583
CurrentTrain: epoch  3, batch     8 | loss: 5.7687273des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7205610275268555 1.0622001886367798
CurrentTrain: epoch  3, batch     9 | loss: 5.7205610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.6544694900512695 1.1499388217926025
CurrentTrain: epoch  3, batch    10 | loss: 5.6544695des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.486059188842773 0.986730694770813
CurrentTrain: epoch  3, batch    11 | loss: 5.4860592des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.953603744506836 0.8693920373916626
CurrentTrain: epoch  3, batch    12 | loss: 4.9536037des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.584843158721924 1.0983784198760986
CurrentTrain: epoch  3, batch    13 | loss: 5.5848432des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.262392997741699 1.0211337804794312
CurrentTrain: epoch  3, batch    14 | loss: 5.2623930des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140032768249512 0.9961339235305786
CurrentTrain: epoch  3, batch    15 | loss: 5.1400328des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.498325824737549 0.9570016860961914
CurrentTrain: epoch  3, batch    16 | loss: 5.4983258des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.739621162414551 0.9503822326660156
CurrentTrain: epoch  3, batch    17 | loss: 5.7396212des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.441994667053223 0.8477289080619812
CurrentTrain: epoch  3, batch    18 | loss: 5.4419947des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.407026767730713 0.9850529432296753
CurrentTrain: epoch  3, batch    19 | loss: 5.4070268des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.29840612411499 1.0056352615356445
CurrentTrain: epoch  3, batch    20 | loss: 5.2984061des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2915849685668945 1.0394574403762817
CurrentTrain: epoch  3, batch    21 | loss: 5.2915850des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.343301296234131 0.9191257953643799
CurrentTrain: epoch  3, batch    22 | loss: 5.3433013des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.226625919342041 0.9959335923194885
CurrentTrain: epoch  3, batch    23 | loss: 5.2266259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.03607702255249 0.6870988607406616
CurrentTrain: epoch  3, batch    24 | loss: 5.0360770des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.109294891357422 1.0195376873016357
CurrentTrain: epoch  3, batch    25 | loss: 5.1092949des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.272613048553467 1.0991891622543335
CurrentTrain: epoch  3, batch    26 | loss: 5.2726130des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.175928115844727 0.9497132301330566
CurrentTrain: epoch  3, batch    27 | loss: 5.1759281des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.34478235244751 1.0801821947097778
CurrentTrain: epoch  3, batch    28 | loss: 5.3447824des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.681675434112549 0.9446146488189697
CurrentTrain: epoch  3, batch    29 | loss: 5.6816754des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.160543441772461 1.1458017826080322
CurrentTrain: epoch  3, batch    30 | loss: 6.1605434des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.885409832000732 0.8732045292854309
CurrentTrain: epoch  3, batch    31 | loss: 4.8854098des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.374021530151367 0.8337904214859009
CurrentTrain: epoch  3, batch    32 | loss: 5.3740215des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.220224857330322 0.829306960105896
CurrentTrain: epoch  3, batch    33 | loss: 5.2202249des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0848708152771 0.9158856868743896
CurrentTrain: epoch  3, batch    34 | loss: 5.0848708des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0665106773376465 0.9240090847015381
CurrentTrain: epoch  3, batch    35 | loss: 5.0665107des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.8889312744140625 1.0663390159606934
CurrentTrain: epoch  3, batch    36 | loss: 5.8889313des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.190898418426514 0.8566985726356506
CurrentTrain: epoch  3, batch    37 | loss: 5.1908984des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959601879119873 0.9252779483795166
CurrentTrain: epoch  3, batch    38 | loss: 4.9596019des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.827635765075684 0.8134810924530029
CurrentTrain: epoch  3, batch    39 | loss: 5.8276358des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140291213989258 0.9769730567932129
CurrentTrain: epoch  3, batch    40 | loss: 5.1402912des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.766822338104248 0.6842474341392517
CurrentTrain: epoch  3, batch    41 | loss: 4.7668223des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.570883750915527 0.8117127418518066
CurrentTrain: epoch  3, batch    42 | loss: 5.5708838des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.099788188934326 0.729293942451477
CurrentTrain: epoch  3, batch    43 | loss: 5.0997882des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.410670280456543 0.9232392311096191
CurrentTrain: epoch  3, batch    44 | loss: 5.4106703des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.390532493591309 0.8209837675094604
CurrentTrain: epoch  3, batch    45 | loss: 5.3905325des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.863620758056641 0.8677620887756348
CurrentTrain: epoch  3, batch    46 | loss: 4.8636208des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.13870906829834 0.7726187705993652
CurrentTrain: epoch  3, batch    47 | loss: 5.1387091des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8672943115234375 0.6362746953964233
CurrentTrain: epoch  3, batch    48 | loss: 4.8672943des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0335259437561035 0.6836175918579102
CurrentTrain: epoch  3, batch    49 | loss: 5.0335259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.008811950683594 0.8716031908988953
CurrentTrain: epoch  3, batch    50 | loss: 5.0088120des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.192744731903076 0.7628194093704224
CurrentTrain: epoch  3, batch    51 | loss: 5.1927447des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.968265533447266 0.8424209952354431
CurrentTrain: epoch  3, batch    52 | loss: 4.9682655des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.672567844390869 0.9232137203216553
CurrentTrain: epoch  3, batch    53 | loss: 5.6725678des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.920748710632324 0.8501842021942139
CurrentTrain: epoch  3, batch    54 | loss: 4.9207487des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.090141296386719 0.957904577255249
CurrentTrain: epoch  3, batch    55 | loss: 5.0901413des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.886087417602539 0.816415548324585
CurrentTrain: epoch  3, batch    56 | loss: 4.8860874des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.327167510986328 0.8300053477287292
CurrentTrain: epoch  3, batch    57 | loss: 5.3271675des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2497477531433105 0.8219953179359436
CurrentTrain: epoch  3, batch    58 | loss: 5.2497478des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.9105000495910645 0.6527562737464905
CurrentTrain: epoch  3, batch    59 | loss: 4.9105000des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.984633445739746 0.9194714426994324
CurrentTrain: epoch  3, batch    60 | loss: 4.9846334des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.813289642333984 0.7433576583862305
CurrentTrain: epoch  3, batch    61 | loss: 4.8132896des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691621780395508 0.5168606042861938
CurrentTrain: epoch  3, batch    62 | loss: 4.6916218des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.693078517913818 0.5873658061027527
CurrentTrain: epoch  4, batch     0 | loss: 4.6930785des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.806028366088867 0.745362401008606
CurrentTrain: epoch  4, batch     1 | loss: 4.8060284des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.904834747314453 0.7988278269767761
CurrentTrain: epoch  4, batch     2 | loss: 4.9048347des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.719254970550537 0.658371090888977
CurrentTrain: epoch  4, batch     3 | loss: 4.7192550des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.959964752197266 0.9529321193695068
CurrentTrain: epoch  4, batch     4 | loss: 4.9599648des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.135072708129883 0.9262544512748718
CurrentTrain: epoch  4, batch     5 | loss: 5.1350727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.84869909286499 0.7296959161758423
CurrentTrain: epoch  4, batch     6 | loss: 4.8486991des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.733144760131836 0.6090331077575684
CurrentTrain: epoch  4, batch     7 | loss: 4.7331448des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.841223239898682 0.8645848035812378
CurrentTrain: epoch  4, batch     8 | loss: 4.8412232des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7772746086120605 0.7030757069587708
CurrentTrain: epoch  4, batch     9 | loss: 4.7772746des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.814172744750977 0.7269054651260376
CurrentTrain: epoch  4, batch    10 | loss: 4.8141727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.166572570800781 0.7654482126235962
CurrentTrain: epoch  4, batch    11 | loss: 5.1665726des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.869943618774414 0.7695276141166687
CurrentTrain: epoch  4, batch    12 | loss: 4.8699436des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.181846618652344 0.8651489615440369
CurrentTrain: epoch  4, batch    13 | loss: 5.1818466des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.493844985961914 0.5341933965682983
CurrentTrain: epoch  4, batch    14 | loss: 4.4938450des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.749185562133789 0.6812607049942017
CurrentTrain: epoch  4, batch    15 | loss: 4.7491856des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.824125289916992 0.7251666784286499
CurrentTrain: epoch  4, batch    16 | loss: 4.8241253des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.799384593963623 0.6490915417671204
CurrentTrain: epoch  4, batch    17 | loss: 4.7993846des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.801788330078125 0.8030322790145874
CurrentTrain: epoch  4, batch    18 | loss: 4.8017883des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.796889781951904 0.7754301428794861
CurrentTrain: epoch  4, batch    19 | loss: 4.7968898des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.153695106506348 0.6834776997566223
CurrentTrain: epoch  4, batch    20 | loss: 5.1536951des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.915266990661621 0.5711497664451599
CurrentTrain: epoch  4, batch    21 | loss: 4.9152670des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.570249557495117 0.6395601630210876
CurrentTrain: epoch  4, batch    22 | loss: 4.5702496des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.478219032287598 0.44560980796813965
CurrentTrain: epoch  4, batch    23 | loss: 4.4782190des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.817839622497559 0.5910068154335022
CurrentTrain: epoch  4, batch    24 | loss: 4.8178396des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.768563270568848 0.7635782957077026
CurrentTrain: epoch  4, batch    25 | loss: 4.7685633des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.609344959259033 0.6901866793632507
CurrentTrain: epoch  4, batch    26 | loss: 4.6093450des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.637467384338379 0.6230311989784241
CurrentTrain: epoch  4, batch    27 | loss: 4.6374674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.767701625823975 0.6288499236106873
CurrentTrain: epoch  4, batch    28 | loss: 4.7677016des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.700007915496826 0.6359813809394836
CurrentTrain: epoch  4, batch    29 | loss: 4.7000079des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.588162422180176 0.6554694175720215
CurrentTrain: epoch  4, batch    30 | loss: 4.5881624des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.732369899749756 0.6765909194946289
CurrentTrain: epoch  4, batch    31 | loss: 4.7323699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.666533470153809 0.6684315204620361
CurrentTrain: epoch  4, batch    32 | loss: 4.6665335des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7807159423828125 0.7347530126571655
CurrentTrain: epoch  4, batch    33 | loss: 4.7807159des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6732892990112305 0.6531981229782104
CurrentTrain: epoch  4, batch    34 | loss: 4.6732893des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.888833045959473 0.7925335764884949
CurrentTrain: epoch  4, batch    35 | loss: 4.8888330des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.699143409729004 0.6564288139343262
CurrentTrain: epoch  4, batch    36 | loss: 4.6991434des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.110682964324951 0.7736770510673523
CurrentTrain: epoch  4, batch    37 | loss: 5.1106830des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704012393951416 0.6345605254173279
CurrentTrain: epoch  4, batch    38 | loss: 4.7040124des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.62897253036499 0.6374919414520264
CurrentTrain: epoch  4, batch    39 | loss: 4.6289725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.552116870880127 0.6075010299682617
CurrentTrain: epoch  4, batch    40 | loss: 4.5521169des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.595989227294922 0.6946349143981934
CurrentTrain: epoch  4, batch    41 | loss: 4.5959892des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.858814239501953 0.7200260162353516
CurrentTrain: epoch  4, batch    42 | loss: 4.8588142des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.929131984710693 0.5336486101150513
CurrentTrain: epoch  4, batch    43 | loss: 4.9291320des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.455041885375977 0.5244512557983398
CurrentTrain: epoch  4, batch    44 | loss: 4.4550419des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.24995231628418 0.6656460762023926
CurrentTrain: epoch  4, batch    45 | loss: 5.2499523des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.745295524597168 0.7060742378234863
CurrentTrain: epoch  4, batch    46 | loss: 4.7452955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.660812854766846 0.6329832673072815
CurrentTrain: epoch  4, batch    47 | loss: 4.6608129des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.638529300689697 0.6848247051239014
CurrentTrain: epoch  4, batch    48 | loss: 4.6385293des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.644412040710449 0.6397827863693237
CurrentTrain: epoch  4, batch    49 | loss: 4.6444120des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.774701118469238 0.8059102892875671
CurrentTrain: epoch  4, batch    50 | loss: 4.7747011des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.615450859069824 0.5835493206977844
CurrentTrain: epoch  4, batch    51 | loss: 4.6154509des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.524040222167969 0.5134026408195496
CurrentTrain: epoch  4, batch    52 | loss: 4.5240402des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8538665771484375 0.6754355430603027
CurrentTrain: epoch  4, batch    53 | loss: 4.8538666des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.653186321258545 0.5864333510398865
CurrentTrain: epoch  4, batch    54 | loss: 4.6531863des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.649025917053223 0.6997280120849609
CurrentTrain: epoch  4, batch    55 | loss: 4.6490259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.565957546234131 0.5201479196548462
CurrentTrain: epoch  4, batch    56 | loss: 4.5659575des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.691095352172852 0.7682797908782959
CurrentTrain: epoch  4, batch    57 | loss: 4.6910954des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590443134307861 0.582393229007721
CurrentTrain: epoch  4, batch    58 | loss: 4.5904431des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.584754467010498 0.6168481111526489
CurrentTrain: epoch  4, batch    59 | loss: 4.5847545des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.592937469482422 0.6070008277893066
CurrentTrain: epoch  4, batch    60 | loss: 4.5929375des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704367637634277 0.6315898895263672
CurrentTrain: epoch  4, batch    61 | loss: 4.7043676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.513129234313965 0.45638883113861084
CurrentTrain: epoch  4, batch    62 | loss: 4.5131292des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.75014066696167 0.7058000564575195
CurrentTrain: epoch  5, batch     0 | loss: 4.7501407des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.630569934844971 0.6423497796058655
CurrentTrain: epoch  5, batch     1 | loss: 4.6305699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.550356388092041 0.5766547322273254
CurrentTrain: epoch  5, batch     2 | loss: 4.5503564des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.533287048339844 0.5767020583152771
CurrentTrain: epoch  5, batch     3 | loss: 4.5332870des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.652379035949707 0.6052058339118958
CurrentTrain: epoch  5, batch     4 | loss: 4.6523790des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.605034351348877 0.5880939960479736
CurrentTrain: epoch  5, batch     5 | loss: 4.6050344des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.602302074432373 0.6390889883041382
CurrentTrain: epoch  5, batch     6 | loss: 4.6023021des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.482685089111328 0.6640385389328003
CurrentTrain: epoch  5, batch     7 | loss: 4.4826851des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.64866304397583 0.6475061774253845
CurrentTrain: epoch  5, batch     8 | loss: 4.6486630des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.44582986831665 0.4502357840538025
CurrentTrain: epoch  5, batch     9 | loss: 4.4458299des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.843441009521484 0.6541036367416382
CurrentTrain: epoch  5, batch    10 | loss: 4.8434410des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.590672492980957 0.6501652002334595
CurrentTrain: epoch  5, batch    11 | loss: 4.5906725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.445680141448975 0.5148342847824097
CurrentTrain: epoch  5, batch    12 | loss: 4.4456801des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.491142749786377 0.5643436908721924
CurrentTrain: epoch  5, batch    13 | loss: 4.4911427des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.483101844787598 0.6623929142951965
CurrentTrain: epoch  5, batch    14 | loss: 4.4831018des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.50734806060791 0.5970326066017151
CurrentTrain: epoch  5, batch    15 | loss: 4.5073481des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.620008945465088 0.6581331491470337
CurrentTrain: epoch  5, batch    16 | loss: 4.6200089des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485630035400391 0.6099047660827637
CurrentTrain: epoch  5, batch    17 | loss: 4.4856300des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.618782043457031 0.6864808797836304
CurrentTrain: epoch  5, batch    18 | loss: 4.6187820des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.487261772155762 0.547090470790863
CurrentTrain: epoch  5, batch    19 | loss: 4.4872618des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416949272155762 0.5254210829734802
CurrentTrain: epoch  5, batch    20 | loss: 4.4169493des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.467197418212891 0.5430623888969421
CurrentTrain: epoch  5, batch    21 | loss: 4.4671974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.485051155090332 0.4818846583366394
CurrentTrain: epoch  5, batch    22 | loss: 4.4850512des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.551313877105713 0.5901062488555908
CurrentTrain: epoch  5, batch    23 | loss: 4.5513139des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.453506946563721 0.498520165681839
CurrentTrain: epoch  5, batch    24 | loss: 4.4535069des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463562488555908 0.5646237134933472
CurrentTrain: epoch  5, batch    25 | loss: 4.4635625des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5455098152160645 0.5900653600692749
CurrentTrain: epoch  5, batch    26 | loss: 4.5455098des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4906439781188965 0.5196026563644409
CurrentTrain: epoch  5, batch    27 | loss: 4.4906440des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463355541229248 0.6276993155479431
CurrentTrain: epoch  5, batch    28 | loss: 4.4633555des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466972351074219 0.6047004461288452
CurrentTrain: epoch  5, batch    29 | loss: 4.4669724des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400462627410889 0.4878422021865845
CurrentTrain: epoch  5, batch    30 | loss: 4.4004626des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.408786296844482 0.5057677030563354
CurrentTrain: epoch  5, batch    31 | loss: 4.4087863des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.466117858886719 0.48728930950164795
CurrentTrain: epoch  5, batch    32 | loss: 4.4661179des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.321932792663574 0.44563430547714233
CurrentTrain: epoch  5, batch    33 | loss: 4.3219328des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.451994895935059 0.4971400499343872
CurrentTrain: epoch  5, batch    34 | loss: 4.4519949des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.360955715179443 0.4419785439968109
CurrentTrain: epoch  5, batch    35 | loss: 4.3609557des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3710784912109375 0.430392861366272
CurrentTrain: epoch  5, batch    36 | loss: 4.3710785des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.347805976867676 0.4565986096858978
CurrentTrain: epoch  5, batch    37 | loss: 4.3478060des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.444753646850586 0.5379966497421265
CurrentTrain: epoch  5, batch    38 | loss: 4.4447536des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.534544944763184 0.5813654661178589
CurrentTrain: epoch  5, batch    39 | loss: 4.5345449des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.415805339813232 0.6031494140625
CurrentTrain: epoch  5, batch    40 | loss: 4.4158053des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.398653507232666 0.4548051953315735
CurrentTrain: epoch  5, batch    41 | loss: 4.3986535des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.596743106842041 0.6089962124824524
CurrentTrain: epoch  5, batch    42 | loss: 4.5967431des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.757709503173828 0.6084247827529907
CurrentTrain: epoch  5, batch    43 | loss: 4.7577095des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.414988040924072 0.5211746692657471
CurrentTrain: epoch  5, batch    44 | loss: 4.4149880des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.372142314910889 0.4713820517063141
CurrentTrain: epoch  5, batch    45 | loss: 4.3721423des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.401346683502197 0.5249283313751221
CurrentTrain: epoch  5, batch    46 | loss: 4.4013467des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3771162033081055 0.5075411796569824
CurrentTrain: epoch  5, batch    47 | loss: 4.3771162des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.41010856628418 0.5496524572372437
CurrentTrain: epoch  5, batch    48 | loss: 4.4101086des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416181564331055 0.49380460381507874
CurrentTrain: epoch  5, batch    49 | loss: 4.4161816des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.341184139251709 0.46353790163993835
CurrentTrain: epoch  5, batch    50 | loss: 4.3411841des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.352419853210449 0.4754869043827057
CurrentTrain: epoch  5, batch    51 | loss: 4.3524199des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.464930057525635 0.5858806371688843
CurrentTrain: epoch  5, batch    52 | loss: 4.4649301des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.403540134429932 0.4749881625175476
CurrentTrain: epoch  5, batch    53 | loss: 4.4035401des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.404925346374512 0.6019455194473267
CurrentTrain: epoch  5, batch    54 | loss: 4.4049253des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.451889514923096 0.5230963230133057
CurrentTrain: epoch  5, batch    55 | loss: 4.4518895des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.415914535522461 0.5216556787490845
CurrentTrain: epoch  5, batch    56 | loss: 4.4159145des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.686596870422363 0.47879573702812195
CurrentTrain: epoch  5, batch    57 | loss: 4.6865969des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.452544212341309 0.45203208923339844
CurrentTrain: epoch  5, batch    58 | loss: 4.4525442des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.405111789703369 0.44897571206092834
CurrentTrain: epoch  5, batch    59 | loss: 4.4051118des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.330862998962402 0.4351142644882202
CurrentTrain: epoch  5, batch    60 | loss: 4.3308630des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.302736759185791 0.395114928483963
CurrentTrain: epoch  5, batch    61 | loss: 4.3027368des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400263786315918 0.3992667496204376
CurrentTrain: epoch  5, batch    62 | loss: 4.4002638des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.452217102050781 0.5105688571929932
CurrentTrain: epoch  6, batch     0 | loss: 4.4522171des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.356934547424316 0.497220903635025
CurrentTrain: epoch  6, batch     1 | loss: 4.3569345des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.281763076782227 0.42392635345458984
CurrentTrain: epoch  6, batch     2 | loss: 4.2817631des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.494631290435791 0.6103181838989258
CurrentTrain: epoch  6, batch     3 | loss: 4.4946313des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2891435623168945 0.31219834089279175
CurrentTrain: epoch  6, batch     4 | loss: 4.2891436des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.386523246765137 0.4889189600944519
CurrentTrain: epoch  6, batch     5 | loss: 4.3865232des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400078296661377 0.5312261581420898
CurrentTrain: epoch  6, batch     6 | loss: 4.4000783des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.355546951293945 0.42998412251472473
CurrentTrain: epoch  6, batch     7 | loss: 4.3555470des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.330164909362793 0.4739121198654175
CurrentTrain: epoch  6, batch     8 | loss: 4.3301649des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.312422275543213 0.36172500252723694
CurrentTrain: epoch  6, batch     9 | loss: 4.3124223des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3818135261535645 0.49301594495773315
CurrentTrain: epoch  6, batch    10 | loss: 4.3818135des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.329300880432129 0.34589827060699463
CurrentTrain: epoch  6, batch    11 | loss: 4.3293009des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.455676555633545 0.5337263345718384
CurrentTrain: epoch  6, batch    12 | loss: 4.4556766des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.301489353179932 0.3882291615009308
CurrentTrain: epoch  6, batch    13 | loss: 4.3014894des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.384359836578369 0.47922396659851074
CurrentTrain: epoch  6, batch    14 | loss: 4.3843598des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3795366287231445 0.4944596588611603
CurrentTrain: epoch  6, batch    15 | loss: 4.3795366des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.354011058807373 0.4598112106323242
CurrentTrain: epoch  6, batch    16 | loss: 4.3540111des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.373839855194092 0.5130545496940613
CurrentTrain: epoch  6, batch    17 | loss: 4.3738399des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.277557849884033 0.2618781328201294
CurrentTrain: epoch  6, batch    18 | loss: 4.2775578des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.333679676055908 0.48399239778518677
CurrentTrain: epoch  6, batch    19 | loss: 4.3336797des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.357703685760498 0.5185374617576599
CurrentTrain: epoch  6, batch    20 | loss: 4.3577037des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.419430732727051 0.4797729253768921
CurrentTrain: epoch  6, batch    21 | loss: 4.4194307des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.346167087554932 0.37624454498291016
CurrentTrain: epoch  6, batch    22 | loss: 4.3461671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.612098693847656 0.3525996804237366
CurrentTrain: epoch  6, batch    23 | loss: 4.6120987des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.353480339050293 0.41711264848709106
CurrentTrain: epoch  6, batch    24 | loss: 4.3534803des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.418686866760254 0.5124208927154541
CurrentTrain: epoch  6, batch    25 | loss: 4.4186869des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.366223335266113 0.4531979560852051
CurrentTrain: epoch  6, batch    26 | loss: 4.3662233des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.292842388153076 0.3975737988948822
CurrentTrain: epoch  6, batch    27 | loss: 4.2928424des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.292102813720703 0.4431830644607544
CurrentTrain: epoch  6, batch    28 | loss: 4.2921028des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3362836837768555 0.43276381492614746
CurrentTrain: epoch  6, batch    29 | loss: 4.3362837des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.311416149139404 0.47432219982147217
CurrentTrain: epoch  6, batch    30 | loss: 4.3114161des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.177957534790039 0.2921806573867798
CurrentTrain: epoch  6, batch    31 | loss: 4.1779575des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.334142684936523 0.373573362827301
CurrentTrain: epoch  6, batch    32 | loss: 4.3341427des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.33186149597168 0.4257867932319641
CurrentTrain: epoch  6, batch    33 | loss: 4.3318615des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.315456390380859 0.3866997957229614
CurrentTrain: epoch  6, batch    34 | loss: 4.3154564des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.274567127227783 0.323820024728775
CurrentTrain: epoch  6, batch    35 | loss: 4.2745671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.281773567199707 0.25927144289016724
CurrentTrain: epoch  6, batch    36 | loss: 4.2817736des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.368920803070068 0.48703426122665405
CurrentTrain: epoch  6, batch    37 | loss: 4.3689208des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.392637252807617 0.5320020914077759
CurrentTrain: epoch  6, batch    38 | loss: 4.3926373des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.227689266204834 0.3340810537338257
CurrentTrain: epoch  6, batch    39 | loss: 4.2276893des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.359152317047119 0.4610876441001892
CurrentTrain: epoch  6, batch    40 | loss: 4.3591523des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.358684062957764 0.5071914196014404
CurrentTrain: epoch  6, batch    41 | loss: 4.3586841des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.294145584106445 0.4679703712463379
CurrentTrain: epoch  6, batch    42 | loss: 4.2941456des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.20233154296875 0.3385872542858124
CurrentTrain: epoch  6, batch    43 | loss: 4.2023315des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.271508693695068 0.4116564989089966
CurrentTrain: epoch  6, batch    44 | loss: 4.2715087des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.341394901275635 0.45743465423583984
CurrentTrain: epoch  6, batch    45 | loss: 4.3413949des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2780256271362305 0.38673436641693115
CurrentTrain: epoch  6, batch    46 | loss: 4.2780256des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.282653331756592 0.37756121158599854
CurrentTrain: epoch  6, batch    47 | loss: 4.2826533des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.324879169464111 0.49529704451560974
CurrentTrain: epoch  6, batch    48 | loss: 4.3248792des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.241979598999023 0.33546292781829834
CurrentTrain: epoch  6, batch    49 | loss: 4.2419796des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.334894180297852 0.45219850540161133
CurrentTrain: epoch  6, batch    50 | loss: 4.3348942des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.292205810546875 0.4472525715827942
CurrentTrain: epoch  6, batch    51 | loss: 4.2922058des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.305724143981934 0.4708091616630554
CurrentTrain: epoch  6, batch    52 | loss: 4.3057241des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.219054222106934 0.2679198086261749
CurrentTrain: epoch  6, batch    53 | loss: 4.2190542des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.269779205322266 0.4060327112674713
CurrentTrain: epoch  6, batch    54 | loss: 4.2697792des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.217987537384033 0.3343498706817627
CurrentTrain: epoch  6, batch    55 | loss: 4.2179875des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.283379554748535 0.3129618465900421
CurrentTrain: epoch  6, batch    56 | loss: 4.2833796des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.260377407073975 0.4298143982887268
CurrentTrain: epoch  6, batch    57 | loss: 4.2603774des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.281445503234863 0.4235180616378784
CurrentTrain: epoch  6, batch    58 | loss: 4.2814455des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.279642105102539 0.385631263256073
CurrentTrain: epoch  6, batch    59 | loss: 4.2796421des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.28358793258667 0.318652480840683
CurrentTrain: epoch  6, batch    60 | loss: 4.2835879des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2817182540893555 0.3780173063278198
CurrentTrain: epoch  6, batch    61 | loss: 4.2817183des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.196983337402344 0.31796854734420776
CurrentTrain: epoch  6, batch    62 | loss: 4.1969833des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.252897262573242 0.316505491733551
CurrentTrain: epoch  7, batch     0 | loss: 4.2528973des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246333122253418 0.3238327205181122
CurrentTrain: epoch  7, batch     1 | loss: 4.2463331des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.259164810180664 0.3619503974914551
CurrentTrain: epoch  7, batch     2 | loss: 4.2591648des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.275287628173828 0.4440391957759857
CurrentTrain: epoch  7, batch     3 | loss: 4.2752876des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2406229972839355 0.33642446994781494
CurrentTrain: epoch  7, batch     4 | loss: 4.2406230des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.24222993850708 0.40645498037338257
CurrentTrain: epoch  7, batch     5 | loss: 4.2422299des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.264197826385498 0.4165000319480896
CurrentTrain: epoch  7, batch     6 | loss: 4.2641978des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.359829425811768 0.43838927149772644
CurrentTrain: epoch  7, batch     7 | loss: 4.3598294des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.14230489730835 0.28905946016311646
CurrentTrain: epoch  7, batch     8 | loss: 4.1423049des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.205665111541748 0.3384940028190613
CurrentTrain: epoch  7, batch     9 | loss: 4.2056651des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.22521448135376 0.35811319947242737
CurrentTrain: epoch  7, batch    10 | loss: 4.2252145des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.167191505432129 0.31690070033073425
CurrentTrain: epoch  7, batch    11 | loss: 4.1671915des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.165436744689941 0.2762293219566345
CurrentTrain: epoch  7, batch    12 | loss: 4.1654367des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.227357864379883 0.3250086307525635
CurrentTrain: epoch  7, batch    13 | loss: 4.2273579des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1824517250061035 0.32582587003707886
CurrentTrain: epoch  7, batch    14 | loss: 4.1824517des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.17197847366333 0.3512197434902191
CurrentTrain: epoch  7, batch    15 | loss: 4.1719785des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2500739097595215 0.34535282850265503
CurrentTrain: epoch  7, batch    16 | loss: 4.2500739des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.509793758392334 0.37265706062316895
CurrentTrain: epoch  7, batch    17 | loss: 4.5097938des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2413129806518555 0.390120267868042
CurrentTrain: epoch  7, batch    18 | loss: 4.2413130des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.294180870056152 0.3896540105342865
CurrentTrain: epoch  7, batch    19 | loss: 4.2941809des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246091365814209 0.32154205441474915
CurrentTrain: epoch  7, batch    20 | loss: 4.2460914des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19834566116333 0.35974809527397156
CurrentTrain: epoch  7, batch    21 | loss: 4.1983457des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1971964836120605 0.30598968267440796
CurrentTrain: epoch  7, batch    22 | loss: 4.1971965des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.215190410614014 0.31430333852767944
CurrentTrain: epoch  7, batch    23 | loss: 4.2151904des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.207679271697998 0.3303612768650055
CurrentTrain: epoch  7, batch    24 | loss: 4.2076793des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.325757026672363 0.3304438292980194
CurrentTrain: epoch  7, batch    25 | loss: 4.3257570des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.17935037612915 0.3279487192630768
CurrentTrain: epoch  7, batch    26 | loss: 4.1793504des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.225945949554443 0.36092662811279297
CurrentTrain: epoch  7, batch    27 | loss: 4.2259459des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.199648380279541 0.3101183772087097
CurrentTrain: epoch  7, batch    28 | loss: 4.1996484des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.216858386993408 0.29364487528800964
CurrentTrain: epoch  7, batch    29 | loss: 4.2168584des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.256723880767822 0.33833053708076477
CurrentTrain: epoch  7, batch    30 | loss: 4.2567239des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.248193264007568 0.35214343667030334
CurrentTrain: epoch  7, batch    31 | loss: 4.2481933des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19377326965332 0.2399899661540985
CurrentTrain: epoch  7, batch    32 | loss: 4.1937733des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3224406242370605 0.3270238935947418
CurrentTrain: epoch  7, batch    33 | loss: 4.3224406des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246668815612793 0.26372307538986206
CurrentTrain: epoch  7, batch    34 | loss: 4.2466688des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.233978748321533 0.38832563161849976
CurrentTrain: epoch  7, batch    35 | loss: 4.2339787des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.289597988128662 0.3497764766216278
CurrentTrain: epoch  7, batch    36 | loss: 4.2895980des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1602396965026855 0.27588456869125366
CurrentTrain: epoch  7, batch    37 | loss: 4.1602397des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.235011100769043 0.37469401955604553
CurrentTrain: epoch  7, batch    38 | loss: 4.2350111des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.158596515655518 0.30866166949272156
CurrentTrain: epoch  7, batch    39 | loss: 4.1585965des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.236466884613037 0.3877008557319641
CurrentTrain: epoch  7, batch    40 | loss: 4.2364669des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.145831108093262 0.27882009744644165
CurrentTrain: epoch  7, batch    41 | loss: 4.1458311des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.20039176940918 0.347456157207489
CurrentTrain: epoch  7, batch    42 | loss: 4.2003918des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.185506820678711 0.303408145904541
CurrentTrain: epoch  7, batch    43 | loss: 4.1855068des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.207218170166016 0.33541128039360046
CurrentTrain: epoch  7, batch    44 | loss: 4.2072182des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.155477046966553 0.3411359488964081
CurrentTrain: epoch  7, batch    45 | loss: 4.1554770des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2056050300598145 0.3302389979362488
CurrentTrain: epoch  7, batch    46 | loss: 4.2056050des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.205203533172607 0.3407249450683594
CurrentTrain: epoch  7, batch    47 | loss: 4.2052035des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.187941551208496 0.3431280851364136
CurrentTrain: epoch  7, batch    48 | loss: 4.1879416des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.202864170074463 0.3687150180339813
CurrentTrain: epoch  7, batch    49 | loss: 4.2028642des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.109827518463135 0.3003075420856476
CurrentTrain: epoch  7, batch    50 | loss: 4.1098275des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2102766036987305 0.378803014755249
CurrentTrain: epoch  7, batch    51 | loss: 4.2102766des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.222535610198975 0.25702571868896484
CurrentTrain: epoch  7, batch    52 | loss: 4.2225356des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.229180335998535 0.3258896470069885
CurrentTrain: epoch  7, batch    53 | loss: 4.2291803des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.226609706878662 0.33659225702285767
CurrentTrain: epoch  7, batch    54 | loss: 4.2266097des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.176504611968994 0.28695422410964966
CurrentTrain: epoch  7, batch    55 | loss: 4.1765046des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.190647602081299 0.25801384449005127
CurrentTrain: epoch  7, batch    56 | loss: 4.1906476des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.175176620483398 0.3226467967033386
CurrentTrain: epoch  7, batch    57 | loss: 4.1751766des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1723432540893555 0.35761263966560364
CurrentTrain: epoch  7, batch    58 | loss: 4.1723433des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.153531551361084 0.22425144910812378
CurrentTrain: epoch  7, batch    59 | loss: 4.1535316des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.248038291931152 0.36851510405540466
CurrentTrain: epoch  7, batch    60 | loss: 4.2480383des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.216278553009033 0.324366956949234
CurrentTrain: epoch  7, batch    61 | loss: 4.2162786des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.176731586456299 0.20586255192756653
CurrentTrain: epoch  7, batch    62 | loss: 4.1767316des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.225344181060791 0.38492482900619507
CurrentTrain: epoch  8, batch     0 | loss: 4.2253442des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.236451625823975 0.34178102016448975
CurrentTrain: epoch  8, batch     1 | loss: 4.2364516des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.199397087097168 0.3280406892299652
CurrentTrain: epoch  8, batch     2 | loss: 4.1993971des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.177271842956543 0.25733262300491333
CurrentTrain: epoch  8, batch     3 | loss: 4.1772718des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1230363845825195 0.24756401777267456
CurrentTrain: epoch  8, batch     4 | loss: 4.1230364des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.187826156616211 0.3183479309082031
CurrentTrain: epoch  8, batch     5 | loss: 4.1878262des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1920037269592285 0.3363102078437805
CurrentTrain: epoch  8, batch     6 | loss: 4.1920037des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.223687648773193 0.329690545797348
CurrentTrain: epoch  8, batch     7 | loss: 4.2236876des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2094807624816895 0.3467128574848175
CurrentTrain: epoch  8, batch     8 | loss: 4.2094808des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.208545684814453 0.34774160385131836
CurrentTrain: epoch  8, batch     9 | loss: 4.2085457des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.15585994720459 0.3246331810951233
CurrentTrain: epoch  8, batch    10 | loss: 4.1558599des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.193225860595703 0.3549947738647461
CurrentTrain: epoch  8, batch    11 | loss: 4.1932259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.218559265136719 0.3401065468788147
CurrentTrain: epoch  8, batch    12 | loss: 4.2185593des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.186861991882324 0.30926233530044556
CurrentTrain: epoch  8, batch    13 | loss: 4.1868620des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.163557529449463 0.3257453143596649
CurrentTrain: epoch  8, batch    14 | loss: 4.1635575des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.136320114135742 0.25213301181793213
CurrentTrain: epoch  8, batch    15 | loss: 4.1363201des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.13822078704834 0.28556036949157715
CurrentTrain: epoch  8, batch    16 | loss: 4.1382208des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.179938316345215 0.3108378052711487
CurrentTrain: epoch  8, batch    17 | loss: 4.1799383des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.190016269683838 0.31037217378616333
CurrentTrain: epoch  8, batch    18 | loss: 4.1900163des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.224770545959473 0.315573126077652
CurrentTrain: epoch  8, batch    19 | loss: 4.2247705des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.083163261413574 0.27784788608551025
CurrentTrain: epoch  8, batch    20 | loss: 4.0831633des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1702165603637695 0.3123280107975006
CurrentTrain: epoch  8, batch    21 | loss: 4.1702166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.11751127243042 0.22672362625598907
CurrentTrain: epoch  8, batch    22 | loss: 4.1175113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.203805923461914 0.3617745339870453
CurrentTrain: epoch  8, batch    23 | loss: 4.2038059des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.22750186920166 0.37182825803756714
CurrentTrain: epoch  8, batch    24 | loss: 4.2275019des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.121248722076416 0.20305556058883667
CurrentTrain: epoch  8, batch    25 | loss: 4.1212487des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.169403076171875 0.2590292692184448
CurrentTrain: epoch  8, batch    26 | loss: 4.1694031des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.213259220123291 0.2796940207481384
CurrentTrain: epoch  8, batch    27 | loss: 4.2132592des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.189671993255615 0.3567882776260376
CurrentTrain: epoch  8, batch    28 | loss: 4.1896720des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.185299396514893 0.30472978949546814
CurrentTrain: epoch  8, batch    29 | loss: 4.1852994des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.166034698486328 0.3179396688938141
CurrentTrain: epoch  8, batch    30 | loss: 4.1660347des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.140401840209961 0.26914694905281067
CurrentTrain: epoch  8, batch    31 | loss: 4.1404018des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.149331092834473 0.20857171714305878
CurrentTrain: epoch  8, batch    32 | loss: 4.1493311des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.203737258911133 0.33381563425064087
CurrentTrain: epoch  8, batch    33 | loss: 4.2037373des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.186506271362305 0.3361527919769287
CurrentTrain: epoch  8, batch    34 | loss: 4.1865063des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1442060470581055 0.27638986706733704
CurrentTrain: epoch  8, batch    35 | loss: 4.1442060des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.172611713409424 0.27308836579322815
CurrentTrain: epoch  8, batch    36 | loss: 4.1726117des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.192976951599121 0.2657942771911621
CurrentTrain: epoch  8, batch    37 | loss: 4.1929770des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.096319198608398 0.2505466341972351
CurrentTrain: epoch  8, batch    38 | loss: 4.0963192des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.196532726287842 0.27052566409111023
CurrentTrain: epoch  8, batch    39 | loss: 4.1965327des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.251791477203369 0.3320668339729309
CurrentTrain: epoch  8, batch    40 | loss: 4.2517915des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19249963760376 0.31342974305152893
CurrentTrain: epoch  8, batch    41 | loss: 4.1924996des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.189453601837158 0.29852429032325745
CurrentTrain: epoch  8, batch    42 | loss: 4.1894536des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.218112468719482 0.3177785873413086
CurrentTrain: epoch  8, batch    43 | loss: 4.2181125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1876678466796875 0.3161643445491791
CurrentTrain: epoch  8, batch    44 | loss: 4.1876678des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19385290145874 0.250881552696228
CurrentTrain: epoch  8, batch    45 | loss: 4.1938529des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.090933322906494 0.23060888051986694
CurrentTrain: epoch  8, batch    46 | loss: 4.0909333des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.129155158996582 0.2086334377527237
CurrentTrain: epoch  8, batch    47 | loss: 4.1291552des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.145321369171143 0.2138875126838684
CurrentTrain: epoch  8, batch    48 | loss: 4.1453214des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.141169548034668 0.2532481551170349
CurrentTrain: epoch  8, batch    49 | loss: 4.1411695des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.149672031402588 0.29048293828964233
CurrentTrain: epoch  8, batch    50 | loss: 4.1496720des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1235032081604 0.19962438941001892
CurrentTrain: epoch  8, batch    51 | loss: 4.1235032des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1956305503845215 0.33023568987846375
CurrentTrain: epoch  8, batch    52 | loss: 4.1956306des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.186799049377441 0.29429149627685547
CurrentTrain: epoch  8, batch    53 | loss: 4.1867990des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.151617050170898 0.29970312118530273
CurrentTrain: epoch  8, batch    54 | loss: 4.1516171des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.104642391204834 0.2222834825515747
CurrentTrain: epoch  8, batch    55 | loss: 4.1046424des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.170926094055176 0.29600510001182556
CurrentTrain: epoch  8, batch    56 | loss: 4.1709261des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1818108558654785 0.3436344861984253
CurrentTrain: epoch  8, batch    57 | loss: 4.1818109des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.148151874542236 0.22517366707324982
CurrentTrain: epoch  8, batch    58 | loss: 4.1481519des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.15537166595459 0.2718844413757324
CurrentTrain: epoch  8, batch    59 | loss: 4.1553717des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1118597984313965 0.2817324995994568
CurrentTrain: epoch  8, batch    60 | loss: 4.1118598des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.171239376068115 0.3023800253868103
CurrentTrain: epoch  8, batch    61 | loss: 4.1712394des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.066489219665527 0.1156611442565918
CurrentTrain: epoch  8, batch    62 | loss: 4.0664892des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.133330345153809 0.26279783248901367
CurrentTrain: epoch  9, batch     0 | loss: 4.1333303des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.181262969970703 0.2264842540025711
CurrentTrain: epoch  9, batch     1 | loss: 4.1812630des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.184458255767822 0.3051076829433441
CurrentTrain: epoch  9, batch     2 | loss: 4.1844583des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.176738739013672 0.254371702671051
CurrentTrain: epoch  9, batch     3 | loss: 4.1767387des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.119837760925293 0.2248719334602356
CurrentTrain: epoch  9, batch     4 | loss: 4.1198378des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1380615234375 0.2783602774143219
CurrentTrain: epoch  9, batch     5 | loss: 4.1380615des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.140147686004639 0.23795515298843384
CurrentTrain: epoch  9, batch     6 | loss: 4.1401477des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.141358375549316 0.2484552562236786
CurrentTrain: epoch  9, batch     7 | loss: 4.1413584des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.0823259353637695 0.209811732172966
CurrentTrain: epoch  9, batch     8 | loss: 4.0823259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.158176898956299 0.2843034863471985
CurrentTrain: epoch  9, batch     9 | loss: 4.1581769des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.150421142578125 0.3081657290458679
CurrentTrain: epoch  9, batch    10 | loss: 4.1504211des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1692891120910645 0.2871791124343872
CurrentTrain: epoch  9, batch    11 | loss: 4.1692891des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.158632755279541 0.281561940908432
CurrentTrain: epoch  9, batch    12 | loss: 4.1586328des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.149319171905518 0.28158122301101685
CurrentTrain: epoch  9, batch    13 | loss: 4.1493192des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.175187110900879 0.3453812599182129
CurrentTrain: epoch  9, batch    14 | loss: 4.1751871des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1961164474487305 0.314262330532074
CurrentTrain: epoch  9, batch    15 | loss: 4.1961164des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.151007175445557 0.2946386933326721
CurrentTrain: epoch  9, batch    16 | loss: 4.1510072des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.193670749664307 0.30036216974258423
CurrentTrain: epoch  9, batch    17 | loss: 4.1936707des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1567792892456055 0.29037904739379883
CurrentTrain: epoch  9, batch    18 | loss: 4.1567793des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.07368803024292 0.23148110508918762
CurrentTrain: epoch  9, batch    19 | loss: 4.0736880des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1799468994140625 0.2681046426296234
CurrentTrain: epoch  9, batch    20 | loss: 4.1799469des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.125595569610596 0.2396591454744339
CurrentTrain: epoch  9, batch    21 | loss: 4.1255956des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.198752403259277 0.28610578179359436
CurrentTrain: epoch  9, batch    22 | loss: 4.1987524des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.139084339141846 0.28005626797676086
CurrentTrain: epoch  9, batch    23 | loss: 4.1390843des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.113168239593506 0.21578598022460938
CurrentTrain: epoch  9, batch    24 | loss: 4.1131682des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1490278244018555 0.2509807050228119
CurrentTrain: epoch  9, batch    25 | loss: 4.1490278des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.112514019012451 0.2674124240875244
CurrentTrain: epoch  9, batch    26 | loss: 4.1125140des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.133876800537109 0.2685071527957916
CurrentTrain: epoch  9, batch    27 | loss: 4.1338768des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.140974044799805 0.3045913577079773
CurrentTrain: epoch  9, batch    28 | loss: 4.1409740des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.126418590545654 0.2568705677986145
CurrentTrain: epoch  9, batch    29 | loss: 4.1264186des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.16248893737793 0.2779405117034912
CurrentTrain: epoch  9, batch    30 | loss: 4.1624889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.114297866821289 0.2588712275028229
CurrentTrain: epoch  9, batch    31 | loss: 4.1142979des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.093368053436279 0.24686554074287415
CurrentTrain: epoch  9, batch    32 | loss: 4.0933681des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.077784061431885 0.23068583011627197
CurrentTrain: epoch  9, batch    33 | loss: 4.0777841des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.142923355102539 0.2913001775741577
CurrentTrain: epoch  9, batch    34 | loss: 4.1429234des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.158047676086426 0.32175302505493164
CurrentTrain: epoch  9, batch    35 | loss: 4.1580477des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.085288047790527 0.1735558807849884
CurrentTrain: epoch  9, batch    36 | loss: 4.0852880des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1403093338012695 0.3218317925930023
CurrentTrain: epoch  9, batch    37 | loss: 4.1403093des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.181674957275391 0.27201443910598755
CurrentTrain: epoch  9, batch    38 | loss: 4.1816750des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.143193244934082 0.2821621596813202
CurrentTrain: epoch  9, batch    39 | loss: 4.1431932des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19276762008667 0.21245133876800537
CurrentTrain: epoch  9, batch    40 | loss: 4.1927676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.085398197174072 0.1962209939956665
CurrentTrain: epoch  9, batch    41 | loss: 4.0853982des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.119721412658691 0.2152438461780548
CurrentTrain: epoch  9, batch    42 | loss: 4.1197214des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.091283321380615 0.21682018041610718
CurrentTrain: epoch  9, batch    43 | loss: 4.0912833des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.118481159210205 0.23580677807331085
CurrentTrain: epoch  9, batch    44 | loss: 4.1184812des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.095992088317871 0.2455289661884308
CurrentTrain: epoch  9, batch    45 | loss: 4.0959921des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.075434684753418 0.21220093965530396
CurrentTrain: epoch  9, batch    46 | loss: 4.0754347des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.093906879425049 0.15028344094753265
CurrentTrain: epoch  9, batch    47 | loss: 4.0939069des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.123295783996582 0.26797640323638916
CurrentTrain: epoch  9, batch    48 | loss: 4.1232958des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.189264297485352 0.2494606375694275
CurrentTrain: epoch  9, batch    49 | loss: 4.1892643des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.161345958709717 0.28310900926589966
CurrentTrain: epoch  9, batch    50 | loss: 4.1613460des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.156708717346191 0.249930277466774
CurrentTrain: epoch  9, batch    51 | loss: 4.1567087des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1271209716796875 0.2546611428260803
CurrentTrain: epoch  9, batch    52 | loss: 4.1271210des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.103661060333252 0.2307565063238144
CurrentTrain: epoch  9, batch    53 | loss: 4.1036611des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.112275123596191 0.2292114794254303
CurrentTrain: epoch  9, batch    54 | loss: 4.1122751des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.112921237945557 0.24796436727046967
CurrentTrain: epoch  9, batch    55 | loss: 4.1129212des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.092489242553711 0.26955506205558777
CurrentTrain: epoch  9, batch    56 | loss: 4.0924892des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.12227725982666 0.25599604845046997
CurrentTrain: epoch  9, batch    57 | loss: 4.1222773des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.107461452484131 0.26109084486961365
CurrentTrain: epoch  9, batch    58 | loss: 4.1074615des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.0928955078125 0.260982483625412
CurrentTrain: epoch  9, batch    59 | loss: 4.0928955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.123079776763916 0.25373128056526184
CurrentTrain: epoch  9, batch    60 | loss: 4.1230798des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.12212610244751 0.2501254379749298
CurrentTrain: epoch  9, batch    61 | loss: 4.1221261des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.062792778015137 0.15487807989120483
CurrentTrain: epoch  9, batch    62 | loss: 4.0627928
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
cur_acc:  ['0.9425']
his_acc:  ['0.9425']
Clustering into  9  clusters
Clusters:  [3 2 0 7 2 2 0 1 6 6 4 1 1 2 5 6 3 2 8 8]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.660098075866699 2.012878894805908
CurrentTrain: epoch  0, batch     0 | loss: 7.6600981des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.192507266998291 2.0397555828094482
CurrentTrain: epoch  0, batch     1 | loss: 7.1925073des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.42062520980835 2.110663414001465
CurrentTrain: epoch  0, batch     2 | loss: 7.4206252des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.261144638061523 0.0
CurrentTrain: epoch  0, batch     3 | loss: 8.2611446des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.215516567230225 2.1354169845581055
CurrentTrain: epoch  1, batch     0 | loss: 7.2155166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.119231224060059 2.2051639556884766
CurrentTrain: epoch  1, batch     1 | loss: 7.1192312des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.270012855529785 2.1459333896636963
CurrentTrain: epoch  1, batch     2 | loss: 6.2700129des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4951653480529785 0.7870950698852539
CurrentTrain: epoch  1, batch     3 | loss: 3.4951653des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.0551886558532715 1.8897104263305664
CurrentTrain: epoch  2, batch     0 | loss: 6.0551887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7290940284729 2.0795791149139404
CurrentTrain: epoch  2, batch     1 | loss: 5.7290940des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.429086208343506 2.103827476501465
CurrentTrain: epoch  2, batch     2 | loss: 5.4290862des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.235621929168701 0.6207929849624634
CurrentTrain: epoch  2, batch     3 | loss: 7.2356219des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.318388938903809 2.176607131958008
CurrentTrain: epoch  3, batch     0 | loss: 6.3183889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.600180625915527 1.9877278804779053
CurrentTrain: epoch  3, batch     1 | loss: 4.6001806des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.32444429397583 2.061845064163208
CurrentTrain: epoch  3, batch     2 | loss: 5.3244443des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.385169506072998 0.7987511157989502
CurrentTrain: epoch  3, batch     3 | loss: 3.3851695des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8723907470703125 1.9306714534759521
CurrentTrain: epoch  4, batch     0 | loss: 4.8723907des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5400166511535645 1.8666208982467651
CurrentTrain: epoch  4, batch     1 | loss: 4.5400167des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.313515663146973 1.92216157913208
CurrentTrain: epoch  4, batch     2 | loss: 5.3135157des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.997044086456299 0.6827372312545776
CurrentTrain: epoch  4, batch     3 | loss: 3.9970441des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.397302627563477 1.9125120639801025
CurrentTrain: epoch  5, batch     0 | loss: 5.3973026des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.791783332824707 1.8633434772491455
CurrentTrain: epoch  5, batch     1 | loss: 3.7917833des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.244021415710449 2.1876235008239746
CurrentTrain: epoch  5, batch     2 | loss: 4.2440214des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.969964504241943 0.6388244032859802
CurrentTrain: epoch  5, batch     3 | loss: 4.9699645des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.168557167053223 1.8428585529327393
CurrentTrain: epoch  6, batch     0 | loss: 4.1685572des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.400559425354004 1.8429646492004395
CurrentTrain: epoch  6, batch     1 | loss: 4.4005594des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.974989414215088 1.915802240371704
CurrentTrain: epoch  6, batch     2 | loss: 3.9749894des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.457205295562744 0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.4572053des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.268564224243164 1.96370267868042
CurrentTrain: epoch  7, batch     0 | loss: 4.2685642des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7728817462921143 2.081712245941162
CurrentTrain: epoch  7, batch     1 | loss: 3.7728817des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.634467601776123 1.8370904922485352
CurrentTrain: epoch  7, batch     2 | loss: 3.6344676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9479248523712158 0.0
CurrentTrain: epoch  7, batch     3 | loss: 1.9479249des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.616055965423584 1.9602042436599731
CurrentTrain: epoch  8, batch     0 | loss: 3.6160560des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.771341323852539 1.6821279525756836
CurrentTrain: epoch  8, batch     1 | loss: 3.7713413des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.560837507247925 1.8881726264953613
CurrentTrain: epoch  8, batch     2 | loss: 3.5608375des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4413254261016846 0.7112554311752319
CurrentTrain: epoch  8, batch     3 | loss: 2.4413254des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.341862678527832 1.9392242431640625
CurrentTrain: epoch  9, batch     0 | loss: 3.3418627des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8219587802886963 1.6428838968276978
CurrentTrain: epoch  9, batch     1 | loss: 3.8219588des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.2358829975128174 1.916623592376709
CurrentTrain: epoch  9, batch     2 | loss: 3.2358830des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2239956855773926 0.5428566932678223
CurrentTrain: epoch  9, batch     3 | loss: 2.2239957
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6413514614105225 2.623692035675049
MemoryTrain:  epoch  0, batch     0 | loss: 3.6413515des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.9662718772888184 1.3299601078033447
MemoryTrain:  epoch  0, batch     1 | loss: 0.9662719des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.091170310974121 2.60445499420166
MemoryTrain:  epoch  1, batch     0 | loss: 3.0911703des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6321759223937988 1.2751256227493286
MemoryTrain:  epoch  1, batch     1 | loss: 1.6321759des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.818350315093994 2.6228244304656982
MemoryTrain:  epoch  2, batch     0 | loss: 2.8183503des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.9199119210243225 1.2332675457000732
MemoryTrain:  epoch  2, batch     1 | loss: 0.9199119des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.115400791168213 2.593381404876709
MemoryTrain:  epoch  3, batch     0 | loss: 2.1154008des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.613421678543091 1.2218807935714722
MemoryTrain:  epoch  3, batch     1 | loss: 2.6134217des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.268399715423584 2.5148308277130127
MemoryTrain:  epoch  4, batch     0 | loss: 2.2683997des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.7031305432319641 1.2805427312850952
MemoryTrain:  epoch  4, batch     1 | loss: 0.7031305des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1705191135406494 2.5559909343719482
MemoryTrain:  epoch  5, batch     0 | loss: 2.1705191des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6420376300811768 1.198569893836975
MemoryTrain:  epoch  5, batch     1 | loss: 0.6420376des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.066882848739624 2.5247673988342285
MemoryTrain:  epoch  6, batch     0 | loss: 2.0668828des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8325977325439453 1.2869105339050293
MemoryTrain:  epoch  6, batch     1 | loss: 0.8325977des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7385220527648926 2.543606758117676
MemoryTrain:  epoch  7, batch     0 | loss: 1.7385221des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.820138454437256 1.0704097747802734
MemoryTrain:  epoch  7, batch     1 | loss: 2.8201385des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.188951015472412 2.5263099670410156
MemoryTrain:  epoch  8, batch     0 | loss: 2.1889510des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6322095990180969 1.2196593284606934
MemoryTrain:  epoch  8, batch     1 | loss: 0.6322096des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9673094749450684 2.5324971675872803
MemoryTrain:  epoch  9, batch     0 | loss: 1.9673095des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.732162356376648 0.9938610196113586
MemoryTrain:  epoch  9, batch     1 | loss: 0.7321624
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 96.63%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 95.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 94.17%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 73.84%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 72.50%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 71.60%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 73.81%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 89.89%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.48%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 92.03%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 92.01%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.94%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.64%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.76%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.69%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 92.23%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.64%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.59%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.27%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 91.03%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.92%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.59%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.52%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 90.00%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 89.97%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 89.74%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 89.52%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 89.03%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 88.55%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 87.96%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 87.56%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.05%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 86.68%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.19%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 85.83%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.54%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 85.13%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 84.58%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 84.49%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.11%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.56%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.03%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.61%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 82.09%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 82.00%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.26%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.84%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.98%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.20%   
cur_acc:  ['0.9425', '0.7381']
his_acc:  ['0.9425', '0.8320']
Clustering into  14  clusters
Clusters:  [ 0  3 12 11  3  3  5  4  7  7  9  2  2  3  1  7  0  3  6  6  0  6  8 13
  1  3 10  0  4  0]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.00811767578125 2.067110061645508
CurrentTrain: epoch  0, batch     0 | loss: 8.0081177des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.251060962677002 2.15399432182312
CurrentTrain: epoch  0, batch     1 | loss: 7.2510610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.037164688110352 2.323357343673706
CurrentTrain: epoch  0, batch     2 | loss: 8.0371647des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.311055183410645 0.7740306854248047
CurrentTrain: epoch  0, batch     3 | loss: 8.3110552des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.551668167114258 2.2875475883483887
CurrentTrain: epoch  1, batch     0 | loss: 6.5516682des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.401828765869141 2.029541254043579
CurrentTrain: epoch  1, batch     1 | loss: 7.4018288des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.035161018371582 2.273451328277588
CurrentTrain: epoch  1, batch     2 | loss: 6.0351610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.269502639770508 0.6987401843070984
CurrentTrain: epoch  1, batch     3 | loss: 6.2695026des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.119919776916504 1.9905877113342285
CurrentTrain: epoch  2, batch     0 | loss: 5.1199198des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.126894474029541 2.16644024848938
CurrentTrain: epoch  2, batch     1 | loss: 6.1268945des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.985502243041992 2.0006632804870605
CurrentTrain: epoch  2, batch     2 | loss: 6.9855022des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.274491310119629 0.5014183521270752
CurrentTrain: epoch  2, batch     3 | loss: 3.2744913des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.773645877838135 2.183253526687622
CurrentTrain: epoch  3, batch     0 | loss: 5.7736459des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.917873859405518 1.938944697380066
CurrentTrain: epoch  3, batch     1 | loss: 4.9178739des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.029171466827393 2.108881950378418
CurrentTrain: epoch  3, batch     2 | loss: 6.0291715des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.036746501922607 0.766047477722168
CurrentTrain: epoch  3, batch     3 | loss: 6.0367465des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.888271808624268 1.9898431301116943
CurrentTrain: epoch  4, batch     0 | loss: 5.8882718des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.4512739181518555 2.209977149963379
CurrentTrain: epoch  4, batch     1 | loss: 5.4512739des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.035068511962891 1.8985216617584229
CurrentTrain: epoch  4, batch     2 | loss: 4.0350685des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.397093772888184 0.8568387031555176
CurrentTrain: epoch  4, batch     3 | loss: 5.3970938des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.626945495605469 1.8289124965667725
CurrentTrain: epoch  5, batch     0 | loss: 5.6269455des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.536165237426758 1.9204617738723755
CurrentTrain: epoch  5, batch     1 | loss: 4.5361652des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.533092021942139 1.8194570541381836
CurrentTrain: epoch  5, batch     2 | loss: 4.5330920des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.520175933837891 0.6809718012809753
CurrentTrain: epoch  5, batch     3 | loss: 7.5201759des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.457126140594482 2.1921589374542236
CurrentTrain: epoch  6, batch     0 | loss: 5.4571261des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0441179275512695 2.070274829864502
CurrentTrain: epoch  6, batch     1 | loss: 5.0441179des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.150999069213867 2.0557632446289062
CurrentTrain: epoch  6, batch     2 | loss: 4.1509991des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.944486618041992 0.7506970167160034
CurrentTrain: epoch  6, batch     3 | loss: 2.9444866des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.278042793273926 2.009658098220825
CurrentTrain: epoch  7, batch     0 | loss: 4.2780428des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.601015090942383 1.9495066404342651
CurrentTrain: epoch  7, batch     1 | loss: 4.6010151des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.619718551635742 1.949186086654663
CurrentTrain: epoch  7, batch     2 | loss: 4.6197186des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.352296829223633 0.6644765138626099
CurrentTrain: epoch  7, batch     3 | loss: 5.3522968des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.593042850494385 2.1503632068634033
CurrentTrain: epoch  8, batch     0 | loss: 4.5930429des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.522334575653076 2.0136237144470215
CurrentTrain: epoch  8, batch     1 | loss: 4.5223346des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2635178565979 1.9281715154647827
CurrentTrain: epoch  8, batch     2 | loss: 4.2635179des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.618973970413208 0.5440627336502075
CurrentTrain: epoch  8, batch     3 | loss: 2.6189740des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.962264060974121 1.7752833366394043
CurrentTrain: epoch  9, batch     0 | loss: 3.9622641des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4484167098999023 1.702448844909668
CurrentTrain: epoch  9, batch     1 | loss: 3.4484167des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.60785436630249 1.8393889665603638
CurrentTrain: epoch  9, batch     2 | loss: 4.6078544des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.071761131286621 0.6796975135803223
CurrentTrain: epoch  9, batch     3 | loss: 5.0717611
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.623880386352539 2.6713919639587402
MemoryTrain:  epoch  0, batch     0 | loss: 2.6238804des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9341344833374023 2.552807331085205
MemoryTrain:  epoch  0, batch     1 | loss: 1.9341345des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5650835037231445 2.7232229709625244
MemoryTrain:  epoch  1, batch     0 | loss: 2.5650835des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4180734157562256 2.469115734100342
MemoryTrain:  epoch  1, batch     1 | loss: 2.4180734des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8656924962997437 2.680150032043457
MemoryTrain:  epoch  2, batch     0 | loss: 1.8656925des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.231541156768799 2.4416558742523193
MemoryTrain:  epoch  2, batch     1 | loss: 2.2315412des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.0762548446655273 2.6161916255950928
MemoryTrain:  epoch  3, batch     0 | loss: 2.0762548des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8735697269439697 2.515855550765991
MemoryTrain:  epoch  3, batch     1 | loss: 1.8735697des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.765425443649292 2.6220028400421143
MemoryTrain:  epoch  4, batch     0 | loss: 1.7654254des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6155868768692017 2.4937782287597656
MemoryTrain:  epoch  4, batch     1 | loss: 1.6155869des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5818885564804077 2.558293342590332
MemoryTrain:  epoch  5, batch     0 | loss: 1.5818886des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3619998693466187 2.5056827068328857
MemoryTrain:  epoch  5, batch     1 | loss: 1.3619999des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4295192956924438 2.5985655784606934
MemoryTrain:  epoch  6, batch     0 | loss: 1.4295193des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2868059873580933 2.4073898792266846
MemoryTrain:  epoch  6, batch     1 | loss: 1.2868060des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4587974548339844 2.5803937911987305
MemoryTrain:  epoch  7, batch     0 | loss: 1.4587975des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3039755821228027 2.456303834915161
MemoryTrain:  epoch  7, batch     1 | loss: 1.3039756des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3719193935394287 2.6139605045318604
MemoryTrain:  epoch  8, batch     0 | loss: 1.3719194des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2724815607070923 2.393845319747925
MemoryTrain:  epoch  8, batch     1 | loss: 1.2724816des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.375301480293274 2.579888343811035
MemoryTrain:  epoch  9, batch     0 | loss: 1.3753015des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.256906270980835 2.429309844970703
MemoryTrain:  epoch  9, batch     1 | loss: 1.2569063
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 61.52%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 70.05%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.04%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.44%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.52%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.44%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.96%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.26%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 92.87%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.72%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.42%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.44%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 92.30%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.94%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 91.59%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 91.25%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 90.99%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 90.30%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.20%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.03%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 89.65%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 89.56%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 89.33%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 89.11%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 88.56%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 88.03%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 87.43%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 86.98%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.48%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 85.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.27%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 85.05%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.71%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.44%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.02%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.99%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.45%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 81.99%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 81.42%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.03%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.81%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 81.71%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 81.74%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 81.78%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 82.25%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 81.70%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 81.21%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 80.67%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 80.11%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 79.55%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 79.17%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 79.27%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 79.55%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 79.19%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 78.84%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 78.45%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 78.15%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 77.72%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.71%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.77%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.09%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.87%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 78.74%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 78.58%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 78.34%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.32%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.23%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.18%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 78.11%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 78.16%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 78.04%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 77.91%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 77.69%   
cur_acc:  ['0.9425', '0.7381', '0.6944']
his_acc:  ['0.9425', '0.8320', '0.7769']
Clustering into  19  clusters
Clusters:  [ 1  2 14 12  3  3 17  7  2 16 11  4  4  3  0 16  1  3  6  6  5  6 15  9
  0  3 18  1 13  5 12  1  1  2 10  1  2  8  7  2]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.377453327178955 2.12186336517334
CurrentTrain: epoch  0, batch     0 | loss: 6.3774533des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.229946613311768 2.1652145385742188
CurrentTrain: epoch  0, batch     1 | loss: 5.2299466des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.041816234588623 1.9455528259277344
CurrentTrain: epoch  0, batch     2 | loss: 6.0418162des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.072900295257568 0.7834615707397461
CurrentTrain: epoch  0, batch     3 | loss: 4.0729003des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.458916664123535 2.285831928253174
CurrentTrain: epoch  1, batch     0 | loss: 5.4589167des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.499059677124023 2.1926329135894775
CurrentTrain: epoch  1, batch     1 | loss: 4.4990597des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.905697345733643 2.231201171875
CurrentTrain: epoch  1, batch     2 | loss: 4.9056973des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.7482681274414062 0.7609933614730835
CurrentTrain: epoch  1, batch     3 | loss: 2.7482681des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.456806182861328 2.098907232284546
CurrentTrain: epoch  2, batch     0 | loss: 5.4568062des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9964818954467773 2.134347438812256
CurrentTrain: epoch  2, batch     1 | loss: 3.9964819des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4470438957214355 2.0502493381500244
CurrentTrain: epoch  2, batch     2 | loss: 3.4470439des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.326594591140747 0.7541368007659912
CurrentTrain: epoch  2, batch     3 | loss: 2.3265946des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.118120193481445 2.1222548484802246
CurrentTrain: epoch  3, batch     0 | loss: 4.1181202des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.050729751586914 2.1566176414489746
CurrentTrain: epoch  3, batch     1 | loss: 4.0507298des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.200440406799316 2.066187858581543
CurrentTrain: epoch  3, batch     2 | loss: 4.2004404des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4998703002929688 0.7779405117034912
CurrentTrain: epoch  3, batch     3 | loss: 2.4998703des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9883670806884766 1.8550775051116943
CurrentTrain: epoch  4, batch     0 | loss: 3.9883671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.684582471847534 1.8516021966934204
CurrentTrain: epoch  4, batch     1 | loss: 3.6845825des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.103034973144531 1.6973309516906738
CurrentTrain: epoch  4, batch     2 | loss: 4.1030350des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.3612723350524902 0.562943696975708
CurrentTrain: epoch  4, batch     3 | loss: 2.3612723des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.109376907348633 2.074345588684082
CurrentTrain: epoch  5, batch     0 | loss: 4.1093769des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.898712158203125 1.8463773727416992
CurrentTrain: epoch  5, batch     1 | loss: 3.8987122des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4034464359283447 2.1324167251586914
CurrentTrain: epoch  5, batch     2 | loss: 3.4034464des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8117926120758057 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.8117926des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.998283863067627 1.864985704421997
CurrentTrain: epoch  6, batch     0 | loss: 2.9982839des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4533286094665527 1.8177627325057983
CurrentTrain: epoch  6, batch     1 | loss: 3.4533286des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.070408821105957 1.9457217454910278
CurrentTrain: epoch  6, batch     2 | loss: 4.0704088des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.874694347381592 0.7443264722824097
CurrentTrain: epoch  6, batch     3 | loss: 2.8746943des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.301145076751709 1.9519635438919067
CurrentTrain: epoch  7, batch     0 | loss: 3.3011451des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3489060401916504 1.929663896560669
CurrentTrain: epoch  7, batch     1 | loss: 3.3489060des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6386191844940186 1.955942988395691
CurrentTrain: epoch  7, batch     2 | loss: 3.6386192des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3095738887786865 0.5076400637626648
CurrentTrain: epoch  7, batch     3 | loss: 3.3095739des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3326289653778076 1.7294039726257324
CurrentTrain: epoch  8, batch     0 | loss: 3.3326290des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.284870147705078 1.9068899154663086
CurrentTrain: epoch  8, batch     1 | loss: 3.2848701des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.0989904403686523 1.8886361122131348
CurrentTrain: epoch  8, batch     2 | loss: 3.0989904des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.976862668991089 0.5752522349357605
CurrentTrain: epoch  8, batch     3 | loss: 2.9768627des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.1010987758636475 1.8759994506835938
CurrentTrain: epoch  9, batch     0 | loss: 3.1010988des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.25369930267334 1.7971906661987305
CurrentTrain: epoch  9, batch     1 | loss: 3.2536993des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.8569495677948 1.7590185403823853
CurrentTrain: epoch  9, batch     2 | loss: 2.8569496des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.3461403846740723 0.5893256664276123
CurrentTrain: epoch  9, batch     3 | loss: 2.3461404
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5836787223815918 2.6586544513702393
MemoryTrain:  epoch  0, batch     0 | loss: 1.5836787des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.8507814407348633 2.692272424697876
MemoryTrain:  epoch  0, batch     1 | loss: 2.8507814des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.083428144454956 1.9495954513549805
MemoryTrain:  epoch  0, batch     2 | loss: 2.0834281des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1468067169189453 2.663181781768799
MemoryTrain:  epoch  1, batch     0 | loss: 2.1468067des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.3002853393554688 2.683969736099243
MemoryTrain:  epoch  1, batch     1 | loss: 2.3002853des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.198843479156494 1.9576683044433594
MemoryTrain:  epoch  1, batch     2 | loss: 3.1988435des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.119750499725342 2.6300454139709473
MemoryTrain:  epoch  2, batch     0 | loss: 2.1197505des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4622347354888916 2.633077621459961
MemoryTrain:  epoch  2, batch     1 | loss: 2.4622347des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1088001728057861 2.005841016769409
MemoryTrain:  epoch  2, batch     2 | loss: 1.1088002des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1129369735717773 2.6079936027526855
MemoryTrain:  epoch  3, batch     0 | loss: 2.1129370des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.827383279800415 2.6472244262695312
MemoryTrain:  epoch  3, batch     1 | loss: 1.8273833des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.140304446220398 1.8700534105300903
MemoryTrain:  epoch  3, batch     2 | loss: 1.1403044des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5266209840774536 2.591156005859375
MemoryTrain:  epoch  4, batch     0 | loss: 1.5266210des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7459467649459839 2.5956602096557617
MemoryTrain:  epoch  4, batch     1 | loss: 1.7459468des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4076433181762695 1.969120979309082
MemoryTrain:  epoch  4, batch     2 | loss: 2.4076433des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.958488941192627 2.545060873031616
MemoryTrain:  epoch  5, batch     0 | loss: 1.9584889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3705286979675293 2.618342638015747
MemoryTrain:  epoch  5, batch     1 | loss: 1.3705287des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0470638275146484 1.9098689556121826
MemoryTrain:  epoch  5, batch     2 | loss: 1.0470638des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4075676202774048 2.603917121887207
MemoryTrain:  epoch  6, batch     0 | loss: 1.4075676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4924391508102417 2.5252015590667725
MemoryTrain:  epoch  6, batch     1 | loss: 1.4924392des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2492598295211792 1.9100372791290283
MemoryTrain:  epoch  6, batch     2 | loss: 1.2492598des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3992353677749634 2.5685901641845703
MemoryTrain:  epoch  7, batch     0 | loss: 1.3992354des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5116209983825684 2.518826961517334
MemoryTrain:  epoch  7, batch     1 | loss: 1.5116210des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.9865807890892029 1.8214111328125
MemoryTrain:  epoch  7, batch     2 | loss: 0.9865808des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3623203039169312 2.498293876647949
MemoryTrain:  epoch  8, batch     0 | loss: 1.3623203des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5118567943572998 2.5094010829925537
MemoryTrain:  epoch  8, batch     1 | loss: 1.5118568des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1728812456130981 1.9426467418670654
MemoryTrain:  epoch  8, batch     2 | loss: 1.1728812des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3403949737548828 2.4833669662475586
MemoryTrain:  epoch  9, batch     0 | loss: 1.3403950des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3394984006881714 2.53334903717041
MemoryTrain:  epoch  9, batch     1 | loss: 1.3394984des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.9658956527709961 1.7191952466964722
MemoryTrain:  epoch  9, batch     2 | loss: 0.9658957
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 0.00%,  total acc: 52.86%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 51.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 61.33%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 61.74%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 62.32%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 63.18%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 63.65%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.50%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.41%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 90.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 90.05%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.90%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.79%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.61%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 89.25%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 88.63%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 88.10%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 87.57%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 87.06%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 86.21%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 85.96%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 85.56%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 85.44%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 85.01%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.51%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 84.01%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 83.53%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 83.18%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 82.65%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 82.32%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 81.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.62%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.43%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 81.13%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 80.89%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.60%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.26%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 79.57%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 78.96%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.30%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.70%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 77.18%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.94%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.38%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.40%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 78.39%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 78.48%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 79.03%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 78.46%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 77.95%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 77.44%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 76.35%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.00%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 76.49%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.03%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 75.20%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 74.84%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 74.40%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.49%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.96%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.92%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 74.89%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 74.77%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 74.62%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 74.51%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 74.37%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 74.26%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.92%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 73.91%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 74.00%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 73.99%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 74.33%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 73.89%   [EVAL] batch:  197 | acc: 18.75%,  total acc: 73.61%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 73.46%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 73.25%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 73.28%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 72.63%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 72.34%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 71.74%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 71.43%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 71.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 72.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.78%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 73.81%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.50%   
cur_acc:  ['0.9425', '0.7381', '0.6944', '0.7550']
his_acc:  ['0.9425', '0.8320', '0.7769', '0.7450']
Clustering into  24  clusters
Clusters:  [11  8 19  6  1  1 23  2  5 10 13  2  2  1  0 10 11  1 14 14  4 14 15  9
  0  1 22 11 20  4  6 11 11  5  3  7  5 21  2  5  8 17  9  3  2 16  5  7
 18 12]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.102992057800293 1.9767286777496338
CurrentTrain: epoch  0, batch     0 | loss: 7.1029921des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.852818489074707 2.0765953063964844
CurrentTrain: epoch  0, batch     1 | loss: 6.8528185des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.791756629943848 1.9927698373794556
CurrentTrain: epoch  0, batch     2 | loss: 6.7917566des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.476537704467773 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.4765377des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.301634311676025 2.076127290725708
CurrentTrain: epoch  1, batch     0 | loss: 6.3016343des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.686252593994141 2.1211585998535156
CurrentTrain: epoch  1, batch     1 | loss: 5.6862526des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.4636735916137695 1.9883636236190796
CurrentTrain: epoch  1, batch     2 | loss: 5.4636736des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.415890216827393 0.6986910104751587
CurrentTrain: epoch  1, batch     3 | loss: 4.4158902des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.3781256675720215 2.133230209350586
CurrentTrain: epoch  2, batch     0 | loss: 5.3781257des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.012371063232422 2.1055784225463867
CurrentTrain: epoch  2, batch     1 | loss: 6.0123711des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.294593811035156 2.071028709411621
CurrentTrain: epoch  2, batch     2 | loss: 5.2945938des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.2475404739379883 0.6904474496841431
CurrentTrain: epoch  2, batch     3 | loss: 3.2475405des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.555379867553711 2.0849924087524414
CurrentTrain: epoch  3, batch     0 | loss: 5.5553799des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.761728763580322 1.9901806116104126
CurrentTrain: epoch  3, batch     1 | loss: 4.7617288des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.682743549346924 1.945200800895691
CurrentTrain: epoch  3, batch     2 | loss: 4.6827435des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.908639907836914 0.611027717590332
CurrentTrain: epoch  3, batch     3 | loss: 4.9086399des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.275210380554199 1.8831912279129028
CurrentTrain: epoch  4, batch     0 | loss: 4.2752104des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.054312229156494 1.7382097244262695
CurrentTrain: epoch  4, batch     1 | loss: 5.0543122des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.619426727294922 2.1443166732788086
CurrentTrain: epoch  4, batch     2 | loss: 4.6194267des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1744632720947266 0.6945838332176208
CurrentTrain: epoch  4, batch     3 | loss: 2.1744633des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.315688133239746 2.012906074523926
CurrentTrain: epoch  5, batch     0 | loss: 4.3156881des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.423140525817871 1.932890772819519
CurrentTrain: epoch  5, batch     1 | loss: 4.4231405des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.499198913574219 1.9847434759140015
CurrentTrain: epoch  5, batch     2 | loss: 4.4991989des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.185728073120117 0.6579797863960266
CurrentTrain: epoch  5, batch     3 | loss: 5.1857281des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.165808200836182 1.8471601009368896
CurrentTrain: epoch  6, batch     0 | loss: 4.1658082des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.08336877822876 1.9067349433898926
CurrentTrain: epoch  6, batch     1 | loss: 4.0833688des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.364767551422119 1.9230902194976807
CurrentTrain: epoch  6, batch     2 | loss: 4.3647676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.2043116092681885 0.6738063097000122
CurrentTrain: epoch  6, batch     3 | loss: 3.2043116des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9654128551483154 1.9228453636169434
CurrentTrain: epoch  7, batch     0 | loss: 3.9654129des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.960695743560791 1.910306692123413
CurrentTrain: epoch  7, batch     1 | loss: 3.9606957des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.175898551940918 1.9235228300094604
CurrentTrain: epoch  7, batch     2 | loss: 4.1758986des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4362401962280273 0.627122700214386
CurrentTrain: epoch  7, batch     3 | loss: 3.4362402des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.678891181945801 2.0393502712249756
CurrentTrain: epoch  8, batch     0 | loss: 3.6788912des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2837371826171875 1.8121941089630127
CurrentTrain: epoch  8, batch     1 | loss: 4.2837372des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8581395149230957 1.8216227293014526
CurrentTrain: epoch  8, batch     2 | loss: 3.8581395des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1660702228546143 0.5087558031082153
CurrentTrain: epoch  8, batch     3 | loss: 2.1660702des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9802045822143555 1.934268593788147
CurrentTrain: epoch  9, batch     0 | loss: 3.9802046des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.150085687637329 1.7560418844223022
CurrentTrain: epoch  9, batch     1 | loss: 3.1500857des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.0920023918151855 1.698908805847168
CurrentTrain: epoch  9, batch     2 | loss: 4.0920024des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.406942367553711 0.565717875957489
CurrentTrain: epoch  9, batch     3 | loss: 2.4069424
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9275562763214111 2.73726749420166
MemoryTrain:  epoch  0, batch     0 | loss: 1.9275563des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.018402099609375 2.6642117500305176
MemoryTrain:  epoch  0, batch     1 | loss: 2.0184021des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7202180624008179 2.734736919403076
MemoryTrain:  epoch  0, batch     2 | loss: 1.7202181des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6578497886657715 0.49136102199554443
MemoryTrain:  epoch  0, batch     3 | loss: 1.6578498des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.694098472595215 2.6761012077331543
MemoryTrain:  epoch  1, batch     0 | loss: 2.6940985des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1306393146514893 2.6107540130615234
MemoryTrain:  epoch  1, batch     1 | loss: 2.1306393des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7567007541656494 2.7142934799194336
MemoryTrain:  epoch  1, batch     2 | loss: 1.7567008des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.4671365022659302 0.7332069277763367
MemoryTrain:  epoch  1, batch     3 | loss: 0.4671365des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1440534591674805 2.665921211242676
MemoryTrain:  epoch  2, batch     0 | loss: 2.1440535des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6610970497131348 2.7565765380859375
MemoryTrain:  epoch  2, batch     1 | loss: 1.6610970des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6772325038909912 2.5860543251037598
MemoryTrain:  epoch  2, batch     2 | loss: 1.6772325des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.38434135913848877 0.6156960725784302
MemoryTrain:  epoch  2, batch     3 | loss: 0.3843414des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5971944332122803 2.701336622238159
MemoryTrain:  epoch  3, batch     0 | loss: 1.5971944des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.483766794204712 2.6350293159484863
MemoryTrain:  epoch  3, batch     1 | loss: 1.4837668des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6617777347564697 2.6198482513427734
MemoryTrain:  epoch  3, batch     2 | loss: 1.6617777des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.4853675961494446 0.6486004590988159
MemoryTrain:  epoch  3, batch     3 | loss: 0.4853676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5725269317626953 2.6491920948028564
MemoryTrain:  epoch  4, batch     0 | loss: 1.5725269des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4631775617599487 2.6855568885803223
MemoryTrain:  epoch  4, batch     1 | loss: 1.4631776des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3749477863311768 2.580047130584717
MemoryTrain:  epoch  4, batch     2 | loss: 1.3749478des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.35855141282081604 0.5909245014190674
MemoryTrain:  epoch  4, batch     3 | loss: 0.3585514des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5012378692626953 2.682742118835449
MemoryTrain:  epoch  5, batch     0 | loss: 1.5012379des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4503889083862305 2.6090965270996094
MemoryTrain:  epoch  5, batch     1 | loss: 1.4503889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3600475788116455 2.569906711578369
MemoryTrain:  epoch  5, batch     2 | loss: 1.3600476des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.32603588700294495 0.6232649087905884
MemoryTrain:  epoch  5, batch     3 | loss: 0.3260359des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.403128743171692 2.6046648025512695
MemoryTrain:  epoch  6, batch     0 | loss: 1.4031287des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3974701166152954 2.6357250213623047
MemoryTrain:  epoch  6, batch     1 | loss: 1.3974701des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3594597578048706 2.5187032222747803
MemoryTrain:  epoch  6, batch     2 | loss: 1.3594598des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.31294122338294983 0.5928528308868408
MemoryTrain:  epoch  6, batch     3 | loss: 0.3129412des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3227770328521729 2.494936466217041
MemoryTrain:  epoch  7, batch     0 | loss: 1.3227770des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3802802562713623 2.631648063659668
MemoryTrain:  epoch  7, batch     1 | loss: 1.3802803des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4206585884094238 2.5595498085021973
MemoryTrain:  epoch  7, batch     2 | loss: 1.4206586des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.35320940613746643 0.574638843536377
MemoryTrain:  epoch  7, batch     3 | loss: 0.3532094des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3518301248550415 2.620147943496704
MemoryTrain:  epoch  8, batch     0 | loss: 1.3518301des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2899020910263062 2.4820594787597656
MemoryTrain:  epoch  8, batch     1 | loss: 1.2899021des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3241578340530396 2.5516390800476074
MemoryTrain:  epoch  8, batch     2 | loss: 1.3241578des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.36905497312545776 0.6351932287216187
MemoryTrain:  epoch  8, batch     3 | loss: 0.3690550des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3049919605255127 2.5096096992492676
MemoryTrain:  epoch  9, batch     0 | loss: 1.3049920des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3323249816894531 2.590052366256714
MemoryTrain:  epoch  9, batch     1 | loss: 1.3323250des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.30455482006073 2.5054705142974854
MemoryTrain:  epoch  9, batch     2 | loss: 1.3045548des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.239795982837677 0.4523645043373108
MemoryTrain:  epoch  9, batch     3 | loss: 0.2397960
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.77%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 57.24%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 57.53%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 59.08%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 59.45%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 60.09%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 60.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 60.87%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.85%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.81%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 81.91%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 83.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 83.94%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.79%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.96%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 83.81%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 83.98%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 84.47%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 85.45%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 85.61%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 85.23%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 84.70%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 84.41%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 83.87%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 82.91%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 82.51%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 81.99%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 81.47%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 80.68%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 80.61%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 80.35%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 80.37%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 80.04%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 79.65%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 79.21%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 78.78%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 78.41%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 78.00%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 77.65%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 77.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.90%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 76.58%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 76.38%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.12%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.76%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 75.23%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.71%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 74.26%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.76%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.29%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 74.74%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 74.95%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 74.95%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 75.15%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.43%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 75.72%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 75.18%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 74.69%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 74.20%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 73.68%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 73.16%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 73.39%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.94%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 72.55%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 72.12%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 71.73%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.31%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 71.99%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 71.89%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 72.10%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 72.33%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  194 | acc: 18.75%,  total acc: 72.76%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 72.24%   [EVAL] batch:  197 | acc: 18.75%,  total acc: 71.97%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 71.56%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 71.51%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 71.03%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 70.72%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 70.42%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 70.11%   [EVAL] batch:  211 | acc: 0.00%,  total acc: 69.78%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 69.72%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 73.02%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 72.88%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 72.85%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 72.70%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.69%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 72.62%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 72.51%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 72.18%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 71.97%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 72.28%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 72.16%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 72.09%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 72.01%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 71.90%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.63%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 71.54%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 71.35%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 71.12%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 71.01%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.00%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.01%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 71.10%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 72.40%   
cur_acc:  ['0.9425', '0.7381', '0.6944', '0.7550', '0.6964']
his_acc:  ['0.9425', '0.8320', '0.7769', '0.7450', '0.7240']
Clustering into  29  clusters
Clusters:  [ 9 16 19  7  2  2 14  6  1 18  0  3  3  2 26 18  9  2 10 10  4 10 23 17
 25  2 12 22 21  4  7  9  9  1  8  5  1 24  6  1 16 15 17  8  6 20  1  5
 27 11 20 10  0 28 22 10  0 11 18 13]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.988859176635742 2.07017183303833
CurrentTrain: epoch  0, batch     0 | loss: 6.9888592des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.095870494842529 1.8419255018234253
CurrentTrain: epoch  0, batch     1 | loss: 7.0958705des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.760677814483643 2.062352180480957
CurrentTrain: epoch  0, batch     2 | loss: 7.7606778des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.822265625 0.0
CurrentTrain: epoch  0, batch     3 | loss: 4.8222656des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.85825777053833 2.2218620777130127
CurrentTrain: epoch  1, batch     0 | loss: 6.8582578des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.182675361633301 2.100390911102295
CurrentTrain: epoch  1, batch     1 | loss: 6.1826754des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.373349666595459 2.1197872161865234
CurrentTrain: epoch  1, batch     2 | loss: 5.3733497des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.140922546386719 0.7729195952415466
CurrentTrain: epoch  1, batch     3 | loss: 5.1409225des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.427249908447266 1.9825987815856934
CurrentTrain: epoch  2, batch     0 | loss: 5.4272499des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.893216133117676 2.0268564224243164
CurrentTrain: epoch  2, batch     1 | loss: 5.8932161des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.988369941711426 1.973961591720581
CurrentTrain: epoch  2, batch     2 | loss: 4.9883699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7693939208984375 0.7366595268249512
CurrentTrain: epoch  2, batch     3 | loss: 4.7693939des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.411276817321777 1.922926902770996
CurrentTrain: epoch  3, batch     0 | loss: 4.4112768des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.423856258392334 2.02590274810791
CurrentTrain: epoch  3, batch     1 | loss: 5.4238563des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.261425971984863 2.2775425910949707
CurrentTrain: epoch  3, batch     2 | loss: 5.2614260des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.663341522216797 0.9391825199127197
CurrentTrain: epoch  3, batch     3 | loss: 4.6633415des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.148042678833008 1.927116870880127
CurrentTrain: epoch  4, batch     0 | loss: 4.1480427des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.277583122253418 1.8406832218170166
CurrentTrain: epoch  4, batch     1 | loss: 5.2775831des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.713608741760254 2.0525224208831787
CurrentTrain: epoch  4, batch     2 | loss: 4.7136087des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.2967047691345215 0.5205192565917969
CurrentTrain: epoch  4, batch     3 | loss: 3.2967048des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.13725471496582 2.0927600860595703
CurrentTrain: epoch  5, batch     0 | loss: 4.1372547des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.902989387512207 1.7402794361114502
CurrentTrain: epoch  5, batch     1 | loss: 4.9029894des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.137580394744873 2.0127429962158203
CurrentTrain: epoch  5, batch     2 | loss: 4.1375804des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.236658811569214 0.47044867277145386
CurrentTrain: epoch  5, batch     3 | loss: 3.2366588des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7769627571105957 1.968968391418457
CurrentTrain: epoch  6, batch     0 | loss: 3.7769628des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.568029880523682 1.993940830230713
CurrentTrain: epoch  6, batch     1 | loss: 4.5680299des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.368376731872559 1.99568510055542
CurrentTrain: epoch  6, batch     2 | loss: 4.3683767des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.5252978801727295 0.467096745967865
CurrentTrain: epoch  6, batch     3 | loss: 3.5252979des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.242721080780029 1.9150116443634033
CurrentTrain: epoch  7, batch     0 | loss: 4.2427211des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.922494411468506 1.9681816101074219
CurrentTrain: epoch  7, batch     1 | loss: 3.9224944des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7685000896453857 1.822602391242981
CurrentTrain: epoch  7, batch     2 | loss: 3.7685001des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9931344985961914 0.5342977046966553
CurrentTrain: epoch  7, batch     3 | loss: 3.9931345des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6301379203796387 1.7293376922607422
CurrentTrain: epoch  8, batch     0 | loss: 3.6301379des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.418257474899292 1.7052693367004395
CurrentTrain: epoch  8, batch     1 | loss: 3.4182575des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.537620544433594 1.8498108386993408
CurrentTrain: epoch  8, batch     2 | loss: 4.5376205des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.0692851543426514 0.5475975275039673
CurrentTrain: epoch  8, batch     3 | loss: 2.0692852des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4084744453430176 1.776214361190796
CurrentTrain: epoch  9, batch     0 | loss: 3.4084744des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3516416549682617 1.624527096748352
CurrentTrain: epoch  9, batch     1 | loss: 3.3516417des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.650686740875244 1.6880910396575928
CurrentTrain: epoch  9, batch     2 | loss: 3.6506867des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.956806182861328 0.6346129179000854
CurrentTrain: epoch  9, batch     3 | loss: 4.9568062
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.167935848236084 2.706503391265869
MemoryTrain:  epoch  0, batch     0 | loss: 2.1679358des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9201469421386719 2.7081804275512695
MemoryTrain:  epoch  0, batch     1 | loss: 1.9201469des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.138218402862549 2.781795024871826
MemoryTrain:  epoch  0, batch     2 | loss: 2.1382184des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6996159553527832 2.468329429626465
MemoryTrain:  epoch  0, batch     3 | loss: 1.6996160des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.885713577270508 2.672495126724243
MemoryTrain:  epoch  1, batch     0 | loss: 2.8857136des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.95394766330719 2.6752336025238037
MemoryTrain:  epoch  1, batch     1 | loss: 1.9539477des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.132290840148926 2.7120513916015625
MemoryTrain:  epoch  1, batch     2 | loss: 2.1322908des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.574472427368164 2.367335319519043
MemoryTrain:  epoch  1, batch     3 | loss: 1.5744724des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8594350814819336 2.6781787872314453
MemoryTrain:  epoch  2, batch     0 | loss: 1.8594351des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7102560997009277 2.6649255752563477
MemoryTrain:  epoch  2, batch     1 | loss: 1.7102561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8796299695968628 2.6791632175445557
MemoryTrain:  epoch  2, batch     2 | loss: 1.8796300des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3705077171325684 2.4269824028015137
MemoryTrain:  epoch  2, batch     3 | loss: 1.3705077des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5359385013580322 2.690267324447632
MemoryTrain:  epoch  3, batch     0 | loss: 1.5359385des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5342440605163574 2.6744346618652344
MemoryTrain:  epoch  3, batch     1 | loss: 1.5342441des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4927144050598145 2.600271701812744
MemoryTrain:  epoch  3, batch     2 | loss: 1.4927144des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.426255226135254 2.313631534576416
MemoryTrain:  epoch  3, batch     3 | loss: 1.4262552des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6153061389923096 2.605724811553955
MemoryTrain:  epoch  4, batch     0 | loss: 1.6153061des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.496633768081665 2.7188405990600586
MemoryTrain:  epoch  4, batch     1 | loss: 1.4966338des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4089686870574951 2.5931506156921387
MemoryTrain:  epoch  4, batch     2 | loss: 1.4089687des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2664076089859009 2.343702793121338
MemoryTrain:  epoch  4, batch     3 | loss: 1.2664076des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3945999145507812 2.657193660736084
MemoryTrain:  epoch  5, batch     0 | loss: 1.3945999des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4388188123703003 2.5796923637390137
MemoryTrain:  epoch  5, batch     1 | loss: 1.4388188des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.614953875541687 2.6076483726501465
MemoryTrain:  epoch  5, batch     2 | loss: 1.6149539des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.500171184539795 2.3493871688842773
MemoryTrain:  epoch  5, batch     3 | loss: 1.5001712des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4020991325378418 2.569335699081421
MemoryTrain:  epoch  6, batch     0 | loss: 1.4020991des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4002165794372559 2.676426887512207
MemoryTrain:  epoch  6, batch     1 | loss: 1.4002166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3206336498260498 2.509303092956543
MemoryTrain:  epoch  6, batch     2 | loss: 1.3206336des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2421143054962158 2.30281925201416
MemoryTrain:  epoch  6, batch     3 | loss: 1.2421143des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3237895965576172 2.500938653945923
MemoryTrain:  epoch  7, batch     0 | loss: 1.3237896des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3607597351074219 2.56011962890625
MemoryTrain:  epoch  7, batch     1 | loss: 1.3607597des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3414506912231445 2.5556094646453857
MemoryTrain:  epoch  7, batch     2 | loss: 1.3414507des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2094743251800537 2.2939870357513428
MemoryTrain:  epoch  7, batch     3 | loss: 1.2094743des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3365663290023804 2.5647504329681396
MemoryTrain:  epoch  8, batch     0 | loss: 1.3365663des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.337538242340088 2.5299038887023926
MemoryTrain:  epoch  8, batch     1 | loss: 1.3375382des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.331058382987976 2.544738292694092
MemoryTrain:  epoch  8, batch     2 | loss: 1.3310584des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1684200763702393 2.1244568824768066
MemoryTrain:  epoch  8, batch     3 | loss: 1.1684201des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.300146460533142 2.485875129699707
MemoryTrain:  epoch  9, batch     0 | loss: 1.3001465des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3126356601715088 2.541731357574463
MemoryTrain:  epoch  9, batch     1 | loss: 1.3126357des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3027344942092896 2.5122289657592773
MemoryTrain:  epoch  9, batch     2 | loss: 1.3027345des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1418559551239014 2.1704423427581787
MemoryTrain:  epoch  9, batch     3 | loss: 1.1418560
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 72.81%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 71.11%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 82.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 83.44%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.07%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.85%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 81.65%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 80.76%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 80.58%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 79.73%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 78.82%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 77.94%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 77.63%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 78.65%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 78.21%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 78.01%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 77.81%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 77.70%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 77.44%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 77.11%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 77.01%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 76.54%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 76.09%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 75.43%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 75.28%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 75.34%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 74.67%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 74.28%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 74.10%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 73.55%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 73.25%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 72.63%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 72.38%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 71.96%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 71.04%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 70.74%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 70.27%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 69.98%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 71.36%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 72.24%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 71.72%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 71.25%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 70.79%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.29%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 69.80%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.49%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.16%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.74%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 69.40%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 68.63%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.98%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 68.60%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 68.18%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 67.77%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 67.40%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 67.00%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 67.91%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 67.56%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 67.22%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 66.88%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 66.54%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 65.88%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.84%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 65.64%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 65.38%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 65.10%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 64.82%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  211 | acc: 0.00%,  total acc: 64.27%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 65.29%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.58%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 68.68%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.47%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 68.35%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 68.16%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 68.05%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 67.91%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 68.43%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 68.19%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 68.08%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 68.00%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.69%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 67.52%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 67.04%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 68.41%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 68.14%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 69.30%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 69.18%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 69.14%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 69.06%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 68.97%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 68.82%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 68.93%   
cur_acc:  ['0.9425', '0.7381', '0.6944', '0.7550', '0.6964', '0.6964']
his_acc:  ['0.9425', '0.8320', '0.7769', '0.7450', '0.7240', '0.6893']
Clustering into  34  clusters
Clusters:  [ 5 20 21 29  0  0 26  6  2 22 32 33 17  0 11 22  5  0  4  4 14  4 12  9
 23  0 10 25 28 14 29  5  5  2  1 13 30 24  6  2 20 31  9  1  6  8  2 13
 27  3  8  4  7 19 25  4  7  3 22 15  0  4 10 11 12 16 18  2  3  2]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.759636878967285 1.7991681098937988
CurrentTrain: epoch  0, batch     0 | loss: 6.7596369des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.23703670501709 1.808659315109253
CurrentTrain: epoch  0, batch     1 | loss: 8.2370367des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.787840843200684 2.123119831085205
CurrentTrain: epoch  0, batch     2 | loss: 7.7878408des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.808507919311523 0.6917275190353394
CurrentTrain: epoch  0, batch     3 | loss: 8.8085079des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.955343246459961 2.151193141937256
CurrentTrain: epoch  1, batch     0 | loss: 6.9553432des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.689674377441406 2.0027031898498535
CurrentTrain: epoch  1, batch     1 | loss: 6.6896744des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.086308479309082 2.037883996963501
CurrentTrain: epoch  1, batch     2 | loss: 6.0863085des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.338814735412598 0.737205982208252
CurrentTrain: epoch  1, batch     3 | loss: 7.3388147des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.938314914703369 1.8879318237304688
CurrentTrain: epoch  2, batch     0 | loss: 5.9383149des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.677732467651367 2.0987796783447266
CurrentTrain: epoch  2, batch     1 | loss: 5.6777325des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.086857795715332 2.1854300498962402
CurrentTrain: epoch  2, batch     2 | loss: 6.0868578des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.1499834060668945 0.8011912107467651
CurrentTrain: epoch  2, batch     3 | loss: 7.1499834des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.692468166351318 1.8408595323562622
CurrentTrain: epoch  3, batch     0 | loss: 5.6924682des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5077996253967285 2.0815744400024414
CurrentTrain: epoch  3, batch     1 | loss: 5.5077996des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.325087547302246 2.053077220916748
CurrentTrain: epoch  3, batch     2 | loss: 5.3250875des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.005489349365234 0.6767809987068176
CurrentTrain: epoch  3, batch     3 | loss: 5.0054893des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.799277305603027 1.9908660650253296
CurrentTrain: epoch  4, batch     0 | loss: 5.7992773des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.574193954467773 1.882473349571228
CurrentTrain: epoch  4, batch     1 | loss: 4.5741940des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.043984413146973 2.062788963317871
CurrentTrain: epoch  4, batch     2 | loss: 5.0439844des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.824244976043701 0.4887993633747101
CurrentTrain: epoch  4, batch     3 | loss: 5.8242450des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8360772132873535 1.5997134447097778
CurrentTrain: epoch  5, batch     0 | loss: 4.8360772des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.800338268280029 1.9968398809432983
CurrentTrain: epoch  5, batch     1 | loss: 4.8003383des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.822100639343262 2.080991268157959
CurrentTrain: epoch  5, batch     2 | loss: 4.8221006des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.096999168395996 0.5371203422546387
CurrentTrain: epoch  5, batch     3 | loss: 4.0969992des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.07038688659668 1.9246810674667358
CurrentTrain: epoch  6, batch     0 | loss: 5.0703869des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.615096092224121 1.6328465938568115
CurrentTrain: epoch  6, batch     1 | loss: 4.6150961des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7544143199920654 1.774691104888916
CurrentTrain: epoch  6, batch     2 | loss: 3.7544143des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.491948127746582 0.6949563026428223
CurrentTrain: epoch  6, batch     3 | loss: 5.4919481des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9709649085998535 1.799340844154358
CurrentTrain: epoch  7, batch     0 | loss: 3.9709649des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.994786262512207 1.89674711227417
CurrentTrain: epoch  7, batch     1 | loss: 4.9947863des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.165138244628906 1.924802303314209
CurrentTrain: epoch  7, batch     2 | loss: 4.1651382des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9548606872558594 0.6084269285202026
CurrentTrain: epoch  7, batch     3 | loss: 3.9548607des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.131986141204834 1.8149521350860596
CurrentTrain: epoch  8, batch     0 | loss: 4.1319861des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.040829658508301 1.8485651016235352
CurrentTrain: epoch  8, batch     1 | loss: 4.0408297des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.346004486083984 2.050023078918457
CurrentTrain: epoch  8, batch     2 | loss: 4.3460045des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.8413493633270264 0.5621775388717651
CurrentTrain: epoch  8, batch     3 | loss: 2.8413494des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8285186290740967 1.9781779050827026
CurrentTrain: epoch  9, batch     0 | loss: 3.8285186des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3356781005859375 1.926382064819336
CurrentTrain: epoch  9, batch     1 | loss: 4.3356781des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8132781982421875 1.8504831790924072
CurrentTrain: epoch  9, batch     2 | loss: 3.8132782des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5615274906158447 0.5034927129745483
CurrentTrain: epoch  9, batch     3 | loss: 2.5615275
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7454804182052612 2.776883602142334
MemoryTrain:  epoch  0, batch     0 | loss: 1.7454804des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6228927373886108 2.766235113143921
MemoryTrain:  epoch  0, batch     1 | loss: 1.6228927des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7491440773010254 2.752707004547119
MemoryTrain:  epoch  0, batch     2 | loss: 1.7491441des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.580633282661438 2.741054058074951
MemoryTrain:  epoch  0, batch     3 | loss: 1.5806333des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8918246030807495 1.6956005096435547
MemoryTrain:  epoch  0, batch     4 | loss: 0.8918246des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8303154706954956 2.7328920364379883
MemoryTrain:  epoch  1, batch     0 | loss: 1.8303155des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9762496948242188 2.7402493953704834
MemoryTrain:  epoch  1, batch     1 | loss: 1.9762497des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7681556940078735 2.724752426147461
MemoryTrain:  epoch  1, batch     2 | loss: 1.7681557des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.881784200668335 2.6770784854888916
MemoryTrain:  epoch  1, batch     3 | loss: 1.8817842des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.022695541381836 1.6219937801361084
MemoryTrain:  epoch  1, batch     4 | loss: 1.0226955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5206646919250488 2.703303575515747
MemoryTrain:  epoch  2, batch     0 | loss: 1.5206647des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.583813190460205 2.676647663116455
MemoryTrain:  epoch  2, batch     1 | loss: 1.5838132des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6840531826019287 2.685077667236328
MemoryTrain:  epoch  2, batch     2 | loss: 1.6840532des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.131770372390747 2.7184786796569824
MemoryTrain:  epoch  2, batch     3 | loss: 2.1317704des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2814385890960693 1.7487205266952515
MemoryTrain:  epoch  2, batch     4 | loss: 1.2814386des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5577749013900757 2.6732263565063477
MemoryTrain:  epoch  3, batch     0 | loss: 1.5577749des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5799801349639893 2.5849101543426514
MemoryTrain:  epoch  3, batch     1 | loss: 1.5799801des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4639021158218384 2.620394706726074
MemoryTrain:  epoch  3, batch     2 | loss: 1.4639021des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6202011108398438 2.6598100662231445
MemoryTrain:  epoch  3, batch     3 | loss: 1.6202011des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1469566822052002 1.8573708534240723
MemoryTrain:  epoch  3, batch     4 | loss: 1.1469567des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3761184215545654 2.6113834381103516
MemoryTrain:  epoch  4, batch     0 | loss: 1.3761184des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5602717399597168 2.708688259124756
MemoryTrain:  epoch  4, batch     1 | loss: 1.5602717des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.39786696434021 2.6661362648010254
MemoryTrain:  epoch  4, batch     2 | loss: 1.3978670des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5468859672546387 2.631838798522949
MemoryTrain:  epoch  4, batch     3 | loss: 1.5468860des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8548312783241272 1.6106538772583008
MemoryTrain:  epoch  4, batch     4 | loss: 0.8548313des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.409422516822815 2.5912184715270996
MemoryTrain:  epoch  5, batch     0 | loss: 1.4094225des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.39237380027771 2.601086139678955
MemoryTrain:  epoch  5, batch     1 | loss: 1.3923738des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4013723134994507 2.615703582763672
MemoryTrain:  epoch  5, batch     2 | loss: 1.4013723des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4037810564041138 2.5278737545013428
MemoryTrain:  epoch  5, batch     3 | loss: 1.4037811des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8127633333206177 1.5123099088668823
MemoryTrain:  epoch  5, batch     4 | loss: 0.8127633des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3501888513565063 2.5616259574890137
MemoryTrain:  epoch  6, batch     0 | loss: 1.3501889des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3108772039413452 2.493577718734741
MemoryTrain:  epoch  6, batch     1 | loss: 1.3108772des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4229720830917358 2.5871386528015137
MemoryTrain:  epoch  6, batch     2 | loss: 1.4229721des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.339937448501587 2.5611231327056885
MemoryTrain:  epoch  6, batch     3 | loss: 1.3399374des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8348318934440613 1.5294698476791382
MemoryTrain:  epoch  6, batch     4 | loss: 0.8348319des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3184181451797485 2.517181396484375
MemoryTrain:  epoch  7, batch     0 | loss: 1.3184181des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3589675426483154 2.5399627685546875
MemoryTrain:  epoch  7, batch     1 | loss: 1.3589675des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3416714668273926 2.5257198810577393
MemoryTrain:  epoch  7, batch     2 | loss: 1.3416715des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3341896533966064 2.5737221240997314
MemoryTrain:  epoch  7, batch     3 | loss: 1.3341897des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8513603210449219 1.6004390716552734
MemoryTrain:  epoch  7, batch     4 | loss: 0.8513603des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3215270042419434 2.5358917713165283
MemoryTrain:  epoch  8, batch     0 | loss: 1.3215270des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.300887107849121 2.4450039863586426
MemoryTrain:  epoch  8, batch     1 | loss: 1.3008871des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.332798719406128 2.5255918502807617
MemoryTrain:  epoch  8, batch     2 | loss: 1.3327987des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3013607263565063 2.510430335998535
MemoryTrain:  epoch  8, batch     3 | loss: 1.3013607des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8685379028320312 1.566603183746338
MemoryTrain:  epoch  8, batch     4 | loss: 0.8685379des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3261189460754395 2.502070665359497
MemoryTrain:  epoch  9, batch     0 | loss: 1.3261189des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.267277717590332 2.407024383544922
MemoryTrain:  epoch  9, batch     1 | loss: 1.2672777des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3074761629104614 2.508239507675171
MemoryTrain:  epoch  9, batch     2 | loss: 1.3074762des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.257346272468567 2.4374465942382812
MemoryTrain:  epoch  9, batch     3 | loss: 1.2573463des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8153730630874634 1.5371906757354736
MemoryTrain:  epoch  9, batch     4 | loss: 0.8153731
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 42.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 50.78%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 49.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.08%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.30%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 44.87%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 43.32%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 41.88%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 40.52%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.21%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 42.61%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 43.57%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 45.18%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 46.18%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 46.79%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 47.70%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 48.72%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 51.07%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 52.91%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 53.84%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 54.72%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 55.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 55.85%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 56.64%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 57.27%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 57.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 57.35%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 57.81%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 58.02%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 58.41%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 58.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 58.77%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 58.41%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 57.84%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 57.58%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 57.66%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 57.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.32%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 83.26%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 83.23%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 82.76%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 82.44%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 81.74%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 81.54%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 80.68%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 80.13%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 79.60%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 79.35%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 80.12%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 80.76%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 79.97%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 79.84%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 79.71%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.42%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 78.46%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 78.27%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 77.66%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 77.53%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 77.29%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 77.02%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 76.66%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 76.32%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 75.85%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 75.58%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 75.26%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 74.94%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 74.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.14%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 73.85%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 73.68%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 73.57%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.13%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.57%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.08%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 71.70%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.17%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.76%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 72.08%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.90%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 72.07%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.25%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 72.57%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 72.64%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 72.12%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 71.65%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 71.19%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.69%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 70.19%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.88%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.53%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 70.11%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 69.73%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 69.28%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 68.91%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 68.83%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 68.00%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 67.19%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 66.90%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 66.55%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 67.98%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 67.29%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 66.28%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 66.26%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 65.66%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 65.37%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.09%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:  211 | acc: 0.00%,  total acc: 64.53%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 65.54%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 65.53%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.40%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:  263 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 68.30%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 68.09%   [EVAL] batch:  266 | acc: 25.00%,  total acc: 67.93%   [EVAL] batch:  267 | acc: 18.75%,  total acc: 67.75%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 68.23%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 68.10%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 67.67%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.45%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.09%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 66.90%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 66.69%   [EVAL] batch:  288 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 66.29%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 66.07%   [EVAL] batch:  291 | acc: 12.50%,  total acc: 65.88%   [EVAL] batch:  292 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 65.58%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.79%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 66.94%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 66.39%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 67.81%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 67.39%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 67.06%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.10%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 67.35%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 67.30%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 67.14%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 67.02%   [EVAL] batch:  369 | acc: 31.25%,  total acc: 66.93%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 66.85%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 66.77%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  375 | acc: 0.00%,  total acc: 66.54%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 66.38%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 66.22%   [EVAL] batch:  378 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 65.72%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 65.71%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 66.11%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 65.96%   [EVAL] batch:  397 | acc: 18.75%,  total acc: 65.84%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 65.66%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 65.35%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 65.04%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 64.88%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.72%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 64.93%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  425 | acc: 37.50%,  total acc: 65.60%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 65.64%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 65.60%   [EVAL] batch:  433 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 65.44%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 65.43%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 65.33%   
cur_acc:  ['0.9425', '0.7381', '0.6944', '0.7550', '0.6964', '0.6964', '0.5704']
his_acc:  ['0.9425', '0.8320', '0.7769', '0.7450', '0.7240', '0.6893', '0.6533']
Clustering into  38  clusters
Clusters:  [ 3 25 23  8  2  2 30  4  1 10 37 36  4  2 28 10  3  2  0  0  6  0 18  7
 26  2  5 29 34  6  8  3  3  1 19 14 33 20 21  1 25 31  7 22  4 17  1 14
 32 13 17  0 12  9 29  0 12 13 10 35  2  0  5 28 18 24 16  1 13  1 15 13
 27  0 21  0 11 12 22 12]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.307896614074707 2.2033746242523193
CurrentTrain: epoch  0, batch     0 | loss: 7.3078966des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.5633087158203125 2.062211751937866
CurrentTrain: epoch  0, batch     1 | loss: 7.5633087des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.777297019958496 2.1117281913757324
CurrentTrain: epoch  0, batch     2 | loss: 6.7772970des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.355497360229492 0.58842933177948
CurrentTrain: epoch  0, batch     3 | loss: 6.3554974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.969526290893555 1.8490735292434692
CurrentTrain: epoch  1, batch     0 | loss: 5.9695263des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.073816299438477 1.9703727960586548
CurrentTrain: epoch  1, batch     1 | loss: 6.0738163des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.063023090362549 1.9380877017974854
CurrentTrain: epoch  1, batch     2 | loss: 6.0630231des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.331570625305176 0.0
CurrentTrain: epoch  1, batch     3 | loss: 3.3315706des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.636169910430908 1.985920786857605
CurrentTrain: epoch  2, batch     0 | loss: 5.6361699des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.671422958374023 1.9063687324523926
CurrentTrain: epoch  2, batch     1 | loss: 5.6714230des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.360358715057373 1.8855199813842773
CurrentTrain: epoch  2, batch     2 | loss: 5.3603587des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.5407915115356445 0.6179978847503662
CurrentTrain: epoch  2, batch     3 | loss: 3.5407915des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.49241304397583 2.0892324447631836
CurrentTrain: epoch  3, batch     0 | loss: 5.4924130des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.880074501037598 1.910021185874939
CurrentTrain: epoch  3, batch     1 | loss: 4.8800745des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8734517097473145 1.9805433750152588
CurrentTrain: epoch  3, batch     2 | loss: 4.8734517des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.864605903625488 0.697786808013916
CurrentTrain: epoch  3, batch     3 | loss: 5.8646059des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.495305061340332 2.034426212310791
CurrentTrain: epoch  4, batch     0 | loss: 4.4953051des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.44536018371582 2.0095863342285156
CurrentTrain: epoch  4, batch     1 | loss: 4.4453602des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.305930137634277 1.9613356590270996
CurrentTrain: epoch  4, batch     2 | loss: 5.3059301des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.672651290893555 0.5964034199714661
CurrentTrain: epoch  4, batch     3 | loss: 6.6726513des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.908788681030273 2.0346336364746094
CurrentTrain: epoch  5, batch     0 | loss: 4.9087887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.713551998138428 1.7580089569091797
CurrentTrain: epoch  5, batch     1 | loss: 4.7135520des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.420267105102539 1.761329174041748
CurrentTrain: epoch  5, batch     2 | loss: 4.4202671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.252446174621582 0.5505208373069763
CurrentTrain: epoch  5, batch     3 | loss: 2.2524462des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.706963539123535 2.0327866077423096
CurrentTrain: epoch  6, batch     0 | loss: 4.7069635des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.409646034240723 1.8339874744415283
CurrentTrain: epoch  6, batch     1 | loss: 4.4096460des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8360157012939453 1.9072813987731934
CurrentTrain: epoch  6, batch     2 | loss: 3.8360157des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5762832164764404 0.6043456792831421
CurrentTrain: epoch  6, batch     3 | loss: 2.5762832des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.419795036315918 1.8536100387573242
CurrentTrain: epoch  7, batch     0 | loss: 4.4197950des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.889524459838867 1.7979345321655273
CurrentTrain: epoch  7, batch     1 | loss: 3.8895245des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9136221408843994 2.0254178047180176
CurrentTrain: epoch  7, batch     2 | loss: 3.9136221des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.158167839050293 0.5873782634735107
CurrentTrain: epoch  7, batch     3 | loss: 3.1581678des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.148746490478516 1.765120506286621
CurrentTrain: epoch  8, batch     0 | loss: 4.1487465des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6075024604797363 1.7038261890411377
CurrentTrain: epoch  8, batch     1 | loss: 3.6075025des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.522216796875 1.891052007675171
CurrentTrain: epoch  8, batch     2 | loss: 3.5222168des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.5275044441223145 0.5452651977539062
CurrentTrain: epoch  8, batch     3 | loss: 3.5275044des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3891844749450684 1.7646199464797974
CurrentTrain: epoch  9, batch     0 | loss: 3.3891845des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8434953689575195 1.9575632810592651
CurrentTrain: epoch  9, batch     1 | loss: 3.8434954des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.714319944381714 1.7214484214782715
CurrentTrain: epoch  9, batch     2 | loss: 3.7143199des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.505786180496216 0.5005980730056763
CurrentTrain: epoch  9, batch     3 | loss: 2.5057862
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7177445888519287 2.791076183319092
MemoryTrain:  epoch  0, batch     0 | loss: 1.7177446des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7741751670837402 2.7183878421783447
MemoryTrain:  epoch  0, batch     1 | loss: 1.7741752des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5693358182907104 2.706968307495117
MemoryTrain:  epoch  0, batch     2 | loss: 1.5693358des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5887539386749268 2.6720921993255615
MemoryTrain:  epoch  0, batch     3 | loss: 1.5887539des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.711998462677002 2.6963791847229004
MemoryTrain:  epoch  0, batch     4 | loss: 1.7119985des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2812938690185547 2.6668601036071777
MemoryTrain:  epoch  1, batch     0 | loss: 2.2812939des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5574874877929688 2.664215087890625
MemoryTrain:  epoch  1, batch     1 | loss: 1.5574875des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.708479642868042 2.7372117042541504
MemoryTrain:  epoch  1, batch     2 | loss: 1.7084796des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1498146057128906 2.714829921722412
MemoryTrain:  epoch  1, batch     3 | loss: 2.1498146des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8132433891296387 2.6637635231018066
MemoryTrain:  epoch  1, batch     4 | loss: 1.8132434des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6494263410568237 2.658421277999878
MemoryTrain:  epoch  2, batch     0 | loss: 1.6494263des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7795956134796143 2.5674476623535156
MemoryTrain:  epoch  2, batch     1 | loss: 1.7795956des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7242432832717896 2.6997222900390625
MemoryTrain:  epoch  2, batch     2 | loss: 1.7242433des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4431548118591309 2.6895499229431152
MemoryTrain:  epoch  2, batch     3 | loss: 1.4431548des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.567387342453003 2.6869609355926514
MemoryTrain:  epoch  2, batch     4 | loss: 1.5673873des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3960449695587158 2.5849356651306152
MemoryTrain:  epoch  3, batch     0 | loss: 1.3960450des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5386581420898438 2.6582396030426025
MemoryTrain:  epoch  3, batch     1 | loss: 1.5386581des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5348424911499023 2.7101235389709473
MemoryTrain:  epoch  3, batch     2 | loss: 1.5348425des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5271776914596558 2.6547744274139404
MemoryTrain:  epoch  3, batch     3 | loss: 1.5271777des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.503509759902954 2.5600671768188477
MemoryTrain:  epoch  3, batch     4 | loss: 1.5035098des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4312466382980347 2.648944616317749
MemoryTrain:  epoch  4, batch     0 | loss: 1.4312466des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4775516986846924 2.5761561393737793
MemoryTrain:  epoch  4, batch     1 | loss: 1.4775517des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.463063359260559 2.543611526489258
MemoryTrain:  epoch  4, batch     2 | loss: 1.4630634des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4946640729904175 2.6659131050109863
MemoryTrain:  epoch  4, batch     3 | loss: 1.4946641des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.464655876159668 2.5896239280700684
MemoryTrain:  epoch  4, batch     4 | loss: 1.4646559des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3883121013641357 2.605412483215332
MemoryTrain:  epoch  5, batch     0 | loss: 1.3883121des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3626395463943481 2.629824638366699
MemoryTrain:  epoch  5, batch     1 | loss: 1.3626395des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.359338641166687 2.512794017791748
MemoryTrain:  epoch  5, batch     2 | loss: 1.3593386des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4537639617919922 2.551124095916748
MemoryTrain:  epoch  5, batch     3 | loss: 1.4537640des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4239866733551025 2.62343430519104
MemoryTrain:  epoch  5, batch     4 | loss: 1.4239867des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3596971035003662 2.5305137634277344
MemoryTrain:  epoch  6, batch     0 | loss: 1.3596971des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.349098563194275 2.5500171184539795
MemoryTrain:  epoch  6, batch     1 | loss: 1.3490986des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3820821046829224 2.588845729827881
MemoryTrain:  epoch  6, batch     2 | loss: 1.3820821des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3638249635696411 2.5599541664123535
MemoryTrain:  epoch  6, batch     3 | loss: 1.3638250des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3593109846115112 2.6148929595947266
MemoryTrain:  epoch  6, batch     4 | loss: 1.3593110des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2964848279953003 2.4362034797668457
MemoryTrain:  epoch  7, batch     0 | loss: 1.2964848des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.364190697669983 2.5346932411193848
MemoryTrain:  epoch  7, batch     1 | loss: 1.3641907des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.343055009841919 2.591855525970459
MemoryTrain:  epoch  7, batch     2 | loss: 1.3430550des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2938896417617798 2.4601075649261475
MemoryTrain:  epoch  7, batch     3 | loss: 1.2938896des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3528268337249756 2.5135788917541504
MemoryTrain:  epoch  7, batch     4 | loss: 1.3528268des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3037201166152954 2.487663984298706
MemoryTrain:  epoch  8, batch     0 | loss: 1.3037201des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2803951501846313 2.4603543281555176
MemoryTrain:  epoch  8, batch     1 | loss: 1.2803952des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3353255987167358 2.474308967590332
MemoryTrain:  epoch  8, batch     2 | loss: 1.3353256des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3336427211761475 2.532576084136963
MemoryTrain:  epoch  8, batch     3 | loss: 1.3336427des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3074376583099365 2.471236228942871
MemoryTrain:  epoch  8, batch     4 | loss: 1.3074377des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.282893419265747 2.4733591079711914
MemoryTrain:  epoch  9, batch     0 | loss: 1.2828934des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2986886501312256 2.4370830059051514
MemoryTrain:  epoch  9, batch     1 | loss: 1.2986887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2979182004928589 2.4989404678344727
MemoryTrain:  epoch  9, batch     2 | loss: 1.2979182des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.31575608253479 2.402761459350586
MemoryTrain:  epoch  9, batch     3 | loss: 1.3157561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2691459655761719 2.4492034912109375
MemoryTrain:  epoch  9, batch     4 | loss: 1.2691460
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 57.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 68.02%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 65.97%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.05%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 77.87%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.98%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 75.88%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 73.96%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 72.95%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 73.13%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 73.12%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 73.07%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 72.87%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 71.45%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 71.03%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 70.20%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 69.75%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 69.22%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 68.93%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 68.40%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 67.67%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 66.85%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 68.55%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 69.34%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.84%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 68.39%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 67.95%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 67.00%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.71%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 67.51%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 66.71%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.50%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.40%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 65.10%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 64.05%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 64.04%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 64.03%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 63.95%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 63.96%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 63.84%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 63.69%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 63.62%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 63.40%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 63.43%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 64.90%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 64.57%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 64.24%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 63.92%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 63.60%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 63.28%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 63.30%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 63.20%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 63.14%   [EVAL] batch:  206 | acc: 12.50%,  total acc: 62.89%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 62.65%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 62.38%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 61.91%   [EVAL] batch:  211 | acc: 0.00%,  total acc: 61.62%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 61.59%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 62.64%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 62.89%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 62.83%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 64.78%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 65.58%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 65.29%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 65.65%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 65.42%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.21%   [EVAL] batch:  284 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 64.82%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 64.61%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 64.41%   [EVAL] batch:  288 | acc: 18.75%,  total acc: 64.25%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 64.03%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 63.81%   [EVAL] batch:  291 | acc: 12.50%,  total acc: 63.63%   [EVAL] batch:  292 | acc: 18.75%,  total acc: 63.48%   [EVAL] batch:  293 | acc: 18.75%,  total acc: 63.33%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 63.37%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 63.48%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 64.62%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 64.37%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 64.23%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 64.38%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 64.22%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 64.08%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 63.96%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 63.83%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 63.73%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 64.64%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 65.22%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  352 | acc: 6.25%,  total acc: 64.89%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 64.52%   [EVAL] batch:  355 | acc: 6.25%,  total acc: 64.36%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:  364 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 64.65%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 64.43%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 64.36%   [EVAL] batch:  370 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:  371 | acc: 31.25%,  total acc: 64.18%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 64.13%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 64.10%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 63.95%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 63.83%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 63.67%   [EVAL] batch:  378 | acc: 6.25%,  total acc: 63.52%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 63.36%   [EVAL] batch:  380 | acc: 0.00%,  total acc: 63.19%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.17%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 63.41%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 63.53%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:  389 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  391 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 63.88%   [EVAL] batch:  395 | acc: 6.25%,  total acc: 63.73%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 63.57%   [EVAL] batch:  397 | acc: 18.75%,  total acc: 63.46%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 63.33%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 63.20%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.06%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 62.90%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 62.59%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.44%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.28%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.29%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.36%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 62.52%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 62.99%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 63.11%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:  422 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 63.32%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 63.25%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  433 | acc: 31.25%,  total acc: 63.08%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 62.96%   [EVAL] batch:  438 | acc: 25.00%,  total acc: 62.87%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 62.86%   [EVAL] batch:  440 | acc: 50.00%,  total acc: 62.83%   [EVAL] batch:  441 | acc: 31.25%,  total acc: 62.75%   [EVAL] batch:  442 | acc: 31.25%,  total acc: 62.68%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 62.63%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 62.68%   [EVAL] batch:  445 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:  446 | acc: 25.00%,  total acc: 62.58%   [EVAL] batch:  447 | acc: 56.25%,  total acc: 62.57%   [EVAL] batch:  448 | acc: 62.50%,  total acc: 62.57%   [EVAL] batch:  449 | acc: 62.50%,  total acc: 62.57%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 62.71%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 62.73%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 62.79%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 62.86%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 62.92%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 62.88%   [EVAL] batch:  458 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 62.81%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 62.73%   [EVAL] batch:  461 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  467 | acc: 93.75%,  total acc: 63.13%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:  478 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 63.88%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 63.86%   [EVAL] batch:  482 | acc: 18.75%,  total acc: 63.77%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 63.70%   [EVAL] batch:  484 | acc: 37.50%,  total acc: 63.65%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 63.61%   [EVAL] batch:  486 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 63.55%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.74%   [EVAL] batch:  492 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  494 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:  497 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:  498 | acc: 25.00%,  total acc: 63.50%   [EVAL] batch:  499 | acc: 25.00%,  total acc: 63.42%   
cur_acc:  ['0.9425', '0.7381', '0.6944', '0.7550', '0.6964', '0.6964', '0.5704', '0.6597']
his_acc:  ['0.9425', '0.8320', '0.7769', '0.7450', '0.7240', '0.6893', '0.6533', '0.6342']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 1 0 3 3 1 0 2 2]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.371790885925293 1.9524296522140503
CurrentTrain: epoch  0, batch     0 | loss: 11.3717909des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.015419960021973 2.1193032264709473
CurrentTrain: epoch  0, batch     1 | loss: 11.0154200des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.658889770507812 1.8405230045318604
CurrentTrain: epoch  0, batch     2 | loss: 10.6588898des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  11.15416145324707 2.2623138427734375
CurrentTrain: epoch  0, batch     3 | loss: 11.1541615des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.887228012084961 1.932140827178955
CurrentTrain: epoch  0, batch     4 | loss: 10.8872280des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.20902156829834 2.0497002601623535
CurrentTrain: epoch  0, batch     5 | loss: 10.2090216des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.561519622802734 1.957972764968872
CurrentTrain: epoch  0, batch     6 | loss: 10.5615196des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.210484504699707 1.9024131298065186
CurrentTrain: epoch  0, batch     7 | loss: 10.2104845des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.57172679901123 2.122526168823242
CurrentTrain: epoch  0, batch     8 | loss: 10.5717268des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.006065368652344 2.083008289337158
CurrentTrain: epoch  0, batch     9 | loss: 10.0060654des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.569249153137207 2.128404378890991
CurrentTrain: epoch  0, batch    10 | loss: 10.5692492des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.691590309143066 2.0896639823913574
CurrentTrain: epoch  0, batch    11 | loss: 9.6915903des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.677961349487305 1.785862684249878
CurrentTrain: epoch  0, batch    12 | loss: 9.6779613des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.682548522949219 2.0680370330810547
CurrentTrain: epoch  0, batch    13 | loss: 9.6825485des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  10.061201095581055 1.9239636659622192
CurrentTrain: epoch  0, batch    14 | loss: 10.0612011des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.195395469665527 2.0578274726867676
CurrentTrain: epoch  0, batch    15 | loss: 9.1953955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.63412857055664 2.1033926010131836
CurrentTrain: epoch  0, batch    16 | loss: 9.6341286des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.368249893188477 1.8761076927185059
CurrentTrain: epoch  0, batch    17 | loss: 9.3682499des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.716928482055664 1.8010348081588745
CurrentTrain: epoch  0, batch    18 | loss: 9.7169285des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.9479398727417 2.1056618690490723
CurrentTrain: epoch  0, batch    19 | loss: 8.9479399des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.947073936462402 1.8294768333435059
CurrentTrain: epoch  0, batch    20 | loss: 8.9470739des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.390833854675293 1.8485347032546997
CurrentTrain: epoch  0, batch    21 | loss: 9.3908339des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.749284744262695 1.7098112106323242
CurrentTrain: epoch  0, batch    22 | loss: 8.7492847des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.984749794006348 1.9623273611068726
CurrentTrain: epoch  0, batch    23 | loss: 8.9847498des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.638594627380371 2.0693936347961426
CurrentTrain: epoch  0, batch    24 | loss: 9.6385946des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.841245651245117 1.6822062730789185
CurrentTrain: epoch  0, batch    25 | loss: 8.8412457des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.462617874145508 2.0273690223693848
CurrentTrain: epoch  0, batch    26 | loss: 9.4626179des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.581672668457031 2.0627946853637695
CurrentTrain: epoch  0, batch    27 | loss: 9.5816727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.172709465026855 1.857635498046875
CurrentTrain: epoch  0, batch    28 | loss: 9.1727095des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.970006942749023 1.709334135055542
CurrentTrain: epoch  0, batch    29 | loss: 8.9700069des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.89604663848877 1.990642786026001
CurrentTrain: epoch  0, batch    30 | loss: 8.8960466des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.148911476135254 2.134723663330078
CurrentTrain: epoch  0, batch    31 | loss: 9.1489115des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.415292739868164 1.9836339950561523
CurrentTrain: epoch  0, batch    32 | loss: 9.4152927des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.963068008422852 2.036952495574951
CurrentTrain: epoch  0, batch    33 | loss: 8.9630680des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.648584365844727 2.0028624534606934
CurrentTrain: epoch  0, batch    34 | loss: 8.6485844des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.130064010620117 1.8117237091064453
CurrentTrain: epoch  0, batch    35 | loss: 8.1300640des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.935195922851562 1.564399242401123
CurrentTrain: epoch  0, batch    36 | loss: 8.9351959des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.532136917114258 1.944216012954712
CurrentTrain: epoch  0, batch    37 | loss: 8.5321369des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.851601600646973 2.0073728561401367
CurrentTrain: epoch  0, batch    38 | loss: 8.8516016des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.969244956970215 1.9500443935394287
CurrentTrain: epoch  0, batch    39 | loss: 8.9692450des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.581220626831055 1.9022436141967773
CurrentTrain: epoch  0, batch    40 | loss: 8.5812206des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.548827171325684 1.8721566200256348
CurrentTrain: epoch  0, batch    41 | loss: 8.5488272des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.365582466125488 1.7823193073272705
CurrentTrain: epoch  0, batch    42 | loss: 8.3655825des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.641122817993164 1.755305290222168
CurrentTrain: epoch  0, batch    43 | loss: 8.6411228des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.21605396270752 1.7308948040008545
CurrentTrain: epoch  0, batch    44 | loss: 8.2160540des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.563328742980957 1.7396292686462402
CurrentTrain: epoch  0, batch    45 | loss: 8.5633287des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.68033218383789 1.8445779085159302
CurrentTrain: epoch  0, batch    46 | loss: 8.6803322des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.111042022705078 1.8678739070892334
CurrentTrain: epoch  0, batch    47 | loss: 8.1110420des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.740348815917969 1.813173532485962
CurrentTrain: epoch  0, batch    48 | loss: 8.7403488des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.023673057556152 1.9081881046295166
CurrentTrain: epoch  0, batch    49 | loss: 8.0236731des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.348052024841309 1.4764811992645264
CurrentTrain: epoch  0, batch    50 | loss: 9.3480520des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.5432448387146 1.6585438251495361
CurrentTrain: epoch  0, batch    51 | loss: 7.5432448des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.298206329345703 1.830163598060608
CurrentTrain: epoch  0, batch    52 | loss: 8.2982063des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.272027969360352 1.5728414058685303
CurrentTrain: epoch  0, batch    53 | loss: 8.2720280des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.635234832763672 1.867311954498291
CurrentTrain: epoch  0, batch    54 | loss: 9.6352348des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.282833099365234 1.8286349773406982
CurrentTrain: epoch  0, batch    55 | loss: 9.2828331des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.590676784515381 1.7849292755126953
CurrentTrain: epoch  0, batch    56 | loss: 7.5906768des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.564345359802246 1.5575604438781738
CurrentTrain: epoch  0, batch    57 | loss: 7.5643454des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.19189739227295 1.789510726928711
CurrentTrain: epoch  0, batch    58 | loss: 8.1918974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.567095756530762 1.5255016088485718
CurrentTrain: epoch  0, batch    59 | loss: 8.5670958des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.298140525817871 1.7590875625610352
CurrentTrain: epoch  0, batch    60 | loss: 8.2981405des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.857266426086426 1.665529727935791
CurrentTrain: epoch  0, batch    61 | loss: 7.8572664des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.3409423828125 1.421335220336914
CurrentTrain: epoch  0, batch    62 | loss: 7.3409424des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.311450004577637 1.6843383312225342
CurrentTrain: epoch  1, batch     0 | loss: 9.3114500des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.233882427215576 1.6929988861083984
CurrentTrain: epoch  1, batch     1 | loss: 7.2338824des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.157867431640625 1.8096799850463867
CurrentTrain: epoch  1, batch     2 | loss: 8.1578674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.584499835968018 1.5428991317749023
CurrentTrain: epoch  1, batch     3 | loss: 7.5844998des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.157846927642822 1.710131287574768
CurrentTrain: epoch  1, batch     4 | loss: 7.1578469des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.000533103942871 1.5195987224578857
CurrentTrain: epoch  1, batch     5 | loss: 7.0005331des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.080015182495117 1.619681715965271
CurrentTrain: epoch  1, batch     6 | loss: 8.0800152des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.923318862915039 1.6681926250457764
CurrentTrain: epoch  1, batch     7 | loss: 7.9233189des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.947123050689697 1.8112313747406006
CurrentTrain: epoch  1, batch     8 | loss: 7.9471231des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.088701248168945 1.19852876663208
CurrentTrain: epoch  1, batch     9 | loss: 7.0887012des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.624504566192627 1.5722135305404663
CurrentTrain: epoch  1, batch    10 | loss: 7.6245046des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.571220397949219 1.7866828441619873
CurrentTrain: epoch  1, batch    11 | loss: 8.5712204des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.617071628570557 1.6052350997924805
CurrentTrain: epoch  1, batch    12 | loss: 7.6170716des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.711164474487305 1.5886693000793457
CurrentTrain: epoch  1, batch    13 | loss: 7.7111645des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.089324951171875 1.8614375591278076
CurrentTrain: epoch  1, batch    14 | loss: 8.0893250des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.684256076812744 1.540218710899353
CurrentTrain: epoch  1, batch    15 | loss: 7.6842561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.218105792999268 1.657028317451477
CurrentTrain: epoch  1, batch    16 | loss: 7.2181058des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.610138893127441 1.6935430765151978
CurrentTrain: epoch  1, batch    17 | loss: 8.6101389des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.398536205291748 1.698822021484375
CurrentTrain: epoch  1, batch    18 | loss: 7.3985362des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.345459938049316 1.6625328063964844
CurrentTrain: epoch  1, batch    19 | loss: 7.3454599des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.607986927032471 1.7196567058563232
CurrentTrain: epoch  1, batch    20 | loss: 7.6079869des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.509167194366455 1.714493989944458
CurrentTrain: epoch  1, batch    21 | loss: 7.5091672des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.175908088684082 1.6378729343414307
CurrentTrain: epoch  1, batch    22 | loss: 7.1759081des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.589618682861328 1.6088576316833496
CurrentTrain: epoch  1, batch    23 | loss: 7.5896187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.789090156555176 1.7056241035461426
CurrentTrain: epoch  1, batch    24 | loss: 7.7890902des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.968113899230957 1.6260945796966553
CurrentTrain: epoch  1, batch    25 | loss: 6.9681139des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.013513565063477 1.5517957210540771
CurrentTrain: epoch  1, batch    26 | loss: 8.0135136des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.618523120880127 1.3138058185577393
CurrentTrain: epoch  1, batch    27 | loss: 6.6185231des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.5362677574157715 1.4007923603057861
CurrentTrain: epoch  1, batch    28 | loss: 6.5362678des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.143293380737305 1.6877057552337646
CurrentTrain: epoch  1, batch    29 | loss: 7.1432934des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.316015720367432 1.6242725849151611
CurrentTrain: epoch  1, batch    30 | loss: 7.3160157des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.523272514343262 1.529435634613037
CurrentTrain: epoch  1, batch    31 | loss: 7.5232725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.807941913604736 1.3058347702026367
CurrentTrain: epoch  1, batch    32 | loss: 6.8079419des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.989721775054932 1.5181230306625366
CurrentTrain: epoch  1, batch    33 | loss: 6.9897218des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.257117748260498 1.529029130935669
CurrentTrain: epoch  1, batch    34 | loss: 7.2571177des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.501272678375244 1.6312795877456665
CurrentTrain: epoch  1, batch    35 | loss: 7.5012727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.084625720977783 1.7226660251617432
CurrentTrain: epoch  1, batch    36 | loss: 7.0846257des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.932854175567627 1.5772507190704346
CurrentTrain: epoch  1, batch    37 | loss: 6.9328542des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.590896129608154 1.769019365310669
CurrentTrain: epoch  1, batch    38 | loss: 7.5908961des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.951082229614258 1.1033304929733276
CurrentTrain: epoch  1, batch    39 | loss: 5.9510822des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.417815685272217 1.4460035562515259
CurrentTrain: epoch  1, batch    40 | loss: 7.4178157des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.733154296875 1.5928633213043213
CurrentTrain: epoch  1, batch    41 | loss: 7.7331543des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.467202186584473 1.544009804725647
CurrentTrain: epoch  1, batch    42 | loss: 6.4672022des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.159755229949951 1.6125577688217163
CurrentTrain: epoch  1, batch    43 | loss: 7.1597552des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.995123386383057 1.554861307144165
CurrentTrain: epoch  1, batch    44 | loss: 7.9951234des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.830560684204102 1.5940535068511963
CurrentTrain: epoch  1, batch    45 | loss: 6.8305607des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.707859039306641 1.340999722480774
CurrentTrain: epoch  1, batch    46 | loss: 6.7078590des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.83644962310791 1.4133150577545166
CurrentTrain: epoch  1, batch    47 | loss: 6.8364496des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.848159313201904 1.3961429595947266
CurrentTrain: epoch  1, batch    48 | loss: 7.8481593des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.581961154937744 1.4574329853057861
CurrentTrain: epoch  1, batch    49 | loss: 6.5819612des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.5 1.435020923614502
CurrentTrain: epoch  1, batch    50 | loss: 6.5000000des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.7845258712768555 1.097264289855957
CurrentTrain: epoch  1, batch    51 | loss: 5.7845259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.769193172454834 1.2853574752807617
CurrentTrain: epoch  1, batch    52 | loss: 6.7691932des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.905831813812256 1.3058230876922607
CurrentTrain: epoch  1, batch    53 | loss: 5.9058318des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.345727920532227 1.3974895477294922
CurrentTrain: epoch  1, batch    54 | loss: 7.3457279des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.187610626220703 1.2333297729492188
CurrentTrain: epoch  1, batch    55 | loss: 6.1876106des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.9577226638793945 1.4633302688598633
CurrentTrain: epoch  1, batch    56 | loss: 6.9577227des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.987067222595215 1.4151328802108765
CurrentTrain: epoch  1, batch    57 | loss: 6.9870672des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.606102466583252 1.3409979343414307
CurrentTrain: epoch  1, batch    58 | loss: 6.6061025des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2967681884765625 1.385848879814148
CurrentTrain: epoch  1, batch    59 | loss: 6.2967682des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.20963716506958 1.4394807815551758
CurrentTrain: epoch  1, batch    60 | loss: 6.2096372des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.102980136871338 1.3949391841888428
CurrentTrain: epoch  1, batch    61 | loss: 7.1029801des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.313748359680176 1.0675886869430542
CurrentTrain: epoch  1, batch    62 | loss: 7.3137484des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.330208778381348 1.2460092306137085
CurrentTrain: epoch  2, batch     0 | loss: 6.3302088des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.912291526794434 1.3675124645233154
CurrentTrain: epoch  2, batch     1 | loss: 5.9122915des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.817536354064941 1.319137454032898
CurrentTrain: epoch  2, batch     2 | loss: 5.8175364des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.914708137512207 1.2891182899475098
CurrentTrain: epoch  2, batch     3 | loss: 5.9147081des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.911091327667236 1.2493435144424438
CurrentTrain: epoch  2, batch     4 | loss: 5.9110913des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.633817195892334 1.2426482439041138
CurrentTrain: epoch  2, batch     5 | loss: 6.6338172des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.9587860107421875 1.2363299131393433
CurrentTrain: epoch  2, batch     6 | loss: 5.9587860des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.017831802368164 1.508591890335083
CurrentTrain: epoch  2, batch     7 | loss: 7.0178318des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.4313435554504395 1.0964126586914062
CurrentTrain: epoch  2, batch     8 | loss: 6.4313436des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.639348030090332 1.2230076789855957
CurrentTrain: epoch  2, batch     9 | loss: 5.6393480des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.167076587677002 1.2761335372924805
CurrentTrain: epoch  2, batch    10 | loss: 6.1670766des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.962006568908691 1.2822370529174805
CurrentTrain: epoch  2, batch    11 | loss: 5.9620066des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.976152420043945 1.1876163482666016
CurrentTrain: epoch  2, batch    12 | loss: 5.9761524des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.059418678283691 1.2817126512527466
CurrentTrain: epoch  2, batch    13 | loss: 6.0594187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.3688130378723145 1.1531517505645752
CurrentTrain: epoch  2, batch    14 | loss: 6.3688130des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.791206359863281 1.2676787376403809
CurrentTrain: epoch  2, batch    15 | loss: 5.7912064des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.134999752044678 1.1383388042449951
CurrentTrain: epoch  2, batch    16 | loss: 6.1349998des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.254356861114502 1.328171968460083
CurrentTrain: epoch  2, batch    17 | loss: 7.2543569des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.732752799987793 1.0515427589416504
CurrentTrain: epoch  2, batch    18 | loss: 5.7327528des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.315732002258301 1.3310656547546387
CurrentTrain: epoch  2, batch    19 | loss: 6.3157320des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.102982997894287 0.8975611329078674
CurrentTrain: epoch  2, batch    20 | loss: 7.1029830des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.49432373046875 1.4126458168029785
CurrentTrain: epoch  2, batch    21 | loss: 6.4943237des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.565073490142822 1.3043299913406372
CurrentTrain: epoch  2, batch    22 | loss: 5.5650735des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.125546932220459 1.2057772874832153
CurrentTrain: epoch  2, batch    23 | loss: 6.1255469des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.528295516967773 1.1345341205596924
CurrentTrain: epoch  2, batch    24 | loss: 5.5282955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.679708003997803 0.7585697174072266
CurrentTrain: epoch  2, batch    25 | loss: 5.6797080des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.385867118835449 1.1792410612106323
CurrentTrain: epoch  2, batch    26 | loss: 5.3858671des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.914516448974609 1.3109338283538818
CurrentTrain: epoch  2, batch    27 | loss: 5.9145164des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.430472373962402 0.9369535446166992
CurrentTrain: epoch  2, batch    28 | loss: 5.4304724des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.297633171081543 1.3762109279632568
CurrentTrain: epoch  2, batch    29 | loss: 6.2976332des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.965693473815918 1.28639817237854
CurrentTrain: epoch  2, batch    30 | loss: 5.9656935des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.320369243621826 1.275956392288208
CurrentTrain: epoch  2, batch    31 | loss: 6.3203692des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.831036567687988 0.9843963384628296
CurrentTrain: epoch  2, batch    32 | loss: 5.8310366des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.05473518371582 1.0989385843276978
CurrentTrain: epoch  2, batch    33 | loss: 6.0547352des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.825899124145508 1.169187068939209
CurrentTrain: epoch  2, batch    34 | loss: 6.8258991des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.003089904785156 1.1334688663482666
CurrentTrain: epoch  2, batch    35 | loss: 6.0030899des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.488922595977783 1.357039451599121
CurrentTrain: epoch  2, batch    36 | loss: 6.4889226des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.887332439422607 1.2231918573379517
CurrentTrain: epoch  2, batch    37 | loss: 5.8873324des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.00144100189209 1.174259066581726
CurrentTrain: epoch  2, batch    38 | loss: 6.0014410des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.323106288909912 1.3496372699737549
CurrentTrain: epoch  2, batch    39 | loss: 6.3231063des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.984805583953857 1.2693595886230469
CurrentTrain: epoch  2, batch    40 | loss: 6.9848056des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.569010257720947 1.1703938245773315
CurrentTrain: epoch  2, batch    41 | loss: 5.5690103des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.699556350708008 1.077822208404541
CurrentTrain: epoch  2, batch    42 | loss: 5.6995564des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.055252552032471 1.115128517150879
CurrentTrain: epoch  2, batch    43 | loss: 6.0552526des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.512211322784424 1.0613442659378052
CurrentTrain: epoch  2, batch    44 | loss: 5.5122113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.312931060791016 0.8388301134109497
CurrentTrain: epoch  2, batch    45 | loss: 5.3129311des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.141670227050781 1.1778420209884644
CurrentTrain: epoch  2, batch    46 | loss: 6.1416702des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.493450164794922 0.9749370813369751
CurrentTrain: epoch  2, batch    47 | loss: 5.4934502des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.17155122756958 1.1535186767578125
CurrentTrain: epoch  2, batch    48 | loss: 6.1715512des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.925441265106201 0.8700931072235107
CurrentTrain: epoch  2, batch    49 | loss: 5.9254413des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.641194820404053 0.9932951331138611
CurrentTrain: epoch  2, batch    50 | loss: 5.6411948des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.858792781829834 1.0198907852172852
CurrentTrain: epoch  2, batch    51 | loss: 5.8587928des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.890398979187012 1.0651856660842896
CurrentTrain: epoch  2, batch    52 | loss: 5.8903990des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.8136444091796875 1.0220227241516113
CurrentTrain: epoch  2, batch    53 | loss: 5.8136444des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.0388593673706055 0.9460697770118713
CurrentTrain: epoch  2, batch    54 | loss: 6.0388594des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.1165642738342285 1.067103385925293
CurrentTrain: epoch  2, batch    55 | loss: 6.1165643des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.761500358581543 1.1048452854156494
CurrentTrain: epoch  2, batch    56 | loss: 5.7615004des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.0074782371521 1.1059296131134033
CurrentTrain: epoch  2, batch    57 | loss: 6.0074782des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.366184234619141 0.9908993244171143
CurrentTrain: epoch  2, batch    58 | loss: 5.3661842des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.545681953430176 1.1606335639953613
CurrentTrain: epoch  2, batch    59 | loss: 5.5456820des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.256232261657715 0.9694855809211731
CurrentTrain: epoch  2, batch    60 | loss: 5.2562323des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.1276984214782715 0.9544508457183838
CurrentTrain: epoch  2, batch    61 | loss: 5.1276984des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.139769554138184 0.6517085433006287
CurrentTrain: epoch  2, batch    62 | loss: 5.1397696des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.337278842926025 0.9670687317848206
CurrentTrain: epoch  3, batch     0 | loss: 5.3372788des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.270808696746826 0.9074010252952576
CurrentTrain: epoch  3, batch     1 | loss: 5.2708087des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.264030456542969 0.9121207594871521
CurrentTrain: epoch  3, batch     2 | loss: 5.2640305des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.029229164123535 0.9173985123634338
CurrentTrain: epoch  3, batch     3 | loss: 5.0292292des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.298352241516113 0.9029241800308228
CurrentTrain: epoch  3, batch     4 | loss: 5.2983522des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.842789649963379 1.1650696992874146
CurrentTrain: epoch  3, batch     5 | loss: 5.8427896des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.307420253753662 1.0574331283569336
CurrentTrain: epoch  3, batch     6 | loss: 5.3074203des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5131120681762695 1.0178701877593994
CurrentTrain: epoch  3, batch     7 | loss: 5.5131121des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.378495216369629 0.8595997095108032
CurrentTrain: epoch  3, batch     8 | loss: 5.3784952des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.553439617156982 0.9470703601837158
CurrentTrain: epoch  3, batch     9 | loss: 5.5534396des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.720654487609863 0.9729962348937988
CurrentTrain: epoch  3, batch    10 | loss: 5.7206545des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.20293664932251 0.821695864200592
CurrentTrain: epoch  3, batch    11 | loss: 5.2029366des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.597062110900879 1.1007959842681885
CurrentTrain: epoch  3, batch    12 | loss: 5.5970621des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.043300628662109 0.7681249380111694
CurrentTrain: epoch  3, batch    13 | loss: 5.0433006des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.2106828689575195 0.9778658151626587
CurrentTrain: epoch  3, batch    14 | loss: 6.2106829des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.947185516357422 0.9525195360183716
CurrentTrain: epoch  3, batch    15 | loss: 5.9471855des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.874127388000488 1.0937954187393188
CurrentTrain: epoch  3, batch    16 | loss: 5.8741274des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.264688968658447 0.7681723833084106
CurrentTrain: epoch  3, batch    17 | loss: 5.2646890des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.376568794250488 0.8795300722122192
CurrentTrain: epoch  3, batch    18 | loss: 5.3765688des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.201600551605225 0.9816538095474243
CurrentTrain: epoch  3, batch    19 | loss: 5.2016006des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.172972679138184 0.9793328046798706
CurrentTrain: epoch  3, batch    20 | loss: 5.1729727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.754384994506836 0.8846223950386047
CurrentTrain: epoch  3, batch    21 | loss: 5.7543850des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.277397155761719 1.0095757246017456
CurrentTrain: epoch  3, batch    22 | loss: 5.2773972des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.659639358520508 0.7435762882232666
CurrentTrain: epoch  3, batch    23 | loss: 5.6596394des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.228350639343262 0.8772122859954834
CurrentTrain: epoch  3, batch    24 | loss: 5.2283506des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.354135036468506 0.9680360555648804
CurrentTrain: epoch  3, batch    25 | loss: 5.3541350des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.097677707672119 0.8971395492553711
CurrentTrain: epoch  3, batch    26 | loss: 5.0976777des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.403892993927002 0.9711788296699524
CurrentTrain: epoch  3, batch    27 | loss: 5.4038930des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.721409797668457 0.8478797078132629
CurrentTrain: epoch  3, batch    28 | loss: 5.7214098des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.336632251739502 0.8877862095832825
CurrentTrain: epoch  3, batch    29 | loss: 5.3366323des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.970477104187012 0.8666371703147888
CurrentTrain: epoch  3, batch    30 | loss: 4.9704771des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.12009334564209 1.0053699016571045
CurrentTrain: epoch  3, batch    31 | loss: 6.1200933des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.24062967300415 0.8443594574928284
CurrentTrain: epoch  3, batch    32 | loss: 5.2406297des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.425069332122803 0.8408234119415283
CurrentTrain: epoch  3, batch    33 | loss: 5.4250693des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.951634883880615 0.9813962578773499
CurrentTrain: epoch  3, batch    34 | loss: 5.9516349des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.934410095214844 0.8137234449386597
CurrentTrain: epoch  3, batch    35 | loss: 4.9344101des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.2486724853515625 0.8558231592178345
CurrentTrain: epoch  3, batch    36 | loss: 5.2486725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.902860641479492 0.7841727137565613
CurrentTrain: epoch  3, batch    37 | loss: 4.9028606des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.208401679992676 0.8771471977233887
CurrentTrain: epoch  3, batch    38 | loss: 5.2084017des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.779496192932129 0.6166002750396729
CurrentTrain: epoch  3, batch    39 | loss: 4.7794962des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.272525310516357 0.9264212846755981
CurrentTrain: epoch  3, batch    40 | loss: 5.2725253des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.597996711730957 0.9176369309425354
CurrentTrain: epoch  3, batch    41 | loss: 5.5979967des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.481264114379883 0.7842162847518921
CurrentTrain: epoch  3, batch    42 | loss: 5.4812641des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.936309337615967 0.7248925566673279
CurrentTrain: epoch  3, batch    43 | loss: 4.9363093des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.9933671951293945 0.8000736832618713
CurrentTrain: epoch  3, batch    44 | loss: 4.9933672des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.682797431945801 1.0790939331054688
CurrentTrain: epoch  3, batch    45 | loss: 5.6827974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7923903465271 0.6626459360122681
CurrentTrain: epoch  3, batch    46 | loss: 4.7923903des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.203073024749756 0.927901566028595
CurrentTrain: epoch  3, batch    47 | loss: 5.2030730des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.258779525756836 0.9664252996444702
CurrentTrain: epoch  3, batch    48 | loss: 5.2587795des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.656688690185547 0.7060887813568115
CurrentTrain: epoch  3, batch    49 | loss: 4.6566887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.699188709259033 1.0271881818771362
CurrentTrain: epoch  3, batch    50 | loss: 5.6991887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.827465534210205 0.7722471952438354
CurrentTrain: epoch  3, batch    51 | loss: 4.8274655des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.992302894592285 0.8235605359077454
CurrentTrain: epoch  3, batch    52 | loss: 4.9923029des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.009912490844727 0.8842244148254395
CurrentTrain: epoch  3, batch    53 | loss: 5.0099125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.097250938415527 0.8716011047363281
CurrentTrain: epoch  3, batch    54 | loss: 5.0972509des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.945362091064453 0.665428638458252
CurrentTrain: epoch  3, batch    55 | loss: 4.9453621des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.96796178817749 0.7113043069839478
CurrentTrain: epoch  3, batch    56 | loss: 4.9679618des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.624978542327881 0.678565263748169
CurrentTrain: epoch  3, batch    57 | loss: 4.6249785des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.005509376525879 0.8047597408294678
CurrentTrain: epoch  3, batch    58 | loss: 5.0055094des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.082848072052002 0.7810186147689819
CurrentTrain: epoch  3, batch    59 | loss: 5.0828481des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.751152038574219 0.7522435188293457
CurrentTrain: epoch  3, batch    60 | loss: 5.7511520des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.266966819763184 0.9130755662918091
CurrentTrain: epoch  3, batch    61 | loss: 5.2669668des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.415323734283447 0.7748987674713135
CurrentTrain: epoch  3, batch    62 | loss: 5.4153237des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.131103992462158 0.9515848755836487
CurrentTrain: epoch  4, batch     0 | loss: 5.1311040des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.764896869659424 0.7636358737945557
CurrentTrain: epoch  4, batch     1 | loss: 4.7648969des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.405322074890137 0.8083329200744629
CurrentTrain: epoch  4, batch     2 | loss: 5.4053221des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.460766315460205 0.7874695062637329
CurrentTrain: epoch  4, batch     3 | loss: 5.4607663des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.946284294128418 0.6829561591148376
CurrentTrain: epoch  4, batch     4 | loss: 4.9462843des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.053570747375488 0.6839079856872559
CurrentTrain: epoch  4, batch     5 | loss: 5.0535707des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.9327392578125 0.8142275214195251
CurrentTrain: epoch  4, batch     6 | loss: 4.9327393des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.016212463378906 0.7445812225341797
CurrentTrain: epoch  4, batch     7 | loss: 5.0162125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.122535705566406 0.776758074760437
CurrentTrain: epoch  4, batch     8 | loss: 5.1225357des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.124222278594971 0.8193161487579346
CurrentTrain: epoch  4, batch     9 | loss: 5.1242223des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.778614521026611 0.6751657128334045
CurrentTrain: epoch  4, batch    10 | loss: 4.7786145des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.737089157104492 0.6319047212600708
CurrentTrain: epoch  4, batch    11 | loss: 4.7370892des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.792387962341309 0.7474040985107422
CurrentTrain: epoch  4, batch    12 | loss: 4.7923880des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.060329914093018 0.9050705432891846
CurrentTrain: epoch  4, batch    13 | loss: 5.0603299des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.902459621429443 0.640782356262207
CurrentTrain: epoch  4, batch    14 | loss: 4.9024596des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.869701862335205 0.7350844740867615
CurrentTrain: epoch  4, batch    15 | loss: 4.8697019des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.842538833618164 0.7755579948425293
CurrentTrain: epoch  4, batch    16 | loss: 4.8425388des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.905187129974365 0.7727915048599243
CurrentTrain: epoch  4, batch    17 | loss: 4.9051871des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.839375972747803 0.8388503193855286
CurrentTrain: epoch  4, batch    18 | loss: 4.8393760des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.85294246673584 0.7237972021102905
CurrentTrain: epoch  4, batch    19 | loss: 4.8529425des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.702381610870361 0.7303110361099243
CurrentTrain: epoch  4, batch    20 | loss: 4.7023816des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.538547515869141 0.5830157399177551
CurrentTrain: epoch  4, batch    21 | loss: 4.5385475des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.152148723602295 0.7550052404403687
CurrentTrain: epoch  4, batch    22 | loss: 5.1521487des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.82732629776001 0.8068548440933228
CurrentTrain: epoch  4, batch    23 | loss: 4.8273263des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.798166275024414 0.8544785380363464
CurrentTrain: epoch  4, batch    24 | loss: 4.7981663des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.774812698364258 0.759086012840271
CurrentTrain: epoch  4, batch    25 | loss: 4.7748127des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6621809005737305 0.6357985734939575
CurrentTrain: epoch  4, batch    26 | loss: 4.6621809des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.805231094360352 0.7744384407997131
CurrentTrain: epoch  4, batch    27 | loss: 4.8052311des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.81974983215332 0.7312217950820923
CurrentTrain: epoch  4, batch    28 | loss: 4.8197498des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.606542110443115 0.5170325636863708
CurrentTrain: epoch  4, batch    29 | loss: 4.6065421des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.825333118438721 0.7481248378753662
CurrentTrain: epoch  4, batch    30 | loss: 4.8253331des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.637280464172363 0.5892544984817505
CurrentTrain: epoch  4, batch    31 | loss: 4.6372805des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.907696723937988 0.7507796287536621
CurrentTrain: epoch  4, batch    32 | loss: 4.9076967des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.772221565246582 0.614012598991394
CurrentTrain: epoch  4, batch    33 | loss: 4.7722216des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.651623725891113 0.5303109884262085
CurrentTrain: epoch  4, batch    34 | loss: 4.6516237des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.756594181060791 0.6697380542755127
CurrentTrain: epoch  4, batch    35 | loss: 4.7565942des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.532634258270264 0.5983757376670837
CurrentTrain: epoch  4, batch    36 | loss: 4.5326343des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.001017093658447 0.718834638595581
CurrentTrain: epoch  4, batch    37 | loss: 5.0010171des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.579624652862549 0.6119452714920044
CurrentTrain: epoch  4, batch    38 | loss: 4.5796247des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.390486717224121 0.6429030895233154
CurrentTrain: epoch  4, batch    39 | loss: 5.3904867des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.744051933288574 0.748571515083313
CurrentTrain: epoch  4, batch    40 | loss: 4.7440519des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.676592826843262 0.6554285287857056
CurrentTrain: epoch  4, batch    41 | loss: 4.6765928des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.8031182289123535 0.6768732070922852
CurrentTrain: epoch  4, batch    42 | loss: 4.8031182des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.723611354827881 0.7123234272003174
CurrentTrain: epoch  4, batch    43 | loss: 4.7236114des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6103835105896 0.5654478669166565
CurrentTrain: epoch  4, batch    44 | loss: 4.6103835des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.792123317718506 0.609467625617981
CurrentTrain: epoch  4, batch    45 | loss: 4.7921233des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.731296062469482 0.6419456601142883
CurrentTrain: epoch  4, batch    46 | loss: 4.7312961des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.53563117980957 0.5172591209411621
CurrentTrain: epoch  4, batch    47 | loss: 4.5356312des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.694387912750244 0.6167154312133789
CurrentTrain: epoch  4, batch    48 | loss: 4.6943879des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.669688701629639 0.5445050001144409
CurrentTrain: epoch  4, batch    49 | loss: 4.6696887des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.530775547027588 0.5649856328964233
CurrentTrain: epoch  4, batch    50 | loss: 4.5307755des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6197829246521 0.6777584552764893
CurrentTrain: epoch  4, batch    51 | loss: 4.6197829des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.698547840118408 0.6426385641098022
CurrentTrain: epoch  4, batch    52 | loss: 4.6985478des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.699143886566162 0.6980203986167908
CurrentTrain: epoch  4, batch    53 | loss: 4.6991439des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.163621425628662 0.7007665038108826
CurrentTrain: epoch  4, batch    54 | loss: 5.1636214des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7606120109558105 0.699143648147583
CurrentTrain: epoch  4, batch    55 | loss: 4.7606120des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.574750900268555 0.6545677185058594
CurrentTrain: epoch  4, batch    56 | loss: 4.5747509des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.499194622039795 0.6192677617073059
CurrentTrain: epoch  4, batch    57 | loss: 4.4991946des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.886796474456787 0.7624753713607788
CurrentTrain: epoch  4, batch    58 | loss: 4.8867965des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.567436695098877 0.651680588722229
CurrentTrain: epoch  4, batch    59 | loss: 4.5674367des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.798503875732422 0.5221383571624756
CurrentTrain: epoch  4, batch    60 | loss: 4.7985039des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.502294540405273 0.6302441358566284
CurrentTrain: epoch  4, batch    61 | loss: 4.5022945des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.359284400939941 0.3122963607311249
CurrentTrain: epoch  4, batch    62 | loss: 4.3592844des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.594956874847412 0.5886242389678955
CurrentTrain: epoch  5, batch     0 | loss: 4.5949569des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.54115104675293 0.632118284702301
CurrentTrain: epoch  5, batch     1 | loss: 4.5411510des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.532718658447266 0.5332274436950684
CurrentTrain: epoch  5, batch     2 | loss: 4.5327187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5657572746276855 0.6455186605453491
CurrentTrain: epoch  5, batch     3 | loss: 4.5657573des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.586307525634766 0.5855433940887451
CurrentTrain: epoch  5, batch     4 | loss: 4.5863075des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.371278762817383 0.3951512575149536
CurrentTrain: epoch  5, batch     5 | loss: 4.3712788des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.54837703704834 0.6693147420883179
CurrentTrain: epoch  5, batch     6 | loss: 4.5483770des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.600937843322754 0.6029316186904907
CurrentTrain: epoch  5, batch     7 | loss: 4.6009378des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5841755867004395 0.6488238573074341
CurrentTrain: epoch  5, batch     8 | loss: 4.5841756des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.463672637939453 0.4989723265171051
CurrentTrain: epoch  5, batch     9 | loss: 4.4636726des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.458212852478027 0.5677056908607483
CurrentTrain: epoch  5, batch    10 | loss: 4.4582129des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.56144905090332 0.5266339182853699
CurrentTrain: epoch  5, batch    11 | loss: 4.5614491des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.446772575378418 0.5390411615371704
CurrentTrain: epoch  5, batch    12 | loss: 4.4467726des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.476202964782715 0.5795177817344666
CurrentTrain: epoch  5, batch    13 | loss: 4.4762030des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.544426918029785 0.5326153039932251
CurrentTrain: epoch  5, batch    14 | loss: 4.5444269des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.435956954956055 0.5771233439445496
CurrentTrain: epoch  5, batch    15 | loss: 4.4359570des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.570015907287598 0.6592903137207031
CurrentTrain: epoch  5, batch    16 | loss: 4.5700159des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.403459548950195 0.4916459321975708
CurrentTrain: epoch  5, batch    17 | loss: 4.4034595des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.503891944885254 0.6292734146118164
CurrentTrain: epoch  5, batch    18 | loss: 4.5038919des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.514217376708984 0.6239316463470459
CurrentTrain: epoch  5, batch    19 | loss: 4.5142174des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.546123027801514 0.5447962284088135
CurrentTrain: epoch  5, batch    20 | loss: 4.5461230des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.384670734405518 0.4868856370449066
CurrentTrain: epoch  5, batch    21 | loss: 4.3846707des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.563713550567627 0.6611856818199158
CurrentTrain: epoch  5, batch    22 | loss: 4.5637136des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.441869258880615 0.44542521238327026
CurrentTrain: epoch  5, batch    23 | loss: 4.4418693des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.529976844787598 0.5655961036682129
CurrentTrain: epoch  5, batch    24 | loss: 4.5299768des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5806474685668945 0.620116114616394
CurrentTrain: epoch  5, batch    25 | loss: 4.5806475des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.507238388061523 0.5009805560112
CurrentTrain: epoch  5, batch    26 | loss: 4.5072384des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.324911594390869 0.6542628407478333
CurrentTrain: epoch  5, batch    27 | loss: 5.3249116des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.5639328956604 0.4587927460670471
CurrentTrain: epoch  5, batch    28 | loss: 4.5639329des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.438464164733887 0.5697140693664551
CurrentTrain: epoch  5, batch    29 | loss: 4.4384642des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.425714492797852 0.5277158617973328
CurrentTrain: epoch  5, batch    30 | loss: 4.4257145des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.419278621673584 0.5155010223388672
CurrentTrain: epoch  5, batch    31 | loss: 4.4192786des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.471404552459717 0.4823659062385559
CurrentTrain: epoch  5, batch    32 | loss: 4.4714046des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.901614189147949 0.5919108986854553
CurrentTrain: epoch  5, batch    33 | loss: 4.9016142des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.512813568115234 0.6214079856872559
CurrentTrain: epoch  5, batch    34 | loss: 4.5128136des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.861846446990967 0.6395868062973022
CurrentTrain: epoch  5, batch    35 | loss: 4.8618464des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.514427661895752 0.4662402868270874
CurrentTrain: epoch  5, batch    36 | loss: 4.5144277des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.615067481994629 0.5302954912185669
CurrentTrain: epoch  5, batch    37 | loss: 4.6150675des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.472209453582764 0.5416100025177002
CurrentTrain: epoch  5, batch    38 | loss: 4.4722095des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.672701358795166 0.5442349910736084
CurrentTrain: epoch  5, batch    39 | loss: 4.6727014des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.57814884185791 0.5168404579162598
CurrentTrain: epoch  5, batch    40 | loss: 4.5781488des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.752537250518799 0.5156129598617554
CurrentTrain: epoch  5, batch    41 | loss: 4.7525373des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.454331874847412 0.4813092350959778
CurrentTrain: epoch  5, batch    42 | loss: 4.4543319des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.534602165222168 0.5703202486038208
CurrentTrain: epoch  5, batch    43 | loss: 4.5346022des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.391172409057617 0.46206432580947876
CurrentTrain: epoch  5, batch    44 | loss: 4.3911724des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.399387836456299 0.5347396731376648
CurrentTrain: epoch  5, batch    45 | loss: 4.3993878des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.361226558685303 0.44392073154449463
CurrentTrain: epoch  5, batch    46 | loss: 4.3612266des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.910618305206299 0.6226493120193481
CurrentTrain: epoch  5, batch    47 | loss: 4.9106183des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.405278205871582 0.554241955280304
CurrentTrain: epoch  5, batch    48 | loss: 4.4052782des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.418508052825928 0.4949527084827423
CurrentTrain: epoch  5, batch    49 | loss: 4.4185081des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.418126583099365 0.5171388387680054
CurrentTrain: epoch  5, batch    50 | loss: 4.4181266des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.486147880554199 0.4763570725917816
CurrentTrain: epoch  5, batch    51 | loss: 4.4861479des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4798383712768555 0.591892421245575
CurrentTrain: epoch  5, batch    52 | loss: 4.4798384des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.471386909484863 0.5585920810699463
CurrentTrain: epoch  5, batch    53 | loss: 4.4713869des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.460452079772949 0.5785118341445923
CurrentTrain: epoch  5, batch    54 | loss: 4.4604521des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.405984401702881 0.5148568749427795
CurrentTrain: epoch  5, batch    55 | loss: 4.4059844des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.313607692718506 0.42506182193756104
CurrentTrain: epoch  5, batch    56 | loss: 4.3136077des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4464592933654785 0.4369902014732361
CurrentTrain: epoch  5, batch    57 | loss: 4.4464593des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.314281940460205 0.336976021528244
CurrentTrain: epoch  5, batch    58 | loss: 4.3142819des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3734588623046875 0.47091326117515564
CurrentTrain: epoch  5, batch    59 | loss: 4.3734589des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.401834487915039 0.3771716356277466
CurrentTrain: epoch  5, batch    60 | loss: 4.4018345des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.406245231628418 0.46754980087280273
CurrentTrain: epoch  5, batch    61 | loss: 4.4062452des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3494873046875 0.32955142855644226
CurrentTrain: epoch  5, batch    62 | loss: 4.3494873des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.380777835845947 0.4496024250984192
CurrentTrain: epoch  6, batch     0 | loss: 4.3807778des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.298957347869873 0.32127898931503296
CurrentTrain: epoch  6, batch     1 | loss: 4.2989573des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.352350234985352 0.37870073318481445
CurrentTrain: epoch  6, batch     2 | loss: 4.3523502des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.416090965270996 0.5070759057998657
CurrentTrain: epoch  6, batch     3 | loss: 4.4160910des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.36453914642334 0.4045236110687256
CurrentTrain: epoch  6, batch     4 | loss: 4.3645391des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3480000495910645 0.4253997802734375
CurrentTrain: epoch  6, batch     5 | loss: 4.3480000des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.426149845123291 0.5251860022544861
CurrentTrain: epoch  6, batch     6 | loss: 4.4261498des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.333482265472412 0.4193214476108551
CurrentTrain: epoch  6, batch     7 | loss: 4.3334823des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.510516166687012 0.5573806166648865
CurrentTrain: epoch  6, batch     8 | loss: 4.5105162des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.425027847290039 0.4198858141899109
CurrentTrain: epoch  6, batch     9 | loss: 4.4250278des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.370506286621094 0.36989930272102356
CurrentTrain: epoch  6, batch    10 | loss: 4.3705063des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.370238780975342 0.45317089557647705
CurrentTrain: epoch  6, batch    11 | loss: 4.3702388des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.304886817932129 0.4365667700767517
CurrentTrain: epoch  6, batch    12 | loss: 4.3048868des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.447336673736572 0.5263372659683228
CurrentTrain: epoch  6, batch    13 | loss: 4.4473367des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.349255084991455 0.460568368434906
CurrentTrain: epoch  6, batch    14 | loss: 4.3492551des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4000654220581055 0.5126264691352844
CurrentTrain: epoch  6, batch    15 | loss: 4.4000654des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.360268592834473 0.4959922730922699
CurrentTrain: epoch  6, batch    16 | loss: 4.3602686des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.360441207885742 0.44380414485931396
CurrentTrain: epoch  6, batch    17 | loss: 4.3604412des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.311830520629883 0.27022331953048706
CurrentTrain: epoch  6, batch    18 | loss: 4.3118305des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3278350830078125 0.4422420263290405
CurrentTrain: epoch  6, batch    19 | loss: 4.3278351des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.4518656730651855 0.4717358648777008
CurrentTrain: epoch  6, batch    20 | loss: 4.4518657des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.560336112976074 0.49734190106391907
CurrentTrain: epoch  6, batch    21 | loss: 4.5603361des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.43010950088501 0.4988340139389038
CurrentTrain: epoch  6, batch    22 | loss: 4.4301095des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.334359169006348 0.4341467022895813
CurrentTrain: epoch  6, batch    23 | loss: 4.3343592des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.377277851104736 0.46554434299468994
CurrentTrain: epoch  6, batch    24 | loss: 4.3772779des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.506911277770996 0.5501198768615723
CurrentTrain: epoch  6, batch    25 | loss: 4.5069113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.361518383026123 0.4112149775028229
CurrentTrain: epoch  6, batch    26 | loss: 4.3615184des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.321658134460449 0.37382742762565613
CurrentTrain: epoch  6, batch    27 | loss: 4.3216581des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.319241523742676 0.4607003331184387
CurrentTrain: epoch  6, batch    28 | loss: 4.3192415des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.39192008972168 0.4419557452201843
CurrentTrain: epoch  6, batch    29 | loss: 4.3919201des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.290597438812256 0.4305001199245453
CurrentTrain: epoch  6, batch    30 | loss: 4.2905974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.333624839782715 0.45194417238235474
CurrentTrain: epoch  6, batch    31 | loss: 4.3336248des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.346329689025879 0.4561992883682251
CurrentTrain: epoch  6, batch    32 | loss: 4.3463297des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3236260414123535 0.45577847957611084
CurrentTrain: epoch  6, batch    33 | loss: 4.3236260des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.360931396484375 0.5269614458084106
CurrentTrain: epoch  6, batch    34 | loss: 4.3609314des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.304778099060059 0.41583114862442017
CurrentTrain: epoch  6, batch    35 | loss: 4.3047781des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.32584285736084 0.4202318787574768
CurrentTrain: epoch  6, batch    36 | loss: 4.3258429des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.225711345672607 0.35455644130706787
CurrentTrain: epoch  6, batch    37 | loss: 4.2257113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.279140472412109 0.39334842562675476
CurrentTrain: epoch  6, batch    38 | loss: 4.2791405des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.315488338470459 0.44719958305358887
CurrentTrain: epoch  6, batch    39 | loss: 4.3154883des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.242314338684082 0.286569744348526
CurrentTrain: epoch  6, batch    40 | loss: 4.2423143des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.347826957702637 0.44768762588500977
CurrentTrain: epoch  6, batch    41 | loss: 4.3478270des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3560566902160645 0.40307295322418213
CurrentTrain: epoch  6, batch    42 | loss: 4.3560567des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.709905624389648 0.4858933389186859
CurrentTrain: epoch  6, batch    43 | loss: 4.7099056des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.302656650543213 0.37993353605270386
CurrentTrain: epoch  6, batch    44 | loss: 4.3026567des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3243727684021 0.38340631127357483
CurrentTrain: epoch  6, batch    45 | loss: 4.3243728des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.299232482910156 0.4335075914859772
CurrentTrain: epoch  6, batch    46 | loss: 4.2992325des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.393268585205078 0.44124311208724976
CurrentTrain: epoch  6, batch    47 | loss: 4.3932686des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.497710704803467 0.44655555486679077
CurrentTrain: epoch  6, batch    48 | loss: 4.4977107des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.317333698272705 0.32386690378189087
CurrentTrain: epoch  6, batch    49 | loss: 4.3173337des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.372941970825195 0.49711379408836365
CurrentTrain: epoch  6, batch    50 | loss: 4.3729420des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2669501304626465 0.32876861095428467
CurrentTrain: epoch  6, batch    51 | loss: 4.2669501des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.21437931060791 0.3644716739654541
CurrentTrain: epoch  6, batch    52 | loss: 4.2143793des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.243166446685791 0.41580259799957275
CurrentTrain: epoch  6, batch    53 | loss: 4.2431664des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.23950719833374 0.40906038880348206
CurrentTrain: epoch  6, batch    54 | loss: 4.2395072des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.308017253875732 0.4429519772529602
CurrentTrain: epoch  6, batch    55 | loss: 4.3080173des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.330833911895752 0.37679195404052734
CurrentTrain: epoch  6, batch    56 | loss: 4.3308339des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.255394458770752 0.35622844099998474
CurrentTrain: epoch  6, batch    57 | loss: 4.2553945des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.284964561462402 0.36756694316864014
CurrentTrain: epoch  6, batch    58 | loss: 4.2849646des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.314428329467773 0.4308154582977295
CurrentTrain: epoch  6, batch    59 | loss: 4.3144283des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3363752365112305 0.4364526569843292
CurrentTrain: epoch  6, batch    60 | loss: 4.3363752des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.194456100463867 0.35684603452682495
CurrentTrain: epoch  6, batch    61 | loss: 4.1944561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246856212615967 0.3213602900505066
CurrentTrain: epoch  6, batch    62 | loss: 4.2468562des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.189990997314453 0.36487987637519836
CurrentTrain: epoch  7, batch     0 | loss: 4.1899910des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.31212854385376 0.4164844751358032
CurrentTrain: epoch  7, batch     1 | loss: 4.3121285des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.255409240722656 0.4059768319129944
CurrentTrain: epoch  7, batch     2 | loss: 4.2554092des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.43032693862915 0.35488536953926086
CurrentTrain: epoch  7, batch     3 | loss: 4.4303269des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.324515342712402 0.41057074069976807
CurrentTrain: epoch  7, batch     4 | loss: 4.3245153des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.286532402038574 0.38417214155197144
CurrentTrain: epoch  7, batch     5 | loss: 4.2865324des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.25857400894165 0.41151881217956543
CurrentTrain: epoch  7, batch     6 | loss: 4.2585740des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.265316963195801 0.40623921155929565
CurrentTrain: epoch  7, batch     7 | loss: 4.2653170des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.294508934020996 0.3603384494781494
CurrentTrain: epoch  7, batch     8 | loss: 4.2945089des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3198089599609375 0.42930781841278076
CurrentTrain: epoch  7, batch     9 | loss: 4.3198090des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.231564521789551 0.2974283993244171
CurrentTrain: epoch  7, batch    10 | loss: 4.2315645des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.309030532836914 0.39653199911117554
CurrentTrain: epoch  7, batch    11 | loss: 4.3090305des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.211300849914551 0.3122410774230957
CurrentTrain: epoch  7, batch    12 | loss: 4.2113008des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.427243709564209 0.4973120391368866
CurrentTrain: epoch  7, batch    13 | loss: 4.4272437des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.301522254943848 0.45576825737953186
CurrentTrain: epoch  7, batch    14 | loss: 4.3015223des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.281188488006592 0.41579437255859375
CurrentTrain: epoch  7, batch    15 | loss: 4.2811885des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.253280162811279 0.3698335587978363
CurrentTrain: epoch  7, batch    16 | loss: 4.2532802des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.391445159912109 0.3626217246055603
CurrentTrain: epoch  7, batch    17 | loss: 4.3914452des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.291969299316406 0.3643554747104645
CurrentTrain: epoch  7, batch    18 | loss: 4.2919693des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.353588581085205 0.4593176245689392
CurrentTrain: epoch  7, batch    19 | loss: 4.3535886des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19444465637207 0.3304210603237152
CurrentTrain: epoch  7, batch    20 | loss: 4.1944447des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.224584102630615 0.3771294355392456
CurrentTrain: epoch  7, batch    21 | loss: 4.2245841des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.211495876312256 0.34210914373397827
CurrentTrain: epoch  7, batch    22 | loss: 4.2114959des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.270745754241943 0.38078010082244873
CurrentTrain: epoch  7, batch    23 | loss: 4.2707458des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.241967678070068 0.39388683438301086
CurrentTrain: epoch  7, batch    24 | loss: 4.2419677des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.285592079162598 0.3743988275527954
CurrentTrain: epoch  7, batch    25 | loss: 4.2855921des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.265413284301758 0.41419553756713867
CurrentTrain: epoch  7, batch    26 | loss: 4.2654133des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.250130653381348 0.28515779972076416
CurrentTrain: epoch  7, batch    27 | loss: 4.2501307des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.258228302001953 0.386490136384964
CurrentTrain: epoch  7, batch    28 | loss: 4.2582283des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.3025922775268555 0.4143252670764923
CurrentTrain: epoch  7, batch    29 | loss: 4.3025923des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.543850421905518 0.4167063236236572
CurrentTrain: epoch  7, batch    30 | loss: 4.5438504des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.276578903198242 0.3871743381023407
CurrentTrain: epoch  7, batch    31 | loss: 4.2765789des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.149586200714111 0.294174462556839
CurrentTrain: epoch  7, batch    32 | loss: 4.1495862des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246616363525391 0.3638286590576172
CurrentTrain: epoch  7, batch    33 | loss: 4.2466164des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.250977993011475 0.4126662015914917
CurrentTrain: epoch  7, batch    34 | loss: 4.2509780des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.232016563415527 0.3677465319633484
CurrentTrain: epoch  7, batch    35 | loss: 4.2320166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2342376708984375 0.3290902376174927
CurrentTrain: epoch  7, batch    36 | loss: 4.2342377des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.254711627960205 0.37415069341659546
CurrentTrain: epoch  7, batch    37 | loss: 4.2547116des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2223920822143555 0.2647286057472229
CurrentTrain: epoch  7, batch    38 | loss: 4.2223921des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.212767601013184 0.2900674641132355
CurrentTrain: epoch  7, batch    39 | loss: 4.2127676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.141648292541504 0.28314924240112305
CurrentTrain: epoch  7, batch    40 | loss: 4.1416483des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.196037769317627 0.3476572334766388
CurrentTrain: epoch  7, batch    41 | loss: 4.1960378des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1844482421875 0.2990662455558777
CurrentTrain: epoch  7, batch    42 | loss: 4.1844482des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.22840690612793 0.3548886179924011
CurrentTrain: epoch  7, batch    43 | loss: 4.2284069des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.217401504516602 0.2675629258155823
CurrentTrain: epoch  7, batch    44 | loss: 4.2174015des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.216369152069092 0.3482568860054016
CurrentTrain: epoch  7, batch    45 | loss: 4.2163692des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.228445053100586 0.3699856996536255
CurrentTrain: epoch  7, batch    46 | loss: 4.2284451des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.256560802459717 0.41124439239501953
CurrentTrain: epoch  7, batch    47 | loss: 4.2565608des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.232559680938721 0.40275126695632935
CurrentTrain: epoch  7, batch    48 | loss: 4.2325597des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.160773754119873 0.2466418445110321
CurrentTrain: epoch  7, batch    49 | loss: 4.1607738des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.242006778717041 0.37290889024734497
CurrentTrain: epoch  7, batch    50 | loss: 4.2420068des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.213846206665039 0.3285408616065979
CurrentTrain: epoch  7, batch    51 | loss: 4.2138462des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.235902309417725 0.3308592438697815
CurrentTrain: epoch  7, batch    52 | loss: 4.2359023des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.190471649169922 0.342294305562973
CurrentTrain: epoch  7, batch    53 | loss: 4.1904716des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.229180812835693 0.31272435188293457
CurrentTrain: epoch  7, batch    54 | loss: 4.2291808des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.065913200378418 0.2778569459915161
CurrentTrain: epoch  7, batch    55 | loss: 4.0659132des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.205617427825928 0.2998600900173187
CurrentTrain: epoch  7, batch    56 | loss: 4.2056174des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.150660991668701 0.26808229088783264
CurrentTrain: epoch  7, batch    57 | loss: 4.1506610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.229507923126221 0.364546000957489
CurrentTrain: epoch  7, batch    58 | loss: 4.2295079des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246364116668701 0.26237648725509644
CurrentTrain: epoch  7, batch    59 | loss: 4.2463641des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.225794315338135 0.34127217531204224
CurrentTrain: epoch  7, batch    60 | loss: 4.2257943des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.173409938812256 0.30431997776031494
CurrentTrain: epoch  7, batch    61 | loss: 4.1734099des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.215534210205078 0.22731004655361176
CurrentTrain: epoch  7, batch    62 | loss: 4.2155342des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.20319128036499 0.3687328100204468
CurrentTrain: epoch  8, batch     0 | loss: 4.2031913des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.188854217529297 0.29346978664398193
CurrentTrain: epoch  8, batch     1 | loss: 4.1888542des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.218271732330322 0.33775657415390015
CurrentTrain: epoch  8, batch     2 | loss: 4.2182717des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1912736892700195 0.27856674790382385
CurrentTrain: epoch  8, batch     3 | loss: 4.1912737des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.195581912994385 0.3330187499523163
CurrentTrain: epoch  8, batch     4 | loss: 4.1955819des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.141901016235352 0.20725567638874054
CurrentTrain: epoch  8, batch     5 | loss: 4.1419010des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.158905506134033 0.351121723651886
CurrentTrain: epoch  8, batch     6 | loss: 4.1589055des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.185216426849365 0.3445802628993988
CurrentTrain: epoch  8, batch     7 | loss: 4.1852164des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.198623180389404 0.2703486680984497
CurrentTrain: epoch  8, batch     8 | loss: 4.1986232des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.157131671905518 0.30627864599227905
CurrentTrain: epoch  8, batch     9 | loss: 4.1571317des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.192836761474609 0.345899760723114
CurrentTrain: epoch  8, batch    10 | loss: 4.1928368des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.135107517242432 0.2623969316482544
CurrentTrain: epoch  8, batch    11 | loss: 4.1351075des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1867523193359375 0.34666794538497925
CurrentTrain: epoch  8, batch    12 | loss: 4.1867523des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.199302673339844 0.28392839431762695
CurrentTrain: epoch  8, batch    13 | loss: 4.1993027des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.206055641174316 0.2810009717941284
CurrentTrain: epoch  8, batch    14 | loss: 4.2060556des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.169893264770508 0.33660128712654114
CurrentTrain: epoch  8, batch    15 | loss: 4.1698933des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.153294086456299 0.28102874755859375
CurrentTrain: epoch  8, batch    16 | loss: 4.1532941des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1345906257629395 0.2185938060283661
CurrentTrain: epoch  8, batch    17 | loss: 4.1345906des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.187060832977295 0.2745087146759033
CurrentTrain: epoch  8, batch    18 | loss: 4.1870608des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.202029228210449 0.37974512577056885
CurrentTrain: epoch  8, batch    19 | loss: 4.2020292des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2088470458984375 0.312069833278656
CurrentTrain: epoch  8, batch    20 | loss: 4.2088470des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.221078872680664 0.30967947840690613
CurrentTrain: epoch  8, batch    21 | loss: 4.2210789des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.230535984039307 0.3451409935951233
CurrentTrain: epoch  8, batch    22 | loss: 4.2305360des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1917314529418945 0.32002103328704834
CurrentTrain: epoch  8, batch    23 | loss: 4.1917315des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.215908527374268 0.35517051815986633
CurrentTrain: epoch  8, batch    24 | loss: 4.2159085des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.275216579437256 0.31828945875167847
CurrentTrain: epoch  8, batch    25 | loss: 4.2752166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.172830581665039 0.3509158790111542
CurrentTrain: epoch  8, batch    26 | loss: 4.1728306des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.183104991912842 0.2743503451347351
CurrentTrain: epoch  8, batch    27 | loss: 4.1831050des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.285573959350586 0.3873057961463928
CurrentTrain: epoch  8, batch    28 | loss: 4.2855740des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.160183906555176 0.30172181129455566
CurrentTrain: epoch  8, batch    29 | loss: 4.1601839des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.141350746154785 0.2858636975288391
CurrentTrain: epoch  8, batch    30 | loss: 4.1413507des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.176167964935303 0.25049877166748047
CurrentTrain: epoch  8, batch    31 | loss: 4.1761680des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.184316635131836 0.29900139570236206
CurrentTrain: epoch  8, batch    32 | loss: 4.1843166des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1037702560424805 0.25419092178344727
CurrentTrain: epoch  8, batch    33 | loss: 4.1037703des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.20197057723999 0.3409031927585602
CurrentTrain: epoch  8, batch    34 | loss: 4.2019706des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.163850784301758 0.3038424253463745
CurrentTrain: epoch  8, batch    35 | loss: 4.1638508des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.173487663269043 0.26987361907958984
CurrentTrain: epoch  8, batch    36 | loss: 4.1734877des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.178638458251953 0.2751167416572571
CurrentTrain: epoch  8, batch    37 | loss: 4.1786385des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.102222442626953 0.2086557298898697
CurrentTrain: epoch  8, batch    38 | loss: 4.1022224des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.193821430206299 0.30701664090156555
CurrentTrain: epoch  8, batch    39 | loss: 4.1938214des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.189626216888428 0.2660765051841736
CurrentTrain: epoch  8, batch    40 | loss: 4.1896262des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.216363430023193 0.3390079140663147
CurrentTrain: epoch  8, batch    41 | loss: 4.2163634des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.225053787231445 0.3206121325492859
CurrentTrain: epoch  8, batch    42 | loss: 4.2250538des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1388397216796875 0.28966569900512695
CurrentTrain: epoch  8, batch    43 | loss: 4.1388397des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.205867290496826 0.35088270902633667
CurrentTrain: epoch  8, batch    44 | loss: 4.2058673des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.131313323974609 0.2775586247444153
CurrentTrain: epoch  8, batch    45 | loss: 4.1313133des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.198879241943359 0.3316073417663574
CurrentTrain: epoch  8, batch    46 | loss: 4.1988792des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.179159641265869 0.22547027468681335
CurrentTrain: epoch  8, batch    47 | loss: 4.1791596des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.187089920043945 0.24043232202529907
CurrentTrain: epoch  8, batch    48 | loss: 4.1870899des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1828413009643555 0.3004415035247803
CurrentTrain: epoch  8, batch    49 | loss: 4.1828413des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1772379875183105 0.29968851804733276
CurrentTrain: epoch  8, batch    50 | loss: 4.1772380des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.247582912445068 0.369605153799057
CurrentTrain: epoch  8, batch    51 | loss: 4.2475829des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.202742576599121 0.29983770847320557
CurrentTrain: epoch  8, batch    52 | loss: 4.2027426des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.194066524505615 0.3511754274368286
CurrentTrain: epoch  8, batch    53 | loss: 4.1940665des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.183652877807617 0.3218408226966858
CurrentTrain: epoch  8, batch    54 | loss: 4.1836529des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.17349100112915 0.23268190026283264
CurrentTrain: epoch  8, batch    55 | loss: 4.1734910des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.206981182098389 0.29493191838264465
CurrentTrain: epoch  8, batch    56 | loss: 4.2069812des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.128519058227539 0.2724837362766266
CurrentTrain: epoch  8, batch    57 | loss: 4.1285191des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.130615234375 0.20806147158145905
CurrentTrain: epoch  8, batch    58 | loss: 4.1306152des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.164185523986816 0.2896714210510254
CurrentTrain: epoch  8, batch    59 | loss: 4.1641855des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.166609287261963 0.2647407650947571
CurrentTrain: epoch  8, batch    60 | loss: 4.1666093des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.146884441375732 0.27233636379241943
CurrentTrain: epoch  8, batch    61 | loss: 4.1468844des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.148573398590088 0.2098235785961151
CurrentTrain: epoch  8, batch    62 | loss: 4.1485734des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.200809955596924 0.3163105845451355
CurrentTrain: epoch  9, batch     0 | loss: 4.2008100des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.153356075286865 0.288815438747406
CurrentTrain: epoch  9, batch     1 | loss: 4.1533561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.14847993850708 0.2521754503250122
CurrentTrain: epoch  9, batch     2 | loss: 4.1484799des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.168156623840332 0.3177734613418579
CurrentTrain: epoch  9, batch     3 | loss: 4.1681566des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.162365436553955 0.24804021418094635
CurrentTrain: epoch  9, batch     4 | loss: 4.1623654des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.156373977661133 0.22016818821430206
CurrentTrain: epoch  9, batch     5 | loss: 4.1563740des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.152553081512451 0.27703550457954407
CurrentTrain: epoch  9, batch     6 | loss: 4.1525531des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.203322410583496 0.2834513187408447
CurrentTrain: epoch  9, batch     7 | loss: 4.2033224des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.069621562957764 0.23232805728912354
CurrentTrain: epoch  9, batch     8 | loss: 4.0696216des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.135510444641113 0.24361583590507507
CurrentTrain: epoch  9, batch     9 | loss: 4.1355104des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2059125900268555 0.3242782950401306
CurrentTrain: epoch  9, batch    10 | loss: 4.2059126des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.22229528427124 0.334320604801178
CurrentTrain: epoch  9, batch    11 | loss: 4.2222953des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.135074615478516 0.2611355781555176
CurrentTrain: epoch  9, batch    12 | loss: 4.1350746des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1363067626953125 0.2714781165122986
CurrentTrain: epoch  9, batch    13 | loss: 4.1363068des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.117128372192383 0.254349023103714
CurrentTrain: epoch  9, batch    14 | loss: 4.1171284des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.178225040435791 0.2730830907821655
CurrentTrain: epoch  9, batch    15 | loss: 4.1782250des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.159298896789551 0.28430455923080444
CurrentTrain: epoch  9, batch    16 | loss: 4.1592989des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.140449047088623 0.26897332072257996
CurrentTrain: epoch  9, batch    17 | loss: 4.1404490des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.114620685577393 0.2713249921798706
CurrentTrain: epoch  9, batch    18 | loss: 4.1146207des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.18945837020874 0.3264702558517456
CurrentTrain: epoch  9, batch    19 | loss: 4.1894584des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.113674163818359 0.2349860966205597
CurrentTrain: epoch  9, batch    20 | loss: 4.1136742des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1540141105651855 0.26184871792793274
CurrentTrain: epoch  9, batch    21 | loss: 4.1540141des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.127366542816162 0.28345292806625366
CurrentTrain: epoch  9, batch    22 | loss: 4.1273665des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.151912689208984 0.26668697595596313
CurrentTrain: epoch  9, batch    23 | loss: 4.1519127des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.154704570770264 0.24347025156021118
CurrentTrain: epoch  9, batch    24 | loss: 4.1547046des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.185112476348877 0.31368288397789
CurrentTrain: epoch  9, batch    25 | loss: 4.1851125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2017412185668945 0.3401017189025879
CurrentTrain: epoch  9, batch    26 | loss: 4.2017412des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.241264343261719 0.34696856141090393
CurrentTrain: epoch  9, batch    27 | loss: 4.2412643des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.149592399597168 0.31252360343933105
CurrentTrain: epoch  9, batch    28 | loss: 4.1495924des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.11658239364624 0.24387606978416443
CurrentTrain: epoch  9, batch    29 | loss: 4.1165824des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.201260089874268 0.339023232460022
CurrentTrain: epoch  9, batch    30 | loss: 4.2012601des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.115085601806641 0.2395915687084198
CurrentTrain: epoch  9, batch    31 | loss: 4.1150856des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.100704669952393 0.21184171736240387
CurrentTrain: epoch  9, batch    32 | loss: 4.1007047des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.144148349761963 0.21632708609104156
CurrentTrain: epoch  9, batch    33 | loss: 4.1441483des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.111084461212158 0.24363037943840027
CurrentTrain: epoch  9, batch    34 | loss: 4.1110845des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.082437038421631 0.2247212827205658
CurrentTrain: epoch  9, batch    35 | loss: 4.0824370des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.073328971862793 0.18549257516860962
CurrentTrain: epoch  9, batch    36 | loss: 4.0733290des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.140527248382568 0.2678466737270355
CurrentTrain: epoch  9, batch    37 | loss: 4.1405272des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.110531330108643 0.22651687264442444
CurrentTrain: epoch  9, batch    38 | loss: 4.1105313des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.132110595703125 0.2620410919189453
CurrentTrain: epoch  9, batch    39 | loss: 4.1321106des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.155632972717285 0.26962074637413025
CurrentTrain: epoch  9, batch    40 | loss: 4.1556330des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.043642520904541 0.19308969378471375
CurrentTrain: epoch  9, batch    41 | loss: 4.0436425des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.134505271911621 0.20650985836982727
CurrentTrain: epoch  9, batch    42 | loss: 4.1345053des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.131923198699951 0.24098727107048035
CurrentTrain: epoch  9, batch    43 | loss: 4.1319232des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.110268592834473 0.2437652349472046
CurrentTrain: epoch  9, batch    44 | loss: 4.1102686des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.102866172790527 0.23667317628860474
CurrentTrain: epoch  9, batch    45 | loss: 4.1028662des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.098350524902344 0.19062259793281555
CurrentTrain: epoch  9, batch    46 | loss: 4.0983505des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.13633918762207 0.23107504844665527
CurrentTrain: epoch  9, batch    47 | loss: 4.1363392des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.107921600341797 0.2069958746433258
CurrentTrain: epoch  9, batch    48 | loss: 4.1079216des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.078799247741699 0.1888408660888672
CurrentTrain: epoch  9, batch    49 | loss: 4.0787992des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.11378288269043 0.24000635743141174
CurrentTrain: epoch  9, batch    50 | loss: 4.1137829des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.085391521453857 0.21465498208999634
CurrentTrain: epoch  9, batch    51 | loss: 4.0853915des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.184499740600586 0.29196256399154663
CurrentTrain: epoch  9, batch    52 | loss: 4.1844997des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.128975868225098 0.24549411237239838
CurrentTrain: epoch  9, batch    53 | loss: 4.1289759des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.117690086364746 0.17257845401763916
CurrentTrain: epoch  9, batch    54 | loss: 4.1176901des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1780242919921875 0.2514609098434448
CurrentTrain: epoch  9, batch    55 | loss: 4.1780243des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.086454391479492 0.2303999364376068
CurrentTrain: epoch  9, batch    56 | loss: 4.0864544des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.128835678100586 0.20734965801239014
CurrentTrain: epoch  9, batch    57 | loss: 4.1288357des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.11151123046875 0.2062082141637802
CurrentTrain: epoch  9, batch    58 | loss: 4.1115112des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.128175735473633 0.2936670482158661
CurrentTrain: epoch  9, batch    59 | loss: 4.1281757des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.117674350738525 0.20615676045417786
CurrentTrain: epoch  9, batch    60 | loss: 4.1176744des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.12504768371582 0.24355575442314148
CurrentTrain: epoch  9, batch    61 | loss: 4.1250477des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.139786243438721 0.145510733127594
CurrentTrain: epoch  9, batch    62 | loss: 4.1397862
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.86%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.15%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.86%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.15%   
cur_acc:  ['0.9415']
his_acc:  ['0.9415']
Clustering into  9  clusters
Clusters:  [3 2 0 1 5 5 0 4 2 2 2 1 6 7 1 8 4 2 3 2]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.976340293884277 1.9235011339187622
CurrentTrain: epoch  0, batch     0 | loss: 8.9763403des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  9.093894004821777 1.83072829246521
CurrentTrain: epoch  0, batch     1 | loss: 9.0938940des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.428448677062988 2.162039279937744
CurrentTrain: epoch  0, batch     2 | loss: 8.4284487des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.97629976272583 0.6125954389572144
CurrentTrain: epoch  0, batch     3 | loss: 6.9762998des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.677085876464844 2.048187732696533
CurrentTrain: epoch  1, batch     0 | loss: 8.6770859des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.445179462432861 1.9876662492752075
CurrentTrain: epoch  1, batch     1 | loss: 7.4451795des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.621415615081787 2.0028228759765625
CurrentTrain: epoch  1, batch     2 | loss: 7.6214156des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.772315502166748 0.6740827560424805
CurrentTrain: epoch  1, batch     3 | loss: 5.7723155des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.156123638153076 1.8082706928253174
CurrentTrain: epoch  2, batch     0 | loss: 7.1561236des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.1458024978637695 1.8790273666381836
CurrentTrain: epoch  2, batch     1 | loss: 6.1458025des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.090319633483887 1.8754541873931885
CurrentTrain: epoch  2, batch     2 | loss: 6.0903196des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.046263694763184 0.7573809623718262
CurrentTrain: epoch  2, batch     3 | loss: 8.0462637des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.214688777923584 2.000218391418457
CurrentTrain: epoch  3, batch     0 | loss: 6.2146888des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.180909156799316 2.093498706817627
CurrentTrain: epoch  3, batch     1 | loss: 6.1809092des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.497795581817627 2.102370023727417
CurrentTrain: epoch  3, batch     2 | loss: 5.4977956des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.291998863220215 0.7058663368225098
CurrentTrain: epoch  3, batch     3 | loss: 6.2919989des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.318770408630371 1.6951825618743896
CurrentTrain: epoch  4, batch     0 | loss: 5.3187704des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.976204872131348 1.5838465690612793
CurrentTrain: epoch  4, batch     1 | loss: 5.9762049des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.148547649383545 1.8050878047943115
CurrentTrain: epoch  4, batch     2 | loss: 5.1485476des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.392388820648193 0.6727551221847534
CurrentTrain: epoch  4, batch     3 | loss: 5.3923888des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.353323459625244 1.6778713464736938
CurrentTrain: epoch  5, batch     0 | loss: 6.3533235des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.399374485015869 1.991722822189331
CurrentTrain: epoch  5, batch     1 | loss: 5.3993745des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.659489154815674 1.7956736087799072
CurrentTrain: epoch  5, batch     2 | loss: 4.6594892des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.7776198387145996 0.6130964756011963
CurrentTrain: epoch  5, batch     3 | loss: 2.7776198des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.318843364715576 1.6939665079116821
CurrentTrain: epoch  6, batch     0 | loss: 5.3188434des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.786681175231934 1.7422809600830078
CurrentTrain: epoch  6, batch     1 | loss: 4.7866812des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.02372407913208 2.075810432434082
CurrentTrain: epoch  6, batch     2 | loss: 5.0237241des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.447299957275391 0.620747447013855
CurrentTrain: epoch  6, batch     3 | loss: 5.4473000des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.902564525604248 1.9104888439178467
CurrentTrain: epoch  7, batch     0 | loss: 4.9025645des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.153094291687012 1.7592793703079224
CurrentTrain: epoch  7, batch     1 | loss: 4.1530943des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.749360084533691 1.806164264678955
CurrentTrain: epoch  7, batch     2 | loss: 4.7493601des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.997166156768799 0.0
CurrentTrain: epoch  7, batch     3 | loss: 7.9971662des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.957800388336182 1.9924306869506836
CurrentTrain: epoch  8, batch     0 | loss: 4.9578004des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.801945209503174 1.8532915115356445
CurrentTrain: epoch  8, batch     1 | loss: 4.8019452des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.19924259185791 1.8693466186523438
CurrentTrain: epoch  8, batch     2 | loss: 4.1992426des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.402723789215088 0.6056622266769409
CurrentTrain: epoch  8, batch     3 | loss: 4.4027238des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.246652126312256 1.8261390924453735
CurrentTrain: epoch  9, batch     0 | loss: 4.2466521des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.988279819488525 1.8414392471313477
CurrentTrain: epoch  9, batch     1 | loss: 4.9882798des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.875385284423828 1.78045654296875
CurrentTrain: epoch  9, batch     2 | loss: 3.8753853des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.521910190582275 0.5219694375991821
CurrentTrain: epoch  9, batch     3 | loss: 4.5219102
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2616326808929443 2.6824450492858887
MemoryTrain:  epoch  0, batch     0 | loss: 2.2616327des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.648151159286499 1.2458727359771729
MemoryTrain:  epoch  0, batch     1 | loss: 1.6481512des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2391490936279297 2.5975747108459473
MemoryTrain:  epoch  1, batch     0 | loss: 2.2391491des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.8120604753494263 1.3590106964111328
MemoryTrain:  epoch  1, batch     1 | loss: 0.8120605des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6204761266708374 2.659719944000244
MemoryTrain:  epoch  2, batch     0 | loss: 1.6204761des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0238033533096313 1.260439395904541
MemoryTrain:  epoch  2, batch     1 | loss: 1.0238034des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3659931421279907 2.5302963256835938
MemoryTrain:  epoch  3, batch     0 | loss: 1.3659931des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6839868426322937 1.3024123907089233
MemoryTrain:  epoch  3, batch     1 | loss: 0.6839868des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3443228006362915 2.583918571472168
MemoryTrain:  epoch  4, batch     0 | loss: 1.3443228des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6275399327278137 1.2111999988555908
MemoryTrain:  epoch  4, batch     1 | loss: 0.6275399des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3339110612869263 2.5768418312072754
MemoryTrain:  epoch  5, batch     0 | loss: 1.3339111des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6982211470603943 1.3050518035888672
MemoryTrain:  epoch  5, batch     1 | loss: 0.6982211des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3095742464065552 2.523104667663574
MemoryTrain:  epoch  6, batch     0 | loss: 1.3095742des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.648561954498291 1.2169960737228394
MemoryTrain:  epoch  6, batch     1 | loss: 0.6485620des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2978652715682983 2.5347628593444824
MemoryTrain:  epoch  7, batch     0 | loss: 1.2978653des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.6803253293037415 1.302458643913269
MemoryTrain:  epoch  7, batch     1 | loss: 0.6803253des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2810204029083252 2.517915964126587
MemoryTrain:  epoch  8, batch     0 | loss: 1.2810204des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.5946730971336365 1.0512444972991943
MemoryTrain:  epoch  8, batch     1 | loss: 0.5946731des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2849565744400024 2.496683359146118
MemoryTrain:  epoch  9, batch     0 | loss: 1.2849566des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.5571171641349792 1.0693690776824951
MemoryTrain:  epoch  9, batch     1 | loss: 0.5571172
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 56.92%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 54.96%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 53.33%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 51.61%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 51.76%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 52.84%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 53.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 54.64%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 55.38%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 55.91%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 56.41%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 57.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 58.99%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.03%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.51%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 60.28%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 59.65%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 59.31%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 58.98%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.55%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 58.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 58.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.19%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 60.57%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.05%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 60.13%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 59.64%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 59.32%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.48%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.83%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.28%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.85%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 91.44%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 89.37%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 87.77%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 87.77%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.18%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 88.15%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 88.03%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 87.65%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 86.91%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 86.48%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 85.85%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 85.23%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.27%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.33%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 82.42%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 81.52%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 80.71%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 79.99%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 80.12%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 80.07%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.20%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 79.98%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 79.20%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 78.68%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.43%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 78.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 78.18%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 77.79%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 77.46%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 77.18%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.12%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 76.90%   
cur_acc:  ['0.9415', '0.5883']
his_acc:  ['0.9415', '0.7690']
Clustering into  14  clusters
Clusters:  [ 1  3 10 11  2  2  9 13  3  5  3  0  4  7  6  0  8  3  1  3 12  1  4  2
  7  5  1  2  0  0]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.230821132659912 2.171259880065918
CurrentTrain: epoch  0, batch     0 | loss: 7.2308211des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.338862419128418 2.101285934448242
CurrentTrain: epoch  0, batch     1 | loss: 7.3388624des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.672192573547363 2.1031370162963867
CurrentTrain: epoch  0, batch     2 | loss: 6.6721926des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.273507595062256 0.8055964112281799
CurrentTrain: epoch  0, batch     3 | loss: 5.2735076des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.01311731338501 2.1704723834991455
CurrentTrain: epoch  1, batch     0 | loss: 6.0131173des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.676686763763428 2.1533069610595703
CurrentTrain: epoch  1, batch     1 | loss: 5.6766868des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.514974117279053 2.056748628616333
CurrentTrain: epoch  1, batch     2 | loss: 5.5149741des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.297950744628906 0.713341236114502
CurrentTrain: epoch  1, batch     3 | loss: 4.2979507des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.270779609680176 2.04378604888916
CurrentTrain: epoch  2, batch     0 | loss: 5.2707796des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.7504682540893555 1.9759061336517334
CurrentTrain: epoch  2, batch     1 | loss: 4.7504683des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.870108604431152 2.0119075775146484
CurrentTrain: epoch  2, batch     2 | loss: 4.8701086des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.404599666595459 0.6702836751937866
CurrentTrain: epoch  2, batch     3 | loss: 5.4045997des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.704593658447266 1.8790910243988037
CurrentTrain: epoch  3, batch     0 | loss: 4.7045937des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9436185359954834 1.8947359323501587
CurrentTrain: epoch  3, batch     1 | loss: 3.9436185des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.026860237121582 2.0108790397644043
CurrentTrain: epoch  3, batch     2 | loss: 4.0268602des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.643743515014648 0.7325615882873535
CurrentTrain: epoch  3, batch     3 | loss: 6.6437435des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6089138984680176 1.7559080123901367
CurrentTrain: epoch  4, batch     0 | loss: 3.6089139des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.1801557540893555 1.9503949880599976
CurrentTrain: epoch  4, batch     1 | loss: 4.1801558des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.333439826965332 1.9051883220672607
CurrentTrain: epoch  4, batch     2 | loss: 4.3334398des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.8815600872039795 0.5551609396934509
CurrentTrain: epoch  4, batch     3 | loss: 2.8815601des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.601349115371704 1.7908883094787598
CurrentTrain: epoch  5, batch     0 | loss: 3.6013491des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.805267572402954 1.8471413850784302
CurrentTrain: epoch  5, batch     1 | loss: 3.8052676des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6552505493164062 1.7120141983032227
CurrentTrain: epoch  5, batch     2 | loss: 3.6552505des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.627498149871826 0.4900272488594055
CurrentTrain: epoch  5, batch     3 | loss: 2.6274981des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.4913318157196045 1.790554165840149
CurrentTrain: epoch  6, batch     0 | loss: 3.4913318des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.238548994064331 1.848361611366272
CurrentTrain: epoch  6, batch     1 | loss: 3.2385490des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.468257427215576 1.830633282661438
CurrentTrain: epoch  6, batch     2 | loss: 3.4682574des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.5404815673828125 0.4341277480125427
CurrentTrain: epoch  6, batch     3 | loss: 3.5404816des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.369670867919922 1.802933692932129
CurrentTrain: epoch  7, batch     0 | loss: 3.3696709des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.1926779747009277 1.8076138496398926
CurrentTrain: epoch  7, batch     1 | loss: 3.1926780des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8080875873565674 1.7754778861999512
CurrentTrain: epoch  7, batch     2 | loss: 3.8080876des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.185075283050537 0.4154490828514099
CurrentTrain: epoch  7, batch     3 | loss: 4.1850753des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.138310432434082 1.7964622974395752
CurrentTrain: epoch  8, batch     0 | loss: 3.1383104des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.290308952331543 1.8816099166870117
CurrentTrain: epoch  8, batch     1 | loss: 3.2903090des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.191392421722412 1.9165613651275635
CurrentTrain: epoch  8, batch     2 | loss: 3.1913924des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.129518508911133 0.0
CurrentTrain: epoch  8, batch     3 | loss: 5.1295185des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.2913126945495605 1.6868771314620972
CurrentTrain: epoch  9, batch     0 | loss: 3.2913127des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.041301965713501 1.8241320848464966
CurrentTrain: epoch  9, batch     1 | loss: 3.0413020des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.9990391731262207 1.7716152667999268
CurrentTrain: epoch  9, batch     2 | loss: 2.9990392des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.236203670501709 0.4469938278198242
CurrentTrain: epoch  9, batch     3 | loss: 2.2362037
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4835219383239746 2.6094868183135986
MemoryTrain:  epoch  0, batch     0 | loss: 2.4835219des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.462405204772949 2.4829304218292236
MemoryTrain:  epoch  0, batch     1 | loss: 2.4624052des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.5813262462615967 2.534055233001709
MemoryTrain:  epoch  1, batch     0 | loss: 3.5813262des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.98968505859375 2.567105531692505
MemoryTrain:  epoch  1, batch     1 | loss: 1.9896851des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.045304775238037 2.6879847049713135
MemoryTrain:  epoch  2, batch     0 | loss: 2.0453048des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5636415481567383 2.3978965282440186
MemoryTrain:  epoch  2, batch     1 | loss: 2.5636415des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.0333926677703857 2.607316493988037
MemoryTrain:  epoch  3, batch     0 | loss: 2.0333927des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.904893159866333 2.332237958908081
MemoryTrain:  epoch  3, batch     1 | loss: 1.9048932des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5304243564605713 2.496802806854248
MemoryTrain:  epoch  4, batch     0 | loss: 2.5304244des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3286654949188232 2.4505813121795654
MemoryTrain:  epoch  4, batch     1 | loss: 1.3286655des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4532750844955444 2.5874173641204834
MemoryTrain:  epoch  5, batch     0 | loss: 1.4532751des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2981390953063965 2.3342418670654297
MemoryTrain:  epoch  5, batch     1 | loss: 2.2981391des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2027621269226074 2.5440826416015625
MemoryTrain:  epoch  6, batch     0 | loss: 2.2027621des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.539023995399475 2.392488956451416
MemoryTrain:  epoch  6, batch     1 | loss: 1.5390240des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7814242839813232 2.5296244621276855
MemoryTrain:  epoch  7, batch     0 | loss: 1.7814243des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7689075469970703 2.36720609664917
MemoryTrain:  epoch  7, batch     1 | loss: 1.7689075des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6810786724090576 2.5045218467712402
MemoryTrain:  epoch  8, batch     0 | loss: 1.6810787des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6273953914642334 2.307542562484741
MemoryTrain:  epoch  8, batch     1 | loss: 1.6273954des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6962206363677979 2.49957275390625
MemoryTrain:  epoch  9, batch     0 | loss: 1.6962206des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5885837078094482 2.2670273780822754
MemoryTrain:  epoch  9, batch     1 | loss: 1.5885837
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.93%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.42%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 91.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.09%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.35%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.44%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.77%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 89.13%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 86.57%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 84.60%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 84.29%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 84.29%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 84.42%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 84.84%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 83.58%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 82.81%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 81.99%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 81.40%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 80.53%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 79.90%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 79.00%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 78.12%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.27%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 76.43%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 75.60%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 74.93%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 75.86%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 75.39%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.17%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 75.05%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 74.95%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 74.79%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 74.49%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 75.56%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.86%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 76.12%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 76.13%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 75.56%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.24%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 75.04%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 74.88%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 74.80%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 74.57%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 74.42%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 74.19%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 74.00%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 74.01%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 74.02%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 73.60%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 73.39%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 73.11%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.94%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 72.70%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 72.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 74.10%   
cur_acc:  ['0.9415', '0.5883', '0.7391']
his_acc:  ['0.9415', '0.7690', '0.7410']
Clustering into  19  clusters
Clusters:  [ 1  2 16 17  3  3 18  0  2  6  2  4 12  9 13 14 15  2  1  2  5  1 12  3
  9  6  1  3  4  4 11  1  0  4  7  4  8  5 10  5]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.712565898895264 1.7742774486541748
CurrentTrain: epoch  0, batch     0 | loss: 6.7125659des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.55664587020874 1.9944336414337158
CurrentTrain: epoch  0, batch     1 | loss: 7.5566459des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.802304267883301 2.0277256965637207
CurrentTrain: epoch  0, batch     2 | loss: 7.8023043des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.737978935241699 0.6571812629699707
CurrentTrain: epoch  0, batch     3 | loss: 6.7379789des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.28956413269043 2.0432000160217285
CurrentTrain: epoch  1, batch     0 | loss: 7.2895641des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.866964340209961 2.114536762237549
CurrentTrain: epoch  1, batch     1 | loss: 5.8669643des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.863515853881836 2.037263870239258
CurrentTrain: epoch  1, batch     2 | loss: 5.8635159des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.519474506378174 0.7528674006462097
CurrentTrain: epoch  1, batch     3 | loss: 7.5194745des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.359244346618652 1.9290772676467896
CurrentTrain: epoch  2, batch     0 | loss: 5.3592443des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.33659553527832 1.8897130489349365
CurrentTrain: epoch  2, batch     1 | loss: 6.3365955des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.014835357666016 1.902620792388916
CurrentTrain: epoch  2, batch     2 | loss: 6.0148354des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.046825885772705 0.0
CurrentTrain: epoch  2, batch     3 | loss: 3.0468259des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.404302597045898 1.4940738677978516
CurrentTrain: epoch  3, batch     0 | loss: 4.4043026des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.157979965209961 1.8170886039733887
CurrentTrain: epoch  3, batch     1 | loss: 6.1579800des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.011770725250244 1.7963635921478271
CurrentTrain: epoch  3, batch     2 | loss: 6.0117707des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.8179423809051514 0.6964555382728577
CurrentTrain: epoch  3, batch     3 | loss: 2.8179424des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.250244617462158 1.8707326650619507
CurrentTrain: epoch  4, batch     0 | loss: 5.2502446des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.961024284362793 1.8360955715179443
CurrentTrain: epoch  4, batch     1 | loss: 4.9610243des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.5885748863220215 1.7767693996429443
CurrentTrain: epoch  4, batch     2 | loss: 5.5885749des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.2963080406188965 0.5880886316299438
CurrentTrain: epoch  4, batch     3 | loss: 4.2963080des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.547183990478516 1.8995733261108398
CurrentTrain: epoch  5, batch     0 | loss: 5.5471840des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.722951889038086 1.9900761842727661
CurrentTrain: epoch  5, batch     1 | loss: 4.7229519des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.287619590759277 1.8045387268066406
CurrentTrain: epoch  5, batch     2 | loss: 5.2876196des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7173829078674316 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.7173829des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.157568454742432 2.044347047805786
CurrentTrain: epoch  6, batch     0 | loss: 5.1575685des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.852770805358887 1.7775802612304688
CurrentTrain: epoch  6, batch     1 | loss: 4.8527708des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.774647235870361 1.8821907043457031
CurrentTrain: epoch  6, batch     2 | loss: 4.7746472des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.3874855041503906 0.6559500098228455
CurrentTrain: epoch  6, batch     3 | loss: 3.3874855des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.281295299530029 1.7693355083465576
CurrentTrain: epoch  7, batch     0 | loss: 5.2812953des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.646557331085205 2.023867607116699
CurrentTrain: epoch  7, batch     1 | loss: 4.6465573des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.008129119873047 1.8575819730758667
CurrentTrain: epoch  7, batch     2 | loss: 4.0081291des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.15773606300354 0.478972852230072
CurrentTrain: epoch  7, batch     3 | loss: 2.1577361des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.251133441925049 1.8702055215835571
CurrentTrain: epoch  8, batch     0 | loss: 4.2511334des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.916866302490234 1.9090577363967896
CurrentTrain: epoch  8, batch     1 | loss: 4.9168663des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.195880889892578 1.5379633903503418
CurrentTrain: epoch  8, batch     2 | loss: 4.1958809des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1839067935943604 0.5885254144668579
CurrentTrain: epoch  8, batch     3 | loss: 2.1839068des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.025667667388916 1.7177767753601074
CurrentTrain: epoch  9, batch     0 | loss: 4.0256677des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.647456645965576 1.8258373737335205
CurrentTrain: epoch  9, batch     1 | loss: 4.6474566des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9874043464660645 1.8418762683868408
CurrentTrain: epoch  9, batch     2 | loss: 3.9874043des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.238519668579102 0.5649377107620239
CurrentTrain: epoch  9, batch     3 | loss: 4.2385197
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.5950112342834473 2.5899691581726074
MemoryTrain:  epoch  0, batch     0 | loss: 2.5950112des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9243308305740356 2.7248787879943848
MemoryTrain:  epoch  0, batch     1 | loss: 1.9243308des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9054694175720215 1.936009407043457
MemoryTrain:  epoch  0, batch     2 | loss: 1.9054694des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.974026679992676 2.558631658554077
MemoryTrain:  epoch  1, batch     0 | loss: 2.9740267des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1473796367645264 2.6501269340515137
MemoryTrain:  epoch  1, batch     1 | loss: 2.1473796des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4790730476379395 1.937449336051941
MemoryTrain:  epoch  1, batch     2 | loss: 1.4790730des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7212198972702026 2.6573758125305176
MemoryTrain:  epoch  2, batch     0 | loss: 1.7212199des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8968560695648193 2.614468812942505
MemoryTrain:  epoch  2, batch     1 | loss: 1.8968561des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2713932991027832 1.8802968263626099
MemoryTrain:  epoch  2, batch     2 | loss: 1.2713933des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5318777561187744 2.676164150238037
MemoryTrain:  epoch  3, batch     0 | loss: 1.5318778des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6409194469451904 2.5268192291259766
MemoryTrain:  epoch  3, batch     1 | loss: 1.6409194des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2514755725860596 1.8999829292297363
MemoryTrain:  epoch  3, batch     2 | loss: 1.2514756des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.470804214477539 2.5796260833740234
MemoryTrain:  epoch  4, batch     0 | loss: 1.4708042des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4268100261688232 2.5624024868011475
MemoryTrain:  epoch  4, batch     1 | loss: 1.4268100des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1385724544525146 1.884283423423767
MemoryTrain:  epoch  4, batch     2 | loss: 1.1385725des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3664050102233887 2.6037611961364746
MemoryTrain:  epoch  5, batch     0 | loss: 1.3664050des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.498720645904541 2.572072982788086
MemoryTrain:  epoch  5, batch     1 | loss: 1.4987206des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.076690673828125 1.7699639797210693
MemoryTrain:  epoch  5, batch     2 | loss: 1.0766907des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3806110620498657 2.531999111175537
MemoryTrain:  epoch  6, batch     0 | loss: 1.3806111des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.383124589920044 2.511831283569336
MemoryTrain:  epoch  6, batch     1 | loss: 1.3831246des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0263726711273193 1.9224292039871216
MemoryTrain:  epoch  6, batch     2 | loss: 1.0263727des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3372557163238525 2.482862949371338
MemoryTrain:  epoch  7, batch     0 | loss: 1.3372557des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3662434816360474 2.546154499053955
MemoryTrain:  epoch  7, batch     1 | loss: 1.3662435des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0531047582626343 1.8382437229156494
MemoryTrain:  epoch  7, batch     2 | loss: 1.0531048des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.336452841758728 2.523221015930176
MemoryTrain:  epoch  8, batch     0 | loss: 1.3364528des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4455841779708862 2.5732293128967285
MemoryTrain:  epoch  8, batch     1 | loss: 1.4455842des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0490397214889526 1.669053316116333
MemoryTrain:  epoch  8, batch     2 | loss: 1.0490397des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.341966986656189 2.5173521041870117
MemoryTrain:  epoch  9, batch     0 | loss: 1.3419670des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3045707941055298 2.4821252822875977
MemoryTrain:  epoch  9, batch     1 | loss: 1.3045708des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.9364023208618164 1.677512764930725
MemoryTrain:  epoch  9, batch     2 | loss: 0.9364023
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 72.26%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 71.44%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 67.96%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 86.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 87.28%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 87.18%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.29%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.09%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.10%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 85.16%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 84.04%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 82.77%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 80.42%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 79.80%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.90%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 79.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 80.19%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 80.34%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 79.52%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 78.65%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 77.87%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 77.25%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 76.44%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 75.78%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 74.93%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 73.28%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 72.49%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 71.71%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 71.08%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.13%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 72.86%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 72.10%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 71.90%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 71.60%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 70.94%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 70.78%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 70.47%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 69.88%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 69.55%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 68.70%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 68.16%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 67.16%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 66.70%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 68.62%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 68.58%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 68.43%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 68.00%   [EVAL] batch:  159 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  161 | acc: 31.25%,  total acc: 67.40%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.38%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 67.13%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 66.85%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 66.33%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 67.72%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.43%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 67.17%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 66.92%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.70%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 66.55%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 66.40%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 66.47%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.93%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 67.15%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 68.94%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 68.80%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 68.59%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 68.72%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 68.42%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 68.20%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 68.10%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 67.95%   
cur_acc:  ['0.9415', '0.5883', '0.7391', '0.6796']
his_acc:  ['0.9415', '0.7690', '0.7410', '0.6795']
Clustering into  24  clusters
Clusters:  [ 9  0 23 14  4  4  7  1 18 17  4  3 16  6 22 21 13 18 10 18  8  1  1  4
  6 17  9  4  3  3  0 10 19  3 12  3  2  8  5  8  0 15  2  5  1 20 18  7
 11 10]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.866601943969727 2.1550188064575195
CurrentTrain: epoch  0, batch     0 | loss: 5.8666019des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.992986679077148 2.0685224533081055
CurrentTrain: epoch  0, batch     1 | loss: 7.9929867des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.250265121459961 2.1071858406066895
CurrentTrain: epoch  0, batch     2 | loss: 7.2502651des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.885767936706543 0.6314797401428223
CurrentTrain: epoch  0, batch     3 | loss: 4.8857679des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.496209144592285 1.9281576871871948
CurrentTrain: epoch  1, batch     0 | loss: 6.4962091des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.810119152069092 2.143512010574341
CurrentTrain: epoch  1, batch     1 | loss: 5.8101192des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.214412689208984 1.8069450855255127
CurrentTrain: epoch  1, batch     2 | loss: 5.2144127des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.80036735534668 0.6376246213912964
CurrentTrain: epoch  1, batch     3 | loss: 4.8003674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.493156433105469 1.8442785739898682
CurrentTrain: epoch  2, batch     0 | loss: 5.4931564des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.072300434112549 1.716957688331604
CurrentTrain: epoch  2, batch     1 | loss: 5.0723004des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.224740028381348 1.9824483394622803
CurrentTrain: epoch  2, batch     2 | loss: 4.2247400des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.190798759460449 0.539577066898346
CurrentTrain: epoch  2, batch     3 | loss: 6.1907988des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.525765419006348 1.8123490810394287
CurrentTrain: epoch  3, batch     0 | loss: 4.5257654des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.403348445892334 1.6482899188995361
CurrentTrain: epoch  3, batch     1 | loss: 4.4033484des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.646522045135498 1.962965726852417
CurrentTrain: epoch  3, batch     2 | loss: 4.6465220des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.102997779846191 0.5722233057022095
CurrentTrain: epoch  3, batch     3 | loss: 6.1029978des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.060273170471191 1.858773112297058
CurrentTrain: epoch  4, batch     0 | loss: 4.0602732des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.052911281585693 1.6883878707885742
CurrentTrain: epoch  4, batch     1 | loss: 5.0529113des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.005434036254883 1.9894418716430664
CurrentTrain: epoch  4, batch     2 | loss: 4.0054340des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.679281234741211 0.6565556526184082
CurrentTrain: epoch  4, batch     3 | loss: 4.6792812des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.447290420532227 2.06191349029541
CurrentTrain: epoch  5, batch     0 | loss: 4.4472904des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.020681381225586 1.9662971496582031
CurrentTrain: epoch  5, batch     1 | loss: 4.0206814des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.152593612670898 1.914565086364746
CurrentTrain: epoch  5, batch     2 | loss: 4.1525936des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.9810128211975098 0.5684453248977661
CurrentTrain: epoch  5, batch     3 | loss: 2.9810128des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.9098172187805176 1.8464958667755127
CurrentTrain: epoch  6, batch     0 | loss: 3.9098172des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.352148532867432 2.0389082431793213
CurrentTrain: epoch  6, batch     1 | loss: 4.3521485des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7513251304626465 2.0385842323303223
CurrentTrain: epoch  6, batch     2 | loss: 3.7513251des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.595663070678711 0.6557369232177734
CurrentTrain: epoch  6, batch     3 | loss: 2.5956631des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.644171714782715 1.7753667831420898
CurrentTrain: epoch  7, batch     0 | loss: 3.6441717des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.606597423553467 1.7519949674606323
CurrentTrain: epoch  7, batch     1 | loss: 3.6065974des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.0125932693481445 1.9580509662628174
CurrentTrain: epoch  7, batch     2 | loss: 4.0125933des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6532328128814697 0.48864510655403137
CurrentTrain: epoch  7, batch     3 | loss: 3.6532328des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.7798733711242676 1.9325268268585205
CurrentTrain: epoch  8, batch     0 | loss: 3.7798734des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.615706443786621 1.9562346935272217
CurrentTrain: epoch  8, batch     1 | loss: 3.6157064des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.8103787899017334 1.752581238746643
CurrentTrain: epoch  8, batch     2 | loss: 3.8103788des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.4257090091705322 0.581220269203186
CurrentTrain: epoch  8, batch     3 | loss: 2.4257090des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.413501739501953 1.855243444442749
CurrentTrain: epoch  9, batch     0 | loss: 3.4135017des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.273007392883301 1.5681679248809814
CurrentTrain: epoch  9, batch     1 | loss: 3.2730074des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.660287618637085 1.7748826742172241
CurrentTrain: epoch  9, batch     2 | loss: 3.6602876des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.6899003982543945 0.4586600065231323
CurrentTrain: epoch  9, batch     3 | loss: 2.6899004
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9572559595108032 2.576742649078369
MemoryTrain:  epoch  0, batch     0 | loss: 1.9572560des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.126516342163086 2.7740001678466797
MemoryTrain:  epoch  0, batch     1 | loss: 2.1265163des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.6404030323028564 2.6740312576293945
MemoryTrain:  epoch  0, batch     2 | loss: 1.6404030des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.3309537172317505 0.5995100736618042
MemoryTrain:  epoch  0, batch     3 | loss: 0.3309537des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.2990784645080566 2.6816964149475098
MemoryTrain:  epoch  1, batch     0 | loss: 2.2990785des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.232555389404297 2.636897325515747
MemoryTrain:  epoch  1, batch     1 | loss: 2.2325554des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.908137321472168 2.6388370990753174
MemoryTrain:  epoch  1, batch     2 | loss: 1.9081373des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.0447816848754883 0.5868498086929321
MemoryTrain:  epoch  1, batch     3 | loss: 1.0447817des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.727218747138977 2.7151153087615967
MemoryTrain:  epoch  2, batch     0 | loss: 1.7272187des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5812681913375854 2.5461044311523438
MemoryTrain:  epoch  2, batch     1 | loss: 1.5812682des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.030714988708496 2.6371140480041504
MemoryTrain:  epoch  2, batch     2 | loss: 2.0307150des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.39699801802635193 0.6829637289047241
MemoryTrain:  epoch  2, batch     3 | loss: 0.3969980des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5329028367996216 2.6567306518554688
MemoryTrain:  epoch  3, batch     0 | loss: 1.5329028des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5514123439788818 2.5648202896118164
MemoryTrain:  epoch  3, batch     1 | loss: 1.5514123des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4197190999984741 2.603738784790039
MemoryTrain:  epoch  3, batch     2 | loss: 1.4197191des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.4298124611377716 0.7405569553375244
MemoryTrain:  epoch  3, batch     3 | loss: 0.4298125des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.367822289466858 2.53607177734375
MemoryTrain:  epoch  4, batch     0 | loss: 1.3678223des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3808510303497314 2.576569080352783
MemoryTrain:  epoch  4, batch     1 | loss: 1.3808510des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5045357942581177 2.647494316101074
MemoryTrain:  epoch  4, batch     2 | loss: 1.5045358des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.43963193893432617 0.7742086052894592
MemoryTrain:  epoch  4, batch     3 | loss: 0.4396319des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4346892833709717 2.4990386962890625
MemoryTrain:  epoch  5, batch     0 | loss: 1.4346893des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4082046747207642 2.583192825317383
MemoryTrain:  epoch  5, batch     1 | loss: 1.4082047des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.396070957183838 2.646193504333496
MemoryTrain:  epoch  5, batch     2 | loss: 1.3960710des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.3432830274105072 0.5826411247253418
MemoryTrain:  epoch  5, batch     3 | loss: 0.3432830des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3120975494384766 2.4948172569274902
MemoryTrain:  epoch  6, batch     0 | loss: 1.3120975des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3410604000091553 2.5869836807250977
MemoryTrain:  epoch  6, batch     1 | loss: 1.3410604des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4027433395385742 2.5768070220947266
MemoryTrain:  epoch  6, batch     2 | loss: 1.4027433des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.46358540654182434 0.5866570472717285
MemoryTrain:  epoch  6, batch     3 | loss: 0.4635854des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3560551404953003 2.5785303115844727
MemoryTrain:  epoch  7, batch     0 | loss: 1.3560551des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3364230394363403 2.4768893718719482
MemoryTrain:  epoch  7, batch     1 | loss: 1.3364230des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3327206373214722 2.5357184410095215
MemoryTrain:  epoch  7, batch     2 | loss: 1.3327206des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.33393800258636475 0.532360315322876
MemoryTrain:  epoch  7, batch     3 | loss: 0.3339380des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3559329509735107 2.5634541511535645
MemoryTrain:  epoch  8, batch     0 | loss: 1.3559330des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.278950810432434 2.445206880569458
MemoryTrain:  epoch  8, batch     1 | loss: 1.2789508des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3426421880722046 2.4442553520202637
MemoryTrain:  epoch  8, batch     2 | loss: 1.3426422des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.3556126058101654 0.6255859732627869
MemoryTrain:  epoch  8, batch     3 | loss: 0.3556126des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3051671981811523 2.4778904914855957
MemoryTrain:  epoch  9, batch     0 | loss: 1.3051672des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.306506633758545 2.501241683959961
MemoryTrain:  epoch  9, batch     1 | loss: 1.3065066des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.278324007987976 2.47739315032959
MemoryTrain:  epoch  9, batch     2 | loss: 1.2783240des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  0.25748908519744873 0.4407457411289215
MemoryTrain:  epoch  9, batch     3 | loss: 0.2574891
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 63.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 87.90%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 86.98%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 86.48%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 85.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 86.62%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 86.42%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 86.48%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 86.49%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.91%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 84.67%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 83.56%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 81.06%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 79.87%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 79.26%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 79.23%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.22%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 79.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 79.88%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 78.12%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 77.35%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 76.60%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 75.79%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 75.14%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 74.30%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 73.47%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 72.66%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 71.10%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 70.55%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 72.66%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.16%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 71.62%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.08%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 70.66%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 70.09%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 69.39%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 68.85%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 67.91%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 67.29%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 66.81%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 65.98%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 67.93%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 67.60%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 67.17%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 67.20%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 66.28%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 65.46%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 67.02%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 67.29%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.00%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 66.74%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 66.00%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 66.18%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 65.79%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 67.48%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 67.25%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 67.32%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 67.10%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 66.71%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 66.38%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 66.26%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 66.22%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.19%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 66.68%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 66.43%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 66.29%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 65.88%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 65.89%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 65.83%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.09%   
cur_acc:  ['0.9415', '0.5883', '0.7391', '0.6796', '0.6994']
his_acc:  ['0.9415', '0.7690', '0.7410', '0.6795', '0.6709']
Clustering into  29  clusters
Clusters:  [14 10 23 24  3  3  2  0  4  8  3  1 18 17  9 25 27  4  7  4  6  0  0  3
 17  8 14  3  1  1 28  7 21  1 15  1 20  6 19  6 10 26 16 19  0 11  4  2
 13  7  5  1  9 16 22  3 18 14 12  5]
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.000639915466309 2.0548858642578125
CurrentTrain: epoch  0, batch     0 | loss: 7.0006399des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.002278327941895 2.0171566009521484
CurrentTrain: epoch  0, batch     1 | loss: 8.0022783des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  7.884464740753174 2.0005788803100586
CurrentTrain: epoch  0, batch     2 | loss: 7.8844647des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.61600399017334 0.6484570503234863
CurrentTrain: epoch  0, batch     3 | loss: 8.6160040des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.9588236808776855 2.1545896530151367
CurrentTrain: epoch  1, batch     0 | loss: 5.9588237des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.272420883178711 2.0648655891418457
CurrentTrain: epoch  1, batch     1 | loss: 6.2724209des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.787304401397705 2.2566347122192383
CurrentTrain: epoch  1, batch     2 | loss: 6.7873044des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  8.193304061889648 0.6529837846755981
CurrentTrain: epoch  1, batch     3 | loss: 8.1933041des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.100268840789795 2.1209497451782227
CurrentTrain: epoch  2, batch     0 | loss: 6.1002688des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.403153419494629 2.1303396224975586
CurrentTrain: epoch  2, batch     1 | loss: 6.4031534des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.70051383972168 1.9599062204360962
CurrentTrain: epoch  2, batch     2 | loss: 5.7005138des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.6433308124542236 0.6844010353088379
CurrentTrain: epoch  2, batch     3 | loss: 3.6433308des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.392213821411133 2.1094207763671875
CurrentTrain: epoch  3, batch     0 | loss: 5.3922138des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.6155595779418945 1.8986148834228516
CurrentTrain: epoch  3, batch     1 | loss: 6.6155596des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.619400501251221 1.8421194553375244
CurrentTrain: epoch  3, batch     2 | loss: 4.6194005des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  6.557793140411377 0.7217046022415161
CurrentTrain: epoch  3, batch     3 | loss: 6.5577931des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.875631809234619 2.0270814895629883
CurrentTrain: epoch  4, batch     0 | loss: 5.8756318des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.250259876251221 1.9496376514434814
CurrentTrain: epoch  4, batch     1 | loss: 5.2502599des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.649336814880371 1.9921536445617676
CurrentTrain: epoch  4, batch     2 | loss: 4.6493368des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.193739175796509 0.5442739129066467
CurrentTrain: epoch  4, batch     3 | loss: 3.1937392des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.046206951141357 1.8334338665008545
CurrentTrain: epoch  5, batch     0 | loss: 5.0462070des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.576174736022949 2.080618381500244
CurrentTrain: epoch  5, batch     1 | loss: 5.5761747des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.201542854309082 1.7940452098846436
CurrentTrain: epoch  5, batch     2 | loss: 4.2015429des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.441406726837158 0.4550110995769501
CurrentTrain: epoch  5, batch     3 | loss: 4.4414067des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.477567672729492 2.021923542022705
CurrentTrain: epoch  6, batch     0 | loss: 4.4775677des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.6765947341918945 1.958336591720581
CurrentTrain: epoch  6, batch     1 | loss: 4.6765947des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.779965400695801 1.8170504570007324
CurrentTrain: epoch  6, batch     2 | loss: 4.7799654des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  5.0679030418396 0.7506411075592041
CurrentTrain: epoch  6, batch     3 | loss: 5.0679030des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.659715175628662 2.044196367263794
CurrentTrain: epoch  7, batch     0 | loss: 4.6597152des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.488165855407715 1.9714107513427734
CurrentTrain: epoch  7, batch     1 | loss: 4.4881659des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.188784599304199 1.9639641046524048
CurrentTrain: epoch  7, batch     2 | loss: 4.1887846des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.1317837238311768 0.6756082773208618
CurrentTrain: epoch  7, batch     3 | loss: 3.1317837des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.392225742340088 1.9112882614135742
CurrentTrain: epoch  8, batch     0 | loss: 4.3922257des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.956416368484497 1.915799617767334
CurrentTrain: epoch  8, batch     1 | loss: 3.9564164des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.234371185302734 1.8319053649902344
CurrentTrain: epoch  8, batch     2 | loss: 4.2343712des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.1821770668029785 0.0
CurrentTrain: epoch  8, batch     3 | loss: 2.1821771des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  3.597745180130005 1.7774767875671387
CurrentTrain: epoch  9, batch     0 | loss: 3.5977452des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.219213962554932 1.6188498735427856
CurrentTrain: epoch  9, batch     1 | loss: 4.2192140des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  4.107161045074463 1.9067853689193726
CurrentTrain: epoch  9, batch     2 | loss: 4.1071610des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.545119285583496 0.515005350112915
CurrentTrain: epoch  9, batch     3 | loss: 2.5451193
des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8444191217422485 2.6961989402770996
MemoryTrain:  epoch  0, batch     0 | loss: 1.8444191des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.921189308166504 2.7969231605529785
MemoryTrain:  epoch  0, batch     1 | loss: 1.9211893des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.467969298362732 2.739112138748169
MemoryTrain:  epoch  0, batch     2 | loss: 1.4679693des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.9389934539794922 2.411226272583008
MemoryTrain:  epoch  0, batch     3 | loss: 1.9389935des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.365065336227417 2.7591257095336914
MemoryTrain:  epoch  1, batch     0 | loss: 2.3650653des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.662732720375061 2.6456525325775146
MemoryTrain:  epoch  1, batch     1 | loss: 1.6627327des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  2.3404617309570312 2.634308338165283
MemoryTrain:  epoch  1, batch     2 | loss: 2.3404617des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.8181010484695435 2.4318575859069824
MemoryTrain:  epoch  1, batch     3 | loss: 1.8181010des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7338404655456543 2.660012722015381
MemoryTrain:  epoch  2, batch     0 | loss: 1.7338405des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.7766071557998657 2.6637420654296875
MemoryTrain:  epoch  2, batch     1 | loss: 1.7766072des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5731149911880493 2.686405658721924
MemoryTrain:  epoch  2, batch     2 | loss: 1.5731150des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.303001880645752 2.3609237670898438
MemoryTrain:  epoch  2, batch     3 | loss: 1.3030019des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5559239387512207 2.6640195846557617
MemoryTrain:  epoch  3, batch     0 | loss: 1.5559239des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4820513725280762 2.5908870697021484
MemoryTrain:  epoch  3, batch     1 | loss: 1.4820514des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.5243409872055054 2.655660390853882
MemoryTrain:  epoch  3, batch     2 | loss: 1.5243410des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.702170968055725 2.4019975662231445
MemoryTrain:  epoch  3, batch     3 | loss: 1.7021710des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.460548758506775 2.7291083335876465
MemoryTrain:  epoch  4, batch     0 | loss: 1.4605488des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4825507402420044 2.583289384841919
MemoryTrain:  epoch  4, batch     1 | loss: 1.4825507des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4835673570632935 2.607562780380249
MemoryTrain:  epoch  4, batch     2 | loss: 1.4835674des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.2462259531021118 2.2134084701538086
MemoryTrain:  epoch  4, batch     3 | loss: 1.2462260des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.472165822982788 2.56714129447937
MemoryTrain:  epoch  5, batch     0 | loss: 1.4721658des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4109523296356201 2.677164077758789
MemoryTrain:  epoch  5, batch     1 | loss: 1.4109523des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.358277440071106 2.537750244140625
MemoryTrain:  epoch  5, batch     2 | loss: 1.3582774des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1951037645339966 2.2413570880889893
MemoryTrain:  epoch  5, batch     3 | loss: 1.1951038des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.43310546875 2.54642391204834
MemoryTrain:  epoch  6, batch     0 | loss: 1.4331055des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3699712753295898 2.604093551635742
MemoryTrain:  epoch  6, batch     1 | loss: 1.3699713des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.368458867073059 2.579406261444092
MemoryTrain:  epoch  6, batch     2 | loss: 1.3684589des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1446640491485596 2.2190911769866943
MemoryTrain:  epoch  6, batch     3 | loss: 1.1446640des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3140051364898682 2.4959867000579834
MemoryTrain:  epoch  7, batch     0 | loss: 1.3140051des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3666819334030151 2.58408260345459
MemoryTrain:  epoch  7, batch     1 | loss: 1.3666819des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3370965719223022 2.541745185852051
MemoryTrain:  epoch  7, batch     2 | loss: 1.3370966des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1962569952011108 2.293642044067383
MemoryTrain:  epoch  7, batch     3 | loss: 1.1962570des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.338544249534607 2.5216026306152344
MemoryTrain:  epoch  8, batch     0 | loss: 1.3385442des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3187084197998047 2.5289840698242188
MemoryTrain:  epoch  8, batch     1 | loss: 1.3187084des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.4028658866882324 2.5120224952697754
MemoryTrain:  epoch  8, batch     2 | loss: 1.4028659des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.170060396194458 2.196723461151123
MemoryTrain:  epoch  8, batch     3 | loss: 1.1700604des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3096768856048584 2.4741692543029785
MemoryTrain:  epoch  9, batch     0 | loss: 1.3096769des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.3469749689102173 2.460324764251709
MemoryTrain:  epoch  9, batch     1 | loss: 1.3469750des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.261653184890747 2.4246859550476074
MemoryTrain:  epoch  9, batch     2 | loss: 1.2616532des requires_grad: False
x requires_grad: True
des requires_grad: True
labels requires_grad: False
weight requires_grad: True
bias requires_grad: True
Losses:  1.1963586807250977 2.281792640686035
MemoryTrain:  epoch  9, batch     3 | loss: 1.1963587
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 72.97%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 73.33%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 73.16%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 72.84%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 72.28%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 86.84%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 86.53%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 86.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.35%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 86.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.71%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 84.38%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 83.27%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 82.01%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 80.78%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 79.60%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 78.99%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 79.05%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.22%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 77.97%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 77.55%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 76.91%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 76.13%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 75.22%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.76%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.99%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 72.37%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.56%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.99%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 69.23%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 68.48%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 67.95%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 70.15%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 69.09%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 68.58%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 68.19%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 67.15%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 66.79%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 66.60%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 65.42%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 65.09%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 64.07%   