#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.824397087097168 2.0209035873413086
CurrentTrain: epoch  0, batch     0 | loss: 13.8453007Losses:  12.468348503112793 1.591482162475586
CurrentTrain: epoch  0, batch     1 | loss: 14.0598307Losses:  12.67782211303711 1.7103097438812256
CurrentTrain: epoch  0, batch     2 | loss: 14.3881321Losses:  13.623159408569336 2.0320770740509033
CurrentTrain: epoch  0, batch     3 | loss: 15.6552362Losses:  13.832799911499023 1.340574860572815
CurrentTrain: epoch  0, batch     4 | loss: 15.1733751Losses:  13.600811004638672 1.4134941101074219
CurrentTrain: epoch  0, batch     5 | loss: 15.0143051Losses:  13.55824089050293 1.27338445186615
CurrentTrain: epoch  0, batch     6 | loss: 14.8316250Losses:  12.851722717285156 1.3275998830795288
CurrentTrain: epoch  0, batch     7 | loss: 14.1793222Losses:  13.660469055175781 1.053663730621338
CurrentTrain: epoch  0, batch     8 | loss: 14.7141323Losses:  12.498300552368164 1.2211201190948486
CurrentTrain: epoch  0, batch     9 | loss: 13.7194204Losses:  12.806596755981445 1.1664763689041138
CurrentTrain: epoch  0, batch    10 | loss: 13.9730730Losses:  13.334266662597656 0.9605990052223206
CurrentTrain: epoch  0, batch    11 | loss: 14.2948656Losses:  12.18533992767334 1.312747597694397
CurrentTrain: epoch  0, batch    12 | loss: 13.4980879Losses:  11.827184677124023 1.2946795225143433
CurrentTrain: epoch  0, batch    13 | loss: 13.1218643Losses:  12.346426010131836 1.318819284439087
CurrentTrain: epoch  0, batch    14 | loss: 13.6652451Losses:  12.256156921386719 0.9697737693786621
CurrentTrain: epoch  0, batch    15 | loss: 13.2259312Losses:  12.510323524475098 0.8018684387207031
CurrentTrain: epoch  0, batch    16 | loss: 13.3121920Losses:  12.230295181274414 1.3093163967132568
CurrentTrain: epoch  0, batch    17 | loss: 13.5396118Losses:  11.876347541809082 1.3643195629119873
CurrentTrain: epoch  0, batch    18 | loss: 13.2406673Losses:  12.454312324523926 1.64296555519104
CurrentTrain: epoch  0, batch    19 | loss: 14.0972776Losses:  12.933189392089844 1.362837791442871
CurrentTrain: epoch  0, batch    20 | loss: 14.2960272Losses:  12.700580596923828 1.4325857162475586
CurrentTrain: epoch  0, batch    21 | loss: 14.1331663Losses:  11.983713150024414 1.3877084255218506
CurrentTrain: epoch  0, batch    22 | loss: 13.3714218Losses:  12.047872543334961 0.8729424476623535
CurrentTrain: epoch  0, batch    23 | loss: 12.9208145Losses:  12.250310897827148 1.6873133182525635
CurrentTrain: epoch  0, batch    24 | loss: 13.9376240Losses:  12.130999565124512 1.3549991846084595
CurrentTrain: epoch  0, batch    25 | loss: 13.4859991Losses:  12.145263671875 1.2821677923202515
CurrentTrain: epoch  0, batch    26 | loss: 13.4274311Losses:  11.922674179077148 1.267034888267517
CurrentTrain: epoch  0, batch    27 | loss: 13.1897087Losses:  11.742912292480469 1.3740665912628174
CurrentTrain: epoch  0, batch    28 | loss: 13.1169786Losses:  11.384822845458984 1.2598955631256104
CurrentTrain: epoch  0, batch    29 | loss: 12.6447182Losses:  11.94849967956543 1.421616792678833
CurrentTrain: epoch  0, batch    30 | loss: 13.3701162Losses:  11.493017196655273 1.514359951019287
CurrentTrain: epoch  0, batch    31 | loss: 13.0073776Losses:  11.408402442932129 0.9529087543487549
CurrentTrain: epoch  0, batch    32 | loss: 12.3613110Losses:  11.236530303955078 1.3137500286102295
CurrentTrain: epoch  0, batch    33 | loss: 12.5502806Losses:  11.063101768493652 1.3010755777359009
CurrentTrain: epoch  0, batch    34 | loss: 12.3641777Losses:  11.252939224243164 1.363290548324585
CurrentTrain: epoch  0, batch    35 | loss: 12.6162300Losses:  10.873517990112305 1.3106555938720703
CurrentTrain: epoch  0, batch    36 | loss: 12.1841736Losses:  11.539288520812988 0.8138501644134521
CurrentTrain: epoch  0, batch    37 | loss: 12.3531389Losses:  11.216374397277832 1.085590124130249
CurrentTrain: epoch  0, batch    38 | loss: 12.3019648Losses:  10.997810363769531 1.0858179330825806
CurrentTrain: epoch  0, batch    39 | loss: 12.0836287Losses:  10.59819221496582 0.8610053658485413
CurrentTrain: epoch  0, batch    40 | loss: 11.4591980Losses:  10.843059539794922 0.946721076965332
CurrentTrain: epoch  0, batch    41 | loss: 11.7897806Losses:  10.54088306427002 1.1584317684173584
CurrentTrain: epoch  0, batch    42 | loss: 11.6993151Losses:  11.05672836303711 0.9265824556350708
CurrentTrain: epoch  0, batch    43 | loss: 11.9833107Losses:  10.43265438079834 1.0518028736114502
CurrentTrain: epoch  0, batch    44 | loss: 11.4844570Losses:  10.433533668518066 1.148106575012207
CurrentTrain: epoch  0, batch    45 | loss: 11.5816402Losses:  10.468358039855957 0.8676224946975708
CurrentTrain: epoch  0, batch    46 | loss: 11.3359804Losses:  10.548032760620117 0.986721396446228
CurrentTrain: epoch  0, batch    47 | loss: 11.5347538Losses:  10.481403350830078 0.9477207064628601
CurrentTrain: epoch  0, batch    48 | loss: 11.4291239Losses:  10.300912857055664 1.0708211660385132
CurrentTrain: epoch  0, batch    49 | loss: 11.3717337Losses:  9.849120140075684 0.7650099396705627
CurrentTrain: epoch  0, batch    50 | loss: 10.6141300Losses:  9.685803413391113 0.8428405523300171
CurrentTrain: epoch  0, batch    51 | loss: 10.5286436Losses:  10.17569351196289 0.8252752423286438
CurrentTrain: epoch  0, batch    52 | loss: 11.0009689Losses:  9.706289291381836 0.8076756000518799
CurrentTrain: epoch  0, batch    53 | loss: 10.5139647Losses:  9.391289710998535 0.7882872223854065
CurrentTrain: epoch  0, batch    54 | loss: 10.1795769Losses:  10.155083656311035 0.873009979724884
CurrentTrain: epoch  0, batch    55 | loss: 11.0280933Losses:  9.733175277709961 0.916494607925415
CurrentTrain: epoch  0, batch    56 | loss: 10.6496696Losses:  9.278182029724121 0.9860649108886719
CurrentTrain: epoch  0, batch    57 | loss: 10.2642469Losses:  9.756529808044434 0.8829638957977295
CurrentTrain: epoch  0, batch    58 | loss: 10.6394939Losses:  9.29413890838623 1.0164265632629395
CurrentTrain: epoch  0, batch    59 | loss: 10.3105659Losses:  9.75161361694336 0.9784639477729797
CurrentTrain: epoch  0, batch    60 | loss: 10.7300777Losses:  9.703929901123047 1.2238818407058716
CurrentTrain: epoch  0, batch    61 | loss: 10.9278116Losses:  9.478538513183594 0.7828182578086853
CurrentTrain: epoch  0, batch    62 | loss: 10.2613564Losses:  9.393296241760254 0.6401581168174744
CurrentTrain: epoch  0, batch    63 | loss: 10.0334539Losses:  8.86533260345459 0.7147061824798584
CurrentTrain: epoch  0, batch    64 | loss: 9.5800390Losses:  9.681266784667969 0.8923208713531494
CurrentTrain: epoch  0, batch    65 | loss: 10.5735874Losses:  9.463846206665039 0.8435420393943787
CurrentTrain: epoch  0, batch    66 | loss: 10.3073883Losses:  8.530994415283203 0.49466562271118164
CurrentTrain: epoch  0, batch    67 | loss: 9.0256596Losses:  9.430278778076172 0.7171585559844971
CurrentTrain: epoch  0, batch    68 | loss: 10.1474371Losses:  8.662191390991211 0.7910416722297668
CurrentTrain: epoch  0, batch    69 | loss: 9.4532328Losses:  8.895148277282715 0.6874954700469971
CurrentTrain: epoch  0, batch    70 | loss: 9.5826435Losses:  8.811272621154785 1.0236618518829346
CurrentTrain: epoch  0, batch    71 | loss: 9.8349342Losses:  9.181025505065918 0.6807680130004883
CurrentTrain: epoch  0, batch    72 | loss: 9.8617935Losses:  8.83019733428955 0.9821235537528992
CurrentTrain: epoch  0, batch    73 | loss: 9.8123207Losses:  8.403877258300781 0.774957001209259
CurrentTrain: epoch  0, batch    74 | loss: 9.1788340Losses:  8.336956024169922 0.846759557723999
CurrentTrain: epoch  0, batch    75 | loss: 9.1837158Losses:  9.12828254699707 0.9676926136016846
CurrentTrain: epoch  0, batch    76 | loss: 10.0959749Losses:  8.563142776489258 0.9975343942642212
CurrentTrain: epoch  0, batch    77 | loss: 9.5606775Losses:  8.562173843383789 0.8887730836868286
CurrentTrain: epoch  0, batch    78 | loss: 9.4509468Losses:  8.449468612670898 0.8613190650939941
CurrentTrain: epoch  0, batch    79 | loss: 9.3107872Losses:  9.27370834350586 1.0162687301635742
CurrentTrain: epoch  0, batch    80 | loss: 10.2899771Losses:  8.372053146362305 0.6209918260574341
CurrentTrain: epoch  0, batch    81 | loss: 8.9930449Losses:  8.304641723632812 0.7901656627655029
CurrentTrain: epoch  0, batch    82 | loss: 9.0948076Losses:  8.140880584716797 0.7629636526107788
CurrentTrain: epoch  0, batch    83 | loss: 8.9038439Losses:  8.426441192626953 0.6670839190483093
CurrentTrain: epoch  0, batch    84 | loss: 9.0935249Losses:  7.655140399932861 0.6634854674339294
CurrentTrain: epoch  0, batch    85 | loss: 8.3186255Losses:  8.371484756469727 0.6398414373397827
CurrentTrain: epoch  0, batch    86 | loss: 9.0113258Losses:  7.999643802642822 0.41525769233703613
CurrentTrain: epoch  0, batch    87 | loss: 8.4149017Losses:  7.704567909240723 0.4938235878944397
CurrentTrain: epoch  0, batch    88 | loss: 8.1983919Losses:  7.932014465332031 0.6439472436904907
CurrentTrain: epoch  0, batch    89 | loss: 8.5759621Losses:  7.673891067504883 0.765390932559967
CurrentTrain: epoch  0, batch    90 | loss: 8.4392824Losses:  7.697422027587891 0.8974053859710693
CurrentTrain: epoch  0, batch    91 | loss: 8.5948277Losses:  7.768701553344727 0.6524830460548401
CurrentTrain: epoch  0, batch    92 | loss: 8.4211845Losses:  7.6858110427856445 0.5043397545814514
CurrentTrain: epoch  0, batch    93 | loss: 8.1901512Losses:  6.88484001159668 0.5259374976158142
CurrentTrain: epoch  0, batch    94 | loss: 7.4107776Losses:  7.703064918518066 0.7962712049484253
CurrentTrain: epoch  0, batch    95 | loss: 8.4993362Losses:  7.311330795288086 0.8191884756088257
CurrentTrain: epoch  0, batch    96 | loss: 8.1305189Losses:  7.598636627197266 1.1851656436920166
CurrentTrain: epoch  0, batch    97 | loss: 8.7838020Losses:  7.090075969696045 0.6625642776489258
CurrentTrain: epoch  0, batch    98 | loss: 7.7526402Losses:  7.271050453186035 0.6487083435058594
CurrentTrain: epoch  0, batch    99 | loss: 7.9197588Losses:  7.882214546203613 0.5473684072494507
CurrentTrain: epoch  0, batch   100 | loss: 8.4295826Losses:  6.975232124328613 0.8827204704284668
CurrentTrain: epoch  0, batch   101 | loss: 7.8579526Losses:  6.633394241333008 0.6224918365478516
CurrentTrain: epoch  0, batch   102 | loss: 7.2558861Losses:  6.795973777770996 0.6290895938873291
CurrentTrain: epoch  0, batch   103 | loss: 7.4250631Losses:  6.776740550994873 0.5764782428741455
CurrentTrain: epoch  0, batch   104 | loss: 7.3532190Losses:  6.869155406951904 0.21493707597255707
CurrentTrain: epoch  0, batch   105 | loss: 7.0840926Losses:  6.66190242767334 0.6579576134681702
CurrentTrain: epoch  0, batch   106 | loss: 7.3198600Losses:  7.007353782653809 0.7539272308349609
CurrentTrain: epoch  0, batch   107 | loss: 7.7612810Losses:  6.7484331130981445 0.7889162302017212
CurrentTrain: epoch  0, batch   108 | loss: 7.5373492Losses:  6.842741012573242 0.8281122446060181
CurrentTrain: epoch  0, batch   109 | loss: 7.6708531Losses:  6.27208948135376 0.5387786030769348
CurrentTrain: epoch  0, batch   110 | loss: 6.8108683Losses:  6.682055473327637 0.5794230699539185
CurrentTrain: epoch  0, batch   111 | loss: 7.2614784Losses:  5.9124250411987305 0.3896246552467346
CurrentTrain: epoch  0, batch   112 | loss: 6.3020496Losses:  6.725105285644531 0.7290723323822021
CurrentTrain: epoch  0, batch   113 | loss: 7.4541779Losses:  6.197905540466309 0.4341543912887573
CurrentTrain: epoch  0, batch   114 | loss: 6.6320601Losses:  6.5333662033081055 0.8643476963043213
CurrentTrain: epoch  0, batch   115 | loss: 7.3977137Losses:  6.229909896850586 0.4949105978012085
CurrentTrain: epoch  0, batch   116 | loss: 6.7248206Losses:  6.049079418182373 0.5879440307617188
CurrentTrain: epoch  0, batch   117 | loss: 6.6370234Losses:  5.8673930168151855 0.4583684802055359
CurrentTrain: epoch  0, batch   118 | loss: 6.3257613Losses:  6.108917236328125 0.5575557351112366
CurrentTrain: epoch  0, batch   119 | loss: 6.6664729Losses:  5.827061653137207 0.46925491094589233
CurrentTrain: epoch  0, batch   120 | loss: 6.2963166Losses:  5.802149772644043 0.459659218788147
CurrentTrain: epoch  0, batch   121 | loss: 6.2618089Losses:  5.810685157775879 0.4008191227912903
CurrentTrain: epoch  0, batch   122 | loss: 6.2115045Losses:  5.912975311279297 0.5323163270950317
CurrentTrain: epoch  0, batch   123 | loss: 6.4452915Losses:  6.305144309997559 0.56178879737854
CurrentTrain: epoch  0, batch   124 | loss: 6.8669329Losses:  5.486779689788818 0.4164595603942871
CurrentTrain: epoch  1, batch     0 | loss: 5.9032393Losses:  6.0237226486206055 0.5689783692359924
CurrentTrain: epoch  1, batch     1 | loss: 6.5927010Losses:  5.327976226806641 0.2541956603527069
CurrentTrain: epoch  1, batch     2 | loss: 5.5821719Losses:  5.239997863769531 0.4916287958621979
CurrentTrain: epoch  1, batch     3 | loss: 5.7316265Losses:  5.858345985412598 0.5251569747924805
CurrentTrain: epoch  1, batch     4 | loss: 6.3835030Losses:  5.366600036621094 0.6269947290420532
CurrentTrain: epoch  1, batch     5 | loss: 5.9935946Losses:  5.235841274261475 0.36969703435897827
CurrentTrain: epoch  1, batch     6 | loss: 5.6055384Losses:  5.560461044311523 0.5339387655258179
CurrentTrain: epoch  1, batch     7 | loss: 6.0943999Losses:  5.62490701675415 0.4266839623451233
CurrentTrain: epoch  1, batch     8 | loss: 6.0515909Losses:  5.982354164123535 0.3950082063674927
CurrentTrain: epoch  1, batch     9 | loss: 6.3773623Losses:  5.183377265930176 0.2591327726840973
CurrentTrain: epoch  1, batch    10 | loss: 5.4425101Losses:  5.224971294403076 0.40251320600509644
CurrentTrain: epoch  1, batch    11 | loss: 5.6274843Losses:  5.202220439910889 0.5027175545692444
CurrentTrain: epoch  1, batch    12 | loss: 5.7049379Losses:  5.333747863769531 0.4231363534927368
CurrentTrain: epoch  1, batch    13 | loss: 5.7568841Losses:  5.085289001464844 0.4207789897918701
CurrentTrain: epoch  1, batch    14 | loss: 5.5060682Losses:  5.138519763946533 0.2389276623725891
CurrentTrain: epoch  1, batch    15 | loss: 5.3774476Losses:  4.998682022094727 0.31539028882980347
CurrentTrain: epoch  1, batch    16 | loss: 5.3140721Losses:  5.408236503601074 0.22395780682563782
CurrentTrain: epoch  1, batch    17 | loss: 5.6321945Losses:  5.538121223449707 0.5421217679977417
CurrentTrain: epoch  1, batch    18 | loss: 6.0802431Losses:  5.294208526611328 0.47670674324035645
CurrentTrain: epoch  1, batch    19 | loss: 5.7709150Losses:  6.012936592102051 0.6033307909965515
CurrentTrain: epoch  1, batch    20 | loss: 6.6162672Losses:  4.874332904815674 0.24781851470470428
CurrentTrain: epoch  1, batch    21 | loss: 5.1221514Losses:  5.365335464477539 0.5053598284721375
CurrentTrain: epoch  1, batch    22 | loss: 5.8706951Losses:  5.180127143859863 0.5141493082046509
CurrentTrain: epoch  1, batch    23 | loss: 5.6942763Losses:  5.3728837966918945 0.518243134021759
CurrentTrain: epoch  1, batch    24 | loss: 5.8911271Losses:  5.266689300537109 0.359790563583374
CurrentTrain: epoch  1, batch    25 | loss: 5.6264801Losses:  5.647842884063721 0.469276487827301
CurrentTrain: epoch  1, batch    26 | loss: 6.1171193Losses:  5.503581523895264 0.5017679929733276
CurrentTrain: epoch  1, batch    27 | loss: 6.0053496Losses:  5.171813011169434 0.41552022099494934
CurrentTrain: epoch  1, batch    28 | loss: 5.5873332Losses:  5.097808361053467 0.2698303461074829
CurrentTrain: epoch  1, batch    29 | loss: 5.3676386Losses:  5.539040565490723 0.5575751066207886
CurrentTrain: epoch  1, batch    30 | loss: 6.0966158Losses:  5.701883792877197 0.30907803773880005
CurrentTrain: epoch  1, batch    31 | loss: 6.0109620Losses:  5.582127571105957 0.17312514781951904
CurrentTrain: epoch  1, batch    32 | loss: 5.7552528Losses:  5.41811466217041 0.5313761234283447
CurrentTrain: epoch  1, batch    33 | loss: 5.9494905Losses:  5.245293617248535 0.29476168751716614
CurrentTrain: epoch  1, batch    34 | loss: 5.5400553Losses:  4.895288467407227 0.32487189769744873
CurrentTrain: epoch  1, batch    35 | loss: 5.2201605Losses:  5.0445475578308105 0.37653446197509766
CurrentTrain: epoch  1, batch    36 | loss: 5.4210820Losses:  5.0907793045043945 0.3176901340484619
CurrentTrain: epoch  1, batch    37 | loss: 5.4084692Losses:  5.248657703399658 0.366973340511322
CurrentTrain: epoch  1, batch    38 | loss: 5.6156311Losses:  5.1073808670043945 0.38938456773757935
CurrentTrain: epoch  1, batch    39 | loss: 5.4967656Losses:  5.568830490112305 0.45403310656547546
CurrentTrain: epoch  1, batch    40 | loss: 6.0228634Losses:  5.19427490234375 0.39261725544929504
CurrentTrain: epoch  1, batch    41 | loss: 5.5868921Losses:  5.434324741363525 0.57211834192276
CurrentTrain: epoch  1, batch    42 | loss: 6.0064430Losses:  5.21834659576416 0.3621923327445984
CurrentTrain: epoch  1, batch    43 | loss: 5.5805387Losses:  5.514261245727539 0.6165792942047119
CurrentTrain: epoch  1, batch    44 | loss: 6.1308403Losses:  5.183193206787109 0.4177711606025696
CurrentTrain: epoch  1, batch    45 | loss: 5.6009645Losses:  5.822864532470703 0.4533110558986664
CurrentTrain: epoch  1, batch    46 | loss: 6.2761755Losses:  4.960272789001465 0.26488369703292847
CurrentTrain: epoch  1, batch    47 | loss: 5.2251563Losses:  5.048879623413086 0.4233075976371765
CurrentTrain: epoch  1, batch    48 | loss: 5.4721870Losses:  5.37709903717041 0.3064955770969391
CurrentTrain: epoch  1, batch    49 | loss: 5.6835947Losses:  4.884807109832764 0.24769604206085205
CurrentTrain: epoch  1, batch    50 | loss: 5.1325030Losses:  4.888799667358398 0.2782176434993744
CurrentTrain: epoch  1, batch    51 | loss: 5.1670175Losses:  4.9015069007873535 0.5043347477912903
CurrentTrain: epoch  1, batch    52 | loss: 5.4058418Losses:  5.284489631652832 0.3146559000015259
CurrentTrain: epoch  1, batch    53 | loss: 5.5991454Losses:  4.889571189880371 0.264411062002182
CurrentTrain: epoch  1, batch    54 | loss: 5.1539822Losses:  5.293107032775879 0.6095423698425293
CurrentTrain: epoch  1, batch    55 | loss: 5.9026494Losses:  5.066431045532227 0.41187456250190735
CurrentTrain: epoch  1, batch    56 | loss: 5.4783058Losses:  4.686986923217773 0.17221203446388245
CurrentTrain: epoch  1, batch    57 | loss: 4.8591990Losses:  5.079551696777344 0.23564447462558746
CurrentTrain: epoch  1, batch    58 | loss: 5.3151960Losses:  4.798531532287598 0.3475044369697571
CurrentTrain: epoch  1, batch    59 | loss: 5.1460361Losses:  4.899566173553467 0.2381702959537506
CurrentTrain: epoch  1, batch    60 | loss: 5.1377363Losses:  4.80211067199707 0.229622483253479
CurrentTrain: epoch  1, batch    61 | loss: 5.0317330Losses:  4.681612968444824 0.22698691487312317
CurrentTrain: epoch  1, batch    62 | loss: 4.9085999Losses:  4.865451335906982 0.3219921588897705
CurrentTrain: epoch  1, batch    63 | loss: 5.1874437Losses:  5.226264476776123 0.39287859201431274
CurrentTrain: epoch  1, batch    64 | loss: 5.6191430Losses:  4.521889686584473 0.2471843957901001
CurrentTrain: epoch  1, batch    65 | loss: 4.7690740Losses:  4.8222126960754395 0.2624516189098358
CurrentTrain: epoch  1, batch    66 | loss: 5.0846643Losses:  4.862186908721924 0.18904653191566467
CurrentTrain: epoch  1, batch    67 | loss: 5.0512333Losses:  5.079475402832031 0.5137214064598083
CurrentTrain: epoch  1, batch    68 | loss: 5.5931969Losses:  5.320225238800049 0.5324463844299316
CurrentTrain: epoch  1, batch    69 | loss: 5.8526716Losses:  5.107515335083008 0.3773183822631836
CurrentTrain: epoch  1, batch    70 | loss: 5.4848337Losses:  4.625486373901367 0.21540309488773346
CurrentTrain: epoch  1, batch    71 | loss: 4.8408895Losses:  5.071075439453125 0.26947999000549316
CurrentTrain: epoch  1, batch    72 | loss: 5.3405552Losses:  5.146487236022949 0.36381959915161133
CurrentTrain: epoch  1, batch    73 | loss: 5.5103068Losses:  5.185451507568359 0.5340768098831177
CurrentTrain: epoch  1, batch    74 | loss: 5.7195282Losses:  4.929989814758301 0.18728995323181152
CurrentTrain: epoch  1, batch    75 | loss: 5.1172800Losses:  4.9293928146362305 0.5438772439956665
CurrentTrain: epoch  1, batch    76 | loss: 5.4732699Losses:  5.828815460205078 0.6495766043663025
CurrentTrain: epoch  1, batch    77 | loss: 6.4783921Losses:  4.81995153427124 0.25881266593933105
CurrentTrain: epoch  1, batch    78 | loss: 5.0787640Losses:  4.528765678405762 0.2320668250322342
CurrentTrain: epoch  1, batch    79 | loss: 4.7608323Losses:  5.216494083404541 0.43254023790359497
CurrentTrain: epoch  1, batch    80 | loss: 5.6490345Losses:  5.089003086090088 0.3243896961212158
CurrentTrain: epoch  1, batch    81 | loss: 5.4133930Losses:  5.141313552856445 0.3044361472129822
CurrentTrain: epoch  1, batch    82 | loss: 5.4457498Losses:  5.532231330871582 0.3869604766368866
CurrentTrain: epoch  1, batch    83 | loss: 5.9191918Losses:  4.831429481506348 0.41279590129852295
CurrentTrain: epoch  1, batch    84 | loss: 5.2442255Losses:  5.098203182220459 0.4975123107433319
CurrentTrain: epoch  1, batch    85 | loss: 5.5957155Losses:  5.105846405029297 0.3834683895111084
CurrentTrain: epoch  1, batch    86 | loss: 5.4893150Losses:  5.0946269035339355 0.5049575567245483
CurrentTrain: epoch  1, batch    87 | loss: 5.5995846Losses:  5.371248722076416 0.4008844494819641
CurrentTrain: epoch  1, batch    88 | loss: 5.7721334Losses:  4.900948524475098 0.32959794998168945
CurrentTrain: epoch  1, batch    89 | loss: 5.2305465Losses:  4.654247760772705 0.3186056613922119
CurrentTrain: epoch  1, batch    90 | loss: 4.9728537Losses:  4.706181049346924 0.2423086166381836
CurrentTrain: epoch  1, batch    91 | loss: 4.9484897Losses:  4.497346878051758 0.29973945021629333
CurrentTrain: epoch  1, batch    92 | loss: 4.7970862Losses:  4.832481384277344 0.37893491983413696
CurrentTrain: epoch  1, batch    93 | loss: 5.2114162Losses:  4.733493804931641 0.3693046569824219
CurrentTrain: epoch  1, batch    94 | loss: 5.1027985Losses:  5.507347106933594 0.42918652296066284
CurrentTrain: epoch  1, batch    95 | loss: 5.9365335Losses:  4.948011875152588 0.3954984247684479
CurrentTrain: epoch  1, batch    96 | loss: 5.3435102Losses:  5.018149375915527 0.34672778844833374
CurrentTrain: epoch  1, batch    97 | loss: 5.3648772Losses:  4.82346248626709 0.3667532801628113
CurrentTrain: epoch  1, batch    98 | loss: 5.1902156Losses:  4.508663654327393 0.32888999581336975
CurrentTrain: epoch  1, batch    99 | loss: 4.8375535Losses:  4.468263626098633 0.16188839077949524
CurrentTrain: epoch  1, batch   100 | loss: 4.6301522Losses:  4.870802879333496 0.28360170125961304
CurrentTrain: epoch  1, batch   101 | loss: 5.1544046Losses:  4.916060924530029 0.4827597737312317
CurrentTrain: epoch  1, batch   102 | loss: 5.3988209Losses:  5.095968246459961 0.27782779932022095
CurrentTrain: epoch  1, batch   103 | loss: 5.3737960Losses:  4.448115825653076 0.24473059177398682
CurrentTrain: epoch  1, batch   104 | loss: 4.6928463Losses:  4.9553680419921875 0.38070279359817505
CurrentTrain: epoch  1, batch   105 | loss: 5.3360710Losses:  4.506807327270508 0.24862146377563477
CurrentTrain: epoch  1, batch   106 | loss: 4.7554288Losses:  4.596837043762207 0.20331710577011108
CurrentTrain: epoch  1, batch   107 | loss: 4.8001542Losses:  4.802011966705322 0.4545615613460541
CurrentTrain: epoch  1, batch   108 | loss: 5.2565737Losses:  5.050897598266602 0.22509241104125977
CurrentTrain: epoch  1, batch   109 | loss: 5.2759900Losses:  4.407988548278809 0.16548368334770203
CurrentTrain: epoch  1, batch   110 | loss: 4.5734720Losses:  4.263651371002197 0.13460883498191833
CurrentTrain: epoch  1, batch   111 | loss: 4.3982601Losses:  4.632260322570801 0.32138362526893616
CurrentTrain: epoch  1, batch   112 | loss: 4.9536438Losses:  4.27380895614624 0.1702941656112671
CurrentTrain: epoch  1, batch   113 | loss: 4.4441032Losses:  4.6302361488342285 0.3054168224334717
CurrentTrain: epoch  1, batch   114 | loss: 4.9356527Losses:  4.639867782592773 0.2994678020477295
CurrentTrain: epoch  1, batch   115 | loss: 4.9393358Losses:  4.756961822509766 0.2763539254665375
CurrentTrain: epoch  1, batch   116 | loss: 5.0333157Losses:  4.3908843994140625 0.18730777502059937
CurrentTrain: epoch  1, batch   117 | loss: 4.5781922Losses:  5.001753807067871 0.41123270988464355
CurrentTrain: epoch  1, batch   118 | loss: 5.4129868Losses:  4.570955276489258 0.2774824798107147
CurrentTrain: epoch  1, batch   119 | loss: 4.8484378Losses:  5.165099143981934 0.48283857107162476
CurrentTrain: epoch  1, batch   120 | loss: 5.6479378Losses:  4.723605155944824 0.3242513835430145
CurrentTrain: epoch  1, batch   121 | loss: 5.0478563Losses:  4.361501693725586 0.2617628276348114
CurrentTrain: epoch  1, batch   122 | loss: 4.6232643Losses:  4.704263687133789 0.24629294872283936
CurrentTrain: epoch  1, batch   123 | loss: 4.9505568Losses:  4.05931282043457 0.13572996854782104
CurrentTrain: epoch  1, batch   124 | loss: 4.1950426Losses:  4.1497015953063965 0.10168717056512833
CurrentTrain: epoch  2, batch     0 | loss: 4.2513885Losses:  4.861288070678711 0.2856302261352539
CurrentTrain: epoch  2, batch     1 | loss: 5.1469183Losses:  4.767189025878906 0.27586737275123596
CurrentTrain: epoch  2, batch     2 | loss: 5.0430565Losses:  4.389555931091309 0.1904970407485962
CurrentTrain: epoch  2, batch     3 | loss: 4.5800529Losses:  4.395689964294434 0.2559274435043335
CurrentTrain: epoch  2, batch     4 | loss: 4.6516175Losses:  4.8215837478637695 0.3969241976737976
CurrentTrain: epoch  2, batch     5 | loss: 5.2185078Losses:  4.513643264770508 0.22511884570121765
CurrentTrain: epoch  2, batch     6 | loss: 4.7387619Losses:  4.235795021057129 0.24369078874588013
CurrentTrain: epoch  2, batch     7 | loss: 4.4794860Losses:  4.667728900909424 0.19662609696388245
CurrentTrain: epoch  2, batch     8 | loss: 4.8643551Losses:  4.365211486816406 0.16919538378715515
CurrentTrain: epoch  2, batch     9 | loss: 4.5344067Losses:  4.299346446990967 0.20149382948875427
CurrentTrain: epoch  2, batch    10 | loss: 4.5008402Losses:  4.332916736602783 0.28604841232299805
CurrentTrain: epoch  2, batch    11 | loss: 4.6189651Losses:  4.359578609466553 0.18921266496181488
CurrentTrain: epoch  2, batch    12 | loss: 4.5487914Losses:  4.578170299530029 0.17253708839416504
CurrentTrain: epoch  2, batch    13 | loss: 4.7507076Losses:  4.305594444274902 0.21908479928970337
CurrentTrain: epoch  2, batch    14 | loss: 4.5246792Losses:  4.931173324584961 0.29308414459228516
CurrentTrain: epoch  2, batch    15 | loss: 5.2242575Losses:  4.372376918792725 0.18961524963378906
CurrentTrain: epoch  2, batch    16 | loss: 4.5619922Losses:  5.847702980041504 0.37371477484703064
CurrentTrain: epoch  2, batch    17 | loss: 6.2214179Losses:  4.27764892578125 0.1675153225660324
CurrentTrain: epoch  2, batch    18 | loss: 4.4451642Losses:  4.487052917480469 0.156977117061615
CurrentTrain: epoch  2, batch    19 | loss: 4.6440301Losses:  4.2868804931640625 0.2169099897146225
CurrentTrain: epoch  2, batch    20 | loss: 4.5037904Losses:  4.294656753540039 0.21070587635040283
CurrentTrain: epoch  2, batch    21 | loss: 4.5053625Losses:  4.202118396759033 0.17778748273849487
CurrentTrain: epoch  2, batch    22 | loss: 4.3799057Losses:  4.462564468383789 0.1685141921043396
CurrentTrain: epoch  2, batch    23 | loss: 4.6310787Losses:  4.458369255065918 0.20020267367362976
CurrentTrain: epoch  2, batch    24 | loss: 4.6585717Losses:  4.6318769454956055 0.2596105933189392
CurrentTrain: epoch  2, batch    25 | loss: 4.8914876Losses:  4.702117443084717 0.3131153881549835
CurrentTrain: epoch  2, batch    26 | loss: 5.0152330Losses:  4.326224327087402 0.14171504974365234
CurrentTrain: epoch  2, batch    27 | loss: 4.4679394Losses:  4.4133124351501465 0.23603959381580353
CurrentTrain: epoch  2, batch    28 | loss: 4.6493521Losses:  4.429465293884277 0.17220917344093323
CurrentTrain: epoch  2, batch    29 | loss: 4.6016746Losses:  4.351773262023926 0.14943194389343262
CurrentTrain: epoch  2, batch    30 | loss: 4.5012054Losses:  4.314460754394531 0.2580772638320923
CurrentTrain: epoch  2, batch    31 | loss: 4.5725379Losses:  4.222742080688477 0.14556936919689178
CurrentTrain: epoch  2, batch    32 | loss: 4.3683114Losses:  4.337989330291748 0.17914101481437683
CurrentTrain: epoch  2, batch    33 | loss: 4.5171304Losses:  4.336082458496094 0.17661607265472412
CurrentTrain: epoch  2, batch    34 | loss: 4.5126987Losses:  4.082880020141602 0.14852005243301392
CurrentTrain: epoch  2, batch    35 | loss: 4.2314000Losses:  4.2521562576293945 0.1256456822156906
CurrentTrain: epoch  2, batch    36 | loss: 4.3778019Losses:  4.84174919128418 0.3092482388019562
CurrentTrain: epoch  2, batch    37 | loss: 5.1509976Losses:  5.384896755218506 0.3496110439300537
CurrentTrain: epoch  2, batch    38 | loss: 5.7345076Losses:  4.210996150970459 0.07780440151691437
CurrentTrain: epoch  2, batch    39 | loss: 4.2888007Losses:  4.602766513824463 0.1490091234445572
CurrentTrain: epoch  2, batch    40 | loss: 4.7517757Losses:  4.251826286315918 0.2674944996833801
CurrentTrain: epoch  2, batch    41 | loss: 4.5193210Losses:  4.632603645324707 0.12077105045318604
CurrentTrain: epoch  2, batch    42 | loss: 4.7533746Losses:  4.262134075164795 0.1350570023059845
CurrentTrain: epoch  2, batch    43 | loss: 4.3971910Losses:  4.541113376617432 0.23618970811367035
CurrentTrain: epoch  2, batch    44 | loss: 4.7773032Losses:  4.185876369476318 0.14210383594036102
CurrentTrain: epoch  2, batch    45 | loss: 4.3279800Losses:  4.368587970733643 0.1626470386981964
CurrentTrain: epoch  2, batch    46 | loss: 4.5312352Losses:  4.621276378631592 0.32828304171562195
CurrentTrain: epoch  2, batch    47 | loss: 4.9495592Losses:  4.591371536254883 0.2236718237400055
CurrentTrain: epoch  2, batch    48 | loss: 4.8150434Losses:  4.734621047973633 0.3217598795890808
CurrentTrain: epoch  2, batch    49 | loss: 5.0563807Losses:  4.274206161499023 0.14639699459075928
CurrentTrain: epoch  2, batch    50 | loss: 4.4206033Losses:  4.29981803894043 0.14802059531211853
CurrentTrain: epoch  2, batch    51 | loss: 4.4478388Losses:  4.310549736022949 0.1604754775762558
CurrentTrain: epoch  2, batch    52 | loss: 4.4710250Losses:  4.395411491394043 0.1562686562538147
CurrentTrain: epoch  2, batch    53 | loss: 4.5516801Losses:  4.459926605224609 0.16620536148548126
CurrentTrain: epoch  2, batch    54 | loss: 4.6261320Losses:  4.451731204986572 0.15365877747535706
CurrentTrain: epoch  2, batch    55 | loss: 4.6053901Losses:  4.215806007385254 0.20097079873085022
CurrentTrain: epoch  2, batch    56 | loss: 4.4167767Losses:  4.4684929847717285 0.12848125398159027
CurrentTrain: epoch  2, batch    57 | loss: 4.5969744Losses:  4.66642951965332 0.20882636308670044
CurrentTrain: epoch  2, batch    58 | loss: 4.8752561Losses:  4.3661298751831055 0.21581009030342102
CurrentTrain: epoch  2, batch    59 | loss: 4.5819402Losses:  4.59207010269165 0.27202317118644714
CurrentTrain: epoch  2, batch    60 | loss: 4.8640933Losses:  4.428558826446533 0.22544357180595398
CurrentTrain: epoch  2, batch    61 | loss: 4.6540022Losses:  4.315757751464844 0.11890305578708649
CurrentTrain: epoch  2, batch    62 | loss: 4.4346609Losses:  4.338459014892578 0.19830864667892456
CurrentTrain: epoch  2, batch    63 | loss: 4.5367675Losses:  4.651035785675049 0.14309842884540558
CurrentTrain: epoch  2, batch    64 | loss: 4.7941341Losses:  4.30329704284668 0.138363316655159
CurrentTrain: epoch  2, batch    65 | loss: 4.4416604Losses:  4.342033863067627 0.19070960581302643
CurrentTrain: epoch  2, batch    66 | loss: 4.5327435Losses:  4.218100547790527 0.23837237060070038
CurrentTrain: epoch  2, batch    67 | loss: 4.4564729Losses:  4.673984050750732 0.23895525932312012
CurrentTrain: epoch  2, batch    68 | loss: 4.9129391Losses:  4.264273166656494 0.21259237825870514
CurrentTrain: epoch  2, batch    69 | loss: 4.4768658Losses:  4.334012031555176 0.11683531105518341
CurrentTrain: epoch  2, batch    70 | loss: 4.4508471Losses:  4.731813430786133 0.25396984815597534
CurrentTrain: epoch  2, batch    71 | loss: 4.9857831Losses:  4.193883895874023 0.20544570684432983
CurrentTrain: epoch  2, batch    72 | loss: 4.3993297Losses:  4.170629978179932 0.14571425318717957
CurrentTrain: epoch  2, batch    73 | loss: 4.3163443Losses:  4.300448417663574 0.17574162781238556
CurrentTrain: epoch  2, batch    74 | loss: 4.4761901Losses:  4.420487403869629 0.17648570239543915
CurrentTrain: epoch  2, batch    75 | loss: 4.5969729Losses:  4.4798994064331055 0.1278018057346344
CurrentTrain: epoch  2, batch    76 | loss: 4.6077013Losses:  4.331075191497803 0.09495937824249268
CurrentTrain: epoch  2, batch    77 | loss: 4.4260345Losses:  4.118441581726074 0.11773602664470673
CurrentTrain: epoch  2, batch    78 | loss: 4.2361774Losses:  4.246458530426025 0.18559153378009796
CurrentTrain: epoch  2, batch    79 | loss: 4.4320502Losses:  4.375150203704834 0.29819610714912415
CurrentTrain: epoch  2, batch    80 | loss: 4.6733465Losses:  4.229983329772949 0.15411627292633057
CurrentTrain: epoch  2, batch    81 | loss: 4.3840995Losses:  4.209362030029297 0.24888911843299866
CurrentTrain: epoch  2, batch    82 | loss: 4.4582510Losses:  4.954414367675781 0.3595879077911377
CurrentTrain: epoch  2, batch    83 | loss: 5.3140020Losses:  4.29890775680542 0.14838743209838867
CurrentTrain: epoch  2, batch    84 | loss: 4.4472952Losses:  4.3386640548706055 0.16814422607421875
CurrentTrain: epoch  2, batch    85 | loss: 4.5068083Losses:  4.42445182800293 0.2052677720785141
CurrentTrain: epoch  2, batch    86 | loss: 4.6297197Losses:  4.427560806274414 0.13510558009147644
CurrentTrain: epoch  2, batch    87 | loss: 4.5626664Losses:  4.5474629402160645 0.3925245702266693
CurrentTrain: epoch  2, batch    88 | loss: 4.9399877Losses:  4.228259086608887 0.15177986025810242
CurrentTrain: epoch  2, batch    89 | loss: 4.3800387Losses:  4.338780403137207 0.15690065920352936
CurrentTrain: epoch  2, batch    90 | loss: 4.4956813Losses:  4.526658535003662 0.16192646324634552
CurrentTrain: epoch  2, batch    91 | loss: 4.6885848Losses:  4.175015926361084 0.1838447004556656
CurrentTrain: epoch  2, batch    92 | loss: 4.3588605Losses:  4.184544563293457 0.10642674565315247
CurrentTrain: epoch  2, batch    93 | loss: 4.2909713Losses:  4.769452095031738 0.2905963659286499
CurrentTrain: epoch  2, batch    94 | loss: 5.0600486Losses:  4.27934455871582 0.20850220322608948
CurrentTrain: epoch  2, batch    95 | loss: 4.4878469Losses:  4.369800567626953 0.12252643704414368
CurrentTrain: epoch  2, batch    96 | loss: 4.4923272Losses:  4.199777603149414 0.19065777957439423
CurrentTrain: epoch  2, batch    97 | loss: 4.3904352Losses:  4.31797981262207 0.25712716579437256
CurrentTrain: epoch  2, batch    98 | loss: 4.5751071Losses:  4.450063705444336 0.20907843112945557
CurrentTrain: epoch  2, batch    99 | loss: 4.6591420Losses:  4.257925987243652 0.14251616597175598
CurrentTrain: epoch  2, batch   100 | loss: 4.4004421Losses:  4.070945739746094 0.06936025619506836
CurrentTrain: epoch  2, batch   101 | loss: 4.1403060Losses:  4.225391387939453 0.13071171939373016
CurrentTrain: epoch  2, batch   102 | loss: 4.3561029Losses:  4.080751419067383 0.11536732316017151
CurrentTrain: epoch  2, batch   103 | loss: 4.1961188Losses:  4.311974048614502 0.21330460906028748
CurrentTrain: epoch  2, batch   104 | loss: 4.5252786Losses:  4.490012168884277 0.10316751152276993
CurrentTrain: epoch  2, batch   105 | loss: 4.5931797Losses:  4.278968811035156 0.1084219366312027
CurrentTrain: epoch  2, batch   106 | loss: 4.3873906Losses:  4.24185848236084 0.16461046040058136
CurrentTrain: epoch  2, batch   107 | loss: 4.4064689Losses:  4.264057159423828 0.14753565192222595
CurrentTrain: epoch  2, batch   108 | loss: 4.4115930Losses:  4.1849775314331055 0.12566792964935303
CurrentTrain: epoch  2, batch   109 | loss: 4.3106456Losses:  4.229992866516113 0.2268146127462387
CurrentTrain: epoch  2, batch   110 | loss: 4.4568076Losses:  4.329567909240723 0.1869356334209442
CurrentTrain: epoch  2, batch   111 | loss: 4.5165033Losses:  4.704830169677734 0.36266347765922546
CurrentTrain: epoch  2, batch   112 | loss: 5.0674934Losses:  4.093292713165283 0.16041876375675201
CurrentTrain: epoch  2, batch   113 | loss: 4.2537117Losses:  4.2700324058532715 0.1504642516374588
CurrentTrain: epoch  2, batch   114 | loss: 4.4204965Losses:  4.220146179199219 0.16048003733158112
CurrentTrain: epoch  2, batch   115 | loss: 4.3806262Losses:  4.224714279174805 0.14762593805789948
CurrentTrain: epoch  2, batch   116 | loss: 4.3723402Losses:  4.147269248962402 0.18823093175888062
CurrentTrain: epoch  2, batch   117 | loss: 4.3355002Losses:  4.190803050994873 0.07166816294193268
CurrentTrain: epoch  2, batch   118 | loss: 4.2624712Losses:  4.234768867492676 0.1488438844680786
CurrentTrain: epoch  2, batch   119 | loss: 4.3836126Losses:  4.262899398803711 0.14049765467643738
CurrentTrain: epoch  2, batch   120 | loss: 4.4033971Losses:  4.2960686683654785 0.24096404016017914
CurrentTrain: epoch  2, batch   121 | loss: 4.5370326Losses:  4.211837291717529 0.1811836063861847
CurrentTrain: epoch  2, batch   122 | loss: 4.3930211Losses:  4.213163375854492 0.1154123917222023
CurrentTrain: epoch  2, batch   123 | loss: 4.3285756Losses:  4.197868824005127 0.11585444211959839
CurrentTrain: epoch  2, batch   124 | loss: 4.3137231Losses:  4.224582672119141 0.049602773040533066
CurrentTrain: epoch  3, batch     0 | loss: 4.2741857Losses:  4.215106964111328 0.23165500164031982
CurrentTrain: epoch  3, batch     1 | loss: 4.4467621Losses:  5.418959617614746 0.4559028446674347
CurrentTrain: epoch  3, batch     2 | loss: 5.8748627Losses:  4.195713043212891 0.1271226406097412
CurrentTrain: epoch  3, batch     3 | loss: 4.3228359Losses:  4.107150077819824 0.12363345175981522
CurrentTrain: epoch  3, batch     4 | loss: 4.2307835Losses:  4.312617301940918 0.16982564330101013
CurrentTrain: epoch  3, batch     5 | loss: 4.4824429Losses:  4.221756935119629 0.15130610764026642
CurrentTrain: epoch  3, batch     6 | loss: 4.3730631Losses:  4.30609655380249 0.1614115685224533
CurrentTrain: epoch  3, batch     7 | loss: 4.4675083Losses:  4.209493637084961 0.11792322248220444
CurrentTrain: epoch  3, batch     8 | loss: 4.3274169Losses:  4.134538650512695 0.16155105829238892
CurrentTrain: epoch  3, batch     9 | loss: 4.2960896Losses:  4.178786277770996 0.11415305733680725
CurrentTrain: epoch  3, batch    10 | loss: 4.2929392Losses:  4.169426441192627 0.09033572673797607
CurrentTrain: epoch  3, batch    11 | loss: 4.2597623Losses:  4.3230485916137695 0.1729203164577484
CurrentTrain: epoch  3, batch    12 | loss: 4.4959688Losses:  4.195111274719238 0.12561851739883423
CurrentTrain: epoch  3, batch    13 | loss: 4.3207297Losses:  4.058841705322266 0.10203075408935547
CurrentTrain: epoch  3, batch    14 | loss: 4.1608725Losses:  4.116205215454102 0.09819851070642471
CurrentTrain: epoch  3, batch    15 | loss: 4.2144036Losses:  4.056066036224365 0.17688526213169098
CurrentTrain: epoch  3, batch    16 | loss: 4.2329512Losses:  4.429869651794434 0.10707433521747589
CurrentTrain: epoch  3, batch    17 | loss: 4.5369439Losses:  4.1374640464782715 0.16201168298721313
CurrentTrain: epoch  3, batch    18 | loss: 4.2994757Losses:  4.225066661834717 0.10007454454898834
CurrentTrain: epoch  3, batch    19 | loss: 4.3251414Losses:  4.085209846496582 0.12773272395133972
CurrentTrain: epoch  3, batch    20 | loss: 4.2129426Losses:  4.394128322601318 0.1542685180902481
CurrentTrain: epoch  3, batch    21 | loss: 4.5483971Losses:  4.164554595947266 0.14320752024650574
CurrentTrain: epoch  3, batch    22 | loss: 4.3077621Losses:  4.143691062927246 0.13803505897521973
CurrentTrain: epoch  3, batch    23 | loss: 4.2817259Losses:  4.05048942565918 0.10456632077693939
CurrentTrain: epoch  3, batch    24 | loss: 4.1550555Losses:  3.9513237476348877 0.06599007546901703
CurrentTrain: epoch  3, batch    25 | loss: 4.0173140Losses:  4.406619071960449 0.1704634577035904
CurrentTrain: epoch  3, batch    26 | loss: 4.5770826Losses:  4.080461502075195 0.1566779613494873
CurrentTrain: epoch  3, batch    27 | loss: 4.2371397Losses:  4.1324334144592285 0.11667752265930176
CurrentTrain: epoch  3, batch    28 | loss: 4.2491112Losses:  4.172564506530762 0.05647531896829605
CurrentTrain: epoch  3, batch    29 | loss: 4.2290397Losses:  4.105183124542236 0.15526501834392548
CurrentTrain: epoch  3, batch    30 | loss: 4.2604480Losses:  4.118403434753418 0.15996691584587097
CurrentTrain: epoch  3, batch    31 | loss: 4.2783704Losses:  4.13180685043335 0.14845457673072815
CurrentTrain: epoch  3, batch    32 | loss: 4.2802615Losses:  4.1165289878845215 0.08888162672519684
CurrentTrain: epoch  3, batch    33 | loss: 4.2054105Losses:  4.411781311035156 0.10960891842842102
CurrentTrain: epoch  3, batch    34 | loss: 4.5213904Losses:  4.3201985359191895 0.15230607986450195
CurrentTrain: epoch  3, batch    35 | loss: 4.4725046Losses:  4.286100387573242 0.13354581594467163
CurrentTrain: epoch  3, batch    36 | loss: 4.4196463Losses:  4.1090850830078125 0.09549309313297272
CurrentTrain: epoch  3, batch    37 | loss: 4.2045784Losses:  4.120993614196777 0.1846732497215271
CurrentTrain: epoch  3, batch    38 | loss: 4.3056669Losses:  4.1762871742248535 0.1357700377702713
CurrentTrain: epoch  3, batch    39 | loss: 4.3120570Losses:  4.131629943847656 0.09971210360527039
CurrentTrain: epoch  3, batch    40 | loss: 4.2313418Losses:  4.217315673828125 0.15129829943180084
CurrentTrain: epoch  3, batch    41 | loss: 4.3686142Losses:  4.1118645668029785 0.14380770921707153
CurrentTrain: epoch  3, batch    42 | loss: 4.2556725Losses:  4.1773481369018555 0.13973397016525269
CurrentTrain: epoch  3, batch    43 | loss: 4.3170819Losses:  4.446581840515137 0.15880176424980164
CurrentTrain: epoch  3, batch    44 | loss: 4.6053834Losses:  4.051835060119629 0.10955557227134705
CurrentTrain: epoch  3, batch    45 | loss: 4.1613908Losses:  4.2117695808410645 0.15121454000473022
CurrentTrain: epoch  3, batch    46 | loss: 4.3629842Losses:  4.10591459274292 0.127401202917099
CurrentTrain: epoch  3, batch    47 | loss: 4.2333159Losses:  4.0860490798950195 0.029048509895801544
CurrentTrain: epoch  3, batch    48 | loss: 4.1150975Losses:  4.140806198120117 0.14954841136932373
CurrentTrain: epoch  3, batch    49 | loss: 4.2903547Losses:  4.077093124389648 0.18016019463539124
CurrentTrain: epoch  3, batch    50 | loss: 4.2572532Losses:  4.063366889953613 0.08107583224773407
CurrentTrain: epoch  3, batch    51 | loss: 4.1444426Losses:  4.170738220214844 0.10648509860038757
CurrentTrain: epoch  3, batch    52 | loss: 4.2772231Losses:  4.188785552978516 0.13639433681964874
CurrentTrain: epoch  3, batch    53 | loss: 4.3251801Losses:  4.202643394470215 0.11469289660453796
CurrentTrain: epoch  3, batch    54 | loss: 4.3173361Losses:  4.104082107543945 0.07956106960773468
CurrentTrain: epoch  3, batch    55 | loss: 4.1836433Losses:  4.173996448516846 0.0982031524181366
CurrentTrain: epoch  3, batch    56 | loss: 4.2721996Losses:  4.134947776794434 0.20750421285629272
CurrentTrain: epoch  3, batch    57 | loss: 4.3424520Losses:  4.118231773376465 0.10814253985881805
CurrentTrain: epoch  3, batch    58 | loss: 4.2263741Losses:  4.165229797363281 0.16488122940063477
CurrentTrain: epoch  3, batch    59 | loss: 4.3301110Losses:  4.199078559875488 0.11711083352565765
CurrentTrain: epoch  3, batch    60 | loss: 4.3161893Losses:  4.288477420806885 0.1428355872631073
CurrentTrain: epoch  3, batch    61 | loss: 4.4313130Losses:  4.031373023986816 0.14195877313613892
CurrentTrain: epoch  3, batch    62 | loss: 4.1733317Losses:  4.033799648284912 0.08373448252677917
CurrentTrain: epoch  3, batch    63 | loss: 4.1175342Losses:  4.070975303649902 0.061947502195835114
CurrentTrain: epoch  3, batch    64 | loss: 4.1329226Losses:  4.14495325088501 0.1152440533041954
CurrentTrain: epoch  3, batch    65 | loss: 4.2601972Losses:  4.152873992919922 0.1152660921216011
CurrentTrain: epoch  3, batch    66 | loss: 4.2681403Losses:  4.039454460144043 0.10062830150127411
CurrentTrain: epoch  3, batch    67 | loss: 4.1400828Losses:  4.056561470031738 0.1314728856086731
CurrentTrain: epoch  3, batch    68 | loss: 4.1880345Losses:  4.172869682312012 0.12976320087909698
CurrentTrain: epoch  3, batch    69 | loss: 4.3026328Losses:  4.0210771560668945 0.08159938454627991
CurrentTrain: epoch  3, batch    70 | loss: 4.1026764Losses:  4.1184821128845215 0.1798756867647171
CurrentTrain: epoch  3, batch    71 | loss: 4.2983580Losses:  4.206164836883545 0.2591376304626465
CurrentTrain: epoch  3, batch    72 | loss: 4.4653025Losses:  4.156563758850098 0.11290480196475983
CurrentTrain: epoch  3, batch    73 | loss: 4.2694688Losses:  4.119617462158203 0.11317737400531769
CurrentTrain: epoch  3, batch    74 | loss: 4.2327948Losses:  4.016658782958984 0.10492806136608124
CurrentTrain: epoch  3, batch    75 | loss: 4.1215868Losses:  4.023683547973633 0.09198407083749771
CurrentTrain: epoch  3, batch    76 | loss: 4.1156678Losses:  4.134059906005859 0.0930994376540184
CurrentTrain: epoch  3, batch    77 | loss: 4.2271595Losses:  4.0712432861328125 0.06808163225650787
CurrentTrain: epoch  3, batch    78 | loss: 4.1393251Losses:  4.219203472137451 0.07142239063978195
CurrentTrain: epoch  3, batch    79 | loss: 4.2906260Losses:  4.06489372253418 0.1379319131374359
CurrentTrain: epoch  3, batch    80 | loss: 4.2028255Losses:  4.043900012969971 0.1082286536693573
CurrentTrain: epoch  3, batch    81 | loss: 4.1521287Losses:  4.034542083740234 0.11231151223182678
CurrentTrain: epoch  3, batch    82 | loss: 4.1468534Losses:  4.063227653503418 0.11379068344831467
CurrentTrain: epoch  3, batch    83 | loss: 4.1770182Losses:  4.115399360656738 0.08708546310663223
CurrentTrain: epoch  3, batch    84 | loss: 4.2024846Losses:  4.077390670776367 0.04077058285474777
CurrentTrain: epoch  3, batch    85 | loss: 4.1181612Losses:  4.060719013214111 0.10090270638465881
CurrentTrain: epoch  3, batch    86 | loss: 4.1616216Losses:  4.106879234313965 0.09080712497234344
CurrentTrain: epoch  3, batch    87 | loss: 4.1976862Losses:  4.287360191345215 0.0829811841249466
CurrentTrain: epoch  3, batch    88 | loss: 4.3703413Losses:  4.0129899978637695 0.1606135219335556
CurrentTrain: epoch  3, batch    89 | loss: 4.1736035Losses:  4.199099540710449 0.09327548742294312
CurrentTrain: epoch  3, batch    90 | loss: 4.2923751Losses:  4.123931884765625 0.0814746618270874
CurrentTrain: epoch  3, batch    91 | loss: 4.2054067Losses:  4.004215717315674 0.12739187479019165
CurrentTrain: epoch  3, batch    92 | loss: 4.1316075Losses:  4.037967681884766 0.08626829087734222
CurrentTrain: epoch  3, batch    93 | loss: 4.1242361Losses:  4.102027893066406 0.10679654031991959
CurrentTrain: epoch  3, batch    94 | loss: 4.2088246Losses:  4.142645359039307 0.09031759947538376
CurrentTrain: epoch  3, batch    95 | loss: 4.2329631Losses:  4.1035990715026855 0.07125258445739746
CurrentTrain: epoch  3, batch    96 | loss: 4.1748514Losses:  4.071146011352539 0.14603698253631592
CurrentTrain: epoch  3, batch    97 | loss: 4.2171831Losses:  4.027570724487305 0.14779317378997803
CurrentTrain: epoch  3, batch    98 | loss: 4.1753640Losses:  4.071193695068359 0.1098097562789917
CurrentTrain: epoch  3, batch    99 | loss: 4.1810036Losses:  4.064322471618652 0.11015767604112625
CurrentTrain: epoch  3, batch   100 | loss: 4.1744800Losses:  4.055822372436523 0.11302432417869568
CurrentTrain: epoch  3, batch   101 | loss: 4.1688466Losses:  4.059401035308838 0.0961550921201706
CurrentTrain: epoch  3, batch   102 | loss: 4.1555562Losses:  4.123135566711426 0.10335898399353027
CurrentTrain: epoch  3, batch   103 | loss: 4.2264948Losses:  4.079535484313965 0.13171736896038055
CurrentTrain: epoch  3, batch   104 | loss: 4.2112527Losses:  4.084183692932129 0.12235739827156067
CurrentTrain: epoch  3, batch   105 | loss: 4.2065411Losses:  4.13436222076416 0.15763454139232635
CurrentTrain: epoch  3, batch   106 | loss: 4.2919970Losses:  4.239213943481445 0.08674405515193939
CurrentTrain: epoch  3, batch   107 | loss: 4.3259578Losses:  4.017541885375977 0.10489502549171448
CurrentTrain: epoch  3, batch   108 | loss: 4.1224370Losses:  4.0959577560424805 0.13439050316810608
CurrentTrain: epoch  3, batch   109 | loss: 4.2303481Losses:  4.018293380737305 0.07322372496128082
CurrentTrain: epoch  3, batch   110 | loss: 4.0915170Losses:  4.076652526855469 0.15593445301055908
CurrentTrain: epoch  3, batch   111 | loss: 4.2325869Losses:  4.0463361740112305 0.15631425380706787
CurrentTrain: epoch  3, batch   112 | loss: 4.2026505Losses:  3.9952666759490967 0.07205601781606674
CurrentTrain: epoch  3, batch   113 | loss: 4.0673227Losses:  4.104607582092285 0.12781491875648499
CurrentTrain: epoch  3, batch   114 | loss: 4.2324224Losses:  4.000030040740967 0.164693683385849
CurrentTrain: epoch  3, batch   115 | loss: 4.1647239Losses:  3.9744811058044434 0.08980903029441833
CurrentTrain: epoch  3, batch   116 | loss: 4.0642900Losses:  4.096213340759277 0.06178734824061394
CurrentTrain: epoch  3, batch   117 | loss: 4.1580005Losses:  4.055549621582031 0.09591089934110641
CurrentTrain: epoch  3, batch   118 | loss: 4.1514606Losses:  4.0830159187316895 0.07988429069519043
CurrentTrain: epoch  3, batch   119 | loss: 4.1629000Losses:  4.100767135620117 0.13199496269226074
CurrentTrain: epoch  3, batch   120 | loss: 4.2327623Losses:  4.072542667388916 0.1595863401889801
CurrentTrain: epoch  3, batch   121 | loss: 4.2321291Losses:  4.0178303718566895 0.10589800775051117
CurrentTrain: epoch  3, batch   122 | loss: 4.1237283Losses:  4.041530132293701 0.11107740551233292
CurrentTrain: epoch  3, batch   123 | loss: 4.1526074Losses:  4.031245231628418 0.05092014744877815
CurrentTrain: epoch  3, batch   124 | loss: 4.0821652Losses:  4.0208659172058105 0.09705464541912079
CurrentTrain: epoch  4, batch     0 | loss: 4.1179204Losses:  3.8447327613830566 0.034536346793174744
CurrentTrain: epoch  4, batch     1 | loss: 3.8792691Losses:  4.047515869140625 0.09234422445297241
CurrentTrain: epoch  4, batch     2 | loss: 4.1398602Losses:  3.9849319458007812 0.09719572216272354
CurrentTrain: epoch  4, batch     3 | loss: 4.0821276Losses:  4.037045955657959 0.13405761122703552
CurrentTrain: epoch  4, batch     4 | loss: 4.1711035Losses:  4.0049333572387695 0.04979492723941803
CurrentTrain: epoch  4, batch     5 | loss: 4.0547285Losses:  4.027825355529785 0.036052681505680084
CurrentTrain: epoch  4, batch     6 | loss: 4.0638781Losses:  4.077859878540039 0.10959763824939728
CurrentTrain: epoch  4, batch     7 | loss: 4.1874576Losses:  4.061939239501953 0.031160127371549606
CurrentTrain: epoch  4, batch     8 | loss: 4.0930996Losses:  3.9693057537078857 0.11482174694538116
CurrentTrain: epoch  4, batch     9 | loss: 4.0841274Losses:  3.96779203414917 0.09430380165576935
CurrentTrain: epoch  4, batch    10 | loss: 4.0620956Losses:  4.042571544647217 0.10869785398244858
CurrentTrain: epoch  4, batch    11 | loss: 4.1512694Losses:  4.0100016593933105 0.05554644763469696
CurrentTrain: epoch  4, batch    12 | loss: 4.0655479Losses:  4.010819911956787 0.14259633421897888
CurrentTrain: epoch  4, batch    13 | loss: 4.1534162Losses:  4.06242036819458 0.13137415051460266
CurrentTrain: epoch  4, batch    14 | loss: 4.1937947Losses:  3.9875149726867676 0.07592631876468658
CurrentTrain: epoch  4, batch    15 | loss: 4.0634413Losses:  3.9710195064544678 0.10991910845041275
CurrentTrain: epoch  4, batch    16 | loss: 4.0809388Losses:  4.021176338195801 0.06347092986106873
CurrentTrain: epoch  4, batch    17 | loss: 4.0846472Losses:  3.986830711364746 0.10981057584285736
CurrentTrain: epoch  4, batch    18 | loss: 4.0966411Losses:  4.078094959259033 0.06791195273399353
CurrentTrain: epoch  4, batch    19 | loss: 4.1460071Losses:  3.947859287261963 0.04939964786171913
CurrentTrain: epoch  4, batch    20 | loss: 3.9972589Losses:  4.015112400054932 0.0903119221329689
CurrentTrain: epoch  4, batch    21 | loss: 4.1054244Losses:  4.037893295288086 0.09496724605560303
CurrentTrain: epoch  4, batch    22 | loss: 4.1328607Losses:  4.052916526794434 0.09654724597930908
CurrentTrain: epoch  4, batch    23 | loss: 4.1494637Losses:  4.002890586853027 0.08545596897602081
CurrentTrain: epoch  4, batch    24 | loss: 4.0883465Losses:  4.000612258911133 0.07287583500146866
CurrentTrain: epoch  4, batch    25 | loss: 4.0734882Losses:  4.031705856323242 0.10327382385730743
CurrentTrain: epoch  4, batch    26 | loss: 4.1349797Losses:  3.996577262878418 0.1065107062458992
CurrentTrain: epoch  4, batch    27 | loss: 4.1030879Losses:  4.050966262817383 0.037573039531707764
CurrentTrain: epoch  4, batch    28 | loss: 4.0885391Losses:  4.03369140625 0.0858403891324997
CurrentTrain: epoch  4, batch    29 | loss: 4.1195316Losses:  4.019118785858154 0.05581413581967354
CurrentTrain: epoch  4, batch    30 | loss: 4.0749331Losses:  4.029566287994385 0.06257542967796326
CurrentTrain: epoch  4, batch    31 | loss: 4.0921416Losses:  4.03233003616333 0.12280909717082977
CurrentTrain: epoch  4, batch    32 | loss: 4.1551390Losses:  4.042767524719238 0.09944605827331543
CurrentTrain: epoch  4, batch    33 | loss: 4.1422138Losses:  3.994281768798828 0.07443341612815857
CurrentTrain: epoch  4, batch    34 | loss: 4.0687151Losses:  4.032953262329102 0.0292466152459383
CurrentTrain: epoch  4, batch    35 | loss: 4.0622001Losses:  4.023596286773682 0.11851498484611511
CurrentTrain: epoch  4, batch    36 | loss: 4.1421113Losses:  4.055150985717773 0.09017185866832733
CurrentTrain: epoch  4, batch    37 | loss: 4.1453228Losses:  3.9850876331329346 0.07538861036300659
CurrentTrain: epoch  4, batch    38 | loss: 4.0604763Losses:  4.031394958496094 0.1494928002357483
CurrentTrain: epoch  4, batch    39 | loss: 4.1808877Losses:  4.161945343017578 0.10674429684877396
CurrentTrain: epoch  4, batch    40 | loss: 4.2686896Losses:  4.043999671936035 0.10533061623573303
CurrentTrain: epoch  4, batch    41 | loss: 4.1493301Losses:  4.044002056121826 0.05885253846645355
CurrentTrain: epoch  4, batch    42 | loss: 4.1028547Losses:  4.024039268493652 0.05750945210456848
CurrentTrain: epoch  4, batch    43 | loss: 4.0815487Losses:  3.9477157592773438 0.11007828265428543
CurrentTrain: epoch  4, batch    44 | loss: 4.0577941Losses:  3.9905874729156494 0.06324367225170135
CurrentTrain: epoch  4, batch    45 | loss: 4.0538311Losses:  3.9009573459625244 0.02712342143058777
CurrentTrain: epoch  4, batch    46 | loss: 3.9280808Losses:  4.030287742614746 0.03742793947458267
CurrentTrain: epoch  4, batch    47 | loss: 4.0677156Losses:  4.126524448394775 0.08556468784809113
CurrentTrain: epoch  4, batch    48 | loss: 4.2120891Losses:  4.035994529724121 0.0700087696313858
CurrentTrain: epoch  4, batch    49 | loss: 4.1060033Losses:  4.038488388061523 0.083683080971241
CurrentTrain: epoch  4, batch    50 | loss: 4.1221714Losses:  4.043240547180176 0.11565380543470383
CurrentTrain: epoch  4, batch    51 | loss: 4.1588945Losses:  4.057825565338135 0.07746488600969315
CurrentTrain: epoch  4, batch    52 | loss: 4.1352906Losses:  3.95906400680542 0.06918684393167496
CurrentTrain: epoch  4, batch    53 | loss: 4.0282507Losses:  4.050360679626465 0.04960237443447113
CurrentTrain: epoch  4, batch    54 | loss: 4.0999632Losses:  4.003925323486328 0.061831649392843246
CurrentTrain: epoch  4, batch    55 | loss: 4.0657568Losses:  4.122132301330566 0.07551031559705734
CurrentTrain: epoch  4, batch    56 | loss: 4.1976428Losses:  3.951296806335449 0.1277601569890976
CurrentTrain: epoch  4, batch    57 | loss: 4.0790567Losses:  3.964510679244995 0.04302695393562317
CurrentTrain: epoch  4, batch    58 | loss: 4.0075378Losses:  4.009893417358398 0.09774554520845413
CurrentTrain: epoch  4, batch    59 | loss: 4.1076388Losses:  3.9839277267456055 0.04299358278512955
CurrentTrain: epoch  4, batch    60 | loss: 4.0269213Losses:  3.9617655277252197 0.043670233339071274
CurrentTrain: epoch  4, batch    61 | loss: 4.0054359Losses:  3.9251108169555664 0.06116390600800514
CurrentTrain: epoch  4, batch    62 | loss: 3.9862747Losses:  4.045661926269531 0.09848020225763321
CurrentTrain: epoch  4, batch    63 | loss: 4.1441422Losses:  3.9920077323913574 0.12552396953105927
CurrentTrain: epoch  4, batch    64 | loss: 4.1175318Losses:  4.418436050415039 0.24254268407821655
CurrentTrain: epoch  4, batch    65 | loss: 4.6609788Losses:  4.0665483474731445 0.08612309396266937
CurrentTrain: epoch  4, batch    66 | loss: 4.1526713Losses:  4.041215896606445 0.08863227069377899
CurrentTrain: epoch  4, batch    67 | loss: 4.1298480Losses:  3.9856410026550293 0.04271174594759941
CurrentTrain: epoch  4, batch    68 | loss: 4.0283527Losses:  4.00987434387207 0.042869046330451965
CurrentTrain: epoch  4, batch    69 | loss: 4.0527434Losses:  3.975071907043457 0.05197780579328537
CurrentTrain: epoch  4, batch    70 | loss: 4.0270495Losses:  4.009683132171631 0.1411852240562439
CurrentTrain: epoch  4, batch    71 | loss: 4.1508684Losses:  4.021411895751953 0.09329229593276978
CurrentTrain: epoch  4, batch    72 | loss: 4.1147041Losses:  4.054976463317871 0.07645431160926819
CurrentTrain: epoch  4, batch    73 | loss: 4.1314306Losses:  4.030406951904297 0.12413355708122253
CurrentTrain: epoch  4, batch    74 | loss: 4.1545405Losses:  4.002688407897949 0.11610367894172668
CurrentTrain: epoch  4, batch    75 | loss: 4.1187921Losses:  3.9976112842559814 0.049522072076797485
CurrentTrain: epoch  4, batch    76 | loss: 4.0471334Losses:  3.9994797706604004 0.10390520095825195
CurrentTrain: epoch  4, batch    77 | loss: 4.1033850Losses:  4.045721530914307 0.10800217092037201
CurrentTrain: epoch  4, batch    78 | loss: 4.1537237Losses:  3.989243507385254 0.12827380001544952
CurrentTrain: epoch  4, batch    79 | loss: 4.1175175Losses:  4.039542198181152 0.050045568495988846
CurrentTrain: epoch  4, batch    80 | loss: 4.0895877Losses:  4.578580379486084 0.11565697938203812
CurrentTrain: epoch  4, batch    81 | loss: 4.6942372Losses:  4.007196426391602 0.1019509956240654
CurrentTrain: epoch  4, batch    82 | loss: 4.1091475Losses:  4.014570236206055 0.10507828742265701
CurrentTrain: epoch  4, batch    83 | loss: 4.1196485Losses:  4.100882530212402 0.09992687404155731
CurrentTrain: epoch  4, batch    84 | loss: 4.2008095Losses:  5.504018306732178 0.5054594874382019
CurrentTrain: epoch  4, batch    85 | loss: 6.0094776Losses:  4.241664886474609 0.08548776805400848
CurrentTrain: epoch  4, batch    86 | loss: 4.3271527Losses:  4.974579334259033 0.09605240821838379
CurrentTrain: epoch  4, batch    87 | loss: 5.0706320Losses:  3.9688339233398438 0.07186245918273926
CurrentTrain: epoch  4, batch    88 | loss: 4.0406961Losses:  4.125772476196289 0.03678646683692932
CurrentTrain: epoch  4, batch    89 | loss: 4.1625590Losses:  4.118101119995117 0.11402224749326706
CurrentTrain: epoch  4, batch    90 | loss: 4.2321234Losses:  4.097560405731201 0.0994795709848404
CurrentTrain: epoch  4, batch    91 | loss: 4.1970401Losses:  4.300165176391602 0.12825486063957214
CurrentTrain: epoch  4, batch    92 | loss: 4.4284201Losses:  3.960653305053711 0.09251735359430313
CurrentTrain: epoch  4, batch    93 | loss: 4.0531707Losses:  4.1532745361328125 0.07929457724094391
CurrentTrain: epoch  4, batch    94 | loss: 4.2325692Losses:  4.312037467956543 0.1265861690044403
CurrentTrain: epoch  4, batch    95 | loss: 4.4386234Losses:  4.922563076019287 0.20705920457839966
CurrentTrain: epoch  4, batch    96 | loss: 5.1296225Losses:  4.100622177124023 0.08068275451660156
CurrentTrain: epoch  4, batch    97 | loss: 4.1813049Losses:  4.10003137588501 0.08313892781734467
CurrentTrain: epoch  4, batch    98 | loss: 4.1831703Losses:  4.019996643066406 0.07716666907072067
CurrentTrain: epoch  4, batch    99 | loss: 4.0971632Losses:  4.083261489868164 0.05491214990615845
CurrentTrain: epoch  4, batch   100 | loss: 4.1381736Losses:  4.165114879608154 0.11867865920066833
CurrentTrain: epoch  4, batch   101 | loss: 4.2837934Losses:  4.033674716949463 0.05094084143638611
CurrentTrain: epoch  4, batch   102 | loss: 4.0846157Losses:  3.989140033721924 0.0549127459526062
CurrentTrain: epoch  4, batch   103 | loss: 4.0440526Losses:  4.302282333374023 0.1196901947259903
CurrentTrain: epoch  4, batch   104 | loss: 4.4219728Losses:  4.027549743652344 0.07933363318443298
CurrentTrain: epoch  4, batch   105 | loss: 4.1068835Losses:  4.718078136444092 0.3699359595775604
CurrentTrain: epoch  4, batch   106 | loss: 5.0880141Losses:  4.118677139282227 0.06701600551605225
CurrentTrain: epoch  4, batch   107 | loss: 4.1856933Losses:  4.12974739074707 0.13035547733306885
CurrentTrain: epoch  4, batch   108 | loss: 4.2601027Losses:  4.461230278015137 0.06599459052085876
CurrentTrain: epoch  4, batch   109 | loss: 4.5272250Losses:  4.13551139831543 0.1324922740459442
CurrentTrain: epoch  4, batch   110 | loss: 4.2680035Losses:  4.139784336090088 0.09247860312461853
CurrentTrain: epoch  4, batch   111 | loss: 4.2322631Losses:  4.040602207183838 0.10021553933620453
CurrentTrain: epoch  4, batch   112 | loss: 4.1408176Losses:  3.960358142852783 0.03717494755983353
CurrentTrain: epoch  4, batch   113 | loss: 3.9975331Losses:  4.016963005065918 0.05775875598192215
CurrentTrain: epoch  4, batch   114 | loss: 4.0747218Losses:  4.123058319091797 0.127174973487854
CurrentTrain: epoch  4, batch   115 | loss: 4.2502332Losses:  4.148795127868652 0.0869547575712204
CurrentTrain: epoch  4, batch   116 | loss: 4.2357497Losses:  3.967808485031128 0.049512214958667755
CurrentTrain: epoch  4, batch   117 | loss: 4.0173206Losses:  4.0954976081848145 0.08265134692192078
CurrentTrain: epoch  4, batch   118 | loss: 4.1781487Losses:  3.9919662475585938 0.0634000226855278
CurrentTrain: epoch  4, batch   119 | loss: 4.0553660Losses:  4.020364761352539 0.08879462629556656
CurrentTrain: epoch  4, batch   120 | loss: 4.1091595Losses:  4.036031723022461 0.0713043287396431
CurrentTrain: epoch  4, batch   121 | loss: 4.1073360Losses:  4.03483772277832 0.09036369621753693
CurrentTrain: epoch  4, batch   122 | loss: 4.1252012Losses:  4.023082733154297 0.0496433824300766
CurrentTrain: epoch  4, batch   123 | loss: 4.0727262Losses:  4.092799663543701 0.10887561738491058
CurrentTrain: epoch  4, batch   124 | loss: 4.2016754Losses:  4.040907382965088 0.05620168522000313
CurrentTrain: epoch  5, batch     0 | loss: 4.0971088Losses:  4.061400890350342 0.1072719395160675
CurrentTrain: epoch  5, batch     1 | loss: 4.1686730Losses:  4.247427940368652 0.10692986845970154
CurrentTrain: epoch  5, batch     2 | loss: 4.3543577Losses:  4.059743881225586 0.08120349794626236
CurrentTrain: epoch  5, batch     3 | loss: 4.1409473Losses:  3.999401092529297 0.060637690126895905
CurrentTrain: epoch  5, batch     4 | loss: 4.0600386Losses:  4.0449395179748535 0.061287738382816315
CurrentTrain: epoch  5, batch     5 | loss: 4.1062274Losses:  3.9900474548339844 0.05623647943139076
CurrentTrain: epoch  5, batch     6 | loss: 4.0462837Losses:  4.017790794372559 0.0703449547290802
CurrentTrain: epoch  5, batch     7 | loss: 4.0881357Losses:  4.040162086486816 0.04978794604539871
CurrentTrain: epoch  5, batch     8 | loss: 4.0899501Losses:  4.198529243469238 0.09969468414783478
CurrentTrain: epoch  5, batch     9 | loss: 4.2982240Losses:  4.052117347717285 0.12549400329589844
CurrentTrain: epoch  5, batch    10 | loss: 4.1776114Losses:  3.9863314628601074 0.06510289758443832
CurrentTrain: epoch  5, batch    11 | loss: 4.0514345Losses:  4.033964157104492 0.0615961030125618
CurrentTrain: epoch  5, batch    12 | loss: 4.0955601Losses:  4.073173522949219 0.0807839184999466
CurrentTrain: epoch  5, batch    13 | loss: 4.1539574Losses:  4.002140522003174 0.12077611684799194
CurrentTrain: epoch  5, batch    14 | loss: 4.1229167Losses:  3.961334705352783 0.053775884211063385
CurrentTrain: epoch  5, batch    15 | loss: 4.0151105Losses:  4.147859573364258 0.08913085609674454
CurrentTrain: epoch  5, batch    16 | loss: 4.2369905Losses:  4.1392292976379395 0.026371629908680916
CurrentTrain: epoch  5, batch    17 | loss: 4.1656008Losses:  3.99947452545166 0.07262320816516876
CurrentTrain: epoch  5, batch    18 | loss: 4.0720978Losses:  3.964632749557495 0.08381962031126022
CurrentTrain: epoch  5, batch    19 | loss: 4.0484524Losses:  4.032747268676758 0.12077197432518005
CurrentTrain: epoch  5, batch    20 | loss: 4.1535192Losses:  4.139469146728516 0.1040257066488266
CurrentTrain: epoch  5, batch    21 | loss: 4.2434950Losses:  4.00373649597168 0.06609871983528137
CurrentTrain: epoch  5, batch    22 | loss: 4.0698352Losses:  4.0777692794799805 0.0781487450003624
CurrentTrain: epoch  5, batch    23 | loss: 4.1559181Losses:  3.962217330932617 0.06705202162265778
CurrentTrain: epoch  5, batch    24 | loss: 4.0292692Losses:  4.22244930267334 0.13645094633102417
CurrentTrain: epoch  5, batch    25 | loss: 4.3589001Losses:  3.990288257598877 0.04759538918733597
CurrentTrain: epoch  5, batch    26 | loss: 4.0378838Losses:  3.9909794330596924 0.08344170451164246
CurrentTrain: epoch  5, batch    27 | loss: 4.0744209Losses:  4.084774494171143 0.11416810750961304
CurrentTrain: epoch  5, batch    28 | loss: 4.1989427Losses:  4.098854064941406 0.06577257066965103
CurrentTrain: epoch  5, batch    29 | loss: 4.1646266Losses:  3.9536356925964355 0.05127210170030594
CurrentTrain: epoch  5, batch    30 | loss: 4.0049076Losses:  4.201039791107178 0.08092483133077621
CurrentTrain: epoch  5, batch    31 | loss: 4.2819648Losses:  3.990642786026001 0.072197824716568
CurrentTrain: epoch  5, batch    32 | loss: 4.0628405Losses:  4.257134437561035 0.09274621307849884
CurrentTrain: epoch  5, batch    33 | loss: 4.3498807Losses:  4.036025524139404 0.08326971530914307
CurrentTrain: epoch  5, batch    34 | loss: 4.1192951Losses:  3.9735589027404785 0.06035113334655762
CurrentTrain: epoch  5, batch    35 | loss: 4.0339098Losses:  3.958872079849243 0.06113430857658386
CurrentTrain: epoch  5, batch    36 | loss: 4.0200062Losses:  4.15938663482666 0.08792585879564285
CurrentTrain: epoch  5, batch    37 | loss: 4.2473125Losses:  4.0237836837768555 0.0717644914984703
CurrentTrain: epoch  5, batch    38 | loss: 4.0955482Losses:  4.012250900268555 0.05001790076494217
CurrentTrain: epoch  5, batch    39 | loss: 4.0622687Losses:  4.123732089996338 0.09229430556297302
CurrentTrain: epoch  5, batch    40 | loss: 4.2160263Losses:  3.9769785404205322 0.07775440812110901
CurrentTrain: epoch  5, batch    41 | loss: 4.0547328Losses:  4.078649520874023 0.06634459644556046
CurrentTrain: epoch  5, batch    42 | loss: 4.1449943Losses:  4.043549537658691 0.1193077564239502
CurrentTrain: epoch  5, batch    43 | loss: 4.1628571Losses:  4.141852855682373 0.10666403919458389
CurrentTrain: epoch  5, batch    44 | loss: 4.2485170Losses:  3.9355056285858154 0.05177147686481476
CurrentTrain: epoch  5, batch    45 | loss: 3.9872770Losses:  4.333777904510498 0.05093169957399368
CurrentTrain: epoch  5, batch    46 | loss: 4.3847098Losses:  3.994434118270874 0.09854333102703094
CurrentTrain: epoch  5, batch    47 | loss: 4.0929775Losses:  3.9541313648223877 0.0485580712556839
CurrentTrain: epoch  5, batch    48 | loss: 4.0026894Losses:  3.9263265132904053 0.0671921968460083
CurrentTrain: epoch  5, batch    49 | loss: 3.9935188Losses:  4.163810729980469 0.06384709477424622
CurrentTrain: epoch  5, batch    50 | loss: 4.2276578Losses:  4.084009647369385 0.09923616051673889
CurrentTrain: epoch  5, batch    51 | loss: 4.1832457Losses:  4.044741630554199 0.09461905062198639
CurrentTrain: epoch  5, batch    52 | loss: 4.1393609Losses:  4.0182600021362305 0.10844836384057999
CurrentTrain: epoch  5, batch    53 | loss: 4.1267085Losses:  4.128162860870361 0.11877992004156113
CurrentTrain: epoch  5, batch    54 | loss: 4.2469430Losses:  3.974663734436035 0.01688120886683464
CurrentTrain: epoch  5, batch    55 | loss: 3.9915450Losses:  4.058145523071289 0.04129292443394661
CurrentTrain: epoch  5, batch    56 | loss: 4.0994387Losses:  4.060259819030762 0.0891033262014389
CurrentTrain: epoch  5, batch    57 | loss: 4.1493630Losses:  3.985832691192627 0.0845976322889328
CurrentTrain: epoch  5, batch    58 | loss: 4.0704303Losses:  4.228608131408691 0.06988655030727386
CurrentTrain: epoch  5, batch    59 | loss: 4.2984948Losses:  3.9542336463928223 0.046837251633405685
CurrentTrain: epoch  5, batch    60 | loss: 4.0010710Losses:  4.002747058868408 0.06962773203849792
CurrentTrain: epoch  5, batch    61 | loss: 4.0723748Losses:  4.025272846221924 0.0419062040746212
CurrentTrain: epoch  5, batch    62 | loss: 4.0671792Losses:  4.049342155456543 0.0912732481956482
CurrentTrain: epoch  5, batch    63 | loss: 4.1406155Losses:  4.117795944213867 0.06049949675798416
CurrentTrain: epoch  5, batch    64 | loss: 4.1782956Losses:  4.023265361785889 0.07121219485998154
CurrentTrain: epoch  5, batch    65 | loss: 4.0944777Losses:  4.000100135803223 0.12597012519836426
CurrentTrain: epoch  5, batch    66 | loss: 4.1260700Losses:  3.9239277839660645 0.05818496644496918
CurrentTrain: epoch  5, batch    67 | loss: 3.9821126Losses:  4.097255706787109 0.09922406077384949
CurrentTrain: epoch  5, batch    68 | loss: 4.1964798Losses:  4.101369380950928 0.10491795837879181
CurrentTrain: epoch  5, batch    69 | loss: 4.2062874Losses:  4.011719226837158 0.04612424597144127
CurrentTrain: epoch  5, batch    70 | loss: 4.0578437Losses:  4.87597131729126 0.1851774901151657
CurrentTrain: epoch  5, batch    71 | loss: 5.0611486Losses:  4.056103706359863 0.08255938440561295
CurrentTrain: epoch  5, batch    72 | loss: 4.1386633Losses:  4.04672908782959 0.06007968261837959
CurrentTrain: epoch  5, batch    73 | loss: 4.1068087Losses:  4.040007591247559 0.1383526474237442
CurrentTrain: epoch  5, batch    74 | loss: 4.1783605Losses:  4.043224334716797 0.048247434198856354
CurrentTrain: epoch  5, batch    75 | loss: 4.0914717Losses:  4.030716896057129 0.10538880527019501
CurrentTrain: epoch  5, batch    76 | loss: 4.1361055Losses:  3.9631118774414062 0.04668070375919342
CurrentTrain: epoch  5, batch    77 | loss: 4.0097928Losses:  4.054990768432617 0.06462720781564713
CurrentTrain: epoch  5, batch    78 | loss: 4.1196179Losses:  3.976846694946289 0.09752359241247177
CurrentTrain: epoch  5, batch    79 | loss: 4.0743704Losses:  3.9921646118164062 0.0642612874507904
CurrentTrain: epoch  5, batch    80 | loss: 4.0564260Losses:  3.964606285095215 0.10832902789115906
CurrentTrain: epoch  5, batch    81 | loss: 4.0729351Losses:  4.168696403503418 0.0871313065290451
CurrentTrain: epoch  5, batch    82 | loss: 4.2558279Losses:  3.932366132736206 0.028845176100730896
CurrentTrain: epoch  5, batch    83 | loss: 3.9612112Losses:  4.022848129272461 0.0554087869822979
CurrentTrain: epoch  5, batch    84 | loss: 4.0782571Losses:  5.577070236206055 0.2598237991333008
CurrentTrain: epoch  5, batch    85 | loss: 5.8368940Losses:  4.446516990661621 0.08518997579813004
CurrentTrain: epoch  5, batch    86 | loss: 4.5317068Losses:  4.094099044799805 0.07010236382484436
CurrentTrain: epoch  5, batch    87 | loss: 4.1642013Losses:  4.0038604736328125 0.06908269971609116
CurrentTrain: epoch  5, batch    88 | loss: 4.0729432Losses:  4.006752014160156 0.12821847200393677
CurrentTrain: epoch  5, batch    89 | loss: 4.1349707Losses:  3.9925119876861572 0.20068931579589844
CurrentTrain: epoch  5, batch    90 | loss: 4.1932011Losses:  4.128569602966309 0.09068150818347931
CurrentTrain: epoch  5, batch    91 | loss: 4.2192512Losses:  4.807075023651123 0.190280482172966
CurrentTrain: epoch  5, batch    92 | loss: 4.9973555Losses:  4.025266647338867 0.08356191962957382
CurrentTrain: epoch  5, batch    93 | loss: 4.1088285Losses:  3.9943044185638428 0.11804328858852386
CurrentTrain: epoch  5, batch    94 | loss: 4.1123476Losses:  4.152473449707031 0.1174226626753807
CurrentTrain: epoch  5, batch    95 | loss: 4.2698960Losses:  4.170947551727295 0.07099106907844543
CurrentTrain: epoch  5, batch    96 | loss: 4.2419386Losses:  3.9504802227020264 0.06602374464273453
CurrentTrain: epoch  5, batch    97 | loss: 4.0165038Losses:  4.004759311676025 0.10402897000312805
CurrentTrain: epoch  5, batch    98 | loss: 4.1087885Losses:  3.985964775085449 0.0896938145160675
CurrentTrain: epoch  5, batch    99 | loss: 4.0756588Losses:  3.9653191566467285 0.04760564863681793
CurrentTrain: epoch  5, batch   100 | loss: 4.0129247Losses:  4.219641208648682 0.06665442883968353
CurrentTrain: epoch  5, batch   101 | loss: 4.2862954Losses:  3.9890544414520264 0.1134955883026123
CurrentTrain: epoch  5, batch   102 | loss: 4.1025500Losses:  4.026976585388184 0.03280925750732422
CurrentTrain: epoch  5, batch   103 | loss: 4.0597858Losses:  3.9582972526550293 0.13096733391284943
CurrentTrain: epoch  5, batch   104 | loss: 4.0892644Losses:  3.917186975479126 0.05489138886332512
CurrentTrain: epoch  5, batch   105 | loss: 3.9720783Losses:  3.991502285003662 0.0998787060379982
CurrentTrain: epoch  5, batch   106 | loss: 4.0913811Losses:  4.127106666564941 0.06367014348506927
CurrentTrain: epoch  5, batch   107 | loss: 4.1907768Losses:  4.058791160583496 0.08065123856067657
CurrentTrain: epoch  5, batch   108 | loss: 4.1394424Losses:  4.091931343078613 0.07156318426132202
CurrentTrain: epoch  5, batch   109 | loss: 4.1634946Losses:  4.073034763336182 0.08448658883571625
CurrentTrain: epoch  5, batch   110 | loss: 4.1575212Losses:  4.127312660217285 0.04882040247321129
CurrentTrain: epoch  5, batch   111 | loss: 4.1761332Losses:  3.989151954650879 0.11526204645633698
CurrentTrain: epoch  5, batch   112 | loss: 4.1044140Losses:  3.9935879707336426 0.061980120837688446
CurrentTrain: epoch  5, batch   113 | loss: 4.0555682Losses:  4.010354995727539 0.08533275872468948
CurrentTrain: epoch  5, batch   114 | loss: 4.0956879Losses:  4.263746738433838 0.03832319751381874
CurrentTrain: epoch  5, batch   115 | loss: 4.3020701Losses:  4.043047904968262 0.05583362653851509
CurrentTrain: epoch  5, batch   116 | loss: 4.0988817Losses:  4.015620231628418 0.07929616421461105
CurrentTrain: epoch  5, batch   117 | loss: 4.0949163Losses:  4.024449348449707 0.10384660959243774
CurrentTrain: epoch  5, batch   118 | loss: 4.1282959Losses:  3.89951229095459 0.04552903771400452
CurrentTrain: epoch  5, batch   119 | loss: 3.9450414Losses:  3.98356556892395 0.05939054489135742
CurrentTrain: epoch  5, batch   120 | loss: 4.0429564Losses:  3.992264747619629 0.08217619359493256
CurrentTrain: epoch  5, batch   121 | loss: 4.0744410Losses:  4.0374226570129395 0.04862828552722931
CurrentTrain: epoch  5, batch   122 | loss: 4.0860510Losses:  3.984795093536377 0.050753526389598846
CurrentTrain: epoch  5, batch   123 | loss: 4.0355487Losses:  4.010506629943848 0.07155720144510269
CurrentTrain: epoch  5, batch   124 | loss: 4.0820637Losses:  4.288166046142578 0.06616556644439697
CurrentTrain: epoch  6, batch     0 | loss: 4.3543315Losses:  3.9415359497070312 0.030086399987339973
CurrentTrain: epoch  6, batch     1 | loss: 3.9716225Losses:  3.9695680141448975 0.06427940726280212
CurrentTrain: epoch  6, batch     2 | loss: 4.0338473Losses:  4.039905548095703 0.06228140369057655
CurrentTrain: epoch  6, batch     3 | loss: 4.1021872Losses:  4.008179664611816 0.08269426971673965
CurrentTrain: epoch  6, batch     4 | loss: 4.0908737Losses:  3.9164819717407227 0.03885146230459213
CurrentTrain: epoch  6, batch     5 | loss: 3.9553335Losses:  3.9496984481811523 0.09557264298200607
CurrentTrain: epoch  6, batch     6 | loss: 4.0452709Losses:  3.9546265602111816 0.09330887347459793
CurrentTrain: epoch  6, batch     7 | loss: 4.0479355Losses:  3.9883739948272705 0.09684345126152039
CurrentTrain: epoch  6, batch     8 | loss: 4.0852175Losses:  3.943826198577881 0.04567079246044159
CurrentTrain: epoch  6, batch     9 | loss: 3.9894969Losses:  4.137081146240234 0.0604945570230484
CurrentTrain: epoch  6, batch    10 | loss: 4.1975756Losses:  4.060381889343262 0.05785534530878067
CurrentTrain: epoch  6, batch    11 | loss: 4.1182370Losses:  4.013923645019531 0.07172777503728867
CurrentTrain: epoch  6, batch    12 | loss: 4.0856514Losses:  4.008029937744141 0.06211692839860916
CurrentTrain: epoch  6, batch    13 | loss: 4.0701470Losses:  3.972195863723755 0.08655893802642822
CurrentTrain: epoch  6, batch    14 | loss: 4.0587549Losses:  4.080287933349609 0.045626237988471985
CurrentTrain: epoch  6, batch    15 | loss: 4.1259141Losses:  3.9481430053710938 0.10370112955570221
CurrentTrain: epoch  6, batch    16 | loss: 4.0518441Losses:  4.075756549835205 0.0843544602394104
CurrentTrain: epoch  6, batch    17 | loss: 4.1601110Losses:  4.079109191894531 0.10338596254587173
CurrentTrain: epoch  6, batch    18 | loss: 4.1824951Losses:  4.01527214050293 0.02075488492846489
CurrentTrain: epoch  6, batch    19 | loss: 4.0360270Losses:  3.9556894302368164 0.07781840860843658
CurrentTrain: epoch  6, batch    20 | loss: 4.0335078Losses:  3.9957709312438965 0.060987524688243866
CurrentTrain: epoch  6, batch    21 | loss: 4.0567584Losses:  3.9741687774658203 0.08996551483869553
CurrentTrain: epoch  6, batch    22 | loss: 4.0641341Losses:  3.9993226528167725 0.031971607357263565
CurrentTrain: epoch  6, batch    23 | loss: 4.0312943Losses:  3.941046714782715 0.07687842100858688
CurrentTrain: epoch  6, batch    24 | loss: 4.0179253Losses:  3.9731385707855225 0.06660064309835434
CurrentTrain: epoch  6, batch    25 | loss: 4.0397391Losses:  3.9609320163726807 0.06392784416675568
CurrentTrain: epoch  6, batch    26 | loss: 4.0248599Losses:  3.9569320678710938 0.07326048612594604
CurrentTrain: epoch  6, batch    27 | loss: 4.0301924Losses:  3.969895362854004 0.09641250222921371
CurrentTrain: epoch  6, batch    28 | loss: 4.0663080Losses:  3.923032760620117 0.06443624198436737
CurrentTrain: epoch  6, batch    29 | loss: 3.9874690Losses:  3.928715467453003 0.0963221937417984
CurrentTrain: epoch  6, batch    30 | loss: 4.0250378Losses:  3.9915573596954346 0.05466153845191002
CurrentTrain: epoch  6, batch    31 | loss: 4.0462189Losses:  4.025501251220703 0.06660309433937073
CurrentTrain: epoch  6, batch    32 | loss: 4.0921044Losses:  4.003469467163086 0.08533607423305511
CurrentTrain: epoch  6, batch    33 | loss: 4.0888057Losses:  3.9901859760284424 0.08566731959581375
CurrentTrain: epoch  6, batch    34 | loss: 4.0758533Losses:  3.9435524940490723 0.028109457343816757
CurrentTrain: epoch  6, batch    35 | loss: 3.9716620Losses:  3.9437906742095947 0.05274204537272453
CurrentTrain: epoch  6, batch    36 | loss: 3.9965327Losses:  4.030189514160156 0.021977173164486885
CurrentTrain: epoch  6, batch    37 | loss: 4.0521665Losses:  3.9863333702087402 0.09634207934141159
CurrentTrain: epoch  6, batch    38 | loss: 4.0826755Losses:  3.939592123031616 0.08978573232889175
CurrentTrain: epoch  6, batch    39 | loss: 4.0293779Losses:  3.981060028076172 0.09513340890407562
CurrentTrain: epoch  6, batch    40 | loss: 4.0761933Losses:  3.9954707622528076 0.07117265462875366
CurrentTrain: epoch  6, batch    41 | loss: 4.0666432Losses:  3.9718148708343506 0.04375990107655525
CurrentTrain: epoch  6, batch    42 | loss: 4.0155749Losses:  3.976533889770508 0.07661201059818268
CurrentTrain: epoch  6, batch    43 | loss: 4.0531459Losses:  3.959907054901123 0.020685214549303055
CurrentTrain: epoch  6, batch    44 | loss: 3.9805923Losses:  3.9846298694610596 0.1012968197464943
CurrentTrain: epoch  6, batch    45 | loss: 4.0859265Losses:  4.017050743103027 0.05440916493535042
CurrentTrain: epoch  6, batch    46 | loss: 4.0714598Losses:  4.10988712310791 0.0930996686220169
CurrentTrain: epoch  6, batch    47 | loss: 4.2029867Losses:  4.151627540588379 0.11520518362522125
CurrentTrain: epoch  6, batch    48 | loss: 4.2668328Losses:  3.917330503463745 0.06456198543310165
CurrentTrain: epoch  6, batch    49 | loss: 3.9818926Losses:  3.981595277786255 0.06651348620653152
CurrentTrain: epoch  6, batch    50 | loss: 4.0481086Losses:  3.966886520385742 0.07554136216640472
CurrentTrain: epoch  6, batch    51 | loss: 4.0424280Losses:  3.9381496906280518 0.0694076269865036
CurrentTrain: epoch  6, batch    52 | loss: 4.0075574Losses:  4.016753196716309 0.08242384344339371
CurrentTrain: epoch  6, batch    53 | loss: 4.0991769Losses:  3.946516752243042 0.05876737833023071
CurrentTrain: epoch  6, batch    54 | loss: 4.0052843Losses:  3.956195116043091 0.07741721719503403
CurrentTrain: epoch  6, batch    55 | loss: 4.0336123Losses:  3.9618782997131348 0.06104743480682373
CurrentTrain: epoch  6, batch    56 | loss: 4.0229259Losses:  3.987154960632324 0.0683491975069046
CurrentTrain: epoch  6, batch    57 | loss: 4.0555043Losses:  3.9632325172424316 0.09999358654022217
CurrentTrain: epoch  6, batch    58 | loss: 4.0632262Losses:  3.9377262592315674 0.07685773074626923
CurrentTrain: epoch  6, batch    59 | loss: 4.0145841Losses:  3.9845352172851562 0.05060841888189316
CurrentTrain: epoch  6, batch    60 | loss: 4.0351439Losses:  4.007672309875488 0.08386340737342834
CurrentTrain: epoch  6, batch    61 | loss: 4.0915356Losses:  3.954040288925171 0.03665098547935486
CurrentTrain: epoch  6, batch    62 | loss: 3.9906912Losses:  4.071722030639648 0.015228847041726112
CurrentTrain: epoch  6, batch    63 | loss: 4.0869508Losses:  4.011006832122803 0.047477416694164276
CurrentTrain: epoch  6, batch    64 | loss: 4.0584841Losses:  3.975226402282715 0.0662168338894844
CurrentTrain: epoch  6, batch    65 | loss: 4.0414433Losses:  3.939584255218506 0.07302476465702057
CurrentTrain: epoch  6, batch    66 | loss: 4.0126090Losses:  3.970843553543091 0.06673040986061096
CurrentTrain: epoch  6, batch    67 | loss: 4.0375738Losses:  3.940920352935791 0.06030828505754471
CurrentTrain: epoch  6, batch    68 | loss: 4.0012288Losses:  3.927407741546631 0.11401861906051636
CurrentTrain: epoch  6, batch    69 | loss: 4.0414262Losses:  3.965606212615967 0.05477700009942055
CurrentTrain: epoch  6, batch    70 | loss: 4.0203834Losses:  3.925333023071289 0.05364057049155235
CurrentTrain: epoch  6, batch    71 | loss: 3.9789736Losses:  3.9872958660125732 0.0278329961001873
CurrentTrain: epoch  6, batch    72 | loss: 4.0151291Losses:  4.002956867218018 0.0586976632475853
CurrentTrain: epoch  6, batch    73 | loss: 4.0616546Losses:  3.952982187271118 0.08571045100688934
CurrentTrain: epoch  6, batch    74 | loss: 4.0386925Losses:  3.9552884101867676 0.06252562254667282
CurrentTrain: epoch  6, batch    75 | loss: 4.0178142Losses:  3.9961891174316406 0.1038200855255127
CurrentTrain: epoch  6, batch    76 | loss: 4.1000090Losses:  3.98032808303833 0.08731811493635178
CurrentTrain: epoch  6, batch    77 | loss: 4.0676460Losses:  3.9816794395446777 0.04490911215543747
CurrentTrain: epoch  6, batch    78 | loss: 4.0265884Losses:  3.9348032474517822 0.07040170580148697
CurrentTrain: epoch  6, batch    79 | loss: 4.0052052Losses:  4.045699119567871 0.042221203446388245
CurrentTrain: epoch  6, batch    80 | loss: 4.0879202Losses:  3.9585256576538086 0.07228158414363861
CurrentTrain: epoch  6, batch    81 | loss: 4.0308070Losses:  3.934659957885742 0.05604560673236847
CurrentTrain: epoch  6, batch    82 | loss: 3.9907055Losses:  3.987823486328125 0.07419143617153168
CurrentTrain: epoch  6, batch    83 | loss: 4.0620151Losses:  3.9329566955566406 0.07515488564968109
CurrentTrain: epoch  6, batch    84 | loss: 4.0081115Losses:  3.9153826236724854 0.06790171563625336
CurrentTrain: epoch  6, batch    85 | loss: 3.9832842Losses:  3.9636597633361816 0.0910419151186943
CurrentTrain: epoch  6, batch    86 | loss: 4.0547018Losses:  3.9085185527801514 0.012459386140108109
CurrentTrain: epoch  6, batch    87 | loss: 3.9209778Losses:  3.9743285179138184 0.07440054416656494
CurrentTrain: epoch  6, batch    88 | loss: 4.0487289Losses:  3.9504880905151367 0.05701985955238342
CurrentTrain: epoch  6, batch    89 | loss: 4.0075078Losses:  4.068971157073975 0.04444359242916107
CurrentTrain: epoch  6, batch    90 | loss: 4.1134148Losses:  3.9546687602996826 0.06210330128669739
CurrentTrain: epoch  6, batch    91 | loss: 4.0167723Losses:  3.9299159049987793 0.0682327002286911
CurrentTrain: epoch  6, batch    92 | loss: 3.9981487Losses:  3.870582103729248 0.050786346197128296
CurrentTrain: epoch  6, batch    93 | loss: 3.9213684Losses:  3.96551251411438 0.04655933752655983
CurrentTrain: epoch  6, batch    94 | loss: 4.0120721Losses:  3.809110403060913 0.057858843356370926
CurrentTrain: epoch  6, batch    95 | loss: 3.8669693Losses:  3.8746790885925293 0.04559723287820816
CurrentTrain: epoch  6, batch    96 | loss: 3.9202764Losses:  3.968843460083008 0.06006576493382454
CurrentTrain: epoch  6, batch    97 | loss: 4.0289092Losses:  3.9557271003723145 0.0549280047416687
CurrentTrain: epoch  6, batch    98 | loss: 4.0106549Losses:  3.9747352600097656 0.036204732954502106
CurrentTrain: epoch  6, batch    99 | loss: 4.0109401Losses:  3.983250617980957 0.06648419797420502
CurrentTrain: epoch  6, batch   100 | loss: 4.0497346Losses:  3.9880239963531494 0.06515127420425415
CurrentTrain: epoch  6, batch   101 | loss: 4.0531754Losses:  3.9174437522888184 0.05148923769593239
CurrentTrain: epoch  6, batch   102 | loss: 3.9689331Losses:  3.968799114227295 0.05695262551307678
CurrentTrain: epoch  6, batch   103 | loss: 4.0257516Losses:  3.908607006072998 0.0710109993815422
CurrentTrain: epoch  6, batch   104 | loss: 3.9796181Losses:  3.993258237838745 0.05281265079975128
CurrentTrain: epoch  6, batch   105 | loss: 4.0460711Losses:  3.9305315017700195 0.043347906321287155
CurrentTrain: epoch  6, batch   106 | loss: 3.9738793Losses:  3.928374767303467 0.02612478658556938
CurrentTrain: epoch  6, batch   107 | loss: 3.9544995Losses:  3.9555816650390625 0.06050213426351547
CurrentTrain: epoch  6, batch   108 | loss: 4.0160837Losses:  3.933354377746582 0.03526398167014122
CurrentTrain: epoch  6, batch   109 | loss: 3.9686184Losses:  3.929410457611084 0.04941914975643158
CurrentTrain: epoch  6, batch   110 | loss: 3.9788296Losses:  3.9168710708618164 0.06271804869174957
CurrentTrain: epoch  6, batch   111 | loss: 3.9795892Losses:  3.9433975219726562 0.044055864214897156
CurrentTrain: epoch  6, batch   112 | loss: 3.9874535Losses:  3.965198040008545 0.04777342826128006
CurrentTrain: epoch  6, batch   113 | loss: 4.0129714Losses:  3.9293904304504395 0.057212382555007935
CurrentTrain: epoch  6, batch   114 | loss: 3.9866028Losses:  3.9461326599121094 0.04821775108575821
CurrentTrain: epoch  6, batch   115 | loss: 3.9943504Losses:  3.8633580207824707 0.06798500567674637
CurrentTrain: epoch  6, batch   116 | loss: 3.9313431Losses:  3.9402389526367188 0.09326684474945068
CurrentTrain: epoch  6, batch   117 | loss: 4.0335059Losses:  3.9401941299438477 0.08662770688533783
CurrentTrain: epoch  6, batch   118 | loss: 4.0268216Losses:  3.9444684982299805 0.08485215157270432
CurrentTrain: epoch  6, batch   119 | loss: 4.0293207Losses:  3.9526219367980957 0.0731516107916832
CurrentTrain: epoch  6, batch   120 | loss: 4.0257735Losses:  3.9468941688537598 0.02107483521103859
CurrentTrain: epoch  6, batch   121 | loss: 3.9679689Losses:  3.9564967155456543 0.04051012545824051
CurrentTrain: epoch  6, batch   122 | loss: 3.9970069Losses:  3.9230599403381348 0.07477112859487534
CurrentTrain: epoch  6, batch   123 | loss: 3.9978311Losses:  3.957791805267334 0.09915183484554291
CurrentTrain: epoch  6, batch   124 | loss: 4.0569434Losses:  3.9213194847106934 0.04781506210565567
CurrentTrain: epoch  7, batch     0 | loss: 3.9691346Losses:  3.94350266456604 0.05376162379980087
CurrentTrain: epoch  7, batch     1 | loss: 3.9972644Losses:  4.002691745758057 0.07873983681201935
CurrentTrain: epoch  7, batch     2 | loss: 4.0814314Losses:  3.985156297683716 0.04428086802363396
CurrentTrain: epoch  7, batch     3 | loss: 4.0294371Losses:  3.9499030113220215 0.06212426722049713
CurrentTrain: epoch  7, batch     4 | loss: 4.0120273Losses:  3.9688143730163574 0.02772018127143383
CurrentTrain: epoch  7, batch     5 | loss: 3.9965346Losses:  3.996941089630127 0.08216147124767303
CurrentTrain: epoch  7, batch     6 | loss: 4.0791025Losses:  3.9312429428100586 0.028719045221805573
CurrentTrain: epoch  7, batch     7 | loss: 3.9599619Losses:  3.9521803855895996 0.04121694341301918
CurrentTrain: epoch  7, batch     8 | loss: 3.9933972Losses:  3.8927249908447266 0.022169534116983414
CurrentTrain: epoch  7, batch     9 | loss: 3.9148946Losses:  3.970611572265625 0.06194259971380234
CurrentTrain: epoch  7, batch    10 | loss: 4.0325541Losses:  4.022220611572266 0.030305080115795135
CurrentTrain: epoch  7, batch    11 | loss: 4.0525255Losses:  3.9992051124572754 0.04872581362724304
CurrentTrain: epoch  7, batch    12 | loss: 4.0479307Losses:  3.960792064666748 0.0741884857416153
CurrentTrain: epoch  7, batch    13 | loss: 4.0349808Losses:  3.9704999923706055 0.05222656950354576
CurrentTrain: epoch  7, batch    14 | loss: 4.0227265Losses:  3.9384751319885254 0.0708148181438446
CurrentTrain: epoch  7, batch    15 | loss: 4.0092897Losses:  3.964865207672119 0.05294950306415558
CurrentTrain: epoch  7, batch    16 | loss: 4.0178146Losses:  3.9957926273345947 0.03442588448524475
CurrentTrain: epoch  7, batch    17 | loss: 4.0302186Losses:  3.9075794219970703 0.08582060784101486
CurrentTrain: epoch  7, batch    18 | loss: 3.9934001Losses:  3.980201005935669 0.0441022589802742
CurrentTrain: epoch  7, batch    19 | loss: 4.0243034Losses:  3.9747331142425537 0.07009916007518768
CurrentTrain: epoch  7, batch    20 | loss: 4.0448322Losses:  3.9307937622070312 0.07275795936584473
CurrentTrain: epoch  7, batch    21 | loss: 4.0035515Losses:  3.9282844066619873 0.06910382211208344
CurrentTrain: epoch  7, batch    22 | loss: 3.9973881Losses:  3.9152274131774902 0.034473925828933716
CurrentTrain: epoch  7, batch    23 | loss: 3.9497013Losses:  4.0062432289123535 0.016776442527770996
CurrentTrain: epoch  7, batch    24 | loss: 4.0230198Losses:  3.919995069503784 0.08779306709766388
CurrentTrain: epoch  7, batch    25 | loss: 4.0077882Losses:  3.964780807495117 0.055000461637973785
CurrentTrain: epoch  7, batch    26 | loss: 4.0197811Losses:  3.892129898071289 0.04787605628371239
CurrentTrain: epoch  7, batch    27 | loss: 3.9400060Losses:  3.984755277633667 0.05960971117019653
CurrentTrain: epoch  7, batch    28 | loss: 4.0443649Losses:  3.957071542739868 0.06356923282146454
CurrentTrain: epoch  7, batch    29 | loss: 4.0206409Losses:  3.9492712020874023 0.0939532220363617
CurrentTrain: epoch  7, batch    30 | loss: 4.0432243Losses:  3.960444450378418 0.07211330533027649
CurrentTrain: epoch  7, batch    31 | loss: 4.0325580Losses:  3.940197467803955 0.044907405972480774
CurrentTrain: epoch  7, batch    32 | loss: 3.9851048Losses:  3.9655065536499023 0.09101284295320511
CurrentTrain: epoch  7, batch    33 | loss: 4.0565195Losses:  3.949036121368408 0.0859915018081665
CurrentTrain: epoch  7, batch    34 | loss: 4.0350275Losses:  3.944443702697754 0.07385939359664917
CurrentTrain: epoch  7, batch    35 | loss: 4.0183029Losses:  3.972487449645996 0.04392659664154053
CurrentTrain: epoch  7, batch    36 | loss: 4.0164142Losses:  3.956995964050293 0.038675036281347275
CurrentTrain: epoch  7, batch    37 | loss: 3.9956710Losses:  3.944845676422119 0.07350999116897583
CurrentTrain: epoch  7, batch    38 | loss: 4.0183558Losses:  3.9440951347351074 0.06550782918930054
CurrentTrain: epoch  7, batch    39 | loss: 4.0096030Losses:  3.9773452281951904 0.04546570032835007
CurrentTrain: epoch  7, batch    40 | loss: 4.0228109Losses:  3.965134620666504 0.04677458852529526
CurrentTrain: epoch  7, batch    41 | loss: 4.0119090Losses:  3.985328197479248 0.06062851473689079
CurrentTrain: epoch  7, batch    42 | loss: 4.0459566Losses:  3.926424980163574 0.050422828644514084
CurrentTrain: epoch  7, batch    43 | loss: 3.9768479Losses:  3.898756265640259 0.03846295177936554
CurrentTrain: epoch  7, batch    44 | loss: 3.9372191Losses:  3.9643564224243164 0.05967376381158829
CurrentTrain: epoch  7, batch    45 | loss: 4.0240302Losses:  3.9989354610443115 0.03430488333106041
CurrentTrain: epoch  7, batch    46 | loss: 4.0332403Losses:  3.951968193054199 0.04116710647940636
CurrentTrain: epoch  7, batch    47 | loss: 3.9931352Losses:  3.9335718154907227 0.058760594576597214
CurrentTrain: epoch  7, batch    48 | loss: 3.9923325Losses:  3.9548940658569336 0.08452066779136658
CurrentTrain: epoch  7, batch    49 | loss: 4.0394149Losses:  3.9711859226226807 0.07891230285167694
CurrentTrain: epoch  7, batch    50 | loss: 4.0500984Losses:  3.97230863571167 0.020009562373161316
CurrentTrain: epoch  7, batch    51 | loss: 3.9923182Losses:  3.970625400543213 0.02801446244120598
CurrentTrain: epoch  7, batch    52 | loss: 3.9986398Losses:  3.8377180099487305 0.03563178703188896
CurrentTrain: epoch  7, batch    53 | loss: 3.8733499Losses:  3.9803314208984375 0.05534112825989723
CurrentTrain: epoch  7, batch    54 | loss: 4.0356727Losses:  3.9476802349090576 0.07385388761758804
CurrentTrain: epoch  7, batch    55 | loss: 4.0215340Losses:  3.932624101638794 0.06272310763597488
CurrentTrain: epoch  7, batch    56 | loss: 3.9953473Losses:  3.9516043663024902 0.027495291084051132
CurrentTrain: epoch  7, batch    57 | loss: 3.9790998Losses:  3.9797778129577637 0.042948607355356216
CurrentTrain: epoch  7, batch    58 | loss: 4.0227265Losses:  3.9147212505340576 0.03871231898665428
CurrentTrain: epoch  7, batch    59 | loss: 3.9534335Losses:  3.9307639598846436 0.06341975927352905
CurrentTrain: epoch  7, batch    60 | loss: 3.9941838Losses:  3.9307444095611572 0.02824140340089798
CurrentTrain: epoch  7, batch    61 | loss: 3.9589858Losses:  3.9650754928588867 0.049185797572135925
CurrentTrain: epoch  7, batch    62 | loss: 4.0142612Losses:  4.004655361175537 0.051214173436164856
CurrentTrain: epoch  7, batch    63 | loss: 4.0558696Losses:  3.9249463081359863 0.04628344625234604
CurrentTrain: epoch  7, batch    64 | loss: 3.9712298Losses:  3.939849853515625 0.022910792380571365
CurrentTrain: epoch  7, batch    65 | loss: 3.9627607Losses:  3.913421630859375 0.0651269406080246
CurrentTrain: epoch  7, batch    66 | loss: 3.9785485Losses:  3.934347629547119 0.045560188591480255
CurrentTrain: epoch  7, batch    67 | loss: 3.9799078Losses:  3.9981629848480225 0.039412032812833786
CurrentTrain: epoch  7, batch    68 | loss: 4.0375752Losses:  4.003348350524902 0.025780076161026955
CurrentTrain: epoch  7, batch    69 | loss: 4.0291286Losses:  3.97428560256958 0.021940715610980988
CurrentTrain: epoch  7, batch    70 | loss: 3.9962263Losses:  3.938652515411377 0.06755447387695312
CurrentTrain: epoch  7, batch    71 | loss: 4.0062070Losses:  3.9270753860473633 0.04065587371587753
CurrentTrain: epoch  7, batch    72 | loss: 3.9677312Losses:  4.010996341705322 0.04526585340499878
CurrentTrain: epoch  7, batch    73 | loss: 4.0562620Losses:  3.926983118057251 0.0571257621049881
CurrentTrain: epoch  7, batch    74 | loss: 3.9841089Losses:  3.935357093811035 0.06025838106870651
CurrentTrain: epoch  7, batch    75 | loss: 3.9956155Losses:  3.9556679725646973 0.08182482421398163
CurrentTrain: epoch  7, batch    76 | loss: 4.0374928Losses:  3.970470428466797 0.043122369796037674
CurrentTrain: epoch  7, batch    77 | loss: 4.0135927Losses:  3.971064805984497 0.034605178982019424
CurrentTrain: epoch  7, batch    78 | loss: 4.0056701Losses:  3.882284164428711 0.04686034098267555
CurrentTrain: epoch  7, batch    79 | loss: 3.9291446Losses:  3.97385573387146 0.03520534187555313
CurrentTrain: epoch  7, batch    80 | loss: 4.0090609Losses:  4.014801979064941 0.07570978999137878
CurrentTrain: epoch  7, batch    81 | loss: 4.0905118Losses:  3.939798355102539 0.061556488275527954
CurrentTrain: epoch  7, batch    82 | loss: 4.0013547Losses:  3.8786849975585938 0.0636039674282074
CurrentTrain: epoch  7, batch    83 | loss: 3.9422889Losses:  3.9070801734924316 0.07500723749399185
CurrentTrain: epoch  7, batch    84 | loss: 3.9820874Losses:  3.985907554626465 0.03568901866674423
CurrentTrain: epoch  7, batch    85 | loss: 4.0215964Losses:  3.990532159805298 0.055587343871593475
CurrentTrain: epoch  7, batch    86 | loss: 4.0461197Losses:  3.988208293914795 0.05818004906177521
CurrentTrain: epoch  7, batch    87 | loss: 4.0463881Losses:  3.956130027770996 0.054143086075782776
CurrentTrain: epoch  7, batch    88 | loss: 4.0102730Losses:  3.911973714828491 0.03304243087768555
CurrentTrain: epoch  7, batch    89 | loss: 3.9450161Losses:  3.919926166534424 0.03579159826040268
CurrentTrain: epoch  7, batch    90 | loss: 3.9557178Losses:  3.9577255249023438 0.046612925827503204
CurrentTrain: epoch  7, batch    91 | loss: 4.0043383Losses:  3.9505250453948975 0.05243352800607681
CurrentTrain: epoch  7, batch    92 | loss: 4.0029588Losses:  3.925548553466797 0.030736371874809265
CurrentTrain: epoch  7, batch    93 | loss: 3.9562850Losses:  3.9562389850616455 0.07715533673763275
CurrentTrain: epoch  7, batch    94 | loss: 4.0333943Losses:  3.867109537124634 0.023384634405374527
CurrentTrain: epoch  7, batch    95 | loss: 3.8904941Losses:  3.9676122665405273 0.07492761313915253
CurrentTrain: epoch  7, batch    96 | loss: 4.0425401Losses:  3.9634971618652344 0.0872800350189209
CurrentTrain: epoch  7, batch    97 | loss: 4.0507774Losses:  3.9768800735473633 0.07039685547351837
CurrentTrain: epoch  7, batch    98 | loss: 4.0472770Losses:  3.975133180618286 0.10147292912006378
CurrentTrain: epoch  7, batch    99 | loss: 4.0766063Losses:  3.9203975200653076 0.0624573789536953
CurrentTrain: epoch  7, batch   100 | loss: 3.9828548Losses:  3.9867196083068848 0.029810622334480286
CurrentTrain: epoch  7, batch   101 | loss: 4.0165300Losses:  3.9399421215057373 0.05801226198673248
CurrentTrain: epoch  7, batch   102 | loss: 3.9979544Losses:  3.9423880577087402 0.04754823073744774
CurrentTrain: epoch  7, batch   103 | loss: 3.9899364Losses:  3.9339699745178223 0.06380297243595123
CurrentTrain: epoch  7, batch   104 | loss: 3.9977729Losses:  3.888735771179199 0.04353215545415878
CurrentTrain: epoch  7, batch   105 | loss: 3.9322679Losses:  3.878018617630005 0.04316353797912598
CurrentTrain: epoch  7, batch   106 | loss: 3.9211822Losses:  3.9844746589660645 0.06155029684305191
CurrentTrain: epoch  7, batch   107 | loss: 4.0460248Losses:  3.901801586151123 0.033383872359991074
CurrentTrain: epoch  7, batch   108 | loss: 3.9351854Losses:  3.9687371253967285 0.041799359023571014
CurrentTrain: epoch  7, batch   109 | loss: 4.0105367Losses:  4.040734767913818 0.04169343784451485
CurrentTrain: epoch  7, batch   110 | loss: 4.0824280Losses:  3.99006724357605 0.041944753378629684
CurrentTrain: epoch  7, batch   111 | loss: 4.0320120Losses:  3.959242582321167 0.021460767835378647
CurrentTrain: epoch  7, batch   112 | loss: 3.9807034Losses:  3.940347671508789 0.04427054896950722
CurrentTrain: epoch  7, batch   113 | loss: 3.9846182Losses:  3.9275014400482178 0.06831834465265274
CurrentTrain: epoch  7, batch   114 | loss: 3.9958198Losses:  3.941141128540039 0.026964889839291573
CurrentTrain: epoch  7, batch   115 | loss: 3.9681060Losses:  3.927518606185913 0.07608998566865921
CurrentTrain: epoch  7, batch   116 | loss: 4.0036087Losses:  3.974130630493164 0.03948110342025757
CurrentTrain: epoch  7, batch   117 | loss: 4.0136118Losses:  3.9608993530273438 0.059456340968608856
CurrentTrain: epoch  7, batch   118 | loss: 4.0203557Losses:  3.9761874675750732 0.047926172614097595
CurrentTrain: epoch  7, batch   119 | loss: 4.0241137Losses:  3.92714786529541 0.048449307680130005
CurrentTrain: epoch  7, batch   120 | loss: 3.9755971Losses:  3.969175338745117 0.0720815360546112
CurrentTrain: epoch  7, batch   121 | loss: 4.0412569Losses:  3.9364423751831055 0.04359353333711624
CurrentTrain: epoch  7, batch   122 | loss: 3.9800360Losses:  3.924774646759033 0.036014970391988754
CurrentTrain: epoch  7, batch   123 | loss: 3.9607897Losses:  4.004817008972168 0.04629840701818466
CurrentTrain: epoch  7, batch   124 | loss: 4.0511155Losses:  3.925323486328125 0.055253028869628906
CurrentTrain: epoch  8, batch     0 | loss: 3.9805765Losses:  3.9290380477905273 0.07976511865854263
CurrentTrain: epoch  8, batch     1 | loss: 4.0088034Losses:  4.01037073135376 0.06727994233369827
CurrentTrain: epoch  8, batch     2 | loss: 4.0776505Losses:  3.9626100063323975 0.04240928590297699
CurrentTrain: epoch  8, batch     3 | loss: 4.0050192Losses:  3.9341881275177 0.052542418241500854
CurrentTrain: epoch  8, batch     4 | loss: 3.9867306Losses:  3.9295687675476074 0.057535767555236816
CurrentTrain: epoch  8, batch     5 | loss: 3.9871044Losses:  3.9319913387298584 0.030694901943206787
CurrentTrain: epoch  8, batch     6 | loss: 3.9626863Losses:  3.8951821327209473 0.02113930881023407
CurrentTrain: epoch  8, batch     7 | loss: 3.9163215Losses:  3.902524948120117 0.06042884662747383
CurrentTrain: epoch  8, batch     8 | loss: 3.9629538Losses:  3.8855857849121094 0.040527500212192535
CurrentTrain: epoch  8, batch     9 | loss: 3.9261134Losses:  3.9715864658355713 0.04343270882964134
CurrentTrain: epoch  8, batch    10 | loss: 4.0150189Losses:  3.928924560546875 0.07474727928638458
CurrentTrain: epoch  8, batch    11 | loss: 4.0036716Losses:  3.932980537414551 0.05394262820482254
CurrentTrain: epoch  8, batch    12 | loss: 3.9869232Losses:  3.888944625854492 0.047436028718948364
CurrentTrain: epoch  8, batch    13 | loss: 3.9363806Losses:  3.929825782775879 0.05350136011838913
CurrentTrain: epoch  8, batch    14 | loss: 3.9833272Losses:  3.9705562591552734 0.04832097887992859
CurrentTrain: epoch  8, batch    15 | loss: 4.0188770Losses:  3.960793972015381 0.05347811430692673
CurrentTrain: epoch  8, batch    16 | loss: 4.0142722Losses:  3.923171043395996 0.03490156680345535
CurrentTrain: epoch  8, batch    17 | loss: 3.9580727Losses:  3.935944080352783 0.04984807223081589
CurrentTrain: epoch  8, batch    18 | loss: 3.9857922Losses:  3.9141197204589844 0.047020137310028076
CurrentTrain: epoch  8, batch    19 | loss: 3.9611399Losses:  3.955049991607666 0.03136839717626572
CurrentTrain: epoch  8, batch    20 | loss: 3.9864185Losses:  3.9463160037994385 0.04042290151119232
CurrentTrain: epoch  8, batch    21 | loss: 3.9867389Losses:  3.9626550674438477 0.05132484808564186
CurrentTrain: epoch  8, batch    22 | loss: 4.0139799Losses:  3.924058437347412 0.02845747023820877
CurrentTrain: epoch  8, batch    23 | loss: 3.9525158Losses:  3.8919568061828613 0.03874677047133446
CurrentTrain: epoch  8, batch    24 | loss: 3.9307036Losses:  3.9027276039123535 0.07544007897377014
CurrentTrain: epoch  8, batch    25 | loss: 3.9781678Losses:  3.9316365718841553 0.040144141763448715
CurrentTrain: epoch  8, batch    26 | loss: 3.9717808Losses:  3.9648935794830322 0.047190144658088684
CurrentTrain: epoch  8, batch    27 | loss: 4.0120835Losses:  3.9343156814575195 0.07876715809106827
CurrentTrain: epoch  8, batch    28 | loss: 4.0130830Losses:  3.9845032691955566 0.06147204712033272
CurrentTrain: epoch  8, batch    29 | loss: 4.0459752Losses:  3.971965789794922 0.03954482451081276
CurrentTrain: epoch  8, batch    30 | loss: 4.0115108Losses:  3.889218807220459 0.07267153263092041
CurrentTrain: epoch  8, batch    31 | loss: 3.9618902Losses:  3.925894260406494 0.03323047608137131
CurrentTrain: epoch  8, batch    32 | loss: 3.9591248Losses:  3.922208309173584 0.0346415713429451
CurrentTrain: epoch  8, batch    33 | loss: 3.9568498Losses:  3.9894471168518066 0.05840107426047325
CurrentTrain: epoch  8, batch    34 | loss: 4.0478482Losses:  3.957089424133301 0.04710594564676285
CurrentTrain: epoch  8, batch    35 | loss: 4.0041952Losses:  3.9086203575134277 0.05214593932032585
CurrentTrain: epoch  8, batch    36 | loss: 3.9607663Losses:  3.9203574657440186 0.05014289543032646
CurrentTrain: epoch  8, batch    37 | loss: 3.9705005Losses:  3.929824113845825 0.026090428233146667
CurrentTrain: epoch  8, batch    38 | loss: 3.9559145Losses:  3.9381349086761475 0.05873508378863335
CurrentTrain: epoch  8, batch    39 | loss: 3.9968700Losses:  3.9853413105010986 0.03877753019332886
CurrentTrain: epoch  8, batch    40 | loss: 4.0241189Losses:  3.84818696975708 0.035850606858730316
CurrentTrain: epoch  8, batch    41 | loss: 3.8840375Losses:  3.923300266265869 0.047288961708545685
CurrentTrain: epoch  8, batch    42 | loss: 3.9705892Losses:  3.9540650844573975 0.04978463053703308
CurrentTrain: epoch  8, batch    43 | loss: 4.0038495Losses:  3.9106643199920654 0.044396497309207916
CurrentTrain: epoch  8, batch    44 | loss: 3.9550607Losses:  3.9386026859283447 0.06911540031433105
CurrentTrain: epoch  8, batch    45 | loss: 4.0077181Losses:  3.9688827991485596 0.048806823790073395
CurrentTrain: epoch  8, batch    46 | loss: 4.0176897Losses:  3.9482979774475098 0.042718589305877686
CurrentTrain: epoch  8, batch    47 | loss: 3.9910166Losses:  3.915836811065674 0.0596000999212265
CurrentTrain: epoch  8, batch    48 | loss: 3.9754369Losses:  3.896807909011841 0.08926209807395935
CurrentTrain: epoch  8, batch    49 | loss: 3.9860699Losses:  3.954359769821167 0.04375557601451874
CurrentTrain: epoch  8, batch    50 | loss: 3.9981153Losses:  3.956857204437256 0.05231734737753868
CurrentTrain: epoch  8, batch    51 | loss: 4.0091743Losses:  3.9230172634124756 0.026310814544558525
CurrentTrain: epoch  8, batch    52 | loss: 3.9493282Losses:  3.943340301513672 0.06511059403419495
CurrentTrain: epoch  8, batch    53 | loss: 4.0084510Losses:  4.033560752868652 0.05500161647796631
CurrentTrain: epoch  8, batch    54 | loss: 4.0885625Losses:  3.916749954223633 0.06813749670982361
CurrentTrain: epoch  8, batch    55 | loss: 3.9848874Losses:  3.984652280807495 0.057109810411930084
CurrentTrain: epoch  8, batch    56 | loss: 4.0417619Losses:  3.9179623126983643 0.08384489268064499
CurrentTrain: epoch  8, batch    57 | loss: 4.0018072Losses:  3.9704318046569824 0.053197845816612244
CurrentTrain: epoch  8, batch    58 | loss: 4.0236297Losses:  3.9513278007507324 0.05143655464053154
CurrentTrain: epoch  8, batch    59 | loss: 4.0027642Losses:  3.92852783203125 0.038939762860536575
CurrentTrain: epoch  8, batch    60 | loss: 3.9674675Losses:  3.9171972274780273 0.04522518068552017
CurrentTrain: epoch  8, batch    61 | loss: 3.9624224Losses:  3.949965000152588 0.02336692437529564
CurrentTrain: epoch  8, batch    62 | loss: 3.9733319Losses:  3.9746532440185547 0.06070239841938019
CurrentTrain: epoch  8, batch    63 | loss: 4.0353556Losses:  3.8910105228424072 0.0376850888133049
CurrentTrain: epoch  8, batch    64 | loss: 3.9286957Losses:  3.9826245307922363 0.0574452206492424
CurrentTrain: epoch  8, batch    65 | loss: 4.0400696Losses:  3.9384822845458984 0.05447541922330856
CurrentTrain: epoch  8, batch    66 | loss: 3.9929576Losses:  3.911090135574341 0.034897349774837494
CurrentTrain: epoch  8, batch    67 | loss: 3.9459875Losses:  3.964742660522461 0.050112344324588776
CurrentTrain: epoch  8, batch    68 | loss: 4.0148549Losses:  3.94752836227417 0.04188653826713562
CurrentTrain: epoch  8, batch    69 | loss: 3.9894149Losses:  3.9351208209991455 0.039718933403491974
CurrentTrain: epoch  8, batch    70 | loss: 3.9748397Losses:  3.9449338912963867 0.07720314711332321
CurrentTrain: epoch  8, batch    71 | loss: 4.0221372Losses:  3.9343981742858887 0.03817533701658249
CurrentTrain: epoch  8, batch    72 | loss: 3.9725735Losses:  3.912724494934082 0.031325969845056534
CurrentTrain: epoch  8, batch    73 | loss: 3.9440506Losses:  3.9894373416900635 0.06295426189899445
CurrentTrain: epoch  8, batch    74 | loss: 4.0523915Losses:  3.962541103363037 0.06903189420700073
CurrentTrain: epoch  8, batch    75 | loss: 4.0315728Losses:  3.9024782180786133 0.0756504237651825
CurrentTrain: epoch  8, batch    76 | loss: 3.9781287Losses:  3.8784444332122803 0.0607500858604908
CurrentTrain: epoch  8, batch    77 | loss: 3.9391944Losses:  3.966050863265991 0.08648532629013062
CurrentTrain: epoch  8, batch    78 | loss: 4.0525360Losses:  3.987682580947876 0.041840724647045135
CurrentTrain: epoch  8, batch    79 | loss: 4.0295234Losses:  3.9606902599334717 0.05586543679237366
CurrentTrain: epoch  8, batch    80 | loss: 4.0165558Losses:  3.9445018768310547 0.0477384552359581
CurrentTrain: epoch  8, batch    81 | loss: 3.9922404Losses:  3.9739980697631836 0.054396092891693115
CurrentTrain: epoch  8, batch    82 | loss: 4.0283942Losses:  3.921096086502075 0.03877008706331253
CurrentTrain: epoch  8, batch    83 | loss: 3.9598663Losses:  3.926616668701172 0.05635971575975418
CurrentTrain: epoch  8, batch    84 | loss: 3.9829764Losses:  3.9289684295654297 0.037769537419080734
CurrentTrain: epoch  8, batch    85 | loss: 3.9667380Losses:  3.938770294189453 0.04684828966856003
CurrentTrain: epoch  8, batch    86 | loss: 3.9856186Losses:  3.9325430393218994 0.05933299660682678
CurrentTrain: epoch  8, batch    87 | loss: 3.9918761Losses:  3.934493064880371 0.042513102293014526
CurrentTrain: epoch  8, batch    88 | loss: 3.9770062Losses:  3.9579343795776367 0.06601503491401672
CurrentTrain: epoch  8, batch    89 | loss: 4.0239496Losses:  3.997286319732666 0.03986864164471626
CurrentTrain: epoch  8, batch    90 | loss: 4.0371552Losses:  4.014784812927246 0.012834000401198864
CurrentTrain: epoch  8, batch    91 | loss: 4.0276189Losses:  3.9206573963165283 0.033984288573265076
CurrentTrain: epoch  8, batch    92 | loss: 3.9546416Losses:  3.883172035217285 0.04654189944267273
CurrentTrain: epoch  8, batch    93 | loss: 3.9297140Losses:  3.896292209625244 0.04861357808113098
CurrentTrain: epoch  8, batch    94 | loss: 3.9449058Losses:  3.91084885597229 0.07194997370243073
CurrentTrain: epoch  8, batch    95 | loss: 3.9827988Losses:  3.889594316482544 0.04676252603530884
CurrentTrain: epoch  8, batch    96 | loss: 3.9363568Losses:  3.8928661346435547 0.028406888246536255
CurrentTrain: epoch  8, batch    97 | loss: 3.9212730Losses:  3.9295310974121094 0.05179804563522339
CurrentTrain: epoch  8, batch    98 | loss: 3.9813292Losses:  3.9447336196899414 0.05273754149675369
CurrentTrain: epoch  8, batch    99 | loss: 3.9974711Losses:  3.902207851409912 0.044238537549972534
CurrentTrain: epoch  8, batch   100 | loss: 3.9464464Losses:  3.9198126792907715 0.04369518160820007
CurrentTrain: epoch  8, batch   101 | loss: 3.9635079Losses:  3.9286797046661377 0.0506468191742897
CurrentTrain: epoch  8, batch   102 | loss: 3.9793265Losses:  3.864976406097412 0.020917948335409164
CurrentTrain: epoch  8, batch   103 | loss: 3.8858943Losses:  3.949916362762451 0.09080155193805695
CurrentTrain: epoch  8, batch   104 | loss: 4.0407181Losses:  3.898439407348633 0.050782617181539536
CurrentTrain: epoch  8, batch   105 | loss: 3.9492221Losses:  3.9417366981506348 0.05655435845255852
CurrentTrain: epoch  8, batch   106 | loss: 3.9982910Losses:  3.9838757514953613 0.04192157834768295
CurrentTrain: epoch  8, batch   107 | loss: 4.0257974Losses:  3.93261456489563 0.07079112529754639
CurrentTrain: epoch  8, batch   108 | loss: 4.0034056Losses:  3.9288854598999023 0.03952670097351074
CurrentTrain: epoch  8, batch   109 | loss: 3.9684122Losses:  3.9358627796173096 0.0565926618874073
CurrentTrain: epoch  8, batch   110 | loss: 3.9924555Losses:  3.9917831420898438 0.030098428949713707
CurrentTrain: epoch  8, batch   111 | loss: 4.0218816Losses:  3.940704345703125 0.05502494424581528
CurrentTrain: epoch  8, batch   112 | loss: 3.9957292Losses:  3.9888761043548584 0.03225080668926239
CurrentTrain: epoch  8, batch   113 | loss: 4.0211267Losses:  3.9791910648345947 0.02848244085907936
CurrentTrain: epoch  8, batch   114 | loss: 4.0076737Losses:  3.9307193756103516 0.06749512255191803
CurrentTrain: epoch  8, batch   115 | loss: 3.9982145Losses:  3.962301015853882 0.04647470638155937
CurrentTrain: epoch  8, batch   116 | loss: 4.0087757Losses:  3.9403820037841797 0.03854670375585556
CurrentTrain: epoch  8, batch   117 | loss: 3.9789288Losses:  3.931023597717285 0.04673708602786064
CurrentTrain: epoch  8, batch   118 | loss: 3.9777608Losses:  3.929823875427246 0.05231570452451706
CurrentTrain: epoch  8, batch   119 | loss: 3.9821396Losses:  3.969119071960449 0.05061781033873558
CurrentTrain: epoch  8, batch   120 | loss: 4.0197368Losses:  3.979325771331787 0.07633984088897705
CurrentTrain: epoch  8, batch   121 | loss: 4.0556655Losses:  3.892329692840576 0.05224015563726425
CurrentTrain: epoch  8, batch   122 | loss: 3.9445698Losses:  3.9487478733062744 0.03897541016340256
CurrentTrain: epoch  8, batch   123 | loss: 3.9877234Losses:  3.9304285049438477 0.023796770721673965
CurrentTrain: epoch  8, batch   124 | loss: 3.9542253Losses:  3.9394326210021973 0.02960282564163208
CurrentTrain: epoch  9, batch     0 | loss: 3.9690354Losses:  3.906475305557251 0.0430929996073246
CurrentTrain: epoch  9, batch     1 | loss: 3.9495683Losses:  3.9572763442993164 0.06418925523757935
CurrentTrain: epoch  9, batch     2 | loss: 4.0214658Losses:  3.934913158416748 0.024557869881391525
CurrentTrain: epoch  9, batch     3 | loss: 3.9594710Losses:  3.96103572845459 0.048428334295749664
CurrentTrain: epoch  9, batch     4 | loss: 4.0094643Losses:  3.9200758934020996 0.04230121895670891
CurrentTrain: epoch  9, batch     5 | loss: 3.9623771Losses:  3.9328765869140625 0.05449395626783371
CurrentTrain: epoch  9, batch     6 | loss: 3.9873705Losses:  3.952310562133789 0.0425260066986084
CurrentTrain: epoch  9, batch     7 | loss: 3.9948366Losses:  3.9014153480529785 0.041290283203125
CurrentTrain: epoch  9, batch     8 | loss: 3.9427056Losses:  3.964797019958496 0.04086299240589142
CurrentTrain: epoch  9, batch     9 | loss: 4.0056601Losses:  3.9831461906433105 0.029111016541719437
CurrentTrain: epoch  9, batch    10 | loss: 4.0122571Losses:  3.9566338062286377 0.05246858298778534
CurrentTrain: epoch  9, batch    11 | loss: 4.0091023Losses:  3.9567103385925293 0.039761707186698914
CurrentTrain: epoch  9, batch    12 | loss: 3.9964721Losses:  3.9190235137939453 0.058149922639131546
CurrentTrain: epoch  9, batch    13 | loss: 3.9771733Losses:  3.956321954727173 0.04392951726913452
CurrentTrain: epoch  9, batch    14 | loss: 4.0002513Losses:  3.951963424682617 0.04150910675525665
CurrentTrain: epoch  9, batch    15 | loss: 3.9934726Losses:  3.844165802001953 0.03219358250498772
CurrentTrain: epoch  9, batch    16 | loss: 3.8763595Losses:  3.9423036575317383 0.04567021131515503
CurrentTrain: epoch  9, batch    17 | loss: 3.9879739Losses:  3.9418067932128906 0.058145102113485336
CurrentTrain: epoch  9, batch    18 | loss: 3.9999518Losses:  3.9340052604675293 0.07561634480953217
CurrentTrain: epoch  9, batch    19 | loss: 4.0096216Losses:  3.9138388633728027 0.04278777912259102
CurrentTrain: epoch  9, batch    20 | loss: 3.9566267Losses:  3.9291203022003174 0.06286096572875977
CurrentTrain: epoch  9, batch    21 | loss: 3.9919813Losses:  3.941739320755005 0.045271191745996475
CurrentTrain: epoch  9, batch    22 | loss: 3.9870105Losses:  3.97902774810791 0.07116466760635376
CurrentTrain: epoch  9, batch    23 | loss: 4.0501924Losses:  3.8789124488830566 0.04252920299768448
CurrentTrain: epoch  9, batch    24 | loss: 3.9214416Losses:  3.9212121963500977 0.06319134682416916
CurrentTrain: epoch  9, batch    25 | loss: 3.9844036Losses:  3.923884868621826 0.03337099775671959
CurrentTrain: epoch  9, batch    26 | loss: 3.9572558Losses:  3.9390270709991455 0.05026566982269287
CurrentTrain: epoch  9, batch    27 | loss: 3.9892926Losses:  3.920423984527588 0.06105594336986542
CurrentTrain: epoch  9, batch    28 | loss: 3.9814799Losses:  3.912571907043457 0.06206486374139786
CurrentTrain: epoch  9, batch    29 | loss: 3.9746368Losses:  3.9502062797546387 0.05361908674240112
CurrentTrain: epoch  9, batch    30 | loss: 4.0038252Losses:  3.934459686279297 0.03158292919397354
CurrentTrain: epoch  9, batch    31 | loss: 3.9660425Losses:  3.9191746711730957 0.06900277733802795
CurrentTrain: epoch  9, batch    32 | loss: 3.9881775Losses:  3.9563968181610107 0.038031160831451416
CurrentTrain: epoch  9, batch    33 | loss: 3.9944279Losses:  3.9837374687194824 0.04001442342996597
CurrentTrain: epoch  9, batch    34 | loss: 4.0237517Losses:  3.941713333129883 0.050290629267692566
CurrentTrain: epoch  9, batch    35 | loss: 3.9920039Losses:  3.9247641563415527 0.04028423875570297
CurrentTrain: epoch  9, batch    36 | loss: 3.9650483Losses:  3.9807286262512207 0.023724669590592384
CurrentTrain: epoch  9, batch    37 | loss: 4.0044532Losses:  3.9106578826904297 0.02612881548702717
CurrentTrain: epoch  9, batch    38 | loss: 3.9367867Losses:  3.8552374839782715 0.02292298525571823
CurrentTrain: epoch  9, batch    39 | loss: 3.8781605Losses:  4.005179405212402 0.026692217215895653
CurrentTrain: epoch  9, batch    40 | loss: 4.0318718Losses:  3.916412830352783 0.05016346648335457
CurrentTrain: epoch  9, batch    41 | loss: 3.9665763Losses:  3.920701742172241 0.043508294969797134
CurrentTrain: epoch  9, batch    42 | loss: 3.9642100Losses:  3.872222900390625 0.06014636531472206
CurrentTrain: epoch  9, batch    43 | loss: 3.9323692Losses:  3.923790454864502 0.03758661076426506
CurrentTrain: epoch  9, batch    44 | loss: 3.9613771Losses:  3.985405921936035 0.05129644647240639
CurrentTrain: epoch  9, batch    45 | loss: 4.0367022Losses:  3.89683198928833 0.04786648601293564
CurrentTrain: epoch  9, batch    46 | loss: 3.9446986Losses:  3.8998031616210938 0.0377877913415432
CurrentTrain: epoch  9, batch    47 | loss: 3.9375908Losses:  3.888101577758789 0.019548483192920685
CurrentTrain: epoch  9, batch    48 | loss: 3.9076500Losses:  3.953533172607422 0.049216873943805695
CurrentTrain: epoch  9, batch    49 | loss: 4.0027499Losses:  3.988882064819336 0.0336940661072731
CurrentTrain: epoch  9, batch    50 | loss: 4.0225763Losses:  3.9651589393615723 0.040827348828315735
CurrentTrain: epoch  9, batch    51 | loss: 4.0059862Losses:  3.9503026008605957 0.049778081476688385
CurrentTrain: epoch  9, batch    52 | loss: 4.0000806Losses:  3.9441564083099365 0.053252287209033966
CurrentTrain: epoch  9, batch    53 | loss: 3.9974086Losses:  3.9273319244384766 0.03434920683503151
CurrentTrain: epoch  9, batch    54 | loss: 3.9616811Losses:  3.9756312370300293 0.05242351442575455
CurrentTrain: epoch  9, batch    55 | loss: 4.0280547Losses:  3.9309091567993164 0.058224890381097794
CurrentTrain: epoch  9, batch    56 | loss: 3.9891341Losses:  3.9323501586914062 0.050545405596494675
CurrentTrain: epoch  9, batch    57 | loss: 3.9828956Losses:  3.9465999603271484 0.0588018000125885
CurrentTrain: epoch  9, batch    58 | loss: 4.0054016Losses:  3.914301872253418 0.039660319685935974
CurrentTrain: epoch  9, batch    59 | loss: 3.9539621Losses:  3.887402057647705 0.04322682321071625
CurrentTrain: epoch  9, batch    60 | loss: 3.9306288Losses:  3.9603915214538574 0.05147038400173187
CurrentTrain: epoch  9, batch    61 | loss: 4.0118618Losses:  3.950376033782959 0.05798408016562462
CurrentTrain: epoch  9, batch    62 | loss: 4.0083599Losses:  3.901669502258301 0.03899727761745453
CurrentTrain: epoch  9, batch    63 | loss: 3.9406667Losses:  3.897592067718506 0.051185786724090576
CurrentTrain: epoch  9, batch    64 | loss: 3.9487779Losses:  3.9451427459716797 0.035592321306467056
CurrentTrain: epoch  9, batch    65 | loss: 3.9807351Losses:  3.900430202484131 0.02009819820523262
CurrentTrain: epoch  9, batch    66 | loss: 3.9205284Losses:  3.9365711212158203 0.031763169914484024
CurrentTrain: epoch  9, batch    67 | loss: 3.9683342Losses:  3.9393410682678223 0.03454308956861496
CurrentTrain: epoch  9, batch    68 | loss: 3.9738841Losses:  3.9011409282684326 0.04357311502099037
CurrentTrain: epoch  9, batch    69 | loss: 3.9447141Losses:  3.9562392234802246 0.06465312838554382
CurrentTrain: epoch  9, batch    70 | loss: 4.0208921Losses:  3.9130046367645264 0.05589991807937622
CurrentTrain: epoch  9, batch    71 | loss: 3.9689045Losses:  3.9214017391204834 0.03730905055999756
CurrentTrain: epoch  9, batch    72 | loss: 3.9587107Losses:  3.9287824630737305 0.030065564438700676
CurrentTrain: epoch  9, batch    73 | loss: 3.9588480Losses:  3.9148221015930176 0.040120720863342285
CurrentTrain: epoch  9, batch    74 | loss: 3.9549427Losses:  3.926316499710083 0.024559002369642258
CurrentTrain: epoch  9, batch    75 | loss: 3.9508755Losses:  3.881751537322998 0.027677979320287704
CurrentTrain: epoch  9, batch    76 | loss: 3.9094296Losses:  3.9304628372192383 0.04252253472805023
CurrentTrain: epoch  9, batch    77 | loss: 3.9729853Losses:  3.9774422645568848 0.03931539133191109
CurrentTrain: epoch  9, batch    78 | loss: 4.0167575Losses:  3.961803436279297 0.05037930607795715
CurrentTrain: epoch  9, batch    79 | loss: 4.0121827Losses:  3.9543585777282715 0.0526173859834671
CurrentTrain: epoch  9, batch    80 | loss: 4.0069761Losses:  3.944401264190674 0.03297506272792816
CurrentTrain: epoch  9, batch    81 | loss: 3.9773762Losses:  3.973996162414551 0.03152374178171158
CurrentTrain: epoch  9, batch    82 | loss: 4.0055199Losses:  3.9794678688049316 0.02532261796295643
CurrentTrain: epoch  9, batch    83 | loss: 4.0047903Losses:  3.9349281787872314 0.032683201134204865
CurrentTrain: epoch  9, batch    84 | loss: 3.9676113Losses:  3.892882823944092 0.025107191875576973
CurrentTrain: epoch  9, batch    85 | loss: 3.9179900Losses:  3.9144256114959717 0.03073395974934101
CurrentTrain: epoch  9, batch    86 | loss: 3.9451597Losses:  3.9226865768432617 0.037696074694395065
CurrentTrain: epoch  9, batch    87 | loss: 3.9603827Losses:  3.927563190460205 0.030710680410265923
CurrentTrain: epoch  9, batch    88 | loss: 3.9582739Losses:  4.027108192443848 0.026036560535430908
CurrentTrain: epoch  9, batch    89 | loss: 4.0531449Losses:  3.9057841300964355 0.07179579138755798
CurrentTrain: epoch  9, batch    90 | loss: 3.9775798Losses:  3.922295570373535 0.04034136235713959
CurrentTrain: epoch  9, batch    91 | loss: 3.9626369Losses:  3.9504308700561523 0.03514159470796585
CurrentTrain: epoch  9, batch    92 | loss: 3.9855726Losses:  3.9580583572387695 0.033941902220249176
CurrentTrain: epoch  9, batch    93 | loss: 3.9920003Losses:  3.936878204345703 0.04829384759068489
CurrentTrain: epoch  9, batch    94 | loss: 3.9851720Losses:  3.9425208568573 0.04737678915262222
CurrentTrain: epoch  9, batch    95 | loss: 3.9898977Losses:  3.9424357414245605 0.04205530136823654
CurrentTrain: epoch  9, batch    96 | loss: 3.9844911Losses:  3.9303994178771973 0.03556281700730324
CurrentTrain: epoch  9, batch    97 | loss: 3.9659622Losses:  3.9576759338378906 0.04448394104838371
CurrentTrain: epoch  9, batch    98 | loss: 4.0021601Losses:  3.952052593231201 0.0645146518945694
CurrentTrain: epoch  9, batch    99 | loss: 4.0165672Losses:  3.9051265716552734 0.04256010055541992
CurrentTrain: epoch  9, batch   100 | loss: 3.9476867Losses:  3.9270453453063965 0.06854739785194397
CurrentTrain: epoch  9, batch   101 | loss: 3.9955928Losses:  3.9132168292999268 0.048556774854660034
CurrentTrain: epoch  9, batch   102 | loss: 3.9617736Losses:  3.960144281387329 0.05739675462245941
CurrentTrain: epoch  9, batch   103 | loss: 4.0175409Losses:  3.9312567710876465 0.03595830500125885
CurrentTrain: epoch  9, batch   104 | loss: 3.9672151Losses:  3.933332920074463 0.02985556796193123
CurrentTrain: epoch  9, batch   105 | loss: 3.9631884Losses:  3.983084201812744 0.04653332382440567
CurrentTrain: epoch  9, batch   106 | loss: 4.0296173Losses:  3.9057159423828125 0.042181093245744705
CurrentTrain: epoch  9, batch   107 | loss: 3.9478970Losses:  3.940605640411377 0.05038518086075783
CurrentTrain: epoch  9, batch   108 | loss: 3.9909909Losses:  3.9705042839050293 0.04480740427970886
CurrentTrain: epoch  9, batch   109 | loss: 4.0153117Losses:  3.9279582500457764 0.043975796550512314
CurrentTrain: epoch  9, batch   110 | loss: 3.9719341Losses:  3.9649853706359863 0.029875177890062332
CurrentTrain: epoch  9, batch   111 | loss: 3.9948606Losses:  3.925628662109375 0.05933501943945885
CurrentTrain: epoch  9, batch   112 | loss: 3.9849637Losses:  3.9399614334106445 0.06000687554478645
CurrentTrain: epoch  9, batch   113 | loss: 3.9999683Losses:  3.932948589324951 0.05293963849544525
CurrentTrain: epoch  9, batch   114 | loss: 3.9858882Losses:  3.8772737979888916 0.06665560603141785
CurrentTrain: epoch  9, batch   115 | loss: 3.9439294Losses:  3.9401662349700928 0.046216804534196854
CurrentTrain: epoch  9, batch   116 | loss: 3.9863830Losses:  3.9183409214019775 0.037907764315605164
CurrentTrain: epoch  9, batch   117 | loss: 3.9562488Losses:  3.9121785163879395 0.04225686192512512
CurrentTrain: epoch  9, batch   118 | loss: 3.9544353Losses:  3.907184362411499 0.06483864784240723
CurrentTrain: epoch  9, batch   119 | loss: 3.9720230Losses:  3.883173704147339 0.027687495574355125
CurrentTrain: epoch  9, batch   120 | loss: 3.9108613Losses:  3.94435453414917 0.05956900119781494
CurrentTrain: epoch  9, batch   121 | loss: 4.0039234Losses:  3.9238128662109375 0.05917967110872269
CurrentTrain: epoch  9, batch   122 | loss: 3.9829926Losses:  3.925708770751953 0.028788886964321136
CurrentTrain: epoch  9, batch   123 | loss: 3.9544976Losses:  3.9282546043395996 0.029347047209739685
CurrentTrain: epoch  9, batch   124 | loss: 3.9576015
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  8.054990768432617 1.3145602941513062
CurrentTrain: epoch  0, batch     0 | loss: 9.3695507Losses:  10.905017852783203 0.897864580154419
CurrentTrain: epoch  0, batch     1 | loss: 11.8028822Losses:  10.175861358642578 0.863926351070404
CurrentTrain: epoch  0, batch     2 | loss: 11.0397873Losses:  9.566357612609863 0.8376712799072266
CurrentTrain: epoch  0, batch     3 | loss: 10.4040289Losses:  7.270074844360352 0.6166232228279114
CurrentTrain: epoch  0, batch     4 | loss: 7.8866982Losses:  6.2846574783325195 1.140446662902832
CurrentTrain: epoch  0, batch     5 | loss: 7.4251041Losses:  8.408605575561523 1.4901162614933128e-07
CurrentTrain: epoch  0, batch     6 | loss: 8.4086056Losses:  4.403536796569824 1.0153757333755493
CurrentTrain: epoch  1, batch     0 | loss: 5.4189124Losses:  3.5156636238098145 0.8846133947372437
CurrentTrain: epoch  1, batch     1 | loss: 4.4002771Losses:  3.0822463035583496 0.842727780342102
CurrentTrain: epoch  1, batch     2 | loss: 3.9249740Losses:  4.938899040222168 0.8670774102210999
CurrentTrain: epoch  1, batch     3 | loss: 5.8059764Losses:  3.800901412963867 0.8984692692756653
CurrentTrain: epoch  1, batch     4 | loss: 4.6993709Losses:  3.7403059005737305 0.7886010408401489
CurrentTrain: epoch  1, batch     5 | loss: 4.5289068Losses:  2.378133773803711 0.28637364506721497
CurrentTrain: epoch  1, batch     6 | loss: 2.6645074Losses:  3.5807125568389893 0.8166889548301697
CurrentTrain: epoch  2, batch     0 | loss: 4.3974013Losses:  3.3919239044189453 0.7055092453956604
CurrentTrain: epoch  2, batch     1 | loss: 4.0974331Losses:  2.898432731628418 0.6015700101852417
CurrentTrain: epoch  2, batch     2 | loss: 3.5000029Losses:  3.6124343872070312 0.6956652402877808
CurrentTrain: epoch  2, batch     3 | loss: 4.3080997Losses:  3.287367582321167 0.6172512769699097
CurrentTrain: epoch  2, batch     4 | loss: 3.9046187Losses:  2.7783780097961426 0.5919385552406311
CurrentTrain: epoch  2, batch     5 | loss: 3.3703165Losses:  4.844044208526611 0.3369220495223999
CurrentTrain: epoch  2, batch     6 | loss: 5.1809664Losses:  2.698870897293091 0.6918840408325195
CurrentTrain: epoch  3, batch     0 | loss: 3.3907549Losses:  3.822242259979248 0.8668567538261414
CurrentTrain: epoch  3, batch     1 | loss: 4.6890988Losses:  2.7452402114868164 0.7540209889411926
CurrentTrain: epoch  3, batch     2 | loss: 3.4992611Losses:  2.514979362487793 0.6214015483856201
CurrentTrain: epoch  3, batch     3 | loss: 3.1363809Losses:  2.456671714782715 0.8727817535400391
CurrentTrain: epoch  3, batch     4 | loss: 3.3294535Losses:  3.1576619148254395 0.802168071269989
CurrentTrain: epoch  3, batch     5 | loss: 3.9598300Losses:  2.1087095737457275 0.23848742246627808
CurrentTrain: epoch  3, batch     6 | loss: 2.3471971Losses:  2.328453540802002 0.6151851415634155
CurrentTrain: epoch  4, batch     0 | loss: 2.9436388Losses:  2.5685434341430664 0.38646286725997925
CurrentTrain: epoch  4, batch     1 | loss: 2.9550064Losses:  2.2502105236053467 0.4683980941772461
CurrentTrain: epoch  4, batch     2 | loss: 2.7186086Losses:  2.352731704711914 0.33795055747032166
CurrentTrain: epoch  4, batch     3 | loss: 2.6906822Losses:  3.0439743995666504 0.395207941532135
CurrentTrain: epoch  4, batch     4 | loss: 3.4391823Losses:  2.9151594638824463 0.44188597798347473
CurrentTrain: epoch  4, batch     5 | loss: 3.3570454Losses:  2.3790411949157715 0.02891666628420353
CurrentTrain: epoch  4, batch     6 | loss: 2.4079578Losses:  2.2972874641418457 0.39992329478263855
CurrentTrain: epoch  5, batch     0 | loss: 2.6972108Losses:  2.848242998123169 0.6433426141738892
CurrentTrain: epoch  5, batch     1 | loss: 3.4915857Losses:  2.1240861415863037 0.3161144256591797
CurrentTrain: epoch  5, batch     2 | loss: 2.4402006Losses:  2.1406826972961426 0.6268153190612793
CurrentTrain: epoch  5, batch     3 | loss: 2.7674980Losses:  2.5885379314422607 0.6202391386032104
CurrentTrain: epoch  5, batch     4 | loss: 3.2087770Losses:  2.336009979248047 0.6002748012542725
CurrentTrain: epoch  5, batch     5 | loss: 2.9362848Losses:  2.870769500732422 0.02770163118839264
CurrentTrain: epoch  5, batch     6 | loss: 2.8984711Losses:  2.1584529876708984 0.5691131949424744
CurrentTrain: epoch  6, batch     0 | loss: 2.7275662Losses:  2.2129135131835938 0.4109691381454468
CurrentTrain: epoch  6, batch     1 | loss: 2.6238828Losses:  2.1585988998413086 0.310352623462677
CurrentTrain: epoch  6, batch     2 | loss: 2.4689515Losses:  2.5445480346679688 0.38002222776412964
CurrentTrain: epoch  6, batch     3 | loss: 2.9245703Losses:  2.541733503341675 0.458940327167511
CurrentTrain: epoch  6, batch     4 | loss: 3.0006738Losses:  2.065706253051758 0.2586349844932556
CurrentTrain: epoch  6, batch     5 | loss: 2.3243413Losses:  2.083472490310669 8.94069742685133e-08
CurrentTrain: epoch  6, batch     6 | loss: 2.0834725Losses:  1.9938355684280396 0.393576055765152
CurrentTrain: epoch  7, batch     0 | loss: 2.3874116Losses:  2.139101028442383 0.3747718632221222
CurrentTrain: epoch  7, batch     1 | loss: 2.5138729Losses:  2.013472557067871 0.5582036972045898
CurrentTrain: epoch  7, batch     2 | loss: 2.5716763Losses:  2.1459603309631348 0.481071412563324
CurrentTrain: epoch  7, batch     3 | loss: 2.6270318Losses:  1.9438111782073975 0.31718116998672485
CurrentTrain: epoch  7, batch     4 | loss: 2.2609923Losses:  1.888507604598999 0.4071381688117981
CurrentTrain: epoch  7, batch     5 | loss: 2.2956457Losses:  1.8622684478759766 8.94069742685133e-08
CurrentTrain: epoch  7, batch     6 | loss: 1.8622686Losses:  1.947319746017456 0.4745679497718811
CurrentTrain: epoch  8, batch     0 | loss: 2.4218876Losses:  2.035259962081909 0.5143998861312866
CurrentTrain: epoch  8, batch     1 | loss: 2.5496597Losses:  1.8629777431488037 0.24883316457271576
CurrentTrain: epoch  8, batch     2 | loss: 2.1118109Losses:  1.925707221031189 0.5084227323532104
CurrentTrain: epoch  8, batch     3 | loss: 2.4341300Losses:  1.9565167427062988 0.4629836678504944
CurrentTrain: epoch  8, batch     4 | loss: 2.4195004Losses:  2.160980701446533 0.3880235552787781
CurrentTrain: epoch  8, batch     5 | loss: 2.5490043Losses:  1.8240753412246704 0.03418329358100891
CurrentTrain: epoch  8, batch     6 | loss: 1.8582586Losses:  1.8591341972351074 0.4142848253250122
CurrentTrain: epoch  9, batch     0 | loss: 2.2734189Losses:  1.8514540195465088 0.3559560179710388
CurrentTrain: epoch  9, batch     1 | loss: 2.2074101Losses:  1.962789535522461 0.3754640519618988
CurrentTrain: epoch  9, batch     2 | loss: 2.3382535Losses:  1.8141742944717407 0.2742869257926941
CurrentTrain: epoch  9, batch     3 | loss: 2.0884612Losses:  1.8483833074569702 0.44453269243240356
CurrentTrain: epoch  9, batch     4 | loss: 2.2929161Losses:  1.7475433349609375 0.29997605085372925
CurrentTrain: epoch  9, batch     5 | loss: 2.0475194Losses:  1.6709916591644287 0.030414622277021408
CurrentTrain: epoch  9, batch     6 | loss: 1.7014062
Losses:  6.198741912841797 0.5823356509208679
MemoryTrain:  epoch  0, batch     0 | loss: 6.7810774Losses:  7.722574234008789 0.5814529061317444
MemoryTrain:  epoch  0, batch     1 | loss: 8.3040276Losses:  10.50309944152832 0.05013661086559296
MemoryTrain:  epoch  0, batch     2 | loss: 10.5532360Losses:  1.6725815534591675 0.14459793269634247
MemoryTrain:  epoch  1, batch     0 | loss: 1.8171794Losses:  1.372093677520752 0.43490204215049744
MemoryTrain:  epoch  1, batch     1 | loss: 1.8069957Losses:  3.6631088256835938 0.5227547287940979
MemoryTrain:  epoch  1, batch     2 | loss: 4.1858635Losses:  2.072169780731201 0.4575992524623871
MemoryTrain:  epoch  2, batch     0 | loss: 2.5297689Losses:  1.4243392944335938 0.29150113463401794
MemoryTrain:  epoch  2, batch     1 | loss: 1.7158405Losses:  0.2730850577354431 0.24667173624038696
MemoryTrain:  epoch  2, batch     2 | loss: 0.5197568Losses:  0.4868811070919037 0.2513631284236908
MemoryTrain:  epoch  3, batch     0 | loss: 0.7382442Losses:  1.9222356081008911 0.6105336546897888
MemoryTrain:  epoch  3, batch     1 | loss: 2.5327692Losses:  0.6827804446220398 0.23864606022834778
MemoryTrain:  epoch  3, batch     2 | loss: 0.9214265Losses:  1.2850388288497925 0.5006730556488037
MemoryTrain:  epoch  4, batch     0 | loss: 1.7857119Losses:  0.42383071780204773 0.2097633183002472
MemoryTrain:  epoch  4, batch     1 | loss: 0.6335940Losses:  0.08597826957702637 0.1368076652288437
MemoryTrain:  epoch  4, batch     2 | loss: 0.2227859Losses:  0.32820552587509155 0.40532320737838745
MemoryTrain:  epoch  5, batch     0 | loss: 0.7335287Losses:  0.825011134147644 0.5281897783279419
MemoryTrain:  epoch  5, batch     1 | loss: 1.3532009Losses:  0.13964329659938812 0.035994552075862885
MemoryTrain:  epoch  5, batch     2 | loss: 0.1756378Losses:  0.31683915853500366 0.3564976453781128
MemoryTrain:  epoch  6, batch     0 | loss: 0.6733368Losses:  0.426410973072052 0.46901193261146545
MemoryTrain:  epoch  6, batch     1 | loss: 0.8954229Losses:  0.11440038681030273 0.128737673163414
MemoryTrain:  epoch  6, batch     2 | loss: 0.2431381Losses:  0.30867213010787964 0.3218518793582916
MemoryTrain:  epoch  7, batch     0 | loss: 0.6305240Losses:  0.1503623127937317 0.5320497751235962
MemoryTrain:  epoch  7, batch     1 | loss: 0.6824121Losses:  0.40848565101623535 0.1744426190853119
MemoryTrain:  epoch  7, batch     2 | loss: 0.5829283Losses:  0.23890787363052368 0.17843888700008392
MemoryTrain:  epoch  8, batch     0 | loss: 0.4173468Losses:  0.19403979182243347 0.49154046177864075
MemoryTrain:  epoch  8, batch     1 | loss: 0.6855803Losses:  0.12518659234046936 0.2572088837623596
MemoryTrain:  epoch  8, batch     2 | loss: 0.3823955Losses:  0.23248697817325592 0.20759177207946777
MemoryTrain:  epoch  9, batch     0 | loss: 0.4400787Losses:  0.22273600101470947 0.6021277904510498
MemoryTrain:  epoch  9, batch     1 | loss: 0.8248638Losses:  0.14711445569992065 0.07122679054737091
MemoryTrain:  epoch  9, batch     2 | loss: 0.2183412
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.33%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.31%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.97%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.84%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.84%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 93.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 93.26%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 93.02%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.87%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.72%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.42%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.44%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 91.94%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 91.82%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 91.40%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.28%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.02%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 90.84%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.66%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 90.35%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 90.08%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 89.72%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 89.23%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 88.68%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.22%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.89%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 87.31%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.99%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 86.01%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.72%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 85.13%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 84.58%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 84.32%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 83.94%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 83.33%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.80%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 82.27%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.76%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.47%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 81.31%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.28%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 82.38%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.75%   
cur_acc:  ['0.9494', '0.7133']
his_acc:  ['0.9494', '0.8275']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  7.407130718231201 1.2688325643539429
CurrentTrain: epoch  0, batch     0 | loss: 8.6759634Losses:  9.677984237670898 1.2085233926773071
CurrentTrain: epoch  0, batch     1 | loss: 10.8865080Losses:  9.176473617553711 1.2697879076004028
CurrentTrain: epoch  0, batch     2 | loss: 10.4462614Losses:  8.44675350189209 1.2471377849578857
CurrentTrain: epoch  0, batch     3 | loss: 9.6938915Losses:  7.519299507141113 1.088942527770996
CurrentTrain: epoch  0, batch     4 | loss: 8.6082420Losses:  6.300246238708496 1.4905203580856323
CurrentTrain: epoch  0, batch     5 | loss: 7.7907667Losses:  6.204627990722656 0.4636717438697815
CurrentTrain: epoch  0, batch     6 | loss: 6.6682997Losses:  3.5304603576660156 1.428184986114502
CurrentTrain: epoch  1, batch     0 | loss: 4.9586453Losses:  3.5868284702301025 1.0773558616638184
CurrentTrain: epoch  1, batch     1 | loss: 4.6641846Losses:  4.96331262588501 1.5441396236419678
CurrentTrain: epoch  1, batch     2 | loss: 6.5074520Losses:  3.9787673950195312 1.379704475402832
CurrentTrain: epoch  1, batch     3 | loss: 5.3584719Losses:  3.001070261001587 0.7700773477554321
CurrentTrain: epoch  1, batch     4 | loss: 3.7711477Losses:  3.9230170249938965 1.307068109512329
CurrentTrain: epoch  1, batch     5 | loss: 5.2300854Losses:  4.118555068969727 0.23428437113761902
CurrentTrain: epoch  1, batch     6 | loss: 4.3528395Losses:  2.946502685546875 0.6692009568214417
CurrentTrain: epoch  2, batch     0 | loss: 3.6157036Losses:  2.915818452835083 0.8134710788726807
CurrentTrain: epoch  2, batch     1 | loss: 3.7292895Losses:  3.3037967681884766 1.0019333362579346
CurrentTrain: epoch  2, batch     2 | loss: 4.3057299Losses:  3.7946882247924805 1.0388717651367188
CurrentTrain: epoch  2, batch     3 | loss: 4.8335600Losses:  3.506857395172119 1.383146047592163
CurrentTrain: epoch  2, batch     4 | loss: 4.8900032Losses:  4.7512946128845215 1.1185956001281738
CurrentTrain: epoch  2, batch     5 | loss: 5.8698902Losses:  2.2186508178710938 0.07714381068944931
CurrentTrain: epoch  2, batch     6 | loss: 2.2957947Losses:  3.32566499710083 0.9353549480438232
CurrentTrain: epoch  3, batch     0 | loss: 4.2610197Losses:  3.1973085403442383 0.8874121904373169
CurrentTrain: epoch  3, batch     1 | loss: 4.0847206Losses:  2.7270641326904297 0.8375455141067505
CurrentTrain: epoch  3, batch     2 | loss: 3.5646095Losses:  3.2196431159973145 1.0098369121551514
CurrentTrain: epoch  3, batch     3 | loss: 4.2294798Losses:  3.6005191802978516 1.1037732362747192
CurrentTrain: epoch  3, batch     4 | loss: 4.7042923Losses:  3.433375835418701 1.1597230434417725
CurrentTrain: epoch  3, batch     5 | loss: 4.5930986Losses:  3.834474563598633 0.1564708948135376
CurrentTrain: epoch  3, batch     6 | loss: 3.9909453Losses:  3.466601848602295 0.734360933303833
CurrentTrain: epoch  4, batch     0 | loss: 4.2009630Losses:  3.612330913543701 0.9956741333007812
CurrentTrain: epoch  4, batch     1 | loss: 4.6080050Losses:  2.8545217514038086 0.8343710899353027
CurrentTrain: epoch  4, batch     2 | loss: 3.6888928Losses:  3.236788034439087 1.0359141826629639
CurrentTrain: epoch  4, batch     3 | loss: 4.2727022Losses:  2.4554591178894043 0.6164097189903259
CurrentTrain: epoch  4, batch     4 | loss: 3.0718689Losses:  2.6133031845092773 0.7108550667762756
CurrentTrain: epoch  4, batch     5 | loss: 3.3241582Losses:  2.427563428878784 0.14222495257854462
CurrentTrain: epoch  4, batch     6 | loss: 2.5697885Losses:  3.2665305137634277 1.0714452266693115
CurrentTrain: epoch  5, batch     0 | loss: 4.3379755Losses:  3.1091771125793457 0.7396122217178345
CurrentTrain: epoch  5, batch     1 | loss: 3.8487892Losses:  2.5821940898895264 0.5249766111373901
CurrentTrain: epoch  5, batch     2 | loss: 3.1071706Losses:  2.5138442516326904 0.5377650260925293
CurrentTrain: epoch  5, batch     3 | loss: 3.0516093Losses:  2.4765865802764893 0.7400118112564087
CurrentTrain: epoch  5, batch     4 | loss: 3.2165985Losses:  2.567108154296875 0.5263561606407166
CurrentTrain: epoch  5, batch     5 | loss: 3.0934644Losses:  4.322206020355225 0.5404942035675049
CurrentTrain: epoch  5, batch     6 | loss: 4.8627005Losses:  2.871793270111084 0.9474098682403564
CurrentTrain: epoch  6, batch     0 | loss: 3.8192031Losses:  2.880948543548584 0.7716729044914246
CurrentTrain: epoch  6, batch     1 | loss: 3.6526215Losses:  2.663980484008789 0.8033937215805054
CurrentTrain: epoch  6, batch     2 | loss: 3.4673743Losses:  2.8870644569396973 0.4866499602794647
CurrentTrain: epoch  6, batch     3 | loss: 3.3737144Losses:  2.3403615951538086 0.4176849126815796
CurrentTrain: epoch  6, batch     4 | loss: 2.7580466Losses:  2.3156533241271973 0.5182317495346069
CurrentTrain: epoch  6, batch     5 | loss: 2.8338852Losses:  2.183328628540039 0.11578956246376038
CurrentTrain: epoch  6, batch     6 | loss: 2.2991183Losses:  2.1992931365966797 0.4458012580871582
CurrentTrain: epoch  7, batch     0 | loss: 2.6450944Losses:  2.4572296142578125 0.6918575167655945
CurrentTrain: epoch  7, batch     1 | loss: 3.1490872Losses:  2.5906565189361572 0.4657859206199646
CurrentTrain: epoch  7, batch     2 | loss: 3.0564425Losses:  2.3873789310455322 0.5959240198135376
CurrentTrain: epoch  7, batch     3 | loss: 2.9833031Losses:  2.514613628387451 0.663745641708374
CurrentTrain: epoch  7, batch     4 | loss: 3.1783593Losses:  2.432234287261963 0.4916628301143646
CurrentTrain: epoch  7, batch     5 | loss: 2.9238970Losses:  3.2430660724639893 0.39136677980422974
CurrentTrain: epoch  7, batch     6 | loss: 3.6344328Losses:  2.252544403076172 0.7044191360473633
CurrentTrain: epoch  8, batch     0 | loss: 2.9569635Losses:  2.4668877124786377 0.5889398455619812
CurrentTrain: epoch  8, batch     1 | loss: 3.0558276Losses:  2.2378416061401367 0.5439668893814087
CurrentTrain: epoch  8, batch     2 | loss: 2.7818084Losses:  2.436770439147949 0.5797911882400513
CurrentTrain: epoch  8, batch     3 | loss: 3.0165615Losses:  2.0614237785339355 0.343063622713089
CurrentTrain: epoch  8, batch     4 | loss: 2.4044874Losses:  2.5079405307769775 0.4675027132034302
CurrentTrain: epoch  8, batch     5 | loss: 2.9754434Losses:  1.9846620559692383 0.0466841496527195
CurrentTrain: epoch  8, batch     6 | loss: 2.0313463Losses:  2.1340854167938232 0.5349457263946533
CurrentTrain: epoch  9, batch     0 | loss: 2.6690311Losses:  2.127892255783081 0.4445743262767792
CurrentTrain: epoch  9, batch     1 | loss: 2.5724666Losses:  1.8896241188049316 0.37879616022109985
CurrentTrain: epoch  9, batch     2 | loss: 2.2684202Losses:  2.1251838207244873 0.3949720859527588
CurrentTrain: epoch  9, batch     3 | loss: 2.5201559Losses:  2.2952589988708496 0.23231706023216248
CurrentTrain: epoch  9, batch     4 | loss: 2.5275760Losses:  2.338535785675049 0.3060663342475891
CurrentTrain: epoch  9, batch     5 | loss: 2.6446021Losses:  2.9472179412841797 0.42572441697120667
CurrentTrain: epoch  9, batch     6 | loss: 3.3729424
Losses:  5.68278694152832 0.3572232127189636
MemoryTrain:  epoch  0, batch     0 | loss: 6.0400100Losses:  9.161431312561035 0.4587287902832031
MemoryTrain:  epoch  0, batch     1 | loss: 9.6201601Losses:  9.923623085021973 0.7871496677398682
MemoryTrain:  epoch  0, batch     2 | loss: 10.7107725Losses:  9.54024887084961 0.20834115147590637
MemoryTrain:  epoch  0, batch     3 | loss: 9.7485905Losses:  0.8629595041275024 0.3527180552482605
MemoryTrain:  epoch  1, batch     0 | loss: 1.2156775Losses:  0.6880390644073486 0.6021248698234558
MemoryTrain:  epoch  1, batch     1 | loss: 1.2901640Losses:  0.8320865631103516 0.34914934635162354
MemoryTrain:  epoch  1, batch     2 | loss: 1.1812359Losses:  0.6368206143379211 0.5355249643325806
MemoryTrain:  epoch  1, batch     3 | loss: 1.1723456Losses:  0.3796190321445465 0.32659509778022766
MemoryTrain:  epoch  2, batch     0 | loss: 0.7062141Losses:  0.5462900400161743 0.4550565183162689
MemoryTrain:  epoch  2, batch     1 | loss: 1.0013466Losses:  0.6290181875228882 0.40486472845077515
MemoryTrain:  epoch  2, batch     2 | loss: 1.0338829Losses:  0.4151232838630676 0.2957741916179657
MemoryTrain:  epoch  2, batch     3 | loss: 0.7108974Losses:  0.3549848794937134 0.3729490041732788
MemoryTrain:  epoch  3, batch     0 | loss: 0.7279339Losses:  0.6200003027915955 0.6367315053939819
MemoryTrain:  epoch  3, batch     1 | loss: 1.2567317Losses:  0.32187068462371826 0.4968079924583435
MemoryTrain:  epoch  3, batch     2 | loss: 0.8186787Losses:  0.19362443685531616 0.2582288980484009
MemoryTrain:  epoch  3, batch     3 | loss: 0.4518533Losses:  0.28358954191207886 0.42500829696655273
MemoryTrain:  epoch  4, batch     0 | loss: 0.7085978Losses:  0.5218477845191956 0.5620452761650085
MemoryTrain:  epoch  4, batch     1 | loss: 1.0838931Losses:  0.2771978974342346 0.46545839309692383
MemoryTrain:  epoch  4, batch     2 | loss: 0.7426563Losses:  0.121506467461586 0.18971335887908936
MemoryTrain:  epoch  4, batch     3 | loss: 0.3112198Losses:  0.20478852093219757 0.46263301372528076
MemoryTrain:  epoch  5, batch     0 | loss: 0.6674215Losses:  0.30305901169776917 0.360294908285141
MemoryTrain:  epoch  5, batch     1 | loss: 0.6633539Losses:  0.27202481031417847 0.29619139432907104
MemoryTrain:  epoch  5, batch     2 | loss: 0.5682162Losses:  0.2761206328868866 0.37815889716148376
MemoryTrain:  epoch  5, batch     3 | loss: 0.6542795Losses:  0.2296181619167328 0.40948551893234253
MemoryTrain:  epoch  6, batch     0 | loss: 0.6391037Losses:  0.2472819685935974 0.3173328638076782
MemoryTrain:  epoch  6, batch     1 | loss: 0.5646148Losses:  0.24358659982681274 0.35767537355422974
MemoryTrain:  epoch  6, batch     2 | loss: 0.6012620Losses:  0.181521475315094 0.38403892517089844
MemoryTrain:  epoch  6, batch     3 | loss: 0.5655604Losses:  0.20709702372550964 0.2797781229019165
MemoryTrain:  epoch  7, batch     0 | loss: 0.4868751Losses:  0.2342875599861145 0.30368757247924805
MemoryTrain:  epoch  7, batch     1 | loss: 0.5379751Losses:  0.23090070486068726 0.5272039175033569
MemoryTrain:  epoch  7, batch     2 | loss: 0.7581046Losses:  0.1991821825504303 0.22252023220062256
MemoryTrain:  epoch  7, batch     3 | loss: 0.4217024Losses:  0.19886906445026398 0.31425440311431885
MemoryTrain:  epoch  8, batch     0 | loss: 0.5131235Losses:  0.2090342938899994 0.5876699686050415
MemoryTrain:  epoch  8, batch     1 | loss: 0.7967043Losses:  0.21405324339866638 0.2912253439426422
MemoryTrain:  epoch  8, batch     2 | loss: 0.5052786Losses:  0.11532960832118988 0.18721650540828705
MemoryTrain:  epoch  8, batch     3 | loss: 0.3025461Losses:  0.17957542836666107 0.3014058470726013
MemoryTrain:  epoch  9, batch     0 | loss: 0.4809813Losses:  0.23071610927581787 0.48773622512817383
MemoryTrain:  epoch  9, batch     1 | loss: 0.7184523Losses:  0.17206019163131714 0.24256275594234467
MemoryTrain:  epoch  9, batch     2 | loss: 0.4146230Losses:  0.19266997277736664 0.17259907722473145
MemoryTrain:  epoch  9, batch     3 | loss: 0.3652691
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 63.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.21%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.26%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.21%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 92.47%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 92.23%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 92.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.94%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 91.80%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 91.53%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 91.33%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.36%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 91.23%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 90.66%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 90.40%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 90.07%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.97%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 89.66%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 89.56%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 89.33%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 89.05%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 88.70%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 88.16%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 87.70%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.93%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 86.68%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.19%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 85.83%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.54%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 85.13%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 84.64%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.00%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.33%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.74%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 82.05%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 81.42%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.97%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.70%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.46%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.16%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.25%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 81.99%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 81.84%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 81.73%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 81.63%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.04%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 82.13%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.83%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 81.47%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 81.07%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 80.81%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 80.59%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 80.25%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.52%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 80.59%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 80.06%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 79.58%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 79.22%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 78.71%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 78.21%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 78.14%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.07%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.42%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 79.24%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.29%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 79.24%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 79.14%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 79.09%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 79.13%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 79.14%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 79.16%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.20%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 79.32%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 79.37%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 79.22%   
cur_acc:  ['0.9494', '0.7133', '0.7321']
his_acc:  ['0.9494', '0.8275', '0.7922']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  7.5160322189331055 1.406774640083313
CurrentTrain: epoch  0, batch     0 | loss: 8.9228067Losses:  8.955997467041016 1.1514966487884521
CurrentTrain: epoch  0, batch     1 | loss: 10.1074944Losses:  7.544687271118164 1.1716768741607666
CurrentTrain: epoch  0, batch     2 | loss: 8.7163639Losses:  7.489492893218994 0.9623813033103943
CurrentTrain: epoch  0, batch     3 | loss: 8.4518738Losses:  6.714254379272461 1.403620958328247
CurrentTrain: epoch  0, batch     4 | loss: 8.1178751Losses:  7.5798211097717285 1.1371746063232422
CurrentTrain: epoch  0, batch     5 | loss: 8.7169952Losses:  3.720960855484009 0.48308056592941284
CurrentTrain: epoch  0, batch     6 | loss: 4.2040415Losses:  2.8790106773376465 1.2297333478927612
CurrentTrain: epoch  1, batch     0 | loss: 4.1087441Losses:  3.4675142765045166 0.8715150356292725
CurrentTrain: epoch  1, batch     1 | loss: 4.3390293Losses:  2.547605514526367 1.0112844705581665
CurrentTrain: epoch  1, batch     2 | loss: 3.5588899Losses:  2.7883028984069824 0.959456205368042
CurrentTrain: epoch  1, batch     3 | loss: 3.7477591Losses:  2.910172462463379 1.3772350549697876
CurrentTrain: epoch  1, batch     4 | loss: 4.2874074Losses:  2.5089659690856934 1.127745509147644
CurrentTrain: epoch  1, batch     5 | loss: 3.6367116Losses:  2.4070982933044434 0.15457689762115479
CurrentTrain: epoch  1, batch     6 | loss: 2.5616751Losses:  3.256180763244629 1.1775907278060913
CurrentTrain: epoch  2, batch     0 | loss: 4.4337716Losses:  2.8979644775390625 0.8656136393547058
CurrentTrain: epoch  2, batch     1 | loss: 3.7635782Losses:  2.45888614654541 0.8962036371231079
CurrentTrain: epoch  2, batch     2 | loss: 3.3550897Losses:  2.180917739868164 0.8495701551437378
CurrentTrain: epoch  2, batch     3 | loss: 3.0304880Losses:  2.083987236022949 0.7315353155136108
CurrentTrain: epoch  2, batch     4 | loss: 2.8155227Losses:  2.0547842979431152 0.5957781076431274
CurrentTrain: epoch  2, batch     5 | loss: 2.6505623Losses:  1.9396276473999023 0.1563986986875534
CurrentTrain: epoch  2, batch     6 | loss: 2.0960264Losses:  2.368910789489746 0.7623470425605774
CurrentTrain: epoch  3, batch     0 | loss: 3.1312578Losses:  2.344942569732666 0.5823093056678772
CurrentTrain: epoch  3, batch     1 | loss: 2.9272518Losses:  2.042102336883545 0.6206309795379639
CurrentTrain: epoch  3, batch     2 | loss: 2.6627333Losses:  2.574488639831543 0.7403551340103149
CurrentTrain: epoch  3, batch     3 | loss: 3.3148437Losses:  2.145745038986206 0.7159650325775146
CurrentTrain: epoch  3, batch     4 | loss: 2.8617101Losses:  2.5857155323028564 0.6673235893249512
CurrentTrain: epoch  3, batch     5 | loss: 3.2530391Losses:  1.7035983800888062 0.10845161974430084
CurrentTrain: epoch  3, batch     6 | loss: 1.8120500Losses:  2.024202346801758 0.5370401740074158
CurrentTrain: epoch  4, batch     0 | loss: 2.5612426Losses:  2.090057134628296 0.5225145220756531
CurrentTrain: epoch  4, batch     1 | loss: 2.6125717Losses:  1.8433960676193237 0.6250859498977661
CurrentTrain: epoch  4, batch     2 | loss: 2.4684820Losses:  1.941896915435791 0.5625463128089905
CurrentTrain: epoch  4, batch     3 | loss: 2.5044432Losses:  2.297992706298828 0.5261550545692444
CurrentTrain: epoch  4, batch     4 | loss: 2.8241477Losses:  2.738784074783325 0.7244776487350464
CurrentTrain: epoch  4, batch     5 | loss: 3.4632616Losses:  1.9302692413330078 0.07397748529911041
CurrentTrain: epoch  4, batch     6 | loss: 2.0042467Losses:  2.1419291496276855 0.3598451316356659
CurrentTrain: epoch  5, batch     0 | loss: 2.5017743Losses:  2.2178292274475098 0.6459437012672424
CurrentTrain: epoch  5, batch     1 | loss: 2.8637729Losses:  2.3561995029449463 0.5879732966423035
CurrentTrain: epoch  5, batch     2 | loss: 2.9441729Losses:  1.7794780731201172 0.37654203176498413
CurrentTrain: epoch  5, batch     3 | loss: 2.1560202Losses:  2.0019898414611816 0.6152029037475586
CurrentTrain: epoch  5, batch     4 | loss: 2.6171927Losses:  1.8231945037841797 0.6007425785064697
CurrentTrain: epoch  5, batch     5 | loss: 2.4239371Losses:  2.0270943641662598 2.9802322387695312e-08
CurrentTrain: epoch  5, batch     6 | loss: 2.0270944Losses:  1.8707700967788696 0.2783859372138977
CurrentTrain: epoch  6, batch     0 | loss: 2.1491561Losses:  1.884597897529602 0.439994215965271
CurrentTrain: epoch  6, batch     1 | loss: 2.3245921Losses:  2.098313331604004 0.4624992609024048
CurrentTrain: epoch  6, batch     2 | loss: 2.5608125Losses:  2.185121536254883 0.48346036672592163
CurrentTrain: epoch  6, batch     3 | loss: 2.6685820Losses:  1.946855902671814 0.5524207353591919
CurrentTrain: epoch  6, batch     4 | loss: 2.4992766Losses:  1.868456244468689 0.561051070690155
CurrentTrain: epoch  6, batch     5 | loss: 2.4295073Losses:  1.7885311841964722 0.10502506792545319
CurrentTrain: epoch  6, batch     6 | loss: 1.8935562Losses:  1.857759952545166 0.35335689783096313
CurrentTrain: epoch  7, batch     0 | loss: 2.2111168Losses:  1.8537673950195312 0.2491043657064438
CurrentTrain: epoch  7, batch     1 | loss: 2.1028717Losses:  1.8487632274627686 0.5109450817108154
CurrentTrain: epoch  7, batch     2 | loss: 2.3597083Losses:  1.7686431407928467 0.4903233051300049
CurrentTrain: epoch  7, batch     3 | loss: 2.2589664Losses:  1.8425860404968262 0.4016779363155365
CurrentTrain: epoch  7, batch     4 | loss: 2.2442639Losses:  2.1422548294067383 0.5556938648223877
CurrentTrain: epoch  7, batch     5 | loss: 2.6979487Losses:  2.2457456588745117 0.18424172699451447
CurrentTrain: epoch  7, batch     6 | loss: 2.4299874Losses:  2.318944215774536 0.4969617426395416
CurrentTrain: epoch  8, batch     0 | loss: 2.8159060Losses:  1.9303464889526367 0.5898056030273438
CurrentTrain: epoch  8, batch     1 | loss: 2.5201521Losses:  1.758277416229248 0.2958640456199646
CurrentTrain: epoch  8, batch     2 | loss: 2.0541415Losses:  1.8278512954711914 0.26702284812927246
CurrentTrain: epoch  8, batch     3 | loss: 2.0948741Losses:  1.820074200630188 0.2959892153739929
CurrentTrain: epoch  8, batch     4 | loss: 2.1160634Losses:  1.7134673595428467 0.2291572093963623
CurrentTrain: epoch  8, batch     5 | loss: 1.9426246Losses:  1.6758460998535156 0.07675716280937195
CurrentTrain: epoch  8, batch     6 | loss: 1.7526033Losses:  1.6958985328674316 0.4474259912967682
CurrentTrain: epoch  9, batch     0 | loss: 2.1433246Losses:  1.9314236640930176 0.2655063271522522
CurrentTrain: epoch  9, batch     1 | loss: 2.1969299Losses:  2.297764778137207 0.37500888109207153
CurrentTrain: epoch  9, batch     2 | loss: 2.6727736Losses:  1.710451602935791 0.282822847366333
CurrentTrain: epoch  9, batch     3 | loss: 1.9932745Losses:  1.6761913299560547 0.16252638399600983
CurrentTrain: epoch  9, batch     4 | loss: 1.8387177Losses:  1.7115215063095093 0.39614033699035645
CurrentTrain: epoch  9, batch     5 | loss: 2.1076617Losses:  1.7556707859039307 0.1765652894973755
CurrentTrain: epoch  9, batch     6 | loss: 1.9322361
Losses:  5.54004430770874 0.433380663394928
MemoryTrain:  epoch  0, batch     0 | loss: 5.9734249Losses:  8.03110122680664 0.4863225817680359
MemoryTrain:  epoch  0, batch     1 | loss: 8.5174236Losses:  9.099458694458008 0.3236600160598755
MemoryTrain:  epoch  0, batch     2 | loss: 9.4231186Losses:  9.94359016418457 0.28285619616508484
MemoryTrain:  epoch  0, batch     3 | loss: 10.2264462Losses:  11.19119644165039 0.36462828516960144
MemoryTrain:  epoch  0, batch     4 | loss: 11.5558243Losses:  0.9841265678405762 0.2917250692844391
MemoryTrain:  epoch  1, batch     0 | loss: 1.2758516Losses:  0.46543246507644653 0.451649934053421
MemoryTrain:  epoch  1, batch     1 | loss: 0.9170824Losses:  0.8824644088745117 0.4823957681655884
MemoryTrain:  epoch  1, batch     2 | loss: 1.3648602Losses:  1.1948397159576416 0.16719040274620056
MemoryTrain:  epoch  1, batch     3 | loss: 1.3620301Losses:  0.9148154854774475 0.531244158744812
MemoryTrain:  epoch  1, batch     4 | loss: 1.4460597Losses:  0.7445451021194458 0.4438977837562561
MemoryTrain:  epoch  2, batch     0 | loss: 1.1884429Losses:  0.3423299789428711 0.3429338335990906
MemoryTrain:  epoch  2, batch     1 | loss: 0.6852638Losses:  0.68004310131073 0.3589969873428345
MemoryTrain:  epoch  2, batch     2 | loss: 1.0390401Losses:  0.7219558358192444 0.34079763293266296
MemoryTrain:  epoch  2, batch     3 | loss: 1.0627534Losses:  0.9229826331138611 0.4127224385738373
MemoryTrain:  epoch  2, batch     4 | loss: 1.3357050Losses:  0.28289496898651123 0.4063638746738434
MemoryTrain:  epoch  3, batch     0 | loss: 0.6892588Losses:  0.6328485608100891 0.45330309867858887
MemoryTrain:  epoch  3, batch     1 | loss: 1.0861516Losses:  0.801948606967926 0.36980700492858887
MemoryTrain:  epoch  3, batch     2 | loss: 1.1717556Losses:  0.2913624942302704 0.27482813596725464
MemoryTrain:  epoch  3, batch     3 | loss: 0.5661906Losses:  0.4168199300765991 0.4126373529434204
MemoryTrain:  epoch  3, batch     4 | loss: 0.8294573Losses:  0.2774578928947449 0.3194478154182434
MemoryTrain:  epoch  4, batch     0 | loss: 0.5969057Losses:  0.289834588766098 0.45867809653282166
MemoryTrain:  epoch  4, batch     1 | loss: 0.7485127Losses:  0.7435424327850342 0.3970659673213959
MemoryTrain:  epoch  4, batch     2 | loss: 1.1406084Losses:  0.2147042602300644 0.26338669657707214
MemoryTrain:  epoch  4, batch     3 | loss: 0.4780909Losses:  0.3670346140861511 0.3916195034980774
MemoryTrain:  epoch  4, batch     4 | loss: 0.7586541Losses:  0.403233140707016 0.3527778089046478
MemoryTrain:  epoch  5, batch     0 | loss: 0.7560109Losses:  0.32043904066085815 0.28558462858200073
MemoryTrain:  epoch  5, batch     1 | loss: 0.6060237Losses:  0.31691646575927734 0.5381206274032593
MemoryTrain:  epoch  5, batch     2 | loss: 0.8550371Losses:  0.37425076961517334 0.45172545313835144
MemoryTrain:  epoch  5, batch     3 | loss: 0.8259763Losses:  0.18794310092926025 0.3045847713947296
MemoryTrain:  epoch  5, batch     4 | loss: 0.4925279Losses:  0.3283587098121643 0.25378966331481934
MemoryTrain:  epoch  6, batch     0 | loss: 0.5821484Losses:  0.2685665190219879 0.32072457671165466
MemoryTrain:  epoch  6, batch     1 | loss: 0.5892911Losses:  0.35963621735572815 0.5573023557662964
MemoryTrain:  epoch  6, batch     2 | loss: 0.9169385Losses:  0.1633242666721344 0.25868260860443115
MemoryTrain:  epoch  6, batch     3 | loss: 0.4220069Losses:  0.31248611211776733 0.49626395106315613
MemoryTrain:  epoch  6, batch     4 | loss: 0.8087500Losses:  0.21293672919273376 0.35211753845214844
MemoryTrain:  epoch  7, batch     0 | loss: 0.5650543Losses:  0.18526887893676758 0.24975743889808655
MemoryTrain:  epoch  7, batch     1 | loss: 0.4350263Losses:  0.21374118328094482 0.2794034481048584
MemoryTrain:  epoch  7, batch     2 | loss: 0.4931446Losses:  0.27503740787506104 0.30746737122535706
MemoryTrain:  epoch  7, batch     3 | loss: 0.5825047Losses:  0.26156795024871826 0.46448853611946106
MemoryTrain:  epoch  7, batch     4 | loss: 0.7260565Losses:  0.1944378912448883 0.21300888061523438
MemoryTrain:  epoch  8, batch     0 | loss: 0.4074468Losses:  0.23847696185112 0.2669544219970703
MemoryTrain:  epoch  8, batch     1 | loss: 0.5054314Losses:  0.28517282009124756 0.36549174785614014
MemoryTrain:  epoch  8, batch     2 | loss: 0.6506646Losses:  0.2937270402908325 0.35918599367141724
MemoryTrain:  epoch  8, batch     3 | loss: 0.6529130Losses:  0.2625105381011963 0.3962816596031189
MemoryTrain:  epoch  8, batch     4 | loss: 0.6587922Losses:  0.32894885540008545 0.5477116107940674
MemoryTrain:  epoch  9, batch     0 | loss: 0.8766605Losses:  0.18114051222801208 0.2117728739976883
MemoryTrain:  epoch  9, batch     1 | loss: 0.3929134Losses:  0.2725931406021118 0.2634165585041046
MemoryTrain:  epoch  9, batch     2 | loss: 0.5360097Losses:  0.26033836603164673 0.30783501267433167
MemoryTrain:  epoch  9, batch     3 | loss: 0.5681734Losses:  0.18476873636245728 0.24280878901481628
MemoryTrain:  epoch  9, batch     4 | loss: 0.4275775
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.42%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.13%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.05%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.35%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.98%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.83%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 89.45%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.19%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.24%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 89.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.80%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.61%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.50%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.32%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 89.22%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 89.12%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.80%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 88.25%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 87.00%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 86.79%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 86.39%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 86.26%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 86.01%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 85.75%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.24%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 84.74%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 84.31%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 84.02%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 83.61%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 83.40%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.88%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 82.49%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 82.23%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 81.92%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 81.55%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.27%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.70%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.60%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.18%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.93%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.85%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 79.66%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 79.53%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 79.39%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 79.31%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 79.13%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 79.01%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 79.41%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.06%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 78.68%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 78.48%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.23%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.91%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.23%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 77.71%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 77.21%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 76.83%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 76.33%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 75.84%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 76.33%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 76.20%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.24%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 76.19%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 76.15%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.35%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 76.38%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.30%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.99%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 77.21%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 77.37%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 77.32%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 77.35%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 77.14%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 77.07%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 76.84%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 76.59%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 76.29%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 75.98%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 75.74%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 75.47%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 75.41%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:  224 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 77.49%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 77.51%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 77.53%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 77.53%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 77.49%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.67%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.98%   
cur_acc:  ['0.9494', '0.7133', '0.7321', '0.8105']
his_acc:  ['0.9494', '0.8275', '0.7922', '0.7798']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  6.894660949707031 0.9983617067337036
CurrentTrain: epoch  0, batch     0 | loss: 7.8930225Losses:  9.46349048614502 0.9854776263237
CurrentTrain: epoch  0, batch     1 | loss: 10.4489679Losses:  8.279500961303711 0.9175471067428589
CurrentTrain: epoch  0, batch     2 | loss: 9.1970482Losses:  8.395037651062012 1.3010469675064087
CurrentTrain: epoch  0, batch     3 | loss: 9.6960850Losses:  6.5200042724609375 0.9762130975723267
CurrentTrain: epoch  0, batch     4 | loss: 7.4962173Losses:  7.391189098358154 0.7518497705459595
CurrentTrain: epoch  0, batch     5 | loss: 8.1430387Losses:  5.28035306930542 8.94069742685133e-08
CurrentTrain: epoch  0, batch     6 | loss: 5.2803531Losses:  3.3391480445861816 0.8026422262191772
CurrentTrain: epoch  1, batch     0 | loss: 4.1417904Losses:  4.002985000610352 1.2048943042755127
CurrentTrain: epoch  1, batch     1 | loss: 5.2078791Losses:  2.968716621398926 0.8104397058486938
CurrentTrain: epoch  1, batch     2 | loss: 3.7791562Losses:  3.4945576190948486 1.189416527748108
CurrentTrain: epoch  1, batch     3 | loss: 4.6839743Losses:  3.085660457611084 1.01846182346344
CurrentTrain: epoch  1, batch     4 | loss: 4.1041222Losses:  3.195070743560791 1.2130460739135742
CurrentTrain: epoch  1, batch     5 | loss: 4.4081168Losses:  3.358466148376465 0.3271353244781494
CurrentTrain: epoch  1, batch     6 | loss: 3.6856015Losses:  3.5581588745117188 1.063154935836792
CurrentTrain: epoch  2, batch     0 | loss: 4.6213140Losses:  2.6504006385803223 1.025174617767334
CurrentTrain: epoch  2, batch     1 | loss: 3.6755753Losses:  3.2119152545928955 1.199021816253662
CurrentTrain: epoch  2, batch     2 | loss: 4.4109373Losses:  3.6138529777526855 0.7892114520072937
CurrentTrain: epoch  2, batch     3 | loss: 4.4030643Losses:  2.445225715637207 0.7999370694160461
CurrentTrain: epoch  2, batch     4 | loss: 3.2451627Losses:  3.169950485229492 0.8856178522109985
CurrentTrain: epoch  2, batch     5 | loss: 4.0555682Losses:  2.339082956314087 0.053187936544418335
CurrentTrain: epoch  2, batch     6 | loss: 2.3922708Losses:  3.6952831745147705 0.9127030372619629
CurrentTrain: epoch  3, batch     0 | loss: 4.6079865Losses:  2.7635459899902344 0.5091864466667175
CurrentTrain: epoch  3, batch     1 | loss: 3.2727325Losses:  2.0859158039093018 0.37003782391548157
CurrentTrain: epoch  3, batch     2 | loss: 2.4559536Losses:  3.287916660308838 0.5707877278327942
CurrentTrain: epoch  3, batch     3 | loss: 3.8587043Losses:  2.989226818084717 0.7494063973426819
CurrentTrain: epoch  3, batch     4 | loss: 3.7386332Losses:  2.307511806488037 0.6484898328781128
CurrentTrain: epoch  3, batch     5 | loss: 2.9560018Losses:  3.331448554992676 0.332376092672348
CurrentTrain: epoch  3, batch     6 | loss: 3.6638246Losses:  2.7515454292297363 0.6766998767852783
CurrentTrain: epoch  4, batch     0 | loss: 3.4282453Losses:  2.2448441982269287 0.4042344391345978
CurrentTrain: epoch  4, batch     1 | loss: 2.6490786Losses:  2.296175718307495 0.7027406096458435
CurrentTrain: epoch  4, batch     2 | loss: 2.9989164Losses:  3.2295949459075928 0.6084372401237488
CurrentTrain: epoch  4, batch     3 | loss: 3.8380322Losses:  2.5507314205169678 0.38176918029785156
CurrentTrain: epoch  4, batch     4 | loss: 2.9325006Losses:  2.3417370319366455 0.6562321186065674
CurrentTrain: epoch  4, batch     5 | loss: 2.9979692Losses:  1.7059215307235718 0.08809024095535278
CurrentTrain: epoch  4, batch     6 | loss: 1.7940118Losses:  2.254150390625 0.7263326644897461
CurrentTrain: epoch  5, batch     0 | loss: 2.9804831Losses:  2.419934034347534 0.7070578932762146
CurrentTrain: epoch  5, batch     1 | loss: 3.1269920Losses:  2.1318459510803223 0.3376115560531616
CurrentTrain: epoch  5, batch     2 | loss: 2.4694576Losses:  2.120326042175293 0.49977266788482666
CurrentTrain: epoch  5, batch     3 | loss: 2.6200986Losses:  2.3860042095184326 0.3977399468421936
CurrentTrain: epoch  5, batch     4 | loss: 2.7837441Losses:  2.279294967651367 0.562246561050415
CurrentTrain: epoch  5, batch     5 | loss: 2.8415415Losses:  2.8839259147644043 0.176121786236763
CurrentTrain: epoch  5, batch     6 | loss: 3.0600476Losses:  2.29502010345459 0.31949305534362793
CurrentTrain: epoch  6, batch     0 | loss: 2.6145132Losses:  2.012701988220215 0.5020264387130737
CurrentTrain: epoch  6, batch     1 | loss: 2.5147285Losses:  2.0977845191955566 0.562400221824646
CurrentTrain: epoch  6, batch     2 | loss: 2.6601849Losses:  2.0305862426757812 0.36869996786117554
CurrentTrain: epoch  6, batch     3 | loss: 2.3992863Losses:  2.0782952308654785 0.5777556300163269
CurrentTrain: epoch  6, batch     4 | loss: 2.6560509Losses:  2.2404913902282715 0.47571516036987305
CurrentTrain: epoch  6, batch     5 | loss: 2.7162066Losses:  1.7308478355407715 0.11354335397481918
CurrentTrain: epoch  6, batch     6 | loss: 1.8443912Losses:  2.1517629623413086 0.4160020351409912
CurrentTrain: epoch  7, batch     0 | loss: 2.5677650Losses:  1.8974273204803467 0.34870481491088867
CurrentTrain: epoch  7, batch     1 | loss: 2.2461321Losses:  2.101464033126831 0.4056686460971832
CurrentTrain: epoch  7, batch     2 | loss: 2.5071328Losses:  1.826664924621582 0.34012851119041443
CurrentTrain: epoch  7, batch     3 | loss: 2.1667933Losses:  1.9507176876068115 0.4357447028160095
CurrentTrain: epoch  7, batch     4 | loss: 2.3864625Losses:  1.944819450378418 0.2822662591934204
CurrentTrain: epoch  7, batch     5 | loss: 2.2270856Losses:  2.0329854488372803 0.2580162286758423
CurrentTrain: epoch  7, batch     6 | loss: 2.2910018Losses:  1.9242262840270996 0.5105245113372803
CurrentTrain: epoch  8, batch     0 | loss: 2.4347508Losses:  1.811565637588501 0.4704900085926056
CurrentTrain: epoch  8, batch     1 | loss: 2.2820556Losses:  1.8257157802581787 0.27387723326683044
CurrentTrain: epoch  8, batch     2 | loss: 2.0995929Losses:  2.128528356552124 0.4130840599536896
CurrentTrain: epoch  8, batch     3 | loss: 2.5416124Losses:  1.7676023244857788 0.22867466509342194
CurrentTrain: epoch  8, batch     4 | loss: 1.9962770Losses:  1.9430696964263916 0.3322187662124634
CurrentTrain: epoch  8, batch     5 | loss: 2.2752886Losses:  1.7039854526519775 0.030077867209911346
CurrentTrain: epoch  8, batch     6 | loss: 1.7340633Losses:  1.8522346019744873 0.4912140369415283
CurrentTrain: epoch  9, batch     0 | loss: 2.3434486Losses:  1.8175140619277954 0.40069130063056946
CurrentTrain: epoch  9, batch     1 | loss: 2.2182055Losses:  1.7432408332824707 0.217106431722641
CurrentTrain: epoch  9, batch     2 | loss: 1.9603473Losses:  1.7989426851272583 0.40662240982055664
CurrentTrain: epoch  9, batch     3 | loss: 2.2055650Losses:  1.8084123134613037 0.315199077129364
CurrentTrain: epoch  9, batch     4 | loss: 2.1236115Losses:  1.9343814849853516 0.4460749924182892
CurrentTrain: epoch  9, batch     5 | loss: 2.3804564Losses:  1.7254626750946045 0.07472453266382217
CurrentTrain: epoch  9, batch     6 | loss: 1.8001872
Losses:  6.129368782043457 0.27281415462493896
MemoryTrain:  epoch  0, batch     0 | loss: 6.4021831Losses:  8.718770980834961 0.29917386174201965
MemoryTrain:  epoch  0, batch     1 | loss: 9.0179453Losses:  9.56791877746582 0.5549881458282471
MemoryTrain:  epoch  0, batch     2 | loss: 10.1229067Losses:  9.039745330810547 0.30134886503219604
MemoryTrain:  epoch  0, batch     3 | loss: 9.3410940Losses:  11.051813125610352 0.33651721477508545
MemoryTrain:  epoch  0, batch     4 | loss: 11.3883305Losses:  10.839303970336914 0.40242043137550354
MemoryTrain:  epoch  0, batch     5 | loss: 11.2417240Losses:  12.405569076538086 0.03917333483695984
MemoryTrain:  epoch  0, batch     6 | loss: 12.4447422Losses:  0.8728317022323608 0.5990474224090576
MemoryTrain:  epoch  1, batch     0 | loss: 1.4718791Losses:  1.7443885803222656 0.32414984703063965
MemoryTrain:  epoch  1, batch     1 | loss: 2.0685384Losses:  0.7439500093460083 0.3455617427825928
MemoryTrain:  epoch  1, batch     2 | loss: 1.0895118Losses:  1.3036670684814453 0.4448615610599518
MemoryTrain:  epoch  1, batch     3 | loss: 1.7485286Losses:  1.174879550933838 0.29271620512008667
MemoryTrain:  epoch  1, batch     4 | loss: 1.4675958Losses:  0.3398516774177551 0.4129469692707062
MemoryTrain:  epoch  1, batch     5 | loss: 0.7527987Losses:  0.14955590665340424 0.020562317222356796
MemoryTrain:  epoch  1, batch     6 | loss: 0.1701182Losses:  0.5706618428230286 0.3372206389904022
MemoryTrain:  epoch  2, batch     0 | loss: 0.9078825Losses:  1.5893361568450928 0.2545192837715149
MemoryTrain:  epoch  2, batch     1 | loss: 1.8438554Losses:  0.442311555147171 0.4576685428619385
MemoryTrain:  epoch  2, batch     2 | loss: 0.8999801Losses:  0.45147567987442017 0.36477673053741455
MemoryTrain:  epoch  2, batch     3 | loss: 0.8162524Losses:  0.5868514776229858 0.3348231911659241
MemoryTrain:  epoch  2, batch     4 | loss: 0.9216747Losses:  0.8473140001296997 0.3929750323295593
MemoryTrain:  epoch  2, batch     5 | loss: 1.2402890Losses:  0.34644830226898193 0.32680827379226685
MemoryTrain:  epoch  2, batch     6 | loss: 0.6732566Losses:  0.5048716068267822 0.23091834783554077
MemoryTrain:  epoch  3, batch     0 | loss: 0.7357900Losses:  0.2981160879135132 0.2474612444639206
MemoryTrain:  epoch  3, batch     1 | loss: 0.5455773Losses:  0.42263543605804443 0.5248581171035767
MemoryTrain:  epoch  3, batch     2 | loss: 0.9474936Losses:  0.2990432679653168 0.4787745177745819
MemoryTrain:  epoch  3, batch     3 | loss: 0.7778178Losses:  0.6745491027832031 0.2713737487792969
MemoryTrain:  epoch  3, batch     4 | loss: 0.9459229Losses:  0.8801402449607849 0.5230616331100464
MemoryTrain:  epoch  3, batch     5 | loss: 1.4032018Losses:  0.30755290389060974 0.2116234302520752
MemoryTrain:  epoch  3, batch     6 | loss: 0.5191764Losses:  0.493093341588974 0.6187963485717773
MemoryTrain:  epoch  4, batch     0 | loss: 1.1118897Losses:  0.3575749695301056 0.22252795100212097
MemoryTrain:  epoch  4, batch     1 | loss: 0.5801029Losses:  0.3626519441604614 0.500511884689331
MemoryTrain:  epoch  4, batch     2 | loss: 0.8631638Losses:  0.3589685559272766 0.4387810528278351
MemoryTrain:  epoch  4, batch     3 | loss: 0.7977496Losses:  0.5621609091758728 0.3821728825569153
MemoryTrain:  epoch  4, batch     4 | loss: 0.9443338Losses:  0.417976975440979 0.31959864497184753
MemoryTrain:  epoch  4, batch     5 | loss: 0.7375757Losses:  0.16615095734596252 0.024381304159760475
MemoryTrain:  epoch  4, batch     6 | loss: 0.1905323Losses:  0.36444899439811707 0.3577113747596741
MemoryTrain:  epoch  5, batch     0 | loss: 0.7221603Losses:  0.4022594094276428 0.27368447184562683
MemoryTrain:  epoch  5, batch     1 | loss: 0.6759439Losses:  0.3658744990825653 0.47061482071876526
MemoryTrain:  epoch  5, batch     2 | loss: 0.8364893Losses:  0.4863620698451996 0.3438774347305298
MemoryTrain:  epoch  5, batch     3 | loss: 0.8302395Losses:  0.3331952393054962 0.3505314290523529
MemoryTrain:  epoch  5, batch     4 | loss: 0.6837267Losses:  0.38214850425720215 0.5186713337898254
MemoryTrain:  epoch  5, batch     5 | loss: 0.9008198Losses:  0.24209414422512054 0.015045944601297379
MemoryTrain:  epoch  5, batch     6 | loss: 0.2571401Losses:  0.5010838508605957 0.41697320342063904
MemoryTrain:  epoch  6, batch     0 | loss: 0.9180571Losses:  0.40364837646484375 0.4617932140827179
MemoryTrain:  epoch  6, batch     1 | loss: 0.8654416Losses:  0.2531546652317047 0.3663073778152466
MemoryTrain:  epoch  6, batch     2 | loss: 0.6194620Losses:  0.31320255994796753 0.41678479313850403
MemoryTrain:  epoch  6, batch     3 | loss: 0.7299874Losses:  0.2722763121128082 0.4480302035808563
MemoryTrain:  epoch  6, batch     4 | loss: 0.7203065Losses:  0.25643983483314514 0.34712982177734375
MemoryTrain:  epoch  6, batch     5 | loss: 0.6035696Losses:  0.193211168050766 0.04712759703397751
MemoryTrain:  epoch  6, batch     6 | loss: 0.2403388Losses:  0.3338428735733032 0.343282014131546
MemoryTrain:  epoch  7, batch     0 | loss: 0.6771249Losses:  0.31380268931388855 0.42795461416244507
MemoryTrain:  epoch  7, batch     1 | loss: 0.7417573Losses:  0.28781747817993164 0.25413647294044495
MemoryTrain:  epoch  7, batch     2 | loss: 0.5419539Losses:  0.25265926122665405 0.25682902336120605
MemoryTrain:  epoch  7, batch     3 | loss: 0.5094883Losses:  0.43084245920181274 0.46846723556518555
MemoryTrain:  epoch  7, batch     4 | loss: 0.8993097Losses:  0.2582082748413086 0.3002087473869324
MemoryTrain:  epoch  7, batch     5 | loss: 0.5584170Losses:  0.2053421437740326 0.006229972466826439
MemoryTrain:  epoch  7, batch     6 | loss: 0.2115721Losses:  0.34304627776145935 0.49895912408828735
MemoryTrain:  epoch  8, batch     0 | loss: 0.8420054Losses:  0.2712218165397644 0.37519603967666626
MemoryTrain:  epoch  8, batch     1 | loss: 0.6464179Losses:  0.31140702962875366 0.24976591765880585
MemoryTrain:  epoch  8, batch     2 | loss: 0.5611730Losses:  0.20270352065563202 0.27767348289489746
MemoryTrain:  epoch  8, batch     3 | loss: 0.4803770Losses:  0.3462581932544708 0.2322152554988861
MemoryTrain:  epoch  8, batch     4 | loss: 0.5784734Losses:  0.25236570835113525 0.3448370397090912
MemoryTrain:  epoch  8, batch     5 | loss: 0.5972028Losses:  0.3364206552505493 0.08884802460670471
MemoryTrain:  epoch  8, batch     6 | loss: 0.4252687Losses:  0.1801806390285492 0.2415255308151245
MemoryTrain:  epoch  9, batch     0 | loss: 0.4217062Losses:  0.31481337547302246 0.4677676558494568
MemoryTrain:  epoch  9, batch     1 | loss: 0.7825810Losses:  0.4254336357116699 0.36566874384880066
MemoryTrain:  epoch  9, batch     2 | loss: 0.7911024Losses:  0.21368956565856934 0.3207804560661316
MemoryTrain:  epoch  9, batch     3 | loss: 0.5344700Losses:  0.34772300720214844 0.3448454737663269
MemoryTrain:  epoch  9, batch     4 | loss: 0.6925685Losses:  0.2547714114189148 0.28790050745010376
MemoryTrain:  epoch  9, batch     5 | loss: 0.5426719Losses:  0.20085276663303375 0.0047076912596821785
MemoryTrain:  epoch  9, batch     6 | loss: 0.2055605
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 60.76%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 60.14%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 60.36%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 60.74%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.71%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 88.36%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.91%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.80%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 87.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 88.25%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.72%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 88.51%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 87.34%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.18%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 86.95%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 86.81%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 85.99%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 85.79%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 85.37%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 85.17%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 84.77%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 84.59%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 84.34%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 84.07%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 83.70%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 83.40%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.91%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 82.50%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 82.10%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 81.89%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 81.44%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 80.81%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 80.45%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 80.21%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 79.85%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 79.63%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 79.35%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 79.13%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.74%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.18%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.64%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.05%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 76.58%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.17%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.94%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.93%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 77.76%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 77.64%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 77.57%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.43%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.88%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.54%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.13%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 76.80%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 76.53%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.26%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.66%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 76.15%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.65%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 75.28%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.80%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.32%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.32%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 75.04%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.89%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 74.78%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 74.96%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.03%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.53%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 75.73%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 75.79%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 76.24%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 76.36%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 76.32%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 76.34%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 76.33%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 76.13%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 76.10%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 76.03%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 75.85%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 75.57%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 75.27%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 74.70%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 74.44%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 75.14%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 76.66%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 77.12%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 77.03%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 76.83%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 76.65%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 76.52%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 76.42%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 76.31%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.13%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 76.10%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 76.09%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 76.06%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 76.05%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 76.00%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 76.05%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 76.26%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 76.06%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 75.94%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 75.85%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.68%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 75.48%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 75.42%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 75.22%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 75.09%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 75.09%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 75.15%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 75.13%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 76.04%   
cur_acc:  ['0.9494', '0.7133', '0.7321', '0.8105', '0.7103']
his_acc:  ['0.9494', '0.8275', '0.7922', '0.7798', '0.7604']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  6.684430122375488 0.9631171822547913
CurrentTrain: epoch  0, batch     0 | loss: 7.6475472Losses:  8.539352416992188 0.9584515690803528
CurrentTrain: epoch  0, batch     1 | loss: 9.4978037Losses:  8.512619018554688 0.7678723335266113
CurrentTrain: epoch  0, batch     2 | loss: 9.2804909Losses:  9.222931861877441 1.032762885093689
CurrentTrain: epoch  0, batch     3 | loss: 10.2556944Losses:  8.537199974060059 1.0875999927520752
CurrentTrain: epoch  0, batch     4 | loss: 9.6247997Losses:  7.5020952224731445 0.9637172222137451
CurrentTrain: epoch  0, batch     5 | loss: 8.4658127Losses:  5.690145492553711 5.960464477539063e-08
CurrentTrain: epoch  0, batch     6 | loss: 5.6901455Losses:  4.3827948570251465 1.1246862411499023
CurrentTrain: epoch  1, batch     0 | loss: 5.5074811Losses:  4.907133102416992 1.0933811664581299
CurrentTrain: epoch  1, batch     1 | loss: 6.0005140Losses:  3.5335400104522705 0.9540979862213135
CurrentTrain: epoch  1, batch     2 | loss: 4.4876380Losses:  3.4276983737945557 0.7855803966522217
CurrentTrain: epoch  1, batch     3 | loss: 4.2132788Losses:  3.1300549507141113 0.8423781394958496
CurrentTrain: epoch  1, batch     4 | loss: 3.9724331Losses:  3.163381576538086 1.0326693058013916
CurrentTrain: epoch  1, batch     5 | loss: 4.1960506Losses:  4.261911392211914 0.3529522120952606
CurrentTrain: epoch  1, batch     6 | loss: 4.6148634Losses:  4.420657157897949 1.0453380346298218
CurrentTrain: epoch  2, batch     0 | loss: 5.4659953Losses:  2.8491437435150146 0.6839441061019897
CurrentTrain: epoch  2, batch     1 | loss: 3.5330877Losses:  3.391016960144043 0.8567986488342285
CurrentTrain: epoch  2, batch     2 | loss: 4.2478156Losses:  4.187094211578369 0.5156146287918091
CurrentTrain: epoch  2, batch     3 | loss: 4.7027087Losses:  3.110942840576172 0.6180766820907593
CurrentTrain: epoch  2, batch     4 | loss: 3.7290196Losses:  2.886655569076538 0.8438742756843567
CurrentTrain: epoch  2, batch     5 | loss: 3.7305298Losses:  3.923936367034912 0.4324811100959778
CurrentTrain: epoch  2, batch     6 | loss: 4.3564177Losses:  2.1636428833007812 0.6516599655151367
CurrentTrain: epoch  3, batch     0 | loss: 2.8153028Losses:  3.298168659210205 0.9187763929367065
CurrentTrain: epoch  3, batch     1 | loss: 4.2169452Losses:  2.8684167861938477 0.6007333993911743
CurrentTrain: epoch  3, batch     2 | loss: 3.4691501Losses:  3.3752901554107666 1.065796136856079
CurrentTrain: epoch  3, batch     3 | loss: 4.4410863Losses:  3.474119186401367 0.8412508368492126
CurrentTrain: epoch  3, batch     4 | loss: 4.3153701Losses:  2.9933505058288574 0.8866435289382935
CurrentTrain: epoch  3, batch     5 | loss: 3.8799939Losses:  4.178296089172363 0.17275115847587585
CurrentTrain: epoch  3, batch     6 | loss: 4.3510470Losses:  2.941744327545166 0.7324236631393433
CurrentTrain: epoch  4, batch     0 | loss: 3.6741681Losses:  1.9911868572235107 0.6078479290008545
CurrentTrain: epoch  4, batch     1 | loss: 2.5990348Losses:  2.7550361156463623 0.6089112162590027
CurrentTrain: epoch  4, batch     2 | loss: 3.3639474Losses:  3.516033887863159 0.6610729098320007
CurrentTrain: epoch  4, batch     3 | loss: 4.1771069Losses:  3.1605031490325928 0.6267182230949402
CurrentTrain: epoch  4, batch     4 | loss: 3.7872214Losses:  2.8256773948669434 0.48909497261047363
CurrentTrain: epoch  4, batch     5 | loss: 3.3147724Losses:  1.896952509880066 0.20965316891670227
CurrentTrain: epoch  4, batch     6 | loss: 2.1066058Losses:  2.052309036254883 0.5431157946586609
CurrentTrain: epoch  5, batch     0 | loss: 2.5954249Losses:  3.263615131378174 0.6133437156677246
CurrentTrain: epoch  5, batch     1 | loss: 3.8769588Losses:  2.5667004585266113 0.4395922124385834
CurrentTrain: epoch  5, batch     2 | loss: 3.0062926Losses:  2.7309131622314453 0.3935337960720062
CurrentTrain: epoch  5, batch     3 | loss: 3.1244469Losses:  2.7377548217773438 0.8153389692306519
CurrentTrain: epoch  5, batch     4 | loss: 3.5530939Losses:  2.3528196811676025 0.42554235458374023
CurrentTrain: epoch  5, batch     5 | loss: 2.7783620Losses:  2.164419651031494 0.09504150599241257
CurrentTrain: epoch  5, batch     6 | loss: 2.2594612Losses:  2.1689836978912354 0.5302810072898865
CurrentTrain: epoch  6, batch     0 | loss: 2.6992648Losses:  2.0675806999206543 0.6132255792617798
CurrentTrain: epoch  6, batch     1 | loss: 2.6808062Losses:  2.6420083045959473 0.688105583190918
CurrentTrain: epoch  6, batch     2 | loss: 3.3301139Losses:  2.1853246688842773 0.6738410592079163
CurrentTrain: epoch  6, batch     3 | loss: 2.8591657Losses:  2.83949613571167 0.8019569516181946
CurrentTrain: epoch  6, batch     4 | loss: 3.6414530Losses:  2.046617269515991 0.5466482639312744
CurrentTrain: epoch  6, batch     5 | loss: 2.5932655Losses:  3.1134963035583496 0.036663975566625595
CurrentTrain: epoch  6, batch     6 | loss: 3.1501603Losses:  2.1003875732421875 0.5824365615844727
CurrentTrain: epoch  7, batch     0 | loss: 2.6828241Losses:  2.2822399139404297 0.5077947974205017
CurrentTrain: epoch  7, batch     1 | loss: 2.7900348Losses:  2.3474698066711426 0.8029494285583496
CurrentTrain: epoch  7, batch     2 | loss: 3.1504192Losses:  2.138154983520508 0.5707389712333679
CurrentTrain: epoch  7, batch     3 | loss: 2.7088940Losses:  2.1282126903533936 0.5828925371170044
CurrentTrain: epoch  7, batch     4 | loss: 2.7111053Losses:  1.8979806900024414 0.273206889629364
CurrentTrain: epoch  7, batch     5 | loss: 2.1711876Losses:  2.8331189155578613 0.13326460123062134
CurrentTrain: epoch  7, batch     6 | loss: 2.9663835Losses:  2.0318822860717773 0.45442909002304077
CurrentTrain: epoch  8, batch     0 | loss: 2.4863114Losses:  1.7862175703048706 0.32793402671813965
CurrentTrain: epoch  8, batch     1 | loss: 2.1141515Losses:  1.823900580406189 0.3076133131980896
CurrentTrain: epoch  8, batch     2 | loss: 2.1315138Losses:  1.8747881650924683 0.460567444562912
CurrentTrain: epoch  8, batch     3 | loss: 2.3353555Losses:  2.2888197898864746 0.612903356552124
CurrentTrain: epoch  8, batch     4 | loss: 2.9017231Losses:  2.537374973297119 0.6646248698234558
CurrentTrain: epoch  8, batch     5 | loss: 3.2019999Losses:  1.688934564590454 0.023946207016706467
CurrentTrain: epoch  8, batch     6 | loss: 1.7128807Losses:  1.746290922164917 0.2982197701931
CurrentTrain: epoch  9, batch     0 | loss: 2.0445106Losses:  1.938995122909546 0.36508047580718994
CurrentTrain: epoch  9, batch     1 | loss: 2.3040757Losses:  1.927259922027588 0.3758235573768616
CurrentTrain: epoch  9, batch     2 | loss: 2.3030834Losses:  1.765523910522461 0.37299850583076477
CurrentTrain: epoch  9, batch     3 | loss: 2.1385224Losses:  2.268845319747925 0.5991997718811035
CurrentTrain: epoch  9, batch     4 | loss: 2.8680451Losses:  1.8093273639678955 0.3680403232574463
CurrentTrain: epoch  9, batch     5 | loss: 2.1773677Losses:  2.934648036956787 0.2853139042854309
CurrentTrain: epoch  9, batch     6 | loss: 3.2199619
Losses:  5.970231533050537 0.41084808111190796
MemoryTrain:  epoch  0, batch     0 | loss: 6.3810797Losses:  8.067148208618164 0.2980363965034485
MemoryTrain:  epoch  0, batch     1 | loss: 8.3651848Losses:  9.079758644104004 0.4875144958496094
MemoryTrain:  epoch  0, batch     2 | loss: 9.5672731Losses:  9.566171646118164 0.4820759892463684
MemoryTrain:  epoch  0, batch     3 | loss: 10.0482473Losses:  9.711098670959473 0.3779605031013489
MemoryTrain:  epoch  0, batch     4 | loss: 10.0890589Losses:  9.702620506286621 0.2846299707889557
MemoryTrain:  epoch  0, batch     5 | loss: 9.9872503Losses:  11.04617977142334 0.39775872230529785
MemoryTrain:  epoch  0, batch     6 | loss: 11.4439383Losses:  10.524849891662598 0.2355746328830719
MemoryTrain:  epoch  0, batch     7 | loss: 10.7604246Losses:  1.242018222808838 0.5048177242279053
MemoryTrain:  epoch  1, batch     0 | loss: 1.7468359Losses:  0.916082501411438 0.2713560163974762
MemoryTrain:  epoch  1, batch     1 | loss: 1.1874385Losses:  1.3726102113723755 0.4789743423461914
MemoryTrain:  epoch  1, batch     2 | loss: 1.8515846Losses:  0.5694948434829712 0.34657493233680725
MemoryTrain:  epoch  1, batch     3 | loss: 0.9160697Losses:  1.0454716682434082 0.44799938797950745
MemoryTrain:  epoch  1, batch     4 | loss: 1.4934710Losses:  0.9600417613983154 0.40109503269195557
MemoryTrain:  epoch  1, batch     5 | loss: 1.3611368Losses:  0.8287943005561829 0.40214139223098755
MemoryTrain:  epoch  1, batch     6 | loss: 1.2309357Losses:  0.7812573909759521 0.19352412223815918
MemoryTrain:  epoch  1, batch     7 | loss: 0.9747815Losses:  0.862929105758667 0.4536731243133545
MemoryTrain:  epoch  2, batch     0 | loss: 1.3166022Losses:  0.532979428768158 0.3772146701812744
MemoryTrain:  epoch  2, batch     1 | loss: 0.9101941Losses:  0.36497971415519714 0.2596172094345093
MemoryTrain:  epoch  2, batch     2 | loss: 0.6245970Losses:  0.9271373152732849 0.638214111328125
MemoryTrain:  epoch  2, batch     3 | loss: 1.5653515Losses:  0.4687900245189667 0.2562907636165619
MemoryTrain:  epoch  2, batch     4 | loss: 0.7250808Losses:  0.7817951440811157 0.3276691138744354
MemoryTrain:  epoch  2, batch     5 | loss: 1.1094643Losses:  0.7760661840438843 0.28710389137268066
MemoryTrain:  epoch  2, batch     6 | loss: 1.0631701Losses:  0.9259597063064575 0.2589564919471741
MemoryTrain:  epoch  2, batch     7 | loss: 1.1849163Losses:  0.35961177945137024 0.33378052711486816
MemoryTrain:  epoch  3, batch     0 | loss: 0.6933923Losses:  0.34355488419532776 0.39358916878700256
MemoryTrain:  epoch  3, batch     1 | loss: 0.7371441Losses:  0.22751420736312866 0.23383530974388123
MemoryTrain:  epoch  3, batch     2 | loss: 0.4613495Losses:  0.6236907839775085 0.5580336451530457
MemoryTrain:  epoch  3, batch     3 | loss: 1.1817244Losses:  0.32843878865242004 0.2465589940547943
MemoryTrain:  epoch  3, batch     4 | loss: 0.5749978Losses:  0.9855069518089294 0.33765870332717896
MemoryTrain:  epoch  3, batch     5 | loss: 1.3231657Losses:  0.7485096454620361 0.5488516092300415
MemoryTrain:  epoch  3, batch     6 | loss: 1.2973613Losses:  1.388243317604065 0.2553134560585022
MemoryTrain:  epoch  3, batch     7 | loss: 1.6435568Losses:  1.0305769443511963 0.41453975439071655
MemoryTrain:  epoch  4, batch     0 | loss: 1.4451168Losses:  0.8421427011489868 0.7071104049682617
MemoryTrain:  epoch  4, batch     1 | loss: 1.5492531Losses:  0.3167790174484253 0.2692168354988098
MemoryTrain:  epoch  4, batch     2 | loss: 0.5859959Losses:  0.43198245763778687 0.2561780512332916
MemoryTrain:  epoch  4, batch     3 | loss: 0.6881605Losses:  0.31805211305618286 0.26776814460754395
MemoryTrain:  epoch  4, batch     4 | loss: 0.5858203Losses:  0.3465465009212494 0.34510552883148193
MemoryTrain:  epoch  4, batch     5 | loss: 0.6916521Losses:  0.3572733700275421 0.3059260845184326
MemoryTrain:  epoch  4, batch     6 | loss: 0.6631994Losses:  0.45536893606185913 0.21900314092636108
MemoryTrain:  epoch  4, batch     7 | loss: 0.6743721Losses:  0.5381752848625183 0.31838592886924744
MemoryTrain:  epoch  5, batch     0 | loss: 0.8565612Losses:  0.24529284238815308 0.26918041706085205
MemoryTrain:  epoch  5, batch     1 | loss: 0.5144733Losses:  0.3517862856388092 0.42126578092575073
MemoryTrain:  epoch  5, batch     2 | loss: 0.7730521Losses:  0.39442479610443115 0.4503178894519806
MemoryTrain:  epoch  5, batch     3 | loss: 0.8447427Losses:  0.4818374216556549 0.5279355049133301
MemoryTrain:  epoch  5, batch     4 | loss: 1.0097729Losses:  0.3656436800956726 0.33131465315818787
MemoryTrain:  epoch  5, batch     5 | loss: 0.6969583Losses:  0.2785128355026245 0.308431476354599
MemoryTrain:  epoch  5, batch     6 | loss: 0.5869443Losses:  0.6221056580543518 0.2550925612449646
MemoryTrain:  epoch  5, batch     7 | loss: 0.8771982Losses:  0.3961641192436218 0.34713804721832275
MemoryTrain:  epoch  6, batch     0 | loss: 0.7433022Losses:  0.27991485595703125 0.29045799374580383
MemoryTrain:  epoch  6, batch     1 | loss: 0.5703728Losses:  0.3586665093898773 0.415720134973526
MemoryTrain:  epoch  6, batch     2 | loss: 0.7743866Losses:  0.27780622243881226 0.3002745509147644
MemoryTrain:  epoch  6, batch     3 | loss: 0.5780808Losses:  0.41045081615448 0.5610904097557068
MemoryTrain:  epoch  6, batch     4 | loss: 0.9715412Losses:  0.2894141376018524 0.3523547947406769
MemoryTrain:  epoch  6, batch     5 | loss: 0.6417689Losses:  0.35387811064720154 0.32926297187805176
MemoryTrain:  epoch  6, batch     6 | loss: 0.6831411Losses:  0.3053756058216095 0.0908091813325882
MemoryTrain:  epoch  6, batch     7 | loss: 0.3961848Losses:  0.4122433066368103 0.6458673477172852
MemoryTrain:  epoch  7, batch     0 | loss: 1.0581107Losses:  0.26509520411491394 0.1974610984325409
MemoryTrain:  epoch  7, batch     1 | loss: 0.4625563Losses:  0.35155513882637024 0.39677733182907104
MemoryTrain:  epoch  7, batch     2 | loss: 0.7483325Losses:  0.41798609495162964 0.4121837615966797
MemoryTrain:  epoch  7, batch     3 | loss: 0.8301699Losses:  0.3119966387748718 0.3046806752681732
MemoryTrain:  epoch  7, batch     4 | loss: 0.6166773Losses:  0.28962308168411255 0.3679816722869873
MemoryTrain:  epoch  7, batch     5 | loss: 0.6576048Losses:  0.28950145840644836 0.32223278284072876
MemoryTrain:  epoch  7, batch     6 | loss: 0.6117343Losses:  0.3592570424079895 0.17014682292938232
MemoryTrain:  epoch  7, batch     7 | loss: 0.5294039Losses:  0.28849929571151733 0.31745368242263794
MemoryTrain:  epoch  8, batch     0 | loss: 0.6059530Losses:  0.32751190662384033 0.42149654030799866
MemoryTrain:  epoch  8, batch     1 | loss: 0.7490084Losses:  0.37944406270980835 0.2701854705810547
MemoryTrain:  epoch  8, batch     2 | loss: 0.6496295Losses:  0.367204487323761 0.3316064476966858
MemoryTrain:  epoch  8, batch     3 | loss: 0.6988109Losses:  0.3357475996017456 0.39414384961128235
MemoryTrain:  epoch  8, batch     4 | loss: 0.7298914Losses:  0.34575724601745605 0.395946741104126
MemoryTrain:  epoch  8, batch     5 | loss: 0.7417040Losses:  0.3722246289253235 0.344167023897171
MemoryTrain:  epoch  8, batch     6 | loss: 0.7163917Losses:  0.35357198119163513 0.12507429718971252
MemoryTrain:  epoch  8, batch     7 | loss: 0.4786463Losses:  0.26329606771469116 0.2577974498271942
MemoryTrain:  epoch  9, batch     0 | loss: 0.5210935Losses:  0.39127087593078613 0.38752174377441406
MemoryTrain:  epoch  9, batch     1 | loss: 0.7787926Losses:  0.32953619956970215 0.37977147102355957
MemoryTrain:  epoch  9, batch     2 | loss: 0.7093077Losses:  0.36290252208709717 0.4857712984085083
MemoryTrain:  epoch  9, batch     3 | loss: 0.8486738Losses:  0.3051789402961731 0.2934766411781311
MemoryTrain:  epoch  9, batch     4 | loss: 0.5986556Losses:  0.2862604260444641 0.44462794065475464
MemoryTrain:  epoch  9, batch     5 | loss: 0.7308884Losses:  0.2958897352218628 0.2729819416999817
MemoryTrain:  epoch  9, batch     6 | loss: 0.5688717Losses:  0.3823462128639221 0.1869368553161621
MemoryTrain:  epoch  9, batch     7 | loss: 0.5692831
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 80.94%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 77.03%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.29%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 77.25%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 76.69%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 87.37%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.24%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 86.21%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 85.59%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 85.31%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 84.94%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 84.58%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 84.33%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 84.33%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 83.90%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 83.77%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 83.64%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 83.42%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 84.21%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 83.96%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 83.52%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.41%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 83.31%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 83.02%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 82.30%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 81.99%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 81.62%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.40%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 81.03%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 80.69%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 80.49%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 80.22%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 79.89%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 79.64%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 79.26%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 79.01%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 78.65%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 77.93%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 77.71%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 77.31%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 76.98%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 76.78%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 76.46%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 76.19%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 75.70%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.12%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 73.59%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.21%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.32%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 74.44%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.15%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.93%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.54%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 73.11%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 72.81%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.53%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.10%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.62%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 72.18%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 71.83%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.37%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.91%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 71.50%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 71.48%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 71.38%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 71.07%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.60%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 70.08%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 69.79%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 69.77%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 69.73%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 69.73%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.78%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 71.64%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.70%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 71.23%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 70.97%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 70.75%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 70.51%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 70.23%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 69.99%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 70.97%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.98%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 70.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 73.35%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 73.29%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.16%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.24%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.18%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 73.16%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 73.11%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.24%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 73.42%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 73.20%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 73.10%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 72.97%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 72.89%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 72.76%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 72.59%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 72.36%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 72.17%   [EVAL] batch:  285 | acc: 0.00%,  total acc: 71.92%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 71.67%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.66%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 71.52%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 71.42%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 71.32%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 71.27%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 72.15%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 72.08%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 72.03%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 72.46%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 72.28%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 73.45%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 73.31%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 73.25%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 73.11%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 72.96%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 72.86%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 72.90%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:  363 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 73.14%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 73.14%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 73.13%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 73.15%   
cur_acc:  ['0.9494', '0.7133', '0.7321', '0.8105', '0.7103', '0.7669']
his_acc:  ['0.9494', '0.8275', '0.7922', '0.7798', '0.7604', '0.7315']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  6.917479515075684 0.9495327472686768
CurrentTrain: epoch  0, batch     0 | loss: 7.8670120Losses:  8.831815719604492 1.484439492225647
CurrentTrain: epoch  0, batch     1 | loss: 10.3162556Losses:  9.960390090942383 0.9326461553573608
CurrentTrain: epoch  0, batch     2 | loss: 10.8930359Losses:  9.134886741638184 0.8319449424743652
CurrentTrain: epoch  0, batch     3 | loss: 9.9668312Losses:  7.57234001159668 1.3549001216888428
CurrentTrain: epoch  0, batch     4 | loss: 8.9272404Losses:  6.457064628601074 1.164597511291504
CurrentTrain: epoch  0, batch     5 | loss: 7.6216621Losses:  7.130756378173828 0.3766898810863495
CurrentTrain: epoch  0, batch     6 | loss: 7.5074463Losses:  3.9195022583007812 1.256099820137024
CurrentTrain: epoch  1, batch     0 | loss: 5.1756020Losses:  4.083401203155518 1.4256792068481445
CurrentTrain: epoch  1, batch     1 | loss: 5.5090804Losses:  4.508261680603027 1.2709417343139648
CurrentTrain: epoch  1, batch     2 | loss: 5.7792034Losses:  3.5097265243530273 1.014986276626587
CurrentTrain: epoch  1, batch     3 | loss: 4.5247126Losses:  3.7668275833129883 1.0500311851501465
CurrentTrain: epoch  1, batch     4 | loss: 4.8168588Losses:  3.7710540294647217 1.1123913526535034
CurrentTrain: epoch  1, batch     5 | loss: 4.8834453Losses:  4.392669677734375 0.37516719102859497
CurrentTrain: epoch  1, batch     6 | loss: 4.7678370Losses:  3.2562263011932373 1.006739616394043
CurrentTrain: epoch  2, batch     0 | loss: 4.2629662Losses:  3.7715272903442383 1.294797420501709
CurrentTrain: epoch  2, batch     1 | loss: 5.0663247Losses:  3.3109917640686035 1.0032068490982056
CurrentTrain: epoch  2, batch     2 | loss: 4.3141985Losses:  3.348395347595215 1.1349003314971924
CurrentTrain: epoch  2, batch     3 | loss: 4.4832954Losses:  3.654451370239258 1.163978934288025
CurrentTrain: epoch  2, batch     4 | loss: 4.8184304Losses:  3.5128471851348877 0.8942118883132935
CurrentTrain: epoch  2, batch     5 | loss: 4.4070592Losses:  4.327122211456299 0.39535409212112427
CurrentTrain: epoch  2, batch     6 | loss: 4.7224765Losses:  3.2197647094726562 0.9493851065635681
CurrentTrain: epoch  3, batch     0 | loss: 4.1691499Losses:  3.893195867538452 1.212995171546936
CurrentTrain: epoch  3, batch     1 | loss: 5.1061912Losses:  3.0224814414978027 1.2402734756469727
CurrentTrain: epoch  3, batch     2 | loss: 4.2627549Losses:  3.2003698348999023 0.8035001754760742
CurrentTrain: epoch  3, batch     3 | loss: 4.0038700Losses:  2.8718318939208984 0.7770830392837524
CurrentTrain: epoch  3, batch     4 | loss: 3.6489148Losses:  2.6343178749084473 1.0982412099838257
CurrentTrain: epoch  3, batch     5 | loss: 3.7325592Losses:  2.2422938346862793 0.11023695766925812
CurrentTrain: epoch  3, batch     6 | loss: 2.3525307Losses:  2.745389938354492 0.8564751148223877
CurrentTrain: epoch  4, batch     0 | loss: 3.6018651Losses:  3.6524758338928223 1.0622068643569946
CurrentTrain: epoch  4, batch     1 | loss: 4.7146826Losses:  2.2115354537963867 0.6412898302078247
CurrentTrain: epoch  4, batch     2 | loss: 2.8528252Losses:  2.708543300628662 0.7368220686912537
CurrentTrain: epoch  4, batch     3 | loss: 3.4453654Losses:  2.5922093391418457 0.8182570934295654
CurrentTrain: epoch  4, batch     4 | loss: 3.4104664Losses:  3.0060980319976807 1.0716588497161865
CurrentTrain: epoch  4, batch     5 | loss: 4.0777569Losses:  4.356678485870361 0.42653101682662964
CurrentTrain: epoch  4, batch     6 | loss: 4.7832093Losses:  2.3807482719421387 0.8138658404350281
CurrentTrain: epoch  5, batch     0 | loss: 3.1946142Losses:  3.305612802505493 1.0270326137542725
CurrentTrain: epoch  5, batch     1 | loss: 4.3326454Losses:  2.256525993347168 0.662521243095398
CurrentTrain: epoch  5, batch     2 | loss: 2.9190474Losses:  2.974414825439453 0.7698361873626709
CurrentTrain: epoch  5, batch     3 | loss: 3.7442510Losses:  2.3780646324157715 0.7050561308860779
CurrentTrain: epoch  5, batch     4 | loss: 3.0831208Losses:  2.674403190612793 1.042029619216919
CurrentTrain: epoch  5, batch     5 | loss: 3.7164328Losses:  1.8603678941726685 0.11729234457015991
CurrentTrain: epoch  5, batch     6 | loss: 1.9776602Losses:  2.4641454219818115 0.8699262142181396
CurrentTrain: epoch  6, batch     0 | loss: 3.3340716Losses:  2.8375818729400635 0.7516046166419983
CurrentTrain: epoch  6, batch     1 | loss: 3.5891864Losses:  2.4900641441345215 0.5434517860412598
CurrentTrain: epoch  6, batch     2 | loss: 3.0335159Losses:  2.3440096378326416 0.8830198049545288
CurrentTrain: epoch  6, batch     3 | loss: 3.2270293Losses:  1.9189236164093018 0.45939093828201294
CurrentTrain: epoch  6, batch     4 | loss: 2.3783145Losses:  1.9982645511627197 0.47034671902656555
CurrentTrain: epoch  6, batch     5 | loss: 2.4686112Losses:  3.4468982219696045 0.1421847939491272
CurrentTrain: epoch  6, batch     6 | loss: 3.5890830Losses:  1.8981256484985352 0.4833250343799591
CurrentTrain: epoch  7, batch     0 | loss: 2.3814507Losses:  2.1136722564697266 0.7385598421096802
CurrentTrain: epoch  7, batch     1 | loss: 2.8522320Losses:  2.2701330184936523 0.5873273611068726
CurrentTrain: epoch  7, batch     2 | loss: 2.8574605Losses:  2.6846961975097656 0.6292622089385986
CurrentTrain: epoch  7, batch     3 | loss: 3.3139584Losses:  2.2935357093811035 0.74129718542099
CurrentTrain: epoch  7, batch     4 | loss: 3.0348330Losses:  2.0978200435638428 0.6944062113761902
CurrentTrain: epoch  7, batch     5 | loss: 2.7922263Losses:  2.4861583709716797 0.43671083450317383
CurrentTrain: epoch  7, batch     6 | loss: 2.9228692Losses:  1.9497815370559692 0.7971311807632446
CurrentTrain: epoch  8, batch     0 | loss: 2.7469127Losses:  2.3767526149749756 0.8352729678153992
CurrentTrain: epoch  8, batch     1 | loss: 3.2120256Losses:  1.8915936946868896 0.457138329744339
CurrentTrain: epoch  8, batch     2 | loss: 2.3487320Losses:  2.371776580810547 0.4671022891998291
CurrentTrain: epoch  8, batch     3 | loss: 2.8388789Losses:  2.2065813541412354 0.7501232624053955
CurrentTrain: epoch  8, batch     4 | loss: 2.9567046Losses:  2.1462833881378174 0.7988407611846924
CurrentTrain: epoch  8, batch     5 | loss: 2.9451241Losses:  1.9337549209594727 0.10111743211746216
CurrentTrain: epoch  8, batch     6 | loss: 2.0348723Losses:  1.9719282388687134 0.5637472867965698
CurrentTrain: epoch  9, batch     0 | loss: 2.5356755Losses:  2.036372423171997 0.5688685178756714
CurrentTrain: epoch  9, batch     1 | loss: 2.6052408Losses:  2.6975250244140625 0.7264208793640137
CurrentTrain: epoch  9, batch     2 | loss: 3.4239459Losses:  1.8021273612976074 0.5029043555259705
CurrentTrain: epoch  9, batch     3 | loss: 2.3050318Losses:  2.2010533809661865 0.616057813167572
CurrentTrain: epoch  9, batch     4 | loss: 2.8171113Losses:  1.9088855981826782 0.5560837984085083
CurrentTrain: epoch  9, batch     5 | loss: 2.4649694Losses:  1.9142061471939087 0.36969536542892456
CurrentTrain: epoch  9, batch     6 | loss: 2.2839015
Losses:  6.235015869140625 0.4969659745693207
MemoryTrain:  epoch  0, batch     0 | loss: 6.7319818Losses:  7.750685691833496 0.3823472857475281
MemoryTrain:  epoch  0, batch     1 | loss: 8.1330328Losses:  10.074115753173828 0.7600749731063843
MemoryTrain:  epoch  0, batch     2 | loss: 10.8341904Losses:  8.86931037902832 0.33327460289001465
MemoryTrain:  epoch  0, batch     3 | loss: 9.2025852Losses:  9.918412208557129 0.44512510299682617
MemoryTrain:  epoch  0, batch     4 | loss: 10.3635368Losses:  10.117326736450195 0.5658832788467407
MemoryTrain:  epoch  0, batch     5 | loss: 10.6832104Losses:  10.245288848876953 0.3968953788280487
MemoryTrain:  epoch  0, batch     6 | loss: 10.6421843Losses:  10.344341278076172 0.5262477397918701
MemoryTrain:  epoch  0, batch     7 | loss: 10.8705893Losses:  10.192892074584961 0.1904982030391693
MemoryTrain:  epoch  0, batch     8 | loss: 10.3833904Losses:  0.9601510167121887 0.3207518756389618
MemoryTrain:  epoch  1, batch     0 | loss: 1.2809029Losses:  0.41077420115470886 0.2891343832015991
MemoryTrain:  epoch  1, batch     1 | loss: 0.6999086Losses:  1.2415111064910889 0.350039005279541
MemoryTrain:  epoch  1, batch     2 | loss: 1.5915501Losses:  1.059627652168274 0.43454188108444214
MemoryTrain:  epoch  1, batch     3 | loss: 1.4941695Losses:  0.6628257632255554 0.5134792923927307
MemoryTrain:  epoch  1, batch     4 | loss: 1.1763051Losses:  0.5430916547775269 0.4794645309448242
MemoryTrain:  epoch  1, batch     5 | loss: 1.0225562Losses:  1.3500804901123047 0.42312243580818176
MemoryTrain:  epoch  1, batch     6 | loss: 1.7732029Losses:  1.1168031692504883 0.4436286389827728
MemoryTrain:  epoch  1, batch     7 | loss: 1.5604318Losses:  0.6561664938926697 0.5118865966796875
MemoryTrain:  epoch  1, batch     8 | loss: 1.1680532Losses:  1.2358092069625854 0.3487367033958435
MemoryTrain:  epoch  2, batch     0 | loss: 1.5845459Losses:  0.468547523021698 0.416663259267807
MemoryTrain:  epoch  2, batch     1 | loss: 0.8852108Losses:  0.3803637623786926 0.43140295147895813
MemoryTrain:  epoch  2, batch     2 | loss: 0.8117667Losses:  0.7769549489021301 0.268729031085968
MemoryTrain:  epoch  2, batch     3 | loss: 1.0456840Losses:  0.8720564246177673 0.4645787477493286
MemoryTrain:  epoch  2, batch     4 | loss: 1.3366351Losses:  0.5643150806427002 0.5050204396247864
MemoryTrain:  epoch  2, batch     5 | loss: 1.0693355Losses:  0.846351146697998 0.431191086769104
MemoryTrain:  epoch  2, batch     6 | loss: 1.2775422Losses:  0.6749758720397949 0.5249065160751343
MemoryTrain:  epoch  2, batch     7 | loss: 1.1998824Losses:  0.9136790037155151 0.3559316098690033
MemoryTrain:  epoch  2, batch     8 | loss: 1.2696106Losses:  0.6665587425231934 0.5208268165588379
MemoryTrain:  epoch  3, batch     0 | loss: 1.1873856Losses:  0.3132108449935913 0.3654906153678894
MemoryTrain:  epoch  3, batch     1 | loss: 0.6787015Losses:  0.7202093005180359 0.43440431356430054
MemoryTrain:  epoch  3, batch     2 | loss: 1.1546136Losses:  0.5481913089752197 0.2612304091453552
MemoryTrain:  epoch  3, batch     3 | loss: 0.8094217Losses:  0.7299010753631592 0.3641849160194397
MemoryTrain:  epoch  3, batch     4 | loss: 1.0940859Losses:  0.425514280796051 0.35073375701904297
MemoryTrain:  epoch  3, batch     5 | loss: 0.7762480Losses:  0.5791013240814209 0.5062301158905029
MemoryTrain:  epoch  3, batch     6 | loss: 1.0853314Losses:  0.8273475766181946 0.4567563831806183
MemoryTrain:  epoch  3, batch     7 | loss: 1.2841040Losses:  0.49581247568130493 0.2996898591518402
MemoryTrain:  epoch  3, batch     8 | loss: 0.7955023Losses:  0.6510348916053772 0.33875972032546997
MemoryTrain:  epoch  4, batch     0 | loss: 0.9897946Losses:  0.2684907019138336 0.30711570382118225
MemoryTrain:  epoch  4, batch     1 | loss: 0.5756064Losses:  0.7238337993621826 0.408486545085907
MemoryTrain:  epoch  4, batch     2 | loss: 1.1323204Losses:  0.4617263078689575 0.40374499559402466
MemoryTrain:  epoch  4, batch     3 | loss: 0.8654713Losses:  0.3215823769569397 0.2969885766506195
MemoryTrain:  epoch  4, batch     4 | loss: 0.6185709Losses:  0.27541419863700867 0.316719114780426
MemoryTrain:  epoch  4, batch     5 | loss: 0.5921333Losses:  0.5077217221260071 0.6003620624542236
MemoryTrain:  epoch  4, batch     6 | loss: 1.1080837Losses:  0.7121759653091431 0.4857519865036011
MemoryTrain:  epoch  4, batch     7 | loss: 1.1979280Losses:  0.5103810429573059 0.4416949450969696
MemoryTrain:  epoch  4, batch     8 | loss: 0.9520760Losses:  0.33425113558769226 0.26615577936172485
MemoryTrain:  epoch  5, batch     0 | loss: 0.6004069Losses:  0.6575331687927246 0.4729965031147003
MemoryTrain:  epoch  5, batch     1 | loss: 1.1305296Losses:  0.4866018295288086 0.4560145437717438
MemoryTrain:  epoch  5, batch     2 | loss: 0.9426163Losses:  0.4424186944961548 0.40268200635910034
MemoryTrain:  epoch  5, batch     3 | loss: 0.8451007Losses:  0.3096599280834198 0.31393560767173767
MemoryTrain:  epoch  5, batch     4 | loss: 0.6235955Losses:  0.4829061031341553 0.45359012484550476
MemoryTrain:  epoch  5, batch     5 | loss: 0.9364963Losses:  0.46103760600090027 0.4425120949745178
MemoryTrain:  epoch  5, batch     6 | loss: 0.9035497Losses:  0.34512895345687866 0.38633590936660767
MemoryTrain:  epoch  5, batch     7 | loss: 0.7314649Losses:  0.38855689764022827 0.19280831515789032
MemoryTrain:  epoch  5, batch     8 | loss: 0.5813652Losses:  0.43970781564712524 0.4761202037334442
MemoryTrain:  epoch  6, batch     0 | loss: 0.9158280Losses:  0.3259035050868988 0.29590028524398804
MemoryTrain:  epoch  6, batch     1 | loss: 0.6218038Losses:  0.3861469626426697 0.33884039521217346
MemoryTrain:  epoch  6, batch     2 | loss: 0.7249874Losses:  0.2772566080093384 0.3061920702457428
MemoryTrain:  epoch  6, batch     3 | loss: 0.5834486Losses:  0.6853912472724915 0.32555481791496277
MemoryTrain:  epoch  6, batch     4 | loss: 1.0109460Losses:  0.4685130715370178 0.4748189151287079
MemoryTrain:  epoch  6, batch     5 | loss: 0.9433320Losses:  0.39589130878448486 0.4035775363445282
MemoryTrain:  epoch  6, batch     6 | loss: 0.7994689Losses:  0.36470845341682434 0.42359301447868347
MemoryTrain:  epoch  6, batch     7 | loss: 0.7883015Losses:  0.5552933216094971 0.2515912652015686
MemoryTrain:  epoch  6, batch     8 | loss: 0.8068846Losses:  0.4122714698314667 0.3786343038082123
MemoryTrain:  epoch  7, batch     0 | loss: 0.7909058Losses:  0.30240511894226074 0.3172548711299896
MemoryTrain:  epoch  7, batch     1 | loss: 0.6196600Losses:  0.4570516347885132 0.40780970454216003
MemoryTrain:  epoch  7, batch     2 | loss: 0.8648614Losses:  0.4007835388183594 0.4272765815258026
MemoryTrain:  epoch  7, batch     3 | loss: 0.8280602Losses:  0.44037926197052 0.4758904278278351
MemoryTrain:  epoch  7, batch     4 | loss: 0.9162697Losses:  0.3019979000091553 0.2597713768482208
MemoryTrain:  epoch  7, batch     5 | loss: 0.5617692Losses:  0.3120477795600891 0.2284931242465973
MemoryTrain:  epoch  7, batch     6 | loss: 0.5405409Losses:  0.5216526389122009 0.36625421047210693
MemoryTrain:  epoch  7, batch     7 | loss: 0.8879068Losses:  0.317396879196167 0.16829237341880798
MemoryTrain:  epoch  7, batch     8 | loss: 0.4856893Losses:  0.2813807725906372 0.33700478076934814
MemoryTrain:  epoch  8, batch     0 | loss: 0.6183856Losses:  0.2806991934776306 0.30710846185684204
MemoryTrain:  epoch  8, batch     1 | loss: 0.5878077Losses:  0.43363770842552185 0.4652983248233795
MemoryTrain:  epoch  8, batch     2 | loss: 0.8989360Losses:  0.4674336314201355 0.5397185683250427
MemoryTrain:  epoch  8, batch     3 | loss: 1.0071522Losses:  0.34584856033325195 0.44359084963798523
MemoryTrain:  epoch  8, batch     4 | loss: 0.7894394Losses:  0.32741349935531616 0.37359145283699036
MemoryTrain:  epoch  8, batch     5 | loss: 0.7010050Losses:  0.34268006682395935 0.3166888654232025
MemoryTrain:  epoch  8, batch     6 | loss: 0.6593689Losses:  0.35749977827072144 0.40165525674819946
MemoryTrain:  epoch  8, batch     7 | loss: 0.7591550Losses:  0.35042616724967957 0.2006782591342926
MemoryTrain:  epoch  8, batch     8 | loss: 0.5511044Losses:  0.5131996870040894 0.4508218765258789
MemoryTrain:  epoch  9, batch     0 | loss: 0.9640216Losses:  0.4402320384979248 0.4240698516368866
MemoryTrain:  epoch  9, batch     1 | loss: 0.8643019Losses:  0.2531769871711731 0.2538742423057556
MemoryTrain:  epoch  9, batch     2 | loss: 0.5070512Losses:  0.31228870153427124 0.36095359921455383
MemoryTrain:  epoch  9, batch     3 | loss: 0.6732423Losses:  0.4279356598854065 0.5547577142715454
MemoryTrain:  epoch  9, batch     4 | loss: 0.9826934Losses:  0.28158658742904663 0.2697110176086426
MemoryTrain:  epoch  9, batch     5 | loss: 0.5512976Losses:  0.2869681715965271 0.31038403511047363
MemoryTrain:  epoch  9, batch     6 | loss: 0.5973522Losses:  0.35491931438446045 0.3174028992652893
MemoryTrain:  epoch  9, batch     7 | loss: 0.6723222Losses:  0.31755682826042175 0.1820393204689026
MemoryTrain:  epoch  9, batch     8 | loss: 0.4995961
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 55.71%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 54.17%   [EVAL] batch:   24 | acc: 6.25%,  total acc: 52.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.48%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 48.84%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.10%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 45.47%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 44.17%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 42.74%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 43.36%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 44.51%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 45.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 46.61%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 47.57%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 48.48%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 49.67%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 50.80%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 53.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 54.02%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 56.52%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 57.53%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 57.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 57.84%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 58.53%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 58.96%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.49%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.09%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 60.02%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 59.96%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 60.31%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.72%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.78%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.31%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.32%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.41%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 82.94%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 82.48%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.26%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 82.04%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.90%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 82.15%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 81.57%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 81.41%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 80.79%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 80.20%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 79.99%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 79.56%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 79.43%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 79.02%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 78.76%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 78.44%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 77.75%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 77.31%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 76.95%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 76.60%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 76.38%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 75.90%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 75.51%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 74.94%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 74.63%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 74.45%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 74.15%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 73.99%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.60%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.03%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.99%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 71.15%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 70.96%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 72.62%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.31%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 72.23%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.39%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 72.01%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 71.21%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 70.94%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.32%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 70.85%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 70.38%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 70.05%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 69.60%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.15%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 69.44%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 68.79%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 68.45%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 68.23%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 67.97%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 67.54%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 67.22%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 67.02%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 66.77%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 68.44%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 68.35%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 68.03%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 67.73%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 67.43%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 66.85%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.57%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 66.55%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 67.72%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 69.78%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 70.52%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 70.41%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 70.37%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 70.36%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.28%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.27%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 70.14%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 70.79%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 70.69%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 70.50%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 70.41%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 70.36%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 69.61%   [EVAL] batch:  285 | acc: 0.00%,  total acc: 69.36%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 69.12%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 68.92%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 68.73%   [EVAL] batch:  289 | acc: 18.75%,  total acc: 68.56%   [EVAL] batch:  290 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:  291 | acc: 18.75%,  total acc: 68.21%   [EVAL] batch:  292 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 67.96%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 67.64%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.41%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  313 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 70.48%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 70.35%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 70.29%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 69.89%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 70.35%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 70.27%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 69.98%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 69.86%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 69.73%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 69.64%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 69.49%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 69.34%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 69.61%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 69.68%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 69.73%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 69.80%   [EVAL] batch:  394 | acc: 6.25%,  total acc: 69.64%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 69.51%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 69.14%   [EVAL] batch:  399 | acc: 6.25%,  total acc: 68.98%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 68.83%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 68.67%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 68.50%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 68.33%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 68.18%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 68.01%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 68.60%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 68.76%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 68.78%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 68.72%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 68.71%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 68.61%   
cur_acc:  ['0.9494', '0.7133', '0.7321', '0.8105', '0.7103', '0.7669', '0.5972']
his_acc:  ['0.9494', '0.8275', '0.7922', '0.7798', '0.7604', '0.7315', '0.6861']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  6.869114398956299 0.9879815578460693
CurrentTrain: epoch  0, batch     0 | loss: 7.8570957Losses:  9.560344696044922 1.294664978981018
CurrentTrain: epoch  0, batch     1 | loss: 10.8550100Losses:  10.242161750793457 1.2180092334747314
CurrentTrain: epoch  0, batch     2 | loss: 11.4601707Losses:  7.833813190460205 1.2563116550445557
CurrentTrain: epoch  0, batch     3 | loss: 9.0901251Losses:  7.406100273132324 0.8715766668319702
CurrentTrain: epoch  0, batch     4 | loss: 8.2776766Losses:  6.815513610839844 0.7346932291984558
CurrentTrain: epoch  0, batch     5 | loss: 7.5502067Losses:  4.484096050262451 0.23560354113578796
CurrentTrain: epoch  0, batch     6 | loss: 4.7196994Losses:  3.1522722244262695 0.9266306757926941
CurrentTrain: epoch  1, batch     0 | loss: 4.0789027Losses:  3.9522604942321777 0.6292210817337036
CurrentTrain: epoch  1, batch     1 | loss: 4.5814815Losses:  4.4903435707092285 1.055187463760376
CurrentTrain: epoch  1, batch     2 | loss: 5.5455313Losses:  3.466552495956421 1.1463236808776855
CurrentTrain: epoch  1, batch     3 | loss: 4.6128759Losses:  3.3327207565307617 0.7484081387519836
CurrentTrain: epoch  1, batch     4 | loss: 4.0811291Losses:  4.634610176086426 0.9778724908828735
CurrentTrain: epoch  1, batch     5 | loss: 5.6124825Losses:  1.8936505317687988 1.1920930376163597e-07
CurrentTrain: epoch  1, batch     6 | loss: 1.8936507Losses:  3.5146141052246094 1.0076779127120972
CurrentTrain: epoch  2, batch     0 | loss: 4.5222921Losses:  3.2394943237304688 0.5155131220817566
CurrentTrain: epoch  2, batch     1 | loss: 3.7550075Losses:  3.461787462234497 0.8328017592430115
CurrentTrain: epoch  2, batch     2 | loss: 4.2945890Losses:  3.8824362754821777 0.8092229962348938
CurrentTrain: epoch  2, batch     3 | loss: 4.6916595Losses:  4.111660003662109 0.7057767510414124
CurrentTrain: epoch  2, batch     4 | loss: 4.8174367Losses:  2.856545925140381 0.6550577878952026
CurrentTrain: epoch  2, batch     5 | loss: 3.5116038Losses:  2.5530879497528076 0.18541893362998962
CurrentTrain: epoch  2, batch     6 | loss: 2.7385068Losses:  3.3261685371398926 1.0572192668914795
CurrentTrain: epoch  3, batch     0 | loss: 4.3833876Losses:  3.320516347885132 0.830157995223999
CurrentTrain: epoch  3, batch     1 | loss: 4.1506743Losses:  3.8196935653686523 0.5958417654037476
CurrentTrain: epoch  3, batch     2 | loss: 4.4155354Losses:  3.0419039726257324 0.5974529385566711
CurrentTrain: epoch  3, batch     3 | loss: 3.6393569Losses:  2.450144052505493 0.4423348009586334
CurrentTrain: epoch  3, batch     4 | loss: 2.8924789Losses:  3.2595696449279785 0.9330551624298096
CurrentTrain: epoch  3, batch     5 | loss: 4.1926250Losses:  3.544487476348877 0.6463477611541748
CurrentTrain: epoch  3, batch     6 | loss: 4.1908350Losses:  2.612994909286499 0.3949262797832489
CurrentTrain: epoch  4, batch     0 | loss: 3.0079212Losses:  2.809689998626709 0.7534907460212708
CurrentTrain: epoch  4, batch     1 | loss: 3.5631807Losses:  2.758338689804077 0.641410231590271
CurrentTrain: epoch  4, batch     2 | loss: 3.3997488Losses:  2.369528293609619 0.6732598543167114
CurrentTrain: epoch  4, batch     3 | loss: 3.0427880Losses:  4.200857639312744 0.9570088386535645
CurrentTrain: epoch  4, batch     4 | loss: 5.1578665Losses:  2.3250484466552734 0.5289402008056641
CurrentTrain: epoch  4, batch     5 | loss: 2.8539886Losses:  3.6969010829925537 0.2687898576259613
CurrentTrain: epoch  4, batch     6 | loss: 3.9656909Losses:  2.523637294769287 0.8470051288604736
CurrentTrain: epoch  5, batch     0 | loss: 3.3706424Losses:  2.7524890899658203 0.5440914034843445
CurrentTrain: epoch  5, batch     1 | loss: 3.2965806Losses:  3.321122884750366 0.4847819209098816
CurrentTrain: epoch  5, batch     2 | loss: 3.8059049Losses:  2.8017730712890625 0.7122188806533813
CurrentTrain: epoch  5, batch     3 | loss: 3.5139918Losses:  2.031097650527954 0.47570520639419556
CurrentTrain: epoch  5, batch     4 | loss: 2.5068028Losses:  2.3533692359924316 0.7130963802337646
CurrentTrain: epoch  5, batch     5 | loss: 3.0664656Losses:  1.8469555377960205 0.0838196724653244
CurrentTrain: epoch  5, batch     6 | loss: 1.9307752Losses:  2.3421871662139893 0.8067235946655273
CurrentTrain: epoch  6, batch     0 | loss: 3.1489108Losses:  2.367572784423828 0.6274614334106445
CurrentTrain: epoch  6, batch     1 | loss: 2.9950342Losses:  2.549593687057495 0.39090728759765625
CurrentTrain: epoch  6, batch     2 | loss: 2.9405010Losses:  2.1914539337158203 0.7100570797920227
CurrentTrain: epoch  6, batch     3 | loss: 2.9015110Losses:  2.021066188812256 0.27075719833374023
CurrentTrain: epoch  6, batch     4 | loss: 2.2918234Losses:  2.2370362281799316 0.38046175241470337
CurrentTrain: epoch  6, batch     5 | loss: 2.6174979Losses:  1.8472719192504883 0.18017318844795227
CurrentTrain: epoch  6, batch     6 | loss: 2.0274451Losses:  2.18520450592041 0.49523255228996277
CurrentTrain: epoch  7, batch     0 | loss: 2.6804371Losses:  2.3029351234436035 0.6285570859909058
CurrentTrain: epoch  7, batch     1 | loss: 2.9314923Losses:  1.9801185131072998 0.34704527258872986
CurrentTrain: epoch  7, batch     2 | loss: 2.3271637Losses:  2.2044878005981445 0.4380335807800293
CurrentTrain: epoch  7, batch     3 | loss: 2.6425214Losses:  2.080645799636841 0.5133358240127563
CurrentTrain: epoch  7, batch     4 | loss: 2.5939817Losses:  2.047919750213623 0.4654243588447571
CurrentTrain: epoch  7, batch     5 | loss: 2.5133440Losses:  1.9683358669281006 0.1072784960269928
CurrentTrain: epoch  7, batch     6 | loss: 2.0756145Losses:  2.0008435249328613 0.46353399753570557
CurrentTrain: epoch  8, batch     0 | loss: 2.4643774Losses:  2.055300712585449 0.32956889271736145
CurrentTrain: epoch  8, batch     1 | loss: 2.3848696Losses:  1.9160420894622803 0.3482503890991211
CurrentTrain: epoch  8, batch     2 | loss: 2.2642925Losses:  1.9541611671447754 0.4970294237136841
CurrentTrain: epoch  8, batch     3 | loss: 2.4511905Losses:  1.869443655014038 0.4437534213066101
CurrentTrain: epoch  8, batch     4 | loss: 2.3131971Losses:  1.88606858253479 0.42892205715179443
CurrentTrain: epoch  8, batch     5 | loss: 2.3149905Losses:  1.9242877960205078 0.04949461668729782
CurrentTrain: epoch  8, batch     6 | loss: 1.9737824Losses:  1.825467586517334 0.3878318667411804
CurrentTrain: epoch  9, batch     0 | loss: 2.2132995Losses:  1.7526825666427612 0.23500359058380127
CurrentTrain: epoch  9, batch     1 | loss: 1.9876862Losses:  1.920480728149414 0.47586122155189514
CurrentTrain: epoch  9, batch     2 | loss: 2.3963420Losses:  1.8779847621917725 0.47826528549194336
CurrentTrain: epoch  9, batch     3 | loss: 2.3562500Losses:  1.8633396625518799 0.4487744867801666
CurrentTrain: epoch  9, batch     4 | loss: 2.3121142Losses:  1.8489837646484375 0.40023455023765564
CurrentTrain: epoch  9, batch     5 | loss: 2.2492182Losses:  1.874910593032837 0.11327981948852539
CurrentTrain: epoch  9, batch     6 | loss: 1.9881904
Losses:  6.00595760345459 0.2764360010623932
MemoryTrain:  epoch  0, batch     0 | loss: 6.2823935Losses:  8.32683277130127 0.37869662046432495
MemoryTrain:  epoch  0, batch     1 | loss: 8.7055292Losses:  9.408792495727539 0.365943968296051
MemoryTrain:  epoch  0, batch     2 | loss: 9.7747364Losses:  9.761287689208984 0.49982646107673645
MemoryTrain:  epoch  0, batch     3 | loss: 10.2611141Losses:  9.43602180480957 0.48223909735679626
MemoryTrain:  epoch  0, batch     4 | loss: 9.9182606Losses:  10.63917064666748 0.8156867623329163
MemoryTrain:  epoch  0, batch     5 | loss: 11.4548578Losses:  10.027532577514648 0.38697075843811035
MemoryTrain:  epoch  0, batch     6 | loss: 10.4145031Losses:  10.47186279296875 0.37706324458122253
MemoryTrain:  epoch  0, batch     7 | loss: 10.8489256Losses:  10.642587661743164 0.28997671604156494
MemoryTrain:  epoch  0, batch     8 | loss: 10.9325647Losses:  10.933029174804688 0.38783255219459534
MemoryTrain:  epoch  0, batch     9 | loss: 11.3208618Losses:  1.1898407936096191 0.3315351605415344
MemoryTrain:  epoch  1, batch     0 | loss: 1.5213759Losses:  1.3209285736083984 0.4818495213985443
MemoryTrain:  epoch  1, batch     1 | loss: 1.8027781Losses:  0.45415693521499634 0.32640984654426575
MemoryTrain:  epoch  1, batch     2 | loss: 0.7805668Losses:  0.6866569519042969 0.3062731623649597
MemoryTrain:  epoch  1, batch     3 | loss: 0.9929301Losses:  0.5797045230865479 0.4104377031326294
MemoryTrain:  epoch  1, batch     4 | loss: 0.9901422Losses:  0.7466000318527222 0.34616655111312866
MemoryTrain:  epoch  1, batch     5 | loss: 1.0927665Losses:  1.1027419567108154 0.3173592984676361
MemoryTrain:  epoch  1, batch     6 | loss: 1.4201013Losses:  1.3765226602554321 0.6471196413040161
MemoryTrain:  epoch  1, batch     7 | loss: 2.0236423Losses:  0.8909270167350769 0.4646001160144806
MemoryTrain:  epoch  1, batch     8 | loss: 1.3555272Losses:  0.6304223537445068 0.37899836897850037
MemoryTrain:  epoch  1, batch     9 | loss: 1.0094208Losses:  0.5779709815979004 0.3945580720901489
MemoryTrain:  epoch  2, batch     0 | loss: 0.9725291Losses:  0.5277115702629089 0.2582640051841736
MemoryTrain:  epoch  2, batch     1 | loss: 0.7859756Losses:  1.027529001235962 0.38795730471611023
MemoryTrain:  epoch  2, batch     2 | loss: 1.4154863Losses:  0.8481532335281372 0.6152583360671997
MemoryTrain:  epoch  2, batch     3 | loss: 1.4634116Losses:  0.514989972114563 0.3474346101284027
MemoryTrain:  epoch  2, batch     4 | loss: 0.8624246Losses:  0.5774645805358887 0.37651845812797546
MemoryTrain:  epoch  2, batch     5 | loss: 0.9539831Losses:  1.0695509910583496 0.5443272590637207
MemoryTrain:  epoch  2, batch     6 | loss: 1.6138783Losses:  0.4715458154678345 0.2930310368537903
MemoryTrain:  epoch  2, batch     7 | loss: 0.7645769Losses:  0.5467180609703064 0.2777460217475891
MemoryTrain:  epoch  2, batch     8 | loss: 0.8244641Losses:  0.3474504351615906 0.25960907340049744
MemoryTrain:  epoch  2, batch     9 | loss: 0.6070595Losses:  0.5456151962280273 0.3868836760520935
MemoryTrain:  epoch  3, batch     0 | loss: 0.9324989Losses:  0.5258302092552185 0.4447329044342041
MemoryTrain:  epoch  3, batch     1 | loss: 0.9705631Losses:  0.7164775133132935 0.4065205156803131
MemoryTrain:  epoch  3, batch     2 | loss: 1.1229980Losses:  0.5902705192565918 0.5440804958343506
MemoryTrain:  epoch  3, batch     3 | loss: 1.1343510Losses:  0.3984716534614563 0.3186243176460266
MemoryTrain:  epoch  3, batch     4 | loss: 0.7170960Losses:  0.4850984215736389 0.35124772787094116
MemoryTrain:  epoch  3, batch     5 | loss: 0.8363461Losses:  0.38986262679100037 0.3411310017108917
MemoryTrain:  epoch  3, batch     6 | loss: 0.7309936Losses:  0.8019534349441528 0.39665400981903076
MemoryTrain:  epoch  3, batch     7 | loss: 1.1986074Losses:  0.346402645111084 0.17905598878860474
MemoryTrain:  epoch  3, batch     8 | loss: 0.5254586Losses:  0.9484826326370239 0.4393746256828308
MemoryTrain:  epoch  3, batch     9 | loss: 1.3878572Losses:  0.4035238027572632 0.31817716360092163
MemoryTrain:  epoch  4, batch     0 | loss: 0.7217010Losses:  0.7005694508552551 0.40196943283081055
MemoryTrain:  epoch  4, batch     1 | loss: 1.1025388Losses:  0.3780119717121124 0.2783079147338867
MemoryTrain:  epoch  4, batch     2 | loss: 0.6563199Losses:  0.6388126611709595 0.4417208433151245
MemoryTrain:  epoch  4, batch     3 | loss: 1.0805335Losses:  0.6389132142066956 0.34198325872421265
MemoryTrain:  epoch  4, batch     4 | loss: 0.9808965Losses:  0.4258102774620056 0.2692756652832031
MemoryTrain:  epoch  4, batch     5 | loss: 0.6950859Losses:  0.5850664377212524 0.4345816671848297
MemoryTrain:  epoch  4, batch     6 | loss: 1.0196481Losses:  0.347125768661499 0.37938201427459717
MemoryTrain:  epoch  4, batch     7 | loss: 0.7265078Losses:  0.45423513650894165 0.43536508083343506
MemoryTrain:  epoch  4, batch     8 | loss: 0.8896002Losses:  0.6397503614425659 0.5036464333534241
MemoryTrain:  epoch  4, batch     9 | loss: 1.1433969Losses:  0.3262118399143219 0.19891029596328735
MemoryTrain:  epoch  5, batch     0 | loss: 0.5251222Losses:  0.4426792860031128 0.4124160408973694
MemoryTrain:  epoch  5, batch     1 | loss: 0.8550953Losses:  0.3918904662132263 0.3420864939689636
MemoryTrain:  epoch  5, batch     2 | loss: 0.7339770Losses:  0.42839449644088745 0.41547510027885437
MemoryTrain:  epoch  5, batch     3 | loss: 0.8438696Losses:  0.6735467910766602 0.4396957457065582
MemoryTrain:  epoch  5, batch     4 | loss: 1.1132425Losses:  0.36033231019973755 0.2384069710969925
MemoryTrain:  epoch  5, batch     5 | loss: 0.5987393Losses:  0.4350321888923645 0.3520067036151886
MemoryTrain:  epoch  5, batch     6 | loss: 0.7870389Losses:  0.6917029619216919 0.5253605842590332
MemoryTrain:  epoch  5, batch     7 | loss: 1.2170635Losses:  0.4517750144004822 0.323516845703125
MemoryTrain:  epoch  5, batch     8 | loss: 0.7752919Losses:  0.4720805883407593 0.36272355914115906
MemoryTrain:  epoch  5, batch     9 | loss: 0.8348042Losses:  0.416313499212265 0.3917033076286316
MemoryTrain:  epoch  6, batch     0 | loss: 0.8080168Losses:  0.3191074728965759 0.3666849732398987
MemoryTrain:  epoch  6, batch     1 | loss: 0.6857924Losses:  0.35510244965553284 0.26441317796707153
MemoryTrain:  epoch  6, batch     2 | loss: 0.6195157Losses:  0.42728161811828613 0.2866000831127167
MemoryTrain:  epoch  6, batch     3 | loss: 0.7138817Losses:  0.38840728998184204 0.32118332386016846
MemoryTrain:  epoch  6, batch     4 | loss: 0.7095906Losses:  0.5303881168365479 0.31330186128616333
MemoryTrain:  epoch  6, batch     5 | loss: 0.8436900Losses:  0.5525028109550476 0.6205311417579651
MemoryTrain:  epoch  6, batch     6 | loss: 1.1730340Losses:  0.470073401927948 0.4374012351036072
MemoryTrain:  epoch  6, batch     7 | loss: 0.9074746Losses:  0.3465837836265564 0.28837698698043823
MemoryTrain:  epoch  6, batch     8 | loss: 0.6349608Losses:  0.4902915954589844 0.40996164083480835
MemoryTrain:  epoch  6, batch     9 | loss: 0.9002532Losses:  0.46768873929977417 0.39619705080986023
MemoryTrain:  epoch  7, batch     0 | loss: 0.8638858Losses:  0.4764575958251953 0.4426532983779907
MemoryTrain:  epoch  7, batch     1 | loss: 0.9191109Losses:  0.39197224378585815 0.317904531955719
MemoryTrain:  epoch  7, batch     2 | loss: 0.7098768Losses:  0.5141786336898804 0.40533319115638733
MemoryTrain:  epoch  7, batch     3 | loss: 0.9195118Losses:  0.364433228969574 0.22535276412963867
MemoryTrain:  epoch  7, batch     4 | loss: 0.5897860Losses:  0.38143301010131836 0.32742899656295776
MemoryTrain:  epoch  7, batch     5 | loss: 0.7088620Losses:  0.46691280603408813 0.44124603271484375
MemoryTrain:  epoch  7, batch     6 | loss: 0.9081588Losses:  0.41444382071495056 0.35547834634780884
MemoryTrain:  epoch  7, batch     7 | loss: 0.7699221Losses:  0.3863352835178375 0.32796066999435425
MemoryTrain:  epoch  7, batch     8 | loss: 0.7142960Losses:  0.34680381417274475 0.2824645936489105
MemoryTrain:  epoch  7, batch     9 | loss: 0.6292684Losses:  0.40935400128364563 0.4026072025299072
MemoryTrain:  epoch  8, batch     0 | loss: 0.8119612Losses:  0.2866228222846985 0.29627525806427
MemoryTrain:  epoch  8, batch     1 | loss: 0.5828981Losses:  0.4560818672180176 0.42588889598846436
MemoryTrain:  epoch  8, batch     2 | loss: 0.8819708Losses:  0.3896156847476959 0.29249244928359985
MemoryTrain:  epoch  8, batch     3 | loss: 0.6821082Losses:  0.44157496094703674 0.27485328912734985
MemoryTrain:  epoch  8, batch     4 | loss: 0.7164283Losses:  0.4514693319797516 0.3297288417816162
MemoryTrain:  epoch  8, batch     5 | loss: 0.7811981Losses:  0.5076353549957275 0.47487324476242065
MemoryTrain:  epoch  8, batch     6 | loss: 0.9825086Losses:  0.3115367293357849 0.25358307361602783
MemoryTrain:  epoch  8, batch     7 | loss: 0.5651198Losses:  0.3766252398490906 0.3509829640388489
MemoryTrain:  epoch  8, batch     8 | loss: 0.7276082Losses:  0.39789125323295593 0.3392374515533447
MemoryTrain:  epoch  8, batch     9 | loss: 0.7371287Losses:  0.47773492336273193 0.3664833903312683
MemoryTrain:  epoch  9, batch     0 | loss: 0.8442183Losses:  0.31778454780578613 0.265661358833313
MemoryTrain:  epoch  9, batch     1 | loss: 0.5834459Losses:  0.3750220537185669 0.34982597827911377
MemoryTrain:  epoch  9, batch     2 | loss: 0.7248480Losses:  0.3711085915565491 0.3072996735572815
MemoryTrain:  epoch  9, batch     3 | loss: 0.6784083Losses:  0.47743529081344604 0.5524033308029175
MemoryTrain:  epoch  9, batch     4 | loss: 1.0298386Losses:  0.4242801368236542 0.35271549224853516
MemoryTrain:  epoch  9, batch     5 | loss: 0.7769957Losses:  0.3732737898826599 0.2936893105506897
MemoryTrain:  epoch  9, batch     6 | loss: 0.6669631Losses:  0.2746640145778656 0.28710421919822693
MemoryTrain:  epoch  9, batch     7 | loss: 0.5617682Losses:  0.3919728994369507 0.3852118253707886
MemoryTrain:  epoch  9, batch     8 | loss: 0.7771847Losses:  0.4214661717414856 0.31448277831077576
MemoryTrain:  epoch  9, batch     9 | loss: 0.7359489
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 76.33%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 76.51%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 75.74%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 74.49%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 74.19%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 73.41%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.61%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 83.77%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 82.31%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 81.98%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 80.66%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 80.38%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 79.83%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 79.66%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 79.60%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 79.35%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 80.18%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 79.63%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 79.19%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 78.66%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 78.24%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 77.72%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 77.23%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 76.85%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 76.47%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 76.04%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 75.14%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 74.66%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 74.27%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 74.14%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 73.89%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 73.60%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 72.94%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 72.71%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 72.26%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 71.96%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 71.41%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.93%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 69.99%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 69.47%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 71.33%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 71.06%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 70.75%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 70.64%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 70.22%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 69.81%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.37%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 69.10%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.54%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.08%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.63%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 68.26%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.82%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.39%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.40%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.68%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 67.42%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 66.65%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 66.12%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 65.88%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 65.48%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 64.79%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 64.60%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.41%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 66.30%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.08%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 65.31%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.03%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 64.78%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 64.53%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 67.67%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 68.40%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 68.59%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  281 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:  283 | acc: 0.00%,  total acc: 67.98%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 67.79%   [EVAL] batch:  285 | acc: 0.00%,  total acc: 67.55%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.31%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 67.21%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 67.16%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  291 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  293 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 66.89%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 66.77%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 66.68%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 66.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.60%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.51%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 67.45%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 67.87%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.72%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 67.57%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 67.35%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 68.43%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 68.34%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 68.20%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 68.03%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 67.92%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 68.36%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 68.31%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 68.19%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.95%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 67.81%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 67.73%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 67.55%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 67.72%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 67.63%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  398 | acc: 25.00%,  total acc: 67.48%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 67.24%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 67.09%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.92%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 66.75%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 66.60%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 66.44%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  419 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  420 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:  421 | acc: 50.00%,  total acc: 66.90%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 66.78%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  425 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 66.68%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 66.66%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  438 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  440 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  442 | acc: 50.00%,  total acc: 66.34%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 66.29%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  446 | acc: 43.75%,  total acc: 66.30%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  457 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  458 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  459 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  460 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  461 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  462 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  480 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 67.65%   [EVAL] batch:  482 | acc: 37.50%,  total acc: 67.59%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:  495 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  496 | acc: 31.25%,  total acc: 67.67%   [EVAL] batch:  497 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  498 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  499 | acc: 43.75%,  total acc: 67.56%   
cur_acc:  ['0.9494', '0.7133', '0.7321', '0.8105', '0.7103', '0.7669', '0.5972', '0.7341']
his_acc:  ['0.9494', '0.8275', '0.7922', '0.7798', '0.7604', '0.7315', '0.6861', '0.6756']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.799554824829102 1.8504819869995117
CurrentTrain: epoch  0, batch     0 | loss: 13.6500368Losses:  12.585941314697266 1.5289442539215088
CurrentTrain: epoch  0, batch     1 | loss: 14.1148853Losses:  12.898881912231445 1.4364216327667236
CurrentTrain: epoch  0, batch     2 | loss: 14.3353033Losses:  12.953125953674316 1.6532549858093262
CurrentTrain: epoch  0, batch     3 | loss: 14.6063805Losses:  13.681381225585938 1.5900859832763672
CurrentTrain: epoch  0, batch     4 | loss: 15.2714672Losses:  12.879151344299316 1.1876764297485352
CurrentTrain: epoch  0, batch     5 | loss: 14.0668278Losses:  13.319050788879395 1.7341910600662231
CurrentTrain: epoch  0, batch     6 | loss: 15.0532417Losses:  13.21700668334961 1.2937893867492676
CurrentTrain: epoch  0, batch     7 | loss: 14.5107956Losses:  12.917997360229492 1.3912608623504639
CurrentTrain: epoch  0, batch     8 | loss: 14.3092585Losses:  12.459152221679688 1.449802279472351
CurrentTrain: epoch  0, batch     9 | loss: 13.9089546Losses:  12.64280891418457 1.144547939300537
CurrentTrain: epoch  0, batch    10 | loss: 13.7873573Losses:  12.736913681030273 1.7222340106964111
CurrentTrain: epoch  0, batch    11 | loss: 14.4591475Losses:  12.665070533752441 1.3465750217437744
CurrentTrain: epoch  0, batch    12 | loss: 14.0116453Losses:  12.3522310256958 1.450066089630127
CurrentTrain: epoch  0, batch    13 | loss: 13.8022976Losses:  12.233546257019043 1.1509783267974854
CurrentTrain: epoch  0, batch    14 | loss: 13.3845243Losses:  11.931953430175781 1.3198165893554688
CurrentTrain: epoch  0, batch    15 | loss: 13.2517700Losses:  12.187524795532227 1.5757231712341309
CurrentTrain: epoch  0, batch    16 | loss: 13.7632484Losses:  12.353458404541016 1.8419462442398071
CurrentTrain: epoch  0, batch    17 | loss: 14.1954050Losses:  12.03981876373291 1.4710978269577026
CurrentTrain: epoch  0, batch    18 | loss: 13.5109167Losses:  11.705850601196289 1.5192698240280151
CurrentTrain: epoch  0, batch    19 | loss: 13.2251205Losses:  12.331331253051758 1.3714301586151123
CurrentTrain: epoch  0, batch    20 | loss: 13.7027617Losses:  12.005794525146484 1.4227981567382812
CurrentTrain: epoch  0, batch    21 | loss: 13.4285927Losses:  12.072927474975586 0.9281606078147888
CurrentTrain: epoch  0, batch    22 | loss: 13.0010881Losses:  11.95488166809082 1.318530559539795
CurrentTrain: epoch  0, batch    23 | loss: 13.2734127Losses:  11.90846061706543 1.4670308828353882
CurrentTrain: epoch  0, batch    24 | loss: 13.3754911Losses:  11.86441421508789 1.1283899545669556
CurrentTrain: epoch  0, batch    25 | loss: 12.9928045Losses:  11.971126556396484 1.168591022491455
CurrentTrain: epoch  0, batch    26 | loss: 13.1397171Losses:  11.473532676696777 1.2415995597839355
CurrentTrain: epoch  0, batch    27 | loss: 12.7151318Losses:  11.244888305664062 0.9536404013633728
CurrentTrain: epoch  0, batch    28 | loss: 12.1985283Losses:  11.250259399414062 0.9268455505371094
CurrentTrain: epoch  0, batch    29 | loss: 12.1771049Losses:  11.397857666015625 0.9179630279541016
CurrentTrain: epoch  0, batch    30 | loss: 12.3158207Losses:  11.654203414916992 1.1317490339279175
CurrentTrain: epoch  0, batch    31 | loss: 12.7859526Losses:  11.615962982177734 0.999319851398468
CurrentTrain: epoch  0, batch    32 | loss: 12.6152830Losses:  10.748696327209473 1.2659622430801392
CurrentTrain: epoch  0, batch    33 | loss: 12.0146589Losses:  10.896636962890625 0.8593210577964783
CurrentTrain: epoch  0, batch    34 | loss: 11.7559576Losses:  10.913618087768555 1.1823318004608154
CurrentTrain: epoch  0, batch    35 | loss: 12.0959501Losses:  10.439228057861328 0.7403584718704224
CurrentTrain: epoch  0, batch    36 | loss: 11.1795864Losses:  10.655970573425293 1.454405665397644
CurrentTrain: epoch  0, batch    37 | loss: 12.1103764Losses:  10.962376594543457 0.8189948201179504
CurrentTrain: epoch  0, batch    38 | loss: 11.7813711Losses:  11.277519226074219 1.1530883312225342
CurrentTrain: epoch  0, batch    39 | loss: 12.4306078Losses:  10.615348815917969 0.9902957677841187
CurrentTrain: epoch  0, batch    40 | loss: 11.6056442Losses:  9.976240158081055 0.8914893865585327
CurrentTrain: epoch  0, batch    41 | loss: 10.8677292Losses:  9.873384475708008 0.6785358190536499
CurrentTrain: epoch  0, batch    42 | loss: 10.5519199Losses:  10.4367036819458 1.020993947982788
CurrentTrain: epoch  0, batch    43 | loss: 11.4576979Losses:  9.817588806152344 0.8674067258834839
CurrentTrain: epoch  0, batch    44 | loss: 10.6849957Losses:  10.655282974243164 0.8026043772697449
CurrentTrain: epoch  0, batch    45 | loss: 11.4578876Losses:  10.0783109664917 0.8354926109313965
CurrentTrain: epoch  0, batch    46 | loss: 10.9138031Losses:  10.087711334228516 0.6899262070655823
CurrentTrain: epoch  0, batch    47 | loss: 10.7776375Losses:  10.037895202636719 0.7232505679130554
CurrentTrain: epoch  0, batch    48 | loss: 10.7611456Losses:  10.061946868896484 0.8345056176185608
CurrentTrain: epoch  0, batch    49 | loss: 10.8964529Losses:  10.256132125854492 1.0364381074905396
CurrentTrain: epoch  0, batch    50 | loss: 11.2925701Losses:  10.116449356079102 0.9146199822425842
CurrentTrain: epoch  0, batch    51 | loss: 11.0310698Losses:  9.872814178466797 1.0450270175933838
CurrentTrain: epoch  0, batch    52 | loss: 10.9178410Losses:  10.486848831176758 0.8905354738235474
CurrentTrain: epoch  0, batch    53 | loss: 11.3773842Losses:  10.017586708068848 1.0946983098983765
CurrentTrain: epoch  0, batch    54 | loss: 11.1122847Losses:  10.230329513549805 1.064424991607666
CurrentTrain: epoch  0, batch    55 | loss: 11.2947540Losses:  9.9674072265625 0.9534462690353394
CurrentTrain: epoch  0, batch    56 | loss: 10.9208536Losses:  9.618760108947754 0.8859704732894897
CurrentTrain: epoch  0, batch    57 | loss: 10.5047302Losses:  9.101016998291016 0.7412773966789246
CurrentTrain: epoch  0, batch    58 | loss: 9.8422947Losses:  9.979427337646484 0.9493962526321411
CurrentTrain: epoch  0, batch    59 | loss: 10.9288235Losses:  9.565415382385254 0.8001428842544556
CurrentTrain: epoch  0, batch    60 | loss: 10.3655586Losses:  9.154088973999023 0.86461341381073
CurrentTrain: epoch  0, batch    61 | loss: 10.0187025Losses:  8.90920639038086 0.9881666898727417
CurrentTrain: epoch  0, batch    62 | loss: 9.8973732Losses:  9.892410278320312 1.3534302711486816
CurrentTrain: epoch  0, batch    63 | loss: 11.2458401Losses:  9.640661239624023 1.0066945552825928
CurrentTrain: epoch  0, batch    64 | loss: 10.6473560Losses:  9.224308967590332 1.0967330932617188
CurrentTrain: epoch  0, batch    65 | loss: 10.3210421Losses:  9.577011108398438 1.274455189704895
CurrentTrain: epoch  0, batch    66 | loss: 10.8514662Losses:  9.481180191040039 0.9881201982498169
CurrentTrain: epoch  0, batch    67 | loss: 10.4693003Losses:  9.032881736755371 1.0555953979492188
CurrentTrain: epoch  0, batch    68 | loss: 10.0884771Losses:  8.310221672058105 0.5747276544570923
CurrentTrain: epoch  0, batch    69 | loss: 8.8849497Losses:  9.120016098022461 1.0569313764572144
CurrentTrain: epoch  0, batch    70 | loss: 10.1769476Losses:  9.190316200256348 0.6923786401748657
CurrentTrain: epoch  0, batch    71 | loss: 9.8826952Losses:  9.648984909057617 0.7870358228683472
CurrentTrain: epoch  0, batch    72 | loss: 10.4360209Losses:  9.235107421875 0.965096116065979
CurrentTrain: epoch  0, batch    73 | loss: 10.2002039Losses:  9.104522705078125 0.8347692489624023
CurrentTrain: epoch  0, batch    74 | loss: 9.9392920Losses:  8.472755432128906 0.7231400609016418
CurrentTrain: epoch  0, batch    75 | loss: 9.1958952Losses:  8.732990264892578 0.8749451041221619
CurrentTrain: epoch  0, batch    76 | loss: 9.6079350Losses:  8.713420867919922 0.7849290370941162
CurrentTrain: epoch  0, batch    77 | loss: 9.4983501Losses:  8.342105865478516 0.8771126866340637
CurrentTrain: epoch  0, batch    78 | loss: 9.2192183Losses:  8.714166641235352 0.8098404407501221
CurrentTrain: epoch  0, batch    79 | loss: 9.5240068Losses:  7.8709564208984375 0.6375887393951416
CurrentTrain: epoch  0, batch    80 | loss: 8.5085449Losses:  8.897909164428711 0.9760949611663818
CurrentTrain: epoch  0, batch    81 | loss: 9.8740044Losses:  8.376726150512695 0.7731998562812805
CurrentTrain: epoch  0, batch    82 | loss: 9.1499262Losses:  8.11980152130127 0.773872971534729
CurrentTrain: epoch  0, batch    83 | loss: 8.8936749Losses:  7.581886291503906 0.475633442401886
CurrentTrain: epoch  0, batch    84 | loss: 8.0575199Losses:  8.216448783874512 0.9241070747375488
CurrentTrain: epoch  0, batch    85 | loss: 9.1405563Losses:  8.007363319396973 0.642278790473938
CurrentTrain: epoch  0, batch    86 | loss: 8.6496420Losses:  7.4810333251953125 0.4821932911872864
CurrentTrain: epoch  0, batch    87 | loss: 7.9632268Losses:  8.084346771240234 0.6695063710212708
CurrentTrain: epoch  0, batch    88 | loss: 8.7538528Losses:  7.911115646362305 0.6781355142593384
CurrentTrain: epoch  0, batch    89 | loss: 8.5892515Losses:  8.280961990356445 0.6782342195510864
CurrentTrain: epoch  0, batch    90 | loss: 8.9591961Losses:  8.024809837341309 0.8540233969688416
CurrentTrain: epoch  0, batch    91 | loss: 8.8788328Losses:  7.6769609451293945 0.7051236629486084
CurrentTrain: epoch  0, batch    92 | loss: 8.3820848Losses:  7.612703323364258 0.4612836241722107
CurrentTrain: epoch  0, batch    93 | loss: 8.0739870Losses:  7.701524257659912 0.6169175505638123
CurrentTrain: epoch  0, batch    94 | loss: 8.3184414Losses:  7.650496006011963 0.6831308603286743
CurrentTrain: epoch  0, batch    95 | loss: 8.3336267Losses:  7.6850457191467285 0.6950479745864868
CurrentTrain: epoch  0, batch    96 | loss: 8.3800936Losses:  7.3717145919799805 0.74695885181427
CurrentTrain: epoch  0, batch    97 | loss: 8.1186733Losses:  6.8518476486206055 0.5922930240631104
CurrentTrain: epoch  0, batch    98 | loss: 7.4441404Losses:  7.508144855499268 0.8260564804077148
CurrentTrain: epoch  0, batch    99 | loss: 8.3342018Losses:  7.23580265045166 0.5921230316162109
CurrentTrain: epoch  0, batch   100 | loss: 7.8279257Losses:  8.114913940429688 0.5055185556411743
CurrentTrain: epoch  0, batch   101 | loss: 8.6204329Losses:  7.128107070922852 0.5470162630081177
CurrentTrain: epoch  0, batch   102 | loss: 7.6751232Losses:  6.667093276977539 0.5161113142967224
CurrentTrain: epoch  0, batch   103 | loss: 7.1832047Losses:  6.322032928466797 0.6576106548309326
CurrentTrain: epoch  0, batch   104 | loss: 6.9796438Losses:  7.093270301818848 0.6871974468231201
CurrentTrain: epoch  0, batch   105 | loss: 7.7804680Losses:  6.953413009643555 0.3670187294483185
CurrentTrain: epoch  0, batch   106 | loss: 7.3204317Losses:  6.634853363037109 0.3983035087585449
CurrentTrain: epoch  0, batch   107 | loss: 7.0331569Losses:  7.091100692749023 0.5191143751144409
CurrentTrain: epoch  0, batch   108 | loss: 7.6102152Losses:  6.591209411621094 0.6258376836776733
CurrentTrain: epoch  0, batch   109 | loss: 7.2170472Losses:  7.493245601654053 0.999335765838623
CurrentTrain: epoch  0, batch   110 | loss: 8.4925814Losses:  7.016304016113281 0.6784216165542603
CurrentTrain: epoch  0, batch   111 | loss: 7.6947255Losses:  6.552859783172607 0.5406943559646606
CurrentTrain: epoch  0, batch   112 | loss: 7.0935540Losses:  6.170330047607422 0.581889808177948
CurrentTrain: epoch  0, batch   113 | loss: 6.7522197Losses:  6.019717216491699 0.40926289558410645
CurrentTrain: epoch  0, batch   114 | loss: 6.4289799Losses:  5.61453914642334 0.22327934205532074
CurrentTrain: epoch  0, batch   115 | loss: 5.8378186Losses:  6.11285924911499 0.7376750707626343
CurrentTrain: epoch  0, batch   116 | loss: 6.8505344Losses:  6.422451019287109 0.5589277744293213
CurrentTrain: epoch  0, batch   117 | loss: 6.9813786Losses:  6.951774597167969 0.5436252355575562
CurrentTrain: epoch  0, batch   118 | loss: 7.4954000Losses:  6.219111919403076 0.6644584536552429
CurrentTrain: epoch  0, batch   119 | loss: 6.8835702Losses:  6.12355899810791 0.6468032598495483
CurrentTrain: epoch  0, batch   120 | loss: 6.7703624Losses:  5.867402076721191 0.6193586587905884
CurrentTrain: epoch  0, batch   121 | loss: 6.4867606Losses:  6.190009117126465 0.48456740379333496
CurrentTrain: epoch  0, batch   122 | loss: 6.6745768Losses:  5.432798385620117 0.5783556699752808
CurrentTrain: epoch  0, batch   123 | loss: 6.0111542Losses:  5.832359313964844 0.6207107305526733
CurrentTrain: epoch  0, batch   124 | loss: 6.4530702Losses:  6.00895881652832 0.5808306932449341
CurrentTrain: epoch  1, batch     0 | loss: 6.5897894Losses:  5.7507548332214355 0.31020069122314453
CurrentTrain: epoch  1, batch     1 | loss: 6.0609555Losses:  5.16282844543457 0.45016077160835266
CurrentTrain: epoch  1, batch     2 | loss: 5.6129894Losses:  5.201605319976807 0.37568849325180054
CurrentTrain: epoch  1, batch     3 | loss: 5.5772939Losses:  5.587160587310791 0.5516565442085266
CurrentTrain: epoch  1, batch     4 | loss: 6.1388173Losses:  5.37398624420166 0.44960397481918335
CurrentTrain: epoch  1, batch     5 | loss: 5.8235903Losses:  6.0671067237854 0.572049617767334
CurrentTrain: epoch  1, batch     6 | loss: 6.6391563Losses:  5.109853267669678 0.4156947731971741
CurrentTrain: epoch  1, batch     7 | loss: 5.5255480Losses:  5.2695231437683105 0.5010088682174683
CurrentTrain: epoch  1, batch     8 | loss: 5.7705321Losses:  4.90219259262085 0.14116553962230682
CurrentTrain: epoch  1, batch     9 | loss: 5.0433583Losses:  5.098464012145996 0.32820117473602295
CurrentTrain: epoch  1, batch    10 | loss: 5.4266653Losses:  5.181730270385742 0.26472601294517517
CurrentTrain: epoch  1, batch    11 | loss: 5.4464564Losses:  5.188467025756836 0.3674052953720093
CurrentTrain: epoch  1, batch    12 | loss: 5.5558724Losses:  5.791969299316406 0.394331157207489
CurrentTrain: epoch  1, batch    13 | loss: 6.1863003Losses:  5.350395202636719 0.5107863545417786
CurrentTrain: epoch  1, batch    14 | loss: 5.8611817Losses:  5.7290143966674805 0.6009728908538818
CurrentTrain: epoch  1, batch    15 | loss: 6.3299875Losses:  5.252274036407471 0.3731159567832947
CurrentTrain: epoch  1, batch    16 | loss: 5.6253901Losses:  5.282215118408203 0.5899671316146851
CurrentTrain: epoch  1, batch    17 | loss: 5.8721824Losses:  5.230648994445801 0.42458927631378174
CurrentTrain: epoch  1, batch    18 | loss: 5.6552382Losses:  5.158871650695801 0.4494103193283081
CurrentTrain: epoch  1, batch    19 | loss: 5.6082821Losses:  5.48680305480957 0.5611255168914795
CurrentTrain: epoch  1, batch    20 | loss: 6.0479288Losses:  5.121400833129883 0.3594454526901245
CurrentTrain: epoch  1, batch    21 | loss: 5.4808464Losses:  6.176023483276367 0.34802260994911194
CurrentTrain: epoch  1, batch    22 | loss: 6.5240459Losses:  5.402355194091797 0.6303118467330933
CurrentTrain: epoch  1, batch    23 | loss: 6.0326672Losses:  5.314756870269775 0.419138640165329
CurrentTrain: epoch  1, batch    24 | loss: 5.7338953Losses:  5.230619430541992 0.5766491889953613
CurrentTrain: epoch  1, batch    25 | loss: 5.8072686Losses:  4.973908424377441 0.357257604598999
CurrentTrain: epoch  1, batch    26 | loss: 5.3311663Losses:  6.164173126220703 0.5612441897392273
CurrentTrain: epoch  1, batch    27 | loss: 6.7254171Losses:  5.642097473144531 0.7555719614028931
CurrentTrain: epoch  1, batch    28 | loss: 6.3976693Losses:  5.327550888061523 0.48647987842559814
CurrentTrain: epoch  1, batch    29 | loss: 5.8140306Losses:  5.654258728027344 0.6185394525527954
CurrentTrain: epoch  1, batch    30 | loss: 6.2727981Losses:  5.248876094818115 0.4351849853992462
CurrentTrain: epoch  1, batch    31 | loss: 5.6840611Losses:  4.968571186065674 0.4479942321777344
CurrentTrain: epoch  1, batch    32 | loss: 5.4165654Losses:  5.264473915100098 0.3296119272708893
CurrentTrain: epoch  1, batch    33 | loss: 5.5940857Losses:  5.841835021972656 0.6813297867774963
CurrentTrain: epoch  1, batch    34 | loss: 6.5231647Losses:  5.604250431060791 0.3509662449359894
CurrentTrain: epoch  1, batch    35 | loss: 5.9552169Losses:  5.260378837585449 0.4704816937446594
CurrentTrain: epoch  1, batch    36 | loss: 5.7308607Losses:  5.065298080444336 0.35133594274520874
CurrentTrain: epoch  1, batch    37 | loss: 5.4166341Losses:  5.2007832527160645 0.478799968957901
CurrentTrain: epoch  1, batch    38 | loss: 5.6795831Losses:  5.24516487121582 0.4734748601913452
CurrentTrain: epoch  1, batch    39 | loss: 5.7186399Losses:  5.581295490264893 0.42984992265701294
CurrentTrain: epoch  1, batch    40 | loss: 6.0111456Losses:  5.085104465484619 0.37875688076019287
CurrentTrain: epoch  1, batch    41 | loss: 5.4638615Losses:  4.997174263000488 0.3030943274497986
CurrentTrain: epoch  1, batch    42 | loss: 5.3002687Losses:  5.628155708312988 0.3796548843383789
CurrentTrain: epoch  1, batch    43 | loss: 6.0078106Losses:  5.483884811401367 0.5303725004196167
CurrentTrain: epoch  1, batch    44 | loss: 6.0142574Losses:  5.243146896362305 0.35104191303253174
CurrentTrain: epoch  1, batch    45 | loss: 5.5941887Losses:  5.150760173797607 0.3792332410812378
CurrentTrain: epoch  1, batch    46 | loss: 5.5299935Losses:  5.552309989929199 0.3759597837924957
CurrentTrain: epoch  1, batch    47 | loss: 5.9282699Losses:  5.032825469970703 0.3770131766796112
CurrentTrain: epoch  1, batch    48 | loss: 5.4098387Losses:  5.5887837409973145 0.49389535188674927
CurrentTrain: epoch  1, batch    49 | loss: 6.0826793Losses:  5.153252124786377 0.49462783336639404
CurrentTrain: epoch  1, batch    50 | loss: 5.6478801Losses:  4.786445140838623 0.24823974072933197
CurrentTrain: epoch  1, batch    51 | loss: 5.0346847Losses:  6.210240364074707 0.5818814635276794
CurrentTrain: epoch  1, batch    52 | loss: 6.7921219Losses:  5.247881889343262 0.30167484283447266
CurrentTrain: epoch  1, batch    53 | loss: 5.5495567Losses:  5.021895408630371 0.3593563437461853
CurrentTrain: epoch  1, batch    54 | loss: 5.3812518Losses:  4.754376411437988 0.1790367066860199
CurrentTrain: epoch  1, batch    55 | loss: 4.9334130Losses:  4.802160739898682 0.10487261414527893
CurrentTrain: epoch  1, batch    56 | loss: 4.9070334Losses:  4.841174125671387 0.31708571314811707
CurrentTrain: epoch  1, batch    57 | loss: 5.1582599Losses:  4.889313697814941 0.32205647230148315
CurrentTrain: epoch  1, batch    58 | loss: 5.2113700Losses:  5.356099605560303 0.42794549465179443
CurrentTrain: epoch  1, batch    59 | loss: 5.7840452Losses:  4.682143211364746 0.29807472229003906
CurrentTrain: epoch  1, batch    60 | loss: 4.9802179Losses:  5.582575798034668 0.5724867582321167
CurrentTrain: epoch  1, batch    61 | loss: 6.1550627Losses:  5.057242393493652 0.3588026165962219
CurrentTrain: epoch  1, batch    62 | loss: 5.4160452Losses:  5.163743495941162 0.21662874519824982
CurrentTrain: epoch  1, batch    63 | loss: 5.3803720Losses:  4.9556989669799805 0.1600514054298401
CurrentTrain: epoch  1, batch    64 | loss: 5.1157503Losses:  4.921700954437256 0.2954484224319458
CurrentTrain: epoch  1, batch    65 | loss: 5.2171493Losses:  5.22847843170166 0.31493014097213745
CurrentTrain: epoch  1, batch    66 | loss: 5.5434084Losses:  5.245635509490967 0.5212775468826294
CurrentTrain: epoch  1, batch    67 | loss: 5.7669129Losses:  4.871207237243652 0.38251811265945435
CurrentTrain: epoch  1, batch    68 | loss: 5.2537255Losses:  5.278450012207031 0.2433202862739563
CurrentTrain: epoch  1, batch    69 | loss: 5.5217705Losses:  5.041074275970459 0.36368346214294434
CurrentTrain: epoch  1, batch    70 | loss: 5.4047575Losses:  5.2829670906066895 0.4603264629840851
CurrentTrain: epoch  1, batch    71 | loss: 5.7432938Losses:  4.924942970275879 0.3326976001262665
CurrentTrain: epoch  1, batch    72 | loss: 5.2576404Losses:  5.019965171813965 0.3933922052383423
CurrentTrain: epoch  1, batch    73 | loss: 5.4133573Losses:  5.151875972747803 0.3916698694229126
CurrentTrain: epoch  1, batch    74 | loss: 5.5435457Losses:  5.030933856964111 0.30962100625038147
CurrentTrain: epoch  1, batch    75 | loss: 5.3405547Losses:  5.161564826965332 0.31159621477127075
CurrentTrain: epoch  1, batch    76 | loss: 5.4731612Losses:  4.986693382263184 0.35385143756866455
CurrentTrain: epoch  1, batch    77 | loss: 5.3405447Losses:  4.8159942626953125 0.2018135040998459
CurrentTrain: epoch  1, batch    78 | loss: 5.0178080Losses:  4.619311332702637 0.262442946434021
CurrentTrain: epoch  1, batch    79 | loss: 4.8817544Losses:  5.1029767990112305 0.41062021255493164
CurrentTrain: epoch  1, batch    80 | loss: 5.5135970Losses:  5.378778457641602 0.25562164187431335
CurrentTrain: epoch  1, batch    81 | loss: 5.6343999Losses:  4.954859733581543 0.29486891627311707
CurrentTrain: epoch  1, batch    82 | loss: 5.2497287Losses:  5.265895366668701 0.3151581287384033
CurrentTrain: epoch  1, batch    83 | loss: 5.5810537Losses:  4.757211208343506 0.30564945936203003
CurrentTrain: epoch  1, batch    84 | loss: 5.0628605Losses:  4.965570449829102 0.44266849756240845
CurrentTrain: epoch  1, batch    85 | loss: 5.4082389Losses:  4.982903480529785 0.4160292446613312
CurrentTrain: epoch  1, batch    86 | loss: 5.3989329Losses:  5.030107021331787 0.22883909940719604
CurrentTrain: epoch  1, batch    87 | loss: 5.2589459Losses:  5.033924102783203 0.304232120513916
CurrentTrain: epoch  1, batch    88 | loss: 5.3381562Losses:  5.301303863525391 0.37020236253738403
CurrentTrain: epoch  1, batch    89 | loss: 5.6715064Losses:  4.846268177032471 0.2633843421936035
CurrentTrain: epoch  1, batch    90 | loss: 5.1096525Losses:  4.70444393157959 0.302681028842926
CurrentTrain: epoch  1, batch    91 | loss: 5.0071249Losses:  4.847253799438477 0.312654972076416
CurrentTrain: epoch  1, batch    92 | loss: 5.1599088Losses:  5.008009910583496 0.22171476483345032
CurrentTrain: epoch  1, batch    93 | loss: 5.2297249Losses:  4.74302339553833 0.28326666355133057
CurrentTrain: epoch  1, batch    94 | loss: 5.0262899Losses:  4.988719940185547 0.22750288248062134
CurrentTrain: epoch  1, batch    95 | loss: 5.2162228Losses:  5.245913028717041 0.23794421553611755
CurrentTrain: epoch  1, batch    96 | loss: 5.4838572Losses:  4.760464668273926 0.23006069660186768
CurrentTrain: epoch  1, batch    97 | loss: 4.9905252Losses:  4.904879093170166 0.22214344143867493
CurrentTrain: epoch  1, batch    98 | loss: 5.1270227Losses:  4.438770771026611 0.25690895318984985
CurrentTrain: epoch  1, batch    99 | loss: 4.6956797Losses:  4.730259895324707 0.18532943725585938
CurrentTrain: epoch  1, batch   100 | loss: 4.9155893Losses:  4.5632805824279785 0.18876007199287415
CurrentTrain: epoch  1, batch   101 | loss: 4.7520409Losses:  4.716626167297363 0.16965225338935852
CurrentTrain: epoch  1, batch   102 | loss: 4.8862786Losses:  4.427111625671387 0.13769163191318512
CurrentTrain: epoch  1, batch   103 | loss: 4.5648031Losses:  5.21942138671875 0.2646030783653259
CurrentTrain: epoch  1, batch   104 | loss: 5.4840245Losses:  4.580532073974609 0.2151661515235901
CurrentTrain: epoch  1, batch   105 | loss: 4.7956982Losses:  4.583645820617676 0.3247368335723877
CurrentTrain: epoch  1, batch   106 | loss: 4.9083824Losses:  4.499965190887451 0.31320616602897644
CurrentTrain: epoch  1, batch   107 | loss: 4.8131714Losses:  4.954708099365234 0.19071608781814575
CurrentTrain: epoch  1, batch   108 | loss: 5.1454244Losses:  4.903948783874512 0.33211660385131836
CurrentTrain: epoch  1, batch   109 | loss: 5.2360654Losses:  4.632512092590332 0.20058490335941315
CurrentTrain: epoch  1, batch   110 | loss: 4.8330970Losses:  4.404687881469727 0.0802285373210907
CurrentTrain: epoch  1, batch   111 | loss: 4.4849162Losses:  4.629715919494629 0.2601703405380249
CurrentTrain: epoch  1, batch   112 | loss: 4.8898864Losses:  4.917987823486328 0.2724732458591461
CurrentTrain: epoch  1, batch   113 | loss: 5.1904612Losses:  4.784313201904297 0.2693367600440979
CurrentTrain: epoch  1, batch   114 | loss: 5.0536499Losses:  4.615138530731201 0.1888236701488495
CurrentTrain: epoch  1, batch   115 | loss: 4.8039622Losses:  4.59696102142334 0.21345359086990356
CurrentTrain: epoch  1, batch   116 | loss: 4.8104148Losses:  4.55228328704834 0.304940402507782
CurrentTrain: epoch  1, batch   117 | loss: 4.8572235Losses:  4.5349531173706055 0.31226933002471924
CurrentTrain: epoch  1, batch   118 | loss: 4.8472223Losses:  4.853700160980225 0.3847142457962036
CurrentTrain: epoch  1, batch   119 | loss: 5.2384143Losses:  4.570730209350586 0.23835667967796326
CurrentTrain: epoch  1, batch   120 | loss: 4.8090868Losses:  4.565335750579834 0.14211349189281464
CurrentTrain: epoch  1, batch   121 | loss: 4.7074494Losses:  4.620335578918457 0.27035588026046753
CurrentTrain: epoch  1, batch   122 | loss: 4.8906913Losses:  5.097557067871094 0.19865760207176208
CurrentTrain: epoch  1, batch   123 | loss: 5.2962146Losses:  4.479672431945801 0.21034857630729675
CurrentTrain: epoch  1, batch   124 | loss: 4.6900210Losses:  4.4511919021606445 0.2278791069984436
CurrentTrain: epoch  2, batch     0 | loss: 4.6790709Losses:  4.548937797546387 0.15655595064163208
CurrentTrain: epoch  2, batch     1 | loss: 4.7054939Losses:  4.204345226287842 0.17115750908851624
CurrentTrain: epoch  2, batch     2 | loss: 4.3755026Losses:  4.439206123352051 0.1945439726114273
CurrentTrain: epoch  2, batch     3 | loss: 4.6337500Losses:  4.486894607543945 0.22758306562900543
CurrentTrain: epoch  2, batch     4 | loss: 4.7144775Losses:  4.342782974243164 0.16285362839698792
CurrentTrain: epoch  2, batch     5 | loss: 4.5056367Losses:  4.428043365478516 0.183524489402771
CurrentTrain: epoch  2, batch     6 | loss: 4.6115680Losses:  4.4005937576293945 0.26448485255241394
CurrentTrain: epoch  2, batch     7 | loss: 4.6650786Losses:  4.278225898742676 0.25589150190353394
CurrentTrain: epoch  2, batch     8 | loss: 4.5341172Losses:  4.410495281219482 0.3033561706542969
CurrentTrain: epoch  2, batch     9 | loss: 4.7138515Losses:  4.595726013183594 0.08432181179523468
CurrentTrain: epoch  2, batch    10 | loss: 4.6800480Losses:  4.652308464050293 0.293666809797287
CurrentTrain: epoch  2, batch    11 | loss: 4.9459753Losses:  4.405400276184082 0.2131502479314804
CurrentTrain: epoch  2, batch    12 | loss: 4.6185503Losses:  4.306296348571777 0.21483781933784485
CurrentTrain: epoch  2, batch    13 | loss: 4.5211344Losses:  4.470294952392578 0.20514041185379028
CurrentTrain: epoch  2, batch    14 | loss: 4.6754355Losses:  4.824923992156982 0.34050431847572327
CurrentTrain: epoch  2, batch    15 | loss: 5.1654282Losses:  4.3810858726501465 0.21313437819480896
CurrentTrain: epoch  2, batch    16 | loss: 4.5942202Losses:  4.650500297546387 0.3094349503517151
CurrentTrain: epoch  2, batch    17 | loss: 4.9599352Losses:  4.164348602294922 0.11926670372486115
CurrentTrain: epoch  2, batch    18 | loss: 4.2836151Losses:  4.41707706451416 0.2554476261138916
CurrentTrain: epoch  2, batch    19 | loss: 4.6725245Losses:  4.305735111236572 0.24140664935112
CurrentTrain: epoch  2, batch    20 | loss: 4.5471416Losses:  4.570518970489502 0.1624571979045868
CurrentTrain: epoch  2, batch    21 | loss: 4.7329760Losses:  4.326144218444824 0.21745756268501282
CurrentTrain: epoch  2, batch    22 | loss: 4.5436020Losses:  4.460214138031006 0.2214498370885849
CurrentTrain: epoch  2, batch    23 | loss: 4.6816640Losses:  4.286914825439453 0.205899178981781
CurrentTrain: epoch  2, batch    24 | loss: 4.4928141Losses:  4.517177581787109 0.09031728655099869
CurrentTrain: epoch  2, batch    25 | loss: 4.6074948Losses:  4.377814292907715 0.12030289322137833
CurrentTrain: epoch  2, batch    26 | loss: 4.4981170Losses:  4.57529354095459 0.27406319975852966
CurrentTrain: epoch  2, batch    27 | loss: 4.8493567Losses:  4.460568904876709 0.1316082775592804
CurrentTrain: epoch  2, batch    28 | loss: 4.5921774Losses:  4.4441914558410645 0.17955337464809418
CurrentTrain: epoch  2, batch    29 | loss: 4.6237450Losses:  4.43375301361084 0.1700541228055954
CurrentTrain: epoch  2, batch    30 | loss: 4.6038070Losses:  4.360772132873535 0.19267603754997253
CurrentTrain: epoch  2, batch    31 | loss: 4.5534482Losses:  4.372065544128418 0.20463010668754578
CurrentTrain: epoch  2, batch    32 | loss: 4.5766954Losses:  4.215421676635742 0.0998370498418808
CurrentTrain: epoch  2, batch    33 | loss: 4.3152585Losses:  4.908472061157227 0.28767019510269165
CurrentTrain: epoch  2, batch    34 | loss: 5.1961422Losses:  4.40106201171875 0.17922016978263855
CurrentTrain: epoch  2, batch    35 | loss: 4.5802822Losses:  4.696630477905273 0.34885674715042114
CurrentTrain: epoch  2, batch    36 | loss: 5.0454874Losses:  4.263116836547852 0.24387244880199432
CurrentTrain: epoch  2, batch    37 | loss: 4.5069895Losses:  4.426169395446777 0.20585182309150696
CurrentTrain: epoch  2, batch    38 | loss: 4.6320214Losses:  4.330410957336426 0.129120334982872
CurrentTrain: epoch  2, batch    39 | loss: 4.4595313Losses:  4.892932415008545 0.12432987987995148
CurrentTrain: epoch  2, batch    40 | loss: 5.0172625Losses:  4.436794281005859 0.11073543131351471
CurrentTrain: epoch  2, batch    41 | loss: 4.5475297Losses:  4.407683849334717 0.18845385313034058
CurrentTrain: epoch  2, batch    42 | loss: 4.5961375Losses:  4.468742370605469 0.19428910315036774
CurrentTrain: epoch  2, batch    43 | loss: 4.6630316Losses:  4.359509468078613 0.19391025602817535
CurrentTrain: epoch  2, batch    44 | loss: 4.5534196Losses:  4.417488098144531 0.19986385107040405
CurrentTrain: epoch  2, batch    45 | loss: 4.6173520Losses:  4.2814226150512695 0.13442015647888184
CurrentTrain: epoch  2, batch    46 | loss: 4.4158430Losses:  4.535804748535156 0.18359336256980896
CurrentTrain: epoch  2, batch    47 | loss: 4.7193980Losses:  4.279807090759277 0.2585620880126953
CurrentTrain: epoch  2, batch    48 | loss: 4.5383692Losses:  4.110814571380615 0.175233393907547
CurrentTrain: epoch  2, batch    49 | loss: 4.2860479Losses:  4.25810432434082 0.17494229972362518
CurrentTrain: epoch  2, batch    50 | loss: 4.4330468Losses:  4.253388404846191 0.10829205065965652
CurrentTrain: epoch  2, batch    51 | loss: 4.3616805Losses:  4.254408836364746 0.14789365231990814
CurrentTrain: epoch  2, batch    52 | loss: 4.4023023Losses:  4.149507999420166 0.15055117011070251
CurrentTrain: epoch  2, batch    53 | loss: 4.3000593Losses:  4.443210601806641 0.19323787093162537
CurrentTrain: epoch  2, batch    54 | loss: 4.6364484Losses:  4.291855812072754 0.21917596459388733
CurrentTrain: epoch  2, batch    55 | loss: 4.5110316Losses:  4.257820129394531 0.13063868880271912
CurrentTrain: epoch  2, batch    56 | loss: 4.3884587Losses:  4.099534034729004 0.07390926778316498
CurrentTrain: epoch  2, batch    57 | loss: 4.1734433Losses:  4.25938606262207 0.17510515451431274
CurrentTrain: epoch  2, batch    58 | loss: 4.4344912Losses:  4.65146541595459 0.33196261525154114
CurrentTrain: epoch  2, batch    59 | loss: 4.9834280Losses:  4.240167617797852 0.10294687002897263
CurrentTrain: epoch  2, batch    60 | loss: 4.3431144Losses:  4.299144744873047 0.17691096663475037
CurrentTrain: epoch  2, batch    61 | loss: 4.4760556Losses:  4.26369571685791 0.15598632395267487
CurrentTrain: epoch  2, batch    62 | loss: 4.4196820Losses:  4.737439155578613 0.41919761896133423
CurrentTrain: epoch  2, batch    63 | loss: 5.1566367Losses:  4.411701202392578 0.16541194915771484
CurrentTrain: epoch  2, batch    64 | loss: 4.5771132Losses:  4.3636794090271 0.1630820333957672
CurrentTrain: epoch  2, batch    65 | loss: 4.5267615Losses:  4.459908962249756 0.2560492157936096
CurrentTrain: epoch  2, batch    66 | loss: 4.7159581Losses:  4.223743438720703 0.139581561088562
CurrentTrain: epoch  2, batch    67 | loss: 4.3633251Losses:  4.2522430419921875 0.17690497636795044
CurrentTrain: epoch  2, batch    68 | loss: 4.4291482Losses:  5.283417701721191 0.1496472954750061
CurrentTrain: epoch  2, batch    69 | loss: 5.4330649Losses:  4.24799919128418 0.20678885281085968
CurrentTrain: epoch  2, batch    70 | loss: 4.4547882Losses:  4.4412641525268555 0.2559456527233124
CurrentTrain: epoch  2, batch    71 | loss: 4.6972098Losses:  4.538122177124023 0.3189527988433838
CurrentTrain: epoch  2, batch    72 | loss: 4.8570747Losses:  4.249001979827881 0.2455400973558426
CurrentTrain: epoch  2, batch    73 | loss: 4.4945421Losses:  4.188567161560059 0.15620335936546326
CurrentTrain: epoch  2, batch    74 | loss: 4.3447704Losses:  4.274702072143555 0.21172663569450378
CurrentTrain: epoch  2, batch    75 | loss: 4.4864287Losses:  4.249272346496582 0.21640238165855408
CurrentTrain: epoch  2, batch    76 | loss: 4.4656749Losses:  4.2607927322387695 0.2649574279785156
CurrentTrain: epoch  2, batch    77 | loss: 4.5257502Losses:  4.363940238952637 0.20930613577365875
CurrentTrain: epoch  2, batch    78 | loss: 4.5732465Losses:  4.200196266174316 0.14291633665561676
CurrentTrain: epoch  2, batch    79 | loss: 4.3431125Losses:  4.752634048461914 0.21767449378967285
CurrentTrain: epoch  2, batch    80 | loss: 4.9703083Losses:  4.939047813415527 0.4366837441921234
CurrentTrain: epoch  2, batch    81 | loss: 5.3757315Losses:  4.80377197265625 0.2622464597225189
CurrentTrain: epoch  2, batch    82 | loss: 5.0660186Losses:  4.194036960601807 0.12280669808387756
CurrentTrain: epoch  2, batch    83 | loss: 4.3168435Losses:  4.271273612976074 0.19344176352024078
CurrentTrain: epoch  2, batch    84 | loss: 4.4647155Losses:  4.231635093688965 0.15477389097213745
CurrentTrain: epoch  2, batch    85 | loss: 4.3864088Losses:  4.215216636657715 0.13714103400707245
CurrentTrain: epoch  2, batch    86 | loss: 4.3523579Losses:  4.583258628845215 0.14108410477638245
CurrentTrain: epoch  2, batch    87 | loss: 4.7243428Losses:  4.218628883361816 0.1587391048669815
CurrentTrain: epoch  2, batch    88 | loss: 4.3773680Losses:  4.212181091308594 0.16412831842899323
CurrentTrain: epoch  2, batch    89 | loss: 4.3763094Losses:  4.284001350402832 0.11750899255275726
CurrentTrain: epoch  2, batch    90 | loss: 4.4015102Losses:  4.217073440551758 0.06019560620188713
CurrentTrain: epoch  2, batch    91 | loss: 4.2772689Losses:  4.674487113952637 0.1998245120048523
CurrentTrain: epoch  2, batch    92 | loss: 4.8743114Losses:  4.1699042320251465 0.1596514880657196
CurrentTrain: epoch  2, batch    93 | loss: 4.3295555Losses:  4.27000093460083 0.12190276384353638
CurrentTrain: epoch  2, batch    94 | loss: 4.3919039Losses:  4.1982574462890625 0.09547417610883713
CurrentTrain: epoch  2, batch    95 | loss: 4.2937317Losses:  4.354147911071777 0.14320555329322815
CurrentTrain: epoch  2, batch    96 | loss: 4.4973536Losses:  4.383138656616211 0.16660994291305542
CurrentTrain: epoch  2, batch    97 | loss: 4.5497484Losses:  4.306542873382568 0.14298740029335022
CurrentTrain: epoch  2, batch    98 | loss: 4.4495301Losses:  4.290676593780518 0.11654747277498245
CurrentTrain: epoch  2, batch    99 | loss: 4.4072242Losses:  4.313444137573242 0.11167864501476288
CurrentTrain: epoch  2, batch   100 | loss: 4.4251227Losses:  4.250574111938477 0.13131925463676453
CurrentTrain: epoch  2, batch   101 | loss: 4.3818932Losses:  4.33450984954834 0.12842731177806854
CurrentTrain: epoch  2, batch   102 | loss: 4.4629374Losses:  4.199667453765869 0.1131826639175415
CurrentTrain: epoch  2, batch   103 | loss: 4.3128500Losses:  4.402410507202148 0.175832137465477
CurrentTrain: epoch  2, batch   104 | loss: 4.5782428Losses:  4.210083484649658 0.1920100748538971
CurrentTrain: epoch  2, batch   105 | loss: 4.4020934Losses:  4.289751052856445 0.10016154497861862
CurrentTrain: epoch  2, batch   106 | loss: 4.3899126Losses:  4.309508323669434 0.13656926155090332
CurrentTrain: epoch  2, batch   107 | loss: 4.4460773Losses:  4.331614971160889 0.17586497962474823
CurrentTrain: epoch  2, batch   108 | loss: 4.5074801Losses:  4.230832576751709 0.07732012122869492
CurrentTrain: epoch  2, batch   109 | loss: 4.3081527Losses:  4.211795806884766 0.12707772850990295
CurrentTrain: epoch  2, batch   110 | loss: 4.3388734Losses:  4.473118782043457 0.2125113159418106
CurrentTrain: epoch  2, batch   111 | loss: 4.6856303Losses:  4.1218366622924805 0.11017347872257233
CurrentTrain: epoch  2, batch   112 | loss: 4.2320104Losses:  4.183727264404297 0.17658793926239014
CurrentTrain: epoch  2, batch   113 | loss: 4.3603153Losses:  4.094239234924316 0.07565845549106598
CurrentTrain: epoch  2, batch   114 | loss: 4.1698976Losses:  4.81472110748291 0.31554388999938965
CurrentTrain: epoch  2, batch   115 | loss: 5.1302652Losses:  4.129484176635742 0.1264645755290985
CurrentTrain: epoch  2, batch   116 | loss: 4.2559485Losses:  4.210358619689941 0.11639286577701569
CurrentTrain: epoch  2, batch   117 | loss: 4.3267517Losses:  4.141284942626953 0.13660451769828796
CurrentTrain: epoch  2, batch   118 | loss: 4.2778893Losses:  4.168356895446777 0.1778956651687622
CurrentTrain: epoch  2, batch   119 | loss: 4.3462524Losses:  4.124174118041992 0.09439167380332947
CurrentTrain: epoch  2, batch   120 | loss: 4.2185659Losses:  4.16982889175415 0.1740964949131012
CurrentTrain: epoch  2, batch   121 | loss: 4.3439255Losses:  4.128308296203613 0.0823751837015152
CurrentTrain: epoch  2, batch   122 | loss: 4.2106833Losses:  4.178487777709961 0.19129998981952667
CurrentTrain: epoch  2, batch   123 | loss: 4.3697877Losses:  4.125051498413086 0.11172973364591599
CurrentTrain: epoch  2, batch   124 | loss: 4.2367811Losses:  4.189846038818359 0.15108422935009003
CurrentTrain: epoch  3, batch     0 | loss: 4.3409305Losses:  4.125978469848633 0.13980799913406372
CurrentTrain: epoch  3, batch     1 | loss: 4.2657866Losses:  4.16981315612793 0.082309290766716
CurrentTrain: epoch  3, batch     2 | loss: 4.2521224Losses:  4.159313201904297 0.11269693076610565
CurrentTrain: epoch  3, batch     3 | loss: 4.2720103Losses:  4.269826889038086 0.1658262312412262
CurrentTrain: epoch  3, batch     4 | loss: 4.4356532Losses:  4.038457870483398 0.11026930809020996
CurrentTrain: epoch  3, batch     5 | loss: 4.1487274Losses:  4.054577827453613 0.1604805290699005
CurrentTrain: epoch  3, batch     6 | loss: 4.2150583Losses:  4.081732273101807 0.1251382827758789
CurrentTrain: epoch  3, batch     7 | loss: 4.2068706Losses:  4.1767578125 0.10807804763317108
CurrentTrain: epoch  3, batch     8 | loss: 4.2848358Losses:  4.149588584899902 0.13710927963256836
CurrentTrain: epoch  3, batch     9 | loss: 4.2866979Losses:  4.267104625701904 0.14704865217208862
CurrentTrain: epoch  3, batch    10 | loss: 4.4141531Losses:  4.150681495666504 0.14631013572216034
CurrentTrain: epoch  3, batch    11 | loss: 4.2969918Losses:  4.122181415557861 0.18387019634246826
CurrentTrain: epoch  3, batch    12 | loss: 4.3060517Losses:  4.2616286277771 0.12383656203746796
CurrentTrain: epoch  3, batch    13 | loss: 4.3854651Losses:  4.232646942138672 0.11070965975522995
CurrentTrain: epoch  3, batch    14 | loss: 4.3433566Losses:  4.200918674468994 0.11006679385900497
CurrentTrain: epoch  3, batch    15 | loss: 4.3109856Losses:  4.0698347091674805 0.0891699343919754
CurrentTrain: epoch  3, batch    16 | loss: 4.1590047Losses:  4.27652645111084 0.21200969815254211
CurrentTrain: epoch  3, batch    17 | loss: 4.4885364Losses:  4.202136993408203 0.1441146731376648
CurrentTrain: epoch  3, batch    18 | loss: 4.3462515Losses:  4.190269947052002 0.1273496448993683
CurrentTrain: epoch  3, batch    19 | loss: 4.3176198Losses:  4.716869354248047 0.23805493116378784
CurrentTrain: epoch  3, batch    20 | loss: 4.9549241Losses:  4.121229648590088 0.16483095288276672
CurrentTrain: epoch  3, batch    21 | loss: 4.2860608Losses:  4.263509273529053 0.15923628211021423
CurrentTrain: epoch  3, batch    22 | loss: 4.4227457Losses:  4.126239776611328 0.14501211047172546
CurrentTrain: epoch  3, batch    23 | loss: 4.2712517Losses:  4.072699546813965 0.11658480018377304
CurrentTrain: epoch  3, batch    24 | loss: 4.1892843Losses:  4.340200901031494 0.1246669590473175
CurrentTrain: epoch  3, batch    25 | loss: 4.4648681Losses:  4.199245452880859 0.1270950436592102
CurrentTrain: epoch  3, batch    26 | loss: 4.3263407Losses:  4.079767227172852 0.13857634365558624
CurrentTrain: epoch  3, batch    27 | loss: 4.2183437Losses:  4.314968109130859 0.10762830078601837
CurrentTrain: epoch  3, batch    28 | loss: 4.4225965Losses:  4.144363880157471 0.1333346962928772
CurrentTrain: epoch  3, batch    29 | loss: 4.2776985Losses:  4.5571699142456055 0.10893218219280243
CurrentTrain: epoch  3, batch    30 | loss: 4.6661019Losses:  4.149801254272461 0.16427434980869293
CurrentTrain: epoch  3, batch    31 | loss: 4.3140755Losses:  4.285256385803223 0.14272351562976837
CurrentTrain: epoch  3, batch    32 | loss: 4.4279799Losses:  4.115848541259766 0.15074464678764343
CurrentTrain: epoch  3, batch    33 | loss: 4.2665930Losses:  4.094231605529785 0.07744286209344864
CurrentTrain: epoch  3, batch    34 | loss: 4.1716743Losses:  4.0496978759765625 0.06728283315896988
CurrentTrain: epoch  3, batch    35 | loss: 4.1169806Losses:  4.089716911315918 0.058201491832733154
CurrentTrain: epoch  3, batch    36 | loss: 4.1479182Losses:  4.11977481842041 0.15480786561965942
CurrentTrain: epoch  3, batch    37 | loss: 4.2745829Losses:  4.125412940979004 0.15071767568588257
CurrentTrain: epoch  3, batch    38 | loss: 4.2761307Losses:  4.120475769042969 0.08012836426496506
CurrentTrain: epoch  3, batch    39 | loss: 4.2006040Losses:  4.197585105895996 0.14328287541866302
CurrentTrain: epoch  3, batch    40 | loss: 4.3408680Losses:  4.018828392028809 0.12901878356933594
CurrentTrain: epoch  3, batch    41 | loss: 4.1478472Losses:  4.065577507019043 0.1236414909362793
CurrentTrain: epoch  3, batch    42 | loss: 4.1892190Losses:  4.196248531341553 0.08428575843572617
CurrentTrain: epoch  3, batch    43 | loss: 4.2805343Losses:  4.099199295043945 0.16909167170524597
CurrentTrain: epoch  3, batch    44 | loss: 4.2682910Losses:  4.090919494628906 0.1373862475156784
CurrentTrain: epoch  3, batch    45 | loss: 4.2283058Losses:  4.217141151428223 0.07970588654279709
CurrentTrain: epoch  3, batch    46 | loss: 4.2968469Losses:  4.169703483581543 0.13882748782634735
CurrentTrain: epoch  3, batch    47 | loss: 4.3085308Losses:  4.160473823547363 0.09417612105607986
CurrentTrain: epoch  3, batch    48 | loss: 4.2546501Losses:  4.063539505004883 0.10538681596517563
CurrentTrain: epoch  3, batch    49 | loss: 4.1689262Losses:  4.169559478759766 0.11581390351057053
CurrentTrain: epoch  3, batch    50 | loss: 4.2853732Losses:  4.015635967254639 0.08513668924570084
CurrentTrain: epoch  3, batch    51 | loss: 4.1007729Losses:  4.0449748039245605 0.16087448596954346
CurrentTrain: epoch  3, batch    52 | loss: 4.2058492Losses:  4.11525297164917 0.1271064579486847
CurrentTrain: epoch  3, batch    53 | loss: 4.2423596Losses:  4.037022590637207 0.14034713804721832
CurrentTrain: epoch  3, batch    54 | loss: 4.1773696Losses:  3.9935765266418457 0.14172965288162231
CurrentTrain: epoch  3, batch    55 | loss: 4.1353064Losses:  4.135396957397461 0.06796509027481079
CurrentTrain: epoch  3, batch    56 | loss: 4.2033620Losses:  4.235177040100098 0.12350009381771088
CurrentTrain: epoch  3, batch    57 | loss: 4.3586769Losses:  4.095864295959473 0.13078966736793518
CurrentTrain: epoch  3, batch    58 | loss: 4.2266541Losses:  4.041665077209473 0.11286564916372299
CurrentTrain: epoch  3, batch    59 | loss: 4.1545305Losses:  4.046977996826172 0.13985908031463623
CurrentTrain: epoch  3, batch    60 | loss: 4.1868372Losses:  4.0618414878845215 0.08748503774404526
CurrentTrain: epoch  3, batch    61 | loss: 4.1493263Losses:  4.114397048950195 0.08169769495725632
CurrentTrain: epoch  3, batch    62 | loss: 4.1960945Losses:  4.189166069030762 0.0597827285528183
CurrentTrain: epoch  3, batch    63 | loss: 4.2489486Losses:  4.025330543518066 0.1532781720161438
CurrentTrain: epoch  3, batch    64 | loss: 4.1786089Losses:  4.09405517578125 0.10136695951223373
CurrentTrain: epoch  3, batch    65 | loss: 4.1954222Losses:  4.105130195617676 0.08632037788629532
CurrentTrain: epoch  3, batch    66 | loss: 4.1914506Losses:  4.097949028015137 0.07746507227420807
CurrentTrain: epoch  3, batch    67 | loss: 4.1754141Losses:  4.105321884155273 0.15982095897197723
CurrentTrain: epoch  3, batch    68 | loss: 4.2651429Losses:  4.105119705200195 0.16227635741233826
CurrentTrain: epoch  3, batch    69 | loss: 4.2673960Losses:  4.040332794189453 0.06880920380353928
CurrentTrain: epoch  3, batch    70 | loss: 4.1091418Losses:  4.0798234939575195 0.1597888171672821
CurrentTrain: epoch  3, batch    71 | loss: 4.2396121Losses:  4.0959272384643555 0.1297483891248703
CurrentTrain: epoch  3, batch    72 | loss: 4.2256756Losses:  4.026926040649414 0.08534841239452362
CurrentTrain: epoch  3, batch    73 | loss: 4.1122746Losses:  4.025882244110107 0.11424814164638519
CurrentTrain: epoch  3, batch    74 | loss: 4.1401305Losses:  4.020099639892578 0.07104725390672684
CurrentTrain: epoch  3, batch    75 | loss: 4.0911469Losses:  4.084980010986328 0.0962207168340683
CurrentTrain: epoch  3, batch    76 | loss: 4.1812005Losses:  4.017298698425293 0.07385804504156113
CurrentTrain: epoch  3, batch    77 | loss: 4.0911570Losses:  4.029945373535156 0.09037316590547562
CurrentTrain: epoch  3, batch    78 | loss: 4.1203184Losses:  4.017414093017578 0.024369673803448677
CurrentTrain: epoch  3, batch    79 | loss: 4.0417838Losses:  4.053368091583252 0.14763768017292023
CurrentTrain: epoch  3, batch    80 | loss: 4.2010059Losses:  4.043882369995117 0.19583339989185333
CurrentTrain: epoch  3, batch    81 | loss: 4.2397156Losses:  4.170778274536133 0.08469291031360626
CurrentTrain: epoch  3, batch    82 | loss: 4.2554712Losses:  4.154355049133301 0.1373196542263031
CurrentTrain: epoch  3, batch    83 | loss: 4.2916746Losses:  4.10725212097168 0.07457968592643738
CurrentTrain: epoch  3, batch    84 | loss: 4.1818318Losses:  4.065608024597168 0.11070065200328827
CurrentTrain: epoch  3, batch    85 | loss: 4.1763086Losses:  4.085124969482422 0.10423486679792404
CurrentTrain: epoch  3, batch    86 | loss: 4.1893597Losses:  4.031920433044434 0.08521248400211334
CurrentTrain: epoch  3, batch    87 | loss: 4.1171331Losses:  4.052425861358643 0.084205761551857
CurrentTrain: epoch  3, batch    88 | loss: 4.1366315Losses:  4.036772727966309 0.09449250251054764
CurrentTrain: epoch  3, batch    89 | loss: 4.1312652Losses:  4.069324016571045 0.12498955428600311
CurrentTrain: epoch  3, batch    90 | loss: 4.1943135Losses:  4.147271156311035 0.13281092047691345
CurrentTrain: epoch  3, batch    91 | loss: 4.2800822Losses:  4.09025764465332 0.20891325175762177
CurrentTrain: epoch  3, batch    92 | loss: 4.2991710Losses:  4.057282447814941 0.059835225343704224
CurrentTrain: epoch  3, batch    93 | loss: 4.1171179Losses:  4.147489070892334 0.1360139399766922
CurrentTrain: epoch  3, batch    94 | loss: 4.2835031Losses:  4.051738262176514 0.08122318983078003
CurrentTrain: epoch  3, batch    95 | loss: 4.1329613Losses:  4.167757987976074 0.14170074462890625
CurrentTrain: epoch  3, batch    96 | loss: 4.3094587Losses:  4.0625176429748535 0.09854806214570999
CurrentTrain: epoch  3, batch    97 | loss: 4.1610656Losses:  4.006565570831299 0.1076209545135498
CurrentTrain: epoch  3, batch    98 | loss: 4.1141863Losses:  3.9898529052734375 0.06316640973091125
CurrentTrain: epoch  3, batch    99 | loss: 4.0530195Losses:  4.14999532699585 0.13889358937740326
CurrentTrain: epoch  3, batch   100 | loss: 4.2888889Losses:  4.09881067276001 0.1111166924238205
CurrentTrain: epoch  3, batch   101 | loss: 4.2099276Losses:  4.0229878425598145 0.12086077779531479
CurrentTrain: epoch  3, batch   102 | loss: 4.1438484Losses:  3.9903366565704346 0.09310519695281982
CurrentTrain: epoch  3, batch   103 | loss: 4.0834417Losses:  4.059972763061523 0.07613350450992584
CurrentTrain: epoch  3, batch   104 | loss: 4.1361065Losses:  3.995791435241699 0.08237983286380768
CurrentTrain: epoch  3, batch   105 | loss: 4.0781713Losses:  3.9658453464508057 0.10235248506069183
CurrentTrain: epoch  3, batch   106 | loss: 4.0681977Losses:  4.031552791595459 0.048639290034770966
CurrentTrain: epoch  3, batch   107 | loss: 4.0801921Losses:  4.021540641784668 0.11790189146995544
CurrentTrain: epoch  3, batch   108 | loss: 4.1394424Losses:  4.058779716491699 0.10306867957115173
CurrentTrain: epoch  3, batch   109 | loss: 4.1618485Losses:  4.036096572875977 0.10229869931936264
CurrentTrain: epoch  3, batch   110 | loss: 4.1383953Losses:  4.078866958618164 0.08965839445590973
CurrentTrain: epoch  3, batch   111 | loss: 4.1685252Losses:  4.103068828582764 0.0947168692946434
CurrentTrain: epoch  3, batch   112 | loss: 4.1977859Losses:  4.049306869506836 0.085977703332901
CurrentTrain: epoch  3, batch   113 | loss: 4.1352844Losses:  3.9180774688720703 0.13887456059455872
CurrentTrain: epoch  3, batch   114 | loss: 4.0569520Losses:  4.0105719566345215 0.047513000667095184
CurrentTrain: epoch  3, batch   115 | loss: 4.0580850Losses:  4.018784523010254 0.10628484934568405
CurrentTrain: epoch  3, batch   116 | loss: 4.1250691Losses:  4.020123481750488 0.11234577745199203
CurrentTrain: epoch  3, batch   117 | loss: 4.1324692Losses:  4.01261043548584 0.08234076201915741
CurrentTrain: epoch  3, batch   118 | loss: 4.0949512Losses:  4.001652717590332 0.17338836193084717
CurrentTrain: epoch  3, batch   119 | loss: 4.1750412Losses:  4.088595390319824 0.13046634197235107
CurrentTrain: epoch  3, batch   120 | loss: 4.2190619Losses:  3.921325922012329 0.11645624786615372
CurrentTrain: epoch  3, batch   121 | loss: 4.0377822Losses:  4.019447326660156 0.11624212563037872
CurrentTrain: epoch  3, batch   122 | loss: 4.1356893Losses:  4.1441240310668945 0.16604852676391602
CurrentTrain: epoch  3, batch   123 | loss: 4.3101726Losses:  4.061214447021484 0.10842929035425186
CurrentTrain: epoch  3, batch   124 | loss: 4.1696439Losses:  4.020451545715332 0.11036500334739685
CurrentTrain: epoch  4, batch     0 | loss: 4.1308165Losses:  4.047974586486816 0.11738166958093643
CurrentTrain: epoch  4, batch     1 | loss: 4.1653562Losses:  3.9544310569763184 0.07972629368305206
CurrentTrain: epoch  4, batch     2 | loss: 4.0341573Losses:  4.0086445808410645 0.15292741358280182
CurrentTrain: epoch  4, batch     3 | loss: 4.1615720Losses:  3.9986412525177 0.05551721900701523
CurrentTrain: epoch  4, batch     4 | loss: 4.0541587Losses:  4.459287643432617 0.16063973307609558
CurrentTrain: epoch  4, batch     5 | loss: 4.6199274Losses:  3.960702419281006 0.10268509387969971
CurrentTrain: epoch  4, batch     6 | loss: 4.0633874Losses:  4.013238906860352 0.12914592027664185
CurrentTrain: epoch  4, batch     7 | loss: 4.1423850Losses:  3.9150192737579346 0.10408549755811691
CurrentTrain: epoch  4, batch     8 | loss: 4.0191050Losses:  4.066898822784424 0.12737753987312317
CurrentTrain: epoch  4, batch     9 | loss: 4.1942763Losses:  4.019752025604248 0.06407444179058075
CurrentTrain: epoch  4, batch    10 | loss: 4.0838265Losses:  4.059345245361328 0.08043276518583298
CurrentTrain: epoch  4, batch    11 | loss: 4.1397781Losses:  4.027859687805176 0.12858334183692932
CurrentTrain: epoch  4, batch    12 | loss: 4.1564431Losses:  4.152972221374512 0.050051454454660416
CurrentTrain: epoch  4, batch    13 | loss: 4.2030239Losses:  3.9987144470214844 0.09712585806846619
CurrentTrain: epoch  4, batch    14 | loss: 4.0958405Losses:  3.982128858566284 0.10988153517246246
CurrentTrain: epoch  4, batch    15 | loss: 4.0920105Losses:  4.059224605560303 0.15968631207942963
CurrentTrain: epoch  4, batch    16 | loss: 4.2189107Losses:  4.039804458618164 0.13314254581928253
CurrentTrain: epoch  4, batch    17 | loss: 4.1729469Losses:  4.032537460327148 0.14599460363388062
CurrentTrain: epoch  4, batch    18 | loss: 4.1785321Losses:  4.1133928298950195 0.06964272260665894
CurrentTrain: epoch  4, batch    19 | loss: 4.1830354Losses:  4.055794715881348 0.08806587755680084
CurrentTrain: epoch  4, batch    20 | loss: 4.1438608Losses:  4.031041145324707 0.11662012338638306
CurrentTrain: epoch  4, batch    21 | loss: 4.1476612Losses:  4.046014308929443 0.06354079395532608
CurrentTrain: epoch  4, batch    22 | loss: 4.1095552Losses:  3.9652867317199707 0.06357219815254211
CurrentTrain: epoch  4, batch    23 | loss: 4.0288591Losses:  3.9751715660095215 0.05205179750919342
CurrentTrain: epoch  4, batch    24 | loss: 4.0272236Losses:  3.978832483291626 0.1008462905883789
CurrentTrain: epoch  4, batch    25 | loss: 4.0796785Losses:  4.020223617553711 0.1075504869222641
CurrentTrain: epoch  4, batch    26 | loss: 4.1277742Losses:  4.004613876342773 0.17546802759170532
CurrentTrain: epoch  4, batch    27 | loss: 4.1800818Losses:  4.046058177947998 0.05613633245229721
CurrentTrain: epoch  4, batch    28 | loss: 4.1021943Losses:  4.075348377227783 0.04401010274887085
CurrentTrain: epoch  4, batch    29 | loss: 4.1193585Losses:  4.036431312561035 0.1014392226934433
CurrentTrain: epoch  4, batch    30 | loss: 4.1378703Losses:  4.007306098937988 0.06783551722764969
CurrentTrain: epoch  4, batch    31 | loss: 4.0751414Losses:  3.982907295227051 0.11525130271911621
CurrentTrain: epoch  4, batch    32 | loss: 4.0981588Losses:  3.938629388809204 0.07497330009937286
CurrentTrain: epoch  4, batch    33 | loss: 4.0136027Losses:  4.043179035186768 0.10080248862504959
CurrentTrain: epoch  4, batch    34 | loss: 4.1439815Losses:  4.048457145690918 0.05817709490656853
CurrentTrain: epoch  4, batch    35 | loss: 4.1066341Losses:  3.9606661796569824 0.15384860336780548
CurrentTrain: epoch  4, batch    36 | loss: 4.1145148Losses:  3.9773802757263184 0.09773010015487671
CurrentTrain: epoch  4, batch    37 | loss: 4.0751104Losses:  3.9897708892822266 0.0819820836186409
CurrentTrain: epoch  4, batch    38 | loss: 4.0717530Losses:  4.024550437927246 0.14689546823501587
CurrentTrain: epoch  4, batch    39 | loss: 4.1714458Losses:  3.97847843170166 0.09674172103404999
CurrentTrain: epoch  4, batch    40 | loss: 4.0752201Losses:  4.043237209320068 0.07129667699337006
CurrentTrain: epoch  4, batch    41 | loss: 4.1145339Losses:  4.038222789764404 0.09105034172534943
CurrentTrain: epoch  4, batch    42 | loss: 4.1292729Losses:  3.971200704574585 0.053551800549030304
CurrentTrain: epoch  4, batch    43 | loss: 4.0247526Losses:  3.9963693618774414 0.10121173411607742
CurrentTrain: epoch  4, batch    44 | loss: 4.0975809Losses:  4.014369010925293 0.07097433507442474
CurrentTrain: epoch  4, batch    45 | loss: 4.0853434Losses:  4.001219272613525 0.10605071485042572
CurrentTrain: epoch  4, batch    46 | loss: 4.1072698Losses:  3.9964466094970703 0.10634100437164307
CurrentTrain: epoch  4, batch    47 | loss: 4.1027875Losses:  3.9906563758850098 0.06231847032904625
CurrentTrain: epoch  4, batch    48 | loss: 4.0529747Losses:  3.978099822998047 0.099972203373909
CurrentTrain: epoch  4, batch    49 | loss: 4.0780721Losses:  3.965691566467285 0.14684532582759857
CurrentTrain: epoch  4, batch    50 | loss: 4.1125369Losses:  4.042535781860352 0.11736318469047546
CurrentTrain: epoch  4, batch    51 | loss: 4.1598988Losses:  4.004233360290527 0.059139277786016464
CurrentTrain: epoch  4, batch    52 | loss: 4.0633726Losses:  3.9788150787353516 0.1019248515367508
CurrentTrain: epoch  4, batch    53 | loss: 4.0807400Losses:  3.934795379638672 0.11765655875205994
CurrentTrain: epoch  4, batch    54 | loss: 4.0524521Losses:  4.0502729415893555 0.06488461047410965
CurrentTrain: epoch  4, batch    55 | loss: 4.1151576Losses:  3.96643328666687 0.12465677410364151
CurrentTrain: epoch  4, batch    56 | loss: 4.0910902Losses:  4.025612831115723 0.0938510149717331
CurrentTrain: epoch  4, batch    57 | loss: 4.1194639Losses:  4.030829906463623 0.046079643070697784
CurrentTrain: epoch  4, batch    58 | loss: 4.0769095Losses:  4.0922088623046875 0.09491939842700958
CurrentTrain: epoch  4, batch    59 | loss: 4.1871281Losses:  4.0314531326293945 0.11786989122629166
CurrentTrain: epoch  4, batch    60 | loss: 4.1493230Losses:  4.0008440017700195 0.06761016696691513
CurrentTrain: epoch  4, batch    61 | loss: 4.0684543Losses:  4.059943675994873 0.06462863087654114
CurrentTrain: epoch  4, batch    62 | loss: 4.1245723Losses:  4.0687761306762695 0.04500821977853775
CurrentTrain: epoch  4, batch    63 | loss: 4.1137843Losses:  4.010942459106445 0.05956973135471344
CurrentTrain: epoch  4, batch    64 | loss: 4.0705123Losses:  4.004418849945068 0.08655813336372375
CurrentTrain: epoch  4, batch    65 | loss: 4.0909772Losses:  3.9990732669830322 0.04291167110204697
CurrentTrain: epoch  4, batch    66 | loss: 4.0419850Losses:  4.089249610900879 0.04585495591163635
CurrentTrain: epoch  4, batch    67 | loss: 4.1351047Losses:  3.9658188819885254 0.04279501736164093
CurrentTrain: epoch  4, batch    68 | loss: 4.0086141Losses:  4.032495498657227 0.030148036777973175
CurrentTrain: epoch  4, batch    69 | loss: 4.0626435Losses:  3.9898767471313477 0.09301278740167618
CurrentTrain: epoch  4, batch    70 | loss: 4.0828896Losses:  3.990125894546509 0.11076349020004272
CurrentTrain: epoch  4, batch    71 | loss: 4.1008892Losses:  4.001967906951904 0.07876507192850113
CurrentTrain: epoch  4, batch    72 | loss: 4.0807328Losses:  4.0116472244262695 0.08156287670135498
CurrentTrain: epoch  4, batch    73 | loss: 4.0932102Losses:  4.049743175506592 0.10013915598392487
CurrentTrain: epoch  4, batch    74 | loss: 4.1498823Losses:  4.052926063537598 0.0861726701259613
CurrentTrain: epoch  4, batch    75 | loss: 4.1390986Losses:  4.029666900634766 0.09268177300691605
CurrentTrain: epoch  4, batch    76 | loss: 4.1223488Losses:  4.015433311462402 0.04446745663881302
CurrentTrain: epoch  4, batch    77 | loss: 4.0599008Losses:  4.018911838531494 0.08246567845344543
CurrentTrain: epoch  4, batch    78 | loss: 4.1013775Losses:  4.019257545471191 0.057111285626888275
CurrentTrain: epoch  4, batch    79 | loss: 4.0763688Losses:  4.001733303070068 0.09452518820762634
CurrentTrain: epoch  4, batch    80 | loss: 4.0962586Losses:  3.9907708168029785 0.10194168984889984
CurrentTrain: epoch  4, batch    81 | loss: 4.0927124Losses:  3.997285842895508 0.06469587981700897
CurrentTrain: epoch  4, batch    82 | loss: 4.0619817Losses:  3.9948887825012207 0.06392288953065872
CurrentTrain: epoch  4, batch    83 | loss: 4.0588117Losses:  3.983041524887085 0.06348000466823578
CurrentTrain: epoch  4, batch    84 | loss: 4.0465217Losses:  3.972015857696533 0.09635785222053528
CurrentTrain: epoch  4, batch    85 | loss: 4.0683737Losses:  4.004373073577881 0.09477682411670685
CurrentTrain: epoch  4, batch    86 | loss: 4.0991497Losses:  4.046189308166504 0.04592020809650421
CurrentTrain: epoch  4, batch    87 | loss: 4.0921097Losses:  3.995981454849243 0.05244767665863037
CurrentTrain: epoch  4, batch    88 | loss: 4.0484290Losses:  3.9800572395324707 0.08392685651779175
CurrentTrain: epoch  4, batch    89 | loss: 4.0639839Losses:  3.9617438316345215 0.1102646142244339
CurrentTrain: epoch  4, batch    90 | loss: 4.0720086Losses:  3.959062337875366 0.08601382374763489
CurrentTrain: epoch  4, batch    91 | loss: 4.0450764Losses:  3.952901601791382 0.05148404836654663
CurrentTrain: epoch  4, batch    92 | loss: 4.0043855Losses:  4.000827312469482 0.09544306993484497
CurrentTrain: epoch  4, batch    93 | loss: 4.0962706Losses:  4.050323486328125 0.05707805976271629
CurrentTrain: epoch  4, batch    94 | loss: 4.1074014Losses:  3.9118595123291016 0.05138768255710602
CurrentTrain: epoch  4, batch    95 | loss: 3.9632473Losses:  3.974295139312744 0.0661773607134819
CurrentTrain: epoch  4, batch    96 | loss: 4.0404725Losses:  4.005328178405762 0.07426835596561432
CurrentTrain: epoch  4, batch    97 | loss: 4.0795965Losses:  4.020528316497803 0.0725947767496109
CurrentTrain: epoch  4, batch    98 | loss: 4.0931230Losses:  4.036714553833008 0.09315230697393417
CurrentTrain: epoch  4, batch    99 | loss: 4.1298671Losses:  3.949093818664551 0.06302187591791153
CurrentTrain: epoch  4, batch   100 | loss: 4.0121155Losses:  3.9810047149658203 0.09176261723041534
CurrentTrain: epoch  4, batch   101 | loss: 4.0727673Losses:  4.0112624168396 0.06961871683597565
CurrentTrain: epoch  4, batch   102 | loss: 4.0808811Losses:  4.006577491760254 0.07004617899656296
CurrentTrain: epoch  4, batch   103 | loss: 4.0766234Losses:  3.9729690551757812 0.07122210413217545
CurrentTrain: epoch  4, batch   104 | loss: 4.0441914Losses:  3.9686408042907715 0.0961977019906044
CurrentTrain: epoch  4, batch   105 | loss: 4.0648384Losses:  3.9887897968292236 0.042689912021160126
CurrentTrain: epoch  4, batch   106 | loss: 4.0314798Losses:  4.009138107299805 0.12799574434757233
CurrentTrain: epoch  4, batch   107 | loss: 4.1371341Losses:  3.9294378757476807 0.10853023827075958
CurrentTrain: epoch  4, batch   108 | loss: 4.0379682Losses:  3.99942946434021 0.05972151458263397
CurrentTrain: epoch  4, batch   109 | loss: 4.0591512Losses:  3.949521541595459 0.10741782188415527
CurrentTrain: epoch  4, batch   110 | loss: 4.0569391Losses:  4.006839275360107 0.07404990494251251
CurrentTrain: epoch  4, batch   111 | loss: 4.0808892Losses:  3.993926763534546 0.09217870235443115
CurrentTrain: epoch  4, batch   112 | loss: 4.0861053Losses:  3.8983733654022217 0.05119682103395462
CurrentTrain: epoch  4, batch   113 | loss: 3.9495702Losses:  3.9827418327331543 0.03881673142313957
CurrentTrain: epoch  4, batch   114 | loss: 4.0215588Losses:  3.996180534362793 0.14609789848327637
CurrentTrain: epoch  4, batch   115 | loss: 4.1422787Losses:  4.009737491607666 0.06697641313076019
CurrentTrain: epoch  4, batch   116 | loss: 4.0767140Losses:  4.039589881896973 0.13489292562007904
CurrentTrain: epoch  4, batch   117 | loss: 4.1744828Losses:  3.886228561401367 0.07920350134372711
CurrentTrain: epoch  4, batch   118 | loss: 3.9654322Losses:  3.9886655807495117 0.09062597155570984
CurrentTrain: epoch  4, batch   119 | loss: 4.0792913Losses:  3.924776315689087 0.10723524540662766
CurrentTrain: epoch  4, batch   120 | loss: 4.0320115Losses:  4.0258893966674805 0.039663270115852356
CurrentTrain: epoch  4, batch   121 | loss: 4.0655527Losses:  3.948712110519409 0.08587154746055603
CurrentTrain: epoch  4, batch   122 | loss: 4.0345836Losses:  3.9943509101867676 0.07896901667118073
CurrentTrain: epoch  4, batch   123 | loss: 4.0733199Losses:  4.034054756164551 0.04098077118396759
CurrentTrain: epoch  4, batch   124 | loss: 4.0750356Losses:  3.9554624557495117 0.08812551200389862
CurrentTrain: epoch  5, batch     0 | loss: 4.0435882Losses:  3.9896044731140137 0.0765269547700882
CurrentTrain: epoch  5, batch     1 | loss: 4.0661316Losses:  3.969627618789673 0.061380110681056976
CurrentTrain: epoch  5, batch     2 | loss: 4.0310078Losses:  3.9618124961853027 0.06994421035051346
CurrentTrain: epoch  5, batch     3 | loss: 4.0317569Losses:  3.9841105937957764 0.0734168067574501
CurrentTrain: epoch  5, batch     4 | loss: 4.0575275Losses:  3.9922823905944824 0.040299974381923676
CurrentTrain: epoch  5, batch     5 | loss: 4.0325823Losses:  3.9943392276763916 0.056547798216342926
CurrentTrain: epoch  5, batch     6 | loss: 4.0508871Losses:  3.9997518062591553 0.05744735896587372
CurrentTrain: epoch  5, batch     7 | loss: 4.0571990Losses:  3.970292091369629 0.06874806433916092
CurrentTrain: epoch  5, batch     8 | loss: 4.0390401Losses:  3.9824769496917725 0.10831145197153091
CurrentTrain: epoch  5, batch     9 | loss: 4.0907884Losses:  3.8354344367980957 0.048648323863744736
CurrentTrain: epoch  5, batch    10 | loss: 3.8840828Losses:  3.952113151550293 0.06455180794000626
CurrentTrain: epoch  5, batch    11 | loss: 4.0166650Losses:  3.938303232192993 0.089919812977314
CurrentTrain: epoch  5, batch    12 | loss: 4.0282230Losses:  3.9620397090911865 0.051457297056913376
CurrentTrain: epoch  5, batch    13 | loss: 4.0134969Losses:  4.015387058258057 0.09937684237957001
CurrentTrain: epoch  5, batch    14 | loss: 4.1147637Losses:  3.9950482845306396 0.042913615703582764
CurrentTrain: epoch  5, batch    15 | loss: 4.0379620Losses:  4.042755126953125 0.09946030378341675
CurrentTrain: epoch  5, batch    16 | loss: 4.1422153Losses:  4.004153728485107 0.07620534300804138
CurrentTrain: epoch  5, batch    17 | loss: 4.0803590Losses:  3.9531009197235107 0.04727887362241745
CurrentTrain: epoch  5, batch    18 | loss: 4.0003796Losses:  3.9375648498535156 0.09128423780202866
CurrentTrain: epoch  5, batch    19 | loss: 4.0288491Losses:  3.9694666862487793 0.07760980725288391
CurrentTrain: epoch  5, batch    20 | loss: 4.0470767Losses:  3.9108095169067383 0.0812833160161972
CurrentTrain: epoch  5, batch    21 | loss: 3.9920928Losses:  3.999675750732422 0.05808454751968384
CurrentTrain: epoch  5, batch    22 | loss: 4.0577602Losses:  3.9549551010131836 0.06152809411287308
CurrentTrain: epoch  5, batch    23 | loss: 4.0164833Losses:  3.9520163536071777 0.07177110016345978
CurrentTrain: epoch  5, batch    24 | loss: 4.0237875Losses:  3.962109327316284 0.04987984895706177
CurrentTrain: epoch  5, batch    25 | loss: 4.0119891Losses:  3.9692773818969727 0.07802210748195648
CurrentTrain: epoch  5, batch    26 | loss: 4.0472994Losses:  3.9662585258483887 0.07302546501159668
CurrentTrain: epoch  5, batch    27 | loss: 4.0392838Losses:  3.9518160820007324 0.07241551578044891
CurrentTrain: epoch  5, batch    28 | loss: 4.0242314Losses:  4.003044128417969 0.02925262600183487
CurrentTrain: epoch  5, batch    29 | loss: 4.0322967Losses:  4.00627326965332 0.0982915610074997
CurrentTrain: epoch  5, batch    30 | loss: 4.1045647Losses:  4.003440856933594 0.05549173802137375
CurrentTrain: epoch  5, batch    31 | loss: 4.0589328Losses:  3.965759754180908 0.07912751287221909
CurrentTrain: epoch  5, batch    32 | loss: 4.0448871Losses:  3.948901414871216 0.08308961242437363
CurrentTrain: epoch  5, batch    33 | loss: 4.0319910Losses:  4.0050435066223145 0.076418936252594
CurrentTrain: epoch  5, batch    34 | loss: 4.0814624Losses:  3.93802547454834 0.0763034075498581
CurrentTrain: epoch  5, batch    35 | loss: 4.0143290Losses:  3.951350212097168 0.05007633566856384
CurrentTrain: epoch  5, batch    36 | loss: 4.0014267Losses:  3.9936819076538086 0.06927841901779175
CurrentTrain: epoch  5, batch    37 | loss: 4.0629601Losses:  3.97090482711792 0.06538447737693787
CurrentTrain: epoch  5, batch    38 | loss: 4.0362892Losses:  3.9670662879943848 0.08103686571121216
CurrentTrain: epoch  5, batch    39 | loss: 4.0481033Losses:  4.035645961761475 0.08468684554100037
CurrentTrain: epoch  5, batch    40 | loss: 4.1203327Losses:  4.011875152587891 0.07796257734298706
CurrentTrain: epoch  5, batch    41 | loss: 4.0898376Losses:  3.9259471893310547 0.032101184129714966
CurrentTrain: epoch  5, batch    42 | loss: 3.9580483Losses:  4.012863636016846 0.07914307713508606
CurrentTrain: epoch  5, batch    43 | loss: 4.0920067Losses:  3.9787516593933105 0.07074491679668427
CurrentTrain: epoch  5, batch    44 | loss: 4.0494967Losses:  3.9944729804992676 0.11687703430652618
CurrentTrain: epoch  5, batch    45 | loss: 4.1113501Losses:  4.005290985107422 0.0695260539650917
CurrentTrain: epoch  5, batch    46 | loss: 4.0748172Losses:  3.923553943634033 0.06109751760959625
CurrentTrain: epoch  5, batch    47 | loss: 3.9846516Losses:  3.9025797843933105 0.05376167967915535
CurrentTrain: epoch  5, batch    48 | loss: 3.9563415Losses:  3.978882074356079 0.06736066192388535
CurrentTrain: epoch  5, batch    49 | loss: 4.0462427Losses:  3.9446678161621094 0.0961054340004921
CurrentTrain: epoch  5, batch    50 | loss: 4.0407734Losses:  3.9665040969848633 0.05762767419219017
CurrentTrain: epoch  5, batch    51 | loss: 4.0241318Losses:  3.973897695541382 0.05398330092430115
CurrentTrain: epoch  5, batch    52 | loss: 4.0278811Losses:  4.023360252380371 0.06878621131181717
CurrentTrain: epoch  5, batch    53 | loss: 4.0921464Losses:  4.003762245178223 0.07710281759500504
CurrentTrain: epoch  5, batch    54 | loss: 4.0808649Losses:  4.028266429901123 0.0556221604347229
CurrentTrain: epoch  5, batch    55 | loss: 4.0838885Losses:  3.979437828063965 0.06148165464401245
CurrentTrain: epoch  5, batch    56 | loss: 4.0409193Losses:  3.9981496334075928 0.07134237140417099
CurrentTrain: epoch  5, batch    57 | loss: 4.0694919Losses:  4.0262346267700195 0.05172187089920044
CurrentTrain: epoch  5, batch    58 | loss: 4.0779567Losses:  3.9720571041107178 0.07776954770088196
CurrentTrain: epoch  5, batch    59 | loss: 4.0498266Losses:  3.9226622581481934 0.024745874106884003
CurrentTrain: epoch  5, batch    60 | loss: 3.9474082Losses:  3.952040910720825 0.09127428382635117
CurrentTrain: epoch  5, batch    61 | loss: 4.0433154Losses:  3.9767894744873047 0.0713176429271698
CurrentTrain: epoch  5, batch    62 | loss: 4.0481071Losses:  3.9800610542297363 0.05471433699131012
CurrentTrain: epoch  5, batch    63 | loss: 4.0347753Losses:  3.971863269805908 0.0770253986120224
CurrentTrain: epoch  5, batch    64 | loss: 4.0488887Losses:  3.9166736602783203 0.08103024959564209
CurrentTrain: epoch  5, batch    65 | loss: 3.9977040Losses:  3.9586665630340576 0.07413166761398315
CurrentTrain: epoch  5, batch    66 | loss: 4.0327983Losses:  3.976830005645752 0.04874555766582489
CurrentTrain: epoch  5, batch    67 | loss: 4.0255756Losses:  3.9629106521606445 0.0879983976483345
CurrentTrain: epoch  5, batch    68 | loss: 4.0509090Losses:  4.006258964538574 0.034865692257881165
CurrentTrain: epoch  5, batch    69 | loss: 4.0411248Losses:  3.984623908996582 0.08874973654747009
CurrentTrain: epoch  5, batch    70 | loss: 4.0733738Losses:  3.9661192893981934 0.05835195630788803
CurrentTrain: epoch  5, batch    71 | loss: 4.0244713Losses:  4.003235816955566 0.046337276697158813
CurrentTrain: epoch  5, batch    72 | loss: 4.0495729Losses:  3.8835411071777344 0.05490340292453766
CurrentTrain: epoch  5, batch    73 | loss: 3.9384446Losses:  3.9717748165130615 0.09024884551763535
CurrentTrain: epoch  5, batch    74 | loss: 4.0620236Losses:  3.9709620475769043 0.06032594293355942
CurrentTrain: epoch  5, batch    75 | loss: 4.0312881Losses:  3.9785449504852295 0.07534634321928024
CurrentTrain: epoch  5, batch    76 | loss: 4.0538912Losses:  3.9580938816070557 0.05236492305994034
CurrentTrain: epoch  5, batch    77 | loss: 4.0104589Losses:  3.9483046531677246 0.06656701862812042
CurrentTrain: epoch  5, batch    78 | loss: 4.0148716Losses:  3.99045729637146 0.05857348442077637
CurrentTrain: epoch  5, batch    79 | loss: 4.0490308Losses:  3.9739904403686523 0.06454043835401535
CurrentTrain: epoch  5, batch    80 | loss: 4.0385308Losses:  3.9787912368774414 0.07368116825819016
CurrentTrain: epoch  5, batch    81 | loss: 4.0524726Losses:  3.992461919784546 0.07146034389734268
CurrentTrain: epoch  5, batch    82 | loss: 4.0639224Losses:  3.9536993503570557 0.05398229509592056
CurrentTrain: epoch  5, batch    83 | loss: 4.0076818Losses:  3.99057674407959 0.0621345192193985
CurrentTrain: epoch  5, batch    84 | loss: 4.0527115Losses:  3.8887462615966797 0.07191255688667297
CurrentTrain: epoch  5, batch    85 | loss: 3.9606588Losses:  3.985198497772217 0.048848722130060196
CurrentTrain: epoch  5, batch    86 | loss: 4.0340471Losses:  3.963412284851074 0.08493717014789581
CurrentTrain: epoch  5, batch    87 | loss: 4.0483494Losses:  3.971374988555908 0.029704183340072632
CurrentTrain: epoch  5, batch    88 | loss: 4.0010791Losses:  3.908655881881714 0.05184711515903473
CurrentTrain: epoch  5, batch    89 | loss: 3.9605031Losses:  4.041754722595215 0.03920084983110428
CurrentTrain: epoch  5, batch    90 | loss: 4.0809555Losses:  4.007328987121582 0.05007736757397652
CurrentTrain: epoch  5, batch    91 | loss: 4.0574064Losses:  3.9474265575408936 0.08853407949209213
CurrentTrain: epoch  5, batch    92 | loss: 4.0359607Losses:  3.9645862579345703 0.04122959449887276
CurrentTrain: epoch  5, batch    93 | loss: 4.0058160Losses:  4.060590744018555 0.03955106809735298
CurrentTrain: epoch  5, batch    94 | loss: 4.1001420Losses:  3.9932761192321777 0.07791340351104736
CurrentTrain: epoch  5, batch    95 | loss: 4.0711894Losses:  3.9759554862976074 0.08234105259180069
CurrentTrain: epoch  5, batch    96 | loss: 4.0582967Losses:  3.9653375148773193 0.043788064271211624
CurrentTrain: epoch  5, batch    97 | loss: 4.0091257Losses:  3.9446280002593994 0.08362869918346405
CurrentTrain: epoch  5, batch    98 | loss: 4.0282569Losses:  3.9863381385803223 0.10328742861747742
CurrentTrain: epoch  5, batch    99 | loss: 4.0896254Losses:  3.9788942337036133 0.0568944476544857
CurrentTrain: epoch  5, batch   100 | loss: 4.0357885Losses:  4.004024505615234 0.07960902154445648
CurrentTrain: epoch  5, batch   101 | loss: 4.0836334Losses:  3.949448347091675 0.05641639232635498
CurrentTrain: epoch  5, batch   102 | loss: 4.0058646Losses:  3.8881311416625977 0.09821879118680954
CurrentTrain: epoch  5, batch   103 | loss: 3.9863498Losses:  3.995199680328369 0.09095123410224915
CurrentTrain: epoch  5, batch   104 | loss: 4.0861511Losses:  3.9485650062561035 0.05798201635479927
CurrentTrain: epoch  5, batch   105 | loss: 4.0065470Losses:  3.9429831504821777 0.05154654383659363
CurrentTrain: epoch  5, batch   106 | loss: 3.9945297Losses:  3.891519546508789 0.09061166644096375
CurrentTrain: epoch  5, batch   107 | loss: 3.9821312Losses:  3.9441964626312256 0.06553676724433899
CurrentTrain: epoch  5, batch   108 | loss: 4.0097332Losses:  3.9970157146453857 0.04816010594367981
CurrentTrain: epoch  5, batch   109 | loss: 4.0451760Losses:  3.9787442684173584 0.09921537339687347
CurrentTrain: epoch  5, batch   110 | loss: 4.0779595Losses:  3.9472475051879883 0.07131078839302063
CurrentTrain: epoch  5, batch   111 | loss: 4.0185585Losses:  4.052578926086426 0.12019872665405273
CurrentTrain: epoch  5, batch   112 | loss: 4.1727777Losses:  3.952003002166748 0.04255625605583191
CurrentTrain: epoch  5, batch   113 | loss: 3.9945593Losses:  3.9747538566589355 0.06897259503602982
CurrentTrain: epoch  5, batch   114 | loss: 4.0437264Losses:  3.9827218055725098 0.055041324347257614
CurrentTrain: epoch  5, batch   115 | loss: 4.0377631Losses:  4.0038065910339355 0.02421867474913597
CurrentTrain: epoch  5, batch   116 | loss: 4.0280252Losses:  3.92868709564209 0.05036519467830658
CurrentTrain: epoch  5, batch   117 | loss: 3.9790523Losses:  3.951903820037842 0.06929396092891693
CurrentTrain: epoch  5, batch   118 | loss: 4.0211978Losses:  3.926941394805908 0.05091428756713867
CurrentTrain: epoch  5, batch   119 | loss: 3.9778557Losses:  3.913060426712036 0.030015474185347557
CurrentTrain: epoch  5, batch   120 | loss: 3.9430759Losses:  3.975935935974121 0.08498279005289078
CurrentTrain: epoch  5, batch   121 | loss: 4.0609188Losses:  3.9607937335968018 0.07056032121181488
CurrentTrain: epoch  5, batch   122 | loss: 4.0313540Losses:  4.011691093444824 0.06961346417665482
CurrentTrain: epoch  5, batch   123 | loss: 4.0813046Losses:  3.9166483879089355 0.0626821368932724
CurrentTrain: epoch  5, batch   124 | loss: 3.9793305Losses:  3.90285587310791 0.06469452381134033
CurrentTrain: epoch  6, batch     0 | loss: 3.9675503Losses:  3.9421334266662598 0.06828609108924866
CurrentTrain: epoch  6, batch     1 | loss: 4.0104194Losses:  3.937976598739624 0.040684834122657776
CurrentTrain: epoch  6, batch     2 | loss: 3.9786615Losses:  3.9548492431640625 0.028384726494550705
CurrentTrain: epoch  6, batch     3 | loss: 3.9832339Losses:  3.9404921531677246 0.08182665705680847
CurrentTrain: epoch  6, batch     4 | loss: 4.0223188Losses:  3.9261481761932373 0.03776688128709793
CurrentTrain: epoch  6, batch     5 | loss: 3.9639151Losses:  3.916354179382324 0.07242604345083237
CurrentTrain: epoch  6, batch     6 | loss: 3.9887803Losses:  3.9267520904541016 0.05510222911834717
CurrentTrain: epoch  6, batch     7 | loss: 3.9818544Losses:  3.868067502975464 0.07477109134197235
CurrentTrain: epoch  6, batch     8 | loss: 3.9428387Losses:  3.972046375274658 0.07900334894657135
CurrentTrain: epoch  6, batch     9 | loss: 4.0510497Losses:  4.005143165588379 0.05090835690498352
CurrentTrain: epoch  6, batch    10 | loss: 4.0560517Losses:  3.957242012023926 0.09821219742298126
CurrentTrain: epoch  6, batch    11 | loss: 4.0554543Losses:  3.9388303756713867 0.08183594793081284
CurrentTrain: epoch  6, batch    12 | loss: 4.0206661Losses:  3.923651933670044 0.07563992589712143
CurrentTrain: epoch  6, batch    13 | loss: 3.9992919Losses:  3.9286694526672363 0.061806052923202515
CurrentTrain: epoch  6, batch    14 | loss: 3.9904754Losses:  3.91632080078125 0.028713582083582878
CurrentTrain: epoch  6, batch    15 | loss: 3.9450343Losses:  3.9358022212982178 0.07669024169445038
CurrentTrain: epoch  6, batch    16 | loss: 4.0124927Losses:  3.997307538986206 0.04450395703315735
CurrentTrain: epoch  6, batch    17 | loss: 4.0418115Losses:  3.9859366416931152 0.06230289489030838
CurrentTrain: epoch  6, batch    18 | loss: 4.0482397Losses:  3.9820051193237305 0.05089380592107773
CurrentTrain: epoch  6, batch    19 | loss: 4.0328989Losses:  3.9300050735473633 0.06387067586183548
CurrentTrain: epoch  6, batch    20 | loss: 3.9938757Losses:  3.973304271697998 0.07281045615673065
CurrentTrain: epoch  6, batch    21 | loss: 4.0461149Losses:  3.979020357131958 0.04098847135901451
CurrentTrain: epoch  6, batch    22 | loss: 4.0200090Losses:  3.958052635192871 0.06517703086137772
CurrentTrain: epoch  6, batch    23 | loss: 4.0232296Losses:  3.9727559089660645 0.03923388943076134
CurrentTrain: epoch  6, batch    24 | loss: 4.0119896Losses:  3.911044120788574 0.03214242681860924
CurrentTrain: epoch  6, batch    25 | loss: 3.9431865Losses:  3.939661741256714 0.05698272958397865
CurrentTrain: epoch  6, batch    26 | loss: 3.9966445Losses:  3.950923442840576 0.09247243404388428
CurrentTrain: epoch  6, batch    27 | loss: 4.0433960Losses:  3.872605800628662 0.0906624123454094
CurrentTrain: epoch  6, batch    28 | loss: 3.9632683Losses:  3.9740066528320312 0.06833118945360184
CurrentTrain: epoch  6, batch    29 | loss: 4.0423379Losses:  3.9356563091278076 0.04139060899615288
CurrentTrain: epoch  6, batch    30 | loss: 3.9770470Losses:  3.9289231300354004 0.05880992114543915
CurrentTrain: epoch  6, batch    31 | loss: 3.9877331Losses:  3.9310507774353027 0.07884762436151505
CurrentTrain: epoch  6, batch    32 | loss: 4.0098982Losses:  3.948023796081543 0.0874723270535469
CurrentTrain: epoch  6, batch    33 | loss: 4.0354962Losses:  3.9120702743530273 0.08187079429626465
CurrentTrain: epoch  6, batch    34 | loss: 3.9939411Losses:  3.998918056488037 0.02887558378279209
CurrentTrain: epoch  6, batch    35 | loss: 4.0277934Losses:  3.9427599906921387 0.06220003589987755
CurrentTrain: epoch  6, batch    36 | loss: 4.0049601Losses:  4.118836879730225 0.10120981186628342
CurrentTrain: epoch  6, batch    37 | loss: 4.2200465Losses:  3.9880282878875732 0.06360971182584763
CurrentTrain: epoch  6, batch    38 | loss: 4.0516381Losses:  3.9799041748046875 0.05724892020225525
CurrentTrain: epoch  6, batch    39 | loss: 4.0371532Losses:  3.9009809494018555 0.020180419087409973
CurrentTrain: epoch  6, batch    40 | loss: 3.9211614Losses:  3.9776010513305664 0.04622606933116913
CurrentTrain: epoch  6, batch    41 | loss: 4.0238271Losses:  4.247688293457031 0.08198816329240799
CurrentTrain: epoch  6, batch    42 | loss: 4.3296766Losses:  3.890191078186035 0.08090714365243912
CurrentTrain: epoch  6, batch    43 | loss: 3.9710982Losses:  3.943635940551758 0.05768517032265663
CurrentTrain: epoch  6, batch    44 | loss: 4.0013213Losses:  3.9449241161346436 0.06896793842315674
CurrentTrain: epoch  6, batch    45 | loss: 4.0138922Losses:  3.9705896377563477 0.047163449227809906
CurrentTrain: epoch  6, batch    46 | loss: 4.0177531Losses:  3.9634475708007812 0.0544469878077507
CurrentTrain: epoch  6, batch    47 | loss: 4.0178947Losses:  3.9700324535369873 0.06036663427948952
CurrentTrain: epoch  6, batch    48 | loss: 4.0303993Losses:  4.114978790283203 0.05410953611135483
CurrentTrain: epoch  6, batch    49 | loss: 4.1690884Losses:  3.9863834381103516 0.06461334228515625
CurrentTrain: epoch  6, batch    50 | loss: 4.0509968Losses:  4.17905855178833 0.2117891013622284
CurrentTrain: epoch  6, batch    51 | loss: 4.3908477Losses:  4.022846221923828 0.04096522927284241
CurrentTrain: epoch  6, batch    52 | loss: 4.0638113Losses:  3.9610776901245117 0.04006309062242508
CurrentTrain: epoch  6, batch    53 | loss: 4.0011406Losses:  3.9964873790740967 0.036884985864162445
CurrentTrain: epoch  6, batch    54 | loss: 4.0333724Losses:  3.9105794429779053 0.04863637685775757
CurrentTrain: epoch  6, batch    55 | loss: 3.9592159Losses:  3.9629454612731934 0.053938128054142
CurrentTrain: epoch  6, batch    56 | loss: 4.0168834Losses:  4.182995796203613 0.1341785192489624
CurrentTrain: epoch  6, batch    57 | loss: 4.3171744Losses:  3.9741415977478027 0.060126494616270065
CurrentTrain: epoch  6, batch    58 | loss: 4.0342679Losses:  3.9671387672424316 0.0539097785949707
CurrentTrain: epoch  6, batch    59 | loss: 4.0210485Losses:  3.9385313987731934 0.031950294971466064
CurrentTrain: epoch  6, batch    60 | loss: 3.9704816Losses:  3.989609956741333 0.09005290269851685
CurrentTrain: epoch  6, batch    61 | loss: 4.0796628Losses:  3.9298062324523926 0.07578302919864655
CurrentTrain: epoch  6, batch    62 | loss: 4.0055895Losses:  3.932295322418213 0.08363935351371765
CurrentTrain: epoch  6, batch    63 | loss: 4.0159345Losses:  3.9818274974823 0.09075315296649933
CurrentTrain: epoch  6, batch    64 | loss: 4.0725808Losses:  3.973796844482422 0.07538731396198273
CurrentTrain: epoch  6, batch    65 | loss: 4.0491843Losses:  3.956976890563965 0.0544578917324543
CurrentTrain: epoch  6, batch    66 | loss: 4.0114346Losses:  3.9546124935150146 0.08647620677947998
CurrentTrain: epoch  6, batch    67 | loss: 4.0410886Losses:  3.945608139038086 0.09492254257202148
CurrentTrain: epoch  6, batch    68 | loss: 4.0405307Losses:  3.9805450439453125 0.042617298662662506
CurrentTrain: epoch  6, batch    69 | loss: 4.0231624Losses:  3.9858691692352295 0.0794248953461647
CurrentTrain: epoch  6, batch    70 | loss: 4.0652943Losses:  3.99094820022583 0.08122682571411133
CurrentTrain: epoch  6, batch    71 | loss: 4.0721750Losses:  3.9447481632232666 0.03522517904639244
CurrentTrain: epoch  6, batch    72 | loss: 3.9799733Losses:  3.964844226837158 0.06460535526275635
CurrentTrain: epoch  6, batch    73 | loss: 4.0294495Losses:  3.9670276641845703 0.031398408114910126
CurrentTrain: epoch  6, batch    74 | loss: 3.9984260Losses:  3.9267265796661377 0.04555865377187729
CurrentTrain: epoch  6, batch    75 | loss: 3.9722853Losses:  3.9524261951446533 0.05963693559169769
CurrentTrain: epoch  6, batch    76 | loss: 4.0120630Losses:  3.97918701171875 0.02815433032810688
CurrentTrain: epoch  6, batch    77 | loss: 4.0073414Losses:  3.9407267570495605 0.04745321720838547
CurrentTrain: epoch  6, batch    78 | loss: 3.9881799Losses:  3.9598355293273926 0.09939318150281906
CurrentTrain: epoch  6, batch    79 | loss: 4.0592289Losses:  3.9336376190185547 0.10167065262794495
CurrentTrain: epoch  6, batch    80 | loss: 4.0353084Losses:  3.940423011779785 0.05684788525104523
CurrentTrain: epoch  6, batch    81 | loss: 3.9972708Losses:  3.9678115844726562 0.0849297046661377
CurrentTrain: epoch  6, batch    82 | loss: 4.0527411Losses:  3.9458489418029785 0.044853441417217255
CurrentTrain: epoch  6, batch    83 | loss: 3.9907024Losses:  3.908634662628174 0.04932257533073425
CurrentTrain: epoch  6, batch    84 | loss: 3.9579573Losses:  3.9923958778381348 0.04421662911772728
CurrentTrain: epoch  6, batch    85 | loss: 4.0366125Losses:  3.9858012199401855 0.053006142377853394
CurrentTrain: epoch  6, batch    86 | loss: 4.0388074Losses:  3.8927931785583496 0.04725227132439613
CurrentTrain: epoch  6, batch    87 | loss: 3.9400454Losses:  3.983912944793701 0.05730653926730156
CurrentTrain: epoch  6, batch    88 | loss: 4.0412197Losses:  3.9139294624328613 0.0517202764749527
CurrentTrain: epoch  6, batch    89 | loss: 3.9656498Losses:  3.9286348819732666 0.025396784767508507
CurrentTrain: epoch  6, batch    90 | loss: 3.9540317Losses:  3.9726834297180176 0.058065060526132584
CurrentTrain: epoch  6, batch    91 | loss: 4.0307484Losses:  4.011310577392578 0.04381285235285759
CurrentTrain: epoch  6, batch    92 | loss: 4.0551233Losses:  3.947835683822632 0.0815870463848114
CurrentTrain: epoch  6, batch    93 | loss: 4.0294228Losses:  3.9672484397888184 0.047491371631622314
CurrentTrain: epoch  6, batch    94 | loss: 4.0147400Losses:  3.918360948562622 0.03959716111421585
CurrentTrain: epoch  6, batch    95 | loss: 3.9579582Losses:  4.003389358520508 0.053624771535396576
CurrentTrain: epoch  6, batch    96 | loss: 4.0570140Losses:  3.9672069549560547 0.05644809082150459
CurrentTrain: epoch  6, batch    97 | loss: 4.0236549Losses:  3.9478650093078613 0.060782965272665024
CurrentTrain: epoch  6, batch    98 | loss: 4.0086479Losses:  3.906773328781128 0.04076426103711128
CurrentTrain: epoch  6, batch    99 | loss: 3.9475377Losses:  3.9604299068450928 0.06081979721784592
CurrentTrain: epoch  6, batch   100 | loss: 4.0212498Losses:  3.9826414585113525 0.07200735062360764
CurrentTrain: epoch  6, batch   101 | loss: 4.0546489Losses:  3.960479974746704 0.062117792665958405
CurrentTrain: epoch  6, batch   102 | loss: 4.0225978Losses:  3.9933109283447266 0.04342499375343323
CurrentTrain: epoch  6, batch   103 | loss: 4.0367360Losses:  3.993058204650879 0.07212386280298233
CurrentTrain: epoch  6, batch   104 | loss: 4.0651822Losses:  3.9918627738952637 0.048778973519802094
CurrentTrain: epoch  6, batch   105 | loss: 4.0406418Losses:  3.8938345909118652 0.06761959195137024
CurrentTrain: epoch  6, batch   106 | loss: 3.9614542Losses:  3.941927433013916 0.07862062752246857
CurrentTrain: epoch  6, batch   107 | loss: 4.0205479Losses:  3.935441493988037 0.06920970231294632
CurrentTrain: epoch  6, batch   108 | loss: 4.0046511Losses:  3.9170520305633545 0.04787459224462509
CurrentTrain: epoch  6, batch   109 | loss: 3.9649267Losses:  3.9822781085968018 0.04420260712504387
CurrentTrain: epoch  6, batch   110 | loss: 4.0264807Losses:  3.946223258972168 0.051720406860113144
CurrentTrain: epoch  6, batch   111 | loss: 3.9979436Losses:  4.010952472686768 0.08502534031867981
CurrentTrain: epoch  6, batch   112 | loss: 4.0959778Losses:  3.9827136993408203 0.049007780849933624
CurrentTrain: epoch  6, batch   113 | loss: 4.0317216Losses:  4.003054141998291 0.06693591177463531
CurrentTrain: epoch  6, batch   114 | loss: 4.0699902Losses:  3.951486110687256 0.060565270483493805
CurrentTrain: epoch  6, batch   115 | loss: 4.0120516Losses:  3.9897611141204834 0.06541922688484192
CurrentTrain: epoch  6, batch   116 | loss: 4.0551805Losses:  3.9780263900756836 0.04899302497506142
CurrentTrain: epoch  6, batch   117 | loss: 4.0270195Losses:  3.9612178802490234 0.05294129252433777
CurrentTrain: epoch  6, batch   118 | loss: 4.0141592Losses:  3.9824914932250977 0.06125025823712349
CurrentTrain: epoch  6, batch   119 | loss: 4.0437417Losses:  3.9670939445495605 0.045910850167274475
CurrentTrain: epoch  6, batch   120 | loss: 4.0130048Losses:  3.944380044937134 0.05483671650290489
CurrentTrain: epoch  6, batch   121 | loss: 3.9992168Losses:  3.8899340629577637 0.0689009577035904
CurrentTrain: epoch  6, batch   122 | loss: 3.9588351Losses:  3.9304356575012207 0.0476737879216671
CurrentTrain: epoch  6, batch   123 | loss: 3.9781094Losses:  3.992886543273926 0.08046205341815948
CurrentTrain: epoch  6, batch   124 | loss: 4.0733485Losses:  4.010886192321777 0.02984604798257351
CurrentTrain: epoch  7, batch     0 | loss: 4.0407324Losses:  3.9805092811584473 0.057551801204681396
CurrentTrain: epoch  7, batch     1 | loss: 4.0380611Losses:  3.975001811981201 0.05732233077287674
CurrentTrain: epoch  7, batch     2 | loss: 4.0323243Losses:  3.953188419342041 0.06279563903808594
CurrentTrain: epoch  7, batch     3 | loss: 4.0159841Losses:  3.9534595012664795 0.042940154671669006
CurrentTrain: epoch  7, batch     4 | loss: 3.9963996Losses:  3.942523241043091 0.07788485288619995
CurrentTrain: epoch  7, batch     5 | loss: 4.0204082Losses:  3.976731538772583 0.06017725169658661
CurrentTrain: epoch  7, batch     6 | loss: 4.0369086Losses:  4.025587558746338 0.04318113625049591
CurrentTrain: epoch  7, batch     7 | loss: 4.0687685Losses:  3.9658498764038086 0.08828416466712952
CurrentTrain: epoch  7, batch     8 | loss: 4.0541339Losses:  3.947894811630249 0.04221922159194946
CurrentTrain: epoch  7, batch     9 | loss: 3.9901140Losses:  3.92616605758667 0.039280980825424194
CurrentTrain: epoch  7, batch    10 | loss: 3.9654469Losses:  3.945613145828247 0.048004671931266785
CurrentTrain: epoch  7, batch    11 | loss: 3.9936178Losses:  3.9494757652282715 0.05905485153198242
CurrentTrain: epoch  7, batch    12 | loss: 4.0085306Losses:  4.000810623168945 0.03353966772556305
CurrentTrain: epoch  7, batch    13 | loss: 4.0343504Losses:  3.982086181640625 0.049789492040872574
CurrentTrain: epoch  7, batch    14 | loss: 4.0318756Losses:  3.9404664039611816 0.04167592525482178
CurrentTrain: epoch  7, batch    15 | loss: 3.9821424Losses:  3.9925949573516846 0.03524467349052429
CurrentTrain: epoch  7, batch    16 | loss: 4.0278397Losses:  3.912951946258545 0.028287449851632118
CurrentTrain: epoch  7, batch    17 | loss: 3.9412394Losses:  3.9715371131896973 0.09278085827827454
CurrentTrain: epoch  7, batch    18 | loss: 4.0643182Losses:  3.9206695556640625 0.044110000133514404
CurrentTrain: epoch  7, batch    19 | loss: 3.9647796Losses:  3.9087696075439453 0.07104842364788055
CurrentTrain: epoch  7, batch    20 | loss: 3.9798181Losses:  3.946312189102173 0.07716594636440277
CurrentTrain: epoch  7, batch    21 | loss: 4.0234780Losses:  3.987924337387085 0.04097307473421097
CurrentTrain: epoch  7, batch    22 | loss: 4.0288973Losses:  3.991393804550171 0.02279600128531456
CurrentTrain: epoch  7, batch    23 | loss: 4.0141897Losses:  3.896425724029541 0.04978109151124954
CurrentTrain: epoch  7, batch    24 | loss: 3.9462068Losses:  3.9536356925964355 0.058780670166015625
CurrentTrain: epoch  7, batch    25 | loss: 4.0124164Losses:  3.9694175720214844 0.08719600737094879
CurrentTrain: epoch  7, batch    26 | loss: 4.0566134Losses:  3.9282307624816895 0.05338084697723389
CurrentTrain: epoch  7, batch    27 | loss: 3.9816117Losses:  3.944977283477783 0.05413390323519707
CurrentTrain: epoch  7, batch    28 | loss: 3.9991112Losses:  3.938445568084717 0.07728148996829987
CurrentTrain: epoch  7, batch    29 | loss: 4.0157270Losses:  3.9424657821655273 0.06048911064863205
CurrentTrain: epoch  7, batch    30 | loss: 4.0029550Losses:  3.9874234199523926 0.05470776557922363
CurrentTrain: epoch  7, batch    31 | loss: 4.0421314Losses:  3.955167531967163 0.025970183312892914
CurrentTrain: epoch  7, batch    32 | loss: 3.9811378Losses:  3.9811453819274902 0.03788973391056061
CurrentTrain: epoch  7, batch    33 | loss: 4.0190353Losses:  3.9975929260253906 0.033990856260061264
CurrentTrain: epoch  7, batch    34 | loss: 4.0315838Losses:  3.980989456176758 0.04757259041070938
CurrentTrain: epoch  7, batch    35 | loss: 4.0285621Losses:  3.94919490814209 0.02848624624311924
CurrentTrain: epoch  7, batch    36 | loss: 3.9776812Losses:  3.9404356479644775 0.03769844025373459
CurrentTrain: epoch  7, batch    37 | loss: 3.9781342Losses:  3.844440460205078 0.03218758851289749
CurrentTrain: epoch  7, batch    38 | loss: 3.8766282Losses:  4.01691198348999 0.10335580259561539
CurrentTrain: epoch  7, batch    39 | loss: 4.1202679Losses:  3.9616644382476807 0.04093273729085922
CurrentTrain: epoch  7, batch    40 | loss: 4.0025973Losses:  3.9668831825256348 0.05694574862718582
CurrentTrain: epoch  7, batch    41 | loss: 4.0238290Losses:  3.9905145168304443 0.049334149807691574
CurrentTrain: epoch  7, batch    42 | loss: 4.0398488Losses:  3.9719223976135254 0.03607828915119171
CurrentTrain: epoch  7, batch    43 | loss: 4.0080009Losses:  4.028879165649414 0.07105767726898193
CurrentTrain: epoch  7, batch    44 | loss: 4.0999370Losses:  3.9593801498413086 0.05749721825122833
CurrentTrain: epoch  7, batch    45 | loss: 4.0168772Losses:  3.9485340118408203 0.03993036225438118
CurrentTrain: epoch  7, batch    46 | loss: 3.9884644Losses:  3.8834941387176514 0.06690974533557892
CurrentTrain: epoch  7, batch    47 | loss: 3.9504039Losses:  3.9220874309539795 0.07417913526296616
CurrentTrain: epoch  7, batch    48 | loss: 3.9962666Losses:  3.982173204421997 0.04201239347457886
CurrentTrain: epoch  7, batch    49 | loss: 4.0241857Losses:  3.970829486846924 0.034730054438114166
CurrentTrain: epoch  7, batch    50 | loss: 4.0055594Losses:  3.937593460083008 0.0724966824054718
CurrentTrain: epoch  7, batch    51 | loss: 4.0100904Losses:  3.928100109100342 0.06205751374363899
CurrentTrain: epoch  7, batch    52 | loss: 3.9901576Losses:  3.985598087310791 0.033537279814481735
CurrentTrain: epoch  7, batch    53 | loss: 4.0191355Losses:  3.978179454803467 0.04395128786563873
CurrentTrain: epoch  7, batch    54 | loss: 4.0221310Losses:  3.9554195404052734 0.04639875143766403
CurrentTrain: epoch  7, batch    55 | loss: 4.0018182Losses:  3.939645528793335 0.033724866807460785
CurrentTrain: epoch  7, batch    56 | loss: 3.9733703Losses:  3.951848268508911 0.04412737861275673
CurrentTrain: epoch  7, batch    57 | loss: 3.9959757Losses:  3.9686732292175293 0.06781421601772308
CurrentTrain: epoch  7, batch    58 | loss: 4.0364876Losses:  3.9920332431793213 0.06617936491966248
CurrentTrain: epoch  7, batch    59 | loss: 4.0582128Losses:  3.9198830127716064 0.06875646859407425
CurrentTrain: epoch  7, batch    60 | loss: 3.9886396Losses:  3.959535837173462 0.06353303045034409
CurrentTrain: epoch  7, batch    61 | loss: 4.0230689Losses:  3.9648995399475098 0.0599074512720108
CurrentTrain: epoch  7, batch    62 | loss: 4.0248070Losses:  3.9039700031280518 0.041587140411138535
CurrentTrain: epoch  7, batch    63 | loss: 3.9455571Losses:  3.8988523483276367 0.03190198168158531
CurrentTrain: epoch  7, batch    64 | loss: 3.9307544Losses:  3.9355034828186035 0.056803710758686066
CurrentTrain: epoch  7, batch    65 | loss: 3.9923072Losses:  3.947575330734253 0.038093216717243195
CurrentTrain: epoch  7, batch    66 | loss: 3.9856687Losses:  3.980715274810791 0.038927435874938965
CurrentTrain: epoch  7, batch    67 | loss: 4.0196428Losses:  3.965111017227173 0.029603995382785797
CurrentTrain: epoch  7, batch    68 | loss: 3.9947150Losses:  3.9505937099456787 0.06712594628334045
CurrentTrain: epoch  7, batch    69 | loss: 4.0177197Losses:  3.944133758544922 0.026908941566944122
CurrentTrain: epoch  7, batch    70 | loss: 3.9710426Losses:  3.9761900901794434 0.052614372223615646
CurrentTrain: epoch  7, batch    71 | loss: 4.0288043Losses:  3.968506336212158 0.04033149033784866
CurrentTrain: epoch  7, batch    72 | loss: 4.0088377Losses:  3.9064292907714844 0.03735467046499252
CurrentTrain: epoch  7, batch    73 | loss: 3.9437840Losses:  3.940918445587158 0.06196359544992447
CurrentTrain: epoch  7, batch    74 | loss: 4.0028820Losses:  3.8895649909973145 0.05322236567735672
CurrentTrain: epoch  7, batch    75 | loss: 3.9427874Losses:  3.9673776626586914 0.03807058930397034
CurrentTrain: epoch  7, batch    76 | loss: 4.0054483Losses:  3.9465014934539795 0.028531616553664207
CurrentTrain: epoch  7, batch    77 | loss: 3.9750330Losses:  3.9770712852478027 0.0435614176094532
CurrentTrain: epoch  7, batch    78 | loss: 4.0206327Losses:  4.003826141357422 0.04737410321831703
CurrentTrain: epoch  7, batch    79 | loss: 4.0512004Losses:  3.884896993637085 0.04267324134707451
CurrentTrain: epoch  7, batch    80 | loss: 3.9275703Losses:  3.965604782104492 0.07258109748363495
CurrentTrain: epoch  7, batch    81 | loss: 4.0381861Losses:  3.936046600341797 0.043745413422584534
CurrentTrain: epoch  7, batch    82 | loss: 3.9797921Losses:  3.9452316761016846 0.043302226811647415
CurrentTrain: epoch  7, batch    83 | loss: 3.9885340Losses:  3.886615037918091 0.0500602126121521
CurrentTrain: epoch  7, batch    84 | loss: 3.9366753Losses:  3.9998583793640137 0.038340337574481964
CurrentTrain: epoch  7, batch    85 | loss: 4.0381989Losses:  3.9396867752075195 0.04935469478368759
CurrentTrain: epoch  7, batch    86 | loss: 3.9890416Losses:  3.8827757835388184 0.03591048717498779
CurrentTrain: epoch  7, batch    87 | loss: 3.9186864Losses:  3.9664864540100098 0.04790986701846123
CurrentTrain: epoch  7, batch    88 | loss: 4.0143962Losses:  3.938786029815674 0.05786144360899925
CurrentTrain: epoch  7, batch    89 | loss: 3.9966474Losses:  3.9059500694274902 0.0813349038362503
CurrentTrain: epoch  7, batch    90 | loss: 3.9872849Losses:  3.942446231842041 0.0384816899895668
CurrentTrain: epoch  7, batch    91 | loss: 3.9809279Losses:  3.9804110527038574 0.01700916327536106
CurrentTrain: epoch  7, batch    92 | loss: 3.9974203Losses:  3.9490890502929688 0.06514483690261841
CurrentTrain: epoch  7, batch    93 | loss: 4.0142341Losses:  3.941668748855591 0.036689240485429764
CurrentTrain: epoch  7, batch    94 | loss: 3.9783580Losses:  3.9075140953063965 0.04248446971178055
CurrentTrain: epoch  7, batch    95 | loss: 3.9499986Losses:  3.9342122077941895 0.05930584669113159
CurrentTrain: epoch  7, batch    96 | loss: 3.9935181Losses:  4.018681049346924 0.056634437292814255
CurrentTrain: epoch  7, batch    97 | loss: 4.0753155Losses:  3.943260908126831 0.05183029919862747
CurrentTrain: epoch  7, batch    98 | loss: 3.9950912Losses:  3.921649932861328 0.012064894661307335
CurrentTrain: epoch  7, batch    99 | loss: 3.9337149Losses:  3.8994979858398438 0.034288663417100906
CurrentTrain: epoch  7, batch   100 | loss: 3.9337866Losses:  3.988931179046631 0.04877728968858719
CurrentTrain: epoch  7, batch   101 | loss: 4.0377083Losses:  3.935605525970459 0.050844915211200714
CurrentTrain: epoch  7, batch   102 | loss: 3.9864504Losses:  3.9240782260894775 0.06227885186672211
CurrentTrain: epoch  7, batch   103 | loss: 3.9863570Losses:  3.977194309234619 0.039932072162628174
CurrentTrain: epoch  7, batch   104 | loss: 4.0171266Losses:  4.031309127807617 0.10604014247655869
CurrentTrain: epoch  7, batch   105 | loss: 4.1373491Losses:  3.9557621479034424 0.07924751192331314
CurrentTrain: epoch  7, batch   106 | loss: 4.0350099Losses:  3.9223904609680176 0.04875893145799637
CurrentTrain: epoch  7, batch   107 | loss: 3.9711494Losses:  3.953545570373535 0.03235665336251259
CurrentTrain: epoch  7, batch   108 | loss: 3.9859023Losses:  4.18836784362793 0.11421853303909302
CurrentTrain: epoch  7, batch   109 | loss: 4.3025866Losses:  3.91459059715271 0.06225282698869705
CurrentTrain: epoch  7, batch   110 | loss: 3.9768434Losses:  3.9880001544952393 0.050094302743673325
CurrentTrain: epoch  7, batch   111 | loss: 4.0380945Losses:  3.9471435546875 0.07090473920106888
CurrentTrain: epoch  7, batch   112 | loss: 4.0180483Losses:  3.9420347213745117 0.0254204198718071
CurrentTrain: epoch  7, batch   113 | loss: 3.9674551Losses:  3.9669041633605957 0.04666288197040558
CurrentTrain: epoch  7, batch   114 | loss: 4.0135670Losses:  3.9475584030151367 0.06188836693763733
CurrentTrain: epoch  7, batch   115 | loss: 4.0094466Losses:  3.96500563621521 0.024233363568782806
CurrentTrain: epoch  7, batch   116 | loss: 3.9892390Losses:  3.977203369140625 0.05451532080769539
CurrentTrain: epoch  7, batch   117 | loss: 4.0317187Losses:  3.918912410736084 0.06231499835848808
CurrentTrain: epoch  7, batch   118 | loss: 3.9812274Losses:  3.9685208797454834 0.02738027460873127
CurrentTrain: epoch  7, batch   119 | loss: 3.9959011Losses:  3.9566071033477783 0.08412154018878937
CurrentTrain: epoch  7, batch   120 | loss: 4.0407286Losses:  5.4188737869262695 0.5274814367294312
CurrentTrain: epoch  7, batch   121 | loss: 5.9463553Losses:  3.9667153358459473 0.11554817855358124
CurrentTrain: epoch  7, batch   122 | loss: 4.0822635Losses:  5.673503398895264 0.5946455597877502
CurrentTrain: epoch  7, batch   123 | loss: 6.2681489Losses:  3.984398365020752 0.04461286589503288
CurrentTrain: epoch  7, batch   124 | loss: 4.0290112Losses:  4.0083513259887695 0.04226381704211235
CurrentTrain: epoch  8, batch     0 | loss: 4.0506153Losses:  4.005774021148682 0.027253812178969383
CurrentTrain: epoch  8, batch     1 | loss: 4.0330276Losses:  3.982194662094116 0.08728696405887604
CurrentTrain: epoch  8, batch     2 | loss: 4.0694818Losses:  4.008996963500977 0.03518211841583252
CurrentTrain: epoch  8, batch     3 | loss: 4.0441790Losses:  4.100337028503418 0.089947909116745
CurrentTrain: epoch  8, batch     4 | loss: 4.1902847Losses:  3.9068796634674072 0.06730020046234131
CurrentTrain: epoch  8, batch     5 | loss: 3.9741797Losses:  4.018460750579834 0.04848315939307213
CurrentTrain: epoch  8, batch     6 | loss: 4.0669441Losses:  3.971676826477051 0.04325133562088013
CurrentTrain: epoch  8, batch     7 | loss: 4.0149283Losses:  3.901704788208008 0.031043320894241333
CurrentTrain: epoch  8, batch     8 | loss: 3.9327481Losses:  3.9562201499938965 0.03157399967312813
CurrentTrain: epoch  8, batch     9 | loss: 3.9877942Losses:  3.9876601696014404 0.06687594950199127
CurrentTrain: epoch  8, batch    10 | loss: 4.0545363Losses:  4.0308942794799805 0.05494094640016556
CurrentTrain: epoch  8, batch    11 | loss: 4.0858355Losses:  4.321345329284668 0.03759296238422394
CurrentTrain: epoch  8, batch    12 | loss: 4.3589382Losses:  4.081054210662842 0.06881056725978851
CurrentTrain: epoch  8, batch    13 | loss: 4.1498647Losses:  3.9816665649414062 0.03892926499247551
CurrentTrain: epoch  8, batch    14 | loss: 4.0205960Losses:  4.060111045837402 0.0773368775844574
CurrentTrain: epoch  8, batch    15 | loss: 4.1374478Losses:  3.9941418170928955 0.032944269478321075
CurrentTrain: epoch  8, batch    16 | loss: 4.0270863Losses:  3.942768096923828 0.04021075367927551
CurrentTrain: epoch  8, batch    17 | loss: 3.9829788Losses:  4.030729293823242 0.03251230716705322
CurrentTrain: epoch  8, batch    18 | loss: 4.0632415Losses:  3.984170913696289 0.030374161899089813
CurrentTrain: epoch  8, batch    19 | loss: 4.0145450Losses:  3.9462504386901855 0.03558860719203949
CurrentTrain: epoch  8, batch    20 | loss: 3.9818389Losses:  4.043540000915527 0.054502636194229126
CurrentTrain: epoch  8, batch    21 | loss: 4.0980425Losses:  3.9462437629699707 0.032601065933704376
CurrentTrain: epoch  8, batch    22 | loss: 3.9788449Losses:  4.058943748474121 0.07622519135475159
CurrentTrain: epoch  8, batch    23 | loss: 4.1351690Losses:  4.086846351623535 0.07627110183238983
CurrentTrain: epoch  8, batch    24 | loss: 4.1631174Losses:  3.9451513290405273 0.027874058112502098
CurrentTrain: epoch  8, batch    25 | loss: 3.9730253Losses:  3.9569220542907715 0.06197551265358925
CurrentTrain: epoch  8, batch    26 | loss: 4.0188975Losses:  3.944869041442871 0.05042277276515961
CurrentTrain: epoch  8, batch    27 | loss: 3.9952917Losses:  3.953275680541992 0.0432744026184082
CurrentTrain: epoch  8, batch    28 | loss: 3.9965501Losses:  3.9453790187835693 0.03478293865919113
CurrentTrain: epoch  8, batch    29 | loss: 3.9801619Losses:  3.9304332733154297 0.059498947113752365
CurrentTrain: epoch  8, batch    30 | loss: 3.9899323Losses:  3.9480533599853516 0.027392614632844925
CurrentTrain: epoch  8, batch    31 | loss: 3.9754460Losses:  3.9307518005371094 0.05855048820376396
CurrentTrain: epoch  8, batch    32 | loss: 3.9893024Losses:  3.95623779296875 0.028232518583536148
CurrentTrain: epoch  8, batch    33 | loss: 3.9844704Losses:  4.065213680267334 0.05415117368102074
CurrentTrain: epoch  8, batch    34 | loss: 4.1193647Losses:  3.9190196990966797 0.049179889261722565
CurrentTrain: epoch  8, batch    35 | loss: 3.9681995Losses:  3.888796806335449 0.06995100528001785
CurrentTrain: epoch  8, batch    36 | loss: 3.9587479Losses:  4.359319686889648 0.043852128088474274
CurrentTrain: epoch  8, batch    37 | loss: 4.4031720Losses:  3.9411585330963135 0.06093307584524155
CurrentTrain: epoch  8, batch    38 | loss: 4.0020914Losses:  3.9364378452301025 0.06921739876270294
CurrentTrain: epoch  8, batch    39 | loss: 4.0056553Losses:  3.9318385124206543 0.03881677985191345
CurrentTrain: epoch  8, batch    40 | loss: 3.9706552Losses:  3.9680979251861572 0.027254067361354828
CurrentTrain: epoch  8, batch    41 | loss: 3.9953520Losses:  3.983299970626831 0.04897056519985199
CurrentTrain: epoch  8, batch    42 | loss: 4.0322704Losses:  3.932624340057373 0.06456616520881653
CurrentTrain: epoch  8, batch    43 | loss: 3.9971905Losses:  3.9451370239257812 0.03175055980682373
CurrentTrain: epoch  8, batch    44 | loss: 3.9768877Losses:  4.0624494552612305 0.06648251414299011
CurrentTrain: epoch  8, batch    45 | loss: 4.1289320Losses:  4.088659763336182 0.021020397543907166
CurrentTrain: epoch  8, batch    46 | loss: 4.1096802Losses:  3.956570625305176 0.05029361695051193
CurrentTrain: epoch  8, batch    47 | loss: 4.0068641Losses:  4.013937950134277 0.04170944541692734
CurrentTrain: epoch  8, batch    48 | loss: 4.0556474Losses:  4.315486431121826 0.1459379643201828
CurrentTrain: epoch  8, batch    49 | loss: 4.4614244Losses:  4.288661956787109 0.07829468697309494
CurrentTrain: epoch  8, batch    50 | loss: 4.3669567Losses:  3.9281599521636963 0.058472201228141785
CurrentTrain: epoch  8, batch    51 | loss: 3.9866321Losses:  4.013764381408691 0.03588058799505234
CurrentTrain: epoch  8, batch    52 | loss: 4.0496449Losses:  4.086948871612549 0.0714046061038971
CurrentTrain: epoch  8, batch    53 | loss: 4.1583533Losses:  3.907662868499756 0.04281669110059738
CurrentTrain: epoch  8, batch    54 | loss: 3.9504795Losses:  3.9763054847717285 0.07319800555706024
CurrentTrain: epoch  8, batch    55 | loss: 4.0495033Losses:  3.8932557106018066 0.05862409994006157
CurrentTrain: epoch  8, batch    56 | loss: 3.9518797Losses:  3.9490573406219482 0.048946771770715714
CurrentTrain: epoch  8, batch    57 | loss: 3.9980042Losses:  4.098764896392822 0.06639261543750763
CurrentTrain: epoch  8, batch    58 | loss: 4.1651573Losses:  3.93231201171875 0.03435705602169037
CurrentTrain: epoch  8, batch    59 | loss: 3.9666691Losses:  3.8907389640808105 0.025868751108646393
CurrentTrain: epoch  8, batch    60 | loss: 3.9166076Losses:  3.960303544998169 0.05619954690337181
CurrentTrain: epoch  8, batch    61 | loss: 4.0165029Losses:  5.35565710067749 0.33165299892425537
CurrentTrain: epoch  8, batch    62 | loss: 5.6873102Losses:  4.640713691711426 0.09609009325504303
CurrentTrain: epoch  8, batch    63 | loss: 4.7368040Losses:  3.9251718521118164 0.0296342596411705
CurrentTrain: epoch  8, batch    64 | loss: 3.9548061Losses:  3.9387998580932617 0.0870475322008133
CurrentTrain: epoch  8, batch    65 | loss: 4.0258474Losses:  3.948103189468384 0.05270020663738251
CurrentTrain: epoch  8, batch    66 | loss: 4.0008035Losses:  3.983407735824585 0.050755664706230164
CurrentTrain: epoch  8, batch    67 | loss: 4.0341635Losses:  4.244993209838867 0.04781193286180496
CurrentTrain: epoch  8, batch    68 | loss: 4.2928052Losses:  3.9333291053771973 0.030985703691840172
CurrentTrain: epoch  8, batch    69 | loss: 3.9643147Losses:  3.920255184173584 0.060698214918375015
CurrentTrain: epoch  8, batch    70 | loss: 3.9809535Losses:  3.9838266372680664 0.04049655795097351
CurrentTrain: epoch  8, batch    71 | loss: 4.0243230Losses:  3.995980978012085 0.03703721612691879
CurrentTrain: epoch  8, batch    72 | loss: 4.0330181Losses:  3.969958782196045 0.10367536544799805
CurrentTrain: epoch  8, batch    73 | loss: 4.0736341Losses:  3.9499454498291016 0.06799820810556412
CurrentTrain: epoch  8, batch    74 | loss: 4.0179439Losses:  4.043577194213867 0.05077649652957916
CurrentTrain: epoch  8, batch    75 | loss: 4.0943537Losses:  3.882359027862549 0.04544079303741455
CurrentTrain: epoch  8, batch    76 | loss: 3.9277997Losses:  3.9426445960998535 0.04470318555831909
CurrentTrain: epoch  8, batch    77 | loss: 3.9873478Losses:  3.9520280361175537 0.08716768026351929
CurrentTrain: epoch  8, batch    78 | loss: 4.0391955Losses:  3.86677622795105 0.021965786814689636
CurrentTrain: epoch  8, batch    79 | loss: 3.8887420Losses:  3.9405856132507324 0.029221083968877792
CurrentTrain: epoch  8, batch    80 | loss: 3.9698067Losses:  4.088663101196289 0.054659001529216766
CurrentTrain: epoch  8, batch    81 | loss: 4.1433220Losses:  3.9248814582824707 0.043851666152477264
CurrentTrain: epoch  8, batch    82 | loss: 3.9687331Losses:  3.940819263458252 0.05262111872434616
CurrentTrain: epoch  8, batch    83 | loss: 3.9934404Losses:  3.977489471435547 0.04214116185903549
CurrentTrain: epoch  8, batch    84 | loss: 4.0196304Losses:  3.9794607162475586 0.03534935414791107
CurrentTrain: epoch  8, batch    85 | loss: 4.0148101Losses:  3.967730760574341 0.043366558849811554
CurrentTrain: epoch  8, batch    86 | loss: 4.0110974Losses:  3.9837496280670166 0.038673147559165955
CurrentTrain: epoch  8, batch    87 | loss: 4.0224228Losses:  3.969517707824707 0.06795471906661987
CurrentTrain: epoch  8, batch    88 | loss: 4.0374722Losses:  4.133399963378906 0.06407162547111511
CurrentTrain: epoch  8, batch    89 | loss: 4.1974716Losses:  3.9418866634368896 0.053441911935806274
CurrentTrain: epoch  8, batch    90 | loss: 3.9953287Losses:  3.931729793548584 0.04189151152968407
CurrentTrain: epoch  8, batch    91 | loss: 3.9736214Losses:  3.966381788253784 0.049855414777994156
CurrentTrain: epoch  8, batch    92 | loss: 4.0162373Losses:  3.9551143646240234 0.03934202343225479
CurrentTrain: epoch  8, batch    93 | loss: 3.9944563Losses:  4.098792552947998 0.024039827287197113
CurrentTrain: epoch  8, batch    94 | loss: 4.1228323Losses:  4.2592058181762695 0.047899167984724045
CurrentTrain: epoch  8, batch    95 | loss: 4.3071051Losses:  3.9576992988586426 0.021073024719953537
CurrentTrain: epoch  8, batch    96 | loss: 3.9787724Losses:  4.177416801452637 0.07729938626289368
CurrentTrain: epoch  8, batch    97 | loss: 4.2547164Losses:  4.039000988006592 0.04632553830742836
CurrentTrain: epoch  8, batch    98 | loss: 4.0853267Losses:  4.0642595291137695 0.05547333136200905
CurrentTrain: epoch  8, batch    99 | loss: 4.1197329Losses:  3.948915719985962 0.046343713998794556
CurrentTrain: epoch  8, batch   100 | loss: 3.9952595Losses:  4.016538619995117 0.03578038513660431
CurrentTrain: epoch  8, batch   101 | loss: 4.0523190Losses:  4.115630626678467 0.04508999362587929
CurrentTrain: epoch  8, batch   102 | loss: 4.1607208Losses:  3.9349937438964844 0.05734806880354881
CurrentTrain: epoch  8, batch   103 | loss: 3.9923418Losses:  3.948305368423462 0.040477581322193146
CurrentTrain: epoch  8, batch   104 | loss: 3.9887829Losses:  3.916236162185669 0.049257367849349976
CurrentTrain: epoch  8, batch   105 | loss: 3.9654934Losses:  3.9557437896728516 0.08154574781656265
CurrentTrain: epoch  8, batch   106 | loss: 4.0372896Losses:  3.9731099605560303 0.053834304213523865
CurrentTrain: epoch  8, batch   107 | loss: 4.0269442Losses:  3.971925735473633 0.04433894157409668
CurrentTrain: epoch  8, batch   108 | loss: 4.0162649Losses:  3.9473230838775635 0.07005494832992554
CurrentTrain: epoch  8, batch   109 | loss: 4.0173779Losses:  4.0112457275390625 0.07508720457553864
CurrentTrain: epoch  8, batch   110 | loss: 4.0863328Losses:  3.9621644020080566 0.04005227982997894
CurrentTrain: epoch  8, batch   111 | loss: 4.0022168Losses:  3.956925868988037 0.048783764243125916
CurrentTrain: epoch  8, batch   112 | loss: 4.0057096Losses:  3.959293842315674 0.045350514352321625
CurrentTrain: epoch  8, batch   113 | loss: 4.0046444Losses:  3.904784679412842 0.03941406309604645
CurrentTrain: epoch  8, batch   114 | loss: 3.9441988Losses:  3.9282712936401367 0.04180624708533287
CurrentTrain: epoch  8, batch   115 | loss: 3.9700775Losses:  3.8578147888183594 0.024391453713178635
CurrentTrain: epoch  8, batch   116 | loss: 3.8822062Losses:  4.05334997177124 0.046913571655750275
CurrentTrain: epoch  8, batch   117 | loss: 4.1002636Losses:  3.9737439155578613 0.04722714424133301
CurrentTrain: epoch  8, batch   118 | loss: 4.0209713Losses:  3.9783167839050293 0.051168374717235565
CurrentTrain: epoch  8, batch   119 | loss: 4.0294852Losses:  3.991840362548828 0.01882246509194374
CurrentTrain: epoch  8, batch   120 | loss: 4.0106630Losses:  4.023059368133545 0.055266208946704865
CurrentTrain: epoch  8, batch   121 | loss: 4.0783257Losses:  4.189582347869873 0.05791334807872772
CurrentTrain: epoch  8, batch   122 | loss: 4.2474957Losses:  3.935910701751709 0.06794508546590805
CurrentTrain: epoch  8, batch   123 | loss: 4.0038557Losses:  3.9782774448394775 0.05562760308384895
CurrentTrain: epoch  8, batch   124 | loss: 4.0339050Losses:  3.996678590774536 0.0582466796040535
CurrentTrain: epoch  9, batch     0 | loss: 4.0549254Losses:  3.954373836517334 0.03701378405094147
CurrentTrain: epoch  9, batch     1 | loss: 3.9913876Losses:  3.9890782833099365 0.05863576382398605
CurrentTrain: epoch  9, batch     2 | loss: 4.0477142Losses:  3.961970806121826 0.040709372609853745
CurrentTrain: epoch  9, batch     3 | loss: 4.0026803Losses:  3.9796760082244873 0.0701955258846283
CurrentTrain: epoch  9, batch     4 | loss: 4.0498714Losses:  4.004533767700195 0.05468478053808212
CurrentTrain: epoch  9, batch     5 | loss: 4.0592184Losses:  3.941600799560547 0.06704137474298477
CurrentTrain: epoch  9, batch     6 | loss: 4.0086422Losses:  3.9816784858703613 0.04640726000070572
CurrentTrain: epoch  9, batch     7 | loss: 4.0280857Losses:  3.9802863597869873 0.022160902619361877
CurrentTrain: epoch  9, batch     8 | loss: 4.0024471Losses:  4.008573532104492 0.05048586428165436
CurrentTrain: epoch  9, batch     9 | loss: 4.0590596Losses:  4.0745720863342285 0.06693273782730103
CurrentTrain: epoch  9, batch    10 | loss: 4.1415048Losses:  4.124066352844238 0.046252377331256866
CurrentTrain: epoch  9, batch    11 | loss: 4.1703186Losses:  3.9450321197509766 0.058674417436122894
CurrentTrain: epoch  9, batch    12 | loss: 4.0037065Losses:  3.99348783493042 0.044345758855342865
CurrentTrain: epoch  9, batch    13 | loss: 4.0378337Losses:  3.929068088531494 0.04936416819691658
CurrentTrain: epoch  9, batch    14 | loss: 3.9784322Losses:  3.9594550132751465 0.03636711463332176
CurrentTrain: epoch  9, batch    15 | loss: 3.9958222Losses:  4.094758987426758 0.0408037006855011
CurrentTrain: epoch  9, batch    16 | loss: 4.1355629Losses:  3.9672017097473145 0.04659397155046463
CurrentTrain: epoch  9, batch    17 | loss: 4.0137959Losses:  3.9213802814483643 0.0426633358001709
CurrentTrain: epoch  9, batch    18 | loss: 3.9640436Losses:  3.978982448577881 0.0775868147611618
CurrentTrain: epoch  9, batch    19 | loss: 4.0565691Losses:  3.961193561553955 0.04596405848860741
CurrentTrain: epoch  9, batch    20 | loss: 4.0071578Losses:  3.9588170051574707 0.04635760188102722
CurrentTrain: epoch  9, batch    21 | loss: 4.0051746Losses:  3.968156337738037 0.03846779838204384
CurrentTrain: epoch  9, batch    22 | loss: 4.0066242Losses:  3.999859571456909 0.05441119149327278
CurrentTrain: epoch  9, batch    23 | loss: 4.0542707Losses:  3.9527225494384766 0.044733937829732895
CurrentTrain: epoch  9, batch    24 | loss: 3.9974566Losses:  3.941279411315918 0.05348832160234451
CurrentTrain: epoch  9, batch    25 | loss: 3.9947677Losses:  3.911256790161133 0.028802968561649323
CurrentTrain: epoch  9, batch    26 | loss: 3.9400597Losses:  3.9722683429718018 0.0379461795091629
CurrentTrain: epoch  9, batch    27 | loss: 4.0102143Losses:  3.919616222381592 0.06531868875026703
CurrentTrain: epoch  9, batch    28 | loss: 3.9849348Losses:  3.976566791534424 0.06011255085468292
CurrentTrain: epoch  9, batch    29 | loss: 4.0366793Losses:  3.958298921585083 0.04669119790196419
CurrentTrain: epoch  9, batch    30 | loss: 4.0049901Losses:  4.034735202789307 0.058518897742033005
CurrentTrain: epoch  9, batch    31 | loss: 4.0932541Losses:  3.9395179748535156 0.08157382160425186
CurrentTrain: epoch  9, batch    32 | loss: 4.0210919Losses:  3.9829330444335938 0.08471095561981201
CurrentTrain: epoch  9, batch    33 | loss: 4.0676441Losses:  4.012007236480713 0.048419252038002014
CurrentTrain: epoch  9, batch    34 | loss: 4.0604267Losses:  3.918109893798828 0.028865203261375427
CurrentTrain: epoch  9, batch    35 | loss: 3.9469750Losses:  3.9415454864501953 0.0739910751581192
CurrentTrain: epoch  9, batch    36 | loss: 4.0155368Losses:  3.921908378601074 0.03029937855899334
CurrentTrain: epoch  9, batch    37 | loss: 3.9522078Losses:  4.018031120300293 0.07594287395477295
CurrentTrain: epoch  9, batch    38 | loss: 4.0939741Losses:  3.966834545135498 0.03828279674053192
CurrentTrain: epoch  9, batch    39 | loss: 4.0051174Losses:  3.9267749786376953 0.06410301476716995
CurrentTrain: epoch  9, batch    40 | loss: 3.9908781Losses:  4.0620527267456055 0.06695172190666199
CurrentTrain: epoch  9, batch    41 | loss: 4.1290045Losses:  3.9275565147399902 0.029308585450053215
CurrentTrain: epoch  9, batch    42 | loss: 3.9568651Losses:  3.969388723373413 0.03620363026857376
CurrentTrain: epoch  9, batch    43 | loss: 4.0055923Losses:  3.94449782371521 0.06504282355308533
CurrentTrain: epoch  9, batch    44 | loss: 4.0095406Losses:  3.9134559631347656 0.06139548122882843
CurrentTrain: epoch  9, batch    45 | loss: 3.9748514Losses:  4.01556396484375 0.05844435840845108
CurrentTrain: epoch  9, batch    46 | loss: 4.0740085Losses:  3.9832897186279297 0.059792451560497284
CurrentTrain: epoch  9, batch    47 | loss: 4.0430822Losses:  3.954834222793579 0.04575300216674805
CurrentTrain: epoch  9, batch    48 | loss: 4.0005875Losses:  3.9746336936950684 0.05009656399488449
CurrentTrain: epoch  9, batch    49 | loss: 4.0247302Losses:  3.9780473709106445 0.05077485367655754
CurrentTrain: epoch  9, batch    50 | loss: 4.0288224Losses:  3.949573516845703 0.0388113409280777
CurrentTrain: epoch  9, batch    51 | loss: 3.9883850Losses:  3.957857847213745 0.062212005257606506
CurrentTrain: epoch  9, batch    52 | loss: 4.0200701Losses:  3.922025203704834 0.04093450307846069
CurrentTrain: epoch  9, batch    53 | loss: 3.9629598Losses:  4.007377624511719 0.05462224781513214
CurrentTrain: epoch  9, batch    54 | loss: 4.0619998Losses:  3.900430202484131 0.04567669332027435
CurrentTrain: epoch  9, batch    55 | loss: 3.9461069Losses:  3.9291253089904785 0.0699666440486908
CurrentTrain: epoch  9, batch    56 | loss: 3.9990919Losses:  4.015913963317871 0.03218390420079231
CurrentTrain: epoch  9, batch    57 | loss: 4.0480981Losses:  3.9450244903564453 0.05206838995218277
CurrentTrain: epoch  9, batch    58 | loss: 3.9970930Losses:  3.9704701900482178 0.0372672975063324
CurrentTrain: epoch  9, batch    59 | loss: 4.0077376Losses:  3.928427219390869 0.03984014689922333
CurrentTrain: epoch  9, batch    60 | loss: 3.9682674Losses:  3.972233772277832 0.05767955631017685
CurrentTrain: epoch  9, batch    61 | loss: 4.0299134Losses:  3.9649901390075684 0.03964074328541756
CurrentTrain: epoch  9, batch    62 | loss: 4.0046310Losses:  3.9352285861968994 0.04336201399564743
CurrentTrain: epoch  9, batch    63 | loss: 3.9785905Losses:  3.9158623218536377 0.036616452038288116
CurrentTrain: epoch  9, batch    64 | loss: 3.9524789Losses:  3.9275808334350586 0.046430401504039764
CurrentTrain: epoch  9, batch    65 | loss: 3.9740112Losses:  3.914564609527588 0.03988923877477646
CurrentTrain: epoch  9, batch    66 | loss: 3.9544539Losses:  3.960573673248291 0.051884397864341736
CurrentTrain: epoch  9, batch    67 | loss: 4.0124578Losses:  3.94711971282959 0.040545932948589325
CurrentTrain: epoch  9, batch    68 | loss: 3.9876657Losses:  3.9140231609344482 0.04490381106734276
CurrentTrain: epoch  9, batch    69 | loss: 3.9589269Losses:  3.868511199951172 0.024108439683914185
CurrentTrain: epoch  9, batch    70 | loss: 3.8926196Losses:  3.9096767902374268 0.07358910143375397
CurrentTrain: epoch  9, batch    71 | loss: 3.9832659Losses:  3.977234363555908 0.05460551381111145
CurrentTrain: epoch  9, batch    72 | loss: 4.0318398Losses:  4.011532783508301 0.02138451114296913
CurrentTrain: epoch  9, batch    73 | loss: 4.0329175Losses:  3.9315991401672363 0.05009220540523529
CurrentTrain: epoch  9, batch    74 | loss: 3.9816914Losses:  3.9381163120269775 0.029959842562675476
CurrentTrain: epoch  9, batch    75 | loss: 3.9680762Losses:  3.917694091796875 0.05869310721755028
CurrentTrain: epoch  9, batch    76 | loss: 3.9763873Losses:  3.864748239517212 0.025419602170586586
CurrentTrain: epoch  9, batch    77 | loss: 3.8901680Losses:  3.9382362365722656 0.0617215596139431
CurrentTrain: epoch  9, batch    78 | loss: 3.9999578Losses:  3.9547300338745117 0.04798194766044617
CurrentTrain: epoch  9, batch    79 | loss: 4.0027118Losses:  3.9129600524902344 0.06787662953138351
CurrentTrain: epoch  9, batch    80 | loss: 3.9808366Losses:  3.968262195587158 0.04629473015666008
CurrentTrain: epoch  9, batch    81 | loss: 4.0145569Losses:  3.9453608989715576 0.036781273782253265
CurrentTrain: epoch  9, batch    82 | loss: 3.9821422Losses:  3.968764543533325 0.031124034896492958
CurrentTrain: epoch  9, batch    83 | loss: 3.9998887Losses:  3.8882651329040527 0.014815961942076683
CurrentTrain: epoch  9, batch    84 | loss: 3.9030812Losses:  3.9731802940368652 0.02016506902873516
CurrentTrain: epoch  9, batch    85 | loss: 3.9933453Losses:  3.9419116973876953 0.04953273385763168
CurrentTrain: epoch  9, batch    86 | loss: 3.9914443Losses:  3.9205212593078613 0.049194470047950745
CurrentTrain: epoch  9, batch    87 | loss: 3.9697158Losses:  3.959425449371338 0.018135078251361847
CurrentTrain: epoch  9, batch    88 | loss: 3.9775605Losses:  3.947551727294922 0.0655803233385086
CurrentTrain: epoch  9, batch    89 | loss: 4.0131321Losses:  3.965705633163452 0.043601520359516144
CurrentTrain: epoch  9, batch    90 | loss: 4.0093074Losses:  3.924494743347168 0.05336366221308708
CurrentTrain: epoch  9, batch    91 | loss: 3.9778583Losses:  3.9239885807037354 0.0568639412522316
CurrentTrain: epoch  9, batch    92 | loss: 3.9808526Losses:  3.939932107925415 0.044917717576026917
CurrentTrain: epoch  9, batch    93 | loss: 3.9848499Losses:  3.9251046180725098 0.03257429599761963
CurrentTrain: epoch  9, batch    94 | loss: 3.9576788Losses:  3.9855570793151855 0.02656533196568489
CurrentTrain: epoch  9, batch    95 | loss: 4.0121226Losses:  3.9087533950805664 0.027422118932008743
CurrentTrain: epoch  9, batch    96 | loss: 3.9361756Losses:  3.969386100769043 0.055851906538009644
CurrentTrain: epoch  9, batch    97 | loss: 4.0252380Losses:  3.969606876373291 0.03935547173023224
CurrentTrain: epoch  9, batch    98 | loss: 4.0089622Losses:  3.9446988105773926 0.03232885152101517
CurrentTrain: epoch  9, batch    99 | loss: 3.9770277Losses:  3.9213943481445312 0.033057041466236115
CurrentTrain: epoch  9, batch   100 | loss: 3.9544513Losses:  4.029024124145508 0.06640021502971649
CurrentTrain: epoch  9, batch   101 | loss: 4.0954242Losses:  4.018435478210449 0.022273000329732895
CurrentTrain: epoch  9, batch   102 | loss: 4.0407085Losses:  3.940735340118408 0.03557387739419937
CurrentTrain: epoch  9, batch   103 | loss: 3.9763093Losses:  3.940861701965332 0.02626468427479267
CurrentTrain: epoch  9, batch   104 | loss: 3.9671264Losses:  3.9781746864318848 0.06185701861977577
CurrentTrain: epoch  9, batch   105 | loss: 4.0400319Losses:  3.9001097679138184 0.03247739374637604
CurrentTrain: epoch  9, batch   106 | loss: 3.9325871Losses:  4.016249656677246 0.018674926832318306
CurrentTrain: epoch  9, batch   107 | loss: 4.0349245Losses:  3.9335851669311523 0.022130697965621948
CurrentTrain: epoch  9, batch   108 | loss: 3.9557159Losses:  3.9251389503479004 0.02874133177101612
CurrentTrain: epoch  9, batch   109 | loss: 3.9538803Losses:  3.8488759994506836 0.03547839820384979
CurrentTrain: epoch  9, batch   110 | loss: 3.8843544Losses:  3.9170749187469482 0.043196529150009155
CurrentTrain: epoch  9, batch   111 | loss: 3.9602714Losses:  3.9703102111816406 0.06262452900409698
CurrentTrain: epoch  9, batch   112 | loss: 4.0329347Losses:  4.732791900634766 0.09011629223823547
CurrentTrain: epoch  9, batch   113 | loss: 4.8229084Losses:  3.927216053009033 0.04256836324930191
CurrentTrain: epoch  9, batch   114 | loss: 3.9697845Losses:  3.9289872646331787 0.025687094777822495
CurrentTrain: epoch  9, batch   115 | loss: 3.9546742Losses:  3.9581165313720703 0.05451066046953201
CurrentTrain: epoch  9, batch   116 | loss: 4.0126271Losses:  3.9602932929992676 0.03353807330131531
CurrentTrain: epoch  9, batch   117 | loss: 3.9938314Losses:  3.991774320602417 0.05238419771194458
CurrentTrain: epoch  9, batch   118 | loss: 4.0441585Losses:  3.9138715267181396 0.03327113389968872
CurrentTrain: epoch  9, batch   119 | loss: 3.9471426Losses:  3.944732189178467 0.024193061515688896
CurrentTrain: epoch  9, batch   120 | loss: 3.9689252Losses:  3.959506034851074 0.053735241293907166
CurrentTrain: epoch  9, batch   121 | loss: 4.0132413Losses:  3.9766957759857178 0.03607913851737976
CurrentTrain: epoch  9, batch   122 | loss: 4.0127749Losses:  3.9638988971710205 0.06371541321277618
CurrentTrain: epoch  9, batch   123 | loss: 4.0276141Losses:  3.9803640842437744 0.037313926964998245
CurrentTrain: epoch  9, batch   124 | loss: 4.0176778
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.73%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.73%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
cur_acc:  ['0.9405']
his_acc:  ['0.9405']
Clustering into  9  clusters
Clusters:  [1 5 4 0 1 1 7 1 6 8 2 0 1 0 0 3 1 1 1 1]
Losses:  8.401674270629883 1.6273431777954102
CurrentTrain: epoch  0, batch     0 | loss: 10.0290174Losses:  9.757914543151855 1.4316151142120361
CurrentTrain: epoch  0, batch     1 | loss: 11.1895294Losses:  10.85698413848877 1.5774657726287842
CurrentTrain: epoch  0, batch     2 | loss: 12.4344501Losses:  9.769868850708008 1.610016107559204
CurrentTrain: epoch  0, batch     3 | loss: 11.3798847Losses:  8.571538925170898 1.8634105920791626
CurrentTrain: epoch  0, batch     4 | loss: 10.4349499Losses:  6.838538646697998 1.8548827171325684
CurrentTrain: epoch  0, batch     5 | loss: 8.6934214Losses:  6.208125114440918 0.3027881681919098
CurrentTrain: epoch  0, batch     6 | loss: 6.5109134Losses:  4.8239264488220215 1.5327410697937012
CurrentTrain: epoch  1, batch     0 | loss: 6.3566675Losses:  4.589417934417725 1.4990410804748535
CurrentTrain: epoch  1, batch     1 | loss: 6.0884590Losses:  4.375850677490234 1.2871612310409546
CurrentTrain: epoch  1, batch     2 | loss: 5.6630120Losses:  4.087405204772949 1.3058583736419678
CurrentTrain: epoch  1, batch     3 | loss: 5.3932638Losses:  3.9221317768096924 1.7967666387557983
CurrentTrain: epoch  1, batch     4 | loss: 5.7188983Losses:  4.696569442749023 1.7312651872634888
CurrentTrain: epoch  1, batch     5 | loss: 6.4278345Losses:  3.704225778579712 0.4542180895805359
CurrentTrain: epoch  1, batch     6 | loss: 4.1584439Losses:  4.221749782562256 0.9159069061279297
CurrentTrain: epoch  2, batch     0 | loss: 5.1376567Losses:  4.1156229972839355 1.2836439609527588
CurrentTrain: epoch  2, batch     1 | loss: 5.3992672Losses:  3.133983850479126 1.140872836112976
CurrentTrain: epoch  2, batch     2 | loss: 4.2748566Losses:  3.993779182434082 1.4629416465759277
CurrentTrain: epoch  2, batch     3 | loss: 5.4567208Losses:  3.5708203315734863 1.316232681274414
CurrentTrain: epoch  2, batch     4 | loss: 4.8870530Losses:  4.227391719818115 1.4336836338043213
CurrentTrain: epoch  2, batch     5 | loss: 5.6610756Losses:  4.917773246765137 0.9544905424118042
CurrentTrain: epoch  2, batch     6 | loss: 5.8722639Losses:  3.8999404907226562 1.464888572692871
CurrentTrain: epoch  3, batch     0 | loss: 5.3648291Losses:  3.0918684005737305 1.191092848777771
CurrentTrain: epoch  3, batch     1 | loss: 4.2829614Losses:  3.5194151401519775 1.4763288497924805
CurrentTrain: epoch  3, batch     2 | loss: 4.9957438Losses:  3.277549982070923 1.2441792488098145
CurrentTrain: epoch  3, batch     3 | loss: 4.5217295Losses:  3.489598274230957 1.3495981693267822
CurrentTrain: epoch  3, batch     4 | loss: 4.8391962Losses:  2.949873208999634 0.7573291063308716
CurrentTrain: epoch  3, batch     5 | loss: 3.7072024Losses:  3.455482006072998 0.5795638561248779
CurrentTrain: epoch  3, batch     6 | loss: 4.0350456Losses:  3.2336995601654053 0.9144248962402344
CurrentTrain: epoch  4, batch     0 | loss: 4.1481247Losses:  2.7009267807006836 1.1972576379776
CurrentTrain: epoch  4, batch     1 | loss: 3.8981843Losses:  2.9442877769470215 0.6146306395530701
CurrentTrain: epoch  4, batch     2 | loss: 3.5589185Losses:  3.5235939025878906 0.7139348983764648
CurrentTrain: epoch  4, batch     3 | loss: 4.2375288Losses:  3.1737446784973145 1.243868112564087
CurrentTrain: epoch  4, batch     4 | loss: 4.4176130Losses:  2.6340107917785645 1.1134536266326904
CurrentTrain: epoch  4, batch     5 | loss: 3.7474644Losses:  3.927043914794922 0.3972024619579315
CurrentTrain: epoch  4, batch     6 | loss: 4.3242464Losses:  2.9011549949645996 0.9169349670410156
CurrentTrain: epoch  5, batch     0 | loss: 3.8180900Losses:  3.509092092514038 0.9573661088943481
CurrentTrain: epoch  5, batch     1 | loss: 4.4664583Losses:  2.8348498344421387 1.1371283531188965
CurrentTrain: epoch  5, batch     2 | loss: 3.9719782Losses:  3.080599069595337 1.2854976654052734
CurrentTrain: epoch  5, batch     3 | loss: 4.3660965Losses:  2.7710256576538086 1.363993525505066
CurrentTrain: epoch  5, batch     4 | loss: 4.1350193Losses:  2.379960060119629 0.885798454284668
CurrentTrain: epoch  5, batch     5 | loss: 3.2657585Losses:  2.1813371181488037 0.34061360359191895
CurrentTrain: epoch  5, batch     6 | loss: 2.5219507Losses:  2.5610928535461426 1.0625088214874268
CurrentTrain: epoch  6, batch     0 | loss: 3.6236017Losses:  2.4265873432159424 0.9974468946456909
CurrentTrain: epoch  6, batch     1 | loss: 3.4240341Losses:  2.8855037689208984 0.8810819387435913
CurrentTrain: epoch  6, batch     2 | loss: 3.7665858Losses:  2.4043211936950684 0.9227823615074158
CurrentTrain: epoch  6, batch     3 | loss: 3.3271036Losses:  2.5974650382995605 0.9442790150642395
CurrentTrain: epoch  6, batch     4 | loss: 3.5417440Losses:  2.632488250732422 1.1676576137542725
CurrentTrain: epoch  6, batch     5 | loss: 3.8001459Losses:  2.567136287689209 0.4163266718387604
CurrentTrain: epoch  6, batch     6 | loss: 2.9834630Losses:  2.390880584716797 0.611678421497345
CurrentTrain: epoch  7, batch     0 | loss: 3.0025589Losses:  2.4080021381378174 0.9973117113113403
CurrentTrain: epoch  7, batch     1 | loss: 3.4053140Losses:  2.0714030265808105 0.9348258376121521
CurrentTrain: epoch  7, batch     2 | loss: 3.0062289Losses:  2.391537666320801 0.9324734807014465
CurrentTrain: epoch  7, batch     3 | loss: 3.3240111Losses:  2.259150266647339 1.048649787902832
CurrentTrain: epoch  7, batch     4 | loss: 3.3078001Losses:  2.5835235118865967 1.0494017601013184
CurrentTrain: epoch  7, batch     5 | loss: 3.6329253Losses:  4.787246227264404 1.4901162614933128e-07
CurrentTrain: epoch  7, batch     6 | loss: 4.7872462Losses:  2.6800460815429688 1.0427231788635254
CurrentTrain: epoch  8, batch     0 | loss: 3.7227693Losses:  2.234484910964966 0.7857171297073364
CurrentTrain: epoch  8, batch     1 | loss: 3.0202022Losses:  2.103132724761963 0.9888202548027039
CurrentTrain: epoch  8, batch     2 | loss: 3.0919530Losses:  2.580639600753784 0.8025511503219604
CurrentTrain: epoch  8, batch     3 | loss: 3.3831906Losses:  2.351114273071289 1.1357508897781372
CurrentTrain: epoch  8, batch     4 | loss: 3.4868650Losses:  1.9697299003601074 0.8246904015541077
CurrentTrain: epoch  8, batch     5 | loss: 2.7944202Losses:  2.1314332485198975 0.3852946162223816
CurrentTrain: epoch  8, batch     6 | loss: 2.5167279Losses:  2.11855149269104 0.7303208112716675
CurrentTrain: epoch  9, batch     0 | loss: 2.8488722Losses:  2.236079692840576 0.7270275354385376
CurrentTrain: epoch  9, batch     1 | loss: 2.9631071Losses:  2.968778133392334 0.722441554069519
CurrentTrain: epoch  9, batch     2 | loss: 3.6912198Losses:  2.0567216873168945 0.6459424495697021
CurrentTrain: epoch  9, batch     3 | loss: 2.7026641Losses:  1.9502202272415161 0.8240892291069031
CurrentTrain: epoch  9, batch     4 | loss: 2.7743094Losses:  2.256195068359375 0.6353778839111328
CurrentTrain: epoch  9, batch     5 | loss: 2.8915730Losses:  1.9741506576538086 0.5245793461799622
CurrentTrain: epoch  9, batch     6 | loss: 2.4987299
Losses:  5.088497638702393 0.49237972497940063
MemoryTrain:  epoch  0, batch     0 | loss: 5.5808773Losses:  8.585680961608887 0.4883219599723816
MemoryTrain:  epoch  0, batch     1 | loss: 9.0740032Losses:  10.841492652893066 0.47716015577316284
MemoryTrain:  epoch  0, batch     2 | loss: 11.3186531Losses:  0.8933609127998352 0.8816658854484558
MemoryTrain:  epoch  1, batch     0 | loss: 1.7750268Losses:  0.48006895184516907 0.6084160804748535
MemoryTrain:  epoch  1, batch     1 | loss: 1.0884850Losses:  0.471574604511261 0.11236775666475296
MemoryTrain:  epoch  1, batch     2 | loss: 0.5839424Losses:  0.3628941774368286 0.4515286684036255
MemoryTrain:  epoch  2, batch     0 | loss: 0.8144228Losses:  0.44450995326042175 0.37469255924224854
MemoryTrain:  epoch  2, batch     1 | loss: 0.8192025Losses:  0.3848479688167572 0.5238599181175232
MemoryTrain:  epoch  2, batch     2 | loss: 0.9087079Losses:  0.2822566032409668 0.4788062870502472
MemoryTrain:  epoch  3, batch     0 | loss: 0.7610629Losses:  0.2961288094520569 0.6281638741493225
MemoryTrain:  epoch  3, batch     1 | loss: 0.9242927Losses:  0.15310120582580566 0.16967321932315826
MemoryTrain:  epoch  3, batch     2 | loss: 0.3227744Losses:  0.282516747713089 0.6207132339477539
MemoryTrain:  epoch  4, batch     0 | loss: 0.9032300Losses:  0.21594542264938354 0.5793094038963318
MemoryTrain:  epoch  4, batch     1 | loss: 0.7952548Losses:  0.10509464144706726 0.15151716768741608
MemoryTrain:  epoch  4, batch     2 | loss: 0.2566118Losses:  0.5885401368141174 0.6580042839050293
MemoryTrain:  epoch  5, batch     0 | loss: 1.2465444Losses:  0.1596866250038147 0.381899893283844
MemoryTrain:  epoch  5, batch     1 | loss: 0.5415865Losses:  0.12949438393115997 0.3030620813369751
MemoryTrain:  epoch  5, batch     2 | loss: 0.4325565Losses:  0.25651291012763977 0.5071291923522949
MemoryTrain:  epoch  6, batch     0 | loss: 0.7636421Losses:  0.25042176246643066 0.4851703643798828
MemoryTrain:  epoch  6, batch     1 | loss: 0.7355921Losses:  0.17619720101356506 0.25117847323417664
MemoryTrain:  epoch  6, batch     2 | loss: 0.4273757Losses:  0.1674560308456421 0.5333046913146973
MemoryTrain:  epoch  7, batch     0 | loss: 0.7007607Losses:  0.1367465853691101 0.38997265696525574
MemoryTrain:  epoch  7, batch     1 | loss: 0.5267192Losses:  0.46885979175567627 0.4311792552471161
MemoryTrain:  epoch  7, batch     2 | loss: 0.9000391Losses:  0.12589243054389954 0.34388846158981323
MemoryTrain:  epoch  8, batch     0 | loss: 0.4697809Losses:  0.19170716404914856 0.4947257936000824
MemoryTrain:  epoch  8, batch     1 | loss: 0.6864330Losses:  0.219569131731987 0.20844314992427826
MemoryTrain:  epoch  8, batch     2 | loss: 0.4280123Losses:  0.17785273492336273 0.46546852588653564
MemoryTrain:  epoch  9, batch     0 | loss: 0.6433213Losses:  0.10921956598758698 0.21985042095184326
MemoryTrain:  epoch  9, batch     1 | loss: 0.3290700Losses:  0.26640909910202026 0.4512179493904114
MemoryTrain:  epoch  9, batch     2 | loss: 0.7176270
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 58.41%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 57.08%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 55.44%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 55.27%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 56.80%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 57.86%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 58.51%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 58.95%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 61.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 62.37%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 61.85%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 62.71%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.00%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.72%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.04%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.44%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.22%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.34%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.35%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 91.60%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 90.38%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 89.30%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 88.90%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 88.14%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.59%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 88.14%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 87.88%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 87.21%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 86.70%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 86.14%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 85.58%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.62%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.68%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 82.76%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 82.00%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 81.25%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 80.52%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 80.46%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 80.63%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 80.61%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 80.28%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 79.89%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.56%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.00%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.18%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.15%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 78.48%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 78.25%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 77.95%   
cur_acc:  ['0.9405', '0.6200']
his_acc:  ['0.9405', '0.7795']
Clustering into  14  clusters
Clusters:  [ 2 11  7  0  2  2 12  2 10  5  9  0  2  0  0  4  2  2  2  2  2  2  2  8
  2  3  2  1 13  6]
Losses:  7.09017276763916 1.0254050493240356
CurrentTrain: epoch  0, batch     0 | loss: 8.1155777Losses:  9.516921997070312 1.168605089187622
CurrentTrain: epoch  0, batch     1 | loss: 10.6855268Losses:  9.912336349487305 0.6807732582092285
CurrentTrain: epoch  0, batch     2 | loss: 10.5931091Losses:  9.575506210327148 0.8410321474075317
CurrentTrain: epoch  0, batch     3 | loss: 10.4165382Losses:  6.483615398406982 1.168936014175415
CurrentTrain: epoch  0, batch     4 | loss: 7.6525517Losses:  5.643580436706543 1.115232229232788
CurrentTrain: epoch  0, batch     5 | loss: 6.7588129Losses:  4.957529544830322 0.2483690083026886
CurrentTrain: epoch  0, batch     6 | loss: 5.2058988Losses:  3.8085315227508545 1.0800986289978027
CurrentTrain: epoch  1, batch     0 | loss: 4.8886299Losses:  3.6731414794921875 1.0030105113983154
CurrentTrain: epoch  1, batch     1 | loss: 4.6761522Losses:  3.466735363006592 0.5953606367111206
CurrentTrain: epoch  1, batch     2 | loss: 4.0620961Losses:  3.748228073120117 0.7531481981277466
CurrentTrain: epoch  1, batch     3 | loss: 4.5013762Losses:  4.093138217926025 0.9892755746841431
CurrentTrain: epoch  1, batch     4 | loss: 5.0824137Losses:  2.4946696758270264 0.6090341210365295
CurrentTrain: epoch  1, batch     5 | loss: 3.1037037Losses:  3.7532973289489746 0.14601697027683258
CurrentTrain: epoch  1, batch     6 | loss: 3.8993144Losses:  3.1944730281829834 0.5985989570617676
CurrentTrain: epoch  2, batch     0 | loss: 3.7930720Losses:  3.306488037109375 0.7912255525588989
CurrentTrain: epoch  2, batch     1 | loss: 4.0977135Losses:  2.6845593452453613 0.7860428094863892
CurrentTrain: epoch  2, batch     2 | loss: 3.4706020Losses:  2.802293539047241 0.4734245836734772
CurrentTrain: epoch  2, batch     3 | loss: 3.2757182Losses:  3.3115930557250977 0.7955034971237183
CurrentTrain: epoch  2, batch     4 | loss: 4.1070967Losses:  2.978692054748535 0.7833475470542908
CurrentTrain: epoch  2, batch     5 | loss: 3.7620397Losses:  3.340488910675049 0.23873069882392883
CurrentTrain: epoch  2, batch     6 | loss: 3.5792196Losses:  3.5120058059692383 0.5769339799880981
CurrentTrain: epoch  3, batch     0 | loss: 4.0889397Losses:  2.640214443206787 0.38677236437797546
CurrentTrain: epoch  3, batch     1 | loss: 3.0269868Losses:  2.312486171722412 0.6648451089859009
CurrentTrain: epoch  3, batch     2 | loss: 2.9773312Losses:  2.1972618103027344 0.3709365427494049
CurrentTrain: epoch  3, batch     3 | loss: 2.5681984Losses:  2.500361680984497 0.4245074391365051
CurrentTrain: epoch  3, batch     4 | loss: 2.9248691Losses:  2.3437695503234863 0.5215237140655518
CurrentTrain: epoch  3, batch     5 | loss: 2.8652933Losses:  3.0613458156585693 0.03654444217681885
CurrentTrain: epoch  3, batch     6 | loss: 3.0978904Losses:  2.1862034797668457 0.48579874634742737
CurrentTrain: epoch  4, batch     0 | loss: 2.6720023Losses:  1.9402170181274414 0.4404466450214386
CurrentTrain: epoch  4, batch     1 | loss: 2.3806636Losses:  2.9744110107421875 0.46971648931503296
CurrentTrain: epoch  4, batch     2 | loss: 3.4441276Losses:  2.093212127685547 0.34007593989372253
CurrentTrain: epoch  4, batch     3 | loss: 2.4332881Losses:  2.488751173019409 0.6884856820106506
CurrentTrain: epoch  4, batch     4 | loss: 3.1772368Losses:  2.3467023372650146 0.49963557720184326
CurrentTrain: epoch  4, batch     5 | loss: 2.8463378Losses:  2.3724145889282227 0.3025650978088379
CurrentTrain: epoch  4, batch     6 | loss: 2.6749797Losses:  2.282297134399414 0.5684559345245361
CurrentTrain: epoch  5, batch     0 | loss: 2.8507531Losses:  2.250371217727661 0.3517886698246002
CurrentTrain: epoch  5, batch     1 | loss: 2.6021600Losses:  2.2927448749542236 0.3958820700645447
CurrentTrain: epoch  5, batch     2 | loss: 2.6886270Losses:  1.857085943222046 0.38645583391189575
CurrentTrain: epoch  5, batch     3 | loss: 2.2435417Losses:  2.0625815391540527 0.39018887281417847
CurrentTrain: epoch  5, batch     4 | loss: 2.4527705Losses:  2.175102710723877 0.4042738080024719
CurrentTrain: epoch  5, batch     5 | loss: 2.5793765Losses:  1.8583898544311523 0.02819793112576008
CurrentTrain: epoch  5, batch     6 | loss: 1.8865877Losses:  1.9420479536056519 0.3978387117385864
CurrentTrain: epoch  6, batch     0 | loss: 2.3398867Losses:  2.2774581909179688 0.5276144742965698
CurrentTrain: epoch  6, batch     1 | loss: 2.8050728Losses:  2.1512792110443115 0.3221963047981262
CurrentTrain: epoch  6, batch     2 | loss: 2.4734755Losses:  1.9084755182266235 0.30832287669181824
CurrentTrain: epoch  6, batch     3 | loss: 2.2167983Losses:  1.7771049737930298 0.21192575991153717
CurrentTrain: epoch  6, batch     4 | loss: 1.9890307Losses:  1.8494772911071777 0.42628079652786255
CurrentTrain: epoch  6, batch     5 | loss: 2.2757580Losses:  1.9662597179412842 0.022853277623653412
CurrentTrain: epoch  6, batch     6 | loss: 1.9891130Losses:  1.8810667991638184 0.3201775848865509
CurrentTrain: epoch  7, batch     0 | loss: 2.2012444Losses:  1.9508411884307861 0.41668128967285156
CurrentTrain: epoch  7, batch     1 | loss: 2.3675225Losses:  1.8644795417785645 0.42450612783432007
CurrentTrain: epoch  7, batch     2 | loss: 2.2889857Losses:  1.7574124336242676 0.30885136127471924
CurrentTrain: epoch  7, batch     3 | loss: 2.0662637Losses:  1.8917899131774902 0.4438427984714508
CurrentTrain: epoch  7, batch     4 | loss: 2.3356328Losses:  2.123812198638916 0.30096548795700073
CurrentTrain: epoch  7, batch     5 | loss: 2.4247777Losses:  1.9068347215652466 0.043634794652462006
CurrentTrain: epoch  7, batch     6 | loss: 1.9504695Losses:  1.8142600059509277 0.3317418098449707
CurrentTrain: epoch  8, batch     0 | loss: 2.1460018Losses:  1.708627700805664 0.29737040400505066
CurrentTrain: epoch  8, batch     1 | loss: 2.0059981Losses:  1.8512024879455566 0.46336567401885986
CurrentTrain: epoch  8, batch     2 | loss: 2.3145680Losses:  1.8456382751464844 0.4459373354911804
CurrentTrain: epoch  8, batch     3 | loss: 2.2915757Losses:  1.8487752676010132 0.3544797897338867
CurrentTrain: epoch  8, batch     4 | loss: 2.2032552Losses:  1.7635881900787354 0.4122806191444397
CurrentTrain: epoch  8, batch     5 | loss: 2.1758687Losses:  2.2457845211029053 2.9802322387695312e-08
CurrentTrain: epoch  8, batch     6 | loss: 2.2457845Losses:  1.758127212524414 0.38451117277145386
CurrentTrain: epoch  9, batch     0 | loss: 2.1426384Losses:  1.8848896026611328 0.26697278022766113
CurrentTrain: epoch  9, batch     1 | loss: 2.1518624Losses:  1.8265360593795776 0.2899724245071411
CurrentTrain: epoch  9, batch     2 | loss: 2.1165085Losses:  1.7477229833602905 0.3217587172985077
CurrentTrain: epoch  9, batch     3 | loss: 2.0694816Losses:  1.7663142681121826 0.3467598557472229
CurrentTrain: epoch  9, batch     4 | loss: 2.1130741Losses:  1.8173975944519043 0.24881169199943542
CurrentTrain: epoch  9, batch     5 | loss: 2.0662093Losses:  1.7124273777008057 0.03655174747109413
CurrentTrain: epoch  9, batch     6 | loss: 1.7489791
Losses:  6.48529052734375 0.5418984293937683
MemoryTrain:  epoch  0, batch     0 | loss: 7.0271888Losses:  8.31916618347168 0.7074416875839233
MemoryTrain:  epoch  0, batch     1 | loss: 9.0266075Losses:  10.523833274841309 0.40582671761512756
MemoryTrain:  epoch  0, batch     2 | loss: 10.9296598Losses:  11.301248550415039 0.24312567710876465
MemoryTrain:  epoch  0, batch     3 | loss: 11.5443745Losses:  1.2264338731765747 0.5214070677757263
MemoryTrain:  epoch  1, batch     0 | loss: 1.7478409Losses:  2.6960277557373047 0.4216510057449341
MemoryTrain:  epoch  1, batch     1 | loss: 3.1176786Losses:  0.9456887245178223 0.4475153088569641
MemoryTrain:  epoch  1, batch     2 | loss: 1.3932040Losses:  1.6373069286346436 0.3756672739982605
MemoryTrain:  epoch  1, batch     3 | loss: 2.0129743Losses:  0.7286340594291687 0.619109570980072
MemoryTrain:  epoch  2, batch     0 | loss: 1.3477436Losses:  1.0250598192214966 0.4488793909549713
MemoryTrain:  epoch  2, batch     1 | loss: 1.4739392Losses:  1.993668794631958 0.5873356461524963
MemoryTrain:  epoch  2, batch     2 | loss: 2.5810044Losses:  1.5983389616012573 0.15071123838424683
MemoryTrain:  epoch  2, batch     3 | loss: 1.7490501Losses:  0.778844952583313 0.43707937002182007
MemoryTrain:  epoch  3, batch     0 | loss: 1.2159243Losses:  1.4601842164993286 0.5218133926391602
MemoryTrain:  epoch  3, batch     1 | loss: 1.9819976Losses:  0.9223439693450928 0.6191084384918213
MemoryTrain:  epoch  3, batch     2 | loss: 1.5414524Losses:  0.7827441692352295 0.14170068502426147
MemoryTrain:  epoch  3, batch     3 | loss: 0.9244449Losses:  1.1272550821304321 0.2811076045036316
MemoryTrain:  epoch  4, batch     0 | loss: 1.4083626Losses:  0.7565585374832153 0.5285300016403198
MemoryTrain:  epoch  4, batch     1 | loss: 1.2850885Losses:  0.8854702711105347 0.7019845247268677
MemoryTrain:  epoch  4, batch     2 | loss: 1.5874548Losses:  0.29346388578414917 0.15244144201278687
MemoryTrain:  epoch  4, batch     3 | loss: 0.4459053Losses:  0.28528255224227905 0.514043927192688
MemoryTrain:  epoch  5, batch     0 | loss: 0.7993265Losses:  0.41971254348754883 0.5782365798950195
MemoryTrain:  epoch  5, batch     1 | loss: 0.9979491Losses:  0.7431373000144958 0.394204318523407
MemoryTrain:  epoch  5, batch     2 | loss: 1.1373416Losses:  0.673759937286377 0.22306528687477112
MemoryTrain:  epoch  5, batch     3 | loss: 0.8968252Losses:  0.3750627040863037 0.45433562994003296
MemoryTrain:  epoch  6, batch     0 | loss: 0.8293983Losses:  0.39427319169044495 0.3955931067466736
MemoryTrain:  epoch  6, batch     1 | loss: 0.7898663Losses:  0.40304577350616455 0.4208550453186035
MemoryTrain:  epoch  6, batch     2 | loss: 0.8239008Losses:  0.508486807346344 0.3440316617488861
MemoryTrain:  epoch  6, batch     3 | loss: 0.8525184Losses:  0.35529714822769165 0.47932201623916626
MemoryTrain:  epoch  7, batch     0 | loss: 0.8346192Losses:  0.49367785453796387 0.666702389717102
MemoryTrain:  epoch  7, batch     1 | loss: 1.1603802Losses:  0.248058021068573 0.3402484655380249
MemoryTrain:  epoch  7, batch     2 | loss: 0.5883065Losses:  0.42789602279663086 0.2868870794773102
MemoryTrain:  epoch  7, batch     3 | loss: 0.7147831Losses:  0.3174688220024109 0.6430968046188354
MemoryTrain:  epoch  8, batch     0 | loss: 0.9605656Losses:  0.46049964427948 0.44637322425842285
MemoryTrain:  epoch  8, batch     1 | loss: 0.9068729Losses:  0.5012416839599609 0.2829974293708801
MemoryTrain:  epoch  8, batch     2 | loss: 0.7842391Losses:  0.2845257520675659 0.2555958032608032
MemoryTrain:  epoch  8, batch     3 | loss: 0.5401216Losses:  0.5807538032531738 0.5846328139305115
MemoryTrain:  epoch  9, batch     0 | loss: 1.1653867Losses:  0.2444816529750824 0.47443878650665283
MemoryTrain:  epoch  9, batch     1 | loss: 0.7189205Losses:  0.2878779172897339 0.40479737520217896
MemoryTrain:  epoch  9, batch     2 | loss: 0.6926753Losses:  0.45099925994873047 0.2774958610534668
MemoryTrain:  epoch  9, batch     3 | loss: 0.7284951
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 62.77%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 61.07%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 60.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.96%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.13%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 91.80%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.73%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 90.97%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 88.27%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 87.03%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 85.73%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 84.47%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 83.70%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.12%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 84.54%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 84.38%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 83.11%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 82.28%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 81.54%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 80.82%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 80.11%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 79.21%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 78.33%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.47%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 76.63%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 75.81%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 75.13%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 75.56%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 76.40%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 75.86%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 75.51%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 75.39%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 75.06%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 74.95%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 74.84%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 74.69%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 74.54%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 74.19%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 74.10%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 75.65%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 75.52%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 75.59%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 75.63%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 75.21%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 74.88%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 74.51%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 74.03%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 73.68%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 73.45%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 73.07%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 72.85%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 72.83%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 72.65%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 72.43%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.37%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 72.23%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 72.10%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 71.93%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 71.80%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 71.67%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 71.32%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 71.05%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 70.82%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 70.48%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 70.26%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.28%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 71.71%   
cur_acc:  ['0.9405', '0.6200', '0.6696']
his_acc:  ['0.9405', '0.7795', '0.7171']
Clustering into  19  clusters
Clusters:  [ 1 11 15  0  1  1 16  1 12 17 10  0  1  0  0  9  1  1  1  1  1  1  1  4
  1  7  1 18 13  5  8  1 14  1  6  2  3  1  1  1]
Losses:  7.342919826507568 1.2293310165405273
CurrentTrain: epoch  0, batch     0 | loss: 8.5722504Losses:  9.1507568359375 1.409621238708496
CurrentTrain: epoch  0, batch     1 | loss: 10.5603781Losses:  8.644904136657715 1.217736005783081
CurrentTrain: epoch  0, batch     2 | loss: 9.8626404Losses:  11.156879425048828 0.9463456273078918
CurrentTrain: epoch  0, batch     3 | loss: 12.1032248Losses:  7.5145039558410645 0.9899569749832153
CurrentTrain: epoch  0, batch     4 | loss: 8.5044613Losses:  8.745145797729492 0.903907299041748
CurrentTrain: epoch  0, batch     5 | loss: 9.6490536Losses:  6.565157890319824 0.2940332293510437
CurrentTrain: epoch  0, batch     6 | loss: 6.8591909Losses:  4.506486892700195 1.1166679859161377
CurrentTrain: epoch  1, batch     0 | loss: 5.6231546Losses:  4.930172443389893 1.309164047241211
CurrentTrain: epoch  1, batch     1 | loss: 6.2393365Losses:  3.5880160331726074 1.0657835006713867
CurrentTrain: epoch  1, batch     2 | loss: 4.6537995Losses:  4.291642189025879 1.002976417541504
CurrentTrain: epoch  1, batch     3 | loss: 5.2946186Losses:  4.074954032897949 0.8614671230316162
CurrentTrain: epoch  1, batch     4 | loss: 4.9364214Losses:  3.7217323780059814 1.1770226955413818
CurrentTrain: epoch  1, batch     5 | loss: 4.8987551Losses:  5.710431098937988 0.4080197215080261
CurrentTrain: epoch  1, batch     6 | loss: 6.1184506Losses:  3.533088207244873 0.9530211091041565
CurrentTrain: epoch  2, batch     0 | loss: 4.4861093Losses:  3.1461129188537598 0.9422664046287537
CurrentTrain: epoch  2, batch     1 | loss: 4.0883794Losses:  3.680751323699951 1.002664566040039
CurrentTrain: epoch  2, batch     2 | loss: 4.6834159Losses:  4.417138576507568 1.0937937498092651
CurrentTrain: epoch  2, batch     3 | loss: 5.5109324Losses:  3.342935562133789 1.0287714004516602
CurrentTrain: epoch  2, batch     4 | loss: 4.3717070Losses:  4.76801872253418 0.6951585412025452
CurrentTrain: epoch  2, batch     5 | loss: 5.4631772Losses:  2.342682123184204 8.94069742685133e-08
CurrentTrain: epoch  2, batch     6 | loss: 2.3426821Losses:  2.6442458629608154 0.6229174137115479
CurrentTrain: epoch  3, batch     0 | loss: 3.2671633Losses:  2.4348809719085693 0.5452226996421814
CurrentTrain: epoch  3, batch     1 | loss: 2.9801037Losses:  4.897512435913086 0.7939940690994263
CurrentTrain: epoch  3, batch     2 | loss: 5.6915064Losses:  3.225834369659424 1.0353624820709229
CurrentTrain: epoch  3, batch     3 | loss: 4.2611971Losses:  3.7789113521575928 0.8553911447525024
CurrentTrain: epoch  3, batch     4 | loss: 4.6343026Losses:  3.8385210037231445 1.0575965642929077
CurrentTrain: epoch  3, batch     5 | loss: 4.8961177Losses:  2.2497169971466064 0.2193686068058014
CurrentTrain: epoch  3, batch     6 | loss: 2.4690857Losses:  3.370035171508789 0.9096537828445435
CurrentTrain: epoch  4, batch     0 | loss: 4.2796888Losses:  3.21212100982666 0.8620380759239197
CurrentTrain: epoch  4, batch     1 | loss: 4.0741591Losses:  3.9486441612243652 0.7706475257873535
CurrentTrain: epoch  4, batch     2 | loss: 4.7192917Losses:  2.29378604888916 0.4617201089859009
CurrentTrain: epoch  4, batch     3 | loss: 2.7555060Losses:  3.6853225231170654 0.8718891143798828
CurrentTrain: epoch  4, batch     4 | loss: 4.5572119Losses:  2.8182342052459717 0.7675510048866272
CurrentTrain: epoch  4, batch     5 | loss: 3.5857852Losses:  2.7576279640197754 0.1297246366739273
CurrentTrain: epoch  4, batch     6 | loss: 2.8873527Losses:  3.6842269897460938 0.9924622774124146
CurrentTrain: epoch  5, batch     0 | loss: 4.6766891Losses:  2.6228480339050293 0.8619424104690552
CurrentTrain: epoch  5, batch     1 | loss: 3.4847903Losses:  2.7997565269470215 0.6826105117797852
CurrentTrain: epoch  5, batch     2 | loss: 3.4823670Losses:  2.7068467140197754 0.7880113124847412
CurrentTrain: epoch  5, batch     3 | loss: 3.4948580Losses:  3.300262212753296 0.6397417783737183
CurrentTrain: epoch  5, batch     4 | loss: 3.9400039Losses:  3.3631131649017334 0.7932476997375488
CurrentTrain: epoch  5, batch     5 | loss: 4.1563606Losses:  1.71896231174469 2.9802322387695312e-08
CurrentTrain: epoch  5, batch     6 | loss: 1.7189623Losses:  3.6643924713134766 0.9340924024581909
CurrentTrain: epoch  6, batch     0 | loss: 4.5984850Losses:  2.5522499084472656 0.7502614259719849
CurrentTrain: epoch  6, batch     1 | loss: 3.3025112Losses:  2.8969831466674805 0.7078129053115845
CurrentTrain: epoch  6, batch     2 | loss: 3.6047959Losses:  2.61645770072937 0.5093986988067627
CurrentTrain: epoch  6, batch     3 | loss: 3.1258564Losses:  2.7062828540802 0.8124651312828064
CurrentTrain: epoch  6, batch     4 | loss: 3.5187480Losses:  2.2587108612060547 0.44416335225105286
CurrentTrain: epoch  6, batch     5 | loss: 2.7028742Losses:  1.983127236366272 0.11189402639865875
CurrentTrain: epoch  6, batch     6 | loss: 2.0950212Losses:  2.6849372386932373 0.5301835536956787
CurrentTrain: epoch  7, batch     0 | loss: 3.2151208Losses:  3.0991389751434326 0.7150684595108032
CurrentTrain: epoch  7, batch     1 | loss: 3.8142076Losses:  2.260748863220215 0.5557763576507568
CurrentTrain: epoch  7, batch     2 | loss: 2.8165252Losses:  2.736534595489502 0.7625827193260193
CurrentTrain: epoch  7, batch     3 | loss: 3.4991174Losses:  2.5299222469329834 0.6969672441482544
CurrentTrain: epoch  7, batch     4 | loss: 3.2268896Losses:  2.292109966278076 0.5801523327827454
CurrentTrain: epoch  7, batch     5 | loss: 2.8722622Losses:  1.7293760776519775 0.10890637338161469
CurrentTrain: epoch  7, batch     6 | loss: 1.8382825Losses:  2.3136916160583496 0.6632909774780273
CurrentTrain: epoch  8, batch     0 | loss: 2.9769826Losses:  2.1865692138671875 0.5647975206375122
CurrentTrain: epoch  8, batch     1 | loss: 2.7513666Losses:  2.172482490539551 0.6574149131774902
CurrentTrain: epoch  8, batch     2 | loss: 2.8298974Losses:  2.290018081665039 0.573594868183136
CurrentTrain: epoch  8, batch     3 | loss: 2.8636129Losses:  2.242375612258911 0.4113752245903015
CurrentTrain: epoch  8, batch     4 | loss: 2.6537509Losses:  2.7450075149536133 0.45758554339408875
CurrentTrain: epoch  8, batch     5 | loss: 3.2025931Losses:  1.764810562133789 0.09012250602245331
CurrentTrain: epoch  8, batch     6 | loss: 1.8549330Losses:  2.0373735427856445 0.5052385926246643
CurrentTrain: epoch  9, batch     0 | loss: 2.5426121Losses:  2.382502317428589 0.42296338081359863
CurrentTrain: epoch  9, batch     1 | loss: 2.8054657Losses:  2.0250015258789062 0.5462052822113037
CurrentTrain: epoch  9, batch     2 | loss: 2.5712068Losses:  2.2406506538391113 0.6052565574645996
CurrentTrain: epoch  9, batch     3 | loss: 2.8459072Losses:  2.0215377807617188 0.5799560546875
CurrentTrain: epoch  9, batch     4 | loss: 2.6014938Losses:  1.938254475593567 0.5721724033355713
CurrentTrain: epoch  9, batch     5 | loss: 2.5104270Losses:  2.3353614807128906 0.1947612464427948
CurrentTrain: epoch  9, batch     6 | loss: 2.5301228
Losses:  5.79493522644043 0.49049508571624756
MemoryTrain:  epoch  0, batch     0 | loss: 6.2854304Losses:  8.037580490112305 0.5753328800201416
MemoryTrain:  epoch  0, batch     1 | loss: 8.6129131Losses:  10.472280502319336 0.3722655773162842
MemoryTrain:  epoch  0, batch     2 | loss: 10.8445463Losses:  10.830597877502441 0.7254318594932556
MemoryTrain:  epoch  0, batch     3 | loss: 11.5560293Losses:  10.956928253173828 0.3921237289905548
MemoryTrain:  epoch  0, batch     4 | loss: 11.3490524Losses:  0.8013055324554443 0.595344603061676
MemoryTrain:  epoch  1, batch     0 | loss: 1.3966501Losses:  1.2377569675445557 0.35478559136390686
MemoryTrain:  epoch  1, batch     1 | loss: 1.5925425Losses:  0.8196893334388733 0.34112095832824707
MemoryTrain:  epoch  1, batch     2 | loss: 1.1608102Losses:  1.2540597915649414 0.5274787545204163
MemoryTrain:  epoch  1, batch     3 | loss: 1.7815385Losses:  0.8348271250724792 0.5126917958259583
MemoryTrain:  epoch  1, batch     4 | loss: 1.3475189Losses:  0.7760055661201477 0.5369442105293274
MemoryTrain:  epoch  2, batch     0 | loss: 1.3129498Losses:  0.79024338722229 0.5441789031028748
MemoryTrain:  epoch  2, batch     1 | loss: 1.3344223Losses:  0.6921095848083496 0.6162725687026978
MemoryTrain:  epoch  2, batch     2 | loss: 1.3083822Losses:  0.39343369007110596 0.2937176823616028
MemoryTrain:  epoch  2, batch     3 | loss: 0.6871514Losses:  0.8769374489784241 0.33643490076065063
MemoryTrain:  epoch  2, batch     4 | loss: 1.2133723Losses:  0.5931098461151123 0.5560574531555176
MemoryTrain:  epoch  3, batch     0 | loss: 1.1491673Losses:  0.5093644857406616 0.5539901256561279
MemoryTrain:  epoch  3, batch     1 | loss: 1.0633546Losses:  0.4706037640571594 0.31924980878829956
MemoryTrain:  epoch  3, batch     2 | loss: 0.7898536Losses:  0.6344234943389893 0.4562937021255493
MemoryTrain:  epoch  3, batch     3 | loss: 1.0907172Losses:  0.6618691682815552 0.5735794305801392
MemoryTrain:  epoch  3, batch     4 | loss: 1.2354486Losses:  0.6491096615791321 0.7331572771072388
MemoryTrain:  epoch  4, batch     0 | loss: 1.3822670Losses:  0.38099342584609985 0.3340010643005371
MemoryTrain:  epoch  4, batch     1 | loss: 0.7149945Losses:  0.44819822907447815 0.596435546875
MemoryTrain:  epoch  4, batch     2 | loss: 1.0446337Losses:  0.3287302255630493 0.36732035875320435
MemoryTrain:  epoch  4, batch     3 | loss: 0.6960506Losses:  0.6043039560317993 0.5395230054855347
MemoryTrain:  epoch  4, batch     4 | loss: 1.1438270Losses:  0.2889024615287781 0.42138832807540894
MemoryTrain:  epoch  5, batch     0 | loss: 0.7102908Losses:  0.2767142951488495 0.31799864768981934
MemoryTrain:  epoch  5, batch     1 | loss: 0.5947130Losses:  0.3722865879535675 0.5851026773452759
MemoryTrain:  epoch  5, batch     2 | loss: 0.9573892Losses:  0.4020899534225464 0.6000407934188843
MemoryTrain:  epoch  5, batch     3 | loss: 1.0021307Losses:  0.7387288808822632 0.5954282283782959
MemoryTrain:  epoch  5, batch     4 | loss: 1.3341571Losses:  0.4012771546840668 0.47943809628486633
MemoryTrain:  epoch  6, batch     0 | loss: 0.8807153Losses:  0.46706557273864746 0.4262695908546448
MemoryTrain:  epoch  6, batch     1 | loss: 0.8933352Losses:  0.3225271701812744 0.38455939292907715
MemoryTrain:  epoch  6, batch     2 | loss: 0.7070866Losses:  0.2870813012123108 0.38840851187705994
MemoryTrain:  epoch  6, batch     3 | loss: 0.6754898Losses:  0.43120020627975464 0.6101599931716919
MemoryTrain:  epoch  6, batch     4 | loss: 1.0413601Losses:  0.36778199672698975 0.5522083044052124
MemoryTrain:  epoch  7, batch     0 | loss: 0.9199903Losses:  0.44143593311309814 0.4626825451850891
MemoryTrain:  epoch  7, batch     1 | loss: 0.9041185Losses:  0.24830257892608643 0.26340585947036743
MemoryTrain:  epoch  7, batch     2 | loss: 0.5117084Losses:  0.2524169981479645 0.44803184270858765
MemoryTrain:  epoch  7, batch     3 | loss: 0.7004489Losses:  0.4723808467388153 0.4534139037132263
MemoryTrain:  epoch  7, batch     4 | loss: 0.9257947Losses:  0.3584679961204529 0.38938066363334656
MemoryTrain:  epoch  8, batch     0 | loss: 0.7478486Losses:  0.30757397413253784 0.3480742275714874
MemoryTrain:  epoch  8, batch     1 | loss: 0.6556482Losses:  0.39491742849349976 0.7562037706375122
MemoryTrain:  epoch  8, batch     2 | loss: 1.1511211Losses:  0.3361470699310303 0.37231138348579407
MemoryTrain:  epoch  8, batch     3 | loss: 0.7084584Losses:  0.3261072635650635 0.3441060781478882
MemoryTrain:  epoch  8, batch     4 | loss: 0.6702133Losses:  0.39349645376205444 0.5117122530937195
MemoryTrain:  epoch  9, batch     0 | loss: 0.9052087Losses:  0.34629392623901367 0.5224328637123108
MemoryTrain:  epoch  9, batch     1 | loss: 0.8687268Losses:  0.261996328830719 0.3486052453517914
MemoryTrain:  epoch  9, batch     2 | loss: 0.6106015Losses:  0.29088205099105835 0.44759559631347656
MemoryTrain:  epoch  9, batch     3 | loss: 0.7384776Losses:  0.33279091119766235 0.30459171533584595
MemoryTrain:  epoch  9, batch     4 | loss: 0.6373826
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 80.79%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 80.82%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 80.42%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 79.76%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 78.70%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 79.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.61%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 76.21%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 75.40%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.01%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 87.21%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 85.96%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 83.58%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 82.44%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 82.63%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 82.08%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 81.40%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 80.59%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 79.87%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 79.17%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 78.48%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 77.60%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 76.74%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.89%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 75.07%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 74.33%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 73.97%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 74.94%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.65%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 74.31%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 73.70%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 73.60%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 73.03%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.11%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 73.09%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 73.00%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 72.71%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 72.39%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 72.26%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 71.18%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.26%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 72.26%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 71.96%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.69%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 71.47%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 71.17%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 70.94%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 70.49%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 70.27%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 69.91%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 69.49%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 69.19%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 68.61%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.54%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 69.84%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 69.63%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 69.55%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 69.48%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.06%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 70.27%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 71.97%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 71.90%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 72.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 72.36%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 71.96%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 71.80%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 71.64%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 71.55%   
cur_acc:  ['0.9405', '0.6200', '0.6696', '0.7540']
his_acc:  ['0.9405', '0.7795', '0.7171', '0.7155']
Clustering into  24  clusters
Clusters:  [ 2  1 15  0  2  2 19  2 14 21 22  0  2  0  0 20  2  2  2  2  2  2  2  9
  2 16  2 23 13 12 18  2 11  2  7 17  8  2  2  2  1  6  2  2  2  3  2 10
  5  4]
Losses:  6.800541400909424 1.111983299255371
CurrentTrain: epoch  0, batch     0 | loss: 7.9125247Losses:  8.39881706237793 0.8914240598678589
CurrentTrain: epoch  0, batch     1 | loss: 9.2902412Losses:  7.767393589019775 1.0914884805679321
CurrentTrain: epoch  0, batch     2 | loss: 8.8588820Losses:  8.146478652954102 1.1462262868881226
CurrentTrain: epoch  0, batch     3 | loss: 9.2927046Losses:  7.535229682922363 1.014346718788147
CurrentTrain: epoch  0, batch     4 | loss: 8.5495768Losses:  6.190204620361328 1.0727617740631104
CurrentTrain: epoch  0, batch     5 | loss: 7.2629662Losses:  3.830320358276367 0.1511393040418625
CurrentTrain: epoch  0, batch     6 | loss: 3.9814596Losses:  2.899899959564209 0.414707750082016
CurrentTrain: epoch  1, batch     0 | loss: 3.3146076Losses:  4.472251892089844 0.7836166620254517
CurrentTrain: epoch  1, batch     1 | loss: 5.2558684Losses:  3.053368091583252 0.7185937166213989
CurrentTrain: epoch  1, batch     2 | loss: 3.7719617Losses:  3.245373249053955 0.9812293648719788
CurrentTrain: epoch  1, batch     3 | loss: 4.2266026Losses:  2.980595111846924 0.9709341526031494
CurrentTrain: epoch  1, batch     4 | loss: 3.9515293Losses:  3.472093105316162 0.7358475923538208
CurrentTrain: epoch  1, batch     5 | loss: 4.2079406Losses:  2.8511743545532227 0.27459487318992615
CurrentTrain: epoch  1, batch     6 | loss: 3.1257691Losses:  3.6952872276306152 0.8038740158081055
CurrentTrain: epoch  2, batch     0 | loss: 4.4991612Losses:  3.02333664894104 0.6605302095413208
CurrentTrain: epoch  2, batch     1 | loss: 3.6838670Losses:  2.624664306640625 0.7320379018783569
CurrentTrain: epoch  2, batch     2 | loss: 3.3567023Losses:  3.3472113609313965 0.9780550003051758
CurrentTrain: epoch  2, batch     3 | loss: 4.3252664Losses:  2.476830244064331 0.8000060319900513
CurrentTrain: epoch  2, batch     4 | loss: 3.2768364Losses:  2.674443244934082 0.6366742849349976
CurrentTrain: epoch  2, batch     5 | loss: 3.3111176Losses:  3.9090938568115234 0.2805716395378113
CurrentTrain: epoch  2, batch     6 | loss: 4.1896653Losses:  2.8792550563812256 0.9759013652801514
CurrentTrain: epoch  3, batch     0 | loss: 3.8551564Losses:  2.1838643550872803 0.5624811053276062
CurrentTrain: epoch  3, batch     1 | loss: 2.7463455Losses:  2.781627655029297 0.6729074120521545
CurrentTrain: epoch  3, batch     2 | loss: 3.4545350Losses:  2.474900245666504 0.7102911472320557
CurrentTrain: epoch  3, batch     3 | loss: 3.1851914Losses:  2.1932618618011475 0.299564927816391
CurrentTrain: epoch  3, batch     4 | loss: 2.4928267Losses:  2.861203908920288 0.7814093232154846
CurrentTrain: epoch  3, batch     5 | loss: 3.6426132Losses:  4.522834300994873 0.3042081594467163
CurrentTrain: epoch  3, batch     6 | loss: 4.8270426Losses:  1.9627625942230225 0.35604387521743774
CurrentTrain: epoch  4, batch     0 | loss: 2.3188064Losses:  2.778780221939087 0.6072360277175903
CurrentTrain: epoch  4, batch     1 | loss: 3.3860164Losses:  2.989121913909912 0.7106505036354065
CurrentTrain: epoch  4, batch     2 | loss: 3.6997724Losses:  2.2748351097106934 0.5073432922363281
CurrentTrain: epoch  4, batch     3 | loss: 2.7821784Losses:  2.11858868598938 0.62285315990448
CurrentTrain: epoch  4, batch     4 | loss: 2.7414417Losses:  2.398345708847046 0.6515503525733948
CurrentTrain: epoch  4, batch     5 | loss: 3.0498960Losses:  3.364187240600586 0.2605810761451721
CurrentTrain: epoch  4, batch     6 | loss: 3.6247683Losses:  2.6152472496032715 0.7018747925758362
CurrentTrain: epoch  5, batch     0 | loss: 3.3171220Losses:  2.366455078125 0.5860353708267212
CurrentTrain: epoch  5, batch     1 | loss: 2.9524903Losses:  2.3456268310546875 0.5936092138290405
CurrentTrain: epoch  5, batch     2 | loss: 2.9392362Losses:  2.0743236541748047 0.5619115829467773
CurrentTrain: epoch  5, batch     3 | loss: 2.6362352Losses:  2.1409072875976562 0.679995059967041
CurrentTrain: epoch  5, batch     4 | loss: 2.8209023Losses:  2.5114874839782715 0.5110635757446289
CurrentTrain: epoch  5, batch     5 | loss: 3.0225511Losses:  2.0016300678253174 0.06850018352270126
CurrentTrain: epoch  5, batch     6 | loss: 2.0701303Losses:  2.084921360015869 0.47205427289009094
CurrentTrain: epoch  6, batch     0 | loss: 2.5569756Losses:  2.401181697845459 0.48535799980163574
CurrentTrain: epoch  6, batch     1 | loss: 2.8865397Losses:  2.238345146179199 0.5498558878898621
CurrentTrain: epoch  6, batch     2 | loss: 2.7882011Losses:  2.301992654800415 0.6534250974655151
CurrentTrain: epoch  6, batch     3 | loss: 2.9554176Losses:  1.8475918769836426 0.33692002296447754
CurrentTrain: epoch  6, batch     4 | loss: 2.1845119Losses:  2.3230791091918945 0.6176655292510986
CurrentTrain: epoch  6, batch     5 | loss: 2.9407446Losses:  1.742730736732483 0.13847167789936066
CurrentTrain: epoch  6, batch     6 | loss: 1.8812025Losses:  2.0487852096557617 0.5460324883460999
CurrentTrain: epoch  7, batch     0 | loss: 2.5948176Losses:  1.8737677335739136 0.366855651140213
CurrentTrain: epoch  7, batch     1 | loss: 2.2406235Losses:  1.8932117223739624 0.4819521903991699
CurrentTrain: epoch  7, batch     2 | loss: 2.3751640Losses:  2.1596622467041016 0.4740484654903412
CurrentTrain: epoch  7, batch     3 | loss: 2.6337106Losses:  1.9733974933624268 0.4975088834762573
CurrentTrain: epoch  7, batch     4 | loss: 2.4709063Losses:  2.1019177436828613 0.552344560623169
CurrentTrain: epoch  7, batch     5 | loss: 2.6542623Losses:  2.4266133308410645 0.32497653365135193
CurrentTrain: epoch  7, batch     6 | loss: 2.7515898Losses:  1.9723386764526367 0.47450771927833557
CurrentTrain: epoch  8, batch     0 | loss: 2.4468465Losses:  1.7814254760742188 0.363093763589859
CurrentTrain: epoch  8, batch     1 | loss: 2.1445193Losses:  1.887882947921753 0.5002622008323669
CurrentTrain: epoch  8, batch     2 | loss: 2.3881452Losses:  2.089308738708496 0.32038718461990356
CurrentTrain: epoch  8, batch     3 | loss: 2.4096959Losses:  2.0447471141815186 0.37320756912231445
CurrentTrain: epoch  8, batch     4 | loss: 2.4179547Losses:  2.0661423206329346 0.31649577617645264
CurrentTrain: epoch  8, batch     5 | loss: 2.3826380Losses:  1.9065923690795898 0.056823357939720154
CurrentTrain: epoch  8, batch     6 | loss: 1.9634157Losses:  1.9500964879989624 0.33015531301498413
CurrentTrain: epoch  9, batch     0 | loss: 2.2802517Losses:  1.9333226680755615 0.5408654808998108
CurrentTrain: epoch  9, batch     1 | loss: 2.4741881Losses:  1.737191915512085 0.3091139495372772
CurrentTrain: epoch  9, batch     2 | loss: 2.0463059Losses:  1.8720098733901978 0.3344932794570923
CurrentTrain: epoch  9, batch     3 | loss: 2.2065032Losses:  1.8852038383483887 0.3344385623931885
CurrentTrain: epoch  9, batch     4 | loss: 2.2196424Losses:  1.9712114334106445 0.42294496297836304
CurrentTrain: epoch  9, batch     5 | loss: 2.3941565Losses:  1.9482531547546387 0.048991356045007706
CurrentTrain: epoch  9, batch     6 | loss: 1.9972445
Losses:  5.925375938415527 0.6814579963684082
MemoryTrain:  epoch  0, batch     0 | loss: 6.6068339Losses:  7.6543450355529785 0.42658671736717224
MemoryTrain:  epoch  0, batch     1 | loss: 8.0809317Losses:  10.286534309387207 0.5234341621398926
MemoryTrain:  epoch  0, batch     2 | loss: 10.8099689Losses:  11.455856323242188 0.5030422210693359
MemoryTrain:  epoch  0, batch     3 | loss: 11.9588985Losses:  9.629716873168945 0.38821977376937866
MemoryTrain:  epoch  0, batch     4 | loss: 10.0179367Losses:  10.338118553161621 0.3400164544582367
MemoryTrain:  epoch  0, batch     5 | loss: 10.6781349Losses:  10.896188735961914 0.12177588045597076
MemoryTrain:  epoch  0, batch     6 | loss: 11.0179644Losses:  1.5062971115112305 0.42719241976737976
MemoryTrain:  epoch  1, batch     0 | loss: 1.9334896Losses:  1.2030797004699707 0.6323990821838379
MemoryTrain:  epoch  1, batch     1 | loss: 1.8354788Losses:  0.9832298755645752 0.3337777256965637
MemoryTrain:  epoch  1, batch     2 | loss: 1.3170075Losses:  0.5755472779273987 0.26414626836776733
MemoryTrain:  epoch  1, batch     3 | loss: 0.8396935Losses:  0.8152079582214355 0.5149722099304199
MemoryTrain:  epoch  1, batch     4 | loss: 1.3301802Losses:  1.3660759925842285 0.3682081699371338
MemoryTrain:  epoch  1, batch     5 | loss: 1.7342842Losses:  1.2841311693191528 0.05245618894696236
MemoryTrain:  epoch  1, batch     6 | loss: 1.3365873Losses:  0.5812394618988037 0.49805763363838196
MemoryTrain:  epoch  2, batch     0 | loss: 1.0792971Losses:  0.625150740146637 0.3398367166519165
MemoryTrain:  epoch  2, batch     1 | loss: 0.9649875Losses:  1.071082592010498 0.4421513080596924
MemoryTrain:  epoch  2, batch     2 | loss: 1.5132339Losses:  0.8267878293991089 0.40373313426971436
MemoryTrain:  epoch  2, batch     3 | loss: 1.2305210Losses:  1.3468525409698486 0.557837724685669
MemoryTrain:  epoch  2, batch     4 | loss: 1.9046903Losses:  0.8890492916107178 0.624678373336792
MemoryTrain:  epoch  2, batch     5 | loss: 1.5137277Losses:  0.20937809348106384 0.026837565004825592
MemoryTrain:  epoch  2, batch     6 | loss: 0.2362157Losses:  0.5244293808937073 0.38361865282058716
MemoryTrain:  epoch  3, batch     0 | loss: 0.9080480Losses:  0.6653302907943726 0.6452571153640747
MemoryTrain:  epoch  3, batch     1 | loss: 1.3105874Losses:  0.8119332790374756 0.5214818716049194
MemoryTrain:  epoch  3, batch     2 | loss: 1.3334152Losses:  0.9857326745986938 0.5801610946655273
MemoryTrain:  epoch  3, batch     3 | loss: 1.5658938Losses:  0.749796450138092 0.3513152301311493
MemoryTrain:  epoch  3, batch     4 | loss: 1.1011117Losses:  0.4975031018257141 0.33840152621269226
MemoryTrain:  epoch  3, batch     5 | loss: 0.8359046Losses:  0.20233824849128723 0.07610055059194565
MemoryTrain:  epoch  3, batch     6 | loss: 0.2784388Losses:  0.49977749586105347 0.33948850631713867
MemoryTrain:  epoch  4, batch     0 | loss: 0.8392660Losses:  0.8518975973129272 0.48924174904823303
MemoryTrain:  epoch  4, batch     1 | loss: 1.3411393Losses:  0.45230454206466675 0.4463276267051697
MemoryTrain:  epoch  4, batch     2 | loss: 0.8986322Losses:  0.5795630812644958 0.431937575340271
MemoryTrain:  epoch  4, batch     3 | loss: 1.0115006Losses:  0.6178050637245178 0.6293672919273376
MemoryTrain:  epoch  4, batch     4 | loss: 1.2471724Losses:  0.4388886094093323 0.43418359756469727
MemoryTrain:  epoch  4, batch     5 | loss: 0.8730722Losses:  0.4693795144557953 0.0057602240704
MemoryTrain:  epoch  4, batch     6 | loss: 0.4751397Losses:  0.7664147615432739 0.5975246429443359
MemoryTrain:  epoch  5, batch     0 | loss: 1.3639394Losses:  0.481935977935791 0.44258061051368713
MemoryTrain:  epoch  5, batch     1 | loss: 0.9245166Losses:  0.45035994052886963 0.4086266756057739
MemoryTrain:  epoch  5, batch     2 | loss: 0.8589866Losses:  0.38880568742752075 0.37244054675102234
MemoryTrain:  epoch  5, batch     3 | loss: 0.7612462Losses:  0.41285547614097595 0.3260800838470459
MemoryTrain:  epoch  5, batch     4 | loss: 0.7389356Losses:  0.4528540372848511 0.4997010827064514
MemoryTrain:  epoch  5, batch     5 | loss: 0.9525551Losses:  0.2002771496772766 0.016064319759607315
MemoryTrain:  epoch  5, batch     6 | loss: 0.2163415Losses:  0.3385853171348572 0.4509851634502411
MemoryTrain:  epoch  6, batch     0 | loss: 0.7895705Losses:  0.3110388517379761 0.3410134017467499
MemoryTrain:  epoch  6, batch     1 | loss: 0.6520523Losses:  0.3133503198623657 0.38894373178482056
MemoryTrain:  epoch  6, batch     2 | loss: 0.7022941Losses:  0.42226552963256836 0.4762415885925293
MemoryTrain:  epoch  6, batch     3 | loss: 0.8985071Losses:  0.3321079611778259 0.49182888865470886
MemoryTrain:  epoch  6, batch     4 | loss: 0.8239368Losses:  0.5402766466140747 0.4815616309642792
MemoryTrain:  epoch  6, batch     5 | loss: 1.0218383Losses:  0.7729799151420593 0.1464512050151825
MemoryTrain:  epoch  6, batch     6 | loss: 0.9194311Losses:  0.4658937454223633 0.5619235038757324
MemoryTrain:  epoch  7, batch     0 | loss: 1.0278172Losses:  0.46497154235839844 0.43445730209350586
MemoryTrain:  epoch  7, batch     1 | loss: 0.8994288Losses:  0.3256964683532715 0.4278497099876404
MemoryTrain:  epoch  7, batch     2 | loss: 0.7535462Losses:  0.2633149027824402 0.28349578380584717
MemoryTrain:  epoch  7, batch     3 | loss: 0.5468107Losses:  0.2612767517566681 0.2944115996360779
MemoryTrain:  epoch  7, batch     4 | loss: 0.5556884Losses:  0.36550280451774597 0.4450904130935669
MemoryTrain:  epoch  7, batch     5 | loss: 0.8105932Losses:  0.45870840549468994 0.09017910063266754
MemoryTrain:  epoch  7, batch     6 | loss: 0.5488875Losses:  0.453657865524292 0.324067622423172
MemoryTrain:  epoch  8, batch     0 | loss: 0.7777255Losses:  0.33385834097862244 0.4750306308269501
MemoryTrain:  epoch  8, batch     1 | loss: 0.8088890Losses:  0.3752281069755554 0.4600844383239746
MemoryTrain:  epoch  8, batch     2 | loss: 0.8353125Losses:  0.5667923092842102 0.46937066316604614
MemoryTrain:  epoch  8, batch     3 | loss: 1.0361630Losses:  0.29855871200561523 0.4242915213108063
MemoryTrain:  epoch  8, batch     4 | loss: 0.7228502Losses:  0.41481858491897583 0.45783886313438416
MemoryTrain:  epoch  8, batch     5 | loss: 0.8726574Losses:  0.2960681617259979 0.023255009204149246
MemoryTrain:  epoch  8, batch     6 | loss: 0.3193232Losses:  0.40344735980033875 0.3740198612213135
MemoryTrain:  epoch  9, batch     0 | loss: 0.7774673Losses:  0.34782668948173523 0.3092539310455322
MemoryTrain:  epoch  9, batch     1 | loss: 0.6570807Losses:  0.40297678112983704 0.42727798223495483
MemoryTrain:  epoch  9, batch     2 | loss: 0.8302548Losses:  0.35119324922561646 0.40764471888542175
MemoryTrain:  epoch  9, batch     3 | loss: 0.7588379Losses:  0.4967624247074127 0.38342201709747314
MemoryTrain:  epoch  9, batch     4 | loss: 0.8801844Losses:  0.38580092787742615 0.44221562147140503
MemoryTrain:  epoch  9, batch     5 | loss: 0.8280165Losses:  0.35667842626571655 0.0341765470802784
MemoryTrain:  epoch  9, batch     6 | loss: 0.3908550
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.09%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.73%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 86.84%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 86.53%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 86.44%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 85.96%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 85.99%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 85.32%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 83.98%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 82.69%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 81.72%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 80.69%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 79.69%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 78.89%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 78.93%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 79.14%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.54%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 79.87%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 80.26%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 79.52%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 78.79%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 77.94%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 77.18%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 76.44%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 75.71%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 74.86%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 74.03%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 73.21%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 72.49%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 71.77%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 72.96%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 72.28%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 71.67%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 71.02%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 70.38%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 69.75%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 69.25%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 69.34%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 69.22%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 69.05%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 68.22%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 67.94%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 68.40%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 68.54%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 68.34%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 68.26%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 67.97%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 67.90%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 66.89%   [EVAL] batch:  161 | acc: 18.75%,  total acc: 66.59%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 65.59%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 65.31%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 65.04%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 64.78%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.26%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 66.07%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 65.86%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 65.58%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 65.43%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 68.40%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 68.29%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 68.35%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 68.80%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 68.72%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 68.47%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 68.32%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.58%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 68.66%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 68.66%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 69.16%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 69.04%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 68.86%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 68.79%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 68.60%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 68.38%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 68.30%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.39%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.43%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.67%   
cur_acc:  ['0.9405', '0.6200', '0.6696', '0.7540', '0.7530']
his_acc:  ['0.9405', '0.7795', '0.7171', '0.7155', '0.6967']
Clustering into  29  clusters
Clusters:  [ 0  1 21  0  0  0 25  0 15 27 16  0  0  2  2  7  0  0  0  0  0  0  0 28
  0 19  0 23 17 22 24  0 14  0 18 20  9  0  0  0  1  8  0  0  0  3  0 13
 26 11 10  4  2  0  0 12  0  6  0  5]
Losses:  6.985007286071777 0.9619536399841309
CurrentTrain: epoch  0, batch     0 | loss: 7.9469609Losses:  8.3350830078125 0.7907881140708923
CurrentTrain: epoch  0, batch     1 | loss: 9.1258707Losses:  8.881937026977539 1.1733779907226562
CurrentTrain: epoch  0, batch     2 | loss: 10.0553150Losses:  7.711900234222412 1.1239056587219238
CurrentTrain: epoch  0, batch     3 | loss: 8.8358059Losses:  7.296999931335449 1.6681135892868042
CurrentTrain: epoch  0, batch     4 | loss: 8.9651136Losses:  5.986284255981445 1.1382694244384766
CurrentTrain: epoch  0, batch     5 | loss: 7.1245537Losses:  7.400333881378174 0.3586592972278595
CurrentTrain: epoch  0, batch     6 | loss: 7.7589931Losses:  3.104531764984131 0.7234691977500916
CurrentTrain: epoch  1, batch     0 | loss: 3.8280010Losses:  3.4111883640289307 1.2045214176177979
CurrentTrain: epoch  1, batch     1 | loss: 4.6157098Losses:  3.497657537460327 1.2937501668930054
CurrentTrain: epoch  1, batch     2 | loss: 4.7914076Losses:  3.5691895484924316 0.6855922341346741
CurrentTrain: epoch  1, batch     3 | loss: 4.2547817Losses:  4.056056499481201 1.1756670475006104
CurrentTrain: epoch  1, batch     4 | loss: 5.2317238Losses:  3.8811960220336914 1.3721351623535156
CurrentTrain: epoch  1, batch     5 | loss: 5.2533312Losses:  6.237765312194824 0.7381281852722168
CurrentTrain: epoch  1, batch     6 | loss: 6.9758935Losses:  3.7418155670166016 0.6582411527633667
CurrentTrain: epoch  2, batch     0 | loss: 4.4000568Losses:  2.956449270248413 0.9189376831054688
CurrentTrain: epoch  2, batch     1 | loss: 3.8753870Losses:  3.055142879486084 1.144086480140686
CurrentTrain: epoch  2, batch     2 | loss: 4.1992292Losses:  4.251346588134766 1.117218017578125
CurrentTrain: epoch  2, batch     3 | loss: 5.3685646Losses:  2.9780306816101074 0.8599281311035156
CurrentTrain: epoch  2, batch     4 | loss: 3.8379588Losses:  3.610445499420166 0.9825211763381958
CurrentTrain: epoch  2, batch     5 | loss: 4.5929666Losses:  2.3375096321105957 0.21730664372444153
CurrentTrain: epoch  2, batch     6 | loss: 2.5548162Losses:  2.548192024230957 0.7817882299423218
CurrentTrain: epoch  3, batch     0 | loss: 3.3299804Losses:  3.2019736766815186 1.0874040126800537
CurrentTrain: epoch  3, batch     1 | loss: 4.2893777Losses:  3.8065743446350098 0.9328486323356628
CurrentTrain: epoch  3, batch     2 | loss: 4.7394228Losses:  3.657593011856079 0.9499977827072144
CurrentTrain: epoch  3, batch     3 | loss: 4.6075907Losses:  2.7538838386535645 0.8576765060424805
CurrentTrain: epoch  3, batch     4 | loss: 3.6115603Losses:  2.069920063018799 0.30425187945365906
CurrentTrain: epoch  3, batch     5 | loss: 2.3741720Losses:  4.467424392700195 0.7094225287437439
CurrentTrain: epoch  3, batch     6 | loss: 5.1768470Losses:  2.7965192794799805 0.7089943885803223
CurrentTrain: epoch  4, batch     0 | loss: 3.5055137Losses:  3.5657665729522705 1.0709669589996338
CurrentTrain: epoch  4, batch     1 | loss: 4.6367335Losses:  3.0379936695098877 0.8758397102355957
CurrentTrain: epoch  4, batch     2 | loss: 3.9138334Losses:  2.8221569061279297 0.7994970679283142
CurrentTrain: epoch  4, batch     3 | loss: 3.6216540Losses:  2.0372421741485596 0.3523859977722168
CurrentTrain: epoch  4, batch     4 | loss: 2.3896282Losses:  2.9080045223236084 0.7705574035644531
CurrentTrain: epoch  4, batch     5 | loss: 3.6785619Losses:  2.2224771976470947 0.21710491180419922
CurrentTrain: epoch  4, batch     6 | loss: 2.4395821Losses:  2.941680908203125 0.5702449083328247
CurrentTrain: epoch  5, batch     0 | loss: 3.5119257Losses:  2.873260974884033 0.527207612991333
CurrentTrain: epoch  5, batch     1 | loss: 3.4004686Losses:  2.9590060710906982 0.6252698302268982
CurrentTrain: epoch  5, batch     2 | loss: 3.5842760Losses:  2.827094554901123 0.895298421382904
CurrentTrain: epoch  5, batch     3 | loss: 3.7223930Losses:  2.4681973457336426 0.5597248673439026
CurrentTrain: epoch  5, batch     4 | loss: 3.0279222Losses:  1.913806438446045 0.3218965530395508
CurrentTrain: epoch  5, batch     5 | loss: 2.2357030Losses:  2.8257710933685303 0.19284765422344208
CurrentTrain: epoch  5, batch     6 | loss: 3.0186188Losses:  2.576439142227173 0.7381960153579712
CurrentTrain: epoch  6, batch     0 | loss: 3.3146353Losses:  2.174865245819092 0.59084153175354
CurrentTrain: epoch  6, batch     1 | loss: 2.7657068Losses:  2.498443841934204 0.6191432476043701
CurrentTrain: epoch  6, batch     2 | loss: 3.1175871Losses:  2.5015087127685547 0.6245745420455933
CurrentTrain: epoch  6, batch     3 | loss: 3.1260834Losses:  2.4834656715393066 0.36466848850250244
CurrentTrain: epoch  6, batch     4 | loss: 2.8481340Losses:  2.570464849472046 0.46196481585502625
CurrentTrain: epoch  6, batch     5 | loss: 3.0324297Losses:  3.1991770267486572 0.09495741128921509
CurrentTrain: epoch  6, batch     6 | loss: 3.2941344Losses:  2.6480607986450195 0.8266392946243286
CurrentTrain: epoch  7, batch     0 | loss: 3.4747000Losses:  2.413973569869995 0.5460857152938843
CurrentTrain: epoch  7, batch     1 | loss: 2.9600592Losses:  2.246066093444824 0.2555435299873352
CurrentTrain: epoch  7, batch     2 | loss: 2.5016096Losses:  2.45056414604187 0.664364218711853
CurrentTrain: epoch  7, batch     3 | loss: 3.1149282Losses:  2.24948787689209 0.4619811773300171
CurrentTrain: epoch  7, batch     4 | loss: 2.7114692Losses:  2.4714572429656982 0.732428789138794
CurrentTrain: epoch  7, batch     5 | loss: 3.2038860Losses:  1.8097079992294312 0.0602390319108963
CurrentTrain: epoch  7, batch     6 | loss: 1.8699471Losses:  2.2336816787719727 0.5266456007957458
CurrentTrain: epoch  8, batch     0 | loss: 2.7603273Losses:  2.3544487953186035 0.5832342505455017
CurrentTrain: epoch  8, batch     1 | loss: 2.9376831Losses:  2.2748825550079346 0.406375527381897
CurrentTrain: epoch  8, batch     2 | loss: 2.6812582Losses:  2.3316619396209717 0.46078968048095703
CurrentTrain: epoch  8, batch     3 | loss: 2.7924516Losses:  2.2446470260620117 0.6180905103683472
CurrentTrain: epoch  8, batch     4 | loss: 2.8627377Losses:  2.3445301055908203 0.565595269203186
CurrentTrain: epoch  8, batch     5 | loss: 2.9101253Losses:  1.7671438455581665 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.7671438Losses:  1.8718390464782715 0.318328857421875
CurrentTrain: epoch  9, batch     0 | loss: 2.1901679Losses:  2.2373361587524414 0.2694470286369324
CurrentTrain: epoch  9, batch     1 | loss: 2.5067832Losses:  2.3349525928497314 0.5862022042274475
CurrentTrain: epoch  9, batch     2 | loss: 2.9211547Losses:  2.330437660217285 0.611290454864502
CurrentTrain: epoch  9, batch     3 | loss: 2.9417281Losses:  2.120242118835449 0.4327077567577362
CurrentTrain: epoch  9, batch     4 | loss: 2.5529499Losses:  2.335554838180542 0.5275946855545044
CurrentTrain: epoch  9, batch     5 | loss: 2.8631496Losses:  1.6926724910736084 0.05651789531111717
CurrentTrain: epoch  9, batch     6 | loss: 1.7491903
Losses:  6.13248348236084 0.4124007523059845
MemoryTrain:  epoch  0, batch     0 | loss: 6.5448842Losses:  8.612918853759766 0.3226383328437805
MemoryTrain:  epoch  0, batch     1 | loss: 8.9355574Losses:  8.581460952758789 0.4980027675628662
MemoryTrain:  epoch  0, batch     2 | loss: 9.0794640Losses:  9.836965560913086 0.6119164824485779
MemoryTrain:  epoch  0, batch     3 | loss: 10.4488821Losses:  9.260831832885742 0.24026024341583252
MemoryTrain:  epoch  0, batch     4 | loss: 9.5010920Losses:  10.132761001586914 0.5451397895812988
MemoryTrain:  epoch  0, batch     5 | loss: 10.6779003Losses:  11.885577201843262 0.6077507138252258
MemoryTrain:  epoch  0, batch     6 | loss: 12.4933281Losses:  10.527591705322266 0.4186527132987976
MemoryTrain:  epoch  0, batch     7 | loss: 10.9462442Losses:  0.8429887890815735 0.5286003351211548
MemoryTrain:  epoch  1, batch     0 | loss: 1.3715892Losses:  1.3557069301605225 0.5651637315750122
MemoryTrain:  epoch  1, batch     1 | loss: 1.9208707Losses:  0.4290829300880432 0.3955930769443512
MemoryTrain:  epoch  1, batch     2 | loss: 0.8246760Losses:  0.7860973477363586 0.43197932839393616
MemoryTrain:  epoch  1, batch     3 | loss: 1.2180767Losses:  0.7234447598457336 0.27513033151626587
MemoryTrain:  epoch  1, batch     4 | loss: 0.9985751Losses:  1.3234319686889648 0.7571706771850586
MemoryTrain:  epoch  1, batch     5 | loss: 2.0806026Losses:  0.5725122690200806 0.30033165216445923
MemoryTrain:  epoch  1, batch     6 | loss: 0.8728439Losses:  0.3385732173919678 0.18821942806243896
MemoryTrain:  epoch  1, batch     7 | loss: 0.5267926Losses:  0.5825960636138916 0.49791374802589417
MemoryTrain:  epoch  2, batch     0 | loss: 1.0805098Losses:  0.49277743697166443 0.49594131112098694
MemoryTrain:  epoch  2, batch     1 | loss: 0.9887187Losses:  1.3246052265167236 0.43673601746559143
MemoryTrain:  epoch  2, batch     2 | loss: 1.7613412Losses:  0.6349583268165588 0.3975149393081665
MemoryTrain:  epoch  2, batch     3 | loss: 1.0324733Losses:  0.4005305767059326 0.3490487039089203
MemoryTrain:  epoch  2, batch     4 | loss: 0.7495793Losses:  0.7499815225601196 0.5346137881278992
MemoryTrain:  epoch  2, batch     5 | loss: 1.2845953Losses:  0.5312023162841797 0.26056963205337524
MemoryTrain:  epoch  2, batch     6 | loss: 0.7917719Losses:  0.7425647974014282 0.44698721170425415
MemoryTrain:  epoch  2, batch     7 | loss: 1.1895521Losses:  0.5604935884475708 0.30833208560943604
MemoryTrain:  epoch  3, batch     0 | loss: 0.8688257Losses:  0.5169218182563782 0.37668076157569885
MemoryTrain:  epoch  3, batch     1 | loss: 0.8936026Losses:  0.485037237405777 0.5491786599159241
MemoryTrain:  epoch  3, batch     2 | loss: 1.0342159Losses:  0.5710139274597168 0.46782955527305603
MemoryTrain:  epoch  3, batch     3 | loss: 1.0388435Losses:  1.0391069650650024 0.7962952852249146
MemoryTrain:  epoch  3, batch     4 | loss: 1.8354023Losses:  0.28064560890197754 0.3505076766014099
MemoryTrain:  epoch  3, batch     5 | loss: 0.6311533Losses:  0.768987238407135 0.41545575857162476
MemoryTrain:  epoch  3, batch     6 | loss: 1.1844430Losses:  0.4265751242637634 0.22296994924545288
MemoryTrain:  epoch  3, batch     7 | loss: 0.6495451Losses:  0.43346261978149414 0.312959223985672
MemoryTrain:  epoch  4, batch     0 | loss: 0.7464218Losses:  0.41081303358078003 0.38605931401252747
MemoryTrain:  epoch  4, batch     1 | loss: 0.7968724Losses:  0.6938344836235046 0.3795854449272156
MemoryTrain:  epoch  4, batch     2 | loss: 1.0734199Losses:  0.44513458013534546 0.453281044960022
MemoryTrain:  epoch  4, batch     3 | loss: 0.8984156Losses:  0.6055285334587097 0.5700486898422241
MemoryTrain:  epoch  4, batch     4 | loss: 1.1755772Losses:  0.44547605514526367 0.45122554898262024
MemoryTrain:  epoch  4, batch     5 | loss: 0.8967016Losses:  0.7147613763809204 0.396481990814209
MemoryTrain:  epoch  4, batch     6 | loss: 1.1112434Losses:  0.3605519235134125 0.1403840184211731
MemoryTrain:  epoch  4, batch     7 | loss: 0.5009359Losses:  0.39536553621292114 0.4262811541557312
MemoryTrain:  epoch  5, batch     0 | loss: 0.8216467Losses:  0.6489197015762329 0.5221668481826782
MemoryTrain:  epoch  5, batch     1 | loss: 1.1710865Losses:  0.32302945852279663 0.32016199827194214
MemoryTrain:  epoch  5, batch     2 | loss: 0.6431915Losses:  0.6562111377716064 0.5028524398803711
MemoryTrain:  epoch  5, batch     3 | loss: 1.1590636Losses:  0.35882991552352905 0.32945483922958374
MemoryTrain:  epoch  5, batch     4 | loss: 0.6882848Losses:  0.375270813703537 0.3729228675365448
MemoryTrain:  epoch  5, batch     5 | loss: 0.7481937Losses:  0.43232935667037964 0.3820905089378357
MemoryTrain:  epoch  5, batch     6 | loss: 0.8144199Losses:  0.45627665519714355 0.38861188292503357
MemoryTrain:  epoch  5, batch     7 | loss: 0.8448886Losses:  0.4140024483203888 0.49480175971984863
MemoryTrain:  epoch  6, batch     0 | loss: 0.9088042Losses:  0.4480186700820923 0.34119272232055664
MemoryTrain:  epoch  6, batch     1 | loss: 0.7892114Losses:  0.49290263652801514 0.4448973834514618
MemoryTrain:  epoch  6, batch     2 | loss: 0.9378000Losses:  0.4450564980506897 0.37417909502983093
MemoryTrain:  epoch  6, batch     3 | loss: 0.8192356Losses:  0.458136647939682 0.3799077868461609
MemoryTrain:  epoch  6, batch     4 | loss: 0.8380444Losses:  0.4186202585697174 0.27971959114074707
MemoryTrain:  epoch  6, batch     5 | loss: 0.6983398Losses:  0.34868794679641724 0.4060092568397522
MemoryTrain:  epoch  6, batch     6 | loss: 0.7546972Losses:  0.21709248423576355 0.17792358994483948
MemoryTrain:  epoch  6, batch     7 | loss: 0.3950161Losses:  0.4417003393173218 0.3018680810928345
MemoryTrain:  epoch  7, batch     0 | loss: 0.7435684Losses:  0.38655024766921997 0.45942434668540955
MemoryTrain:  epoch  7, batch     1 | loss: 0.8459746Losses:  0.3495926558971405 0.33128654956817627
MemoryTrain:  epoch  7, batch     2 | loss: 0.6808792Losses:  0.3692184090614319 0.3504062294960022
MemoryTrain:  epoch  7, batch     3 | loss: 0.7196246Losses:  0.3687146306037903 0.43362879753112793
MemoryTrain:  epoch  7, batch     4 | loss: 0.8023434Losses:  0.4635488986968994 0.5326025485992432
MemoryTrain:  epoch  7, batch     5 | loss: 0.9961514Losses:  0.38072463870048523 0.3141913115978241
MemoryTrain:  epoch  7, batch     6 | loss: 0.6949160Losses:  0.3642628788948059 0.10587577521800995
MemoryTrain:  epoch  7, batch     7 | loss: 0.4701387Losses:  0.5512840747833252 0.3729802966117859
MemoryTrain:  epoch  8, batch     0 | loss: 0.9242644Losses:  0.34339797496795654 0.44907182455062866
MemoryTrain:  epoch  8, batch     1 | loss: 0.7924698Losses:  0.3124588429927826 0.2012828290462494
MemoryTrain:  epoch  8, batch     2 | loss: 0.5137417Losses:  0.41112595796585083 0.39825040102005005
MemoryTrain:  epoch  8, batch     3 | loss: 0.8093764Losses:  0.4337819218635559 0.37971603870391846
MemoryTrain:  epoch  8, batch     4 | loss: 0.8134980Losses:  0.43823227286338806 0.4678022563457489
MemoryTrain:  epoch  8, batch     5 | loss: 0.9060345Losses:  0.4821009635925293 0.4426541328430176
MemoryTrain:  epoch  8, batch     6 | loss: 0.9247551Losses:  0.33315590023994446 0.11474914848804474
MemoryTrain:  epoch  8, batch     7 | loss: 0.4479051Losses:  0.3295098543167114 0.2926718592643738
MemoryTrain:  epoch  9, batch     0 | loss: 0.6221817Losses:  0.3865128457546234 0.5169072151184082
MemoryTrain:  epoch  9, batch     1 | loss: 0.9034201Losses:  0.49313732981681824 0.3551780581474304
MemoryTrain:  epoch  9, batch     2 | loss: 0.8483154Losses:  0.30134668946266174 0.289404034614563
MemoryTrain:  epoch  9, batch     3 | loss: 0.5907507Losses:  0.4075794816017151 0.384157657623291
MemoryTrain:  epoch  9, batch     4 | loss: 0.7917371Losses:  0.32312631607055664 0.28106433153152466
MemoryTrain:  epoch  9, batch     5 | loss: 0.6041906Losses:  0.4757612645626068 0.47261273860931396
MemoryTrain:  epoch  9, batch     6 | loss: 0.9483740Losses:  0.5467671155929565 0.37944549322128296
MemoryTrain:  epoch  9, batch     7 | loss: 0.9262126
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 61.91%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.60%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 82.34%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 81.05%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 79.81%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 77.61%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 76.47%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 75.72%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 76.77%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 76.81%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 76.46%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 76.36%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 75.38%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 74.70%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 73.96%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 73.09%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 72.31%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 71.55%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 70.81%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 70.01%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 69.31%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 68.54%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 68.98%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 68.34%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 67.78%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 67.16%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 66.55%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 65.96%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 65.49%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 65.30%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 65.12%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 65.74%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 65.69%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 65.80%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 65.71%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 64.92%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.91%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 64.78%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 64.65%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 64.56%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 64.26%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 64.09%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 63.23%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 62.93%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 62.68%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 63.91%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 64.35%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 64.43%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 64.22%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 63.87%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 63.74%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 63.63%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 63.74%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 63.83%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 64.02%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.07%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 64.19%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 64.32%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 64.20%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 64.17%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 66.18%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.90%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.93%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 66.47%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 66.18%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 65.85%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 65.96%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 68.29%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.99%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 67.62%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 68.07%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 67.88%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.70%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 67.54%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 67.40%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 67.58%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 67.84%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 67.74%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 67.76%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 68.07%   
cur_acc:  ['0.9405', '0.6200', '0.6696', '0.7540', '0.7530', '0.6935']
his_acc:  ['0.9405', '0.7795', '0.7171', '0.7155', '0.6967', '0.6807']
Clustering into  34  clusters
Clusters:  [ 1  2 24  1  1  1 30  1 22  0 31  1  1  3  3 19  1  1  1  1  1  1  1 32
  1  0  1 27 17 26 25  1 33  1 20 23 21  1  1  1  2 15  1  1  1  9  1 16
 29 28 14 10  3  1  1 18  1  4  1 12  8 13  1  1  6 11  7  1  1  5]
Losses:  6.6722941398620605 1.1332616806030273
CurrentTrain: epoch  0, batch     0 | loss: 7.8055558Losses:  8.888609886169434 0.9778016805648804
CurrentTrain: epoch  0, batch     1 | loss: 9.8664112Losses:  7.6748809814453125 1.4719054698944092
CurrentTrain: epoch  0, batch     2 | loss: 9.1467867Losses:  7.564131736755371 1.2354660034179688
CurrentTrain: epoch  0, batch     3 | loss: 8.7995977Losses:  5.638083457946777 1.3312065601348877
CurrentTrain: epoch  0, batch     4 | loss: 6.9692898Losses:  6.185611724853516 0.6417618989944458
CurrentTrain: epoch  0, batch     5 | loss: 6.8273735Losses:  6.219642639160156 0.4923880100250244
CurrentTrain: epoch  0, batch     6 | loss: 6.7120304Losses:  3.405027151107788 0.8020036816596985
CurrentTrain: epoch  1, batch     0 | loss: 4.2070308Losses:  2.6255135536193848 1.0347412824630737
CurrentTrain: epoch  1, batch     1 | loss: 3.6602550Losses:  3.2462520599365234 0.6269757747650146
CurrentTrain: epoch  1, batch     2 | loss: 3.8732278Losses:  3.0103540420532227 1.1198941469192505
CurrentTrain: epoch  1, batch     3 | loss: 4.1302481Losses:  2.6499550342559814 1.36939537525177
CurrentTrain: epoch  1, batch     4 | loss: 4.0193505Losses:  2.436185836791992 1.1579797267913818
CurrentTrain: epoch  1, batch     5 | loss: 3.5941656Losses:  2.3773512840270996 0.2673046588897705
CurrentTrain: epoch  1, batch     6 | loss: 2.6446559Losses:  2.588989019393921 1.1260745525360107
CurrentTrain: epoch  2, batch     0 | loss: 3.7150636Losses:  2.4879863262176514 0.7851554155349731
CurrentTrain: epoch  2, batch     1 | loss: 3.2731419Losses:  2.671577215194702 0.6782882213592529
CurrentTrain: epoch  2, batch     2 | loss: 3.3498654Losses:  2.4199979305267334 0.8838974237442017
CurrentTrain: epoch  2, batch     3 | loss: 3.3038955Losses:  2.401710033416748 1.1145575046539307
CurrentTrain: epoch  2, batch     4 | loss: 3.5162675Losses:  2.4168481826782227 0.9795502424240112
CurrentTrain: epoch  2, batch     5 | loss: 3.3963985Losses:  1.7569797039031982 8.94069742685133e-08
CurrentTrain: epoch  2, batch     6 | loss: 1.7569798Losses:  2.1045854091644287 0.6382295489311218
CurrentTrain: epoch  3, batch     0 | loss: 2.7428150Losses:  2.2832958698272705 0.8017753958702087
CurrentTrain: epoch  3, batch     1 | loss: 3.0850713Losses:  2.2553915977478027 0.8214008808135986
CurrentTrain: epoch  3, batch     2 | loss: 3.0767925Losses:  2.3678877353668213 0.6056919097900391
CurrentTrain: epoch  3, batch     3 | loss: 2.9735796Losses:  2.154120683670044 0.43977782130241394
CurrentTrain: epoch  3, batch     4 | loss: 2.5938985Losses:  2.590784788131714 1.0239933729171753
CurrentTrain: epoch  3, batch     5 | loss: 3.6147780Losses:  2.248398780822754 0.125723198056221
CurrentTrain: epoch  3, batch     6 | loss: 2.3741219Losses:  2.046955108642578 0.43417513370513916
CurrentTrain: epoch  4, batch     0 | loss: 2.4811301Losses:  2.5535707473754883 0.5001109838485718
CurrentTrain: epoch  4, batch     1 | loss: 3.0536819Losses:  2.3428292274475098 0.8298614025115967
CurrentTrain: epoch  4, batch     2 | loss: 3.1726906Losses:  2.208327054977417 0.8401838541030884
CurrentTrain: epoch  4, batch     3 | loss: 3.0485110Losses:  2.013019323348999 0.2137812227010727
CurrentTrain: epoch  4, batch     4 | loss: 2.2268004Losses:  1.777491569519043 0.4102194607257843
CurrentTrain: epoch  4, batch     5 | loss: 2.1877110Losses:  1.788795828819275 0.14513495564460754
CurrentTrain: epoch  4, batch     6 | loss: 1.9339308Losses:  1.9724512100219727 0.5491364002227783
CurrentTrain: epoch  5, batch     0 | loss: 2.5215876Losses:  2.517442226409912 0.4982902705669403
CurrentTrain: epoch  5, batch     1 | loss: 3.0157325Losses:  2.0980892181396484 0.30864620208740234
CurrentTrain: epoch  5, batch     2 | loss: 2.4067354Losses:  1.890716314315796 0.3030000329017639
CurrentTrain: epoch  5, batch     3 | loss: 2.1937163Losses:  1.9640864133834839 0.40113988518714905
CurrentTrain: epoch  5, batch     4 | loss: 2.3652263Losses:  1.918964147567749 0.410552054643631
CurrentTrain: epoch  5, batch     5 | loss: 2.3295162Losses:  2.0438547134399414 0.13090729713439941
CurrentTrain: epoch  5, batch     6 | loss: 2.1747620Losses:  1.9649240970611572 0.39993512630462646
CurrentTrain: epoch  6, batch     0 | loss: 2.3648591Losses:  1.943723440170288 0.5093656182289124
CurrentTrain: epoch  6, batch     1 | loss: 2.4530890Losses:  2.26359224319458 0.7611714005470276
CurrentTrain: epoch  6, batch     2 | loss: 3.0247636Losses:  1.9155503511428833 0.36152714490890503
CurrentTrain: epoch  6, batch     3 | loss: 2.2770774Losses:  1.9514799118041992 0.555213451385498
CurrentTrain: epoch  6, batch     4 | loss: 2.5066934Losses:  1.7675223350524902 0.2971484661102295
CurrentTrain: epoch  6, batch     5 | loss: 2.0646708Losses:  1.797098159790039 0.061789777129888535
CurrentTrain: epoch  6, batch     6 | loss: 1.8588879Losses:  1.9165239334106445 0.5432496070861816
CurrentTrain: epoch  7, batch     0 | loss: 2.4597735Losses:  1.7623474597930908 0.5594905018806458
CurrentTrain: epoch  7, batch     1 | loss: 2.3218379Losses:  1.725462794303894 0.29583922028541565
CurrentTrain: epoch  7, batch     2 | loss: 2.0213020Losses:  1.8953120708465576 0.5293213129043579
CurrentTrain: epoch  7, batch     3 | loss: 2.4246335Losses:  2.0030887126922607 0.14482997357845306
CurrentTrain: epoch  7, batch     4 | loss: 2.1479187Losses:  2.13557767868042 0.6306269764900208
CurrentTrain: epoch  7, batch     5 | loss: 2.7662046Losses:  1.884873390197754 0.06555140018463135
CurrentTrain: epoch  7, batch     6 | loss: 1.9504248Losses:  1.8935402631759644 0.49370330572128296
CurrentTrain: epoch  8, batch     0 | loss: 2.3872435Losses:  1.8714401721954346 0.2588975727558136
CurrentTrain: epoch  8, batch     1 | loss: 2.1303377Losses:  1.6979973316192627 0.21122656762599945
CurrentTrain: epoch  8, batch     2 | loss: 1.9092239Losses:  2.262946128845215 0.43483656644821167
CurrentTrain: epoch  8, batch     3 | loss: 2.6977828Losses:  1.8274085521697998 0.3627176582813263
CurrentTrain: epoch  8, batch     4 | loss: 2.1901262Losses:  1.7511389255523682 0.4365444481372833
CurrentTrain: epoch  8, batch     5 | loss: 2.1876833Losses:  1.7405858039855957 0.06180574372410774
CurrentTrain: epoch  8, batch     6 | loss: 1.8023915Losses:  2.1928510665893555 0.347114622592926
CurrentTrain: epoch  9, batch     0 | loss: 2.5399656Losses:  1.814694881439209 0.2189520299434662
CurrentTrain: epoch  9, batch     1 | loss: 2.0336468Losses:  1.7242114543914795 0.25488561391830444
CurrentTrain: epoch  9, batch     2 | loss: 1.9790971Losses:  1.8229608535766602 0.41951656341552734
CurrentTrain: epoch  9, batch     3 | loss: 2.2424774Losses:  1.8104362487792969 0.5215039253234863
CurrentTrain: epoch  9, batch     4 | loss: 2.3319402Losses:  1.6931580305099487 0.338641881942749
CurrentTrain: epoch  9, batch     5 | loss: 2.0317998Losses:  1.6930596828460693 0.08819708228111267
CurrentTrain: epoch  9, batch     6 | loss: 1.7812568
Losses:  6.032514572143555 0.4455094635486603
MemoryTrain:  epoch  0, batch     0 | loss: 6.4780240Losses:  7.545987129211426 0.2825179994106293
MemoryTrain:  epoch  0, batch     1 | loss: 7.8285050Losses:  9.239852905273438 0.38248270750045776
MemoryTrain:  epoch  0, batch     2 | loss: 9.6223354Losses:  9.426080703735352 0.4188251495361328
MemoryTrain:  epoch  0, batch     3 | loss: 9.8449059Losses:  9.425790786743164 0.37737399339675903
MemoryTrain:  epoch  0, batch     4 | loss: 9.8031645Losses:  9.467025756835938 0.4771208167076111
MemoryTrain:  epoch  0, batch     5 | loss: 9.9441462Losses:  10.694783210754395 0.4473438262939453
MemoryTrain:  epoch  0, batch     6 | loss: 11.1421270Losses:  10.808747291564941 0.47184059023857117
MemoryTrain:  epoch  0, batch     7 | loss: 11.2805882Losses:  10.647530555725098 0.38554584980010986
MemoryTrain:  epoch  0, batch     8 | loss: 11.0330763Losses:  1.045238971710205 0.4489520788192749
MemoryTrain:  epoch  1, batch     0 | loss: 1.4941911Losses:  0.8608435392379761 0.4332502484321594
MemoryTrain:  epoch  1, batch     1 | loss: 1.2940938Losses:  1.2311252355575562 0.3462914824485779
MemoryTrain:  epoch  1, batch     2 | loss: 1.5774167Losses:  0.6535376906394958 0.33119165897369385
MemoryTrain:  epoch  1, batch     3 | loss: 0.9847293Losses:  0.6607903242111206 0.4369259774684906
MemoryTrain:  epoch  1, batch     4 | loss: 1.0977163Losses:  1.0029361248016357 0.34895390272140503
MemoryTrain:  epoch  1, batch     5 | loss: 1.3518901Losses:  0.5560687780380249 0.3232104778289795
MemoryTrain:  epoch  1, batch     6 | loss: 0.8792793Losses:  0.4633711278438568 0.4364050626754761
MemoryTrain:  epoch  1, batch     7 | loss: 0.8997762Losses:  1.2810701131820679 0.5488811135292053
MemoryTrain:  epoch  1, batch     8 | loss: 1.8299513Losses:  0.6762619018554688 0.3318741023540497
MemoryTrain:  epoch  2, batch     0 | loss: 1.0081360Losses:  0.5827100872993469 0.4227999448776245
MemoryTrain:  epoch  2, batch     1 | loss: 1.0055101Losses:  0.7263205051422119 0.419753760099411
MemoryTrain:  epoch  2, batch     2 | loss: 1.1460743Losses:  0.6536300182342529 0.41127949953079224
MemoryTrain:  epoch  2, batch     3 | loss: 1.0649095Losses:  0.9708825349807739 0.39212143421173096
MemoryTrain:  epoch  2, batch     4 | loss: 1.3630040Losses:  0.5320502519607544 0.48283785581588745
MemoryTrain:  epoch  2, batch     5 | loss: 1.0148880Losses:  0.5058208107948303 0.4591008424758911
MemoryTrain:  epoch  2, batch     6 | loss: 0.9649217Losses:  0.36150193214416504 0.2754073143005371
MemoryTrain:  epoch  2, batch     7 | loss: 0.6369092Losses:  0.4364964962005615 0.34747084975242615
MemoryTrain:  epoch  2, batch     8 | loss: 0.7839674Losses:  0.5424883961677551 0.26624107360839844
MemoryTrain:  epoch  3, batch     0 | loss: 0.8087295Losses:  0.3675239384174347 0.3118595778942108
MemoryTrain:  epoch  3, batch     1 | loss: 0.6793835Losses:  0.39521461725234985 0.38061317801475525
MemoryTrain:  epoch  3, batch     2 | loss: 0.7758278Losses:  0.39085641503334045 0.2983754873275757
MemoryTrain:  epoch  3, batch     3 | loss: 0.6892319Losses:  0.5412927269935608 0.4572646915912628
MemoryTrain:  epoch  3, batch     4 | loss: 0.9985574Losses:  0.4816233217716217 0.5210316181182861
MemoryTrain:  epoch  3, batch     5 | loss: 1.0026549Losses:  0.3175843060016632 0.23269376158714294
MemoryTrain:  epoch  3, batch     6 | loss: 0.5502781Losses:  0.7074887752532959 0.5255343914031982
MemoryTrain:  epoch  3, batch     7 | loss: 1.2330232Losses:  0.481492817401886 0.39689111709594727
MemoryTrain:  epoch  3, batch     8 | loss: 0.8783839Losses:  0.39054933190345764 0.2963365912437439
MemoryTrain:  epoch  4, batch     0 | loss: 0.6868860Losses:  0.4266960620880127 0.3626610338687897
MemoryTrain:  epoch  4, batch     1 | loss: 0.7893571Losses:  0.41150182485580444 0.3842950463294983
MemoryTrain:  epoch  4, batch     2 | loss: 0.7957969Losses:  0.4970308244228363 0.3249567747116089
MemoryTrain:  epoch  4, batch     3 | loss: 0.8219876Losses:  0.5697680711746216 0.4320547580718994
MemoryTrain:  epoch  4, batch     4 | loss: 1.0018228Losses:  0.5055087804794312 0.5148087739944458
MemoryTrain:  epoch  4, batch     5 | loss: 1.0203176Losses:  0.520016610622406 0.49053990840911865
MemoryTrain:  epoch  4, batch     6 | loss: 1.0105565Losses:  0.4112469255924225 0.3794209957122803
MemoryTrain:  epoch  4, batch     7 | loss: 0.7906679Losses:  0.3615054786205292 0.2108478546142578
MemoryTrain:  epoch  4, batch     8 | loss: 0.5723534Losses:  0.4201372563838959 0.4454452395439148
MemoryTrain:  epoch  5, batch     0 | loss: 0.8655825Losses:  0.4779653251171112 0.5148695707321167
MemoryTrain:  epoch  5, batch     1 | loss: 0.9928349Losses:  0.32064542174339294 0.19005893170833588
MemoryTrain:  epoch  5, batch     2 | loss: 0.5107043Losses:  0.43769901990890503 0.2940865755081177
MemoryTrain:  epoch  5, batch     3 | loss: 0.7317856Losses:  0.5696002244949341 0.48452407121658325
MemoryTrain:  epoch  5, batch     4 | loss: 1.0541244Losses:  0.3582366108894348 0.23262134194374084
MemoryTrain:  epoch  5, batch     5 | loss: 0.5908580Losses:  0.3866644501686096 0.31708401441574097
MemoryTrain:  epoch  5, batch     6 | loss: 0.7037485Losses:  0.4175911247730255 0.29645782709121704
MemoryTrain:  epoch  5, batch     7 | loss: 0.7140490Losses:  0.3820814788341522 0.2374938577413559
MemoryTrain:  epoch  5, batch     8 | loss: 0.6195753Losses:  0.49894195795059204 0.50631183385849
MemoryTrain:  epoch  6, batch     0 | loss: 1.0052538Losses:  0.29142001271247864 0.25976645946502686
MemoryTrain:  epoch  6, batch     1 | loss: 0.5511864Losses:  0.4862881898880005 0.510083794593811
MemoryTrain:  epoch  6, batch     2 | loss: 0.9963720Losses:  0.38856297731399536 0.30516690015792847
MemoryTrain:  epoch  6, batch     3 | loss: 0.6937299Losses:  0.3032480478286743 0.2178923636674881
MemoryTrain:  epoch  6, batch     4 | loss: 0.5211404Losses:  0.4294869303703308 0.39603525400161743
MemoryTrain:  epoch  6, batch     5 | loss: 0.8255222Losses:  0.42721590399742126 0.4248414635658264
MemoryTrain:  epoch  6, batch     6 | loss: 0.8520573Losses:  0.41538769006729126 0.2748933434486389
MemoryTrain:  epoch  6, batch     7 | loss: 0.6902810Losses:  0.5119099617004395 0.26207229495048523
MemoryTrain:  epoch  6, batch     8 | loss: 0.7739823Losses:  0.4289235472679138 0.41443347930908203
MemoryTrain:  epoch  7, batch     0 | loss: 0.8433570Losses:  0.49587568640708923 0.3200511336326599
MemoryTrain:  epoch  7, batch     1 | loss: 0.8159268Losses:  0.4324681758880615 0.5112279057502747
MemoryTrain:  epoch  7, batch     2 | loss: 0.9436961Losses:  0.37774115800857544 0.38403794169425964
MemoryTrain:  epoch  7, batch     3 | loss: 0.7617791Losses:  0.3424595594406128 0.2795773446559906
MemoryTrain:  epoch  7, batch     4 | loss: 0.6220369Losses:  0.34237295389175415 0.37894028425216675
MemoryTrain:  epoch  7, batch     5 | loss: 0.7213132Losses:  0.21942169964313507 0.27604973316192627
MemoryTrain:  epoch  7, batch     6 | loss: 0.4954714Losses:  0.4919525980949402 0.4430723488330841
MemoryTrain:  epoch  7, batch     7 | loss: 0.9350250Losses:  0.4402560591697693 0.3405481278896332
MemoryTrain:  epoch  7, batch     8 | loss: 0.7808042Losses:  0.38743025064468384 0.3071061968803406
MemoryTrain:  epoch  8, batch     0 | loss: 0.6945364Losses:  0.3962803781032562 0.42416563630104065
MemoryTrain:  epoch  8, batch     1 | loss: 0.8204460Losses:  0.30699533224105835 0.39308401942253113
MemoryTrain:  epoch  8, batch     2 | loss: 0.7000793Losses:  0.4379931092262268 0.4138907194137573
MemoryTrain:  epoch  8, batch     3 | loss: 0.8518838Losses:  0.3764116168022156 0.23683053255081177
MemoryTrain:  epoch  8, batch     4 | loss: 0.6132421Losses:  0.43800467252731323 0.37384241819381714
MemoryTrain:  epoch  8, batch     5 | loss: 0.8118471Losses:  0.38831448554992676 0.3380987048149109
MemoryTrain:  epoch  8, batch     6 | loss: 0.7264132Losses:  0.35128599405288696 0.3431675434112549
MemoryTrain:  epoch  8, batch     7 | loss: 0.6944535Losses:  0.3928802013397217 0.2938094735145569
MemoryTrain:  epoch  8, batch     8 | loss: 0.6866897Losses:  0.328549325466156 0.2547977566719055
MemoryTrain:  epoch  9, batch     0 | loss: 0.5833471Losses:  0.464252233505249 0.5028740167617798
MemoryTrain:  epoch  9, batch     1 | loss: 0.9671263Losses:  0.3704374432563782 0.29912933707237244
MemoryTrain:  epoch  9, batch     2 | loss: 0.6695668Losses:  0.35724136233329773 0.29314008355140686
MemoryTrain:  epoch  9, batch     3 | loss: 0.6503814Losses:  0.3500106930732727 0.28567206859588623
MemoryTrain:  epoch  9, batch     4 | loss: 0.6356828Losses:  0.28810980916023254 0.2864786684513092
MemoryTrain:  epoch  9, batch     5 | loss: 0.5745885Losses:  0.3649415969848633 0.384859561920166
MemoryTrain:  epoch  9, batch     6 | loss: 0.7498012Losses:  0.4316675066947937 0.3877089023590088
MemoryTrain:  epoch  9, batch     7 | loss: 0.8193764Losses:  0.31255313754081726 0.2538054585456848
MemoryTrain:  epoch  9, batch     8 | loss: 0.5663586
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 77.66%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 77.39%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.47%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.93%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 81.91%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.64%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 81.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.19%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 81.59%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 81.47%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 80.92%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.39%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 79.79%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 79.30%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.34%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 76.15%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 75.19%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 74.16%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 73.07%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 72.37%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 73.36%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 73.00%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 72.94%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.76%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 72.18%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 71.54%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 69.93%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 69.19%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 68.39%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 67.76%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 66.32%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 64.88%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 64.18%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 63.70%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.13%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 66.30%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 64.60%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 63.45%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 63.05%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 62.99%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 63.50%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 63.38%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 63.27%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 63.20%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 63.04%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 62.94%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 64.03%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 63.78%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 63.64%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 63.61%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 63.68%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 63.59%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 63.46%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 63.07%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 62.98%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 62.94%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 62.77%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 62.77%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 62.65%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 62.54%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 62.46%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 62.35%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 62.31%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 62.28%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 62.06%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 61.52%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 61.27%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 61.03%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 60.89%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 61.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 61.55%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 62.36%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 62.47%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 62.60%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 62.74%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 62.84%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 62.93%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 62.83%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 62.83%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 62.66%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 62.56%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 62.56%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 62.69%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 62.78%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 62.94%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 63.24%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 63.38%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 63.38%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 63.25%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 63.00%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 63.06%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 64.98%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 64.94%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 65.08%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 65.51%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.72%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 65.50%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 65.06%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 64.92%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 64.97%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 65.16%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.04%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 65.01%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 64.95%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 65.65%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 65.49%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 65.46%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 65.30%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 65.11%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.21%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 66.17%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 66.65%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 66.64%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 66.65%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 66.64%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 66.37%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.63%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 66.43%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 66.26%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 66.10%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 65.91%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 65.81%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 66.22%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 66.14%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.10%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 66.74%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 66.79%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 66.70%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 66.68%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 66.60%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  395 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 66.34%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  398 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 66.82%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 66.79%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  425 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.09%   
cur_acc:  ['0.9405', '0.6200', '0.6696', '0.7540', '0.7530', '0.6935', '0.7847']
his_acc:  ['0.9405', '0.7795', '0.7171', '0.7155', '0.6967', '0.6807', '0.6809']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 35  0  0  1  1 19  0  0  0  0  0  0  0 25
  0 34  0 23 31 36 26  0 22  0 20 12 17  0  0  0  5 29  0  0  0  2  0 37
 27  9 32 28  1  0  0 18  0 30  0 16  8 10  0  0 13 15 14  0  0  7  2  3
  0  0  6  0  0  0  4 11]
Losses:  6.7067551612854 0.9569696187973022
CurrentTrain: epoch  0, batch     0 | loss: 7.6637249Losses:  8.933159828186035 1.1293628215789795
CurrentTrain: epoch  0, batch     1 | loss: 10.0625229Losses:  8.572998046875 1.1474485397338867
CurrentTrain: epoch  0, batch     2 | loss: 9.7204466Losses:  9.395828247070312 0.9849754571914673
CurrentTrain: epoch  0, batch     3 | loss: 10.3808041Losses:  7.8600053787231445 0.9951267838478088
CurrentTrain: epoch  0, batch     4 | loss: 8.8551321Losses:  6.72976541519165 0.8544224500656128
CurrentTrain: epoch  0, batch     5 | loss: 7.5841880Losses:  3.699733257293701 0.3347111642360687
CurrentTrain: epoch  0, batch     6 | loss: 4.0344443Losses:  3.95676851272583 1.1437077522277832
CurrentTrain: epoch  1, batch     0 | loss: 5.1004763Losses:  4.2077436447143555 0.6684952974319458
CurrentTrain: epoch  1, batch     1 | loss: 4.8762388Losses:  4.253751754760742 1.0397075414657593
CurrentTrain: epoch  1, batch     2 | loss: 5.2934594Losses:  3.483783721923828 1.0583326816558838
CurrentTrain: epoch  1, batch     3 | loss: 4.5421162Losses:  2.9941086769104004 0.523922324180603
CurrentTrain: epoch  1, batch     4 | loss: 3.5180311Losses:  4.115484714508057 0.9253467321395874
CurrentTrain: epoch  1, batch     5 | loss: 5.0408316Losses:  3.0708160400390625 0.17702987790107727
CurrentTrain: epoch  1, batch     6 | loss: 3.2478459Losses:  2.794999599456787 1.0188677310943604
CurrentTrain: epoch  2, batch     0 | loss: 3.8138673Losses:  3.622335433959961 0.9030073881149292
CurrentTrain: epoch  2, batch     1 | loss: 4.5253429Losses:  3.5740256309509277 0.7474014759063721
CurrentTrain: epoch  2, batch     2 | loss: 4.3214273Losses:  3.0040934085845947 0.47736889123916626
CurrentTrain: epoch  2, batch     3 | loss: 3.4814622Losses:  3.666895627975464 0.9671250581741333
CurrentTrain: epoch  2, batch     4 | loss: 4.6340208Losses:  3.0631256103515625 0.9386383295059204
CurrentTrain: epoch  2, batch     5 | loss: 4.0017638Losses:  2.0536391735076904 0.14639359712600708
CurrentTrain: epoch  2, batch     6 | loss: 2.2000327Losses:  2.8018789291381836 0.7563040852546692
CurrentTrain: epoch  3, batch     0 | loss: 3.5581830Losses:  3.2761144638061523 0.771199107170105
CurrentTrain: epoch  3, batch     1 | loss: 4.0473137Losses:  2.6487131118774414 0.8153948783874512
CurrentTrain: epoch  3, batch     2 | loss: 3.4641080Losses:  3.4014787673950195 0.5671298503875732
CurrentTrain: epoch  3, batch     3 | loss: 3.9686086Losses:  2.93121600151062 0.718684196472168
CurrentTrain: epoch  3, batch     4 | loss: 3.6499002Losses:  2.544443130493164 0.7370189428329468
CurrentTrain: epoch  3, batch     5 | loss: 3.2814622Losses:  2.114720344543457 0.4049230217933655
CurrentTrain: epoch  3, batch     6 | loss: 2.5196433Losses:  2.90299129486084 0.7207271456718445
CurrentTrain: epoch  4, batch     0 | loss: 3.6237185Losses:  2.6058976650238037 0.768272340297699
CurrentTrain: epoch  4, batch     1 | loss: 3.3741701Losses:  2.894155979156494 0.7998737096786499
CurrentTrain: epoch  4, batch     2 | loss: 3.6940298Losses:  2.220475196838379 0.5265978574752808
CurrentTrain: epoch  4, batch     3 | loss: 2.7470732Losses:  2.4054667949676514 0.7830772399902344
CurrentTrain: epoch  4, batch     4 | loss: 3.1885440Losses:  2.1587462425231934 0.5236259698867798
CurrentTrain: epoch  4, batch     5 | loss: 2.6823721Losses:  2.3704142570495605 0.21192921698093414
CurrentTrain: epoch  4, batch     6 | loss: 2.5823436Losses:  2.1892502307891846 0.5468871593475342
CurrentTrain: epoch  5, batch     0 | loss: 2.7361374Losses:  1.9091277122497559 0.4493821859359741
CurrentTrain: epoch  5, batch     1 | loss: 2.3585100Losses:  2.55318546295166 0.5471372008323669
CurrentTrain: epoch  5, batch     2 | loss: 3.1003227Losses:  2.4838242530822754 0.6402013301849365
CurrentTrain: epoch  5, batch     3 | loss: 3.1240256Losses:  2.2721500396728516 0.5092648267745972
CurrentTrain: epoch  5, batch     4 | loss: 2.7814150Losses:  2.336162567138672 0.6122442483901978
CurrentTrain: epoch  5, batch     5 | loss: 2.9484067Losses:  2.2297778129577637 0.2450939416885376
CurrentTrain: epoch  5, batch     6 | loss: 2.4748716Losses:  2.127470016479492 0.3524879515171051
CurrentTrain: epoch  6, batch     0 | loss: 2.4799581Losses:  1.9527645111083984 0.4106388986110687
CurrentTrain: epoch  6, batch     1 | loss: 2.3634033Losses:  2.0956664085388184 0.426352322101593
CurrentTrain: epoch  6, batch     2 | loss: 2.5220187Losses:  2.4877240657806396 0.7129676342010498
CurrentTrain: epoch  6, batch     3 | loss: 3.2006917Losses:  1.8351001739501953 0.3945249021053314
CurrentTrain: epoch  6, batch     4 | loss: 2.2296250Losses:  2.2275147438049316 0.5886919498443604
CurrentTrain: epoch  6, batch     5 | loss: 2.8162067Losses:  3.0922043323516846 0.5811487436294556
CurrentTrain: epoch  6, batch     6 | loss: 3.6733532Losses:  1.775442361831665 0.2577034533023834
CurrentTrain: epoch  7, batch     0 | loss: 2.0331459Losses:  2.2852048873901367 0.489395409822464
CurrentTrain: epoch  7, batch     1 | loss: 2.7746003Losses:  1.9937713146209717 0.629658043384552
CurrentTrain: epoch  7, batch     2 | loss: 2.6234293Losses:  2.1613495349884033 0.660450279712677
CurrentTrain: epoch  7, batch     3 | loss: 2.8217998Losses:  2.0543107986450195 0.5827505588531494
CurrentTrain: epoch  7, batch     4 | loss: 2.6370614Losses:  1.9358140230178833 0.44114193320274353
CurrentTrain: epoch  7, batch     5 | loss: 2.3769560Losses:  1.9137117862701416 0.039616264402866364
CurrentTrain: epoch  7, batch     6 | loss: 1.9533280Losses:  1.912820816040039 0.4914192855358124
CurrentTrain: epoch  8, batch     0 | loss: 2.4042401Losses:  1.9405436515808105 0.42652627825737
CurrentTrain: epoch  8, batch     1 | loss: 2.3670700Losses:  1.8080034255981445 0.324553906917572
CurrentTrain: epoch  8, batch     2 | loss: 2.1325574Losses:  2.0607833862304688 0.6468847990036011
CurrentTrain: epoch  8, batch     3 | loss: 2.7076683Losses:  1.8378127813339233 0.26483869552612305
CurrentTrain: epoch  8, batch     4 | loss: 2.1026516Losses:  1.9998124837875366 0.31487295031547546
CurrentTrain: epoch  8, batch     5 | loss: 2.3146853Losses:  1.6752707958221436 0.025898592546582222
CurrentTrain: epoch  8, batch     6 | loss: 1.7011694Losses:  1.9342821836471558 0.4004591703414917
CurrentTrain: epoch  9, batch     0 | loss: 2.3347414Losses:  1.8454077243804932 0.37151503562927246
CurrentTrain: epoch  9, batch     1 | loss: 2.2169228Losses:  1.94755220413208 0.3624340295791626
CurrentTrain: epoch  9, batch     2 | loss: 2.3099861Losses:  1.768056869506836 0.30120015144348145
CurrentTrain: epoch  9, batch     3 | loss: 2.0692570Losses:  1.7497490644454956 0.2560223937034607
CurrentTrain: epoch  9, batch     4 | loss: 2.0057714Losses:  1.8968887329101562 0.5651450157165527
CurrentTrain: epoch  9, batch     5 | loss: 2.4620337Losses:  1.8242136240005493 0.08834642171859741
CurrentTrain: epoch  9, batch     6 | loss: 1.9125600
Losses:  6.091303825378418 0.3399032950401306
MemoryTrain:  epoch  0, batch     0 | loss: 6.4312072Losses:  7.763828277587891 0.46243518590927124
MemoryTrain:  epoch  0, batch     1 | loss: 8.2262630Losses:  8.615018844604492 0.3677358627319336
MemoryTrain:  epoch  0, batch     2 | loss: 8.9827547Losses:  9.558115005493164 0.476359099149704
MemoryTrain:  epoch  0, batch     3 | loss: 10.0344744Losses:  10.229667663574219 0.31152260303497314
MemoryTrain:  epoch  0, batch     4 | loss: 10.5411901Losses:  9.99059009552002 0.4032733142375946
MemoryTrain:  epoch  0, batch     5 | loss: 10.3938637Losses:  10.135406494140625 0.38938379287719727
MemoryTrain:  epoch  0, batch     6 | loss: 10.5247898Losses:  10.642916679382324 0.38717517256736755
MemoryTrain:  epoch  0, batch     7 | loss: 11.0300922Losses:  10.178229331970215 0.5246952772140503
MemoryTrain:  epoch  0, batch     8 | loss: 10.7029247Losses:  11.283737182617188 0.28780609369277954
MemoryTrain:  epoch  0, batch     9 | loss: 11.5715437Losses:  1.0908091068267822 0.43237239122390747
MemoryTrain:  epoch  1, batch     0 | loss: 1.5231814Losses:  0.7339130640029907 0.37866702675819397
MemoryTrain:  epoch  1, batch     1 | loss: 1.1125801Losses:  0.5176602602005005 0.39375123381614685
MemoryTrain:  epoch  1, batch     2 | loss: 0.9114115Losses:  1.5200741291046143 0.41884151101112366
MemoryTrain:  epoch  1, batch     3 | loss: 1.9389156Losses:  0.8388881683349609 0.27539747953414917
MemoryTrain:  epoch  1, batch     4 | loss: 1.1142857Losses:  0.7894214391708374 0.4116051495075226
MemoryTrain:  epoch  1, batch     5 | loss: 1.2010266Losses:  1.102076768875122 0.32422614097595215
MemoryTrain:  epoch  1, batch     6 | loss: 1.4263029Losses:  0.7448940277099609 0.35765498876571655
MemoryTrain:  epoch  1, batch     7 | loss: 1.1025491Losses:  0.7910083532333374 0.26825013756752014
MemoryTrain:  epoch  1, batch     8 | loss: 1.0592585Losses:  1.3260818719863892 0.5663697719573975
MemoryTrain:  epoch  1, batch     9 | loss: 1.8924516Losses:  0.9967892169952393 0.3406342566013336
MemoryTrain:  epoch  2, batch     0 | loss: 1.3374234Losses:  1.0393810272216797 0.42625829577445984
MemoryTrain:  epoch  2, batch     1 | loss: 1.4656394Losses:  0.5629733204841614 0.35223260521888733
MemoryTrain:  epoch  2, batch     2 | loss: 0.9152060Losses:  0.9257290959358215 0.35526910424232483
MemoryTrain:  epoch  2, batch     3 | loss: 1.2809982Losses:  0.5006555318832397 0.37906405329704285
MemoryTrain:  epoch  2, batch     4 | loss: 0.8797196Losses:  0.5560096502304077 0.38146790862083435
MemoryTrain:  epoch  2, batch     5 | loss: 0.9374776Losses:  0.5360643267631531 0.5078965425491333
MemoryTrain:  epoch  2, batch     6 | loss: 1.0439608Losses:  0.806268572807312 0.4484805464744568
MemoryTrain:  epoch  2, batch     7 | loss: 1.2547491Losses:  0.4930260181427002 0.34286919236183167
MemoryTrain:  epoch  2, batch     8 | loss: 0.8358952Losses:  0.5797711610794067 0.34499233961105347
MemoryTrain:  epoch  2, batch     9 | loss: 0.9247635Losses:  0.5120732188224792 0.26135045289993286
MemoryTrain:  epoch  3, batch     0 | loss: 0.7734237Losses:  0.3239349126815796 0.26428478956222534
MemoryTrain:  epoch  3, batch     1 | loss: 0.5882197Losses:  0.5536155700683594 0.4261842370033264
MemoryTrain:  epoch  3, batch     2 | loss: 0.9797998Losses:  0.5188798308372498 0.39531683921813965
MemoryTrain:  epoch  3, batch     3 | loss: 0.9141967Losses:  0.457847535610199 0.3935948312282562
MemoryTrain:  epoch  3, batch     4 | loss: 0.8514423Losses:  0.8390686511993408 0.37308621406555176
MemoryTrain:  epoch  3, batch     5 | loss: 1.2121549Losses:  0.47781166434288025 0.3886696994304657
MemoryTrain:  epoch  3, batch     6 | loss: 0.8664814Losses:  0.6687751412391663 0.3418036997318268
MemoryTrain:  epoch  3, batch     7 | loss: 1.0105789Losses:  0.503714919090271 0.2750532925128937
MemoryTrain:  epoch  3, batch     8 | loss: 0.7787682Losses:  0.5226844549179077 0.41702935099601746
MemoryTrain:  epoch  3, batch     9 | loss: 0.9397138Losses:  0.4359690248966217 0.3400781750679016
MemoryTrain:  epoch  4, batch     0 | loss: 0.7760472Losses:  0.6080326437950134 0.3742887079715729
MemoryTrain:  epoch  4, batch     1 | loss: 0.9823214Losses:  0.6130456924438477 0.41143208742141724
MemoryTrain:  epoch  4, batch     2 | loss: 1.0244777Losses:  0.5049306750297546 0.457768976688385
MemoryTrain:  epoch  4, batch     3 | loss: 0.9626997Losses:  0.32718223333358765 0.42841434478759766
MemoryTrain:  epoch  4, batch     4 | loss: 0.7555966Losses:  0.6722472906112671 0.31920427083969116
MemoryTrain:  epoch  4, batch     5 | loss: 0.9914516Losses:  0.44604581594467163 0.40323173999786377
MemoryTrain:  epoch  4, batch     6 | loss: 0.8492776Losses:  0.4123838543891907 0.3593984842300415
MemoryTrain:  epoch  4, batch     7 | loss: 0.7717823Losses:  0.4166620373725891 0.3244849443435669
MemoryTrain:  epoch  4, batch     8 | loss: 0.7411470Losses:  0.4570160508155823 0.2717558741569519
MemoryTrain:  epoch  4, batch     9 | loss: 0.7287719Losses:  0.36761999130249023 0.27841007709503174
MemoryTrain:  epoch  5, batch     0 | loss: 0.6460301Losses:  0.42848560214042664 0.3960086703300476
MemoryTrain:  epoch  5, batch     1 | loss: 0.8244942Losses:  0.48231643438339233 0.3758874535560608
MemoryTrain:  epoch  5, batch     2 | loss: 0.8582039Losses:  0.5671846270561218 0.41334760189056396
MemoryTrain:  epoch  5, batch     3 | loss: 0.9805322Losses:  0.42331862449645996 0.3617674708366394
MemoryTrain:  epoch  5, batch     4 | loss: 0.7850861Losses:  0.5579637289047241 0.38521820306777954
MemoryTrain:  epoch  5, batch     5 | loss: 0.9431819Losses:  0.4084232747554779 0.28305819630622864
MemoryTrain:  epoch  5, batch     6 | loss: 0.6914815Losses:  0.46411624550819397 0.4239017367362976
MemoryTrain:  epoch  5, batch     7 | loss: 0.8880180Losses:  0.3984440863132477 0.37799808382987976
MemoryTrain:  epoch  5, batch     8 | loss: 0.7764422Losses:  0.47888511419296265 0.29957592487335205
MemoryTrain:  epoch  5, batch     9 | loss: 0.7784610Losses:  0.4946366846561432 0.3412907123565674
MemoryTrain:  epoch  6, batch     0 | loss: 0.8359274Losses:  0.39231041073799133 0.2401464581489563
MemoryTrain:  epoch  6, batch     1 | loss: 0.6324569Losses:  0.517624020576477 0.4385683536529541
MemoryTrain:  epoch  6, batch     2 | loss: 0.9561924Losses:  0.5268138647079468 0.34194353222846985
MemoryTrain:  epoch  6, batch     3 | loss: 0.8687574Losses:  0.3494341969490051 0.24968765676021576
MemoryTrain:  epoch  6, batch     4 | loss: 0.5991219Losses:  0.4726828336715698 0.5406578779220581
MemoryTrain:  epoch  6, batch     5 | loss: 1.0133407Losses:  0.43414145708084106 0.3712252080440521
MemoryTrain:  epoch  6, batch     6 | loss: 0.8053666Losses:  0.4040096402168274 0.40261924266815186
MemoryTrain:  epoch  6, batch     7 | loss: 0.8066289Losses:  0.4295593500137329 0.3581998646259308
MemoryTrain:  epoch  6, batch     8 | loss: 0.7877592Losses:  0.36509615182876587 0.2861866354942322
MemoryTrain:  epoch  6, batch     9 | loss: 0.6512828Losses:  0.4070473909378052 0.3229545056819916
MemoryTrain:  epoch  7, batch     0 | loss: 0.7300019Losses:  0.5346704721450806 0.6118101477622986
MemoryTrain:  epoch  7, batch     1 | loss: 1.1464806Losses:  0.5205135345458984 0.4525997042655945
MemoryTrain:  epoch  7, batch     2 | loss: 0.9731132Losses:  0.4275808334350586 0.30633169412612915
MemoryTrain:  epoch  7, batch     3 | loss: 0.7339125Losses:  0.44116318225860596 0.3259243369102478
MemoryTrain:  epoch  7, batch     4 | loss: 0.7670875Losses:  0.4010865092277527 0.34517747163772583
MemoryTrain:  epoch  7, batch     5 | loss: 0.7462640Losses:  0.5591405034065247 0.3063102960586548
MemoryTrain:  epoch  7, batch     6 | loss: 0.8654508Losses:  0.3896975517272949 0.3151693046092987
MemoryTrain:  epoch  7, batch     7 | loss: 0.7048669Losses:  0.4217059016227722 0.2874870300292969
MemoryTrain:  epoch  7, batch     8 | loss: 0.7091929Losses:  0.34997472167015076 0.29923751950263977
MemoryTrain:  epoch  7, batch     9 | loss: 0.6492122Losses:  0.2932892143726349 0.16948257386684418
MemoryTrain:  epoch  8, batch     0 | loss: 0.4627718Losses:  0.509390652179718 0.43247973918914795
MemoryTrain:  epoch  8, batch     1 | loss: 0.9418704Losses:  0.4731636941432953 0.3677496612071991
MemoryTrain:  epoch  8, batch     2 | loss: 0.8409134Losses:  0.33679670095443726 0.2359720915555954
MemoryTrain:  epoch  8, batch     3 | loss: 0.5727688Losses:  0.4759255349636078 0.5029958486557007
MemoryTrain:  epoch  8, batch     4 | loss: 0.9789214Losses:  0.38972872495651245 0.35985231399536133
MemoryTrain:  epoch  8, batch     5 | loss: 0.7495810Losses:  0.38030198216438293 0.3011927604675293
MemoryTrain:  epoch  8, batch     6 | loss: 0.6814947Losses:  0.44095808267593384 0.4296976327896118
MemoryTrain:  epoch  8, batch     7 | loss: 0.8706557Losses:  0.39400187134742737 0.22755908966064453
MemoryTrain:  epoch  8, batch     8 | loss: 0.6215609Losses:  0.4776759743690491 0.3720182180404663
MemoryTrain:  epoch  8, batch     9 | loss: 0.8496942Losses:  0.38668614625930786 0.40110716223716736
MemoryTrain:  epoch  9, batch     0 | loss: 0.7877933Losses:  0.3804074227809906 0.42566031217575073
MemoryTrain:  epoch  9, batch     1 | loss: 0.8060677Losses:  0.389437735080719 0.3330743908882141
MemoryTrain:  epoch  9, batch     2 | loss: 0.7225121Losses:  0.4789252281188965 0.39426013827323914
MemoryTrain:  epoch  9, batch     3 | loss: 0.8731854Losses:  0.3192967176437378 0.24423176050186157
MemoryTrain:  epoch  9, batch     4 | loss: 0.5635285Losses:  0.41350889205932617 0.3619398772716522
MemoryTrain:  epoch  9, batch     5 | loss: 0.7754488Losses:  0.37264448404312134 0.21746860444545746
MemoryTrain:  epoch  9, batch     6 | loss: 0.5901131Losses:  0.36721742153167725 0.3020969033241272
MemoryTrain:  epoch  9, batch     7 | loss: 0.6693143Losses:  0.3056391179561615 0.2951952815055847
MemoryTrain:  epoch  9, batch     8 | loss: 0.6008344Losses:  0.43314358592033386 0.4695550799369812
MemoryTrain:  epoch  9, batch     9 | loss: 0.9026986
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.30%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 78.24%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 76.49%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 76.47%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 79.92%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 78.45%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 75.51%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 73.81%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 72.66%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 71.54%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 70.55%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 69.59%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 68.57%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 67.95%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 68.05%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 68.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 68.44%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 67.91%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 67.24%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 65.74%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 65.04%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 64.44%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 63.85%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 63.13%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 61.81%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 61.14%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 60.48%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 60.04%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 60.26%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 60.70%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 61.10%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 61.30%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 61.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 62.44%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 62.68%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 62.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 62.97%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 62.38%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 61.87%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 61.31%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 60.75%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 60.21%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 59.90%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 59.92%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 60.05%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 60.29%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 60.36%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 60.49%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 60.56%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 60.47%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 60.43%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 60.35%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 60.37%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 60.48%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 60.50%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 60.22%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 59.89%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 59.62%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 59.30%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 59.28%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 58.92%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 59.09%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 59.40%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 59.61%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 59.81%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 60.06%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 60.26%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 60.21%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 60.04%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 60.02%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 60.04%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 59.92%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 59.98%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 60.10%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 60.03%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 60.14%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 60.11%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 59.93%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 59.95%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 59.80%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 59.74%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 59.64%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 59.62%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 59.47%   [EVAL] batch:  157 | acc: 31.25%,  total acc: 59.30%   [EVAL] batch:  158 | acc: 25.00%,  total acc: 59.08%   [EVAL] batch:  159 | acc: 25.00%,  total acc: 58.87%   [EVAL] batch:  160 | acc: 37.50%,  total acc: 58.73%   [EVAL] batch:  161 | acc: 18.75%,  total acc: 58.49%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 58.40%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 58.35%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 58.37%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 58.32%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 58.27%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 58.32%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 58.13%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 57.89%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 57.63%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 57.37%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 57.15%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 57.04%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 57.28%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 57.52%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 57.76%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 58.23%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 58.46%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 58.59%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 58.50%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 58.49%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 58.48%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 58.43%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 58.49%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 58.41%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 58.27%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 58.19%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 58.08%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 57.94%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 57.80%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 57.73%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 57.88%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 57.91%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 57.93%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 58.05%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 58.07%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 58.22%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 58.24%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 58.32%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 58.41%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 58.55%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 58.60%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 58.74%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 58.76%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 58.71%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 58.58%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 58.57%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 58.44%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 58.40%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 58.48%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 58.67%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 58.87%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 59.00%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 59.50%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 59.69%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 59.87%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 60.41%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 60.58%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 60.65%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 60.74%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 60.72%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 60.78%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 60.92%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 61.01%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 60.76%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 60.66%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 60.53%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 60.51%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 60.44%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 60.50%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 60.76%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 60.87%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 60.98%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 61.12%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 60.89%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 60.67%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 60.50%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 60.28%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 60.07%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 59.88%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 59.96%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 60.02%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 60.08%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 60.14%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 60.17%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 60.30%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 60.26%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 60.30%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 60.33%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 60.29%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 60.23%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 60.23%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 60.31%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 60.30%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 60.31%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 60.22%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 60.21%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 60.17%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 60.39%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 60.54%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 60.68%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 60.95%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 61.14%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 61.10%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 61.02%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 61.00%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 61.05%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 61.10%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 61.08%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 61.06%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 60.89%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 60.79%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 60.62%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 60.43%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 60.35%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 60.52%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 60.57%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 60.64%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 60.63%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 60.55%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 60.52%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 60.44%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 60.37%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 60.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 60.73%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 60.86%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 60.98%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 61.22%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 61.30%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 61.39%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 61.49%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 61.58%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 61.80%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 61.80%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 61.77%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 61.81%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 61.77%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 61.79%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 61.85%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 62.17%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 62.48%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 62.38%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 62.27%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 62.12%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 61.99%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 61.86%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 61.73%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 61.77%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 61.88%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 61.99%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 62.09%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 61.91%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 61.77%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 61.60%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 61.44%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 61.34%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 61.45%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 61.64%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 61.75%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 61.67%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 61.55%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 61.45%   [EVAL] batch:  356 | acc: 31.25%,  total acc: 61.36%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 61.30%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 61.19%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 61.15%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 61.10%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 61.02%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 60.98%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 60.89%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 60.92%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 60.89%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 60.87%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 60.96%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 61.05%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 61.59%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 61.64%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 61.68%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 61.96%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 62.05%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 62.02%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 62.03%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 61.97%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 61.93%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 61.86%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 61.80%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 61.79%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 61.68%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 61.59%   [EVAL] batch:  397 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 61.48%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 61.38%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 61.47%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 61.66%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 62.06%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.09%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 62.13%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 62.15%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 62.54%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  425 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 63.17%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 63.22%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 63.30%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 63.68%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 63.64%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 63.59%   [EVAL] batch:  440 | acc: 31.25%,  total acc: 63.52%   [EVAL] batch:  441 | acc: 18.75%,  total acc: 63.42%   [EVAL] batch:  442 | acc: 37.50%,  total acc: 63.36%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.47%   [EVAL] batch:  446 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:  450 | acc: 37.50%,  total acc: 63.66%   [EVAL] batch:  451 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:  452 | acc: 43.75%,  total acc: 63.52%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 63.49%   [EVAL] batch:  455 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 63.51%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 63.74%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  464 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  466 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  471 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  479 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  482 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:  483 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  485 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 65.22%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  489 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 65.35%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:  494 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 65.34%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 65.38%   
cur_acc:  ['0.9405', '0.6200', '0.6696', '0.7540', '0.7530', '0.6935', '0.7847', '0.7649']
his_acc:  ['0.9405', '0.7795', '0.7171', '0.7155', '0.6967', '0.6807', '0.6809', '0.6538']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.611526489257812 1.4018974304199219
CurrentTrain: epoch  0, batch     0 | loss: 13.0134239Losses:  12.736392974853516 1.898935317993164
CurrentTrain: epoch  0, batch     1 | loss: 14.6353283Losses:  13.29405403137207 1.8016258478164673
CurrentTrain: epoch  0, batch     2 | loss: 15.0956802Losses:  13.5064697265625 1.6467769145965576
CurrentTrain: epoch  0, batch     3 | loss: 15.1532469Losses:  13.123817443847656 1.6570401191711426
CurrentTrain: epoch  0, batch     4 | loss: 14.7808571Losses:  13.368227005004883 1.1797125339508057
CurrentTrain: epoch  0, batch     5 | loss: 14.5479393Losses:  12.87125015258789 1.3598929643630981
CurrentTrain: epoch  0, batch     6 | loss: 14.2311430Losses:  13.732901573181152 1.2493596076965332
CurrentTrain: epoch  0, batch     7 | loss: 14.9822617Losses:  13.050355911254883 1.366018533706665
CurrentTrain: epoch  0, batch     8 | loss: 14.4163742Losses:  12.96142578125 1.360562801361084
CurrentTrain: epoch  0, batch     9 | loss: 14.3219891Losses:  13.633889198303223 1.008002519607544
CurrentTrain: epoch  0, batch    10 | loss: 14.6418915Losses:  12.854387283325195 1.6202059984207153
CurrentTrain: epoch  0, batch    11 | loss: 14.4745932Losses:  12.383485794067383 1.455075979232788
CurrentTrain: epoch  0, batch    12 | loss: 13.8385620Losses:  13.090797424316406 1.427721619606018
CurrentTrain: epoch  0, batch    13 | loss: 14.5185194Losses:  12.284133911132812 0.9656612873077393
CurrentTrain: epoch  0, batch    14 | loss: 13.2497950Losses:  12.24570369720459 1.462975025177002
CurrentTrain: epoch  0, batch    15 | loss: 13.7086792Losses:  12.194876670837402 0.7304881811141968
CurrentTrain: epoch  0, batch    16 | loss: 12.9253645Losses:  11.706123352050781 1.2906111478805542
CurrentTrain: epoch  0, batch    17 | loss: 12.9967346Losses:  12.227349281311035 0.9410712122917175
CurrentTrain: epoch  0, batch    18 | loss: 13.1684208Losses:  11.871274948120117 1.7653861045837402
CurrentTrain: epoch  0, batch    19 | loss: 13.6366615Losses:  12.084441184997559 1.3603057861328125
CurrentTrain: epoch  0, batch    20 | loss: 13.4447470Losses:  11.874343872070312 1.7184572219848633
CurrentTrain: epoch  0, batch    21 | loss: 13.5928011Losses:  12.194429397583008 1.842572569847107
CurrentTrain: epoch  0, batch    22 | loss: 14.0370016Losses:  11.406850814819336 1.5273339748382568
CurrentTrain: epoch  0, batch    23 | loss: 12.9341850Losses:  12.045400619506836 1.8311262130737305
CurrentTrain: epoch  0, batch    24 | loss: 13.8765268Losses:  11.807229995727539 1.2926435470581055
CurrentTrain: epoch  0, batch    25 | loss: 13.0998735Losses:  11.638334274291992 1.0695037841796875
CurrentTrain: epoch  0, batch    26 | loss: 12.7078381Losses:  11.424449920654297 1.4072871208190918
CurrentTrain: epoch  0, batch    27 | loss: 12.8317375Losses:  11.79085922241211 0.6578162908554077
CurrentTrain: epoch  0, batch    28 | loss: 12.4486752Losses:  11.217123985290527 0.9135874509811401
CurrentTrain: epoch  0, batch    29 | loss: 12.1307116Losses:  11.593689918518066 1.253960371017456
CurrentTrain: epoch  0, batch    30 | loss: 12.8476505Losses:  11.208694458007812 0.7561951875686646
CurrentTrain: epoch  0, batch    31 | loss: 11.9648895Losses:  11.463834762573242 0.9250843524932861
CurrentTrain: epoch  0, batch    32 | loss: 12.3889189Losses:  11.197608947753906 1.1951090097427368
CurrentTrain: epoch  0, batch    33 | loss: 12.3927183Losses:  11.048349380493164 1.3024709224700928
CurrentTrain: epoch  0, batch    34 | loss: 12.3508205Losses:  10.943920135498047 1.1861592531204224
CurrentTrain: epoch  0, batch    35 | loss: 12.1300793Losses:  11.345993995666504 1.1634222269058228
CurrentTrain: epoch  0, batch    36 | loss: 12.5094166Losses:  11.122394561767578 1.1699044704437256
CurrentTrain: epoch  0, batch    37 | loss: 12.2922993Losses:  11.008764266967773 1.4246591329574585
CurrentTrain: epoch  0, batch    38 | loss: 12.4334230Losses:  11.565014839172363 0.7952341437339783
CurrentTrain: epoch  0, batch    39 | loss: 12.3602486Losses:  11.095235824584961 0.8665881156921387
CurrentTrain: epoch  0, batch    40 | loss: 11.9618244Losses:  10.836329460144043 1.104567289352417
CurrentTrain: epoch  0, batch    41 | loss: 11.9408970Losses:  10.802370071411133 1.033937692642212
CurrentTrain: epoch  0, batch    42 | loss: 11.8363075Losses:  10.741006851196289 1.13075852394104
CurrentTrain: epoch  0, batch    43 | loss: 11.8717651Losses:  10.603687286376953 1.1741418838500977
CurrentTrain: epoch  0, batch    44 | loss: 11.7778292Losses:  10.702524185180664 1.012333869934082
CurrentTrain: epoch  0, batch    45 | loss: 11.7148581Losses:  10.469715118408203 0.8306048512458801
CurrentTrain: epoch  0, batch    46 | loss: 11.3003197Losses:  10.30095386505127 1.1490849256515503
CurrentTrain: epoch  0, batch    47 | loss: 11.4500389Losses:  10.42619514465332 0.9166786074638367
CurrentTrain: epoch  0, batch    48 | loss: 11.3428736Losses:  10.135843276977539 0.990062415599823
CurrentTrain: epoch  0, batch    49 | loss: 11.1259060Losses:  10.027233123779297 0.9422152042388916
CurrentTrain: epoch  0, batch    50 | loss: 10.9694481Losses:  10.145005226135254 0.8959430456161499
CurrentTrain: epoch  0, batch    51 | loss: 11.0409479Losses:  10.202670097351074 1.0961840152740479
CurrentTrain: epoch  0, batch    52 | loss: 11.2988539Losses:  10.25287914276123 1.0721206665039062
CurrentTrain: epoch  0, batch    53 | loss: 11.3249998Losses:  10.69605827331543 0.7348654270172119
CurrentTrain: epoch  0, batch    54 | loss: 11.4309235Losses:  9.985544204711914 0.7187513113021851
CurrentTrain: epoch  0, batch    55 | loss: 10.7042952Losses:  9.97191047668457 0.8988389372825623
CurrentTrain: epoch  0, batch    56 | loss: 10.8707495Losses:  10.076807975769043 1.1167055368423462
CurrentTrain: epoch  0, batch    57 | loss: 11.1935139Losses:  9.362014770507812 1.2548683881759644
CurrentTrain: epoch  0, batch    58 | loss: 10.6168833Losses:  9.403985977172852 0.7886393070220947
CurrentTrain: epoch  0, batch    59 | loss: 10.1926250Losses:  9.672201156616211 0.9194397926330566
CurrentTrain: epoch  0, batch    60 | loss: 10.5916405Losses:  9.435951232910156 0.97588050365448
CurrentTrain: epoch  0, batch    61 | loss: 10.4118319Losses:  9.617011070251465 0.35127124190330505
CurrentTrain: epoch  0, batch    62 | loss: 9.9682827Losses:  9.362253189086914 0.7515650987625122
CurrentTrain: epoch  0, batch    63 | loss: 10.1138182Losses:  9.365974426269531 1.0140001773834229
CurrentTrain: epoch  0, batch    64 | loss: 10.3799744Losses:  9.242561340332031 0.5415664911270142
CurrentTrain: epoch  0, batch    65 | loss: 9.7841282Losses:  9.794923782348633 0.8404620885848999
CurrentTrain: epoch  0, batch    66 | loss: 10.6353855Losses:  9.076727867126465 0.4536023437976837
CurrentTrain: epoch  0, batch    67 | loss: 9.5303307Losses:  9.110381126403809 0.8721416592597961
CurrentTrain: epoch  0, batch    68 | loss: 9.9825230Losses:  8.817727088928223 0.803577184677124
CurrentTrain: epoch  0, batch    69 | loss: 9.6213045Losses:  9.215033531188965 0.8842127323150635
CurrentTrain: epoch  0, batch    70 | loss: 10.0992460Losses:  8.734432220458984 0.491703599691391
CurrentTrain: epoch  0, batch    71 | loss: 9.2261362Losses:  8.656572341918945 0.7449831366539001
CurrentTrain: epoch  0, batch    72 | loss: 9.4015551Losses:  8.748468399047852 0.9308358430862427
CurrentTrain: epoch  0, batch    73 | loss: 9.6793041Losses:  8.786876678466797 0.6277879476547241
CurrentTrain: epoch  0, batch    74 | loss: 9.4146643Losses:  8.994365692138672 0.9571340680122375
CurrentTrain: epoch  0, batch    75 | loss: 9.9514999Losses:  8.596092224121094 0.7831195592880249
CurrentTrain: epoch  0, batch    76 | loss: 9.3792114Losses:  8.590367317199707 0.5758386850357056
CurrentTrain: epoch  0, batch    77 | loss: 9.1662064Losses:  8.988286018371582 1.0888935327529907
CurrentTrain: epoch  0, batch    78 | loss: 10.0771799Losses:  8.674484252929688 0.590246319770813
CurrentTrain: epoch  0, batch    79 | loss: 9.2647305Losses:  8.983926773071289 0.5468693971633911
CurrentTrain: epoch  0, batch    80 | loss: 9.5307961Losses:  8.437578201293945 0.4517630934715271
CurrentTrain: epoch  0, batch    81 | loss: 8.8893414Losses:  8.199931144714355 0.8254985213279724
CurrentTrain: epoch  0, batch    82 | loss: 9.0254297Losses:  8.167497634887695 0.6080222129821777
CurrentTrain: epoch  0, batch    83 | loss: 8.7755203Losses:  8.667766571044922 0.8866698741912842
CurrentTrain: epoch  0, batch    84 | loss: 9.5544367Losses:  7.494722366333008 0.754777193069458
CurrentTrain: epoch  0, batch    85 | loss: 8.2494993Losses:  8.202760696411133 0.81428062915802
CurrentTrain: epoch  0, batch    86 | loss: 9.0170412Losses:  7.800426483154297 0.8052996397018433
CurrentTrain: epoch  0, batch    87 | loss: 8.6057262Losses:  8.28257942199707 0.8750507235527039
CurrentTrain: epoch  0, batch    88 | loss: 9.1576300Losses:  7.830731391906738 0.48925527930259705
CurrentTrain: epoch  0, batch    89 | loss: 8.3199863Losses:  8.219382286071777 0.7514111995697021
CurrentTrain: epoch  0, batch    90 | loss: 8.9707937Losses:  7.97601318359375 0.7062891721725464
CurrentTrain: epoch  0, batch    91 | loss: 8.6823025Losses:  7.92527437210083 0.5907641053199768
CurrentTrain: epoch  0, batch    92 | loss: 8.5160389Losses:  7.693242073059082 0.545620322227478
CurrentTrain: epoch  0, batch    93 | loss: 8.2388620Losses:  7.850741863250732 0.9056463241577148
CurrentTrain: epoch  0, batch    94 | loss: 8.7563877Losses:  6.946377754211426 0.6104293465614319
CurrentTrain: epoch  0, batch    95 | loss: 7.5568070Losses:  7.0460944175720215 0.46160057187080383
CurrentTrain: epoch  0, batch    96 | loss: 7.5076952Losses:  7.76876163482666 0.7889782190322876
CurrentTrain: epoch  0, batch    97 | loss: 8.5577402Losses:  7.108860969543457 0.7728532552719116
CurrentTrain: epoch  0, batch    98 | loss: 7.8817143Losses:  7.466521263122559 0.7291831970214844
CurrentTrain: epoch  0, batch    99 | loss: 8.1957045Losses:  6.96931266784668 0.501013994216919
CurrentTrain: epoch  0, batch   100 | loss: 7.4703264Losses:  6.973982810974121 0.6870729923248291
CurrentTrain: epoch  0, batch   101 | loss: 7.6610556Losses:  6.924927711486816 0.6595653891563416
CurrentTrain: epoch  0, batch   102 | loss: 7.5844932Losses:  6.759500503540039 0.7272799015045166
CurrentTrain: epoch  0, batch   103 | loss: 7.4867802Losses:  6.536437034606934 0.5528878569602966
CurrentTrain: epoch  0, batch   104 | loss: 7.0893250Losses:  6.70506477355957 0.6022698879241943
CurrentTrain: epoch  0, batch   105 | loss: 7.3073349Losses:  6.694060325622559 0.5046486258506775
CurrentTrain: epoch  0, batch   106 | loss: 7.1987090Losses:  6.567037105560303 0.4598907232284546
CurrentTrain: epoch  0, batch   107 | loss: 7.0269279Losses:  6.987677574157715 0.504968523979187
CurrentTrain: epoch  0, batch   108 | loss: 7.4926462Losses:  6.851047039031982 0.5707232356071472
CurrentTrain: epoch  0, batch   109 | loss: 7.4217701Losses:  6.534813404083252 0.6728174090385437
CurrentTrain: epoch  0, batch   110 | loss: 7.2076306Losses:  6.422371864318848 0.4127713441848755
CurrentTrain: epoch  0, batch   111 | loss: 6.8351431Losses:  6.869662761688232 0.895923376083374
CurrentTrain: epoch  0, batch   112 | loss: 7.7655859Losses:  6.020822525024414 0.5670934915542603
CurrentTrain: epoch  0, batch   113 | loss: 6.5879159Losses:  5.8558831214904785 0.5514169931411743
CurrentTrain: epoch  0, batch   114 | loss: 6.4073000Losses:  6.0478386878967285 0.5843492746353149
CurrentTrain: epoch  0, batch   115 | loss: 6.6321878Losses:  6.7135772705078125 0.7733293175697327
CurrentTrain: epoch  0, batch   116 | loss: 7.4869065Losses:  5.871764183044434 0.5438550114631653
CurrentTrain: epoch  0, batch   117 | loss: 6.4156194Losses:  5.930088996887207 0.5274753570556641
CurrentTrain: epoch  0, batch   118 | loss: 6.4575644Losses:  5.96303129196167 0.6069725751876831
CurrentTrain: epoch  0, batch   119 | loss: 6.5700040Losses:  5.6055521965026855 0.6155562400817871
CurrentTrain: epoch  0, batch   120 | loss: 6.2211084Losses:  5.252377510070801 0.39363735914230347
CurrentTrain: epoch  0, batch   121 | loss: 5.6460147Losses:  5.510207176208496 0.4379928708076477
CurrentTrain: epoch  0, batch   122 | loss: 5.9482002Losses:  5.429081916809082 0.4914146661758423
CurrentTrain: epoch  0, batch   123 | loss: 5.9204965Losses:  5.222128391265869 0.46374237537384033
CurrentTrain: epoch  0, batch   124 | loss: 5.6858706Losses:  6.031467437744141 0.7113813161849976
CurrentTrain: epoch  1, batch     0 | loss: 6.7428489Losses:  5.473923683166504 0.3490194082260132
CurrentTrain: epoch  1, batch     1 | loss: 5.8229432Losses:  5.330963134765625 0.5075734257698059
CurrentTrain: epoch  1, batch     2 | loss: 5.8385367Losses:  5.213872909545898 0.4684017300605774
CurrentTrain: epoch  1, batch     3 | loss: 5.6822748Losses:  5.352255821228027 0.41023701429367065
CurrentTrain: epoch  1, batch     4 | loss: 5.7624927Losses:  4.981799602508545 0.5714457035064697
CurrentTrain: epoch  1, batch     5 | loss: 5.5532455Losses:  5.936934471130371 0.5923243761062622
CurrentTrain: epoch  1, batch     6 | loss: 6.5292587Losses:  5.427567958831787 0.48629045486450195
CurrentTrain: epoch  1, batch     7 | loss: 5.9138584Losses:  5.218639373779297 0.32671719789505005
CurrentTrain: epoch  1, batch     8 | loss: 5.5453568Losses:  4.8081488609313965 0.39278799295425415
CurrentTrain: epoch  1, batch     9 | loss: 5.2009368Losses:  5.1655168533325195 0.3816939890384674
CurrentTrain: epoch  1, batch    10 | loss: 5.5472107Losses:  5.2181549072265625 0.6363675594329834
CurrentTrain: epoch  1, batch    11 | loss: 5.8545227Losses:  5.327126502990723 0.555732011795044
CurrentTrain: epoch  1, batch    12 | loss: 5.8828583Losses:  5.245003700256348 0.42793405055999756
CurrentTrain: epoch  1, batch    13 | loss: 5.6729379Losses:  5.311015605926514 0.4180643558502197
CurrentTrain: epoch  1, batch    14 | loss: 5.7290802Losses:  5.352382659912109 0.5172168016433716
CurrentTrain: epoch  1, batch    15 | loss: 5.8695993Losses:  5.906898498535156 0.7314157485961914
CurrentTrain: epoch  1, batch    16 | loss: 6.6383142Losses:  5.194557189941406 0.28354281187057495
CurrentTrain: epoch  1, batch    17 | loss: 5.4780998Losses:  5.070671081542969 0.5306181311607361
CurrentTrain: epoch  1, batch    18 | loss: 5.6012893Losses:  5.3669843673706055 0.5684337019920349
CurrentTrain: epoch  1, batch    19 | loss: 5.9354181Losses:  4.868310928344727 0.42123496532440186
CurrentTrain: epoch  1, batch    20 | loss: 5.2895460Losses:  5.194300651550293 0.32861703634262085
CurrentTrain: epoch  1, batch    21 | loss: 5.5229177Losses:  5.5870561599731445 0.5799169540405273
CurrentTrain: epoch  1, batch    22 | loss: 6.1669731Losses:  5.44455623626709 0.5718631744384766
CurrentTrain: epoch  1, batch    23 | loss: 6.0164194Losses:  5.355712890625 0.4143105745315552
CurrentTrain: epoch  1, batch    24 | loss: 5.7700233Losses:  5.4799909591674805 0.4116685390472412
CurrentTrain: epoch  1, batch    25 | loss: 5.8916597Losses:  4.862278938293457 0.3757961392402649
CurrentTrain: epoch  1, batch    26 | loss: 5.2380753Losses:  5.112664699554443 0.4549351930618286
CurrentTrain: epoch  1, batch    27 | loss: 5.5675998Losses:  5.393206596374512 0.4360296130180359
CurrentTrain: epoch  1, batch    28 | loss: 5.8292360Losses:  5.323122978210449 0.30696916580200195
CurrentTrain: epoch  1, batch    29 | loss: 5.6300921Losses:  5.335941314697266 0.423051118850708
CurrentTrain: epoch  1, batch    30 | loss: 5.7589922Losses:  4.90860652923584 0.30680108070373535
CurrentTrain: epoch  1, batch    31 | loss: 5.2154074Losses:  6.020505428314209 0.3666079044342041
CurrentTrain: epoch  1, batch    32 | loss: 6.3871136Losses:  5.776669502258301 0.48307880759239197
CurrentTrain: epoch  1, batch    33 | loss: 6.2597485Losses:  5.126701354980469 0.36104676127433777
CurrentTrain: epoch  1, batch    34 | loss: 5.4877481Losses:  4.941321849822998 0.2606685161590576
CurrentTrain: epoch  1, batch    35 | loss: 5.2019901Losses:  6.0141706466674805 0.45318403840065
CurrentTrain: epoch  1, batch    36 | loss: 6.4673548Losses:  4.910801887512207 0.1503094732761383
CurrentTrain: epoch  1, batch    37 | loss: 5.0611115Losses:  5.638564109802246 0.3634657859802246
CurrentTrain: epoch  1, batch    38 | loss: 6.0020299Losses:  6.015978813171387 0.40820425748825073
CurrentTrain: epoch  1, batch    39 | loss: 6.4241829Losses:  5.161818504333496 0.28142935037612915
CurrentTrain: epoch  1, batch    40 | loss: 5.4432478Losses:  5.093876361846924 0.20665189623832703
CurrentTrain: epoch  1, batch    41 | loss: 5.3005280Losses:  4.691230773925781 0.26347535848617554
CurrentTrain: epoch  1, batch    42 | loss: 4.9547062Losses:  4.830841064453125 0.3431553244590759
CurrentTrain: epoch  1, batch    43 | loss: 5.1739964Losses:  5.001630783081055 0.30943354964256287
CurrentTrain: epoch  1, batch    44 | loss: 5.3110642Losses:  4.631800174713135 0.23405760526657104
CurrentTrain: epoch  1, batch    45 | loss: 4.8658576Losses:  4.888721466064453 0.4231480360031128
CurrentTrain: epoch  1, batch    46 | loss: 5.3118696Losses:  4.776883602142334 0.20243015885353088
CurrentTrain: epoch  1, batch    47 | loss: 4.9793139Losses:  4.8457841873168945 0.32453006505966187
CurrentTrain: epoch  1, batch    48 | loss: 5.1703143Losses:  5.166608810424805 0.5512653589248657
CurrentTrain: epoch  1, batch    49 | loss: 5.7178741Losses:  5.278092384338379 0.4580546021461487
CurrentTrain: epoch  1, batch    50 | loss: 5.7361469Losses:  5.036247253417969 0.22391772270202637
CurrentTrain: epoch  1, batch    51 | loss: 5.2601652Losses:  5.570651054382324 0.6191283464431763
CurrentTrain: epoch  1, batch    52 | loss: 6.1897793Losses:  5.780209064483643 0.39915817975997925
CurrentTrain: epoch  1, batch    53 | loss: 6.1793671Losses:  4.793890476226807 0.4967614710330963
CurrentTrain: epoch  1, batch    54 | loss: 5.2906518Losses:  5.287108421325684 0.2766050398349762
CurrentTrain: epoch  1, batch    55 | loss: 5.5637136Losses:  5.18171501159668 0.44794145226478577
CurrentTrain: epoch  1, batch    56 | loss: 5.6296563Losses:  5.086392402648926 0.30402207374572754
CurrentTrain: epoch  1, batch    57 | loss: 5.3904142Losses:  5.212663650512695 0.5603320002555847
CurrentTrain: epoch  1, batch    58 | loss: 5.7729955Losses:  5.594221115112305 0.6805959939956665
CurrentTrain: epoch  1, batch    59 | loss: 6.2748170Losses:  4.803552627563477 0.11210618168115616
CurrentTrain: epoch  1, batch    60 | loss: 4.9156590Losses:  4.8963823318481445 0.33882445096969604
CurrentTrain: epoch  1, batch    61 | loss: 5.2352066Losses:  5.128973960876465 0.3973979949951172
CurrentTrain: epoch  1, batch    62 | loss: 5.5263720Losses:  5.52968692779541 0.44266945123672485
CurrentTrain: epoch  1, batch    63 | loss: 5.9723563Losses:  5.187270641326904 0.40190771222114563
CurrentTrain: epoch  1, batch    64 | loss: 5.5891786Losses:  4.994744777679443 0.4289151132106781
CurrentTrain: epoch  1, batch    65 | loss: 5.4236598Losses:  5.572807312011719 0.5283194184303284
CurrentTrain: epoch  1, batch    66 | loss: 6.1011267Losses:  5.795952796936035 0.23879985511302948
CurrentTrain: epoch  1, batch    67 | loss: 6.0347528Losses:  4.8221330642700195 0.3673952519893646
CurrentTrain: epoch  1, batch    68 | loss: 5.1895285Losses:  5.390397071838379 0.27573472261428833
CurrentTrain: epoch  1, batch    69 | loss: 5.6661320Losses:  4.823301315307617 0.14700612425804138
CurrentTrain: epoch  1, batch    70 | loss: 4.9703074Losses:  4.88478946685791 0.2465958297252655
CurrentTrain: epoch  1, batch    71 | loss: 5.1313853Losses:  4.753903388977051 0.3997401297092438
CurrentTrain: epoch  1, batch    72 | loss: 5.1536436Losses:  5.1342453956604 0.35142382979393005
CurrentTrain: epoch  1, batch    73 | loss: 5.4856691Losses:  5.630566596984863 0.5625706315040588
CurrentTrain: epoch  1, batch    74 | loss: 6.1931372Losses:  4.993795394897461 0.3003885746002197
CurrentTrain: epoch  1, batch    75 | loss: 5.2941837Losses:  5.1747541427612305 0.3268892168998718
CurrentTrain: epoch  1, batch    76 | loss: 5.5016432Losses:  5.020431995391846 0.3690687417984009
CurrentTrain: epoch  1, batch    77 | loss: 5.3895006Losses:  5.150264739990234 0.49493080377578735
CurrentTrain: epoch  1, batch    78 | loss: 5.6451955Losses:  5.3855881690979 0.39642348885536194
CurrentTrain: epoch  1, batch    79 | loss: 5.7820115Losses:  4.6784515380859375 0.31368690729141235
CurrentTrain: epoch  1, batch    80 | loss: 4.9921384Losses:  5.101139068603516 0.30093926191329956
CurrentTrain: epoch  1, batch    81 | loss: 5.4020782Losses:  4.914253234863281 0.385988712310791
CurrentTrain: epoch  1, batch    82 | loss: 5.3002419Losses:  6.099166393280029 0.31070366501808167
CurrentTrain: epoch  1, batch    83 | loss: 6.4098701Losses:  5.476850986480713 0.5154098272323608
CurrentTrain: epoch  1, batch    84 | loss: 5.9922609Losses:  4.8909831047058105 0.24037089943885803
CurrentTrain: epoch  1, batch    85 | loss: 5.1313539Losses:  5.015822410583496 0.38394874334335327
CurrentTrain: epoch  1, batch    86 | loss: 5.3997712Losses:  5.177692413330078 0.2934252619743347
CurrentTrain: epoch  1, batch    87 | loss: 5.4711175Losses:  5.0151543617248535 0.30546221137046814
CurrentTrain: epoch  1, batch    88 | loss: 5.3206167Losses:  4.565793991088867 0.16647090017795563
CurrentTrain: epoch  1, batch    89 | loss: 4.7322650Losses:  5.505434989929199 0.5256763100624084
CurrentTrain: epoch  1, batch    90 | loss: 6.0311112Losses:  4.995080947875977 0.40789365768432617
CurrentTrain: epoch  1, batch    91 | loss: 5.4029746Losses:  4.898708343505859 0.3101467490196228
CurrentTrain: epoch  1, batch    92 | loss: 5.2088552Losses:  4.3898820877075195 0.20821507275104523
CurrentTrain: epoch  1, batch    93 | loss: 4.5980973Losses:  4.681407928466797 0.18229389190673828
CurrentTrain: epoch  1, batch    94 | loss: 4.8637018Losses:  5.229118347167969 0.27948445081710815
CurrentTrain: epoch  1, batch    95 | loss: 5.5086026Losses:  5.437420845031738 0.4005588889122009
CurrentTrain: epoch  1, batch    96 | loss: 5.8379798Losses:  4.756556510925293 0.2779257893562317
CurrentTrain: epoch  1, batch    97 | loss: 5.0344825Losses:  4.695476531982422 0.1631903499364853
CurrentTrain: epoch  1, batch    98 | loss: 4.8586669Losses:  4.551279067993164 0.1813504695892334
CurrentTrain: epoch  1, batch    99 | loss: 4.7326298Losses:  4.7415337562561035 0.3178577125072479
CurrentTrain: epoch  1, batch   100 | loss: 5.0593915Losses:  4.434329032897949 0.2459620088338852
CurrentTrain: epoch  1, batch   101 | loss: 4.6802912Losses:  5.29464054107666 0.47861960530281067
CurrentTrain: epoch  1, batch   102 | loss: 5.7732601Losses:  4.789440631866455 0.2824590504169464
CurrentTrain: epoch  1, batch   103 | loss: 5.0718999Losses:  4.482897758483887 0.24150440096855164
CurrentTrain: epoch  1, batch   104 | loss: 4.7244020Losses:  4.742077827453613 0.3542243242263794
CurrentTrain: epoch  1, batch   105 | loss: 5.0963020Losses:  4.377335548400879 0.2271120846271515
CurrentTrain: epoch  1, batch   106 | loss: 4.6044478Losses:  4.524792671203613 0.28584086894989014
CurrentTrain: epoch  1, batch   107 | loss: 4.8106337Losses:  4.9001054763793945 0.3207167387008667
CurrentTrain: epoch  1, batch   108 | loss: 5.2208223Losses:  4.722265720367432 0.3937546908855438
CurrentTrain: epoch  1, batch   109 | loss: 5.1160202Losses:  4.497318267822266 0.28568118810653687
CurrentTrain: epoch  1, batch   110 | loss: 4.7829995Losses:  4.841177940368652 0.4319082498550415
CurrentTrain: epoch  1, batch   111 | loss: 5.2730861Losses:  4.7668914794921875 0.2585708200931549
CurrentTrain: epoch  1, batch   112 | loss: 5.0254622Losses:  4.396023750305176 0.16553117334842682
CurrentTrain: epoch  1, batch   113 | loss: 4.5615549Losses:  4.692027568817139 0.15295198559761047
CurrentTrain: epoch  1, batch   114 | loss: 4.8449798Losses:  4.4189348220825195 0.2972986102104187
CurrentTrain: epoch  1, batch   115 | loss: 4.7162333Losses:  4.508772373199463 0.22764423489570618
CurrentTrain: epoch  1, batch   116 | loss: 4.7364168Losses:  5.039134502410889 0.3423605263233185
CurrentTrain: epoch  1, batch   117 | loss: 5.3814950Losses:  5.101273536682129 0.4735497832298279
CurrentTrain: epoch  1, batch   118 | loss: 5.5748234Losses:  4.480718612670898 0.20757326483726501
CurrentTrain: epoch  1, batch   119 | loss: 4.6882920Losses:  4.415334701538086 0.21448031067848206
CurrentTrain: epoch  1, batch   120 | loss: 4.6298151Losses:  5.364953517913818 0.3702951669692993
CurrentTrain: epoch  1, batch   121 | loss: 5.7352486Losses:  4.618587493896484 0.23225614428520203
CurrentTrain: epoch  1, batch   122 | loss: 4.8508434Losses:  4.472655296325684 0.289279580116272
CurrentTrain: epoch  1, batch   123 | loss: 4.7619348Losses:  4.370926856994629 0.27640342712402344
CurrentTrain: epoch  1, batch   124 | loss: 4.6473303Losses:  4.695125579833984 0.2037779986858368
CurrentTrain: epoch  2, batch     0 | loss: 4.8989034Losses:  4.458247184753418 0.30854639410972595
CurrentTrain: epoch  2, batch     1 | loss: 4.7667937Losses:  4.551580429077148 0.28028056025505066
CurrentTrain: epoch  2, batch     2 | loss: 4.8318610Losses:  4.518717288970947 0.16235458850860596
CurrentTrain: epoch  2, batch     3 | loss: 4.6810718Losses:  4.599468231201172 0.17646655440330505
CurrentTrain: epoch  2, batch     4 | loss: 4.7759347Losses:  4.921535968780518 0.28734534978866577
CurrentTrain: epoch  2, batch     5 | loss: 5.2088814Losses:  4.431767463684082 0.19494524598121643
CurrentTrain: epoch  2, batch     6 | loss: 4.6267128Losses:  4.420930862426758 0.1274736225605011
CurrentTrain: epoch  2, batch     7 | loss: 4.5484047Losses:  4.143353462219238 0.13852083683013916
CurrentTrain: epoch  2, batch     8 | loss: 4.2818742Losses:  4.776885986328125 0.20656973123550415
CurrentTrain: epoch  2, batch     9 | loss: 4.9834557Losses:  4.438567638397217 0.22766727209091187
CurrentTrain: epoch  2, batch    10 | loss: 4.6662350Losses:  4.27791690826416 0.2495773583650589
CurrentTrain: epoch  2, batch    11 | loss: 4.5274944Losses:  4.342985153198242 0.1530488133430481
CurrentTrain: epoch  2, batch    12 | loss: 4.4960341Losses:  4.3755269050598145 0.25480443239212036
CurrentTrain: epoch  2, batch    13 | loss: 4.6303315Losses:  4.627174377441406 0.2640821933746338
CurrentTrain: epoch  2, batch    14 | loss: 4.8912563Losses:  4.474908351898193 0.2569080591201782
CurrentTrain: epoch  2, batch    15 | loss: 4.7318163Losses:  4.643279075622559 0.22973230481147766
CurrentTrain: epoch  2, batch    16 | loss: 4.8730116Losses:  4.516253471374512 0.36061322689056396
CurrentTrain: epoch  2, batch    17 | loss: 4.8768668Losses:  4.705645561218262 0.1654869019985199
CurrentTrain: epoch  2, batch    18 | loss: 4.8711324Losses:  4.164324760437012 0.12035338580608368
CurrentTrain: epoch  2, batch    19 | loss: 4.2846780Losses:  4.404966354370117 0.16178999841213226
CurrentTrain: epoch  2, batch    20 | loss: 4.5667562Losses:  4.311033248901367 0.23092490434646606
CurrentTrain: epoch  2, batch    21 | loss: 4.5419583Losses:  4.352756500244141 0.12226120382547379
CurrentTrain: epoch  2, batch    22 | loss: 4.4750175Losses:  5.442790985107422 0.5093445777893066
CurrentTrain: epoch  2, batch    23 | loss: 5.9521356Losses:  4.3935441970825195 0.15261203050613403
CurrentTrain: epoch  2, batch    24 | loss: 4.5461564Losses:  4.40860652923584 0.20958763360977173
CurrentTrain: epoch  2, batch    25 | loss: 4.6181941Losses:  4.212009429931641 0.15412680804729462
CurrentTrain: epoch  2, batch    26 | loss: 4.3661361Losses:  4.274277687072754 0.2670551538467407
CurrentTrain: epoch  2, batch    27 | loss: 4.5413327Losses:  4.402663707733154 0.12373853474855423
CurrentTrain: epoch  2, batch    28 | loss: 4.5264025Losses:  4.3806610107421875 0.2123492807149887
CurrentTrain: epoch  2, batch    29 | loss: 4.5930104Losses:  4.497060298919678 0.1782657653093338
CurrentTrain: epoch  2, batch    30 | loss: 4.6753259Losses:  4.775443077087402 0.3245450258255005
CurrentTrain: epoch  2, batch    31 | loss: 5.0999880Losses:  4.363307952880859 0.1148461252450943
CurrentTrain: epoch  2, batch    32 | loss: 4.4781542Losses:  4.590217590332031 0.14450226724147797
CurrentTrain: epoch  2, batch    33 | loss: 4.7347198Losses:  5.085678577423096 0.5113102793693542
CurrentTrain: epoch  2, batch    34 | loss: 5.5969887Losses:  4.258677959442139 0.21153366565704346
CurrentTrain: epoch  2, batch    35 | loss: 4.4702115Losses:  4.441980838775635 0.2588633894920349
CurrentTrain: epoch  2, batch    36 | loss: 4.7008443Losses:  4.403408527374268 0.15317952632904053
CurrentTrain: epoch  2, batch    37 | loss: 4.5565882Losses:  4.2109375 0.23555107414722443
CurrentTrain: epoch  2, batch    38 | loss: 4.4464884Losses:  4.232954025268555 0.18095582723617554
CurrentTrain: epoch  2, batch    39 | loss: 4.4139099Losses:  4.301259994506836 0.1089787632226944
CurrentTrain: epoch  2, batch    40 | loss: 4.4102387Losses:  4.618852138519287 0.16950297355651855
CurrentTrain: epoch  2, batch    41 | loss: 4.7883549Losses:  4.536916732788086 0.27135759592056274
CurrentTrain: epoch  2, batch    42 | loss: 4.8082743Losses:  4.747838020324707 0.2391466498374939
CurrentTrain: epoch  2, batch    43 | loss: 4.9869847Losses:  4.400506973266602 0.13600391149520874
CurrentTrain: epoch  2, batch    44 | loss: 4.5365109Losses:  4.376762390136719 0.13401564955711365
CurrentTrain: epoch  2, batch    45 | loss: 4.5107780Losses:  4.591127872467041 0.20435196161270142
CurrentTrain: epoch  2, batch    46 | loss: 4.7954798Losses:  4.550822734832764 0.18278026580810547
CurrentTrain: epoch  2, batch    47 | loss: 4.7336030Losses:  5.134405136108398 0.32195472717285156
CurrentTrain: epoch  2, batch    48 | loss: 5.4563599Losses:  4.2852325439453125 0.15990211069583893
CurrentTrain: epoch  2, batch    49 | loss: 4.4451346Losses:  4.602322578430176 0.34013086557388306
CurrentTrain: epoch  2, batch    50 | loss: 4.9424534Losses:  4.319039344787598 0.2301679402589798
CurrentTrain: epoch  2, batch    51 | loss: 4.5492072Losses:  4.333463668823242 0.27320632338523865
CurrentTrain: epoch  2, batch    52 | loss: 4.6066699Losses:  4.468252182006836 0.19891692698001862
CurrentTrain: epoch  2, batch    53 | loss: 4.6671691Losses:  4.484228134155273 0.11628078669309616
CurrentTrain: epoch  2, batch    54 | loss: 4.6005087Losses:  4.391770362854004 0.20416133105754852
CurrentTrain: epoch  2, batch    55 | loss: 4.5959315Losses:  4.706545829772949 0.20525284111499786
CurrentTrain: epoch  2, batch    56 | loss: 4.9117985Losses:  4.554593563079834 0.21242183446884155
CurrentTrain: epoch  2, batch    57 | loss: 4.7670155Losses:  4.56821346282959 0.19928744435310364
CurrentTrain: epoch  2, batch    58 | loss: 4.7675009Losses:  4.432735443115234 0.22385448217391968
CurrentTrain: epoch  2, batch    59 | loss: 4.6565900Losses:  4.219666004180908 0.1742042601108551
CurrentTrain: epoch  2, batch    60 | loss: 4.3938704Losses:  4.296680450439453 0.17685359716415405
CurrentTrain: epoch  2, batch    61 | loss: 4.4735341Losses:  4.454987525939941 0.2297552525997162
CurrentTrain: epoch  2, batch    62 | loss: 4.6847429Losses:  4.1313300132751465 0.05602816119790077
CurrentTrain: epoch  2, batch    63 | loss: 4.1873584Losses:  4.260378837585449 0.15271760523319244
CurrentTrain: epoch  2, batch    64 | loss: 4.4130964Losses:  4.200913429260254 0.14199891686439514
CurrentTrain: epoch  2, batch    65 | loss: 4.3429122Losses:  4.713893890380859 0.13886012136936188
CurrentTrain: epoch  2, batch    66 | loss: 4.8527541Losses:  4.3365888595581055 0.20629911124706268
CurrentTrain: epoch  2, batch    67 | loss: 4.5428882Losses:  4.358572006225586 0.19579273462295532
CurrentTrain: epoch  2, batch    68 | loss: 4.5543647Losses:  4.377861499786377 0.2609310448169708
CurrentTrain: epoch  2, batch    69 | loss: 4.6387925Losses:  4.3857927322387695 0.2687702775001526
CurrentTrain: epoch  2, batch    70 | loss: 4.6545630Losses:  4.494688034057617 0.3389337956905365
CurrentTrain: epoch  2, batch    71 | loss: 4.8336220Losses:  4.441608428955078 0.12587504088878632
CurrentTrain: epoch  2, batch    72 | loss: 4.5674834Losses:  5.022469520568848 0.2542002201080322
CurrentTrain: epoch  2, batch    73 | loss: 5.2766695Losses:  4.306041717529297 0.19093528389930725
CurrentTrain: epoch  2, batch    74 | loss: 4.4969769Losses:  4.519754886627197 0.22942490875720978
CurrentTrain: epoch  2, batch    75 | loss: 4.7491798Losses:  4.441981315612793 0.23037785291671753
CurrentTrain: epoch  2, batch    76 | loss: 4.6723590Losses:  4.141981601715088 0.12510105967521667
CurrentTrain: epoch  2, batch    77 | loss: 4.2670827Losses:  4.311706066131592 0.16869330406188965
CurrentTrain: epoch  2, batch    78 | loss: 4.4803991Losses:  4.287360191345215 0.13165487349033356
CurrentTrain: epoch  2, batch    79 | loss: 4.4190149Losses:  4.171740531921387 0.21230392158031464
CurrentTrain: epoch  2, batch    80 | loss: 4.3840446Losses:  4.3142499923706055 0.13058198988437653
CurrentTrain: epoch  2, batch    81 | loss: 4.4448318Losses:  4.25732946395874 0.23618298768997192
CurrentTrain: epoch  2, batch    82 | loss: 4.4935126Losses:  4.287537574768066 0.15431031584739685
CurrentTrain: epoch  2, batch    83 | loss: 4.4418478Losses:  4.296323299407959 0.13209259510040283
CurrentTrain: epoch  2, batch    84 | loss: 4.4284158Losses:  4.254728317260742 0.1727118194103241
CurrentTrain: epoch  2, batch    85 | loss: 4.4274402Losses:  4.197336196899414 0.17395973205566406
CurrentTrain: epoch  2, batch    86 | loss: 4.3712959Losses:  4.283760070800781 0.1556570827960968
CurrentTrain: epoch  2, batch    87 | loss: 4.4394174Losses:  4.565558910369873 0.29255211353302
CurrentTrain: epoch  2, batch    88 | loss: 4.8581109Losses:  4.368025779724121 0.2093231976032257
CurrentTrain: epoch  2, batch    89 | loss: 4.5773492Losses:  4.300073623657227 0.12070674449205399
CurrentTrain: epoch  2, batch    90 | loss: 4.4207802Losses:  4.326053619384766 0.27147236466407776
CurrentTrain: epoch  2, batch    91 | loss: 4.5975261Losses:  4.289391994476318 0.1496499925851822
CurrentTrain: epoch  2, batch    92 | loss: 4.4390421Losses:  4.232045650482178 0.1542109251022339
CurrentTrain: epoch  2, batch    93 | loss: 4.3862567Losses:  4.167022705078125 0.12514016032218933
CurrentTrain: epoch  2, batch    94 | loss: 4.2921629Losses:  4.458905220031738 0.167705237865448
CurrentTrain: epoch  2, batch    95 | loss: 4.6266103Losses:  4.397485733032227 0.21454498171806335
CurrentTrain: epoch  2, batch    96 | loss: 4.6120305Losses:  4.256755828857422 0.2284870594739914
CurrentTrain: epoch  2, batch    97 | loss: 4.4852428Losses:  4.779682636260986 0.13379628956317902
CurrentTrain: epoch  2, batch    98 | loss: 4.9134789Losses:  4.393146514892578 0.13068723678588867
CurrentTrain: epoch  2, batch    99 | loss: 4.5238338Losses:  4.204067230224609 0.1274634748697281
CurrentTrain: epoch  2, batch   100 | loss: 4.3315306Losses:  4.935314178466797 0.4819125533103943
CurrentTrain: epoch  2, batch   101 | loss: 5.4172268Losses:  4.192760467529297 0.07189223170280457
CurrentTrain: epoch  2, batch   102 | loss: 4.2646527Losses:  4.225648880004883 0.1661088913679123
CurrentTrain: epoch  2, batch   103 | loss: 4.3917580Losses:  4.111023902893066 0.09232614189386368
CurrentTrain: epoch  2, batch   104 | loss: 4.2033501Losses:  4.39628791809082 0.09974111616611481
CurrentTrain: epoch  2, batch   105 | loss: 4.4960289Losses:  4.141124725341797 0.17381229996681213
CurrentTrain: epoch  2, batch   106 | loss: 4.3149371Losses:  4.214511871337891 0.2723895013332367
CurrentTrain: epoch  2, batch   107 | loss: 4.4869013Losses:  4.225351810455322 0.1901319921016693
CurrentTrain: epoch  2, batch   108 | loss: 4.4154840Losses:  4.307805061340332 0.22343707084655762
CurrentTrain: epoch  2, batch   109 | loss: 4.5312424Losses:  4.097899436950684 0.2274080067873001
CurrentTrain: epoch  2, batch   110 | loss: 4.3253074Losses:  4.351382732391357 0.15818247199058533
CurrentTrain: epoch  2, batch   111 | loss: 4.5095654Losses:  4.3269267082214355 0.16387727856636047
CurrentTrain: epoch  2, batch   112 | loss: 4.4908042Losses:  4.182621002197266 0.11999678611755371
CurrentTrain: epoch  2, batch   113 | loss: 4.3026180Losses:  4.282679557800293 0.2163875699043274
CurrentTrain: epoch  2, batch   114 | loss: 4.4990673Losses:  4.453700065612793 0.1915905475616455
CurrentTrain: epoch  2, batch   115 | loss: 4.6452904Losses:  4.311489105224609 0.18639841675758362
CurrentTrain: epoch  2, batch   116 | loss: 4.4978876Losses:  4.28307580947876 0.13774354755878448
CurrentTrain: epoch  2, batch   117 | loss: 4.4208193Losses:  4.623495578765869 0.08650223165750504
CurrentTrain: epoch  2, batch   118 | loss: 4.7099977Losses:  4.146576881408691 0.15215575695037842
CurrentTrain: epoch  2, batch   119 | loss: 4.2987328Losses:  4.1774115562438965 0.19527167081832886
CurrentTrain: epoch  2, batch   120 | loss: 4.3726830Losses:  4.392586708068848 0.1672949194908142
CurrentTrain: epoch  2, batch   121 | loss: 4.5598817Losses:  4.023284912109375 0.05831391364336014
CurrentTrain: epoch  2, batch   122 | loss: 4.0815988Losses:  4.395106792449951 0.18106018006801605
CurrentTrain: epoch  2, batch   123 | loss: 4.5761671Losses:  4.061195373535156 0.0894239991903305
CurrentTrain: epoch  2, batch   124 | loss: 4.1506195Losses:  4.125034332275391 0.1636531949043274
CurrentTrain: epoch  3, batch     0 | loss: 4.2886877Losses:  4.151175498962402 0.17431212961673737
CurrentTrain: epoch  3, batch     1 | loss: 4.3254876Losses:  4.213747978210449 0.1101604774594307
CurrentTrain: epoch  3, batch     2 | loss: 4.3239083Losses:  4.053210258483887 0.07180386781692505
CurrentTrain: epoch  3, batch     3 | loss: 4.1250143Losses:  4.174269676208496 0.12316712737083435
CurrentTrain: epoch  3, batch     4 | loss: 4.2974367Losses:  4.338383674621582 0.17286837100982666
CurrentTrain: epoch  3, batch     5 | loss: 4.5112519Losses:  4.231882095336914 0.1718379557132721
CurrentTrain: epoch  3, batch     6 | loss: 4.4037199Losses:  4.250147342681885 0.18771183490753174
CurrentTrain: epoch  3, batch     7 | loss: 4.4378591Losses:  4.125840187072754 0.1262221783399582
CurrentTrain: epoch  3, batch     8 | loss: 4.2520623Losses:  4.165394306182861 0.135541632771492
CurrentTrain: epoch  3, batch     9 | loss: 4.3009357Losses:  4.237576484680176 0.08398550748825073
CurrentTrain: epoch  3, batch    10 | loss: 4.3215618Losses:  4.170772075653076 0.09347588568925858
CurrentTrain: epoch  3, batch    11 | loss: 4.2642479Losses:  4.167291641235352 0.13715164363384247
CurrentTrain: epoch  3, batch    12 | loss: 4.3044434Losses:  4.1423115730285645 0.10708168894052505
CurrentTrain: epoch  3, batch    13 | loss: 4.2493935Losses:  4.1291022300720215 0.14361242949962616
CurrentTrain: epoch  3, batch    14 | loss: 4.2727146Losses:  4.232919692993164 0.1703290194272995
CurrentTrain: epoch  3, batch    15 | loss: 4.4032488Losses:  4.136044502258301 0.09497347474098206
CurrentTrain: epoch  3, batch    16 | loss: 4.2310181Losses:  4.251636505126953 0.19569247961044312
CurrentTrain: epoch  3, batch    17 | loss: 4.4473290Losses:  4.339241981506348 0.20512180030345917
CurrentTrain: epoch  3, batch    18 | loss: 4.5443640Losses:  4.071746826171875 0.0524655282497406
CurrentTrain: epoch  3, batch    19 | loss: 4.1242123Losses:  3.9894325733184814 0.13033826649188995
CurrentTrain: epoch  3, batch    20 | loss: 4.1197710Losses:  4.170287132263184 0.12499591708183289
CurrentTrain: epoch  3, batch    21 | loss: 4.2952828Losses:  4.038227081298828 0.14112673699855804
CurrentTrain: epoch  3, batch    22 | loss: 4.1793537Losses:  4.133403778076172 0.17931146919727325
CurrentTrain: epoch  3, batch    23 | loss: 4.3127151Losses:  4.113680839538574 0.07996267080307007
CurrentTrain: epoch  3, batch    24 | loss: 4.1936436Losses:  4.292181968688965 0.16924159228801727
CurrentTrain: epoch  3, batch    25 | loss: 4.4614234Losses:  4.169035911560059 0.12032406777143478
CurrentTrain: epoch  3, batch    26 | loss: 4.2893600Losses:  4.095426559448242 0.17216861248016357
CurrentTrain: epoch  3, batch    27 | loss: 4.2675953Losses:  4.161717414855957 0.10405117273330688
CurrentTrain: epoch  3, batch    28 | loss: 4.2657685Losses:  4.304028511047363 0.1931915283203125
CurrentTrain: epoch  3, batch    29 | loss: 4.4972200Losses:  4.1985063552856445 0.16275332868099213
CurrentTrain: epoch  3, batch    30 | loss: 4.3612595Losses:  4.126372337341309 0.08229263126850128
CurrentTrain: epoch  3, batch    31 | loss: 4.2086649Losses:  4.271909236907959 0.0954628437757492
CurrentTrain: epoch  3, batch    32 | loss: 4.3673720Losses:  4.145434379577637 0.11031252145767212
CurrentTrain: epoch  3, batch    33 | loss: 4.2557468Losses:  4.260946273803711 0.07942389696836472
CurrentTrain: epoch  3, batch    34 | loss: 4.3403702Losses:  4.233068466186523 0.17417950928211212
CurrentTrain: epoch  3, batch    35 | loss: 4.4072480Losses:  4.142320156097412 0.1552249789237976
CurrentTrain: epoch  3, batch    36 | loss: 4.2975450Losses:  4.006331443786621 0.09504614770412445
CurrentTrain: epoch  3, batch    37 | loss: 4.1013775Losses:  4.213247299194336 0.1403171569108963
CurrentTrain: epoch  3, batch    38 | loss: 4.3535643Losses:  4.100545883178711 0.16304682195186615
CurrentTrain: epoch  3, batch    39 | loss: 4.2635927Losses:  4.085324287414551 0.13730372488498688
CurrentTrain: epoch  3, batch    40 | loss: 4.2226281Losses:  4.175158500671387 0.12721097469329834
CurrentTrain: epoch  3, batch    41 | loss: 4.3023696Losses:  4.093867301940918 0.10559271275997162
CurrentTrain: epoch  3, batch    42 | loss: 4.1994600Losses:  4.049585342407227 0.15038712322711945
CurrentTrain: epoch  3, batch    43 | loss: 4.1999726Losses:  4.209835529327393 0.14368778467178345
CurrentTrain: epoch  3, batch    44 | loss: 4.3535233Losses:  4.240107536315918 0.13138583302497864
CurrentTrain: epoch  3, batch    45 | loss: 4.3714933Losses:  4.079899787902832 0.06065625324845314
CurrentTrain: epoch  3, batch    46 | loss: 4.1405559Losses:  4.129122734069824 0.12158143520355225
CurrentTrain: epoch  3, batch    47 | loss: 4.2507043Losses:  4.161938667297363 0.0809611976146698
CurrentTrain: epoch  3, batch    48 | loss: 4.2428999Losses:  4.151313304901123 0.11513902992010117
CurrentTrain: epoch  3, batch    49 | loss: 4.2664523Losses:  4.111356735229492 0.11159445345401764
CurrentTrain: epoch  3, batch    50 | loss: 4.2229514Losses:  4.181107044219971 0.14681194722652435
CurrentTrain: epoch  3, batch    51 | loss: 4.3279190Losses:  4.194191932678223 0.13205137848854065
CurrentTrain: epoch  3, batch    52 | loss: 4.3262434Losses:  4.227754592895508 0.10424084961414337
CurrentTrain: epoch  3, batch    53 | loss: 4.3319955Losses:  4.21502685546875 0.07767774909734726
CurrentTrain: epoch  3, batch    54 | loss: 4.2927046Losses:  4.347248554229736 0.14003407955169678
CurrentTrain: epoch  3, batch    55 | loss: 4.4872828Losses:  4.167187690734863 0.19251859188079834
CurrentTrain: epoch  3, batch    56 | loss: 4.3597064Losses:  4.130080223083496 0.16331149637699127
CurrentTrain: epoch  3, batch    57 | loss: 4.2933917Losses:  4.048397064208984 0.08470615744590759
CurrentTrain: epoch  3, batch    58 | loss: 4.1331034Losses:  4.140446662902832 0.1097935140132904
CurrentTrain: epoch  3, batch    59 | loss: 4.2502403Losses:  3.991576910018921 0.08599970489740372
CurrentTrain: epoch  3, batch    60 | loss: 4.0775766Losses:  4.219272613525391 0.07215897738933563
CurrentTrain: epoch  3, batch    61 | loss: 4.2914314Losses:  4.087929725646973 0.08399730920791626
CurrentTrain: epoch  3, batch    62 | loss: 4.1719270Losses:  4.191890716552734 0.11829648911952972
CurrentTrain: epoch  3, batch    63 | loss: 4.3101873Losses:  4.043658256530762 0.17142519354820251
CurrentTrain: epoch  3, batch    64 | loss: 4.2150836Losses:  4.232054710388184 0.09572684019804001
CurrentTrain: epoch  3, batch    65 | loss: 4.3277817Losses:  4.028144836425781 0.18794885277748108
CurrentTrain: epoch  3, batch    66 | loss: 4.2160935Losses:  4.123237609863281 0.13029707968235016
CurrentTrain: epoch  3, batch    67 | loss: 4.2535348Losses:  4.029513359069824 0.17985942959785461
CurrentTrain: epoch  3, batch    68 | loss: 4.2093730Losses:  4.145186424255371 0.11647261679172516
CurrentTrain: epoch  3, batch    69 | loss: 4.2616591Losses:  4.3394060134887695 0.15720269083976746
CurrentTrain: epoch  3, batch    70 | loss: 4.4966087Losses:  4.088040828704834 0.07954727113246918
CurrentTrain: epoch  3, batch    71 | loss: 4.1675882Losses:  4.120250701904297 0.15007518231868744
CurrentTrain: epoch  3, batch    72 | loss: 4.2703257Losses:  4.060054302215576 0.15852944552898407
CurrentTrain: epoch  3, batch    73 | loss: 4.2185836Losses:  4.019733428955078 0.10184577107429504
CurrentTrain: epoch  3, batch    74 | loss: 4.1215792Losses:  4.146883487701416 0.1589503139257431
CurrentTrain: epoch  3, batch    75 | loss: 4.3058338Losses:  3.950920820236206 0.06152014806866646
CurrentTrain: epoch  3, batch    76 | loss: 4.0124412Losses:  5.190201282501221 0.3935718536376953
CurrentTrain: epoch  3, batch    77 | loss: 5.5837731Losses:  4.383739948272705 0.17736798524856567
CurrentTrain: epoch  3, batch    78 | loss: 4.5611081Losses:  4.333151817321777 0.1577836275100708
CurrentTrain: epoch  3, batch    79 | loss: 4.4909353Losses:  4.151573181152344 0.06776462495326996
CurrentTrain: epoch  3, batch    80 | loss: 4.2193379Losses:  4.069951057434082 0.09636497497558594
CurrentTrain: epoch  3, batch    81 | loss: 4.1663160Losses:  4.164999485015869 0.12148182094097137
CurrentTrain: epoch  3, batch    82 | loss: 4.2864814Losses:  4.07975959777832 0.06391559541225433
CurrentTrain: epoch  3, batch    83 | loss: 4.1436753Losses:  4.105649471282959 0.07902541011571884
CurrentTrain: epoch  3, batch    84 | loss: 4.1846747Losses:  4.2891845703125 0.16389380395412445
CurrentTrain: epoch  3, batch    85 | loss: 4.4530783Losses:  4.087870121002197 0.10984501242637634
CurrentTrain: epoch  3, batch    86 | loss: 4.1977153Losses:  3.9889063835144043 0.11267411708831787
CurrentTrain: epoch  3, batch    87 | loss: 4.1015806Losses:  4.021031379699707 0.09229288250207901
CurrentTrain: epoch  3, batch    88 | loss: 4.1133242Losses:  4.067844867706299 0.10441562533378601
CurrentTrain: epoch  3, batch    89 | loss: 4.1722603Losses:  4.0841875076293945 0.1007017195224762
CurrentTrain: epoch  3, batch    90 | loss: 4.1848893Losses:  4.370355606079102 0.12281805276870728
CurrentTrain: epoch  3, batch    91 | loss: 4.4931736Losses:  4.227582931518555 0.10370492935180664
CurrentTrain: epoch  3, batch    92 | loss: 4.3312879Losses:  3.985255718231201 0.04586811363697052
CurrentTrain: epoch  3, batch    93 | loss: 4.0311236Losses:  4.046090126037598 0.17521916329860687
CurrentTrain: epoch  3, batch    94 | loss: 4.2213092Losses:  4.15301513671875 0.18399114906787872
CurrentTrain: epoch  3, batch    95 | loss: 4.3370061Losses:  4.002047538757324 0.06571812927722931
CurrentTrain: epoch  3, batch    96 | loss: 4.0677657Losses:  4.068932056427002 0.09846435487270355
CurrentTrain: epoch  3, batch    97 | loss: 4.1673965Losses:  4.04390811920166 0.09431469440460205
CurrentTrain: epoch  3, batch    98 | loss: 4.1382227Losses:  4.162795066833496 0.10191632807254791
CurrentTrain: epoch  3, batch    99 | loss: 4.2647114Losses:  4.1484599113464355 0.15574903786182404
CurrentTrain: epoch  3, batch   100 | loss: 4.3042088Losses:  4.067031383514404 0.06774246692657471
CurrentTrain: epoch  3, batch   101 | loss: 4.1347737Losses:  4.103878021240234 0.12061156332492828
CurrentTrain: epoch  3, batch   102 | loss: 4.2244897Losses:  4.132138729095459 0.07975399494171143
CurrentTrain: epoch  3, batch   103 | loss: 4.2118926Losses:  4.074864387512207 0.08997663855552673
CurrentTrain: epoch  3, batch   104 | loss: 4.1648412Losses:  4.033080101013184 0.07198086380958557
CurrentTrain: epoch  3, batch   105 | loss: 4.1050611Losses:  4.055658340454102 0.06522676348686218
CurrentTrain: epoch  3, batch   106 | loss: 4.1208849Losses:  4.091323375701904 0.11285650730133057
CurrentTrain: epoch  3, batch   107 | loss: 4.2041798Losses:  4.103004455566406 0.15086844563484192
CurrentTrain: epoch  3, batch   108 | loss: 4.2538729Losses:  4.103631973266602 0.12009354680776596
CurrentTrain: epoch  3, batch   109 | loss: 4.2237253Losses:  4.1304731369018555 0.1446722149848938
CurrentTrain: epoch  3, batch   110 | loss: 4.2751455Losses:  4.0748090744018555 0.08448177576065063
CurrentTrain: epoch  3, batch   111 | loss: 4.1592908Losses:  4.046961784362793 0.08406313508749008
CurrentTrain: epoch  3, batch   112 | loss: 4.1310248Losses:  4.0626020431518555 0.06634579598903656
CurrentTrain: epoch  3, batch   113 | loss: 4.1289477Losses:  4.057912826538086 0.05558516085147858
CurrentTrain: epoch  3, batch   114 | loss: 4.1134982Losses:  4.090023994445801 0.14369186758995056
CurrentTrain: epoch  3, batch   115 | loss: 4.2337160Losses:  4.0047783851623535 0.09337518364191055
CurrentTrain: epoch  3, batch   116 | loss: 4.0981536Losses:  4.81040620803833 0.3496484160423279
CurrentTrain: epoch  3, batch   117 | loss: 5.1600547Losses:  4.04750394821167 0.09022824466228485
CurrentTrain: epoch  3, batch   118 | loss: 4.1377320Losses:  4.054168224334717 0.13717229664325714
CurrentTrain: epoch  3, batch   119 | loss: 4.1913404Losses:  4.18101167678833 0.12682171165943146
CurrentTrain: epoch  3, batch   120 | loss: 4.3078332Losses:  4.044929504394531 0.14681844413280487
CurrentTrain: epoch  3, batch   121 | loss: 4.1917481Losses:  4.050736904144287 0.15100055932998657
CurrentTrain: epoch  3, batch   122 | loss: 4.2017374Losses:  4.061055660247803 0.1569184809923172
CurrentTrain: epoch  3, batch   123 | loss: 4.2179742Losses:  4.008853435516357 0.07043419778347015
CurrentTrain: epoch  3, batch   124 | loss: 4.0792875Losses:  4.030843734741211 0.10318288207054138
CurrentTrain: epoch  4, batch     0 | loss: 4.1340265Losses:  4.094003200531006 0.0750502198934555
CurrentTrain: epoch  4, batch     1 | loss: 4.1690536Losses:  4.135179042816162 0.16029781103134155
CurrentTrain: epoch  4, batch     2 | loss: 4.2954769Losses:  4.029328346252441 0.053961969912052155
CurrentTrain: epoch  4, batch     3 | loss: 4.0832901Losses:  4.172479152679443 0.11979794502258301
CurrentTrain: epoch  4, batch     4 | loss: 4.2922773Losses:  4.275400638580322 0.12114657461643219
CurrentTrain: epoch  4, batch     5 | loss: 4.3965473Losses:  4.029417514801025 0.07472258806228638
CurrentTrain: epoch  4, batch     6 | loss: 4.1041403Losses:  4.026381492614746 0.08483985811471939
CurrentTrain: epoch  4, batch     7 | loss: 4.1112213Losses:  4.059533596038818 0.07336277514696121
CurrentTrain: epoch  4, batch     8 | loss: 4.1328964Losses:  4.131476402282715 0.10444873571395874
CurrentTrain: epoch  4, batch     9 | loss: 4.2359252Losses:  4.12816858291626 0.15119406580924988
CurrentTrain: epoch  4, batch    10 | loss: 4.2793627Losses:  4.245072841644287 0.08713377267122269
CurrentTrain: epoch  4, batch    11 | loss: 4.3322067Losses:  4.067275047302246 0.07287859171628952
CurrentTrain: epoch  4, batch    12 | loss: 4.1401534Losses:  4.011843681335449 0.07669034600257874
CurrentTrain: epoch  4, batch    13 | loss: 4.0885339Losses:  4.037219047546387 0.08734515309333801
CurrentTrain: epoch  4, batch    14 | loss: 4.1245642Losses:  4.043393135070801 0.07798804342746735
CurrentTrain: epoch  4, batch    15 | loss: 4.1213813Losses:  4.019009590148926 0.05170174315571785
CurrentTrain: epoch  4, batch    16 | loss: 4.0707111Losses:  4.005019187927246 0.11917875707149506
CurrentTrain: epoch  4, batch    17 | loss: 4.1241980Losses:  4.085517883300781 0.11998636275529861
CurrentTrain: epoch  4, batch    18 | loss: 4.2055044Losses:  4.018970966339111 0.1261817216873169
CurrentTrain: epoch  4, batch    19 | loss: 4.1451526Losses:  4.055046081542969 0.11613369733095169
CurrentTrain: epoch  4, batch    20 | loss: 4.1711798Losses:  4.04726505279541 0.1341002881526947
CurrentTrain: epoch  4, batch    21 | loss: 4.1813655Losses:  4.069041728973389 0.07748321443796158
CurrentTrain: epoch  4, batch    22 | loss: 4.1465249Losses:  4.025273323059082 0.040674299001693726
CurrentTrain: epoch  4, batch    23 | loss: 4.0659475Losses:  4.108379364013672 0.07329994440078735
CurrentTrain: epoch  4, batch    24 | loss: 4.1816792Losses:  4.033663749694824 0.09552159905433655
CurrentTrain: epoch  4, batch    25 | loss: 4.1291852Losses:  4.065165042877197 0.0506477914750576
CurrentTrain: epoch  4, batch    26 | loss: 4.1158128Losses:  4.058584213256836 0.12251828610897064
CurrentTrain: epoch  4, batch    27 | loss: 4.1811023Losses:  4.086627006530762 0.09493587911128998
CurrentTrain: epoch  4, batch    28 | loss: 4.1815629Losses:  4.040702819824219 0.07767681032419205
CurrentTrain: epoch  4, batch    29 | loss: 4.1183796Losses:  4.077749252319336 0.06873801350593567
CurrentTrain: epoch  4, batch    30 | loss: 4.1464872Losses:  4.041080474853516 0.14951777458190918
CurrentTrain: epoch  4, batch    31 | loss: 4.1905985Losses:  4.005740642547607 0.0898122563958168
CurrentTrain: epoch  4, batch    32 | loss: 4.0955529Losses:  4.143378257751465 0.05888934060931206
CurrentTrain: epoch  4, batch    33 | loss: 4.2022676Losses:  4.116259574890137 0.2225763201713562
CurrentTrain: epoch  4, batch    34 | loss: 4.3388357Losses:  4.087465286254883 0.08881030231714249
CurrentTrain: epoch  4, batch    35 | loss: 4.1762757Losses:  4.071323394775391 0.12701387703418732
CurrentTrain: epoch  4, batch    36 | loss: 4.1983371Losses:  4.065805435180664 0.053319863975048065
CurrentTrain: epoch  4, batch    37 | loss: 4.1191254Losses:  4.021793842315674 0.11372353136539459
CurrentTrain: epoch  4, batch    38 | loss: 4.1355176Losses:  4.055688858032227 0.07755578309297562
CurrentTrain: epoch  4, batch    39 | loss: 4.1332445Losses:  4.0513386726379395 0.1557912826538086
CurrentTrain: epoch  4, batch    40 | loss: 4.2071300Losses:  4.036835670471191 0.10010838508605957
CurrentTrain: epoch  4, batch    41 | loss: 4.1369438Losses:  4.063814163208008 0.0726863294839859
CurrentTrain: epoch  4, batch    42 | loss: 4.1365004Losses:  4.080941200256348 0.10546697676181793
CurrentTrain: epoch  4, batch    43 | loss: 4.1864080Losses:  4.051778793334961 0.0649975836277008
CurrentTrain: epoch  4, batch    44 | loss: 4.1167765Losses:  4.103999614715576 0.05362917110323906
CurrentTrain: epoch  4, batch    45 | loss: 4.1576290Losses:  4.053123474121094 0.13156059384346008
CurrentTrain: epoch  4, batch    46 | loss: 4.1846843Losses:  3.9851481914520264 0.11225704848766327
CurrentTrain: epoch  4, batch    47 | loss: 4.0974054Losses:  3.986903190612793 0.09094871580600739
CurrentTrain: epoch  4, batch    48 | loss: 4.0778518Losses:  3.9917163848876953 0.05134769529104233
CurrentTrain: epoch  4, batch    49 | loss: 4.0430641Losses:  4.14259147644043 0.09515339136123657
CurrentTrain: epoch  4, batch    50 | loss: 4.2377448Losses:  4.063333511352539 0.11401490867137909
CurrentTrain: epoch  4, batch    51 | loss: 4.1773486Losses:  4.038767337799072 0.11635102331638336
CurrentTrain: epoch  4, batch    52 | loss: 4.1551185Losses:  4.022231101989746 0.10646265000104904
CurrentTrain: epoch  4, batch    53 | loss: 4.1286936Losses:  4.1293840408325195 0.06587245315313339
CurrentTrain: epoch  4, batch    54 | loss: 4.1952567Losses:  4.02058219909668 0.11649379879236221
CurrentTrain: epoch  4, batch    55 | loss: 4.1370759Losses:  4.018382549285889 0.13004590570926666
CurrentTrain: epoch  4, batch    56 | loss: 4.1484284Losses:  4.014944076538086 0.0887485221028328
CurrentTrain: epoch  4, batch    57 | loss: 4.1036925Losses:  4.148280620574951 0.10936301946640015
CurrentTrain: epoch  4, batch    58 | loss: 4.2576437Losses:  3.96785831451416 0.06917020678520203
CurrentTrain: epoch  4, batch    59 | loss: 4.0370283Losses:  4.0817766189575195 0.07764799892902374
CurrentTrain: epoch  4, batch    60 | loss: 4.1594248Losses:  4.024175643920898 0.10849125683307648
CurrentTrain: epoch  4, batch    61 | loss: 4.1326671Losses:  4.029122352600098 0.07881386578083038
CurrentTrain: epoch  4, batch    62 | loss: 4.1079364Losses:  4.043313503265381 0.08657924830913544
CurrentTrain: epoch  4, batch    63 | loss: 4.1298928Losses:  3.992185354232788 0.12421290576457977
CurrentTrain: epoch  4, batch    64 | loss: 4.1163983Losses:  3.9809741973876953 0.14504782855510712
CurrentTrain: epoch  4, batch    65 | loss: 4.1260219Losses:  4.082233428955078 0.08252482116222382
CurrentTrain: epoch  4, batch    66 | loss: 4.1647582Losses:  4.103677749633789 0.06314367055892944
CurrentTrain: epoch  4, batch    67 | loss: 4.1668215Losses:  4.054564476013184 0.06011395901441574
CurrentTrain: epoch  4, batch    68 | loss: 4.1146784Losses:  3.9670703411102295 0.13682636618614197
CurrentTrain: epoch  4, batch    69 | loss: 4.1038966Losses:  3.9787163734436035 0.09560263901948929
CurrentTrain: epoch  4, batch    70 | loss: 4.0743189Losses:  3.9519567489624023 0.08874303102493286
CurrentTrain: epoch  4, batch    71 | loss: 4.0407000Losses:  4.061042785644531 0.06401567161083221
CurrentTrain: epoch  4, batch    72 | loss: 4.1250587Losses:  4.01075553894043 0.11007799208164215
CurrentTrain: epoch  4, batch    73 | loss: 4.1208334Losses:  4.066944599151611 0.10891440510749817
CurrentTrain: epoch  4, batch    74 | loss: 4.1758590Losses:  4.040371894836426 0.12903299927711487
CurrentTrain: epoch  4, batch    75 | loss: 4.1694050Losses:  4.015326976776123 0.05532583221793175
CurrentTrain: epoch  4, batch    76 | loss: 4.0706530Losses:  3.994373321533203 0.06598081439733505
CurrentTrain: epoch  4, batch    77 | loss: 4.0603542Losses:  4.030856132507324 0.08148831874132156
CurrentTrain: epoch  4, batch    78 | loss: 4.1123443Losses:  4.023540496826172 0.10265398025512695
CurrentTrain: epoch  4, batch    79 | loss: 4.1261945Losses:  4.012969017028809 0.08662131428718567
CurrentTrain: epoch  4, batch    80 | loss: 4.0995903Losses:  3.958047866821289 0.11717404425144196
CurrentTrain: epoch  4, batch    81 | loss: 4.0752220Losses:  3.9878907203674316 0.1301225870847702
CurrentTrain: epoch  4, batch    82 | loss: 4.1180134Losses:  3.9777514934539795 0.1560434103012085
CurrentTrain: epoch  4, batch    83 | loss: 4.1337948Losses:  3.9605727195739746 0.07779280096292496
CurrentTrain: epoch  4, batch    84 | loss: 4.0383654Losses:  4.052422046661377 0.06855957210063934
CurrentTrain: epoch  4, batch    85 | loss: 4.1209817Losses:  4.034069061279297 0.10733570158481598
CurrentTrain: epoch  4, batch    86 | loss: 4.1414046Losses:  3.973428726196289 0.1393899917602539
CurrentTrain: epoch  4, batch    87 | loss: 4.1128187Losses:  4.044241428375244 0.0750957652926445
CurrentTrain: epoch  4, batch    88 | loss: 4.1193371Losses:  4.0261969566345215 0.06675353646278381
CurrentTrain: epoch  4, batch    89 | loss: 4.0929503Losses:  3.988255262374878 0.12205900996923447
CurrentTrain: epoch  4, batch    90 | loss: 4.1103144Losses:  4.038379192352295 0.05979268252849579
CurrentTrain: epoch  4, batch    91 | loss: 4.0981717Losses:  4.012872695922852 0.06555287539958954
CurrentTrain: epoch  4, batch    92 | loss: 4.0784254Losses:  3.9287667274475098 0.13232967257499695
CurrentTrain: epoch  4, batch    93 | loss: 4.0610962Losses:  4.05839729309082 0.05041546747088432
CurrentTrain: epoch  4, batch    94 | loss: 4.1088128Losses:  4.001435279846191 0.13853047788143158
CurrentTrain: epoch  4, batch    95 | loss: 4.1399655Losses:  3.925246000289917 0.036337509751319885
CurrentTrain: epoch  4, batch    96 | loss: 3.9615836Losses:  4.011364936828613 0.14050044119358063
CurrentTrain: epoch  4, batch    97 | loss: 4.1518655Losses:  4.003248691558838 0.111753910779953
CurrentTrain: epoch  4, batch    98 | loss: 4.1150026Losses:  4.023024559020996 0.056168898940086365
CurrentTrain: epoch  4, batch    99 | loss: 4.0791936Losses:  4.071573257446289 0.05802459269762039
CurrentTrain: epoch  4, batch   100 | loss: 4.1295977Losses:  4.014863014221191 0.09996551275253296
CurrentTrain: epoch  4, batch   101 | loss: 4.1148286Losses:  4.0127763748168945 0.05952851474285126
CurrentTrain: epoch  4, batch   102 | loss: 4.0723047Losses:  3.8421084880828857 0.07408533990383148
CurrentTrain: epoch  4, batch   103 | loss: 3.9161937Losses:  3.9931087493896484 0.08375140279531479
CurrentTrain: epoch  4, batch   104 | loss: 4.0768600Losses:  3.9448699951171875 0.07383144646883011
CurrentTrain: epoch  4, batch   105 | loss: 4.0187016Losses:  4.006086826324463 0.05953415855765343
CurrentTrain: epoch  4, batch   106 | loss: 4.0656209Losses:  3.9995360374450684 0.0699048638343811
CurrentTrain: epoch  4, batch   107 | loss: 4.0694408Losses:  3.9392828941345215 0.12790417671203613
CurrentTrain: epoch  4, batch   108 | loss: 4.0671873Losses:  3.987199306488037 0.0698436051607132
CurrentTrain: epoch  4, batch   109 | loss: 4.0570431Losses:  3.9522440433502197 0.06475798785686493
CurrentTrain: epoch  4, batch   110 | loss: 4.0170021Losses:  4.022402286529541 0.08093184232711792
CurrentTrain: epoch  4, batch   111 | loss: 4.1033340Losses:  3.9498281478881836 0.0572674497961998
CurrentTrain: epoch  4, batch   112 | loss: 4.0070958Losses:  3.9308485984802246 0.03977435082197189
CurrentTrain: epoch  4, batch   113 | loss: 3.9706230Losses:  4.053745746612549 0.08964070677757263
CurrentTrain: epoch  4, batch   114 | loss: 4.1433864Losses:  3.9057886600494385 0.037996452301740646
CurrentTrain: epoch  4, batch   115 | loss: 3.9437852Losses:  3.989393711090088 0.1003580093383789
CurrentTrain: epoch  4, batch   116 | loss: 4.0897517Losses:  3.966512680053711 0.12053106725215912
CurrentTrain: epoch  4, batch   117 | loss: 4.0870438Losses:  3.968780755996704 0.10312403738498688
CurrentTrain: epoch  4, batch   118 | loss: 4.0719047Losses:  4.028237819671631 0.06052669137716293
CurrentTrain: epoch  4, batch   119 | loss: 4.0887647Losses:  3.9819674491882324 0.06594562530517578
CurrentTrain: epoch  4, batch   120 | loss: 4.0479131Losses:  3.9475255012512207 0.08072290569543839
CurrentTrain: epoch  4, batch   121 | loss: 4.0282483Losses:  3.891753911972046 0.034987833350896835
CurrentTrain: epoch  4, batch   122 | loss: 3.9267418Losses:  4.050821781158447 0.06059644743800163
CurrentTrain: epoch  4, batch   123 | loss: 4.1114182Losses:  3.995652914047241 0.06685711443424225
CurrentTrain: epoch  4, batch   124 | loss: 4.0625100Losses:  3.9881606101989746 0.09785906970500946
CurrentTrain: epoch  5, batch     0 | loss: 4.0860195Losses:  3.980046272277832 0.06812753528356552
CurrentTrain: epoch  5, batch     1 | loss: 4.0481739Losses:  3.9143495559692383 0.043673641979694366
CurrentTrain: epoch  5, batch     2 | loss: 3.9580233Losses:  3.964704751968384 0.0628381296992302
CurrentTrain: epoch  5, batch     3 | loss: 4.0275431Losses:  4.062771797180176 0.0684099942445755
CurrentTrain: epoch  5, batch     4 | loss: 4.1311817Losses:  4.01533842086792 0.09940040856599808
CurrentTrain: epoch  5, batch     5 | loss: 4.1147389Losses:  3.9553632736206055 0.07133851945400238
CurrentTrain: epoch  5, batch     6 | loss: 4.0267019Losses:  3.984299659729004 0.09487481415271759
CurrentTrain: epoch  5, batch     7 | loss: 4.0791745Losses:  3.9494566917419434 0.07605264335870743
CurrentTrain: epoch  5, batch     8 | loss: 4.0255094Losses:  3.9500269889831543 0.04070991277694702
CurrentTrain: epoch  5, batch     9 | loss: 3.9907370Losses:  3.9877843856811523 0.0649246871471405
CurrentTrain: epoch  5, batch    10 | loss: 4.0527091Losses:  3.955050468444824 0.030127227306365967
CurrentTrain: epoch  5, batch    11 | loss: 3.9851778Losses:  3.9483895301818848 0.07508006691932678
CurrentTrain: epoch  5, batch    12 | loss: 4.0234694Losses:  4.0194268226623535 0.0945873111486435
CurrentTrain: epoch  5, batch    13 | loss: 4.1140141Losses:  4.025380611419678 0.0855337381362915
CurrentTrain: epoch  5, batch    14 | loss: 4.1109142Losses:  3.98996639251709 0.12916389107704163
CurrentTrain: epoch  5, batch    15 | loss: 4.1191301Losses:  3.932295799255371 0.0907747894525528
CurrentTrain: epoch  5, batch    16 | loss: 4.0230708Losses:  3.9722628593444824 0.08134284615516663
CurrentTrain: epoch  5, batch    17 | loss: 4.0536056Losses:  3.9899678230285645 0.06710716336965561
CurrentTrain: epoch  5, batch    18 | loss: 4.0570750Losses:  4.0150885581970215 0.0760650485754013
CurrentTrain: epoch  5, batch    19 | loss: 4.0911536Losses:  3.9883992671966553 0.07412637770175934
CurrentTrain: epoch  5, batch    20 | loss: 4.0625257Losses:  4.031436443328857 0.048542458564043045
CurrentTrain: epoch  5, batch    21 | loss: 4.0799789Losses:  3.956681251525879 0.09658527374267578
CurrentTrain: epoch  5, batch    22 | loss: 4.0532665Losses:  3.9801018238067627 0.06061097979545593
CurrentTrain: epoch  5, batch    23 | loss: 4.0407128Losses:  4.01334285736084 0.06576866656541824
CurrentTrain: epoch  5, batch    24 | loss: 4.0791116Losses:  3.9898626804351807 0.06555599719285965
CurrentTrain: epoch  5, batch    25 | loss: 4.0554185Losses:  4.004360675811768 0.07831165194511414
CurrentTrain: epoch  5, batch    26 | loss: 4.0826721Losses:  4.013723373413086 0.05936224013566971
CurrentTrain: epoch  5, batch    27 | loss: 4.0730858Losses:  4.000250816345215 0.061095111072063446
CurrentTrain: epoch  5, batch    28 | loss: 4.0613461Losses:  3.9563403129577637 0.021405518054962158
CurrentTrain: epoch  5, batch    29 | loss: 3.9777458Losses:  3.975139617919922 0.0765622928738594
CurrentTrain: epoch  5, batch    30 | loss: 4.0517020Losses:  3.963669538497925 0.08003342151641846
CurrentTrain: epoch  5, batch    31 | loss: 4.0437031Losses:  3.9339284896850586 0.06850741058588028
CurrentTrain: epoch  5, batch    32 | loss: 4.0024357Losses:  3.867260217666626 0.06596413254737854
CurrentTrain: epoch  5, batch    33 | loss: 3.9332244Losses:  3.957030773162842 0.05738656222820282
CurrentTrain: epoch  5, batch    34 | loss: 4.0144172Losses:  3.9879841804504395 0.07714994251728058
CurrentTrain: epoch  5, batch    35 | loss: 4.0651340Losses:  3.9311869144439697 0.05917436629533768
CurrentTrain: epoch  5, batch    36 | loss: 3.9903612Losses:  3.9679036140441895 0.03781053423881531
CurrentTrain: epoch  5, batch    37 | loss: 4.0057139Losses:  4.01229190826416 0.09175097197294235
CurrentTrain: epoch  5, batch    38 | loss: 4.1040430Losses:  3.961670398712158 0.09454582631587982
CurrentTrain: epoch  5, batch    39 | loss: 4.0562162Losses:  4.012261390686035 0.07847413420677185
CurrentTrain: epoch  5, batch    40 | loss: 4.0907354Losses:  3.9461636543273926 0.07456669956445694
CurrentTrain: epoch  5, batch    41 | loss: 4.0207305Losses:  3.9227919578552246 0.07303471118211746
CurrentTrain: epoch  5, batch    42 | loss: 3.9958267Losses:  3.9735183715820312 0.06318167597055435
CurrentTrain: epoch  5, batch    43 | loss: 4.0367002Losses:  3.9451661109924316 0.07826158404350281
CurrentTrain: epoch  5, batch    44 | loss: 4.0234275Losses:  3.9509711265563965 0.06141528859734535
CurrentTrain: epoch  5, batch    45 | loss: 4.0123863Losses:  3.973947763442993 0.058297060430049896
CurrentTrain: epoch  5, batch    46 | loss: 4.0322447Losses:  3.9399590492248535 0.06859204918146133
CurrentTrain: epoch  5, batch    47 | loss: 4.0085511Losses:  3.9884347915649414 0.11088085174560547
CurrentTrain: epoch  5, batch    48 | loss: 4.0993156Losses:  3.918705463409424 0.06005649268627167
CurrentTrain: epoch  5, batch    49 | loss: 3.9787619Losses:  3.9334282875061035 0.053546153008937836
CurrentTrain: epoch  5, batch    50 | loss: 3.9869745Losses:  3.9153504371643066 0.07003086805343628
CurrentTrain: epoch  5, batch    51 | loss: 3.9853814Losses:  3.975787401199341 0.0667097195982933
CurrentTrain: epoch  5, batch    52 | loss: 4.0424972Losses:  3.992292881011963 0.061285316944122314
CurrentTrain: epoch  5, batch    53 | loss: 4.0535784Losses:  4.0124359130859375 0.03928074240684509
CurrentTrain: epoch  5, batch    54 | loss: 4.0517168Losses:  3.990954637527466 0.06583864986896515
CurrentTrain: epoch  5, batch    55 | loss: 4.0567932Losses:  4.023102760314941 0.07927503436803818
CurrentTrain: epoch  5, batch    56 | loss: 4.1023779Losses:  3.989960193634033 0.03366084769368172
CurrentTrain: epoch  5, batch    57 | loss: 4.0236211Losses:  3.9626455307006836 0.11669647693634033
CurrentTrain: epoch  5, batch    58 | loss: 4.0793419Losses:  3.9892117977142334 0.046965889632701874
CurrentTrain: epoch  5, batch    59 | loss: 4.0361776Losses:  3.9670159816741943 0.09550359100103378
CurrentTrain: epoch  5, batch    60 | loss: 4.0625196Losses:  3.969301223754883 0.0714995488524437
CurrentTrain: epoch  5, batch    61 | loss: 4.0408006Losses:  3.900717258453369 0.0641268640756607
CurrentTrain: epoch  5, batch    62 | loss: 3.9648442Losses:  3.968022108078003 0.05062972009181976
CurrentTrain: epoch  5, batch    63 | loss: 4.0186520Losses:  3.9522695541381836 0.07562272250652313
CurrentTrain: epoch  5, batch    64 | loss: 4.0278921Losses:  3.9529380798339844 0.07242906093597412
CurrentTrain: epoch  5, batch    65 | loss: 4.0253673Losses:  3.9195454120635986 0.05028984695672989
CurrentTrain: epoch  5, batch    66 | loss: 3.9698353Losses:  3.972066879272461 0.08099375665187836
CurrentTrain: epoch  5, batch    67 | loss: 4.0530605Losses:  3.98533296585083 0.08030270040035248
CurrentTrain: epoch  5, batch    68 | loss: 4.0656357Losses:  3.958433151245117 0.05997698754072189
CurrentTrain: epoch  5, batch    69 | loss: 4.0184102Losses:  3.9426493644714355 0.08010156452655792
CurrentTrain: epoch  5, batch    70 | loss: 4.0227509Losses:  3.970994710922241 0.07299264520406723
CurrentTrain: epoch  5, batch    71 | loss: 4.0439873Losses:  3.9823038578033447 0.11144505441188812
CurrentTrain: epoch  5, batch    72 | loss: 4.0937490Losses:  4.014432907104492 0.04561455547809601
CurrentTrain: epoch  5, batch    73 | loss: 4.0600476Losses:  3.9637842178344727 0.0668574795126915
CurrentTrain: epoch  5, batch    74 | loss: 4.0306416Losses:  3.9827661514282227 0.05649909749627113
CurrentTrain: epoch  5, batch    75 | loss: 4.0392652Losses:  3.984677791595459 0.05503227561712265
CurrentTrain: epoch  5, batch    76 | loss: 4.0397100Losses:  3.9032390117645264 0.10950247943401337
CurrentTrain: epoch  5, batch    77 | loss: 4.0127416Losses:  3.95013165473938 0.07537206262350082
CurrentTrain: epoch  5, batch    78 | loss: 4.0255036Losses:  3.985898017883301 0.08492603898048401
CurrentTrain: epoch  5, batch    79 | loss: 4.0708241Losses:  4.016574859619141 0.1424657702445984
CurrentTrain: epoch  5, batch    80 | loss: 4.1590405Losses:  3.9494118690490723 0.05200870335102081
CurrentTrain: epoch  5, batch    81 | loss: 4.0014205Losses:  3.930424213409424 0.05087653174996376
CurrentTrain: epoch  5, batch    82 | loss: 3.9813008Losses:  3.9966678619384766 0.08356422185897827
CurrentTrain: epoch  5, batch    83 | loss: 4.0802321Losses:  3.962484359741211 0.1151541918516159
CurrentTrain: epoch  5, batch    84 | loss: 4.0776386Losses:  3.9416637420654297 0.08737678825855255
CurrentTrain: epoch  5, batch    85 | loss: 4.0290403Losses:  3.9844629764556885 0.09394469857215881
CurrentTrain: epoch  5, batch    86 | loss: 4.0784078Losses:  3.9683971405029297 0.07475771009922028
CurrentTrain: epoch  5, batch    87 | loss: 4.0431547Losses:  3.9148573875427246 0.08659741282463074
CurrentTrain: epoch  5, batch    88 | loss: 4.0014548Losses:  3.9918601512908936 0.06292390078306198
CurrentTrain: epoch  5, batch    89 | loss: 4.0547838Losses:  3.960947036743164 0.06080779805779457
CurrentTrain: epoch  5, batch    90 | loss: 4.0217547Losses:  3.9652979373931885 0.08015991747379303
CurrentTrain: epoch  5, batch    91 | loss: 4.0454578Losses:  3.9374046325683594 0.05794260650873184
CurrentTrain: epoch  5, batch    92 | loss: 3.9953473Losses:  4.013849258422852 0.05868847668170929
CurrentTrain: epoch  5, batch    93 | loss: 4.0725379Losses:  3.919428825378418 0.04874873906373978
CurrentTrain: epoch  5, batch    94 | loss: 3.9681776Losses:  3.917141914367676 0.10296741873025894
CurrentTrain: epoch  5, batch    95 | loss: 4.0201092Losses:  3.897759437561035 0.04128498584032059
CurrentTrain: epoch  5, batch    96 | loss: 3.9390445Losses:  4.0157694816589355 0.09865844249725342
CurrentTrain: epoch  5, batch    97 | loss: 4.1144280Losses:  3.98288893699646 0.08143683522939682
CurrentTrain: epoch  5, batch    98 | loss: 4.0643258Losses:  3.9931955337524414 0.04661013185977936
CurrentTrain: epoch  5, batch    99 | loss: 4.0398059Losses:  3.9495625495910645 0.0683978945016861
CurrentTrain: epoch  5, batch   100 | loss: 4.0179605Losses:  4.019615173339844 0.05886570364236832
CurrentTrain: epoch  5, batch   101 | loss: 4.0784807Losses:  3.929403781890869 0.06973645091056824
CurrentTrain: epoch  5, batch   102 | loss: 3.9991403Losses:  3.981121301651001 0.06809195131063461
CurrentTrain: epoch  5, batch   103 | loss: 4.0492134Losses:  3.9993672370910645 0.05597926676273346
CurrentTrain: epoch  5, batch   104 | loss: 4.0553465Losses:  3.9603188037872314 0.07770754396915436
CurrentTrain: epoch  5, batch   105 | loss: 4.0380263Losses:  3.9821724891662598 0.06960274279117584
CurrentTrain: epoch  5, batch   106 | loss: 4.0517755Losses:  3.9575135707855225 0.07934819906949997
CurrentTrain: epoch  5, batch   107 | loss: 4.0368619Losses:  4.026528358459473 0.03400759771466255
CurrentTrain: epoch  5, batch   108 | loss: 4.0605359Losses:  3.963887929916382 0.07042767107486725
CurrentTrain: epoch  5, batch   109 | loss: 4.0343156Losses:  3.9628968238830566 0.08184071630239487
CurrentTrain: epoch  5, batch   110 | loss: 4.0447373Losses:  3.953634262084961 0.05637417733669281
CurrentTrain: epoch  5, batch   111 | loss: 4.0100083Losses:  3.926949977874756 0.08874587714672089
CurrentTrain: epoch  5, batch   112 | loss: 4.0156960Losses:  3.9571399688720703 0.04234320670366287
CurrentTrain: epoch  5, batch   113 | loss: 3.9994831Losses:  3.90289306640625 0.06926949322223663
CurrentTrain: epoch  5, batch   114 | loss: 3.9721625Losses:  3.9671237468719482 0.0694928914308548
CurrentTrain: epoch  5, batch   115 | loss: 4.0366168Losses:  3.9313442707061768 0.04227268695831299
CurrentTrain: epoch  5, batch   116 | loss: 3.9736171Losses:  3.9370670318603516 0.05763772502541542
CurrentTrain: epoch  5, batch   117 | loss: 3.9947047Losses:  3.9140751361846924 0.03218153119087219
CurrentTrain: epoch  5, batch   118 | loss: 3.9462566Losses:  3.9408202171325684 0.08820642530918121
CurrentTrain: epoch  5, batch   119 | loss: 4.0290265Losses:  3.951753616333008 0.06737230718135834
CurrentTrain: epoch  5, batch   120 | loss: 4.0191259Losses:  3.961533308029175 0.10194674134254456
CurrentTrain: epoch  5, batch   121 | loss: 4.0634799Losses:  3.9645066261291504 0.11240881681442261
CurrentTrain: epoch  5, batch   122 | loss: 4.0769153Losses:  3.924769163131714 0.0471608005464077
CurrentTrain: epoch  5, batch   123 | loss: 3.9719300Losses:  3.9570465087890625 0.0724371150135994
CurrentTrain: epoch  5, batch   124 | loss: 4.0294838Losses:  3.950265884399414 0.06624643504619598
CurrentTrain: epoch  6, batch     0 | loss: 4.0165124Losses:  3.977377414703369 0.054128497838974
CurrentTrain: epoch  6, batch     1 | loss: 4.0315061Losses:  3.9596567153930664 0.06224588304758072
CurrentTrain: epoch  6, batch     2 | loss: 4.0219026Losses:  3.9496476650238037 0.08288659900426865
CurrentTrain: epoch  6, batch     3 | loss: 4.0325341Losses:  3.9630048274993896 0.04818020015954971
CurrentTrain: epoch  6, batch     4 | loss: 4.0111852Losses:  3.963179349899292 0.061756640672683716
CurrentTrain: epoch  6, batch     5 | loss: 4.0249362Losses:  4.018338203430176 0.10909727215766907
CurrentTrain: epoch  6, batch     6 | loss: 4.1274357Losses:  3.9716176986694336 0.07572847604751587
CurrentTrain: epoch  6, batch     7 | loss: 4.0473461Losses:  3.9641189575195312 0.042551688849925995
CurrentTrain: epoch  6, batch     8 | loss: 4.0066705Losses:  3.940917491912842 0.08240634202957153
CurrentTrain: epoch  6, batch     9 | loss: 4.0233240Losses:  3.9705142974853516 0.06640232354402542
CurrentTrain: epoch  6, batch    10 | loss: 4.0369167Losses:  3.9860548973083496 0.034903012216091156
CurrentTrain: epoch  6, batch    11 | loss: 4.0209579Losses:  3.9746060371398926 0.04479784518480301
CurrentTrain: epoch  6, batch    12 | loss: 4.0194039Losses:  3.9320435523986816 0.049382537603378296
CurrentTrain: epoch  6, batch    13 | loss: 3.9814260Losses:  3.9967844486236572 0.07960063219070435
CurrentTrain: epoch  6, batch    14 | loss: 4.0763850Losses:  3.9485039710998535 0.04641040042042732
CurrentTrain: epoch  6, batch    15 | loss: 3.9949143Losses:  3.9497082233428955 0.09279975295066833
CurrentTrain: epoch  6, batch    16 | loss: 4.0425081Losses:  3.9076967239379883 0.04940946027636528
CurrentTrain: epoch  6, batch    17 | loss: 3.9571061Losses:  3.977491855621338 0.0797751322388649
CurrentTrain: epoch  6, batch    18 | loss: 4.0572672Losses:  3.9102783203125 0.034740179777145386
CurrentTrain: epoch  6, batch    19 | loss: 3.9450185Losses:  3.9887986183166504 0.06346826255321503
CurrentTrain: epoch  6, batch    20 | loss: 4.0522671Losses:  3.982424259185791 0.04200815409421921
CurrentTrain: epoch  6, batch    21 | loss: 4.0244322Losses:  3.9222888946533203 0.04090728238224983
CurrentTrain: epoch  6, batch    22 | loss: 3.9631963Losses:  3.951173782348633 0.04088764637708664
CurrentTrain: epoch  6, batch    23 | loss: 3.9920614Losses:  3.94891095161438 0.06676819920539856
CurrentTrain: epoch  6, batch    24 | loss: 4.0156794Losses:  3.9206807613372803 0.04436536505818367
CurrentTrain: epoch  6, batch    25 | loss: 3.9650462Losses:  3.9598498344421387 0.04885387420654297
CurrentTrain: epoch  6, batch    26 | loss: 4.0087037Losses:  3.89896297454834 0.04962911829352379
CurrentTrain: epoch  6, batch    27 | loss: 3.9485922Losses:  3.9449028968811035 0.06529083102941513
CurrentTrain: epoch  6, batch    28 | loss: 4.0101938Losses:  3.957594871520996 0.050958845764398575
CurrentTrain: epoch  6, batch    29 | loss: 4.0085535Losses:  3.96207857131958 0.05027676373720169
CurrentTrain: epoch  6, batch    30 | loss: 4.0123553Losses:  3.9877779483795166 0.06024898588657379
CurrentTrain: epoch  6, batch    31 | loss: 4.0480270Losses:  3.9632091522216797 0.06850491464138031
CurrentTrain: epoch  6, batch    32 | loss: 4.0317140Losses:  3.9370813369750977 0.06856904923915863
CurrentTrain: epoch  6, batch    33 | loss: 4.0056505Losses:  3.9477217197418213 0.06139563024044037
CurrentTrain: epoch  6, batch    34 | loss: 4.0091171Losses:  3.923206090927124 0.058053672313690186
CurrentTrain: epoch  6, batch    35 | loss: 3.9812598Losses:  3.9083056449890137 0.04040299355983734
CurrentTrain: epoch  6, batch    36 | loss: 3.9487085Losses:  3.9414546489715576 0.07294554263353348
CurrentTrain: epoch  6, batch    37 | loss: 4.0144000Losses:  3.9826059341430664 0.07987033575773239
CurrentTrain: epoch  6, batch    38 | loss: 4.0624762Losses:  3.9143552780151367 0.058690354228019714
CurrentTrain: epoch  6, batch    39 | loss: 3.9730456Losses:  3.94726300239563 0.10871320962905884
CurrentTrain: epoch  6, batch    40 | loss: 4.0559764Losses:  3.966146469116211 0.079490527510643
CurrentTrain: epoch  6, batch    41 | loss: 4.0456371Losses:  3.9199092388153076 0.07616059482097626
CurrentTrain: epoch  6, batch    42 | loss: 3.9960699Losses:  3.9781105518341064 0.09527271240949631
CurrentTrain: epoch  6, batch    43 | loss: 4.0733833Losses:  3.9506258964538574 0.09932079911231995
CurrentTrain: epoch  6, batch    44 | loss: 4.0499468Losses:  3.8992490768432617 0.0367310494184494
CurrentTrain: epoch  6, batch    45 | loss: 3.9359801Losses:  3.9164676666259766 0.03994870185852051
CurrentTrain: epoch  6, batch    46 | loss: 3.9564164Losses:  3.951570749282837 0.05239023268222809
CurrentTrain: epoch  6, batch    47 | loss: 4.0039611Losses:  3.9131507873535156 0.040238332003355026
CurrentTrain: epoch  6, batch    48 | loss: 3.9533892Losses:  3.959609031677246 0.09399653226137161
CurrentTrain: epoch  6, batch    49 | loss: 4.0536056Losses:  3.9351959228515625 0.05483276769518852
CurrentTrain: epoch  6, batch    50 | loss: 3.9900286Losses:  3.9671072959899902 0.06856203079223633
CurrentTrain: epoch  6, batch    51 | loss: 4.0356693Losses:  3.969620704650879 0.04217495024204254
CurrentTrain: epoch  6, batch    52 | loss: 4.0117955Losses:  3.9875447750091553 0.05785548686981201
CurrentTrain: epoch  6, batch    53 | loss: 4.0454001Losses:  3.84586763381958 0.055700916796922684
CurrentTrain: epoch  6, batch    54 | loss: 3.9015687Losses:  3.8627874851226807 0.0509161651134491
CurrentTrain: epoch  6, batch    55 | loss: 3.9137037Losses:  3.9385986328125 0.055039387196302414
CurrentTrain: epoch  6, batch    56 | loss: 3.9936380Losses:  3.9633865356445312 0.0439775176346302
CurrentTrain: epoch  6, batch    57 | loss: 4.0073643Losses:  3.9484524726867676 0.06921611726284027
CurrentTrain: epoch  6, batch    58 | loss: 4.0176687Losses:  4.045172691345215 0.03669923171401024
CurrentTrain: epoch  6, batch    59 | loss: 4.0818720Losses:  3.949409246444702 0.03588850796222687
CurrentTrain: epoch  6, batch    60 | loss: 3.9852977Losses:  3.97643780708313 0.05098755657672882
CurrentTrain: epoch  6, batch    61 | loss: 4.0274253Losses:  3.928887128829956 0.05018053948879242
CurrentTrain: epoch  6, batch    62 | loss: 3.9790676Losses:  3.9628853797912598 0.0679449662566185
CurrentTrain: epoch  6, batch    63 | loss: 4.0308304Losses:  3.901876449584961 0.07573820650577545
CurrentTrain: epoch  6, batch    64 | loss: 3.9776146Losses:  3.9108312129974365 0.03709038719534874
CurrentTrain: epoch  6, batch    65 | loss: 3.9479215Losses:  3.942349910736084 0.07949432730674744
CurrentTrain: epoch  6, batch    66 | loss: 4.0218444Losses:  3.956627368927002 0.07585301995277405
CurrentTrain: epoch  6, batch    67 | loss: 4.0324802Losses:  3.9818458557128906 0.07840217649936676
CurrentTrain: epoch  6, batch    68 | loss: 4.0602479Losses:  4.590822696685791 0.1554078906774521
CurrentTrain: epoch  6, batch    69 | loss: 4.7462306Losses:  3.876563549041748 0.04553920775651932
CurrentTrain: epoch  6, batch    70 | loss: 3.9221027Losses:  3.991464138031006 0.05703812465071678
CurrentTrain: epoch  6, batch    71 | loss: 4.0485024Losses:  3.9846153259277344 0.06970487534999847
CurrentTrain: epoch  6, batch    72 | loss: 4.0543203Losses:  3.978494167327881 0.018646469339728355
CurrentTrain: epoch  6, batch    73 | loss: 3.9971406Losses:  5.097226142883301 0.1314878761768341
CurrentTrain: epoch  6, batch    74 | loss: 5.2287140Losses:  3.9216885566711426 0.047122806310653687
CurrentTrain: epoch  6, batch    75 | loss: 3.9688113Losses:  3.9658398628234863 0.05167929828166962
CurrentTrain: epoch  6, batch    76 | loss: 4.0175190Losses:  4.064091682434082 0.07385751605033875
CurrentTrain: epoch  6, batch    77 | loss: 4.1379490Losses:  3.9862070083618164 0.05544909089803696
CurrentTrain: epoch  6, batch    78 | loss: 4.0416560Losses:  4.43114709854126 0.15299005806446075
CurrentTrain: epoch  6, batch    79 | loss: 4.5841370Losses:  3.9484596252441406 0.06687885522842407
CurrentTrain: epoch  6, batch    80 | loss: 4.0153384Losses:  4.036011695861816 0.06093166023492813
CurrentTrain: epoch  6, batch    81 | loss: 4.0969434Losses:  4.152397155761719 0.07438726723194122
CurrentTrain: epoch  6, batch    82 | loss: 4.2267842Losses:  3.9290027618408203 0.030910367146134377
CurrentTrain: epoch  6, batch    83 | loss: 3.9599130Losses:  4.097886085510254 0.04383210465312004
CurrentTrain: epoch  6, batch    84 | loss: 4.1417184Losses:  3.9502766132354736 0.05052810534834862
CurrentTrain: epoch  6, batch    85 | loss: 4.0008049Losses:  4.003623008728027 0.08225859701633453
CurrentTrain: epoch  6, batch    86 | loss: 4.0858817Losses:  4.012645244598389 0.09396572411060333
CurrentTrain: epoch  6, batch    87 | loss: 4.1066108Losses:  3.952807664871216 0.0475454218685627
CurrentTrain: epoch  6, batch    88 | loss: 4.0003529Losses:  4.014750003814697 0.04871436581015587
CurrentTrain: epoch  6, batch    89 | loss: 4.0634642Losses:  4.102270603179932 0.09987437725067139
CurrentTrain: epoch  6, batch    90 | loss: 4.2021451Losses:  3.9765772819519043 0.047802671790122986
CurrentTrain: epoch  6, batch    91 | loss: 4.0243797Losses:  3.933650255203247 0.06876444071531296
CurrentTrain: epoch  6, batch    92 | loss: 4.0024147Losses:  3.9417543411254883 0.11163219809532166
CurrentTrain: epoch  6, batch    93 | loss: 4.0533867Losses:  3.9706215858459473 0.06763206422328949
CurrentTrain: epoch  6, batch    94 | loss: 4.0382538Losses:  3.9674689769744873 0.03769419342279434
CurrentTrain: epoch  6, batch    95 | loss: 4.0051632Losses:  4.0912299156188965 0.05646080896258354
CurrentTrain: epoch  6, batch    96 | loss: 4.1476908Losses:  3.927483558654785 0.06532653421163559
CurrentTrain: epoch  6, batch    97 | loss: 3.9928100Losses:  4.030810356140137 0.06384143233299255
CurrentTrain: epoch  6, batch    98 | loss: 4.0946517Losses:  4.041380882263184 0.10632828623056412
CurrentTrain: epoch  6, batch    99 | loss: 4.1477094Losses:  3.92872953414917 0.047509998083114624
CurrentTrain: epoch  6, batch   100 | loss: 3.9762394Losses:  4.003430366516113 0.05993904173374176
CurrentTrain: epoch  6, batch   101 | loss: 4.0633693Losses:  3.9691219329833984 0.07148877531290054
CurrentTrain: epoch  6, batch   102 | loss: 4.0406108Losses:  3.9005212783813477 0.02882556989789009
CurrentTrain: epoch  6, batch   103 | loss: 3.9293468Losses:  4.002910614013672 0.08492852747440338
CurrentTrain: epoch  6, batch   104 | loss: 4.0878391Losses:  4.044903755187988 0.07338107377290726
CurrentTrain: epoch  6, batch   105 | loss: 4.1182847Losses:  3.97477388381958 0.07671003043651581
CurrentTrain: epoch  6, batch   106 | loss: 4.0514841Losses:  4.122771263122559 0.052251119166612625
CurrentTrain: epoch  6, batch   107 | loss: 4.1750226Losses:  4.007061004638672 0.0771745890378952
CurrentTrain: epoch  6, batch   108 | loss: 4.0842357Losses:  3.965726852416992 0.09342309832572937
CurrentTrain: epoch  6, batch   109 | loss: 4.0591497Losses:  4.0199785232543945 0.04610350355505943
CurrentTrain: epoch  6, batch   110 | loss: 4.0660820Losses:  3.939732551574707 0.056395575404167175
CurrentTrain: epoch  6, batch   111 | loss: 3.9961281Losses:  4.003302097320557 0.03732195869088173
CurrentTrain: epoch  6, batch   112 | loss: 4.0406241Losses:  4.006570339202881 0.056983642280101776
CurrentTrain: epoch  6, batch   113 | loss: 4.0635538Losses:  5.386116981506348 0.4998164772987366
CurrentTrain: epoch  6, batch   114 | loss: 5.8859334Losses:  3.959702253341675 0.060252007097005844
CurrentTrain: epoch  6, batch   115 | loss: 4.0199542Losses:  4.000975131988525 0.07167673110961914
CurrentTrain: epoch  6, batch   116 | loss: 4.0726519Losses:  4.197804927825928 0.04398302733898163
CurrentTrain: epoch  6, batch   117 | loss: 4.2417879Losses:  4.034381866455078 0.05974140763282776
CurrentTrain: epoch  6, batch   118 | loss: 4.0941234Losses:  4.081879615783691 0.1532244235277176
CurrentTrain: epoch  6, batch   119 | loss: 4.2351041Losses:  4.011545658111572 0.05571366474032402
CurrentTrain: epoch  6, batch   120 | loss: 4.0672593Losses:  4.245100975036621 0.05355826020240784
CurrentTrain: epoch  6, batch   121 | loss: 4.2986593Losses:  3.9840636253356934 0.07560612261295319
CurrentTrain: epoch  6, batch   122 | loss: 4.0596700Losses:  4.245961666107178 0.026592593640089035
CurrentTrain: epoch  6, batch   123 | loss: 4.2725544Losses:  3.989737033843994 0.055223751813173294
CurrentTrain: epoch  6, batch   124 | loss: 4.0449610Losses:  4.313125133514404 0.04515628516674042
CurrentTrain: epoch  7, batch     0 | loss: 4.3582816Losses:  4.019425392150879 0.09806597977876663
CurrentTrain: epoch  7, batch     1 | loss: 4.1174912Losses:  4.107800006866455 0.06731103360652924
CurrentTrain: epoch  7, batch     2 | loss: 4.1751108Losses:  3.973846435546875 0.03527441620826721
CurrentTrain: epoch  7, batch     3 | loss: 4.0091209Losses:  4.036882400512695 0.04727781191468239
CurrentTrain: epoch  7, batch     4 | loss: 4.0841603Losses:  4.124148845672607 0.051945365965366364
CurrentTrain: epoch  7, batch     5 | loss: 4.1760941Losses:  4.076383590698242 0.07015097141265869
CurrentTrain: epoch  7, batch     6 | loss: 4.1465344Losses:  4.017005920410156 0.08913226425647736
CurrentTrain: epoch  7, batch     7 | loss: 4.1061382Losses:  4.152897834777832 0.09217981994152069
CurrentTrain: epoch  7, batch     8 | loss: 4.2450776Losses:  3.922687530517578 0.038626737892627716
CurrentTrain: epoch  7, batch     9 | loss: 3.9613142Losses:  4.196610927581787 0.07676564157009125
CurrentTrain: epoch  7, batch    10 | loss: 4.2733765Losses:  3.980098247528076 0.0879506841301918
CurrentTrain: epoch  7, batch    11 | loss: 4.0680490Losses:  4.026546478271484 0.06294873356819153
CurrentTrain: epoch  7, batch    12 | loss: 4.0894952Losses:  3.936281204223633 0.032377663999795914
CurrentTrain: epoch  7, batch    13 | loss: 3.9686589Losses:  3.9647645950317383 0.060059804469347
CurrentTrain: epoch  7, batch    14 | loss: 4.0248246Losses:  4.062556266784668 0.040035054087638855
CurrentTrain: epoch  7, batch    15 | loss: 4.1025915Losses:  3.9721126556396484 0.03625566139817238
CurrentTrain: epoch  7, batch    16 | loss: 4.0083685Losses:  4.05683708190918 0.08796144276857376
CurrentTrain: epoch  7, batch    17 | loss: 4.1447988Losses:  3.9713027477264404 0.054130569100379944
CurrentTrain: epoch  7, batch    18 | loss: 4.0254335Losses:  3.955890655517578 0.05137307941913605
CurrentTrain: epoch  7, batch    19 | loss: 4.0072637Losses:  4.127535343170166 0.09937135130167007
CurrentTrain: epoch  7, batch    20 | loss: 4.2269068Losses:  4.037076950073242 0.042139679193496704
CurrentTrain: epoch  7, batch    21 | loss: 4.0792165Losses:  4.0707597732543945 0.06155707687139511
CurrentTrain: epoch  7, batch    22 | loss: 4.1323171Losses:  3.9241909980773926 0.041885364800691605
CurrentTrain: epoch  7, batch    23 | loss: 3.9660764Losses:  3.9778552055358887 0.062024351209402084
CurrentTrain: epoch  7, batch    24 | loss: 4.0398793Losses:  3.9528093338012695 0.06158183515071869
CurrentTrain: epoch  7, batch    25 | loss: 4.0143909Losses:  4.112588882446289 0.04916825890541077
CurrentTrain: epoch  7, batch    26 | loss: 4.1617570Losses:  3.9313931465148926 0.04358161240816116
CurrentTrain: epoch  7, batch    27 | loss: 3.9749749Losses:  4.100669860839844 0.09937506169080734
CurrentTrain: epoch  7, batch    28 | loss: 4.2000451Losses:  3.96820068359375 0.0360955074429512
CurrentTrain: epoch  7, batch    29 | loss: 4.0042963Losses:  4.103436470031738 0.06786645948886871
CurrentTrain: epoch  7, batch    30 | loss: 4.1713028Losses:  3.9381117820739746 0.04807901009917259
CurrentTrain: epoch  7, batch    31 | loss: 3.9861908Losses:  4.08547306060791 0.05928332731127739
CurrentTrain: epoch  7, batch    32 | loss: 4.1447563Losses:  4.006002426147461 0.03941013664007187
CurrentTrain: epoch  7, batch    33 | loss: 4.0454125Losses:  3.963782548904419 0.03532017767429352
CurrentTrain: epoch  7, batch    34 | loss: 3.9991028Losses:  4.998085021972656 0.3224782645702362
CurrentTrain: epoch  7, batch    35 | loss: 5.3205633Losses:  3.944723129272461 0.03991514816880226
CurrentTrain: epoch  7, batch    36 | loss: 3.9846382Losses:  3.9838976860046387 0.07053113728761673
CurrentTrain: epoch  7, batch    37 | loss: 4.0544291Losses:  4.026279926300049 0.03780393674969673
CurrentTrain: epoch  7, batch    38 | loss: 4.0640841Losses:  4.029370307922363 0.04973236843943596
CurrentTrain: epoch  7, batch    39 | loss: 4.0791025Losses:  4.008153915405273 0.03477252274751663
CurrentTrain: epoch  7, batch    40 | loss: 4.0429263Losses:  3.9341177940368652 0.046969518065452576
CurrentTrain: epoch  7, batch    41 | loss: 3.9810872Losses:  3.954427480697632 0.03750636428594589
CurrentTrain: epoch  7, batch    42 | loss: 3.9919338Losses:  4.046876430511475 0.03241613879799843
CurrentTrain: epoch  7, batch    43 | loss: 4.0792928Losses:  3.941073417663574 0.029227443039417267
CurrentTrain: epoch  7, batch    44 | loss: 3.9703009Losses:  4.167233467102051 0.07245449721813202
CurrentTrain: epoch  7, batch    45 | loss: 4.2396879Losses:  3.9283101558685303 0.08345408737659454
CurrentTrain: epoch  7, batch    46 | loss: 4.0117640Losses:  4.2208662033081055 0.11161878705024719
CurrentTrain: epoch  7, batch    47 | loss: 4.3324852Losses:  3.9517948627471924 0.07857055217027664
CurrentTrain: epoch  7, batch    48 | loss: 4.0303655Losses:  3.9444992542266846 0.047155290842056274
CurrentTrain: epoch  7, batch    49 | loss: 3.9916546Losses:  3.911370277404785 0.04345870018005371
CurrentTrain: epoch  7, batch    50 | loss: 3.9548290Losses:  3.970893621444702 0.02748807519674301
CurrentTrain: epoch  7, batch    51 | loss: 3.9983816Losses:  3.9448277950286865 0.04723510146141052
CurrentTrain: epoch  7, batch    52 | loss: 3.9920628Losses:  3.9814558029174805 0.06367575377225876
CurrentTrain: epoch  7, batch    53 | loss: 4.0451317Losses:  4.058956623077393 0.041461553424596786
CurrentTrain: epoch  7, batch    54 | loss: 4.1004181Losses:  4.011114120483398 0.10826730728149414
CurrentTrain: epoch  7, batch    55 | loss: 4.1193814Losses:  3.983255386352539 0.03485923632979393
CurrentTrain: epoch  7, batch    56 | loss: 4.0181146Losses:  3.9701859951019287 0.07605332136154175
CurrentTrain: epoch  7, batch    57 | loss: 4.0462394Losses:  4.068221092224121 0.05963393300771713
CurrentTrain: epoch  7, batch    58 | loss: 4.1278548Losses:  3.993173122406006 0.06437505781650543
CurrentTrain: epoch  7, batch    59 | loss: 4.0575480Losses:  4.003222942352295 0.09175986796617508
CurrentTrain: epoch  7, batch    60 | loss: 4.0949826Losses:  3.995521068572998 0.01646972820162773
CurrentTrain: epoch  7, batch    61 | loss: 4.0119910Losses:  3.969770669937134 0.07391351461410522
CurrentTrain: epoch  7, batch    62 | loss: 4.0436840Losses:  4.049750328063965 0.04092046618461609
CurrentTrain: epoch  7, batch    63 | loss: 4.0906706Losses:  3.8768293857574463 0.03166695311665535
CurrentTrain: epoch  7, batch    64 | loss: 3.9084964Losses:  3.984184741973877 0.07775583863258362
CurrentTrain: epoch  7, batch    65 | loss: 4.0619407Losses:  3.949493885040283 0.05855337902903557
CurrentTrain: epoch  7, batch    66 | loss: 4.0080471Losses:  3.972557306289673 0.07706481963396072
CurrentTrain: epoch  7, batch    67 | loss: 4.0496221Losses:  4.020588397979736 0.03928976133465767
CurrentTrain: epoch  7, batch    68 | loss: 4.0598783Losses:  3.942760467529297 0.05018770694732666
CurrentTrain: epoch  7, batch    69 | loss: 3.9929481Losses:  3.9228503704071045 0.04845177009701729
CurrentTrain: epoch  7, batch    70 | loss: 3.9713020Losses:  4.059139251708984 0.03099377453327179
CurrentTrain: epoch  7, batch    71 | loss: 4.0901332Losses:  3.9449312686920166 0.060340020805597305
CurrentTrain: epoch  7, batch    72 | loss: 4.0052714Losses:  3.912238597869873 0.09163876622915268
CurrentTrain: epoch  7, batch    73 | loss: 4.0038772Losses:  4.013983249664307 0.07457147538661957
CurrentTrain: epoch  7, batch    74 | loss: 4.0885549Losses:  3.956663131713867 0.041302092373371124
CurrentTrain: epoch  7, batch    75 | loss: 3.9979653Losses:  3.8797333240509033 0.016444234177470207
CurrentTrain: epoch  7, batch    76 | loss: 3.8961775Losses:  3.9746484756469727 0.04426062852144241
CurrentTrain: epoch  7, batch    77 | loss: 4.0189090Losses:  3.916354179382324 0.035431861877441406
CurrentTrain: epoch  7, batch    78 | loss: 3.9517860Losses:  3.9781739711761475 0.05751591920852661
CurrentTrain: epoch  7, batch    79 | loss: 4.0356898Losses:  3.954035758972168 0.04192398861050606
CurrentTrain: epoch  7, batch    80 | loss: 3.9959598Losses:  3.912370204925537 0.01951732113957405
CurrentTrain: epoch  7, batch    81 | loss: 3.9318876Losses:  4.014195919036865 0.049966033548116684
CurrentTrain: epoch  7, batch    82 | loss: 4.0641618Losses:  4.027948379516602 0.050277501344680786
CurrentTrain: epoch  7, batch    83 | loss: 4.0782261Losses:  3.968121290206909 0.07244214415550232
CurrentTrain: epoch  7, batch    84 | loss: 4.0405636Losses:  4.04305362701416 0.05998637527227402
CurrentTrain: epoch  7, batch    85 | loss: 4.1030402Losses:  3.9994304180145264 0.030639640986919403
CurrentTrain: epoch  7, batch    86 | loss: 4.0300698Losses:  3.969658613204956 0.05414161831140518
CurrentTrain: epoch  7, batch    87 | loss: 4.0238004Losses:  3.969998359680176 0.07608906924724579
CurrentTrain: epoch  7, batch    88 | loss: 4.0460873Losses:  3.916131019592285 0.03534012287855148
CurrentTrain: epoch  7, batch    89 | loss: 3.9514711Losses:  3.9478871822357178 0.050913285464048386
CurrentTrain: epoch  7, batch    90 | loss: 3.9988005Losses:  3.974517583847046 0.061661671847105026
CurrentTrain: epoch  7, batch    91 | loss: 4.0361791Losses:  3.9867639541625977 0.047212786972522736
CurrentTrain: epoch  7, batch    92 | loss: 4.0339766Losses:  3.9477720260620117 0.031582824885845184
CurrentTrain: epoch  7, batch    93 | loss: 3.9793549Losses:  3.9467735290527344 0.041432589292526245
CurrentTrain: epoch  7, batch    94 | loss: 3.9882061Losses:  4.008516788482666 0.043831534683704376
CurrentTrain: epoch  7, batch    95 | loss: 4.0523481Losses:  3.981855869293213 0.06271761655807495
CurrentTrain: epoch  7, batch    96 | loss: 4.0445733Losses:  4.040844440460205 0.050407566130161285
CurrentTrain: epoch  7, batch    97 | loss: 4.0912519Losses:  3.975398540496826 0.055156223475933075
CurrentTrain: epoch  7, batch    98 | loss: 4.0305548Losses:  3.9817452430725098 0.05263625457882881
CurrentTrain: epoch  7, batch    99 | loss: 4.0343814Losses:  3.9715499877929688 0.022052612155675888
CurrentTrain: epoch  7, batch   100 | loss: 3.9936025Losses:  3.976897954940796 0.06507041305303574
CurrentTrain: epoch  7, batch   101 | loss: 4.0419683Losses:  3.962961196899414 0.046420641243457794
CurrentTrain: epoch  7, batch   102 | loss: 4.0093818Losses:  4.004985809326172 0.05995716154575348
CurrentTrain: epoch  7, batch   103 | loss: 4.0649428Losses:  4.034039497375488 0.06218642741441727
CurrentTrain: epoch  7, batch   104 | loss: 4.0962257Losses:  4.016733169555664 0.06678151339292526
CurrentTrain: epoch  7, batch   105 | loss: 4.0835147Losses:  3.9593615531921387 0.0741271898150444
CurrentTrain: epoch  7, batch   106 | loss: 4.0334888Losses:  3.9263243675231934 0.06152356043457985
CurrentTrain: epoch  7, batch   107 | loss: 3.9878480Losses:  3.9263296127319336 0.0707767903804779
CurrentTrain: epoch  7, batch   108 | loss: 3.9971063Losses:  3.929861545562744 0.04158400744199753
CurrentTrain: epoch  7, batch   109 | loss: 3.9714456Losses:  3.9758176803588867 0.0484251007437706
CurrentTrain: epoch  7, batch   110 | loss: 4.0242429Losses:  3.9597249031066895 0.05818880721926689
CurrentTrain: epoch  7, batch   111 | loss: 4.0179138Losses:  3.967789649963379 0.0673225149512291
CurrentTrain: epoch  7, batch   112 | loss: 4.0351124Losses:  3.9663238525390625 0.06910239160060883
CurrentTrain: epoch  7, batch   113 | loss: 4.0354261Losses:  4.028170585632324 0.07915110141038895
CurrentTrain: epoch  7, batch   114 | loss: 4.1073217Losses:  3.929089307785034 0.06586537510156631
CurrentTrain: epoch  7, batch   115 | loss: 3.9949546Losses:  3.9506704807281494 0.044679343700408936
CurrentTrain: epoch  7, batch   116 | loss: 3.9953499Losses:  3.9865617752075195 0.0743425041437149
CurrentTrain: epoch  7, batch   117 | loss: 4.0609045Losses:  3.9099342823028564 0.04222773015499115
CurrentTrain: epoch  7, batch   118 | loss: 3.9521620Losses:  3.9216318130493164 0.05595429986715317
CurrentTrain: epoch  7, batch   119 | loss: 3.9775860Losses:  3.959315299987793 0.03440099209547043
CurrentTrain: epoch  7, batch   120 | loss: 3.9937162Losses:  4.0020341873168945 0.06307520717382431
CurrentTrain: epoch  7, batch   121 | loss: 4.0651093Losses:  5.064692497253418 0.13456155359745026
CurrentTrain: epoch  7, batch   122 | loss: 5.1992540Losses:  3.9615206718444824 0.05615530535578728
CurrentTrain: epoch  7, batch   123 | loss: 4.0176759Losses:  3.931063175201416 0.04808450862765312
CurrentTrain: epoch  7, batch   124 | loss: 3.9791477Losses:  4.001866340637207 0.0418035089969635
CurrentTrain: epoch  8, batch     0 | loss: 4.0436697Losses:  3.964169979095459 0.042941734194755554
CurrentTrain: epoch  8, batch     1 | loss: 4.0071115Losses:  4.035961151123047 0.04430842027068138
CurrentTrain: epoch  8, batch     2 | loss: 4.0802693Losses:  3.997683525085449 0.038827866315841675
CurrentTrain: epoch  8, batch     3 | loss: 4.0365114Losses:  3.936542510986328 0.05779099836945534
CurrentTrain: epoch  8, batch     4 | loss: 3.9943335Losses:  3.943459987640381 0.055429607629776
CurrentTrain: epoch  8, batch     5 | loss: 3.9988897Losses:  4.044429302215576 0.06228625401854515
CurrentTrain: epoch  8, batch     6 | loss: 4.1067157Losses:  3.9759817123413086 0.06046987324953079
CurrentTrain: epoch  8, batch     7 | loss: 4.0364518Losses:  3.9887821674346924 0.03153562173247337
CurrentTrain: epoch  8, batch     8 | loss: 4.0203176Losses:  4.024423122406006 0.04017014056444168
CurrentTrain: epoch  8, batch     9 | loss: 4.0645933Losses:  3.965141773223877 0.05718230828642845
CurrentTrain: epoch  8, batch    10 | loss: 4.0223241Losses:  3.862196683883667 0.01608876883983612
CurrentTrain: epoch  8, batch    11 | loss: 3.8782854Losses:  4.03137731552124 0.07984606921672821
CurrentTrain: epoch  8, batch    12 | loss: 4.1112232Losses:  3.9404091835021973 0.024176929146051407
CurrentTrain: epoch  8, batch    13 | loss: 3.9645860Losses:  3.962336540222168 0.05724678933620453
CurrentTrain: epoch  8, batch    14 | loss: 4.0195832Losses:  4.011285305023193 0.08895446360111237
CurrentTrain: epoch  8, batch    15 | loss: 4.1002398Losses:  3.9783642292022705 0.04136376082897186
CurrentTrain: epoch  8, batch    16 | loss: 4.0197282Losses:  3.9276866912841797 0.04731648415327072
CurrentTrain: epoch  8, batch    17 | loss: 3.9750032Losses:  4.029400825500488 0.05788636580109596
CurrentTrain: epoch  8, batch    18 | loss: 4.0872874Losses:  3.9021618366241455 0.02475813589990139
CurrentTrain: epoch  8, batch    19 | loss: 3.9269199Losses:  3.9892046451568604 0.0315418466925621
CurrentTrain: epoch  8, batch    20 | loss: 4.0207467Losses:  4.034699440002441 0.04319356009364128
CurrentTrain: epoch  8, batch    21 | loss: 4.0778928Losses:  3.9122581481933594 0.046800509095191956
CurrentTrain: epoch  8, batch    22 | loss: 3.9590588Losses:  4.0955891609191895 0.06696515530347824
CurrentTrain: epoch  8, batch    23 | loss: 4.1625543Losses:  3.9566266536712646 0.0838869959115982
CurrentTrain: epoch  8, batch    24 | loss: 4.0405135Losses:  3.99780011177063 0.05420548841357231
CurrentTrain: epoch  8, batch    25 | loss: 4.0520058Losses:  3.958806037902832 0.033353328704833984
CurrentTrain: epoch  8, batch    26 | loss: 3.9921594Losses:  3.941073417663574 0.057179659605026245
CurrentTrain: epoch  8, batch    27 | loss: 3.9982531Losses:  3.9074771404266357 0.02553599514067173
CurrentTrain: epoch  8, batch    28 | loss: 3.9330132Losses:  3.9538514614105225 0.0979524776339531
CurrentTrain: epoch  8, batch    29 | loss: 4.0518041Losses:  4.000617027282715 0.06835274398326874
CurrentTrain: epoch  8, batch    30 | loss: 4.0689697Losses:  3.950798988342285 0.04217365384101868
CurrentTrain: epoch  8, batch    31 | loss: 3.9929726Losses:  3.9990391731262207 0.07698296010494232
CurrentTrain: epoch  8, batch    32 | loss: 4.0760221Losses:  3.9485507011413574 0.028469745069742203
CurrentTrain: epoch  8, batch    33 | loss: 3.9770205Losses:  3.9578497409820557 0.03922121599316597
CurrentTrain: epoch  8, batch    34 | loss: 3.9970710Losses:  3.976572036743164 0.06044570729136467
CurrentTrain: epoch  8, batch    35 | loss: 4.0370178Losses:  3.9006876945495605 0.06182820349931717
CurrentTrain: epoch  8, batch    36 | loss: 3.9625158Losses:  4.00139045715332 0.0681038498878479
CurrentTrain: epoch  8, batch    37 | loss: 4.0694942Losses:  3.916685104370117 0.06563723087310791
CurrentTrain: epoch  8, batch    38 | loss: 3.9823222Losses:  3.9368836879730225 0.08060348033905029
CurrentTrain: epoch  8, batch    39 | loss: 4.0174870Losses:  3.9415454864501953 0.036895960569381714
CurrentTrain: epoch  8, batch    40 | loss: 3.9784415Losses:  3.932626724243164 0.03130693733692169
CurrentTrain: epoch  8, batch    41 | loss: 3.9639337Losses:  3.9995338916778564 0.047289829701185226
CurrentTrain: epoch  8, batch    42 | loss: 4.0468235Losses:  4.061570167541504 0.03115004673600197
CurrentTrain: epoch  8, batch    43 | loss: 4.0927200Losses:  3.9666686058044434 0.04623257368803024
CurrentTrain: epoch  8, batch    44 | loss: 4.0129013Losses:  3.9353079795837402 0.08065170049667358
CurrentTrain: epoch  8, batch    45 | loss: 4.0159597Losses:  3.9986324310302734 0.03892115503549576
CurrentTrain: epoch  8, batch    46 | loss: 4.0375538Losses:  3.939199447631836 0.08239182084798813
CurrentTrain: epoch  8, batch    47 | loss: 4.0215912Losses:  3.957622528076172 0.06536635011434555
CurrentTrain: epoch  8, batch    48 | loss: 4.0229888Losses:  4.022490501403809 0.04478289932012558
CurrentTrain: epoch  8, batch    49 | loss: 4.0672736Losses:  3.9565398693084717 0.04995685815811157
CurrentTrain: epoch  8, batch    50 | loss: 4.0064969Losses:  3.931641101837158 0.0532609298825264
CurrentTrain: epoch  8, batch    51 | loss: 3.9849021Losses:  3.9863595962524414 0.04066361486911774
CurrentTrain: epoch  8, batch    52 | loss: 4.0270233Losses:  3.92918062210083 0.04105713963508606
CurrentTrain: epoch  8, batch    53 | loss: 3.9702377Losses:  3.943422317504883 0.059564635157585144
CurrentTrain: epoch  8, batch    54 | loss: 4.0029869Losses:  3.9894156455993652 0.046442218124866486
CurrentTrain: epoch  8, batch    55 | loss: 4.0358577Losses:  4.0310587882995605 0.03878868743777275
CurrentTrain: epoch  8, batch    56 | loss: 4.0698476Losses:  3.945282459259033 0.06474301218986511
CurrentTrain: epoch  8, batch    57 | loss: 4.0100255Losses:  3.9143543243408203 0.022595932707190514
CurrentTrain: epoch  8, batch    58 | loss: 3.9369502Losses:  4.038117408752441 0.02900487743318081
CurrentTrain: epoch  8, batch    59 | loss: 4.0671225Losses:  3.9104349613189697 0.06486513465642929
CurrentTrain: epoch  8, batch    60 | loss: 3.9753001Losses:  4.007491588592529 0.04452434182167053
CurrentTrain: epoch  8, batch    61 | loss: 4.0520158Losses:  3.9977407455444336 0.06786634773015976
CurrentTrain: epoch  8, batch    62 | loss: 4.0656071Losses:  3.9640283584594727 0.04599148780107498
CurrentTrain: epoch  8, batch    63 | loss: 4.0100198Losses:  3.929417371749878 0.030025679618120193
CurrentTrain: epoch  8, batch    64 | loss: 3.9594431Losses:  3.9623494148254395 0.05494702234864235
CurrentTrain: epoch  8, batch    65 | loss: 4.0172963Losses:  3.9561069011688232 0.04076920449733734
CurrentTrain: epoch  8, batch    66 | loss: 3.9968760Losses:  3.8955907821655273 0.07321144640445709
CurrentTrain: epoch  8, batch    67 | loss: 3.9688022Losses:  3.9432051181793213 0.0731695219874382
CurrentTrain: epoch  8, batch    68 | loss: 4.0163746Losses:  3.9031288623809814 0.030592409893870354
CurrentTrain: epoch  8, batch    69 | loss: 3.9337213Losses:  3.9676971435546875 0.04273250326514244
CurrentTrain: epoch  8, batch    70 | loss: 4.0104299Losses:  3.9897820949554443 0.03391498699784279
CurrentTrain: epoch  8, batch    71 | loss: 4.0236969Losses:  3.9906628131866455 0.06540261209011078
CurrentTrain: epoch  8, batch    72 | loss: 4.0560656Losses:  3.985125780105591 0.053897857666015625
CurrentTrain: epoch  8, batch    73 | loss: 4.0390234Losses:  3.9668259620666504 0.06775956600904465
CurrentTrain: epoch  8, batch    74 | loss: 4.0345855Losses:  3.9466636180877686 0.0399547778069973
CurrentTrain: epoch  8, batch    75 | loss: 3.9866183Losses:  3.9228708744049072 0.014271872118115425
CurrentTrain: epoch  8, batch    76 | loss: 3.9371428Losses:  3.917107582092285 0.05952722951769829
CurrentTrain: epoch  8, batch    77 | loss: 3.9766347Losses:  3.9804940223693848 0.06253640353679657
CurrentTrain: epoch  8, batch    78 | loss: 4.0430303Losses:  3.944117307662964 0.062711201608181
CurrentTrain: epoch  8, batch    79 | loss: 4.0068283Losses:  3.929607391357422 0.028363104909658432
CurrentTrain: epoch  8, batch    80 | loss: 3.9579704Losses:  3.95393967628479 0.040832117199897766
CurrentTrain: epoch  8, batch    81 | loss: 3.9947717Losses:  3.9449148178100586 0.06616422533988953
CurrentTrain: epoch  8, batch    82 | loss: 4.0110788Losses:  3.8774919509887695 0.02458658069372177
CurrentTrain: epoch  8, batch    83 | loss: 3.9020786Losses:  3.945600748062134 0.03143088519573212
CurrentTrain: epoch  8, batch    84 | loss: 3.9770317Losses:  3.9696853160858154 0.05677001178264618
CurrentTrain: epoch  8, batch    85 | loss: 4.0264554Losses:  3.8990886211395264 0.06896791607141495
CurrentTrain: epoch  8, batch    86 | loss: 3.9680564Losses:  3.991729736328125 0.047959256917238235
CurrentTrain: epoch  8, batch    87 | loss: 4.0396891Losses:  3.9915342330932617 0.0546153225004673
CurrentTrain: epoch  8, batch    88 | loss: 4.0461497Losses:  4.031589508056641 0.09678489714860916
CurrentTrain: epoch  8, batch    89 | loss: 4.1283746Losses:  3.943274736404419 0.06554342806339264
CurrentTrain: epoch  8, batch    90 | loss: 4.0088181Losses:  3.905409097671509 0.04992847144603729
CurrentTrain: epoch  8, batch    91 | loss: 3.9553375Losses:  3.964724063873291 0.061723362654447556
CurrentTrain: epoch  8, batch    92 | loss: 4.0264473Losses:  3.9246928691864014 0.043544307351112366
CurrentTrain: epoch  8, batch    93 | loss: 3.9682372Losses:  3.935134172439575 0.03323020040988922
CurrentTrain: epoch  8, batch    94 | loss: 3.9683645Losses:  3.972538709640503 0.03655404597520828
CurrentTrain: epoch  8, batch    95 | loss: 4.0090928Losses:  3.929572582244873 0.03462516516447067
CurrentTrain: epoch  8, batch    96 | loss: 3.9641976Losses:  3.9811666011810303 0.04329124838113785
CurrentTrain: epoch  8, batch    97 | loss: 4.0244579Losses:  3.9442501068115234 0.04339691251516342
CurrentTrain: epoch  8, batch    98 | loss: 3.9876471Losses:  3.877561569213867 0.06696709245443344
CurrentTrain: epoch  8, batch    99 | loss: 3.9445286Losses:  3.9236621856689453 0.055520087480545044
CurrentTrain: epoch  8, batch   100 | loss: 3.9791822Losses:  3.9486377239227295 0.04069622606039047
CurrentTrain: epoch  8, batch   101 | loss: 3.9893339Losses:  4.029361724853516 0.03743944317102432
CurrentTrain: epoch  8, batch   102 | loss: 4.0668011Losses:  3.9395694732666016 0.05856640636920929
CurrentTrain: epoch  8, batch   103 | loss: 3.9981358Losses:  3.976706027984619 0.04915347322821617
CurrentTrain: epoch  8, batch   104 | loss: 4.0258594Losses:  3.9492855072021484 0.048355862498283386
CurrentTrain: epoch  8, batch   105 | loss: 3.9976413Losses:  3.9646291732788086 0.055433787405490875
CurrentTrain: epoch  8, batch   106 | loss: 4.0200629Losses:  3.9375548362731934 0.08465729653835297
CurrentTrain: epoch  8, batch   107 | loss: 4.0222120Losses:  3.9395430088043213 0.039293769747018814
CurrentTrain: epoch  8, batch   108 | loss: 3.9788368Losses:  3.946972131729126 0.033111948519945145
CurrentTrain: epoch  8, batch   109 | loss: 3.9800842Losses:  3.884575843811035 0.01852998323738575
CurrentTrain: epoch  8, batch   110 | loss: 3.9031057Losses:  3.9631848335266113 0.04181455820798874
CurrentTrain: epoch  8, batch   111 | loss: 4.0049992Losses:  3.8950839042663574 0.053759630769491196
CurrentTrain: epoch  8, batch   112 | loss: 3.9488435Losses:  3.9267141819000244 0.048631250858306885
CurrentTrain: epoch  8, batch   113 | loss: 3.9753454Losses:  4.004782199859619 0.03410189598798752
CurrentTrain: epoch  8, batch   114 | loss: 4.0388842Losses:  3.9133074283599854 0.059936635196208954
CurrentTrain: epoch  8, batch   115 | loss: 3.9732440Losses:  3.9193387031555176 0.042847082018852234
CurrentTrain: epoch  8, batch   116 | loss: 3.9621859Losses:  3.8678464889526367 0.03883912414312363
CurrentTrain: epoch  8, batch   117 | loss: 3.9066856Losses:  3.969686508178711 0.0460575707256794
CurrentTrain: epoch  8, batch   118 | loss: 4.0157442Losses:  3.949655532836914 0.06834974884986877
CurrentTrain: epoch  8, batch   119 | loss: 4.0180054Losses:  3.9701738357543945 0.035352349281311035
CurrentTrain: epoch  8, batch   120 | loss: 4.0055261Losses:  3.9247403144836426 0.0714360848069191
CurrentTrain: epoch  8, batch   121 | loss: 3.9961765Losses:  3.8909287452697754 0.047143496572971344
CurrentTrain: epoch  8, batch   122 | loss: 3.9380722Losses:  3.952435255050659 0.03424609825015068
CurrentTrain: epoch  8, batch   123 | loss: 3.9866815Losses:  3.928311824798584 0.04541609436273575
CurrentTrain: epoch  8, batch   124 | loss: 3.9737279Losses:  3.911095142364502 0.06909488141536713
CurrentTrain: epoch  9, batch     0 | loss: 3.9801900Losses:  3.9218921661376953 0.07163342833518982
CurrentTrain: epoch  9, batch     1 | loss: 3.9935255Losses:  3.9345107078552246 0.03671518713235855
CurrentTrain: epoch  9, batch     2 | loss: 3.9712260Losses:  3.9469828605651855 0.050573766231536865
CurrentTrain: epoch  9, batch     3 | loss: 3.9975567Losses:  3.917114734649658 0.02483227103948593
CurrentTrain: epoch  9, batch     4 | loss: 3.9419470Losses:  3.919891357421875 0.03025054559111595
CurrentTrain: epoch  9, batch     5 | loss: 3.9501419Losses:  3.940833568572998 0.031101834028959274
CurrentTrain: epoch  9, batch     6 | loss: 3.9719355Losses:  3.962920665740967 0.052047304809093475
CurrentTrain: epoch  9, batch     7 | loss: 4.0149679Losses:  3.9590070247650146 0.03557373210787773
CurrentTrain: epoch  9, batch     8 | loss: 3.9945807Losses:  3.973717451095581 0.0586986318230629
CurrentTrain: epoch  9, batch     9 | loss: 4.0324159Losses:  3.9775917530059814 0.045415204018354416
CurrentTrain: epoch  9, batch    10 | loss: 4.0230069Losses:  3.885110855102539 0.03758672624826431
CurrentTrain: epoch  9, batch    11 | loss: 3.9226975Losses:  3.9526865482330322 0.03408132493495941
CurrentTrain: epoch  9, batch    12 | loss: 3.9867678Losses:  3.9813859462738037 0.05433585122227669
CurrentTrain: epoch  9, batch    13 | loss: 4.0357218Losses:  3.9209792613983154 0.050124067813158035
CurrentTrain: epoch  9, batch    14 | loss: 3.9711034Losses:  3.9587974548339844 0.06812157481908798
CurrentTrain: epoch  9, batch    15 | loss: 4.0269189Losses:  3.990931272506714 0.02203117311000824
CurrentTrain: epoch  9, batch    16 | loss: 4.0129623Losses:  3.925109386444092 0.03835265338420868
CurrentTrain: epoch  9, batch    17 | loss: 3.9634621Losses:  3.935818672180176 0.03197489678859711
CurrentTrain: epoch  9, batch    18 | loss: 3.9677935Losses:  3.9065046310424805 0.04834340140223503
CurrentTrain: epoch  9, batch    19 | loss: 3.9548481Losses:  3.946817636489868 0.04803802818059921
CurrentTrain: epoch  9, batch    20 | loss: 3.9948556Losses:  3.9426093101501465 0.03584836795926094
CurrentTrain: epoch  9, batch    21 | loss: 3.9784577Losses:  3.9882702827453613 0.03779659792780876
CurrentTrain: epoch  9, batch    22 | loss: 4.0260668Losses:  3.958930015563965 0.04986089468002319
CurrentTrain: epoch  9, batch    23 | loss: 4.0087910Losses:  3.970425844192505 0.03685857728123665
CurrentTrain: epoch  9, batch    24 | loss: 4.0072846Losses:  3.924610137939453 0.028740208595991135
CurrentTrain: epoch  9, batch    25 | loss: 3.9533503Losses:  3.9323740005493164 0.04460786283016205
CurrentTrain: epoch  9, batch    26 | loss: 3.9769819Losses:  3.938692331314087 0.06220633536577225
CurrentTrain: epoch  9, batch    27 | loss: 4.0008988Losses:  3.9265174865722656 0.04719339683651924
CurrentTrain: epoch  9, batch    28 | loss: 3.9737108Losses:  3.9461331367492676 0.04071306437253952
CurrentTrain: epoch  9, batch    29 | loss: 3.9868462Losses:  3.947157621383667 0.05846433341503143
CurrentTrain: epoch  9, batch    30 | loss: 4.0056219Losses:  3.9408419132232666 0.05802871659398079
CurrentTrain: epoch  9, batch    31 | loss: 3.9988706Losses:  3.9601449966430664 0.039629824459552765
CurrentTrain: epoch  9, batch    32 | loss: 3.9997749Losses:  3.920020580291748 0.0471271276473999
CurrentTrain: epoch  9, batch    33 | loss: 3.9671478Losses:  3.9125704765319824 0.05013233423233032
CurrentTrain: epoch  9, batch    34 | loss: 3.9627028Losses:  3.950761556625366 0.04087032377719879
CurrentTrain: epoch  9, batch    35 | loss: 3.9916320Losses:  3.8869669437408447 0.047250472009181976
CurrentTrain: epoch  9, batch    36 | loss: 3.9342175Losses:  3.8891048431396484 0.03160838410258293
CurrentTrain: epoch  9, batch    37 | loss: 3.9207132Losses:  3.9589993953704834 0.06464879214763641
CurrentTrain: epoch  9, batch    38 | loss: 4.0236483Losses:  3.9836273193359375 0.028088055551052094
CurrentTrain: epoch  9, batch    39 | loss: 4.0117154Losses:  3.841212749481201 0.08126446604728699
CurrentTrain: epoch  9, batch    40 | loss: 3.9224772Losses:  3.9041101932525635 0.04806645214557648
CurrentTrain: epoch  9, batch    41 | loss: 3.9521766Losses:  3.951256275177002 0.05536865442991257
CurrentTrain: epoch  9, batch    42 | loss: 4.0066247Losses:  3.9606597423553467 0.05880654230713844
CurrentTrain: epoch  9, batch    43 | loss: 4.0194664Losses:  3.9273338317871094 0.06068798899650574
CurrentTrain: epoch  9, batch    44 | loss: 3.9880219Losses:  3.959914207458496 0.057815104722976685
CurrentTrain: epoch  9, batch    45 | loss: 4.0177293Losses:  3.906787157058716 0.03931033983826637
CurrentTrain: epoch  9, batch    46 | loss: 3.9460976Losses:  3.962114095687866 0.027571795508265495
CurrentTrain: epoch  9, batch    47 | loss: 3.9896858Losses:  3.912431240081787 0.07383929193019867
CurrentTrain: epoch  9, batch    48 | loss: 3.9862704Losses:  3.8711047172546387 0.03910871222615242
CurrentTrain: epoch  9, batch    49 | loss: 3.9102135Losses:  3.963979721069336 0.04435533285140991
CurrentTrain: epoch  9, batch    50 | loss: 4.0083351Losses:  3.9621784687042236 0.037501972168684006
CurrentTrain: epoch  9, batch    51 | loss: 3.9996805Losses:  3.915245532989502 0.05835722014307976
CurrentTrain: epoch  9, batch    52 | loss: 3.9736028Losses:  3.9533650875091553 0.06202610954642296
CurrentTrain: epoch  9, batch    53 | loss: 4.0153913Losses:  3.956186532974243 0.06380507349967957
CurrentTrain: epoch  9, batch    54 | loss: 4.0199914Losses:  3.869819402694702 0.02720848098397255
CurrentTrain: epoch  9, batch    55 | loss: 3.8970280Losses:  3.9295883178710938 0.058207545429468155
CurrentTrain: epoch  9, batch    56 | loss: 3.9877958Losses:  3.905984401702881 0.061848171055316925
CurrentTrain: epoch  9, batch    57 | loss: 3.9678326Losses:  3.9814696311950684 0.04754883795976639
CurrentTrain: epoch  9, batch    58 | loss: 4.0290184Losses:  3.918867826461792 0.040390100330114365
CurrentTrain: epoch  9, batch    59 | loss: 3.9592578Losses:  3.899402141571045 0.02553609386086464
CurrentTrain: epoch  9, batch    60 | loss: 3.9249382Losses:  3.9273781776428223 0.04513739049434662
CurrentTrain: epoch  9, batch    61 | loss: 3.9725156Losses:  3.9301741123199463 0.02653340995311737
CurrentTrain: epoch  9, batch    62 | loss: 3.9567075Losses:  3.9300503730773926 0.03925011307001114
CurrentTrain: epoch  9, batch    63 | loss: 3.9693005Losses:  3.9275264739990234 0.015111095272004604
CurrentTrain: epoch  9, batch    64 | loss: 3.9426377Losses:  3.9290270805358887 0.03981531783938408
CurrentTrain: epoch  9, batch    65 | loss: 3.9688425Losses:  3.9508607387542725 0.014283481054008007
CurrentTrain: epoch  9, batch    66 | loss: 3.9651442Losses:  3.9698121547698975 0.02290308102965355
CurrentTrain: epoch  9, batch    67 | loss: 3.9927151Losses:  3.909575939178467 0.03224726766347885
CurrentTrain: epoch  9, batch    68 | loss: 3.9418232Losses:  3.9378066062927246 0.05820590257644653
CurrentTrain: epoch  9, batch    69 | loss: 3.9960124Losses:  3.9215030670166016 0.07894694805145264
CurrentTrain: epoch  9, batch    70 | loss: 4.0004501Losses:  3.9894371032714844 0.03664697706699371
CurrentTrain: epoch  9, batch    71 | loss: 4.0260839Losses:  3.9420697689056396 0.02700972929596901
CurrentTrain: epoch  9, batch    72 | loss: 3.9690795Losses:  3.9479892253875732 0.08312807232141495
CurrentTrain: epoch  9, batch    73 | loss: 4.0311174Losses:  3.899791955947876 0.018761124461889267
CurrentTrain: epoch  9, batch    74 | loss: 3.9185531Losses:  3.9516055583953857 0.047983236610889435
CurrentTrain: epoch  9, batch    75 | loss: 3.9995887Losses:  3.982591390609741 0.05700496584177017
CurrentTrain: epoch  9, batch    76 | loss: 4.0395966Losses:  3.8964173793792725 0.022399384528398514
CurrentTrain: epoch  9, batch    77 | loss: 3.9188168Losses:  3.952780246734619 0.04732558876276016
CurrentTrain: epoch  9, batch    78 | loss: 4.0001059Losses:  3.9620862007141113 0.0522933304309845
CurrentTrain: epoch  9, batch    79 | loss: 4.0143795Losses:  3.8886959552764893 0.04065622761845589
CurrentTrain: epoch  9, batch    80 | loss: 3.9293523Losses:  3.9335079193115234 0.05521998926997185
CurrentTrain: epoch  9, batch    81 | loss: 3.9887278Losses:  3.9623990058898926 0.05616707727313042
CurrentTrain: epoch  9, batch    82 | loss: 4.0185661Losses:  3.9470555782318115 0.042880263179540634
CurrentTrain: epoch  9, batch    83 | loss: 3.9899359Losses:  4.017672538757324 0.023237846791744232
CurrentTrain: epoch  9, batch    84 | loss: 4.0409102Losses:  3.931673765182495 0.05087067931890488
CurrentTrain: epoch  9, batch    85 | loss: 3.9825444Losses:  3.919908046722412 0.05015074461698532
CurrentTrain: epoch  9, batch    86 | loss: 3.9700587Losses:  3.9321484565734863 0.03431978076696396
CurrentTrain: epoch  9, batch    87 | loss: 3.9664683Losses:  3.958528518676758 0.02377023920416832
CurrentTrain: epoch  9, batch    88 | loss: 3.9822989Losses:  3.901247024536133 0.05380640923976898
CurrentTrain: epoch  9, batch    89 | loss: 3.9550533Losses:  3.9499497413635254 0.046526357531547546
CurrentTrain: epoch  9, batch    90 | loss: 3.9964762Losses:  3.9587855339050293 0.052114520221948624
CurrentTrain: epoch  9, batch    91 | loss: 4.0109000Losses:  3.8277699947357178 0.029246149584650993
CurrentTrain: epoch  9, batch    92 | loss: 3.8570161Losses:  3.894268751144409 0.03213394060730934
CurrentTrain: epoch  9, batch    93 | loss: 3.9264028Losses:  3.8908333778381348 0.012911397032439709
CurrentTrain: epoch  9, batch    94 | loss: 3.9037447Losses:  3.951594114303589 0.03668328374624252
CurrentTrain: epoch  9, batch    95 | loss: 3.9882774Losses:  3.851015567779541 0.04307558014988899
CurrentTrain: epoch  9, batch    96 | loss: 3.8940911Losses:  3.894120693206787 0.02594497799873352
CurrentTrain: epoch  9, batch    97 | loss: 3.9200656Losses:  3.885125160217285 0.04103584215044975
CurrentTrain: epoch  9, batch    98 | loss: 3.9261611Losses:  3.914353370666504 0.042981307953596115
CurrentTrain: epoch  9, batch    99 | loss: 3.9573348Losses:  3.8966524600982666 0.06197236478328705
CurrentTrain: epoch  9, batch   100 | loss: 3.9586248Losses:  3.9330434799194336 0.02493136003613472
CurrentTrain: epoch  9, batch   101 | loss: 3.9579749Losses:  3.9370203018188477 0.05951007455587387
CurrentTrain: epoch  9, batch   102 | loss: 3.9965303Losses:  3.942157745361328 0.08313203603029251
CurrentTrain: epoch  9, batch   103 | loss: 4.0252900Losses:  3.9449644088745117 0.056458935141563416
CurrentTrain: epoch  9, batch   104 | loss: 4.0014234Losses:  4.000252723693848 0.03863067179918289
CurrentTrain: epoch  9, batch   105 | loss: 4.0388832Losses:  3.978062629699707 0.04360109567642212
CurrentTrain: epoch  9, batch   106 | loss: 4.0216637Losses:  3.8897552490234375 0.034697916358709335
CurrentTrain: epoch  9, batch   107 | loss: 3.9244533Losses:  3.985499858856201 0.047898754477500916
CurrentTrain: epoch  9, batch   108 | loss: 4.0333986Losses:  3.905076026916504 0.04923257231712341
CurrentTrain: epoch  9, batch   109 | loss: 3.9543085Losses:  3.9535961151123047 0.02107282355427742
CurrentTrain: epoch  9, batch   110 | loss: 3.9746690Losses:  3.958472490310669 0.058482710272073746
CurrentTrain: epoch  9, batch   111 | loss: 4.0169554Losses:  3.894804000854492 0.027552524581551552
CurrentTrain: epoch  9, batch   112 | loss: 3.9223566Losses:  3.935072422027588 0.050369374454021454
CurrentTrain: epoch  9, batch   113 | loss: 3.9854417Losses:  3.9261865615844727 0.0345299169421196
CurrentTrain: epoch  9, batch   114 | loss: 3.9607165Losses:  3.8965258598327637 0.025289826095104218
CurrentTrain: epoch  9, batch   115 | loss: 3.9218156Losses:  3.9801881313323975 0.042908817529678345
CurrentTrain: epoch  9, batch   116 | loss: 4.0230970Losses:  3.879417657852173 0.04082975164055824
CurrentTrain: epoch  9, batch   117 | loss: 3.9202473Losses:  3.927149772644043 0.025457166135311127
CurrentTrain: epoch  9, batch   118 | loss: 3.9526069Losses:  3.981321334838867 0.030432909727096558
CurrentTrain: epoch  9, batch   119 | loss: 4.0117540Losses:  3.9630441665649414 0.04229412600398064
CurrentTrain: epoch  9, batch   120 | loss: 4.0053382Losses:  3.907271385192871 0.04794245958328247
CurrentTrain: epoch  9, batch   121 | loss: 3.9552138Losses:  3.9471187591552734 0.04915068298578262
CurrentTrain: epoch  9, batch   122 | loss: 3.9962695Losses:  4.0380072593688965 0.0312589630484581
CurrentTrain: epoch  9, batch   123 | loss: 4.0692663Losses:  3.948789119720459 0.022431638091802597
CurrentTrain: epoch  9, batch   124 | loss: 3.9712207
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  8.292182922363281 1.6701667308807373
CurrentTrain: epoch  0, batch     0 | loss: 9.9623499Losses:  10.692848205566406 1.4881051778793335
CurrentTrain: epoch  0, batch     1 | loss: 12.1809530Losses:  10.690841674804688 1.5917325019836426
CurrentTrain: epoch  0, batch     2 | loss: 12.2825737Losses:  10.158344268798828 1.106508493423462
CurrentTrain: epoch  0, batch     3 | loss: 11.2648525Losses:  8.261056900024414 1.3440725803375244
CurrentTrain: epoch  0, batch     4 | loss: 9.6051292Losses:  8.46086597442627 1.422812819480896
CurrentTrain: epoch  0, batch     5 | loss: 9.8836784Losses:  4.262540817260742 0.2159109115600586
CurrentTrain: epoch  0, batch     6 | loss: 4.4784517Losses:  5.318991184234619 1.312453031539917
CurrentTrain: epoch  1, batch     0 | loss: 6.6314440Losses:  4.556666374206543 1.1515448093414307
CurrentTrain: epoch  1, batch     1 | loss: 5.7082109Losses:  3.607142925262451 1.2985107898712158
CurrentTrain: epoch  1, batch     2 | loss: 4.9056540Losses:  5.597489833831787 1.4068262577056885
CurrentTrain: epoch  1, batch     3 | loss: 7.0043163Losses:  4.348491668701172 1.4501177072525024
CurrentTrain: epoch  1, batch     4 | loss: 5.7986093Losses:  4.625388145446777 1.040410041809082
CurrentTrain: epoch  1, batch     5 | loss: 5.6657982Losses:  3.169464111328125 0.4021448493003845
CurrentTrain: epoch  1, batch     6 | loss: 3.5716090Losses:  4.08304500579834 0.8227431178092957
CurrentTrain: epoch  2, batch     0 | loss: 4.9057879Losses:  3.862081527709961 1.3341898918151855
CurrentTrain: epoch  2, batch     1 | loss: 5.1962714Losses:  5.280765533447266 1.2395424842834473
CurrentTrain: epoch  2, batch     2 | loss: 6.5203080Losses:  3.983109712600708 0.8149669766426086
CurrentTrain: epoch  2, batch     3 | loss: 4.7980766Losses:  4.121914863586426 1.259276032447815
CurrentTrain: epoch  2, batch     4 | loss: 5.3811908Losses:  4.1511311531066895 1.1592624187469482
CurrentTrain: epoch  2, batch     5 | loss: 5.3103933Losses:  3.2010743618011475 0.2422524094581604
CurrentTrain: epoch  2, batch     6 | loss: 3.4433267Losses:  3.7499918937683105 0.6705628037452698
CurrentTrain: epoch  3, batch     0 | loss: 4.4205546Losses:  4.557507514953613 0.7997795343399048
CurrentTrain: epoch  3, batch     1 | loss: 5.3572869Losses:  3.370069980621338 0.9956667423248291
CurrentTrain: epoch  3, batch     2 | loss: 4.3657370Losses:  3.8491389751434326 0.791755735874176
CurrentTrain: epoch  3, batch     3 | loss: 4.6408949Losses:  3.8487868309020996 1.4306926727294922
CurrentTrain: epoch  3, batch     4 | loss: 5.2794795Losses:  4.048681259155273 1.2435290813446045
CurrentTrain: epoch  3, batch     5 | loss: 5.2922106Losses:  3.4730868339538574 0.18329504132270813
CurrentTrain: epoch  3, batch     6 | loss: 3.6563818Losses:  3.2629899978637695 0.8950523734092712
CurrentTrain: epoch  4, batch     0 | loss: 4.1580424Losses:  2.609529495239258 0.6755421161651611
CurrentTrain: epoch  4, batch     1 | loss: 3.2850716Losses:  4.622899055480957 0.7137033939361572
CurrentTrain: epoch  4, batch     2 | loss: 5.3366022Losses:  3.7051753997802734 1.1304477453231812
CurrentTrain: epoch  4, batch     3 | loss: 4.8356233Losses:  3.4129064083099365 0.8081580400466919
CurrentTrain: epoch  4, batch     4 | loss: 4.2210646Losses:  2.617957353591919 0.8722318410873413
CurrentTrain: epoch  4, batch     5 | loss: 3.4901891Losses:  6.302707195281982 1.1920930376163597e-07
CurrentTrain: epoch  4, batch     6 | loss: 6.3027072Losses:  3.6136231422424316 0.9106343984603882
CurrentTrain: epoch  5, batch     0 | loss: 4.5242577Losses:  2.978938341140747 0.6506964564323425
CurrentTrain: epoch  5, batch     1 | loss: 3.6296349Losses:  3.2174131870269775 0.7710908651351929
CurrentTrain: epoch  5, batch     2 | loss: 3.9885039Losses:  2.4842753410339355 0.7416487336158752
CurrentTrain: epoch  5, batch     3 | loss: 3.2259240Losses:  4.173742294311523 1.0712306499481201
CurrentTrain: epoch  5, batch     4 | loss: 5.2449732Losses:  3.5978081226348877 0.996256947517395
CurrentTrain: epoch  5, batch     5 | loss: 4.5940652Losses:  1.7551219463348389 0.08147551864385605
CurrentTrain: epoch  5, batch     6 | loss: 1.8365974Losses:  3.4930593967437744 1.1396057605743408
CurrentTrain: epoch  6, batch     0 | loss: 4.6326652Losses:  2.849482774734497 0.9778009057044983
CurrentTrain: epoch  6, batch     1 | loss: 3.8272836Losses:  2.336078643798828 0.6001646518707275
CurrentTrain: epoch  6, batch     2 | loss: 2.9362433Losses:  3.7097055912017822 0.9846920967102051
CurrentTrain: epoch  6, batch     3 | loss: 4.6943979Losses:  2.7483997344970703 0.7984486818313599
CurrentTrain: epoch  6, batch     4 | loss: 3.5468483Losses:  3.5379638671875 0.680551290512085
CurrentTrain: epoch  6, batch     5 | loss: 4.2185154Losses:  2.935490131378174 1.1920930376163597e-07
CurrentTrain: epoch  6, batch     6 | loss: 2.9354904Losses:  3.132657527923584 0.5543124079704285
CurrentTrain: epoch  7, batch     0 | loss: 3.6869700Losses:  3.530702590942383 0.5697939395904541
CurrentTrain: epoch  7, batch     1 | loss: 4.1004963Losses:  2.5387299060821533 0.6374714374542236
CurrentTrain: epoch  7, batch     2 | loss: 3.1762013Losses:  2.8152828216552734 0.8649432063102722
CurrentTrain: epoch  7, batch     3 | loss: 3.6802261Losses:  2.3329806327819824 0.5706892013549805
CurrentTrain: epoch  7, batch     4 | loss: 2.9036698Losses:  2.8139233589172363 0.8308796286582947
CurrentTrain: epoch  7, batch     5 | loss: 3.6448030Losses:  2.9497246742248535 0.24812471866607666
CurrentTrain: epoch  7, batch     6 | loss: 3.1978493Losses:  2.909907341003418 0.9760206341743469
CurrentTrain: epoch  8, batch     0 | loss: 3.8859279Losses:  2.244274139404297 0.7166703343391418
CurrentTrain: epoch  8, batch     1 | loss: 2.9609444Losses:  3.0106654167175293 0.842932939529419
CurrentTrain: epoch  8, batch     2 | loss: 3.8535984Losses:  2.107783794403076 0.6081169843673706
CurrentTrain: epoch  8, batch     3 | loss: 2.7159009Losses:  2.485307216644287 0.5693712830543518
CurrentTrain: epoch  8, batch     4 | loss: 3.0546784Losses:  3.3576040267944336 0.5640374422073364
CurrentTrain: epoch  8, batch     5 | loss: 3.9216413Losses:  1.774370789527893 0.10622505098581314
CurrentTrain: epoch  8, batch     6 | loss: 1.8805958Losses:  3.0128068923950195 0.6340733170509338
CurrentTrain: epoch  9, batch     0 | loss: 3.6468801Losses:  2.5222508907318115 0.6698285341262817
CurrentTrain: epoch  9, batch     1 | loss: 3.1920795Losses:  2.5833487510681152 0.7327158451080322
CurrentTrain: epoch  9, batch     2 | loss: 3.3160646Losses:  2.413583755493164 0.5390622019767761
CurrentTrain: epoch  9, batch     3 | loss: 2.9526460Losses:  2.294017791748047 0.41703686118125916
CurrentTrain: epoch  9, batch     4 | loss: 2.7110546Losses:  2.171003818511963 0.5375038981437683
CurrentTrain: epoch  9, batch     5 | loss: 2.7085078Losses:  1.7267999649047852 0.07783878594636917
CurrentTrain: epoch  9, batch     6 | loss: 1.8046387
Losses:  5.44655704498291 0.8466513156890869
MemoryTrain:  epoch  0, batch     0 | loss: 6.2932081Losses:  7.956820487976074 0.5506657958030701
MemoryTrain:  epoch  0, batch     1 | loss: 8.5074863Losses:  10.614982604980469 0.24543096125125885
MemoryTrain:  epoch  0, batch     2 | loss: 10.8604136Losses:  1.013486385345459 0.6278159022331238
MemoryTrain:  epoch  1, batch     0 | loss: 1.6413023Losses:  1.0167020559310913 0.6442577838897705
MemoryTrain:  epoch  1, batch     1 | loss: 1.6609598Losses:  0.5953463315963745 0.27125686407089233
MemoryTrain:  epoch  1, batch     2 | loss: 0.8666032Losses:  0.5845757126808167 0.47465503215789795
MemoryTrain:  epoch  2, batch     0 | loss: 1.0592308Losses:  0.7128034830093384 0.47141605615615845
MemoryTrain:  epoch  2, batch     1 | loss: 1.1842196Losses:  0.1655867099761963 0.1270732879638672
MemoryTrain:  epoch  2, batch     2 | loss: 0.2926600Losses:  0.3749619126319885 0.4876243472099304
MemoryTrain:  epoch  3, batch     0 | loss: 0.8625863Losses:  0.4820420444011688 0.5920471549034119
MemoryTrain:  epoch  3, batch     1 | loss: 1.0740892Losses:  0.17817112803459167 0.09854213148355484
MemoryTrain:  epoch  3, batch     2 | loss: 0.2767133Losses:  0.20836657285690308 0.41155630350112915
MemoryTrain:  epoch  4, batch     0 | loss: 0.6199229Losses:  0.3809492588043213 0.43377476930618286
MemoryTrain:  epoch  4, batch     1 | loss: 0.8147240Losses:  0.3862665593624115 0.1041237860918045
MemoryTrain:  epoch  4, batch     2 | loss: 0.4903904Losses:  0.22017809748649597 0.2747079133987427
MemoryTrain:  epoch  5, batch     0 | loss: 0.4948860Losses:  0.3154999613761902 0.6555797457695007
MemoryTrain:  epoch  5, batch     1 | loss: 0.9710797Losses:  0.27832725644111633 0.12304005771875381
MemoryTrain:  epoch  5, batch     2 | loss: 0.4013673Losses:  0.11619677394628525 0.2331237643957138
MemoryTrain:  epoch  6, batch     0 | loss: 0.3493205Losses:  0.3832992911338806 0.6034354567527771
MemoryTrain:  epoch  6, batch     1 | loss: 0.9867347Losses:  0.19960066676139832 0.3606967628002167
MemoryTrain:  epoch  6, batch     2 | loss: 0.5602974Losses:  0.292056679725647 0.4731399714946747
MemoryTrain:  epoch  7, batch     0 | loss: 0.7651967Losses:  0.1809544563293457 0.4237346053123474
MemoryTrain:  epoch  7, batch     1 | loss: 0.6046891Losses:  0.16039666533470154 0.12913469970226288
MemoryTrain:  epoch  7, batch     2 | loss: 0.2895314Losses:  0.233291894197464 0.40178024768829346
MemoryTrain:  epoch  8, batch     0 | loss: 0.6350721Losses:  0.1798250675201416 0.4377129077911377
MemoryTrain:  epoch  8, batch     1 | loss: 0.6175380Losses:  0.20098233222961426 0.1298590749502182
MemoryTrain:  epoch  8, batch     2 | loss: 0.3308414Losses:  0.20836281776428223 0.38837262988090515
MemoryTrain:  epoch  9, batch     0 | loss: 0.5967355Losses:  0.1921422779560089 0.534616231918335
MemoryTrain:  epoch  9, batch     1 | loss: 0.7267585Losses:  0.08800601214170456 0.043999917805194855
MemoryTrain:  epoch  9, batch     2 | loss: 0.1320059
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 76.11%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 75.33%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 74.78%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.47%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 73.65%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 73.26%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 72.28%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 71.63%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.81%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.40%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 94.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 92.29%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 89.68%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 88.53%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 87.50%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 86.59%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 86.70%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 86.53%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.28%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 86.18%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.20%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 85.94%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 86.13%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 85.94%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 85.74%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 86.26%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 86.57%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.71%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 87.25%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 87.25%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 87.14%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 87.02%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.91%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 86.80%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 86.40%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 86.14%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 85.75%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 85.49%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 85.51%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 85.67%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 85.61%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 85.10%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 84.76%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 84.48%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 83.99%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 83.57%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 83.25%   
cur_acc:  ['0.9474', '0.7163']
his_acc:  ['0.9474', '0.8325']
Clustering into  14  clusters
Clusters:  [ 2  9 13  2  2  2  0  2 10  1  0  2  4  2 11  6  7  2  2  2  2  2  2  8
  2  1  2  3 12  5]
Losses:  7.652027606964111 1.0898141860961914
CurrentTrain: epoch  0, batch     0 | loss: 8.7418423Losses:  11.07528018951416 0.9740321636199951
CurrentTrain: epoch  0, batch     1 | loss: 12.0493126Losses:  8.424576759338379 1.0945626497268677
CurrentTrain: epoch  0, batch     2 | loss: 9.5191393Losses:  7.202104568481445 1.2110023498535156
CurrentTrain: epoch  0, batch     3 | loss: 8.4131069Losses:  7.281177520751953 1.0736417770385742
CurrentTrain: epoch  0, batch     4 | loss: 8.3548193Losses:  8.310283660888672 0.5955266952514648
CurrentTrain: epoch  0, batch     5 | loss: 8.9058104Losses:  5.230750560760498 0.07267876714468002
CurrentTrain: epoch  0, batch     6 | loss: 5.3034291Losses:  4.462335109710693 0.7929005026817322
CurrentTrain: epoch  1, batch     0 | loss: 5.2552357Losses:  2.961400032043457 0.6550986766815186
CurrentTrain: epoch  1, batch     1 | loss: 3.6164987Losses:  3.823371410369873 1.1055378913879395
CurrentTrain: epoch  1, batch     2 | loss: 4.9289093Losses:  3.694279432296753 0.6979995965957642
CurrentTrain: epoch  1, batch     3 | loss: 4.3922791Losses:  3.047276735305786 0.7829140424728394
CurrentTrain: epoch  1, batch     4 | loss: 3.8301907Losses:  3.029419422149658 0.9599121809005737
CurrentTrain: epoch  1, batch     5 | loss: 3.9893317Losses:  3.7782135009765625 0.1654140055179596
CurrentTrain: epoch  1, batch     6 | loss: 3.9436276Losses:  3.3183462619781494 0.9652249813079834
CurrentTrain: epoch  2, batch     0 | loss: 4.2835712Losses:  2.6997320652008057 0.6576928496360779
CurrentTrain: epoch  2, batch     1 | loss: 3.3574250Losses:  3.406903028488159 0.8669746518135071
CurrentTrain: epoch  2, batch     2 | loss: 4.2738776Losses:  2.5026721954345703 0.5439034700393677
CurrentTrain: epoch  2, batch     3 | loss: 3.0465755Losses:  3.150285243988037 0.6610430479049683
CurrentTrain: epoch  2, batch     4 | loss: 3.8113284Losses:  2.900791645050049 0.6692496538162231
CurrentTrain: epoch  2, batch     5 | loss: 3.5700412Losses:  2.2098798751831055 0.12948356568813324
CurrentTrain: epoch  2, batch     6 | loss: 2.3393633Losses:  3.265239715576172 0.6738423109054565
CurrentTrain: epoch  3, batch     0 | loss: 3.9390821Losses:  2.1010262966156006 0.4911482036113739
CurrentTrain: epoch  3, batch     1 | loss: 2.5921745Losses:  2.805145263671875 0.5364575982093811
CurrentTrain: epoch  3, batch     2 | loss: 3.3416028Losses:  2.2890307903289795 0.6510961055755615
CurrentTrain: epoch  3, batch     3 | loss: 2.9401269Losses:  2.1560862064361572 0.5852501392364502
CurrentTrain: epoch  3, batch     4 | loss: 2.7413363Losses:  2.7676351070404053 0.4626701772212982
CurrentTrain: epoch  3, batch     5 | loss: 3.2303052Losses:  2.121408462524414 0.1858118325471878
CurrentTrain: epoch  3, batch     6 | loss: 2.3072202Losses:  2.348660707473755 0.42846912145614624
CurrentTrain: epoch  4, batch     0 | loss: 2.7771299Losses:  2.2226874828338623 0.48593491315841675
CurrentTrain: epoch  4, batch     1 | loss: 2.7086225Losses:  2.7103989124298096 0.5136343836784363
CurrentTrain: epoch  4, batch     2 | loss: 3.2240334Losses:  2.1089632511138916 0.5744925737380981
CurrentTrain: epoch  4, batch     3 | loss: 2.6834559Losses:  2.4271066188812256 0.4513583779335022
CurrentTrain: epoch  4, batch     4 | loss: 2.8784649Losses:  2.09926176071167 0.39809757471084595
CurrentTrain: epoch  4, batch     5 | loss: 2.4973593Losses:  1.7769968509674072 0.05072781443595886
CurrentTrain: epoch  4, batch     6 | loss: 1.8277247Losses:  2.2952849864959717 0.6206536293029785
CurrentTrain: epoch  5, batch     0 | loss: 2.9159386Losses:  1.8357319831848145 0.24057002365589142
CurrentTrain: epoch  5, batch     1 | loss: 2.0763021Losses:  2.1890363693237305 0.5297491550445557
CurrentTrain: epoch  5, batch     2 | loss: 2.7187855Losses:  2.2263059616088867 0.34199339151382446
CurrentTrain: epoch  5, batch     3 | loss: 2.5682993Losses:  2.3083949089050293 0.5194763541221619
CurrentTrain: epoch  5, batch     4 | loss: 2.8278713Losses:  2.1971776485443115 0.33725249767303467
CurrentTrain: epoch  5, batch     5 | loss: 2.5344300Losses:  1.7964529991149902 0.2708563208580017
CurrentTrain: epoch  5, batch     6 | loss: 2.0673094Losses:  2.0703561305999756 0.6465463638305664
CurrentTrain: epoch  6, batch     0 | loss: 2.7169025Losses:  2.399719715118408 0.3710353970527649
CurrentTrain: epoch  6, batch     1 | loss: 2.7707551Losses:  2.2174313068389893 0.5730093121528625
CurrentTrain: epoch  6, batch     2 | loss: 2.7904406Losses:  1.7725574970245361 0.28284773230552673
CurrentTrain: epoch  6, batch     3 | loss: 2.0554051Losses:  1.8110016584396362 0.34620916843414307
CurrentTrain: epoch  6, batch     4 | loss: 2.1572108Losses:  1.8681447505950928 0.2500251531600952
CurrentTrain: epoch  6, batch     5 | loss: 2.1181698Losses:  1.7045140266418457 0.18592002987861633
CurrentTrain: epoch  6, batch     6 | loss: 1.8904340Losses:  1.9464507102966309 0.32674023509025574
CurrentTrain: epoch  7, batch     0 | loss: 2.2731910Losses:  1.7610523700714111 0.29527124762535095
CurrentTrain: epoch  7, batch     1 | loss: 2.0563235Losses:  1.8618876934051514 0.37662041187286377
CurrentTrain: epoch  7, batch     2 | loss: 2.2385082Losses:  1.9467086791992188 0.2243577241897583
CurrentTrain: epoch  7, batch     3 | loss: 2.1710663Losses:  1.980304479598999 0.32214683294296265
CurrentTrain: epoch  7, batch     4 | loss: 2.3024514Losses:  1.985806941986084 0.4388335943222046
CurrentTrain: epoch  7, batch     5 | loss: 2.4246407Losses:  1.757178544998169 0.051740456372499466
CurrentTrain: epoch  7, batch     6 | loss: 1.8089190Losses:  1.810492753982544 0.3702102303504944
CurrentTrain: epoch  8, batch     0 | loss: 2.1807029Losses:  1.7470049858093262 0.15446053445339203
CurrentTrain: epoch  8, batch     1 | loss: 1.9014655Losses:  1.7780828475952148 0.3368537127971649
CurrentTrain: epoch  8, batch     2 | loss: 2.1149366Losses:  1.9327985048294067 0.2212008833885193
CurrentTrain: epoch  8, batch     3 | loss: 2.1539993Losses:  1.929315209388733 0.3582729697227478
CurrentTrain: epoch  8, batch     4 | loss: 2.2875881Losses:  1.7339634895324707 0.2358018159866333
CurrentTrain: epoch  8, batch     5 | loss: 1.9697653Losses:  2.081899881362915 0.22571371495723724
CurrentTrain: epoch  8, batch     6 | loss: 2.3076136Losses:  1.7554947137832642 0.2664428949356079
CurrentTrain: epoch  9, batch     0 | loss: 2.0219376Losses:  1.6921534538269043 0.25505438446998596
CurrentTrain: epoch  9, batch     1 | loss: 1.9472078Losses:  1.7403392791748047 0.2744057774543762
CurrentTrain: epoch  9, batch     2 | loss: 2.0147450Losses:  1.8688548803329468 0.39699268341064453
CurrentTrain: epoch  9, batch     3 | loss: 2.2658477Losses:  1.7758427858352661 0.4020766019821167
CurrentTrain: epoch  9, batch     4 | loss: 2.1779194Losses:  1.826244592666626 0.5872464179992676
CurrentTrain: epoch  9, batch     5 | loss: 2.4134910Losses:  1.8925108909606934 0.24047896265983582
CurrentTrain: epoch  9, batch     6 | loss: 2.1329899
Losses:  5.964657783508301 0.31929370760917664
MemoryTrain:  epoch  0, batch     0 | loss: 6.2839513Losses:  9.264305114746094 0.7247371673583984
MemoryTrain:  epoch  0, batch     1 | loss: 9.9890423Losses:  9.509355545043945 0.332606703042984
MemoryTrain:  epoch  0, batch     2 | loss: 9.8419619Losses:  11.5877103805542 0.3990945518016815
MemoryTrain:  epoch  0, batch     3 | loss: 11.9868050Losses:  1.3226821422576904 0.42092961072921753
MemoryTrain:  epoch  1, batch     0 | loss: 1.7436118Losses:  1.693472146987915 0.306557297706604
MemoryTrain:  epoch  1, batch     1 | loss: 2.0000296Losses:  1.841386079788208 0.3414614200592041
MemoryTrain:  epoch  1, batch     2 | loss: 2.1828475Losses:  1.443137526512146 0.6725737452507019
MemoryTrain:  epoch  1, batch     3 | loss: 2.1157112Losses:  0.944435179233551 0.5290622115135193
MemoryTrain:  epoch  2, batch     0 | loss: 1.4734974Losses:  2.089200496673584 0.5578794479370117
MemoryTrain:  epoch  2, batch     1 | loss: 2.6470799Losses:  1.0154125690460205 0.3042059540748596
MemoryTrain:  epoch  2, batch     2 | loss: 1.3196185Losses:  1.4169902801513672 0.34095168113708496
MemoryTrain:  epoch  2, batch     3 | loss: 1.7579420Losses:  0.8616017699241638 0.31567394733428955
MemoryTrain:  epoch  3, batch     0 | loss: 1.1772757Losses:  1.5937119722366333 0.41322648525238037
MemoryTrain:  epoch  3, batch     1 | loss: 2.0069385Losses:  0.6927936673164368 0.4217069447040558
MemoryTrain:  epoch  3, batch     2 | loss: 1.1145006Losses:  0.840613067150116 0.7377786040306091
MemoryTrain:  epoch  3, batch     3 | loss: 1.5783917Losses:  1.3223211765289307 0.4729679226875305
MemoryTrain:  epoch  4, batch     0 | loss: 1.7952890Losses:  0.6442492008209229 0.3855161666870117
MemoryTrain:  epoch  4, batch     1 | loss: 1.0297654Losses:  0.5241368412971497 0.33383041620254517
MemoryTrain:  epoch  4, batch     2 | loss: 0.8579673Losses:  0.6221728324890137 0.4323478639125824
MemoryTrain:  epoch  4, batch     3 | loss: 1.0545207Losses:  1.1073380708694458 0.5010334849357605
MemoryTrain:  epoch  5, batch     0 | loss: 1.6083715Losses:  0.4869823455810547 0.4915825426578522
MemoryTrain:  epoch  5, batch     1 | loss: 0.9785649Losses:  0.5175329446792603 0.40672677755355835
MemoryTrain:  epoch  5, batch     2 | loss: 0.9242597Losses:  0.2996412515640259 0.3248652219772339
MemoryTrain:  epoch  5, batch     3 | loss: 0.6245065Losses:  0.8531348705291748 0.4419736862182617
MemoryTrain:  epoch  6, batch     0 | loss: 1.2951086Losses:  0.33854803442955017 0.38030120730400085
MemoryTrain:  epoch  6, batch     1 | loss: 0.7188492Losses:  0.2733275592327118 0.4669176936149597
MemoryTrain:  epoch  6, batch     2 | loss: 0.7402452Losses:  0.350417822599411 0.28587833046913147
MemoryTrain:  epoch  6, batch     3 | loss: 0.6362962Losses:  0.18557025492191315 0.3656318485736847
MemoryTrain:  epoch  7, batch     0 | loss: 0.5512021Losses:  0.3535607159137726 0.6327455639839172
MemoryTrain:  epoch  7, batch     1 | loss: 0.9863063Losses:  0.7653728127479553 0.5805345773696899
MemoryTrain:  epoch  7, batch     2 | loss: 1.3459074Losses:  0.1597369909286499 0.18115344643592834
MemoryTrain:  epoch  7, batch     3 | loss: 0.3408904Losses:  0.328521728515625 0.23880793154239655
MemoryTrain:  epoch  8, batch     0 | loss: 0.5673296Losses:  0.339869886636734 0.4442989230155945
MemoryTrain:  epoch  8, batch     1 | loss: 0.7841688Losses:  0.296109139919281 0.6399822235107422
MemoryTrain:  epoch  8, batch     2 | loss: 0.9360914Losses:  0.26612794399261475 0.25642725825309753
MemoryTrain:  epoch  8, batch     3 | loss: 0.5225552Losses:  0.4240146279335022 0.28010210394859314
MemoryTrain:  epoch  9, batch     0 | loss: 0.7041167Losses:  0.22755499184131622 0.274352490901947
MemoryTrain:  epoch  9, batch     1 | loss: 0.5019075Losses:  0.422931432723999 0.7387913465499878
MemoryTrain:  epoch  9, batch     2 | loss: 1.1617228Losses:  0.16786304116249084 0.3449804186820984
MemoryTrain:  epoch  9, batch     3 | loss: 0.5128435
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 73.86%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 65.35%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 60.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.26%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.86%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.91%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.35%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 89.36%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 88.08%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 86.93%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 85.91%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 84.74%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 83.88%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 83.25%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 82.88%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.85%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.87%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 82.85%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.02%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 82.56%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 82.61%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.53%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 84.53%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 84.56%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 84.41%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 84.31%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.40%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 84.11%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 83.74%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 83.49%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 83.30%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 82.96%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 83.23%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.32%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 83.19%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 82.66%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 82.08%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 81.56%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 81.00%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 80.44%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 80.00%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 79.81%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 79.58%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 79.42%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 80.16%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 80.22%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 80.13%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 80.19%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 80.16%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 80.16%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 79.96%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 79.88%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 79.53%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 79.42%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 79.47%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 79.44%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 79.37%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 79.42%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 79.27%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 79.25%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 78.98%   [EVAL] batch:  157 | acc: 37.50%,  total acc: 78.72%   [EVAL] batch:  158 | acc: 31.25%,  total acc: 78.42%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 78.16%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 77.99%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 77.78%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 77.61%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 77.44%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 77.35%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 77.22%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 77.13%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 76.97%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 76.81%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 76.47%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 76.06%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 75.62%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 75.22%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 74.82%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 74.50%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.93%   
cur_acc:  ['0.9474', '0.7163', '0.6786']
his_acc:  ['0.9474', '0.8325', '0.7593']
Clustering into  19  clusters
Clusters:  [ 2  9 13  2  2  2 17  2 18  0 14  2 11  2 12 16 15  2  2  2  2  2  2 10
  2  0  2  7  5  8  2  3  2  2  4  2  2  2  1  6]
Losses:  7.251930236816406 1.245734453201294
CurrentTrain: epoch  0, batch     0 | loss: 8.4976645Losses:  9.490345001220703 1.2542833089828491
CurrentTrain: epoch  0, batch     1 | loss: 10.7446280Losses:  9.346567153930664 1.1965019702911377
CurrentTrain: epoch  0, batch     2 | loss: 10.5430689Losses:  8.005353927612305 0.9521855711936951
CurrentTrain: epoch  0, batch     3 | loss: 8.9575396Losses:  8.528985023498535 0.7751985788345337
CurrentTrain: epoch  0, batch     4 | loss: 9.3041840Losses:  5.874755859375 1.4208009243011475
CurrentTrain: epoch  0, batch     5 | loss: 7.2955570Losses:  7.142332553863525 0.6204037666320801
CurrentTrain: epoch  0, batch     6 | loss: 7.7627363Losses:  3.8705391883850098 1.018873929977417
CurrentTrain: epoch  1, batch     0 | loss: 4.8894129Losses:  4.429174900054932 1.0014194250106812
CurrentTrain: epoch  1, batch     1 | loss: 5.4305944Losses:  4.498317718505859 1.3564329147338867
CurrentTrain: epoch  1, batch     2 | loss: 5.8547506Losses:  3.871605396270752 1.0978554487228394
CurrentTrain: epoch  1, batch     3 | loss: 4.9694610Losses:  3.9335365295410156 1.0830039978027344
CurrentTrain: epoch  1, batch     4 | loss: 5.0165405Losses:  2.7367115020751953 1.003525972366333
CurrentTrain: epoch  1, batch     5 | loss: 3.7402375Losses:  4.553133010864258 0.24024957418441772
CurrentTrain: epoch  1, batch     6 | loss: 4.7933826Losses:  3.687041997909546 0.7898404598236084
CurrentTrain: epoch  2, batch     0 | loss: 4.4768825Losses:  4.093816757202148 0.9491000175476074
CurrentTrain: epoch  2, batch     1 | loss: 5.0429168Losses:  3.8159589767456055 1.0299477577209473
CurrentTrain: epoch  2, batch     2 | loss: 4.8459067Losses:  3.146062135696411 0.707054615020752
CurrentTrain: epoch  2, batch     3 | loss: 3.8531168Losses:  3.3196330070495605 0.7553151249885559
CurrentTrain: epoch  2, batch     4 | loss: 4.0749483Losses:  2.8026397228240967 0.7760147452354431
CurrentTrain: epoch  2, batch     5 | loss: 3.5786545Losses:  4.08640193939209 0.6034377813339233
CurrentTrain: epoch  2, batch     6 | loss: 4.6898398Losses:  3.1137890815734863 0.9228259325027466
CurrentTrain: epoch  3, batch     0 | loss: 4.0366149Losses:  3.0115232467651367 1.0522267818450928
CurrentTrain: epoch  3, batch     1 | loss: 4.0637503Losses:  2.2527740001678467 0.8728452920913696
CurrentTrain: epoch  3, batch     2 | loss: 3.1256194Losses:  3.974470615386963 0.8304911851882935
CurrentTrain: epoch  3, batch     3 | loss: 4.8049617Losses:  2.558718681335449 0.7995487451553345
CurrentTrain: epoch  3, batch     4 | loss: 3.3582673Losses:  3.7359182834625244 0.9750460982322693
CurrentTrain: epoch  3, batch     5 | loss: 4.7109642Losses:  4.010218620300293 0.4742179811000824
CurrentTrain: epoch  3, batch     6 | loss: 4.4844365Losses:  2.3274712562561035 0.6527129411697388
CurrentTrain: epoch  4, batch     0 | loss: 2.9801841Losses:  2.960153102874756 0.5242291688919067
CurrentTrain: epoch  4, batch     1 | loss: 3.4843822Losses:  3.2951319217681885 0.8031716346740723
CurrentTrain: epoch  4, batch     2 | loss: 4.0983038Losses:  3.450162410736084 0.943421483039856
CurrentTrain: epoch  4, batch     3 | loss: 4.3935838Losses:  2.6716365814208984 0.6055409908294678
CurrentTrain: epoch  4, batch     4 | loss: 3.2771776Losses:  2.6332902908325195 0.5687509179115295
CurrentTrain: epoch  4, batch     5 | loss: 3.2020411Losses:  3.1520628929138184 0.42464300990104675
CurrentTrain: epoch  4, batch     6 | loss: 3.5767059Losses:  2.2843308448791504 0.6359541416168213
CurrentTrain: epoch  5, batch     0 | loss: 2.9202850Losses:  2.5775465965270996 0.7804499864578247
CurrentTrain: epoch  5, batch     1 | loss: 3.3579965Losses:  2.924205780029297 0.7049217224121094
CurrentTrain: epoch  5, batch     2 | loss: 3.6291275Losses:  2.4039244651794434 0.5894908308982849
CurrentTrain: epoch  5, batch     3 | loss: 2.9934154Losses:  3.372986316680908 0.6682764291763306
CurrentTrain: epoch  5, batch     4 | loss: 4.0412626Losses:  2.69150972366333 0.8446848392486572
CurrentTrain: epoch  5, batch     5 | loss: 3.5361946Losses:  2.212233066558838 0.03197752684354782
CurrentTrain: epoch  5, batch     6 | loss: 2.2442105Losses:  3.0578997135162354 0.8867224454879761
CurrentTrain: epoch  6, batch     0 | loss: 3.9446220Losses:  2.917051076889038 0.7662888765335083
CurrentTrain: epoch  6, batch     1 | loss: 3.6833401Losses:  2.1696765422821045 0.4372369349002838
CurrentTrain: epoch  6, batch     2 | loss: 2.6069136Losses:  2.435377836227417 0.5261576175689697
CurrentTrain: epoch  6, batch     3 | loss: 2.9615355Losses:  2.189483404159546 0.5914292931556702
CurrentTrain: epoch  6, batch     4 | loss: 2.7809126Losses:  2.005821943283081 0.5511748790740967
CurrentTrain: epoch  6, batch     5 | loss: 2.5569968Losses:  3.7494590282440186 0.6447087526321411
CurrentTrain: epoch  6, batch     6 | loss: 4.3941679Losses:  2.503183364868164 0.8114259243011475
CurrentTrain: epoch  7, batch     0 | loss: 3.3146093Losses:  2.6395065784454346 0.8062953948974609
CurrentTrain: epoch  7, batch     1 | loss: 3.4458020Losses:  2.6146435737609863 0.6323778033256531
CurrentTrain: epoch  7, batch     2 | loss: 3.2470214Losses:  2.4611644744873047 0.6916726231575012
CurrentTrain: epoch  7, batch     3 | loss: 3.1528370Losses:  2.268322229385376 0.7487808465957642
CurrentTrain: epoch  7, batch     4 | loss: 3.0171032Losses:  1.8479115962982178 0.4327962100505829
CurrentTrain: epoch  7, batch     5 | loss: 2.2807078Losses:  1.8865605592727661 0.05528772622346878
CurrentTrain: epoch  7, batch     6 | loss: 1.9418483Losses:  2.0258235931396484 0.39762943983078003
CurrentTrain: epoch  8, batch     0 | loss: 2.4234531Losses:  2.2461061477661133 0.5417707562446594
CurrentTrain: epoch  8, batch     1 | loss: 2.7878768Losses:  2.3592042922973633 0.6693298816680908
CurrentTrain: epoch  8, batch     2 | loss: 3.0285342Losses:  2.960435628890991 0.5982261896133423
CurrentTrain: epoch  8, batch     3 | loss: 3.5586619Losses:  2.0335421562194824 0.4368126392364502
CurrentTrain: epoch  8, batch     4 | loss: 2.4703548Losses:  2.0820603370666504 0.38713616132736206
CurrentTrain: epoch  8, batch     5 | loss: 2.4691966Losses:  1.785829782485962 8.94069742685133e-08
CurrentTrain: epoch  8, batch     6 | loss: 1.7858299Losses:  1.76881742477417 0.27806347608566284
CurrentTrain: epoch  9, batch     0 | loss: 2.0468810Losses:  1.801017165184021 0.43896403908729553
CurrentTrain: epoch  9, batch     1 | loss: 2.2399812Losses:  2.390547513961792 0.42256009578704834
CurrentTrain: epoch  9, batch     2 | loss: 2.8131075Losses:  2.3360543251037598 0.7282966375350952
CurrentTrain: epoch  9, batch     3 | loss: 3.0643511Losses:  2.3263349533081055 0.38744035363197327
CurrentTrain: epoch  9, batch     4 | loss: 2.7137754Losses:  2.4011073112487793 0.6382638216018677
CurrentTrain: epoch  9, batch     5 | loss: 3.0393710Losses:  1.851484775543213 0.03069605864584446
CurrentTrain: epoch  9, batch     6 | loss: 1.8821808
Losses:  5.363816738128662 0.39356836676597595
MemoryTrain:  epoch  0, batch     0 | loss: 5.7573853Losses:  8.63254165649414 0.2553296685218811
MemoryTrain:  epoch  0, batch     1 | loss: 8.8878717Losses:  9.231016159057617 0.7105978727340698
MemoryTrain:  epoch  0, batch     2 | loss: 9.9416142Losses:  10.29071044921875 0.5728882551193237
MemoryTrain:  epoch  0, batch     3 | loss: 10.8635988Losses:  10.416383743286133 0.3781612515449524
MemoryTrain:  epoch  0, batch     4 | loss: 10.7945452Losses:  1.0038095712661743 0.7021907567977905
MemoryTrain:  epoch  1, batch     0 | loss: 1.7060003Losses:  0.8863635063171387 0.41894465684890747
MemoryTrain:  epoch  1, batch     1 | loss: 1.3053081Losses:  0.7954320907592773 0.46804943680763245
MemoryTrain:  epoch  1, batch     2 | loss: 1.2634815Losses:  0.4283938705921173 0.4010140597820282
MemoryTrain:  epoch  1, batch     3 | loss: 0.8294079Losses:  1.11124849319458 0.7447081208229065
MemoryTrain:  epoch  1, batch     4 | loss: 1.8559566Losses:  0.6250883340835571 0.3591991662979126
MemoryTrain:  epoch  2, batch     0 | loss: 0.9842875Losses:  0.391593873500824 0.3770293593406677
MemoryTrain:  epoch  2, batch     1 | loss: 0.7686232Losses:  0.5053145885467529 0.5257973670959473
MemoryTrain:  epoch  2, batch     2 | loss: 1.0311120Losses:  0.36531299352645874 0.41966140270233154
MemoryTrain:  epoch  2, batch     3 | loss: 0.7849744Losses:  0.857143759727478 0.8455331921577454
MemoryTrain:  epoch  2, batch     4 | loss: 1.7026770Losses:  0.553931713104248 0.4870033860206604
MemoryTrain:  epoch  3, batch     0 | loss: 1.0409350Losses:  0.4093216359615326 0.4104387164115906
MemoryTrain:  epoch  3, batch     1 | loss: 0.8197603Losses:  0.5094510912895203 0.48474276065826416
MemoryTrain:  epoch  3, batch     2 | loss: 0.9941939Losses:  0.27836453914642334 0.4126569628715515
MemoryTrain:  epoch  3, batch     3 | loss: 0.6910215Losses:  0.29246658086776733 0.4903814494609833
MemoryTrain:  epoch  3, batch     4 | loss: 0.7828480Losses:  0.371707558631897 0.526999294757843
MemoryTrain:  epoch  4, batch     0 | loss: 0.8987069Losses:  0.30683255195617676 0.4025081396102905
MemoryTrain:  epoch  4, batch     1 | loss: 0.7093407Losses:  0.32970282435417175 0.5010997653007507
MemoryTrain:  epoch  4, batch     2 | loss: 0.8308026Losses:  0.37024277448654175 0.3877604603767395
MemoryTrain:  epoch  4, batch     3 | loss: 0.7580032Losses:  0.2856552302837372 0.49516987800598145
MemoryTrain:  epoch  4, batch     4 | loss: 0.7808251Losses:  0.395580917596817 0.7216801047325134
MemoryTrain:  epoch  5, batch     0 | loss: 1.1172611Losses:  0.33753329515457153 0.48994287848472595
MemoryTrain:  epoch  5, batch     1 | loss: 0.8274761Losses:  0.2641645073890686 0.33233141899108887
MemoryTrain:  epoch  5, batch     2 | loss: 0.5964959Losses:  0.2672196626663208 0.251101553440094
MemoryTrain:  epoch  5, batch     3 | loss: 0.5183212Losses:  0.272438108921051 0.4564846456050873
MemoryTrain:  epoch  5, batch     4 | loss: 0.7289227Losses:  0.35701489448547363 0.7025660276412964
MemoryTrain:  epoch  6, batch     0 | loss: 1.0595809Losses:  0.23237740993499756 0.4117293357849121
MemoryTrain:  epoch  6, batch     1 | loss: 0.6441067Losses:  0.34541773796081543 0.41034650802612305
MemoryTrain:  epoch  6, batch     2 | loss: 0.7557642Losses:  0.2816130518913269 0.5057381987571716
MemoryTrain:  epoch  6, batch     3 | loss: 0.7873513Losses:  0.2675349712371826 0.2541147768497467
MemoryTrain:  epoch  6, batch     4 | loss: 0.5216497Losses:  0.2886883020401001 0.40617239475250244
MemoryTrain:  epoch  7, batch     0 | loss: 0.6948607Losses:  0.28918761014938354 0.49266770482063293
MemoryTrain:  epoch  7, batch     1 | loss: 0.7818553Losses:  0.2602652907371521 0.5241507291793823
MemoryTrain:  epoch  7, batch     2 | loss: 0.7844160Losses:  0.34502696990966797 0.4414503574371338
MemoryTrain:  epoch  7, batch     3 | loss: 0.7864773Losses:  0.21626484394073486 0.35523080825805664
MemoryTrain:  epoch  7, batch     4 | loss: 0.5714957Losses:  0.20843423902988434 0.3916456997394562
MemoryTrain:  epoch  8, batch     0 | loss: 0.6000800Losses:  0.2422635555267334 0.35033494234085083
MemoryTrain:  epoch  8, batch     1 | loss: 0.5925985Losses:  0.3336805999279022 0.5385048389434814
MemoryTrain:  epoch  8, batch     2 | loss: 0.8721855Losses:  0.29506543278694153 0.4928658604621887
MemoryTrain:  epoch  8, batch     3 | loss: 0.7879313Losses:  0.2640555500984192 0.31241849064826965
MemoryTrain:  epoch  8, batch     4 | loss: 0.5764741Losses:  0.2533327341079712 0.45368605852127075
MemoryTrain:  epoch  9, batch     0 | loss: 0.7070188Losses:  0.18712928891181946 0.4261852502822876
MemoryTrain:  epoch  9, batch     1 | loss: 0.6133145Losses:  0.2373199313879013 0.3508601188659668
MemoryTrain:  epoch  9, batch     2 | loss: 0.5881801Losses:  0.28982362151145935 0.47438162565231323
MemoryTrain:  epoch  9, batch     3 | loss: 0.7642052Losses:  0.22929739952087402 0.25391554832458496
MemoryTrain:  epoch  9, batch     4 | loss: 0.4832129
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 44.14%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 41.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 44.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 70.28%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.37%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 69.98%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.95%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 92.43%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 90.31%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 89.86%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.72%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 88.89%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 87.50%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 85.04%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 82.72%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 81.88%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 81.34%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 81.17%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 81.17%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 81.17%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 81.17%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 81.40%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 81.40%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 80.74%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 80.60%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 81.32%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.04%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 82.98%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 82.87%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 82.96%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.54%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 81.94%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 81.31%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.80%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 80.29%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.85%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.76%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.13%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.24%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.15%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 79.53%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 78.87%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 78.33%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 77.74%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 77.22%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 76.70%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 76.44%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 76.08%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 75.83%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 75.58%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 75.29%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 75.33%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.03%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 75.63%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 75.52%   [EVAL] batch:  155 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 75.36%   [EVAL] batch:  157 | acc: 25.00%,  total acc: 75.04%   [EVAL] batch:  158 | acc: 18.75%,  total acc: 74.69%   [EVAL] batch:  159 | acc: 25.00%,  total acc: 74.38%   [EVAL] batch:  160 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:  161 | acc: 18.75%,  total acc: 73.77%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 73.58%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 73.48%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 73.20%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.14%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.04%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 72.72%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 72.30%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 71.50%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 71.12%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 70.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 71.79%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 71.84%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 71.35%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 70.98%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 70.74%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 70.56%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 70.30%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 70.35%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 70.81%   [EVAL] batch:  200 | acc: 18.75%,  total acc: 70.55%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.24%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 69.95%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 69.70%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 69.48%   [EVAL] batch:  205 | acc: 12.50%,  total acc: 69.21%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.14%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 71.09%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 71.44%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 71.55%   
cur_acc:  ['0.9474', '0.7163', '0.6786', '0.6964']
his_acc:  ['0.9474', '0.8325', '0.7593', '0.7155']
Clustering into  24  clusters
Clusters:  [ 3 23 16  4  3  3 22  3 18  1 17  3 14  3 15 21  8  3  3  3  3  3  3 12
  3  1  3 19 13  0  3 20  3  3  0  3  3  3 11  9  4 10  3  3  5  6  2  3
  3  7]
Losses:  6.45728874206543 0.8367561101913452
CurrentTrain: epoch  0, batch     0 | loss: 7.2940450Losses:  8.589225769042969 1.064154863357544
CurrentTrain: epoch  0, batch     1 | loss: 9.6533804Losses:  8.597818374633789 1.6803243160247803
CurrentTrain: epoch  0, batch     2 | loss: 10.2781429Losses:  7.1922760009765625 1.2559521198272705
CurrentTrain: epoch  0, batch     3 | loss: 8.4482279Losses:  7.144055366516113 1.620673418045044
CurrentTrain: epoch  0, batch     4 | loss: 8.7647285Losses:  6.1434221267700195 0.9255839586257935
CurrentTrain: epoch  0, batch     5 | loss: 7.0690060Losses:  6.340350151062012 0.9747084379196167
CurrentTrain: epoch  0, batch     6 | loss: 7.3150587Losses:  2.7077646255493164 1.242077350616455
CurrentTrain: epoch  1, batch     0 | loss: 3.9498420Losses:  2.8208389282226562 1.1587111949920654
CurrentTrain: epoch  1, batch     1 | loss: 3.9795501Losses:  3.1439294815063477 1.0004130601882935
CurrentTrain: epoch  1, batch     2 | loss: 4.1443424Losses:  2.430565595626831 1.2467561960220337
CurrentTrain: epoch  1, batch     3 | loss: 3.6773219Losses:  2.661736488342285 0.5562564730644226
CurrentTrain: epoch  1, batch     4 | loss: 3.2179930Losses:  3.5663247108459473 1.2497880458831787
CurrentTrain: epoch  1, batch     5 | loss: 4.8161125Losses:  1.7772196531295776 0.4883434772491455
CurrentTrain: epoch  1, batch     6 | loss: 2.2655630Losses:  3.1442055702209473 0.9232553839683533
CurrentTrain: epoch  2, batch     0 | loss: 4.0674610Losses:  2.0763511657714844 0.2754228711128235
CurrentTrain: epoch  2, batch     1 | loss: 2.3517740Losses:  2.690260648727417 0.8072935342788696
CurrentTrain: epoch  2, batch     2 | loss: 3.4975543Losses:  2.3711905479431152 0.6634856462478638
CurrentTrain: epoch  2, batch     3 | loss: 3.0346761Losses:  2.4310359954833984 0.8353152275085449
CurrentTrain: epoch  2, batch     4 | loss: 3.2663512Losses:  2.6398701667785645 0.9631325006484985
CurrentTrain: epoch  2, batch     5 | loss: 3.6030025Losses:  1.8395274877548218 0.054808586835861206
CurrentTrain: epoch  2, batch     6 | loss: 1.8943361Losses:  2.6170575618743896 0.7944377660751343
CurrentTrain: epoch  3, batch     0 | loss: 3.4114952Losses:  2.574428081512451 0.8386281132698059
CurrentTrain: epoch  3, batch     1 | loss: 3.4130561Losses:  2.1076831817626953 0.5923036932945251
CurrentTrain: epoch  3, batch     2 | loss: 2.6999869Losses:  2.1600210666656494 1.0321906805038452
CurrentTrain: epoch  3, batch     3 | loss: 3.1922116Losses:  1.9905611276626587 0.7028533220291138
CurrentTrain: epoch  3, batch     4 | loss: 2.6934144Losses:  2.0494818687438965 0.48896485567092896
CurrentTrain: epoch  3, batch     5 | loss: 2.5384467Losses:  1.7864909172058105 0.03949499502778053
CurrentTrain: epoch  3, batch     6 | loss: 1.8259859Losses:  1.8385660648345947 0.1791379302740097
CurrentTrain: epoch  4, batch     0 | loss: 2.0177040Losses:  1.96543550491333 0.548937201499939
CurrentTrain: epoch  4, batch     1 | loss: 2.5143728Losses:  2.1970648765563965 0.7743698954582214
CurrentTrain: epoch  4, batch     2 | loss: 2.9714348Losses:  1.8000164031982422 0.428783118724823
CurrentTrain: epoch  4, batch     3 | loss: 2.2287996Losses:  2.1091978549957275 0.9156358242034912
CurrentTrain: epoch  4, batch     4 | loss: 3.0248337Losses:  2.504523992538452 0.7992032170295715
CurrentTrain: epoch  4, batch     5 | loss: 3.3037271Losses:  2.486236572265625 8.94069742685133e-08
CurrentTrain: epoch  4, batch     6 | loss: 2.4862366Losses:  2.0125467777252197 0.6878287196159363
CurrentTrain: epoch  5, batch     0 | loss: 2.7003756Losses:  1.9445945024490356 0.7964468598365784
CurrentTrain: epoch  5, batch     1 | loss: 2.7410414Losses:  2.3526718616485596 0.5676037073135376
CurrentTrain: epoch  5, batch     2 | loss: 2.9202757Losses:  2.176560401916504 0.4721793532371521
CurrentTrain: epoch  5, batch     3 | loss: 2.6487398Losses:  1.915208339691162 0.319111168384552
CurrentTrain: epoch  5, batch     4 | loss: 2.2343194Losses:  1.7895145416259766 0.5994204878807068
CurrentTrain: epoch  5, batch     5 | loss: 2.3889351Losses:  1.7964727878570557 0.29495692253112793
CurrentTrain: epoch  5, batch     6 | loss: 2.0914297Losses:  2.4276537895202637 0.3482809066772461
CurrentTrain: epoch  6, batch     0 | loss: 2.7759347Losses:  1.9806712865829468 0.6227582097053528
CurrentTrain: epoch  6, batch     1 | loss: 2.6034296Losses:  1.9317054748535156 0.6780354976654053
CurrentTrain: epoch  6, batch     2 | loss: 2.6097410Losses:  1.783200740814209 0.5543884038925171
CurrentTrain: epoch  6, batch     3 | loss: 2.3375893Losses:  1.904951572418213 0.2771034836769104
CurrentTrain: epoch  6, batch     4 | loss: 2.1820550Losses:  1.7533824443817139 0.4096008837223053
CurrentTrain: epoch  6, batch     5 | loss: 2.1629834Losses:  1.760420560836792 0.19444292783737183
CurrentTrain: epoch  6, batch     6 | loss: 1.9548635Losses:  1.7580041885375977 0.5207383632659912
CurrentTrain: epoch  7, batch     0 | loss: 2.2787426Losses:  1.870679259300232 0.45195430517196655
CurrentTrain: epoch  7, batch     1 | loss: 2.3226335Losses:  1.7503647804260254 0.5905905961990356
CurrentTrain: epoch  7, batch     2 | loss: 2.3409553Losses:  2.159503698348999 0.5445516109466553
CurrentTrain: epoch  7, batch     3 | loss: 2.7040553Losses:  1.890453815460205 0.5516529083251953
CurrentTrain: epoch  7, batch     4 | loss: 2.4421067Losses:  1.881327509880066 0.4302898645401001
CurrentTrain: epoch  7, batch     5 | loss: 2.3116174Losses:  2.276799201965332 0.029547367244958878
CurrentTrain: epoch  7, batch     6 | loss: 2.3063467Losses:  2.171128511428833 0.6585510969161987
CurrentTrain: epoch  8, batch     0 | loss: 2.8296795Losses:  1.858185052871704 0.3868112564086914
CurrentTrain: epoch  8, batch     1 | loss: 2.2449963Losses:  1.8635680675506592 0.29447484016418457
CurrentTrain: epoch  8, batch     2 | loss: 2.1580429Losses:  1.8432958126068115 0.27634066343307495
CurrentTrain: epoch  8, batch     3 | loss: 2.1196365Losses:  1.869497537612915 0.4293357729911804
CurrentTrain: epoch  8, batch     4 | loss: 2.2988334Losses:  1.6963592767715454 0.27728086709976196
CurrentTrain: epoch  8, batch     5 | loss: 1.9736402Losses:  1.6991039514541626 0.07469887286424637
CurrentTrain: epoch  8, batch     6 | loss: 1.7738029Losses:  1.8024287223815918 0.2763356566429138
CurrentTrain: epoch  9, batch     0 | loss: 2.0787644Losses:  1.8057093620300293 0.4446069598197937
CurrentTrain: epoch  9, batch     1 | loss: 2.2503164Losses:  1.7082430124282837 0.3928118348121643
CurrentTrain: epoch  9, batch     2 | loss: 2.1010549Losses:  2.1121315956115723 0.4081951677799225
CurrentTrain: epoch  9, batch     3 | loss: 2.5203269Losses:  1.8221619129180908 0.4046987295150757
CurrentTrain: epoch  9, batch     4 | loss: 2.2268605Losses:  1.7109150886535645 0.4440408945083618
CurrentTrain: epoch  9, batch     5 | loss: 2.1549559Losses:  1.6885120868682861 0.017110370099544525
CurrentTrain: epoch  9, batch     6 | loss: 1.7056224
Losses:  5.878683090209961 0.44788482785224915
MemoryTrain:  epoch  0, batch     0 | loss: 6.3265681Losses:  8.541479110717773 0.3971169888973236
MemoryTrain:  epoch  0, batch     1 | loss: 8.9385958Losses:  9.388082504272461 0.36742934584617615
MemoryTrain:  epoch  0, batch     2 | loss: 9.7555122Losses:  10.163335800170898 0.3966246247291565
MemoryTrain:  epoch  0, batch     3 | loss: 10.5599604Losses:  9.645328521728516 0.42322540283203125
MemoryTrain:  epoch  0, batch     4 | loss: 10.0685539Losses:  11.229362487792969 0.6672534942626953
MemoryTrain:  epoch  0, batch     5 | loss: 11.8966160Losses:  10.243267059326172 0.013485550880432129
MemoryTrain:  epoch  0, batch     6 | loss: 10.2567530Losses:  0.6780152320861816 0.4935294985771179
MemoryTrain:  epoch  1, batch     0 | loss: 1.1715448Losses:  0.7432612180709839 0.24168941378593445
MemoryTrain:  epoch  1, batch     1 | loss: 0.9849507Losses:  1.1641545295715332 0.35637611150741577
MemoryTrain:  epoch  1, batch     2 | loss: 1.5205307Losses:  0.6802599430084229 0.3855949938297272
MemoryTrain:  epoch  1, batch     3 | loss: 1.0658549Losses:  0.7338939905166626 0.5661702752113342
MemoryTrain:  epoch  1, batch     4 | loss: 1.3000643Losses:  0.7846944332122803 0.44354939460754395
MemoryTrain:  epoch  1, batch     5 | loss: 1.2282438Losses:  0.6884526014328003 0.2933727502822876
MemoryTrain:  epoch  1, batch     6 | loss: 0.9818254Losses:  0.39435964822769165 0.4250237047672272
MemoryTrain:  epoch  2, batch     0 | loss: 0.8193834Losses:  0.6103782653808594 0.4204443693161011
MemoryTrain:  epoch  2, batch     1 | loss: 1.0308226Losses:  0.5186944007873535 0.2876758873462677
MemoryTrain:  epoch  2, batch     2 | loss: 0.8063703Losses:  1.0547442436218262 0.6947304010391235
MemoryTrain:  epoch  2, batch     3 | loss: 1.7494746Losses:  0.38979214429855347 0.46123141050338745
MemoryTrain:  epoch  2, batch     4 | loss: 0.8510236Losses:  0.34110528230667114 0.4159671664237976
MemoryTrain:  epoch  2, batch     5 | loss: 0.7570724Losses:  0.3912338614463806 0.0437794104218483
MemoryTrain:  epoch  2, batch     6 | loss: 0.4350133Losses:  0.3462975025177002 0.4419090151786804
MemoryTrain:  epoch  3, batch     0 | loss: 0.7882065Losses:  0.4202044606208801 0.40448957681655884
MemoryTrain:  epoch  3, batch     1 | loss: 0.8246940Losses:  0.3649050295352936 0.36227214336395264
MemoryTrain:  epoch  3, batch     2 | loss: 0.7271771Losses:  0.33624696731567383 0.39424216747283936
MemoryTrain:  epoch  3, batch     3 | loss: 0.7304891Losses:  0.34521323442459106 0.33691728115081787
MemoryTrain:  epoch  3, batch     4 | loss: 0.6821305Losses:  0.3912811875343323 0.5242054462432861
MemoryTrain:  epoch  3, batch     5 | loss: 0.9154866Losses:  2.6244866847991943 0.570244550704956
MemoryTrain:  epoch  3, batch     6 | loss: 3.1947312Losses:  0.222891703248024 0.27619096636772156
MemoryTrain:  epoch  4, batch     0 | loss: 0.4990827Losses:  0.4560742676258087 0.42104440927505493
MemoryTrain:  epoch  4, batch     1 | loss: 0.8771187Losses:  0.47717973589897156 0.4602181315422058
MemoryTrain:  epoch  4, batch     2 | loss: 0.9373978Losses:  0.3211953043937683 0.49685904383659363
MemoryTrain:  epoch  4, batch     3 | loss: 0.8180543Losses:  0.3965330123901367 0.5715255737304688
MemoryTrain:  epoch  4, batch     4 | loss: 0.9680586Losses:  0.31150808930397034 0.32741376757621765
MemoryTrain:  epoch  4, batch     5 | loss: 0.6389219Losses:  0.9038065671920776 0.013787655159831047
MemoryTrain:  epoch  4, batch     6 | loss: 0.9175942Losses:  0.33018508553504944 0.5980979204177856
MemoryTrain:  epoch  5, batch     0 | loss: 0.9282830Losses:  0.309803307056427 0.3602007329463959
MemoryTrain:  epoch  5, batch     1 | loss: 0.6700040Losses:  0.3398660719394684 0.4674041271209717
MemoryTrain:  epoch  5, batch     2 | loss: 0.8072702Losses:  0.2941829562187195 0.39585793018341064
MemoryTrain:  epoch  5, batch     3 | loss: 0.6900409Losses:  0.44951504468917847 0.32374346256256104
MemoryTrain:  epoch  5, batch     4 | loss: 0.7732585Losses:  0.24285683035850525 0.3420145809650421
MemoryTrain:  epoch  5, batch     5 | loss: 0.5848714Losses:  1.3947190046310425 0.030621232464909554
MemoryTrain:  epoch  5, batch     6 | loss: 1.4253403Losses:  0.3112124800682068 0.42862895131111145
MemoryTrain:  epoch  6, batch     0 | loss: 0.7398415Losses:  0.22744208574295044 0.21156737208366394
MemoryTrain:  epoch  6, batch     1 | loss: 0.4390095Losses:  0.21512684226036072 0.3394225835800171
MemoryTrain:  epoch  6, batch     2 | loss: 0.5545495Losses:  0.3226357102394104 0.5973182320594788
MemoryTrain:  epoch  6, batch     3 | loss: 0.9199539Losses:  0.33615821599960327 0.5244503617286682
MemoryTrain:  epoch  6, batch     4 | loss: 0.8606086Losses:  0.29062849283218384 0.4927910566329956
MemoryTrain:  epoch  6, batch     5 | loss: 0.7834195Losses:  0.403817355632782 0.04989146068692207
MemoryTrain:  epoch  6, batch     6 | loss: 0.4537088Losses:  0.36873432993888855 0.7082661390304565
MemoryTrain:  epoch  7, batch     0 | loss: 1.0770005Losses:  0.2708472013473511 0.39389389753341675
MemoryTrain:  epoch  7, batch     1 | loss: 0.6647411Losses:  0.29020804166793823 0.3595713973045349
MemoryTrain:  epoch  7, batch     2 | loss: 0.6497794Losses:  0.4469645619392395 0.4343101382255554
MemoryTrain:  epoch  7, batch     3 | loss: 0.8812747Losses:  0.23091912269592285 0.20478203892707825
MemoryTrain:  epoch  7, batch     4 | loss: 0.4357012Losses:  0.2359796166419983 0.2240554690361023
MemoryTrain:  epoch  7, batch     5 | loss: 0.4600351Losses:  0.2909376621246338 0.06150588393211365
MemoryTrain:  epoch  7, batch     6 | loss: 0.3524435Losses:  0.24063757061958313 0.24217264354228973
MemoryTrain:  epoch  8, batch     0 | loss: 0.4828102Losses:  0.3285336196422577 0.5193802714347839
MemoryTrain:  epoch  8, batch     1 | loss: 0.8479139Losses:  0.2510518729686737 0.21655339002609253
MemoryTrain:  epoch  8, batch     2 | loss: 0.4676053Losses:  0.2875533699989319 0.3650769591331482
MemoryTrain:  epoch  8, batch     3 | loss: 0.6526303Losses:  0.3043554127216339 0.35630619525909424
MemoryTrain:  epoch  8, batch     4 | loss: 0.6606616Losses:  0.2813841998577118 0.33066707849502563
MemoryTrain:  epoch  8, batch     5 | loss: 0.6120512Losses:  0.35453420877456665 0.19734328985214233
MemoryTrain:  epoch  8, batch     6 | loss: 0.5518775Losses:  0.2721734046936035 0.19800320267677307
MemoryTrain:  epoch  9, batch     0 | loss: 0.4701766Losses:  0.2356095314025879 0.3790777623653412
MemoryTrain:  epoch  9, batch     1 | loss: 0.6146873Losses:  0.39511650800704956 0.5106300115585327
MemoryTrain:  epoch  9, batch     2 | loss: 0.9057465Losses:  0.3565009832382202 0.44306665658950806
MemoryTrain:  epoch  9, batch     3 | loss: 0.7995676Losses:  0.32257142663002014 0.2873348593711853
MemoryTrain:  epoch  9, batch     4 | loss: 0.6099063Losses:  0.2726130783557892 0.34930160641670227
MemoryTrain:  epoch  9, batch     5 | loss: 0.6219147Losses:  0.2498391568660736 0.027357270941138268
MemoryTrain:  epoch  9, batch     6 | loss: 0.2771964
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 82.93%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.02%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.30%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 83.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.33%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.36%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 87.92%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.09%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 86.90%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 85.35%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 83.71%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.93%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 82.08%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 81.43%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 80.74%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.66%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 81.01%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.17%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 81.40%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 81.18%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 80.96%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 80.89%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.80%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.31%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 83.21%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 83.07%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 82.99%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 83.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.65%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.06%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.91%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 80.63%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 80.08%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.98%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.41%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 79.79%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 79.13%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 78.64%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 78.05%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 77.52%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 77.00%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 76.43%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 75.97%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 75.87%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.54%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.33%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 76.08%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.93%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 75.80%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 75.52%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 75.24%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 75.04%   [EVAL] batch:  158 | acc: 25.00%,  total acc: 74.72%   [EVAL] batch:  159 | acc: 31.25%,  total acc: 74.45%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 74.30%   [EVAL] batch:  161 | acc: 31.25%,  total acc: 74.04%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 73.85%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.78%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.71%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 73.47%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.40%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 73.26%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 72.94%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 72.51%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 72.09%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 71.68%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 71.30%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 70.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 72.13%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.04%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 71.73%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 71.41%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 71.07%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.80%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 70.50%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 70.26%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.61%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 70.35%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 70.16%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 69.94%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 71.83%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 72.11%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 71.75%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.67%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 71.50%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 72.45%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 72.34%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 72.18%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 72.05%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 71.75%   [EVAL] batch:  273 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 72.38%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 73.88%   
cur_acc:  ['0.9474', '0.7163', '0.6786', '0.6964', '0.8333']
his_acc:  ['0.9474', '0.8325', '0.7593', '0.7155', '0.7388']
Clustering into  29  clusters
Clusters:  [ 0 27 18  0  0  0 23  0 21 28 14  0 15  0 16 20 19  0  0  0  0  0  0 25
  0  9  0 24  7 26  0 22  0 13 17  0  0  0 10  4  8 11  0  0 12  6  5  0
  0  2  3  0  0  0  0  1  0  0  0  0]
Losses:  7.115988731384277 1.207151174545288
CurrentTrain: epoch  0, batch     0 | loss: 8.3231401Losses:  9.420865058898926 1.1728496551513672
CurrentTrain: epoch  0, batch     1 | loss: 10.5937147Losses:  9.671958923339844 1.6590793132781982
CurrentTrain: epoch  0, batch     2 | loss: 11.3310385Losses:  7.1122283935546875 1.5570673942565918
CurrentTrain: epoch  0, batch     3 | loss: 8.6692963Losses:  7.418875694274902 1.2070591449737549
CurrentTrain: epoch  0, batch     4 | loss: 8.6259346Losses:  6.277932643890381 1.6038403511047363
CurrentTrain: epoch  0, batch     5 | loss: 7.8817730Losses:  5.812492370605469 0.4656885266304016
CurrentTrain: epoch  0, batch     6 | loss: 6.2781811Losses:  4.667816162109375 1.3035496473312378
CurrentTrain: epoch  1, batch     0 | loss: 5.9713659Losses:  3.6652984619140625 1.4283180236816406
CurrentTrain: epoch  1, batch     1 | loss: 5.0936165Losses:  3.234342336654663 0.7403903007507324
CurrentTrain: epoch  1, batch     2 | loss: 3.9747326Losses:  4.291121959686279 1.4165343046188354
CurrentTrain: epoch  1, batch     3 | loss: 5.7076564Losses:  3.7516989707946777 1.0370571613311768
CurrentTrain: epoch  1, batch     4 | loss: 4.7887564Losses:  3.3961894512176514 1.2649962902069092
CurrentTrain: epoch  1, batch     5 | loss: 4.6611857Losses:  3.095545768737793 0.5956517457962036
CurrentTrain: epoch  1, batch     6 | loss: 3.6911974Losses:  4.110285758972168 1.0259926319122314
CurrentTrain: epoch  2, batch     0 | loss: 5.1362782Losses:  3.123903751373291 1.1834707260131836
CurrentTrain: epoch  2, batch     1 | loss: 4.3073745Losses:  3.763418197631836 1.002284049987793
CurrentTrain: epoch  2, batch     2 | loss: 4.7657022Losses:  2.871641159057617 0.6659505367279053
CurrentTrain: epoch  2, batch     3 | loss: 3.5375917Losses:  3.0404772758483887 1.1717934608459473
CurrentTrain: epoch  2, batch     4 | loss: 4.2122707Losses:  3.2723305225372314 1.307608962059021
CurrentTrain: epoch  2, batch     5 | loss: 4.5799394Losses:  4.6277055740356445 0.5821859836578369
CurrentTrain: epoch  2, batch     6 | loss: 5.2098913Losses:  3.0239744186401367 1.0942026376724243
CurrentTrain: epoch  3, batch     0 | loss: 4.1181769Losses:  3.111863374710083 0.9205183982849121
CurrentTrain: epoch  3, batch     1 | loss: 4.0323820Losses:  3.007294178009033 1.0546845197677612
CurrentTrain: epoch  3, batch     2 | loss: 4.0619788Losses:  2.552917718887329 0.7762808799743652
CurrentTrain: epoch  3, batch     3 | loss: 3.3291986Losses:  2.928661823272705 0.8342792987823486
CurrentTrain: epoch  3, batch     4 | loss: 3.7629411Losses:  3.7722058296203613 0.9338550567626953
CurrentTrain: epoch  3, batch     5 | loss: 4.7060609Losses:  3.724243640899658 0.30176639556884766
CurrentTrain: epoch  3, batch     6 | loss: 4.0260100Losses:  3.331462860107422 1.1564546823501587
CurrentTrain: epoch  4, batch     0 | loss: 4.4879174Losses:  2.1642987728118896 0.6778910160064697
CurrentTrain: epoch  4, batch     1 | loss: 2.8421898Losses:  3.390005111694336 1.059499740600586
CurrentTrain: epoch  4, batch     2 | loss: 4.4495049Losses:  2.63614559173584 0.8724970817565918
CurrentTrain: epoch  4, batch     3 | loss: 3.5086427Losses:  2.750319480895996 0.927247166633606
CurrentTrain: epoch  4, batch     4 | loss: 3.6775665Losses:  2.6614229679107666 0.6961371898651123
CurrentTrain: epoch  4, batch     5 | loss: 3.3575602Losses:  2.0696535110473633 0.18228282034397125
CurrentTrain: epoch  4, batch     6 | loss: 2.2519364Losses:  2.0889177322387695 0.6529549360275269
CurrentTrain: epoch  5, batch     0 | loss: 2.7418728Losses:  3.0148837566375732 0.8906993865966797
CurrentTrain: epoch  5, batch     1 | loss: 3.9055831Losses:  2.4270787239074707 0.7384175062179565
CurrentTrain: epoch  5, batch     2 | loss: 3.1654963Losses:  2.459057331085205 0.7329167723655701
CurrentTrain: epoch  5, batch     3 | loss: 3.1919742Losses:  2.6692380905151367 0.8648321628570557
CurrentTrain: epoch  5, batch     4 | loss: 3.5340703Losses:  2.4218366146087646 0.8915619850158691
CurrentTrain: epoch  5, batch     5 | loss: 3.3133986Losses:  3.939382791519165 0.47613704204559326
CurrentTrain: epoch  5, batch     6 | loss: 4.4155197Losses:  3.015691041946411 0.754128634929657
CurrentTrain: epoch  6, batch     0 | loss: 3.7698197Losses:  2.4230146408081055 0.837412416934967
CurrentTrain: epoch  6, batch     1 | loss: 3.2604270Losses:  2.4681358337402344 0.5205038785934448
CurrentTrain: epoch  6, batch     2 | loss: 2.9886398Losses:  2.052262544631958 0.7886782884597778
CurrentTrain: epoch  6, batch     3 | loss: 2.8409410Losses:  2.093775510787964 0.5035268664360046
CurrentTrain: epoch  6, batch     4 | loss: 2.5973024Losses:  2.36177134513855 0.7557652592658997
CurrentTrain: epoch  6, batch     5 | loss: 3.1175365Losses:  2.5437543392181396 0.3257436454296112
CurrentTrain: epoch  6, batch     6 | loss: 2.8694980Losses:  3.009108781814575 0.6878862380981445
CurrentTrain: epoch  7, batch     0 | loss: 3.6969950Losses:  1.8693697452545166 0.4017623960971832
CurrentTrain: epoch  7, batch     1 | loss: 2.2711322Losses:  2.353668689727783 0.6584988832473755
CurrentTrain: epoch  7, batch     2 | loss: 3.0121675Losses:  1.8507728576660156 0.5744853615760803
CurrentTrain: epoch  7, batch     3 | loss: 2.4252582Losses:  2.1230573654174805 0.7259559631347656
CurrentTrain: epoch  7, batch     4 | loss: 2.8490133Losses:  2.381394863128662 0.7846764922142029
CurrentTrain: epoch  7, batch     5 | loss: 3.1660714Losses:  2.131896495819092 0.15148012340068817
CurrentTrain: epoch  7, batch     6 | loss: 2.2833767Losses:  1.9716784954071045 0.7601111531257629
CurrentTrain: epoch  8, batch     0 | loss: 2.7317896Losses:  3.2570650577545166 0.4603825807571411
CurrentTrain: epoch  8, batch     1 | loss: 3.7174478Losses:  1.940510869026184 0.660495936870575
CurrentTrain: epoch  8, batch     2 | loss: 2.6010067Losses:  2.040559768676758 0.5058153867721558
CurrentTrain: epoch  8, batch     3 | loss: 2.5463753Losses:  2.0318093299865723 0.5310987830162048
CurrentTrain: epoch  8, batch     4 | loss: 2.5629082Losses:  1.8897053003311157 0.5417373180389404
CurrentTrain: epoch  8, batch     5 | loss: 2.4314427Losses:  1.8794441223144531 0.12617939710617065
CurrentTrain: epoch  8, batch     6 | loss: 2.0056236Losses:  2.146373987197876 0.7389113306999207
CurrentTrain: epoch  9, batch     0 | loss: 2.8852854Losses:  2.40342116355896 0.44778376817703247
CurrentTrain: epoch  9, batch     1 | loss: 2.8512049Losses:  1.8488330841064453 0.39032113552093506
CurrentTrain: epoch  9, batch     2 | loss: 2.2391543Losses:  2.5385794639587402 0.6179565191268921
CurrentTrain: epoch  9, batch     3 | loss: 3.1565361Losses:  1.7651278972625732 0.45142778754234314
CurrentTrain: epoch  9, batch     4 | loss: 2.2165556Losses:  1.8306232690811157 0.4648541808128357
CurrentTrain: epoch  9, batch     5 | loss: 2.2954774Losses:  1.9166901111602783 0.40712323784828186
CurrentTrain: epoch  9, batch     6 | loss: 2.3238134
Losses:  5.80526065826416 0.5032219886779785
MemoryTrain:  epoch  0, batch     0 | loss: 6.3084826Losses:  8.196449279785156 0.5105792880058289
MemoryTrain:  epoch  0, batch     1 | loss: 8.7070284Losses:  9.120149612426758 0.40204527974128723
MemoryTrain:  epoch  0, batch     2 | loss: 9.5221949Losses:  9.431282043457031 0.396717369556427
MemoryTrain:  epoch  0, batch     3 | loss: 9.8279991Losses:  10.527961730957031 0.5451005697250366
MemoryTrain:  epoch  0, batch     4 | loss: 11.0730619Losses:  10.051712036132812 0.3431723713874817
MemoryTrain:  epoch  0, batch     5 | loss: 10.3948841Losses:  9.973217010498047 0.5311536192893982
MemoryTrain:  epoch  0, batch     6 | loss: 10.5043707Losses:  10.588943481445312 0.28342771530151367
MemoryTrain:  epoch  0, batch     7 | loss: 10.8723717Losses:  0.7232766151428223 0.4279988408088684
MemoryTrain:  epoch  1, batch     0 | loss: 1.1512754Losses:  0.6387662887573242 0.31387776136398315
MemoryTrain:  epoch  1, batch     1 | loss: 0.9526441Losses:  0.7930804491043091 0.4332789182662964
MemoryTrain:  epoch  1, batch     2 | loss: 1.2263594Losses:  1.3660888671875 0.7362645864486694
MemoryTrain:  epoch  1, batch     3 | loss: 2.1023536Losses:  0.41571566462516785 0.28117626905441284
MemoryTrain:  epoch  1, batch     4 | loss: 0.6968919Losses:  1.1677196025848389 0.5114946365356445
MemoryTrain:  epoch  1, batch     5 | loss: 1.6792142Losses:  0.6312812566757202 0.5279303789138794
MemoryTrain:  epoch  1, batch     6 | loss: 1.1592116Losses:  0.49339956045150757 0.30724093317985535
MemoryTrain:  epoch  1, batch     7 | loss: 0.8006405Losses:  0.553188145160675 0.562358021736145
MemoryTrain:  epoch  2, batch     0 | loss: 1.1155462Losses:  0.8800731897354126 0.35743725299835205
MemoryTrain:  epoch  2, batch     1 | loss: 1.2375104Losses:  0.603728175163269 0.5850836038589478
MemoryTrain:  epoch  2, batch     2 | loss: 1.1888118Losses:  0.228663831949234 0.27930590510368347
MemoryTrain:  epoch  2, batch     3 | loss: 0.5079697Losses:  0.6746983528137207 0.557004451751709
MemoryTrain:  epoch  2, batch     4 | loss: 1.2317028Losses:  0.407342791557312 0.47051534056663513
MemoryTrain:  epoch  2, batch     5 | loss: 0.8778582Losses:  0.5743776559829712 0.32237526774406433
MemoryTrain:  epoch  2, batch     6 | loss: 0.8967530Losses:  0.3217504024505615 0.12353193759918213
MemoryTrain:  epoch  2, batch     7 | loss: 0.4452823Losses:  0.4440949559211731 0.303291380405426
MemoryTrain:  epoch  3, batch     0 | loss: 0.7473863Losses:  0.5201106667518616 0.45814818143844604
MemoryTrain:  epoch  3, batch     1 | loss: 0.9782588Losses:  0.3817608654499054 0.3424152433872223
MemoryTrain:  epoch  3, batch     2 | loss: 0.7241761Losses:  0.4890297055244446 0.5336012840270996
MemoryTrain:  epoch  3, batch     3 | loss: 1.0226309Losses:  0.33846068382263184 0.43351396918296814
MemoryTrain:  epoch  3, batch     4 | loss: 0.7719747Losses:  0.34928858280181885 0.42869624495506287
MemoryTrain:  epoch  3, batch     5 | loss: 0.7779849Losses:  0.465915322303772 0.5387625098228455
MemoryTrain:  epoch  3, batch     6 | loss: 1.0046778Losses:  0.42098861932754517 0.3146047592163086
MemoryTrain:  epoch  3, batch     7 | loss: 0.7355934Losses:  0.5026343464851379 0.6546869277954102
MemoryTrain:  epoch  4, batch     0 | loss: 1.1573212Losses:  0.5196188688278198 0.6769299507141113
MemoryTrain:  epoch  4, batch     1 | loss: 1.1965488Losses:  0.26457369327545166 0.22224006056785583
MemoryTrain:  epoch  4, batch     2 | loss: 0.4868138Losses:  0.33357107639312744 0.36822709441185
MemoryTrain:  epoch  4, batch     3 | loss: 0.7017982Losses:  0.4589657187461853 0.46340832114219666
MemoryTrain:  epoch  4, batch     4 | loss: 0.9223740Losses:  0.4845139980316162 0.3373655378818512
MemoryTrain:  epoch  4, batch     5 | loss: 0.8218795Losses:  0.2945650517940521 0.4293576776981354
MemoryTrain:  epoch  4, batch     6 | loss: 0.7239227Losses:  0.3551521301269531 0.19801905751228333
MemoryTrain:  epoch  4, batch     7 | loss: 0.5531712Losses:  0.3301718831062317 0.3283309042453766
MemoryTrain:  epoch  5, batch     0 | loss: 0.6585028Losses:  0.37238559126853943 0.5708902478218079
MemoryTrain:  epoch  5, batch     1 | loss: 0.9432758Losses:  0.3017388582229614 0.4681285619735718
MemoryTrain:  epoch  5, batch     2 | loss: 0.7698674Losses:  0.5170199871063232 0.5045145750045776
MemoryTrain:  epoch  5, batch     3 | loss: 1.0215346Losses:  0.4170825481414795 0.5712875723838806
MemoryTrain:  epoch  5, batch     4 | loss: 0.9883701Losses:  0.2639940679073334 0.25376713275909424
MemoryTrain:  epoch  5, batch     5 | loss: 0.5177612Losses:  0.3430232107639313 0.38889598846435547
MemoryTrain:  epoch  5, batch     6 | loss: 0.7319192Losses:  0.486568421125412 0.11108309030532837
MemoryTrain:  epoch  5, batch     7 | loss: 0.5976515Losses:  0.34612518548965454 0.3034558594226837
MemoryTrain:  epoch  6, batch     0 | loss: 0.6495811Losses:  0.384991317987442 0.43458127975463867
MemoryTrain:  epoch  6, batch     1 | loss: 0.8195726Losses:  0.40462929010391235 0.41158345341682434
MemoryTrain:  epoch  6, batch     2 | loss: 0.8162128Losses:  0.4041217565536499 0.48612940311431885
MemoryTrain:  epoch  6, batch     3 | loss: 0.8902512Losses:  0.26015862822532654 0.30348843336105347
MemoryTrain:  epoch  6, batch     4 | loss: 0.5636470Losses:  0.39473891258239746 0.6044914722442627
MemoryTrain:  epoch  6, batch     5 | loss: 0.9992304Losses:  0.323194682598114 0.5117857456207275
MemoryTrain:  epoch  6, batch     6 | loss: 0.8349804Losses:  0.30898740887641907 0.09895636141300201
MemoryTrain:  epoch  6, batch     7 | loss: 0.4079438Losses:  0.336147278547287 0.4572892487049103
MemoryTrain:  epoch  7, batch     0 | loss: 0.7934365Losses:  0.2913903295993805 0.31893813610076904
MemoryTrain:  epoch  7, batch     1 | loss: 0.6103284Losses:  0.3039724826812744 0.36346885561943054
MemoryTrain:  epoch  7, batch     2 | loss: 0.6674414Losses:  0.26789286732673645 0.3304377794265747
MemoryTrain:  epoch  7, batch     3 | loss: 0.5983306Losses:  0.3539598286151886 0.35508039593696594
MemoryTrain:  epoch  7, batch     4 | loss: 0.7090402Losses:  0.3436163067817688 0.547777533531189
MemoryTrain:  epoch  7, batch     5 | loss: 0.8913938Losses:  0.4157426357269287 0.4642723798751831
MemoryTrain:  epoch  7, batch     6 | loss: 0.8800150Losses:  0.3290693759918213 0.16009962558746338
MemoryTrain:  epoch  7, batch     7 | loss: 0.4891690Losses:  0.28701651096343994 0.3617590069770813
MemoryTrain:  epoch  8, batch     0 | loss: 0.6487755Losses:  0.31112146377563477 0.44465309381484985
MemoryTrain:  epoch  8, batch     1 | loss: 0.7557746Losses:  0.269359290599823 0.31307703256607056
MemoryTrain:  epoch  8, batch     2 | loss: 0.5824363Losses:  0.2972993850708008 0.39585188031196594
MemoryTrain:  epoch  8, batch     3 | loss: 0.6931512Losses:  0.3583175837993622 0.3399168848991394
MemoryTrain:  epoch  8, batch     4 | loss: 0.6982344Losses:  0.2613956332206726 0.23701107501983643
MemoryTrain:  epoch  8, batch     5 | loss: 0.4984067Losses:  0.3236480951309204 0.3888775110244751
MemoryTrain:  epoch  8, batch     6 | loss: 0.7125256Losses:  0.5343887209892273 0.2628673315048218
MemoryTrain:  epoch  8, batch     7 | loss: 0.7972561Losses:  0.2582286596298218 0.28091755509376526
MemoryTrain:  epoch  9, batch     0 | loss: 0.5391462Losses:  0.33009207248687744 0.5089493989944458
MemoryTrain:  epoch  9, batch     1 | loss: 0.8390415Losses:  0.2904893159866333 0.3639490306377411
MemoryTrain:  epoch  9, batch     2 | loss: 0.6544384Losses:  0.30505144596099854 0.319346159696579
MemoryTrain:  epoch  9, batch     3 | loss: 0.6243976Losses:  0.36966317892074585 0.47848206758499146
MemoryTrain:  epoch  9, batch     4 | loss: 0.8481452Losses:  0.33188074827194214 0.439355731010437
MemoryTrain:  epoch  9, batch     5 | loss: 0.7712365Losses:  0.3160625100135803 0.37361007928848267
MemoryTrain:  epoch  9, batch     6 | loss: 0.6896726Losses:  0.2943410873413086 0.10966841876506805
MemoryTrain:  epoch  9, batch     7 | loss: 0.4040095
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 63.75%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.57%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 52.78%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 50.89%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.14%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 47.92%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 46.57%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 47.07%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 48.48%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 49.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 50.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 51.74%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 52.70%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 53.78%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 54.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 60.24%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 60.33%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 61.08%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 61.57%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 61.51%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 61.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 61.21%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.37%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.17%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 86.73%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 86.10%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 85.59%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 85.21%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 84.84%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.68%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 83.69%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 83.37%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 82.18%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 81.53%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.07%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 80.99%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 80.64%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.90%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.77%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 78.77%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 78.42%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 78.24%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 80.38%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 80.39%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 80.22%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 80.17%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.24%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.85%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.22%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 78.61%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 78.01%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 77.70%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 77.18%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 77.10%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 77.08%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 76.45%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 75.92%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 75.36%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 74.85%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 74.40%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 74.16%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 73.87%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 73.63%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 73.35%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 73.27%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 73.09%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 73.20%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 74.01%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 73.84%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 73.81%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 73.32%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 73.13%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 73.10%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 72.94%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 72.83%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 72.12%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 71.98%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 71.64%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 71.46%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 71.25%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 70.99%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 70.82%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 70.77%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 70.67%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 69.33%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 68.93%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 69.08%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 68.78%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 68.46%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 67.94%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 67.69%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 68.10%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 67.83%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 67.29%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 69.77%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 69.51%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 70.01%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 69.79%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 69.76%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 69.47%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 69.41%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.31%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 69.28%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.33%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 69.91%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 69.91%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 69.76%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 69.64%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 69.52%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 69.45%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 69.21%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 69.09%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 68.98%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 68.73%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 71.01%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.33%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 71.14%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 70.85%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.72%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 70.58%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 70.51%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 70.53%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:  325 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  326 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  331 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:  332 | acc: 37.50%,  total acc: 70.91%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 70.77%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 70.25%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 70.06%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 69.85%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 69.65%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 69.46%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 69.30%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 69.99%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 69.98%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:  364 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.98%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.82%   
cur_acc:  ['0.9474', '0.7163', '0.6786', '0.6964', '0.8333', '0.6121']
his_acc:  ['0.9474', '0.8325', '0.7593', '0.7155', '0.7388', '0.6982']
Clustering into  34  clusters
Clusters:  [ 0 23 24  0  0  0 28  0 19 33 26  0 20  0 17 11  9  0  0  0  0  0  0 25
  0 16  0 27 31 32  0 29  0  0 22  0  0  0 12 21 18  5  0  0 13 30 14  0
  0 15 10  0  0  2  2  8  0  0  0  0  4  7  2  0  0  3  0  6  0  1]
Losses:  6.8193678855896 1.0818548202514648
CurrentTrain: epoch  0, batch     0 | loss: 7.9012227Losses:  8.539077758789062 1.310171365737915
CurrentTrain: epoch  0, batch     1 | loss: 9.8492489Losses:  7.9570183753967285 1.2093091011047363
CurrentTrain: epoch  0, batch     2 | loss: 9.1663275Losses:  7.437925815582275 1.1317853927612305
CurrentTrain: epoch  0, batch     3 | loss: 8.5697117Losses:  7.18739652633667 1.3697552680969238
CurrentTrain: epoch  0, batch     4 | loss: 8.5571518Losses:  5.482759952545166 1.1027987003326416
CurrentTrain: epoch  0, batch     5 | loss: 6.5855589Losses:  6.453736782073975 0.4714478850364685
CurrentTrain: epoch  0, batch     6 | loss: 6.9251847Losses:  4.1536712646484375 0.7067041993141174
CurrentTrain: epoch  1, batch     0 | loss: 4.8603754Losses:  4.177461624145508 0.8367442488670349
CurrentTrain: epoch  1, batch     1 | loss: 5.0142059Losses:  4.29386568069458 1.178615689277649
CurrentTrain: epoch  1, batch     2 | loss: 5.4724813Losses:  3.397712469100952 0.9751594662666321
CurrentTrain: epoch  1, batch     3 | loss: 4.3728719Losses:  3.171576976776123 0.8505772352218628
CurrentTrain: epoch  1, batch     4 | loss: 4.0221543Losses:  3.5835351943969727 0.967014729976654
CurrentTrain: epoch  1, batch     5 | loss: 4.5505500Losses:  3.0297605991363525 0.6461086273193359
CurrentTrain: epoch  1, batch     6 | loss: 3.6758692Losses:  3.4901235103607178 0.7725784182548523
CurrentTrain: epoch  2, batch     0 | loss: 4.2627020Losses:  3.3095059394836426 0.8063225746154785
CurrentTrain: epoch  2, batch     1 | loss: 4.1158285Losses:  3.125072479248047 0.8748095035552979
CurrentTrain: epoch  2, batch     2 | loss: 3.9998820Losses:  3.4173121452331543 0.9723871946334839
CurrentTrain: epoch  2, batch     3 | loss: 4.3896995Losses:  2.8959527015686035 0.4990934729576111
CurrentTrain: epoch  2, batch     4 | loss: 3.3950462Losses:  3.944898843765259 1.1043918132781982
CurrentTrain: epoch  2, batch     5 | loss: 5.0492907Losses:  4.761761665344238 0.4945358633995056
CurrentTrain: epoch  2, batch     6 | loss: 5.2562976Losses:  2.8270044326782227 0.6148556470870972
CurrentTrain: epoch  3, batch     0 | loss: 3.4418602Losses:  3.8001580238342285 1.189886450767517
CurrentTrain: epoch  3, batch     1 | loss: 4.9900446Losses:  2.682156562805176 0.57570880651474
CurrentTrain: epoch  3, batch     2 | loss: 3.2578654Losses:  2.7569692134857178 0.6694283485412598
CurrentTrain: epoch  3, batch     3 | loss: 3.4263976Losses:  3.563277006149292 0.614548921585083
CurrentTrain: epoch  3, batch     4 | loss: 4.1778259Losses:  2.9997024536132812 0.7141356468200684
CurrentTrain: epoch  3, batch     5 | loss: 3.7138381Losses:  2.449902057647705 0.15615758299827576
CurrentTrain: epoch  3, batch     6 | loss: 2.6060596Losses:  2.915652275085449 0.7973060011863708
CurrentTrain: epoch  4, batch     0 | loss: 3.7129583Losses:  2.4575133323669434 0.5109388828277588
CurrentTrain: epoch  4, batch     1 | loss: 2.9684522Losses:  3.4152562618255615 0.7675557732582092
CurrentTrain: epoch  4, batch     2 | loss: 4.1828122Losses:  3.014983892440796 0.8031474947929382
CurrentTrain: epoch  4, batch     3 | loss: 3.8181314Losses:  2.6931910514831543 0.7000110149383545
CurrentTrain: epoch  4, batch     4 | loss: 3.3932021Losses:  2.304212808609009 0.5810333490371704
CurrentTrain: epoch  4, batch     5 | loss: 2.8852463Losses:  3.480705499649048 0.2597532868385315
CurrentTrain: epoch  4, batch     6 | loss: 3.7404587Losses:  2.711622476577759 0.7833260297775269
CurrentTrain: epoch  5, batch     0 | loss: 3.4949484Losses:  3.0524978637695312 0.735496997833252
CurrentTrain: epoch  5, batch     1 | loss: 3.7879949Losses:  2.5771584510803223 0.6390939950942993
CurrentTrain: epoch  5, batch     2 | loss: 3.2162523Losses:  2.2561140060424805 0.5246303081512451
CurrentTrain: epoch  5, batch     3 | loss: 2.7807443Losses:  2.0016613006591797 0.4057416021823883
CurrentTrain: epoch  5, batch     4 | loss: 2.4074030Losses:  3.076603412628174 0.6792786717414856
CurrentTrain: epoch  5, batch     5 | loss: 3.7558820Losses:  2.8454575538635254 0.11083975434303284
CurrentTrain: epoch  5, batch     6 | loss: 2.9562974Losses:  2.246507406234741 0.5724799633026123
CurrentTrain: epoch  6, batch     0 | loss: 2.8189874Losses:  2.7874631881713867 0.5100091695785522
CurrentTrain: epoch  6, batch     1 | loss: 3.2974725Losses:  2.2914047241210938 0.5318478941917419
CurrentTrain: epoch  6, batch     2 | loss: 2.8232527Losses:  1.9894821643829346 0.3468226194381714
CurrentTrain: epoch  6, batch     3 | loss: 2.3363047Losses:  2.5669894218444824 0.5843930244445801
CurrentTrain: epoch  6, batch     4 | loss: 3.1513824Losses:  2.6855926513671875 0.7409415245056152
CurrentTrain: epoch  6, batch     5 | loss: 3.4265342Losses:  3.3780832290649414 0.2155432403087616
CurrentTrain: epoch  6, batch     6 | loss: 3.5936265Losses:  2.023970127105713 0.3939867317676544
CurrentTrain: epoch  7, batch     0 | loss: 2.4179568Losses:  2.1259524822235107 0.35562968254089355
CurrentTrain: epoch  7, batch     1 | loss: 2.4815822Losses:  2.68557071685791 0.5907073020935059
CurrentTrain: epoch  7, batch     2 | loss: 3.2762780Losses:  2.0322771072387695 0.40138116478919983
CurrentTrain: epoch  7, batch     3 | loss: 2.4336584Losses:  2.50620698928833 0.4357787072658539
CurrentTrain: epoch  7, batch     4 | loss: 2.9419856Losses:  2.6848721504211426 0.47196412086486816
CurrentTrain: epoch  7, batch     5 | loss: 3.1568363Losses:  2.191657543182373 2.6822092991096724e-07
CurrentTrain: epoch  7, batch     6 | loss: 2.1916578Losses:  2.214461326599121 0.4774271845817566
CurrentTrain: epoch  8, batch     0 | loss: 2.6918886Losses:  2.4309303760528564 0.3827992081642151
CurrentTrain: epoch  8, batch     1 | loss: 2.8137295Losses:  2.100123405456543 0.39642778038978577
CurrentTrain: epoch  8, batch     2 | loss: 2.4965513Losses:  1.929935097694397 0.2807919383049011
CurrentTrain: epoch  8, batch     3 | loss: 2.2107270Losses:  2.254143238067627 0.4741846024990082
CurrentTrain: epoch  8, batch     4 | loss: 2.7283278Losses:  2.167705535888672 0.4744163751602173
CurrentTrain: epoch  8, batch     5 | loss: 2.6421218Losses:  2.555903911590576 0.04569046199321747
CurrentTrain: epoch  8, batch     6 | loss: 2.6015944Losses:  2.104846477508545 0.40242454409599304
CurrentTrain: epoch  9, batch     0 | loss: 2.5072711Losses:  2.2993674278259277 0.44222837686538696
CurrentTrain: epoch  9, batch     1 | loss: 2.7415957Losses:  2.1723053455352783 0.4182289242744446
CurrentTrain: epoch  9, batch     2 | loss: 2.5905342Losses:  2.2474474906921387 0.5136147737503052
CurrentTrain: epoch  9, batch     3 | loss: 2.7610621Losses:  1.8251287937164307 0.2518022954463959
CurrentTrain: epoch  9, batch     4 | loss: 2.0769310Losses:  2.2265877723693848 0.4724888801574707
CurrentTrain: epoch  9, batch     5 | loss: 2.6990767Losses:  1.792966604232788 0.09578098356723785
CurrentTrain: epoch  9, batch     6 | loss: 1.8887476
Losses:  6.16795539855957 0.3525831699371338
MemoryTrain:  epoch  0, batch     0 | loss: 6.5205383Losses:  8.860995292663574 0.33753806352615356
MemoryTrain:  epoch  0, batch     1 | loss: 9.1985331Losses:  9.260217666625977 0.3692486882209778
MemoryTrain:  epoch  0, batch     2 | loss: 9.6294661Losses:  10.728532791137695 0.49599188566207886
MemoryTrain:  epoch  0, batch     3 | loss: 11.2245245Losses:  10.480016708374023 0.5491346120834351
MemoryTrain:  epoch  0, batch     4 | loss: 11.0291510Losses:  10.20883560180664 0.32855987548828125
MemoryTrain:  epoch  0, batch     5 | loss: 10.5373955Losses:  9.372933387756348 0.26773518323898315
MemoryTrain:  epoch  0, batch     6 | loss: 9.6406689Losses:  10.777531623840332 0.48517146706581116
MemoryTrain:  epoch  0, batch     7 | loss: 11.2627029Losses:  11.062274932861328 0.42870020866394043
MemoryTrain:  epoch  0, batch     8 | loss: 11.4909754Losses:  0.5117772817611694 0.35066866874694824
MemoryTrain:  epoch  1, batch     0 | loss: 0.8624460Losses:  1.2692791223526 0.37941622734069824
MemoryTrain:  epoch  1, batch     1 | loss: 1.6486953Losses:  1.10197913646698 0.4333207607269287
MemoryTrain:  epoch  1, batch     2 | loss: 1.5352999Losses:  1.0588473081588745 0.44976913928985596
MemoryTrain:  epoch  1, batch     3 | loss: 1.5086164Losses:  1.3140143156051636 0.6222774982452393
MemoryTrain:  epoch  1, batch     4 | loss: 1.9362918Losses:  0.616570234298706 0.41174203157424927
MemoryTrain:  epoch  1, batch     5 | loss: 1.0283122Losses:  0.7289868593215942 0.30138099193573
MemoryTrain:  epoch  1, batch     6 | loss: 1.0303679Losses:  0.8516775965690613 0.44833290576934814
MemoryTrain:  epoch  1, batch     7 | loss: 1.3000104Losses:  0.5766329169273376 0.27476537227630615
MemoryTrain:  epoch  1, batch     8 | loss: 0.8513983Losses:  0.9389278888702393 0.6424365043640137
MemoryTrain:  epoch  2, batch     0 | loss: 1.5813644Losses:  0.27498680353164673 0.29185694456100464
MemoryTrain:  epoch  2, batch     1 | loss: 0.5668437Losses:  0.515552282333374 0.44139501452445984
MemoryTrain:  epoch  2, batch     2 | loss: 0.9569473Losses:  0.5191082954406738 0.4834854006767273
MemoryTrain:  epoch  2, batch     3 | loss: 1.0025938Losses:  0.37027907371520996 0.2902352809906006
MemoryTrain:  epoch  2, batch     4 | loss: 0.6605144Losses:  0.8584693670272827 0.3851839005947113
MemoryTrain:  epoch  2, batch     5 | loss: 1.2436533Losses:  0.601866602897644 0.4224720299243927
MemoryTrain:  epoch  2, batch     6 | loss: 1.0243386Losses:  0.8487160205841064 0.3695981800556183
MemoryTrain:  epoch  2, batch     7 | loss: 1.2183142Losses:  0.9291050434112549 0.3017899692058563
MemoryTrain:  epoch  2, batch     8 | loss: 1.2308950Losses:  0.44669634103775024 0.35309234261512756
MemoryTrain:  epoch  3, batch     0 | loss: 0.7997887Losses:  0.631470799446106 0.3761318325996399
MemoryTrain:  epoch  3, batch     1 | loss: 1.0076027Losses:  0.6648472547531128 0.3388712704181671
MemoryTrain:  epoch  3, batch     2 | loss: 1.0037185Losses:  0.29967689514160156 0.2997511625289917
MemoryTrain:  epoch  3, batch     3 | loss: 0.5994281Losses:  0.4835031032562256 0.4733809232711792
MemoryTrain:  epoch  3, batch     4 | loss: 0.9568840Losses:  0.4187396764755249 0.4855971932411194
MemoryTrain:  epoch  3, batch     5 | loss: 0.9043369Losses:  0.6191076636314392 0.4428666830062866
MemoryTrain:  epoch  3, batch     6 | loss: 1.0619743Losses:  0.4649675488471985 0.43629786372184753
MemoryTrain:  epoch  3, batch     7 | loss: 0.9012654Losses:  0.665742039680481 0.3062172532081604
MemoryTrain:  epoch  3, batch     8 | loss: 0.9719593Losses:  0.3904258906841278 0.3228115141391754
MemoryTrain:  epoch  4, batch     0 | loss: 0.7132374Losses:  0.5097015500068665 0.37569770216941833
MemoryTrain:  epoch  4, batch     1 | loss: 0.8853992Losses:  0.31494253873825073 0.36109185218811035
MemoryTrain:  epoch  4, batch     2 | loss: 0.6760344Losses:  0.43538519740104675 0.418390154838562
MemoryTrain:  epoch  4, batch     3 | loss: 0.8537754Losses:  0.5855701565742493 0.3890836536884308
MemoryTrain:  epoch  4, batch     4 | loss: 0.9746538Losses:  0.525532066822052 0.43239420652389526
MemoryTrain:  epoch  4, batch     5 | loss: 0.9579263Losses:  0.3199211359024048 0.25394219160079956
MemoryTrain:  epoch  4, batch     6 | loss: 0.5738633Losses:  0.34563010931015015 0.37430042028427124
MemoryTrain:  epoch  4, batch     7 | loss: 0.7199305Losses:  0.6086055040359497 0.4863605499267578
MemoryTrain:  epoch  4, batch     8 | loss: 1.0949661Losses:  0.4124166667461395 0.4580327868461609
MemoryTrain:  epoch  5, batch     0 | loss: 0.8704494Losses:  0.3155040740966797 0.2608362138271332
MemoryTrain:  epoch  5, batch     1 | loss: 0.5763403Losses:  0.3724818825721741 0.4533444046974182
MemoryTrain:  epoch  5, batch     2 | loss: 0.8258263Losses:  0.2848530411720276 0.33173075318336487
MemoryTrain:  epoch  5, batch     3 | loss: 0.6165838Losses:  0.4258062243461609 0.4062449336051941
MemoryTrain:  epoch  5, batch     4 | loss: 0.8320512Losses:  0.4388047158718109 0.33676084876060486
MemoryTrain:  epoch  5, batch     5 | loss: 0.7755656Losses:  0.45801758766174316 0.4280311167240143
MemoryTrain:  epoch  5, batch     6 | loss: 0.8860487Losses:  0.5268131494522095 0.4075421988964081
MemoryTrain:  epoch  5, batch     7 | loss: 0.9343554Losses:  0.382271409034729 0.3122709393501282
MemoryTrain:  epoch  5, batch     8 | loss: 0.6945423Losses:  0.41099411249160767 0.33491671085357666
MemoryTrain:  epoch  6, batch     0 | loss: 0.7459108Losses:  0.4040369987487793 0.348344624042511
MemoryTrain:  epoch  6, batch     1 | loss: 0.7523816Losses:  0.2859203815460205 0.3610571622848511
MemoryTrain:  epoch  6, batch     2 | loss: 0.6469775Losses:  0.4295613467693329 0.49448394775390625
MemoryTrain:  epoch  6, batch     3 | loss: 0.9240453Losses:  0.3812071681022644 0.46982279419898987
MemoryTrain:  epoch  6, batch     4 | loss: 0.8510300Losses:  0.437686562538147 0.370297908782959
MemoryTrain:  epoch  6, batch     5 | loss: 0.8079845Losses:  0.4908679723739624 0.3680083751678467
MemoryTrain:  epoch  6, batch     6 | loss: 0.8588763Losses:  0.3332388401031494 0.3275206685066223
MemoryTrain:  epoch  6, batch     7 | loss: 0.6607595Losses:  0.4084474742412567 0.25065433979034424
MemoryTrain:  epoch  6, batch     8 | loss: 0.6591018Losses:  0.3727235496044159 0.43358272314071655
MemoryTrain:  epoch  7, batch     0 | loss: 0.8063062Losses:  0.47141867876052856 0.4653073847293854
MemoryTrain:  epoch  7, batch     1 | loss: 0.9367261Losses:  0.3394930362701416 0.33331194519996643
MemoryTrain:  epoch  7, batch     2 | loss: 0.6728050Losses:  0.37902402877807617 0.3621063232421875
MemoryTrain:  epoch  7, batch     3 | loss: 0.7411304Losses:  0.46884685754776 0.4112311005592346
MemoryTrain:  epoch  7, batch     4 | loss: 0.8800780Losses:  0.34798985719680786 0.23716504871845245
MemoryTrain:  epoch  7, batch     5 | loss: 0.5851549Losses:  0.3199068307876587 0.29023680090904236
MemoryTrain:  epoch  7, batch     6 | loss: 0.6101437Losses:  0.38182681798934937 0.3452630639076233
MemoryTrain:  epoch  7, batch     7 | loss: 0.7270899Losses:  0.5658966302871704 0.43430936336517334
MemoryTrain:  epoch  7, batch     8 | loss: 1.0002060Losses:  0.4184005558490753 0.41898682713508606
MemoryTrain:  epoch  8, batch     0 | loss: 0.8373874Losses:  0.409885048866272 0.38852831721305847
MemoryTrain:  epoch  8, batch     1 | loss: 0.7984134Losses:  0.375957190990448 0.3465345799922943
MemoryTrain:  epoch  8, batch     2 | loss: 0.7224917Losses:  0.322391539812088 0.25612038373947144
MemoryTrain:  epoch  8, batch     3 | loss: 0.5785120Losses:  0.42814159393310547 0.5579191446304321
MemoryTrain:  epoch  8, batch     4 | loss: 0.9860607Losses:  0.436045378446579 0.4955902099609375
MemoryTrain:  epoch  8, batch     5 | loss: 0.9316356Losses:  0.2626839876174927 0.26078593730926514
MemoryTrain:  epoch  8, batch     6 | loss: 0.5234699Losses:  0.46048399806022644 0.3894674479961395
MemoryTrain:  epoch  8, batch     7 | loss: 0.8499514Losses:  0.29156893491744995 0.18591885268688202
MemoryTrain:  epoch  8, batch     8 | loss: 0.4774878Losses:  0.3725696802139282 0.3044416308403015
MemoryTrain:  epoch  9, batch     0 | loss: 0.6770113Losses:  0.26766330003738403 0.23003730177879333
MemoryTrain:  epoch  9, batch     1 | loss: 0.4977006Losses:  0.33289745450019836 0.2532747983932495
MemoryTrain:  epoch  9, batch     2 | loss: 0.5861722Losses:  0.31234118342399597 0.2528400719165802
MemoryTrain:  epoch  9, batch     3 | loss: 0.5651813Losses:  0.3326898217201233 0.353154718875885
MemoryTrain:  epoch  9, batch     4 | loss: 0.6858445Losses:  0.38082098960876465 0.37845578789711
MemoryTrain:  epoch  9, batch     5 | loss: 0.7592767Losses:  0.4227895140647888 0.45907390117645264
MemoryTrain:  epoch  9, batch     6 | loss: 0.8818634Losses:  0.44516080617904663 0.35567474365234375
MemoryTrain:  epoch  9, batch     7 | loss: 0.8008355Losses:  0.46986231207847595 0.4234088659286499
MemoryTrain:  epoch  9, batch     8 | loss: 0.8932712
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 57.11%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 55.42%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.63%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 54.10%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 60.53%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 59.94%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 59.53%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 58.87%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 58.38%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 57.34%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 56.65%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 56.12%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 55.74%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 55.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 55.64%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 56.13%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 56.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 56.71%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 57.70%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 58.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.51%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 59.22%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 59.69%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 60.42%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.27%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 86.70%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 86.18%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 85.34%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 84.85%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 84.48%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 83.53%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 82.60%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 81.81%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 81.34%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.07%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.16%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 80.47%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.07%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.17%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 79.69%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 79.09%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 77.78%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 77.44%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.96%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 76.41%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 76.03%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 75.36%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 75.07%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 77.91%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 77.82%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 77.67%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 77.64%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 77.45%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.91%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 76.26%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 75.80%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 75.51%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 74.38%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 73.87%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 73.32%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 72.35%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 72.12%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 71.90%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 71.63%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 71.37%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 72.42%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 72.26%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 71.50%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 71.32%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 70.92%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 70.70%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 70.48%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:  158 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 69.45%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 69.25%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 68.57%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 68.17%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 67.77%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 67.38%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 67.03%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 66.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 67.74%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 66.85%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.63%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 66.39%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 66.92%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 66.62%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 66.21%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 66.01%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 65.81%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.04%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 67.69%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 68.23%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 68.03%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 67.91%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 68.34%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.37%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.30%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:  262 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 68.13%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 68.09%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.00%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 67.70%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 67.46%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 66.93%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 67.85%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.88%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 69.10%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 69.31%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 69.11%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 68.89%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 68.67%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 68.46%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 68.13%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 68.08%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 67.50%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.32%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 67.12%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 66.94%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 66.78%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 66.64%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 67.48%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 67.49%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 67.52%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 67.53%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 67.49%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 67.34%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  388 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:  389 | acc: 6.25%,  total acc: 67.37%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  391 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 66.91%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 67.13%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 66.96%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 66.81%   [EVAL] batch:  403 | acc: 18.75%,  total acc: 66.69%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 66.54%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 66.38%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 66.68%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  413 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:  414 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 66.58%   [EVAL] batch:  417 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  418 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:  419 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 66.23%   [EVAL] batch:  422 | acc: 31.25%,  total acc: 66.15%   [EVAL] batch:  423 | acc: 37.50%,  total acc: 66.08%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 66.42%   
cur_acc:  ['0.9474', '0.7163', '0.6786', '0.6964', '0.8333', '0.6121', '0.6042']
his_acc:  ['0.9474', '0.8325', '0.7593', '0.7155', '0.7388', '0.6982', '0.6642']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0  0  0  0 12
  0 37  0 23 31 36  2 34  0  0 30  0  0  0 27 11 18 29  0  0 28 32 13  0
  0 17 16  0  0  1  1 20  0  0  0  0 15  9  1  0  0  8  0 14  0  7  5 10
  0  0  0  2  0  3  6  4]
Losses:  6.87617826461792 1.0381492376327515
CurrentTrain: epoch  0, batch     0 | loss: 7.9143276Losses:  9.11080265045166 0.8755794167518616
CurrentTrain: epoch  0, batch     1 | loss: 9.9863825Losses:  7.439516067504883 0.8842092752456665
CurrentTrain: epoch  0, batch     2 | loss: 8.3237257Losses:  8.205547332763672 0.7789076566696167
CurrentTrain: epoch  0, batch     3 | loss: 8.9844551Losses:  6.745279312133789 1.3462905883789062
CurrentTrain: epoch  0, batch     4 | loss: 8.0915699Losses:  5.393572807312012 1.0131585597991943
CurrentTrain: epoch  0, batch     5 | loss: 6.4067316Losses:  5.164699554443359 0.13845030963420868
CurrentTrain: epoch  0, batch     6 | loss: 5.3031497Losses:  3.359147548675537 0.8969409465789795
CurrentTrain: epoch  1, batch     0 | loss: 4.2560883Losses:  3.3376448154449463 0.9010740518569946
CurrentTrain: epoch  1, batch     1 | loss: 4.2387190Losses:  2.848027229309082 0.8246116042137146
CurrentTrain: epoch  1, batch     2 | loss: 3.6726389Losses:  3.922677516937256 1.0302295684814453
CurrentTrain: epoch  1, batch     3 | loss: 4.9529071Losses:  3.4914605617523193 0.8712116479873657
CurrentTrain: epoch  1, batch     4 | loss: 4.3626723Losses:  2.995356559753418 0.8728996515274048
CurrentTrain: epoch  1, batch     5 | loss: 3.8682561Losses:  2.7054433822631836 0.06927748024463654
CurrentTrain: epoch  1, batch     6 | loss: 2.7747209Losses:  3.818969249725342 1.0225645303726196
CurrentTrain: epoch  2, batch     0 | loss: 4.8415337Losses:  3.7414464950561523 0.7306721210479736
CurrentTrain: epoch  2, batch     1 | loss: 4.4721184Losses:  2.1558918952941895 0.4984790086746216
CurrentTrain: epoch  2, batch     2 | loss: 2.6543708Losses:  2.895014762878418 0.679526686668396
CurrentTrain: epoch  2, batch     3 | loss: 3.5745416Losses:  2.7262096405029297 0.7776089310646057
CurrentTrain: epoch  2, batch     4 | loss: 3.5038185Losses:  3.029414653778076 0.6988040804862976
CurrentTrain: epoch  2, batch     5 | loss: 3.7282188Losses:  2.1236495971679688 0.03919075429439545
CurrentTrain: epoch  2, batch     6 | loss: 2.1628404Losses:  2.683265209197998 0.3866080641746521
CurrentTrain: epoch  3, batch     0 | loss: 3.0698733Losses:  2.680849075317383 0.7055127620697021
CurrentTrain: epoch  3, batch     1 | loss: 3.3863618Losses:  2.3519773483276367 0.41224655508995056
CurrentTrain: epoch  3, batch     2 | loss: 2.7642238Losses:  2.8549585342407227 0.6381323337554932
CurrentTrain: epoch  3, batch     3 | loss: 3.4930909Losses:  2.961740016937256 0.6313442587852478
CurrentTrain: epoch  3, batch     4 | loss: 3.5930843Losses:  2.4293103218078613 0.6902413368225098
CurrentTrain: epoch  3, batch     5 | loss: 3.1195517Losses:  1.7638845443725586 0.16509389877319336
CurrentTrain: epoch  3, batch     6 | loss: 1.9289784Losses:  3.6419098377227783 0.40450727939605713
CurrentTrain: epoch  4, batch     0 | loss: 4.0464172Losses:  2.0235607624053955 0.40960079431533813
CurrentTrain: epoch  4, batch     1 | loss: 2.4331615Losses:  2.192000389099121 0.38245370984077454
CurrentTrain: epoch  4, batch     2 | loss: 2.5744541Losses:  2.1014537811279297 0.4508092403411865
CurrentTrain: epoch  4, batch     3 | loss: 2.5522630Losses:  2.4205119609832764 0.819092869758606
CurrentTrain: epoch  4, batch     4 | loss: 3.2396049Losses:  1.8965767621994019 0.4301964044570923
CurrentTrain: epoch  4, batch     5 | loss: 2.3267732Losses:  2.79819393157959 0.2703034281730652
CurrentTrain: epoch  4, batch     6 | loss: 3.0684974Losses:  2.4248900413513184 0.6466706991195679
CurrentTrain: epoch  5, batch     0 | loss: 3.0715609Losses:  1.854313850402832 0.25787612795829773
CurrentTrain: epoch  5, batch     1 | loss: 2.1121900Losses:  2.2244157791137695 0.595090389251709
CurrentTrain: epoch  5, batch     2 | loss: 2.8195062Losses:  2.355508804321289 0.5839523077011108
CurrentTrain: epoch  5, batch     3 | loss: 2.9394612Losses:  2.140934944152832 0.540374755859375
CurrentTrain: epoch  5, batch     4 | loss: 2.6813097Losses:  2.0650899410247803 0.5342152118682861
CurrentTrain: epoch  5, batch     5 | loss: 2.5993052Losses:  1.7457633018493652 0.18988266587257385
CurrentTrain: epoch  5, batch     6 | loss: 1.9356459Losses:  2.0774624347686768 0.4625592827796936
CurrentTrain: epoch  6, batch     0 | loss: 2.5400217Losses:  2.1689305305480957 0.5013214349746704
CurrentTrain: epoch  6, batch     1 | loss: 2.6702518Losses:  1.7495614290237427 0.32757657766342163
CurrentTrain: epoch  6, batch     2 | loss: 2.0771379Losses:  1.992210030555725 0.554813027381897
CurrentTrain: epoch  6, batch     3 | loss: 2.5470231Losses:  1.813319444656372 0.2479580044746399
CurrentTrain: epoch  6, batch     4 | loss: 2.0612774Losses:  2.185685157775879 0.4421176612377167
CurrentTrain: epoch  6, batch     5 | loss: 2.6278028Losses:  1.7562531232833862 0.1132640689611435
CurrentTrain: epoch  6, batch     6 | loss: 1.8695172Losses:  1.9164113998413086 0.32078030705451965
CurrentTrain: epoch  7, batch     0 | loss: 2.2371917Losses:  2.260554790496826 0.3699648380279541
CurrentTrain: epoch  7, batch     1 | loss: 2.6305196Losses:  1.9235403537750244 0.34812402725219727
CurrentTrain: epoch  7, batch     2 | loss: 2.2716644Losses:  1.77421236038208 0.40979236364364624
CurrentTrain: epoch  7, batch     3 | loss: 2.1840048Losses:  1.7218528985977173 0.29131484031677246
CurrentTrain: epoch  7, batch     4 | loss: 2.0131679Losses:  1.862515926361084 0.41757720708847046
CurrentTrain: epoch  7, batch     5 | loss: 2.2800932Losses:  1.8123087882995605 0.10492828488349915
CurrentTrain: epoch  7, batch     6 | loss: 1.9172370Losses:  1.89788818359375 0.3500714600086212
CurrentTrain: epoch  8, batch     0 | loss: 2.2479596Losses:  1.7050858736038208 0.17847026884555817
CurrentTrain: epoch  8, batch     1 | loss: 1.8835561Losses:  1.7615331411361694 0.3249053359031677
CurrentTrain: epoch  8, batch     2 | loss: 2.0864384Losses:  1.8545098304748535 0.23630400002002716
CurrentTrain: epoch  8, batch     3 | loss: 2.0908139Losses:  1.8299024105072021 0.43344053626060486
CurrentTrain: epoch  8, batch     4 | loss: 2.2633429Losses:  1.908564805984497 0.3714648485183716
CurrentTrain: epoch  8, batch     5 | loss: 2.2800298Losses:  1.6889344453811646 0.12651026248931885
CurrentTrain: epoch  8, batch     6 | loss: 1.8154447Losses:  1.6935029029846191 0.2894449830055237
CurrentTrain: epoch  9, batch     0 | loss: 1.9829478Losses:  1.745226502418518 0.29083386063575745
CurrentTrain: epoch  9, batch     1 | loss: 2.0360603Losses:  1.8140716552734375 0.33703964948654175
CurrentTrain: epoch  9, batch     2 | loss: 2.1511114Losses:  1.8128011226654053 0.30989986658096313
CurrentTrain: epoch  9, batch     3 | loss: 2.1227009Losses:  1.8544414043426514 0.36272770166397095
CurrentTrain: epoch  9, batch     4 | loss: 2.2171690Losses:  1.7105050086975098 0.29508161544799805
CurrentTrain: epoch  9, batch     5 | loss: 2.0055866Losses:  1.7108054161071777 0.12848573923110962
CurrentTrain: epoch  9, batch     6 | loss: 1.8392911
Losses:  6.521254539489746 0.5145608186721802
MemoryTrain:  epoch  0, batch     0 | loss: 7.0358152Losses:  8.094722747802734 0.31359028816223145
MemoryTrain:  epoch  0, batch     1 | loss: 8.4083128Losses:  8.746990203857422 0.14772343635559082
MemoryTrain:  epoch  0, batch     2 | loss: 8.8947134Losses:  10.06173038482666 0.5262546539306641
MemoryTrain:  epoch  0, batch     3 | loss: 10.5879850Losses:  10.309499740600586 0.3650652766227722
MemoryTrain:  epoch  0, batch     4 | loss: 10.6745653Losses:  10.150351524353027 0.22557300329208374
MemoryTrain:  epoch  0, batch     5 | loss: 10.3759241Losses:  11.248451232910156 0.44179415702819824
MemoryTrain:  epoch  0, batch     6 | loss: 11.6902456Losses:  10.896405220031738 0.5371540188789368
MemoryTrain:  epoch  0, batch     7 | loss: 11.4335594Losses:  10.52309799194336 0.3265184462070465
MemoryTrain:  epoch  0, batch     8 | loss: 10.8496161Losses:  11.22137451171875 0.503095269203186
MemoryTrain:  epoch  0, batch     9 | loss: 11.7244701Losses:  0.9381297826766968 0.4782025218009949
MemoryTrain:  epoch  1, batch     0 | loss: 1.4163322Losses:  1.2703568935394287 0.2828196883201599
MemoryTrain:  epoch  1, batch     1 | loss: 1.5531766Losses:  1.0390255451202393 0.2225981503725052
MemoryTrain:  epoch  1, batch     2 | loss: 1.2616237Losses:  0.5255467891693115 0.4119687080383301
MemoryTrain:  epoch  1, batch     3 | loss: 0.9375155Losses:  0.8488003611564636 0.3283545672893524
MemoryTrain:  epoch  1, batch     4 | loss: 1.1771549Losses:  0.3708862364292145 0.15223725140094757
MemoryTrain:  epoch  1, batch     5 | loss: 0.5231235Losses:  1.6054753065109253 0.5782567262649536
MemoryTrain:  epoch  1, batch     6 | loss: 2.1837320Losses:  0.943577766418457 0.49275559186935425
MemoryTrain:  epoch  1, batch     7 | loss: 1.4363334Losses:  1.6403164863586426 0.610696017742157
MemoryTrain:  epoch  1, batch     8 | loss: 2.2510126Losses:  1.3757078647613525 0.3448975384235382
MemoryTrain:  epoch  1, batch     9 | loss: 1.7206054Losses:  0.7092798948287964 0.3562220335006714
MemoryTrain:  epoch  2, batch     0 | loss: 1.0655019Losses:  0.569072961807251 0.39856666326522827
MemoryTrain:  epoch  2, batch     1 | loss: 0.9676396Losses:  0.7764618992805481 0.3354541063308716
MemoryTrain:  epoch  2, batch     2 | loss: 1.1119161Losses:  0.4440937638282776 0.5286707282066345
MemoryTrain:  epoch  2, batch     3 | loss: 0.9727645Losses:  1.1720147132873535 0.4154629111289978
MemoryTrain:  epoch  2, batch     4 | loss: 1.5874777Losses:  0.6524389982223511 0.5063083171844482
MemoryTrain:  epoch  2, batch     5 | loss: 1.1587473Losses:  0.4877735376358032 0.3070760667324066
MemoryTrain:  epoch  2, batch     6 | loss: 0.7948496Losses:  0.43508845567703247 0.309706449508667
MemoryTrain:  epoch  2, batch     7 | loss: 0.7447949Losses:  0.9413390159606934 0.4739278554916382
MemoryTrain:  epoch  2, batch     8 | loss: 1.4152669Losses:  0.6886594295501709 0.32770174741744995
MemoryTrain:  epoch  2, batch     9 | loss: 1.0163612Losses:  0.4597925543785095 0.32120004296302795
MemoryTrain:  epoch  3, batch     0 | loss: 0.7809926Losses:  0.3755589425563812 0.3355620205402374
MemoryTrain:  epoch  3, batch     1 | loss: 0.7111210Losses:  0.52583247423172 0.3884809613227844
MemoryTrain:  epoch  3, batch     2 | loss: 0.9143134Losses:  0.8202383518218994 0.3796217143535614
MemoryTrain:  epoch  3, batch     3 | loss: 1.1998601Losses:  0.4492369294166565 0.26043224334716797
MemoryTrain:  epoch  3, batch     4 | loss: 0.7096692Losses:  0.49795135855674744 0.39781737327575684
MemoryTrain:  epoch  3, batch     5 | loss: 0.8957688Losses:  0.6806321144104004 0.5957584381103516
MemoryTrain:  epoch  3, batch     6 | loss: 1.2763906Losses:  0.6974388360977173 0.38101473450660706
MemoryTrain:  epoch  3, batch     7 | loss: 1.0784535Losses:  0.42106232047080994 0.389202356338501
MemoryTrain:  epoch  3, batch     8 | loss: 0.8102647Losses:  0.5136035680770874 0.2704305052757263
MemoryTrain:  epoch  3, batch     9 | loss: 0.7840341Losses:  0.4815816283226013 0.5387095212936401
MemoryTrain:  epoch  4, batch     0 | loss: 1.0202911Losses:  0.442577987909317 0.30428069829940796
MemoryTrain:  epoch  4, batch     1 | loss: 0.7468587Losses:  0.6062244176864624 0.38077181577682495
MemoryTrain:  epoch  4, batch     2 | loss: 0.9869962Losses:  0.407117635011673 0.3182377219200134
MemoryTrain:  epoch  4, batch     3 | loss: 0.7253554Losses:  0.5096975564956665 0.31942272186279297
MemoryTrain:  epoch  4, batch     4 | loss: 0.8291203Losses:  0.393687903881073 0.2748148441314697
MemoryTrain:  epoch  4, batch     5 | loss: 0.6685027Losses:  0.6342654228210449 0.38849687576293945
MemoryTrain:  epoch  4, batch     6 | loss: 1.0227623Losses:  0.37929844856262207 0.3105155825614929
MemoryTrain:  epoch  4, batch     7 | loss: 0.6898140Losses:  0.7913324236869812 0.5896600484848022
MemoryTrain:  epoch  4, batch     8 | loss: 1.3809924Losses:  0.385979562997818 0.34594351053237915
MemoryTrain:  epoch  4, batch     9 | loss: 0.7319231Losses:  0.4460589587688446 0.3120249807834625
MemoryTrain:  epoch  5, batch     0 | loss: 0.7580839Losses:  0.37949123978614807 0.34603041410446167
MemoryTrain:  epoch  5, batch     1 | loss: 0.7255217Losses:  0.5421280860900879 0.47728896141052246
MemoryTrain:  epoch  5, batch     2 | loss: 1.0194170Losses:  0.47777026891708374 0.49541983008384705
MemoryTrain:  epoch  5, batch     3 | loss: 0.9731901Losses:  0.6000185608863831 0.5036160945892334
MemoryTrain:  epoch  5, batch     4 | loss: 1.1036346Losses:  0.33224794268608093 0.2505606710910797
MemoryTrain:  epoch  5, batch     5 | loss: 0.5828086Losses:  0.3893321752548218 0.33828574419021606
MemoryTrain:  epoch  5, batch     6 | loss: 0.7276179Losses:  0.7500855922698975 0.5221481323242188
MemoryTrain:  epoch  5, batch     7 | loss: 1.2722337Losses:  0.36966991424560547 0.21261511743068695
MemoryTrain:  epoch  5, batch     8 | loss: 0.5822850Losses:  0.3729156255722046 0.2560948133468628
MemoryTrain:  epoch  5, batch     9 | loss: 0.6290104Losses:  0.47735410928726196 0.4300476014614105
MemoryTrain:  epoch  6, batch     0 | loss: 0.9074017Losses:  0.33455121517181396 0.32393878698349
MemoryTrain:  epoch  6, batch     1 | loss: 0.6584900Losses:  0.2958211302757263 0.20380976796150208
MemoryTrain:  epoch  6, batch     2 | loss: 0.4996309Losses:  0.4297434687614441 0.4485572278499603
MemoryTrain:  epoch  6, batch     3 | loss: 0.8783007Losses:  0.40566468238830566 0.27485111355781555
MemoryTrain:  epoch  6, batch     4 | loss: 0.6805158Losses:  0.4066630005836487 0.35951918363571167
MemoryTrain:  epoch  6, batch     5 | loss: 0.7661822Losses:  0.3242129683494568 0.24255934357643127
MemoryTrain:  epoch  6, batch     6 | loss: 0.5667723Losses:  0.3764687776565552 0.4335903823375702
MemoryTrain:  epoch  6, batch     7 | loss: 0.8100592Losses:  0.6339873671531677 0.4698132276535034
MemoryTrain:  epoch  6, batch     8 | loss: 1.1038005Losses:  0.4340161681175232 0.38595759868621826
MemoryTrain:  epoch  6, batch     9 | loss: 0.8199738Losses:  0.4426655173301697 0.3673584759235382
MemoryTrain:  epoch  7, batch     0 | loss: 0.8100240Losses:  0.3565884232521057 0.4410938024520874
MemoryTrain:  epoch  7, batch     1 | loss: 0.7976822Losses:  0.5387688875198364 0.5061333179473877
MemoryTrain:  epoch  7, batch     2 | loss: 1.0449022Losses:  0.3787047266960144 0.38852089643478394
MemoryTrain:  epoch  7, batch     3 | loss: 0.7672256Losses:  0.3885985314846039 0.3850860893726349
MemoryTrain:  epoch  7, batch     4 | loss: 0.7736846Losses:  0.4079777002334595 0.3193334937095642
MemoryTrain:  epoch  7, batch     5 | loss: 0.7273112Losses:  0.30450648069381714 0.29633164405822754
MemoryTrain:  epoch  7, batch     6 | loss: 0.6008381Losses:  0.30514097213745117 0.20452991127967834
MemoryTrain:  epoch  7, batch     7 | loss: 0.5096709Losses:  0.3813460171222687 0.24632325768470764
MemoryTrain:  epoch  7, batch     8 | loss: 0.6276693Losses:  0.4107707440853119 0.41140031814575195
MemoryTrain:  epoch  7, batch     9 | loss: 0.8221711Losses:  0.3633379638195038 0.2744261920452118
MemoryTrain:  epoch  8, batch     0 | loss: 0.6377642Losses:  0.41851186752319336 0.3314828872680664
MemoryTrain:  epoch  8, batch     1 | loss: 0.7499948Losses:  0.332974910736084 0.36854010820388794
MemoryTrain:  epoch  8, batch     2 | loss: 0.7015150Losses:  0.747063398361206 0.4795532822608948
MemoryTrain:  epoch  8, batch     3 | loss: 1.2266166Losses:  0.31628531217575073 0.27908504009246826
MemoryTrain:  epoch  8, batch     4 | loss: 0.5953704Losses:  0.37780138850212097 0.3650369644165039
MemoryTrain:  epoch  8, batch     5 | loss: 0.7428384Losses:  0.35650238394737244 0.3001794219017029
MemoryTrain:  epoch  8, batch     6 | loss: 0.6566818Losses:  0.5273957252502441 0.3588360548019409
MemoryTrain:  epoch  8, batch     7 | loss: 0.8862318Losses:  0.3800124526023865 0.2825314998626709
MemoryTrain:  epoch  8, batch     8 | loss: 0.6625440Losses:  0.4116670489311218 0.47275304794311523
MemoryTrain:  epoch  8, batch     9 | loss: 0.8844201Losses:  0.42690736055374146 0.596646785736084
MemoryTrain:  epoch  9, batch     0 | loss: 1.0235541Losses:  0.4141307473182678 0.2704010307788849
MemoryTrain:  epoch  9, batch     1 | loss: 0.6845318Losses:  0.40347909927368164 0.33732038736343384
MemoryTrain:  epoch  9, batch     2 | loss: 0.7407995Losses:  0.5619117617607117 0.3507016897201538
MemoryTrain:  epoch  9, batch     3 | loss: 0.9126135Losses:  0.29749438166618347 0.18326471745967865
MemoryTrain:  epoch  9, batch     4 | loss: 0.4807591Losses:  0.3752882480621338 0.2988505959510803
MemoryTrain:  epoch  9, batch     5 | loss: 0.6741388Losses:  0.3492756485939026 0.2536255121231079
MemoryTrain:  epoch  9, batch     6 | loss: 0.6029012Losses:  0.284480482339859 0.2106475830078125
MemoryTrain:  epoch  9, batch     7 | loss: 0.4951281Losses:  0.5334566831588745 0.4331013858318329
MemoryTrain:  epoch  9, batch     8 | loss: 0.9665581Losses:  0.3909069299697876 0.34319132566452026
MemoryTrain:  epoch  9, batch     9 | loss: 0.7340983
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 65.26%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 63.72%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 81.60%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 80.91%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 78.81%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 78.18%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.02%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.58%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 75.96%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 75.19%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 74.44%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 73.81%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 73.46%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 73.42%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 73.00%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 72.86%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 72.64%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 72.20%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 71.44%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 70.99%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 70.81%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 70.41%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 69.56%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 68.97%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 72.59%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 72.51%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.11%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 71.50%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 71.02%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 70.15%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 70.36%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 69.36%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 68.85%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 68.40%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 68.00%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 67.57%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 67.05%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 66.89%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 67.85%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 67.54%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 67.00%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 66.58%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 65.65%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 65.34%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:  157 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  158 | acc: 18.75%,  total acc: 64.27%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 64.10%   [EVAL] batch:  160 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:  162 | acc: 31.25%,  total acc: 63.57%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 63.60%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 63.63%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 63.69%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 63.72%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 63.42%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 63.05%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 62.68%   [EVAL] batch:  172 | acc: 0.00%,  total acc: 62.32%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 62.00%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 61.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 62.53%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 63.05%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 63.08%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 63.14%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 63.21%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 63.23%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 62.80%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 62.27%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 62.05%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 61.92%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 62.63%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 62.84%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 62.56%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 62.10%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 61.89%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 61.71%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 62.32%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 62.79%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 64.37%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 64.27%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.02%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 64.95%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 64.68%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 64.61%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 64.50%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 64.99%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 64.99%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 64.93%   [EVAL] batch:  262 | acc: 43.75%,  total acc: 64.85%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 64.68%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 64.50%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 64.28%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 64.05%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 63.83%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 63.60%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 63.18%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 64.09%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 64.11%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 64.14%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 64.13%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 64.15%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 66.10%   [EVAL] batch:  314 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 65.48%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 65.27%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 65.14%   [EVAL] batch:  319 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 65.39%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 65.29%   [EVAL] batch:  332 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 64.76%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 64.63%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 64.44%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 64.25%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 63.89%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 63.74%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 63.61%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 63.80%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  356 | acc: 31.25%,  total acc: 64.43%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 64.28%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 64.12%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 63.96%   [EVAL] batch:  360 | acc: 0.00%,  total acc: 63.78%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 63.60%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 63.57%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 63.65%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 63.65%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 63.63%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 63.66%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 63.59%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 63.60%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 64.00%   [EVAL] batch:  388 | acc: 0.00%,  total acc: 63.83%   [EVAL] batch:  389 | acc: 6.25%,  total acc: 63.69%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 63.54%   [EVAL] batch:  391 | acc: 0.00%,  total acc: 63.38%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 63.23%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 63.13%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 63.53%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 63.37%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 63.23%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 63.10%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 62.96%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.81%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 62.99%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 63.30%   [EVAL] batch:  413 | acc: 43.75%,  total acc: 63.25%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 63.22%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 63.17%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 63.10%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 63.08%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 63.02%   [EVAL] batch:  422 | acc: 43.75%,  total acc: 62.97%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 62.88%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 62.88%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 62.91%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 62.94%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:  437 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:  441 | acc: 68.75%,  total acc: 63.50%   [EVAL] batch:  442 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  448 | acc: 31.25%,  total acc: 63.49%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  451 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 63.53%   [EVAL] batch:  453 | acc: 43.75%,  total acc: 63.49%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  455 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  460 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:  462 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  463 | acc: 56.25%,  total acc: 63.86%   [EVAL] batch:  464 | acc: 37.50%,  total acc: 63.80%   [EVAL] batch:  465 | acc: 50.00%,  total acc: 63.77%   [EVAL] batch:  466 | acc: 37.50%,  total acc: 63.72%   [EVAL] batch:  467 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:  468 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 63.66%   [EVAL] batch:  470 | acc: 43.75%,  total acc: 63.61%   [EVAL] batch:  471 | acc: 43.75%,  total acc: 63.57%   [EVAL] batch:  472 | acc: 37.50%,  total acc: 63.52%   [EVAL] batch:  473 | acc: 43.75%,  total acc: 63.48%   [EVAL] batch:  474 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 63.62%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 63.68%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 63.67%   [EVAL] batch:  482 | acc: 75.00%,  total acc: 63.69%   [EVAL] batch:  483 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 63.65%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 63.66%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 63.68%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 64.30%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 64.48%   
cur_acc:  ['0.9474', '0.7163', '0.6786', '0.6964', '0.8333', '0.6121', '0.6042', '0.7093']
his_acc:  ['0.9474', '0.8325', '0.7593', '0.7155', '0.7388', '0.6982', '0.6642', '0.6448']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.784954071044922 1.3374130725860596
CurrentTrain: epoch  0, batch     0 | loss: 13.1223669Losses:  12.657736778259277 1.2248526811599731
CurrentTrain: epoch  0, batch     1 | loss: 13.8825893Losses:  13.510543823242188 1.4416372776031494
CurrentTrain: epoch  0, batch     2 | loss: 14.9521809Losses:  13.211421966552734 1.467043399810791
CurrentTrain: epoch  0, batch     3 | loss: 14.6784649Losses:  13.90137004852295 1.4098849296569824
CurrentTrain: epoch  0, batch     4 | loss: 15.3112545Losses:  14.094131469726562 0.9253373146057129
CurrentTrain: epoch  0, batch     5 | loss: 15.0194683Losses:  12.990470886230469 1.5742311477661133
CurrentTrain: epoch  0, batch     6 | loss: 14.5647020Losses:  13.372716903686523 1.4849154949188232
CurrentTrain: epoch  0, batch     7 | loss: 14.8576326Losses:  12.500490188598633 1.5883090496063232
CurrentTrain: epoch  0, batch     8 | loss: 14.0887995Losses:  12.99494743347168 1.4529023170471191
CurrentTrain: epoch  0, batch     9 | loss: 14.4478493Losses:  12.728272438049316 1.4571113586425781
CurrentTrain: epoch  0, batch    10 | loss: 14.1853838Losses:  12.760146141052246 1.1481825113296509
CurrentTrain: epoch  0, batch    11 | loss: 13.9083290Losses:  11.624954223632812 0.9116918444633484
CurrentTrain: epoch  0, batch    12 | loss: 12.5366459Losses:  12.279156684875488 1.7540743350982666
CurrentTrain: epoch  0, batch    13 | loss: 14.0332308Losses:  11.63524055480957 1.4493980407714844
CurrentTrain: epoch  0, batch    14 | loss: 13.0846386Losses:  11.829474449157715 1.5828471183776855
CurrentTrain: epoch  0, batch    15 | loss: 13.4123211Losses:  11.890324592590332 1.641214370727539
CurrentTrain: epoch  0, batch    16 | loss: 13.5315390Losses:  11.933540344238281 1.5345386266708374
CurrentTrain: epoch  0, batch    17 | loss: 13.4680786Losses:  12.41659927368164 1.7145652770996094
CurrentTrain: epoch  0, batch    18 | loss: 14.1311646Losses:  12.153820037841797 1.3245818614959717
CurrentTrain: epoch  0, batch    19 | loss: 13.4784021Losses:  12.256429672241211 1.3442914485931396
CurrentTrain: epoch  0, batch    20 | loss: 13.6007214Losses:  12.161450386047363 1.048001766204834
CurrentTrain: epoch  0, batch    21 | loss: 13.2094517Losses:  11.513980865478516 1.4943708181381226
CurrentTrain: epoch  0, batch    22 | loss: 13.0083513Losses:  12.032551765441895 1.5291796922683716
CurrentTrain: epoch  0, batch    23 | loss: 13.5617313Losses:  11.784954071044922 1.4595696926116943
CurrentTrain: epoch  0, batch    24 | loss: 13.2445240Losses:  11.470114707946777 1.1140040159225464
CurrentTrain: epoch  0, batch    25 | loss: 12.5841188Losses:  12.021026611328125 1.199265956878662
CurrentTrain: epoch  0, batch    26 | loss: 13.2202930Losses:  11.28860855102539 1.0828481912612915
CurrentTrain: epoch  0, batch    27 | loss: 12.3714571Losses:  11.263028144836426 1.0338326692581177
CurrentTrain: epoch  0, batch    28 | loss: 12.2968607Losses:  11.306924819946289 1.0856651067733765
CurrentTrain: epoch  0, batch    29 | loss: 12.3925896Losses:  10.813623428344727 0.8702647686004639
CurrentTrain: epoch  0, batch    30 | loss: 11.6838884Losses:  11.168445587158203 1.0397405624389648
CurrentTrain: epoch  0, batch    31 | loss: 12.2081861Losses:  11.363031387329102 1.155052661895752
CurrentTrain: epoch  0, batch    32 | loss: 12.5180836Losses:  12.295877456665039 0.9385143518447876
CurrentTrain: epoch  0, batch    33 | loss: 13.2343922Losses:  11.647525787353516 1.0449552536010742
CurrentTrain: epoch  0, batch    34 | loss: 12.6924810Losses:  11.598967552185059 1.138031244277954
CurrentTrain: epoch  0, batch    35 | loss: 12.7369986Losses:  11.555870056152344 0.9693750739097595
CurrentTrain: epoch  0, batch    36 | loss: 12.5252447Losses:  11.749658584594727 1.0655217170715332
CurrentTrain: epoch  0, batch    37 | loss: 12.8151798Losses:  11.384511947631836 0.915335476398468
CurrentTrain: epoch  0, batch    38 | loss: 12.2998476Losses:  10.475618362426758 0.3519093096256256
CurrentTrain: epoch  0, batch    39 | loss: 10.8275280Losses:  11.585029602050781 1.3632373809814453
CurrentTrain: epoch  0, batch    40 | loss: 12.9482670Losses:  10.72245979309082 1.108898401260376
CurrentTrain: epoch  0, batch    41 | loss: 11.8313580Losses:  10.751932144165039 1.0169627666473389
CurrentTrain: epoch  0, batch    42 | loss: 11.7688951Losses:  10.29017448425293 0.9650744199752808
CurrentTrain: epoch  0, batch    43 | loss: 11.2552490Losses:  10.370901107788086 1.0644057989120483
CurrentTrain: epoch  0, batch    44 | loss: 11.4353065Losses:  10.486909866333008 0.9821559190750122
CurrentTrain: epoch  0, batch    45 | loss: 11.4690657Losses:  9.797025680541992 1.0258994102478027
CurrentTrain: epoch  0, batch    46 | loss: 10.8229256Losses:  10.300752639770508 0.7807846069335938
CurrentTrain: epoch  0, batch    47 | loss: 11.0815372Losses:  10.709157943725586 1.0734179019927979
CurrentTrain: epoch  0, batch    48 | loss: 11.7825756Losses:  10.460766792297363 1.1961253881454468
CurrentTrain: epoch  0, batch    49 | loss: 11.6568918Losses:  10.382357597351074 0.6057935357093811
CurrentTrain: epoch  0, batch    50 | loss: 10.9881516Losses:  9.499317169189453 0.7632710933685303
CurrentTrain: epoch  0, batch    51 | loss: 10.2625885Losses:  10.20795726776123 0.9049131870269775
CurrentTrain: epoch  0, batch    52 | loss: 11.1128702Losses:  10.05901050567627 1.1035970449447632
CurrentTrain: epoch  0, batch    53 | loss: 11.1626072Losses:  10.094991683959961 0.7559778690338135
CurrentTrain: epoch  0, batch    54 | loss: 10.8509693Losses:  10.417123794555664 1.1454048156738281
CurrentTrain: epoch  0, batch    55 | loss: 11.5625286Losses:  10.129171371459961 0.968683660030365
CurrentTrain: epoch  0, batch    56 | loss: 11.0978546Losses:  9.55352783203125 0.9036673903465271
CurrentTrain: epoch  0, batch    57 | loss: 10.4571953Losses:  9.454148292541504 0.7618263959884644
CurrentTrain: epoch  0, batch    58 | loss: 10.2159748Losses:  9.355223655700684 0.9295206069946289
CurrentTrain: epoch  0, batch    59 | loss: 10.2847443Losses:  9.82180118560791 1.209833025932312
CurrentTrain: epoch  0, batch    60 | loss: 11.0316343Losses:  9.950311660766602 1.0327904224395752
CurrentTrain: epoch  0, batch    61 | loss: 10.9831018Losses:  9.449711799621582 0.8812710642814636
CurrentTrain: epoch  0, batch    62 | loss: 10.3309832Losses:  9.026277542114258 0.5303230881690979
CurrentTrain: epoch  0, batch    63 | loss: 9.5566006Losses:  9.613447189331055 0.7289273738861084
CurrentTrain: epoch  0, batch    64 | loss: 10.3423748Losses:  9.168793678283691 0.8895601034164429
CurrentTrain: epoch  0, batch    65 | loss: 10.0583534Losses:  9.80299186706543 0.5325139164924622
CurrentTrain: epoch  0, batch    66 | loss: 10.3355055Losses:  9.017727851867676 0.7997419238090515
CurrentTrain: epoch  0, batch    67 | loss: 9.8174696Losses:  9.190441131591797 0.8109195232391357
CurrentTrain: epoch  0, batch    68 | loss: 10.0013609Losses:  9.15576457977295 0.7643452882766724
CurrentTrain: epoch  0, batch    69 | loss: 9.9201097Losses:  9.427753448486328 0.7102323174476624
CurrentTrain: epoch  0, batch    70 | loss: 10.1379862Losses:  8.844087600708008 0.865251898765564
CurrentTrain: epoch  0, batch    71 | loss: 9.7093391Losses:  8.299556732177734 0.7878430485725403
CurrentTrain: epoch  0, batch    72 | loss: 9.0873995Losses:  8.88056755065918 0.8863985538482666
CurrentTrain: epoch  0, batch    73 | loss: 9.7669659Losses:  8.994444847106934 0.6792822480201721
CurrentTrain: epoch  0, batch    74 | loss: 9.6737270Losses:  8.336634635925293 0.8318307995796204
CurrentTrain: epoch  0, batch    75 | loss: 9.1684656Losses:  8.503473281860352 0.687134325504303
CurrentTrain: epoch  0, batch    76 | loss: 9.1906080Losses:  8.76950454711914 0.8752421736717224
CurrentTrain: epoch  0, batch    77 | loss: 9.6447468Losses:  8.497278213500977 0.7529855966567993
CurrentTrain: epoch  0, batch    78 | loss: 9.2502642Losses:  8.840749740600586 0.8197264671325684
CurrentTrain: epoch  0, batch    79 | loss: 9.6604767Losses:  8.670520782470703 1.0296627283096313
CurrentTrain: epoch  0, batch    80 | loss: 9.7001839Losses:  8.334444046020508 0.7780798673629761
CurrentTrain: epoch  0, batch    81 | loss: 9.1125240Losses:  8.262786865234375 0.7278056144714355
CurrentTrain: epoch  0, batch    82 | loss: 8.9905930Losses:  7.9154052734375 0.6866121292114258
CurrentTrain: epoch  0, batch    83 | loss: 8.6020174Losses:  8.251380920410156 0.8229646682739258
CurrentTrain: epoch  0, batch    84 | loss: 9.0743456Losses:  7.700959205627441 0.4475957155227661
CurrentTrain: epoch  0, batch    85 | loss: 8.1485548Losses:  8.23550033569336 0.6484801769256592
CurrentTrain: epoch  0, batch    86 | loss: 8.8839808Losses:  7.661129474639893 0.607196033000946
CurrentTrain: epoch  0, batch    87 | loss: 8.2683258Losses:  8.206147193908691 0.7936373353004456
CurrentTrain: epoch  0, batch    88 | loss: 8.9997845Losses:  7.972176551818848 0.7726646065711975
CurrentTrain: epoch  0, batch    89 | loss: 8.7448416Losses:  7.774626731872559 0.42865845561027527
CurrentTrain: epoch  0, batch    90 | loss: 8.2032852Losses:  7.608644008636475 0.6003404855728149
CurrentTrain: epoch  0, batch    91 | loss: 8.2089844Losses:  8.225301742553711 0.4331728219985962
CurrentTrain: epoch  0, batch    92 | loss: 8.6584749Losses:  7.688152313232422 0.6160732507705688
CurrentTrain: epoch  0, batch    93 | loss: 8.3042259Losses:  7.347585678100586 0.6546303033828735
CurrentTrain: epoch  0, batch    94 | loss: 8.0022163Losses:  7.36942720413208 0.6508827209472656
CurrentTrain: epoch  0, batch    95 | loss: 8.0203094Losses:  6.716263771057129 0.5173781514167786
CurrentTrain: epoch  0, batch    96 | loss: 7.2336421Losses:  7.241499900817871 0.6488380432128906
CurrentTrain: epoch  0, batch    97 | loss: 7.8903379Losses:  7.488883972167969 0.6498328447341919
CurrentTrain: epoch  0, batch    98 | loss: 8.1387167Losses:  7.233016014099121 0.5506076812744141
CurrentTrain: epoch  0, batch    99 | loss: 7.7836237Losses:  7.008081912994385 0.5259710550308228
CurrentTrain: epoch  0, batch   100 | loss: 7.5340528Losses:  7.6330132484436035 0.5930218696594238
CurrentTrain: epoch  0, batch   101 | loss: 8.2260351Losses:  6.727644920349121 0.6291936039924622
CurrentTrain: epoch  0, batch   102 | loss: 7.3568387Losses:  6.4596405029296875 0.545862078666687
CurrentTrain: epoch  0, batch   103 | loss: 7.0055027Losses:  6.511605262756348 0.6593237519264221
CurrentTrain: epoch  0, batch   104 | loss: 7.1709290Losses:  6.768434524536133 0.5801339149475098
CurrentTrain: epoch  0, batch   105 | loss: 7.3485684Losses:  6.554803371429443 0.43840375542640686
CurrentTrain: epoch  0, batch   106 | loss: 6.9932070Losses:  6.61539363861084 0.525465190410614
CurrentTrain: epoch  0, batch   107 | loss: 7.1408587Losses:  6.39752197265625 0.381001353263855
CurrentTrain: epoch  0, batch   108 | loss: 6.7785234Losses:  6.527820587158203 0.6908069849014282
CurrentTrain: epoch  0, batch   109 | loss: 7.2186275Losses:  6.093175888061523 0.6535652875900269
CurrentTrain: epoch  0, batch   110 | loss: 6.7467413Losses:  6.618993759155273 0.7049567699432373
CurrentTrain: epoch  0, batch   111 | loss: 7.3239508Losses:  6.282827377319336 0.6070913076400757
CurrentTrain: epoch  0, batch   112 | loss: 6.8899188Losses:  6.859492301940918 0.7298727035522461
CurrentTrain: epoch  0, batch   113 | loss: 7.5893650Losses:  5.884281635284424 0.46070727705955505
CurrentTrain: epoch  0, batch   114 | loss: 6.3449888Losses:  6.126025199890137 0.41064831614494324
CurrentTrain: epoch  0, batch   115 | loss: 6.5366735Losses:  6.007851600646973 0.5570549964904785
CurrentTrain: epoch  0, batch   116 | loss: 6.5649066Losses:  6.170820236206055 0.3787800669670105
CurrentTrain: epoch  0, batch   117 | loss: 6.5496001Losses:  5.975470542907715 0.6073955297470093
CurrentTrain: epoch  0, batch   118 | loss: 6.5828662Losses:  5.7890167236328125 0.5895140767097473
CurrentTrain: epoch  0, batch   119 | loss: 6.3785310Losses:  5.546521186828613 0.29390984773635864
CurrentTrain: epoch  0, batch   120 | loss: 5.8404312Losses:  6.180044174194336 0.5995611548423767
CurrentTrain: epoch  0, batch   121 | loss: 6.7796054Losses:  5.4566497802734375 0.38966524600982666
CurrentTrain: epoch  0, batch   122 | loss: 5.8463149Losses:  6.297368049621582 0.4552057683467865
CurrentTrain: epoch  0, batch   123 | loss: 6.7525740Losses:  5.604106426239014 0.4720805883407593
CurrentTrain: epoch  0, batch   124 | loss: 6.0761871Losses:  5.139955043792725 0.35781294107437134
CurrentTrain: epoch  1, batch     0 | loss: 5.4977679Losses:  5.338475227355957 0.5115952491760254
CurrentTrain: epoch  1, batch     1 | loss: 5.8500705Losses:  5.235675811767578 0.43882936239242554
CurrentTrain: epoch  1, batch     2 | loss: 5.6745052Losses:  5.691963195800781 0.4669751226902008
CurrentTrain: epoch  1, batch     3 | loss: 6.1589384Losses:  5.168061256408691 0.5183265209197998
CurrentTrain: epoch  1, batch     4 | loss: 5.6863880Losses:  5.115042686462402 0.43726980686187744
CurrentTrain: epoch  1, batch     5 | loss: 5.5523124Losses:  6.129580020904541 0.4037891626358032
CurrentTrain: epoch  1, batch     6 | loss: 6.5333691Losses:  5.22039794921875 0.4858352541923523
CurrentTrain: epoch  1, batch     7 | loss: 5.7062330Losses:  5.099042892456055 0.42114725708961487
CurrentTrain: epoch  1, batch     8 | loss: 5.5201902Losses:  5.402923583984375 0.3718690276145935
CurrentTrain: epoch  1, batch     9 | loss: 5.7747927Losses:  5.522524833679199 0.3705899715423584
CurrentTrain: epoch  1, batch    10 | loss: 5.8931150Losses:  5.472874641418457 0.41111189126968384
CurrentTrain: epoch  1, batch    11 | loss: 5.8839865Losses:  5.695431232452393 0.40045756101608276
CurrentTrain: epoch  1, batch    12 | loss: 6.0958886Losses:  5.756146430969238 0.6804249882698059
CurrentTrain: epoch  1, batch    13 | loss: 6.4365716Losses:  5.2866082191467285 0.38594353199005127
CurrentTrain: epoch  1, batch    14 | loss: 5.6725516Losses:  5.413227558135986 0.38547641038894653
CurrentTrain: epoch  1, batch    15 | loss: 5.7987041Losses:  5.836840629577637 0.5426137447357178
CurrentTrain: epoch  1, batch    16 | loss: 6.3794546Losses:  5.092288970947266 0.45516788959503174
CurrentTrain: epoch  1, batch    17 | loss: 5.5474567Losses:  5.46214485168457 0.5466445088386536
CurrentTrain: epoch  1, batch    18 | loss: 6.0087895Losses:  5.360401153564453 0.35407885909080505
CurrentTrain: epoch  1, batch    19 | loss: 5.7144799Losses:  5.077845573425293 0.32586491107940674
CurrentTrain: epoch  1, batch    20 | loss: 5.4037104Losses:  5.493639945983887 0.4724199175834656
CurrentTrain: epoch  1, batch    21 | loss: 5.9660597Losses:  5.612461566925049 0.42381513118743896
CurrentTrain: epoch  1, batch    22 | loss: 6.0362768Losses:  4.951587677001953 0.41288846731185913
CurrentTrain: epoch  1, batch    23 | loss: 5.3644762Losses:  5.616564750671387 0.4696499705314636
CurrentTrain: epoch  1, batch    24 | loss: 6.0862145Losses:  5.713267803192139 0.5861021876335144
CurrentTrain: epoch  1, batch    25 | loss: 6.2993698Losses:  5.460799694061279 0.42356908321380615
CurrentTrain: epoch  1, batch    26 | loss: 5.8843689Losses:  5.584013938903809 0.5600385665893555
CurrentTrain: epoch  1, batch    27 | loss: 6.1440525Losses:  5.066275596618652 0.2900051474571228
CurrentTrain: epoch  1, batch    28 | loss: 5.3562808Losses:  5.071259498596191 0.24103927612304688
CurrentTrain: epoch  1, batch    29 | loss: 5.3122988Losses:  5.343183994293213 0.4367424249649048
CurrentTrain: epoch  1, batch    30 | loss: 5.7799263Losses:  5.157317161560059 0.41004905104637146
CurrentTrain: epoch  1, batch    31 | loss: 5.5673661Losses:  5.285356521606445 0.4578322768211365
CurrentTrain: epoch  1, batch    32 | loss: 5.7431889Losses:  4.808038711547852 0.1542198807001114
CurrentTrain: epoch  1, batch    33 | loss: 4.9622588Losses:  4.972980976104736 0.29565227031707764
CurrentTrain: epoch  1, batch    34 | loss: 5.2686334Losses:  4.677432060241699 0.2489844560623169
CurrentTrain: epoch  1, batch    35 | loss: 4.9264164Losses:  4.911288261413574 0.2812997102737427
CurrentTrain: epoch  1, batch    36 | loss: 5.1925879Losses:  5.193663597106934 0.41970664262771606
CurrentTrain: epoch  1, batch    37 | loss: 5.6133704Losses:  5.186424255371094 0.444706529378891
CurrentTrain: epoch  1, batch    38 | loss: 5.6311307Losses:  5.061663627624512 0.4535500705242157
CurrentTrain: epoch  1, batch    39 | loss: 5.5152135Losses:  5.331474304199219 0.39817267656326294
CurrentTrain: epoch  1, batch    40 | loss: 5.7296472Losses:  4.898068428039551 0.3053385019302368
CurrentTrain: epoch  1, batch    41 | loss: 5.2034068Losses:  5.268495559692383 0.4195193350315094
CurrentTrain: epoch  1, batch    42 | loss: 5.6880150Losses:  4.831113815307617 0.36539894342422485
CurrentTrain: epoch  1, batch    43 | loss: 5.1965127Losses:  5.608967304229736 0.7328779697418213
CurrentTrain: epoch  1, batch    44 | loss: 6.3418455Losses:  4.693698883056641 0.3230065405368805
CurrentTrain: epoch  1, batch    45 | loss: 5.0167055Losses:  4.817295551300049 0.32178789377212524
CurrentTrain: epoch  1, batch    46 | loss: 5.1390834Losses:  5.022952079772949 0.2172495424747467
CurrentTrain: epoch  1, batch    47 | loss: 5.2402015Losses:  5.178973197937012 0.2837424874305725
CurrentTrain: epoch  1, batch    48 | loss: 5.4627156Losses:  5.028897762298584 0.40825819969177246
CurrentTrain: epoch  1, batch    49 | loss: 5.4371557Losses:  4.983504295349121 0.28195250034332275
CurrentTrain: epoch  1, batch    50 | loss: 5.2654567Losses:  5.469987869262695 0.3911065459251404
CurrentTrain: epoch  1, batch    51 | loss: 5.8610945Losses:  5.130245208740234 0.34643054008483887
CurrentTrain: epoch  1, batch    52 | loss: 5.4766760Losses:  4.972014427185059 0.24680912494659424
CurrentTrain: epoch  1, batch    53 | loss: 5.2188234Losses:  5.413281440734863 0.5107417106628418
CurrentTrain: epoch  1, batch    54 | loss: 5.9240232Losses:  5.195068359375 0.581972062587738
CurrentTrain: epoch  1, batch    55 | loss: 5.7770405Losses:  5.220879554748535 0.3745732009410858
CurrentTrain: epoch  1, batch    56 | loss: 5.5954528Losses:  4.994988441467285 0.19478854537010193
CurrentTrain: epoch  1, batch    57 | loss: 5.1897769Losses:  5.131442070007324 0.3646848499774933
CurrentTrain: epoch  1, batch    58 | loss: 5.4961271Losses:  5.418502330780029 0.6773415207862854
CurrentTrain: epoch  1, batch    59 | loss: 6.0958438Losses:  4.908855438232422 0.4049079120159149
CurrentTrain: epoch  1, batch    60 | loss: 5.3137631Losses:  4.703452110290527 0.20686808228492737
CurrentTrain: epoch  1, batch    61 | loss: 4.9103203Losses:  5.172545433044434 0.3101004958152771
CurrentTrain: epoch  1, batch    62 | loss: 5.4826460Losses:  5.146791458129883 0.41898298263549805
CurrentTrain: epoch  1, batch    63 | loss: 5.5657744Losses:  4.611617088317871 0.17704540491104126
CurrentTrain: epoch  1, batch    64 | loss: 4.7886624Losses:  4.807318687438965 0.2915051579475403
CurrentTrain: epoch  1, batch    65 | loss: 5.0988240Losses:  4.728677749633789 0.18423444032669067
CurrentTrain: epoch  1, batch    66 | loss: 4.9129124Losses:  5.328648567199707 0.41848212480545044
CurrentTrain: epoch  1, batch    67 | loss: 5.7471309Losses:  5.088095188140869 0.301885724067688
CurrentTrain: epoch  1, batch    68 | loss: 5.3899808Losses:  4.78757381439209 0.2073705941438675
CurrentTrain: epoch  1, batch    69 | loss: 4.9949446Losses:  5.258044719696045 0.4209755063056946
CurrentTrain: epoch  1, batch    70 | loss: 5.6790204Losses:  5.008968353271484 0.1932048350572586
CurrentTrain: epoch  1, batch    71 | loss: 5.2021732Losses:  4.594772815704346 0.2961675822734833
CurrentTrain: epoch  1, batch    72 | loss: 4.8909402Losses:  4.949089050292969 0.25632256269454956
CurrentTrain: epoch  1, batch    73 | loss: 5.2054114Losses:  4.871872901916504 0.3039862811565399
CurrentTrain: epoch  1, batch    74 | loss: 5.1758590Losses:  4.624856472015381 0.16187222301959991
CurrentTrain: epoch  1, batch    75 | loss: 4.7867289Losses:  5.07635498046875 0.2819663882255554
CurrentTrain: epoch  1, batch    76 | loss: 5.3583212Losses:  5.42270565032959 0.34910863637924194
CurrentTrain: epoch  1, batch    77 | loss: 5.7718143Losses:  5.137596130371094 0.45768898725509644
CurrentTrain: epoch  1, batch    78 | loss: 5.5952849Losses:  4.9551496505737305 0.44234520196914673
CurrentTrain: epoch  1, batch    79 | loss: 5.3974948Losses:  4.48674201965332 0.24319525063037872
CurrentTrain: epoch  1, batch    80 | loss: 4.7299371Losses:  5.053415775299072 0.3886364698410034
CurrentTrain: epoch  1, batch    81 | loss: 5.4420524Losses:  4.758727073669434 0.3680989146232605
CurrentTrain: epoch  1, batch    82 | loss: 5.1268258Losses:  4.874250888824463 0.3210930824279785
CurrentTrain: epoch  1, batch    83 | loss: 5.1953440Losses:  4.650219917297363 0.19252166152000427
CurrentTrain: epoch  1, batch    84 | loss: 4.8427415Losses:  4.966217041015625 0.26636767387390137
CurrentTrain: epoch  1, batch    85 | loss: 5.2325850Losses:  4.670750141143799 0.2963097095489502
CurrentTrain: epoch  1, batch    86 | loss: 4.9670601Losses:  4.68579626083374 0.1597757190465927
CurrentTrain: epoch  1, batch    87 | loss: 4.8455720Losses:  5.247097492218018 0.31493037939071655
CurrentTrain: epoch  1, batch    88 | loss: 5.5620279Losses:  5.096609115600586 0.5053310990333557
CurrentTrain: epoch  1, batch    89 | loss: 5.6019402Losses:  5.39725923538208 0.5002626180648804
CurrentTrain: epoch  1, batch    90 | loss: 5.8975220Losses:  5.319740295410156 0.4534454345703125
CurrentTrain: epoch  1, batch    91 | loss: 5.7731857Losses:  5.402756214141846 0.4157083034515381
CurrentTrain: epoch  1, batch    92 | loss: 5.8184643Losses:  4.633403778076172 0.22562357783317566
CurrentTrain: epoch  1, batch    93 | loss: 4.8590274Losses:  4.678138256072998 0.4058387279510498
CurrentTrain: epoch  1, batch    94 | loss: 5.0839767Losses:  5.174181938171387 0.4614076316356659
CurrentTrain: epoch  1, batch    95 | loss: 5.6355896Losses:  5.355536937713623 0.5131784677505493
CurrentTrain: epoch  1, batch    96 | loss: 5.8687153Losses:  4.5642499923706055 0.20351794362068176
CurrentTrain: epoch  1, batch    97 | loss: 4.7677679Losses:  5.268174171447754 0.5177093148231506
CurrentTrain: epoch  1, batch    98 | loss: 5.7858834Losses:  4.5160627365112305 0.3900865316390991
CurrentTrain: epoch  1, batch    99 | loss: 4.9061494Losses:  4.940839767456055 0.18848389387130737
CurrentTrain: epoch  1, batch   100 | loss: 5.1293235Losses:  4.871980667114258 0.3595404028892517
CurrentTrain: epoch  1, batch   101 | loss: 5.2315211Losses:  5.0546112060546875 0.5196679830551147
CurrentTrain: epoch  1, batch   102 | loss: 5.5742793Losses:  4.676776885986328 0.19735002517700195
CurrentTrain: epoch  1, batch   103 | loss: 4.8741269Losses:  4.676004409790039 0.32384660840034485
CurrentTrain: epoch  1, batch   104 | loss: 4.9998512Losses:  4.752073287963867 0.2526174783706665
CurrentTrain: epoch  1, batch   105 | loss: 5.0046906Losses:  4.767758369445801 0.4260520040988922
CurrentTrain: epoch  1, batch   106 | loss: 5.1938105Losses:  5.309406280517578 0.2746817469596863
CurrentTrain: epoch  1, batch   107 | loss: 5.5840878Losses:  4.607794284820557 0.23159989714622498
CurrentTrain: epoch  1, batch   108 | loss: 4.8393941Losses:  4.554721832275391 0.10905066877603531
CurrentTrain: epoch  1, batch   109 | loss: 4.6637726Losses:  4.655897617340088 0.19300079345703125
CurrentTrain: epoch  1, batch   110 | loss: 4.8488984Losses:  4.948088645935059 0.26462674140930176
CurrentTrain: epoch  1, batch   111 | loss: 5.2127151Losses:  5.016073226928711 0.2602735757827759
CurrentTrain: epoch  1, batch   112 | loss: 5.2763467Losses:  4.900611400604248 0.3758593201637268
CurrentTrain: epoch  1, batch   113 | loss: 5.2764707Losses:  4.894580841064453 0.3257414698600769
CurrentTrain: epoch  1, batch   114 | loss: 5.2203221Losses:  4.94842004776001 0.37965846061706543
CurrentTrain: epoch  1, batch   115 | loss: 5.3280783Losses:  4.812955856323242 0.3621305823326111
CurrentTrain: epoch  1, batch   116 | loss: 5.1750865Losses:  4.647500038146973 0.2924641966819763
CurrentTrain: epoch  1, batch   117 | loss: 4.9399643Losses:  4.569493293762207 0.2953619062900543
CurrentTrain: epoch  1, batch   118 | loss: 4.8648553Losses:  4.493989944458008 0.33513858914375305
CurrentTrain: epoch  1, batch   119 | loss: 4.8291287Losses:  4.570888996124268 0.21172542870044708
CurrentTrain: epoch  1, batch   120 | loss: 4.7826142Losses:  4.6737871170043945 0.32457756996154785
CurrentTrain: epoch  1, batch   121 | loss: 4.9983644Losses:  4.412252902984619 0.1828906387090683
CurrentTrain: epoch  1, batch   122 | loss: 4.5951433Losses:  4.466069221496582 0.24320413172245026
CurrentTrain: epoch  1, batch   123 | loss: 4.7092733Losses:  5.851558685302734 0.5126122236251831
CurrentTrain: epoch  1, batch   124 | loss: 6.3641710Losses:  4.281130790710449 0.24247726798057556
CurrentTrain: epoch  2, batch     0 | loss: 4.5236082Losses:  4.301563739776611 0.15508997440338135
CurrentTrain: epoch  2, batch     1 | loss: 4.4566536Losses:  4.465177536010742 0.27728042006492615
CurrentTrain: epoch  2, batch     2 | loss: 4.7424579Losses:  4.506773948669434 0.1909659206867218
CurrentTrain: epoch  2, batch     3 | loss: 4.6977401Losses:  4.370326519012451 0.26530754566192627
CurrentTrain: epoch  2, batch     4 | loss: 4.6356339Losses:  4.538724422454834 0.15380148589611053
CurrentTrain: epoch  2, batch     5 | loss: 4.6925259Losses:  4.332822322845459 0.1291675865650177
CurrentTrain: epoch  2, batch     6 | loss: 4.4619899Losses:  4.760797023773193 0.13264784216880798
CurrentTrain: epoch  2, batch     7 | loss: 4.8934450Losses:  4.658832550048828 0.2341517210006714
CurrentTrain: epoch  2, batch     8 | loss: 4.8929844Losses:  4.427488803863525 0.26686397194862366
CurrentTrain: epoch  2, batch     9 | loss: 4.6943526Losses:  4.623041152954102 0.2134501039981842
CurrentTrain: epoch  2, batch    10 | loss: 4.8364911Losses:  4.296951770782471 0.14472238719463348
CurrentTrain: epoch  2, batch    11 | loss: 4.4416742Losses:  4.394594192504883 0.25428739190101624
CurrentTrain: epoch  2, batch    12 | loss: 4.6488814Losses:  4.275203704833984 0.10049499571323395
CurrentTrain: epoch  2, batch    13 | loss: 4.3756986Losses:  4.241934776306152 0.24878957867622375
CurrentTrain: epoch  2, batch    14 | loss: 4.4907246Losses:  4.373498916625977 0.29201674461364746
CurrentTrain: epoch  2, batch    15 | loss: 4.6655159Losses:  4.220184326171875 0.10761434584856033
CurrentTrain: epoch  2, batch    16 | loss: 4.3277988Losses:  5.145317554473877 0.2864651083946228
CurrentTrain: epoch  2, batch    17 | loss: 5.4317827Losses:  4.238583564758301 0.1791701316833496
CurrentTrain: epoch  2, batch    18 | loss: 4.4177537Losses:  4.840050220489502 0.3172509968280792
CurrentTrain: epoch  2, batch    19 | loss: 5.1573014Losses:  4.165098190307617 0.18575727939605713
CurrentTrain: epoch  2, batch    20 | loss: 4.3508554Losses:  5.0922441482543945 0.4279974699020386
CurrentTrain: epoch  2, batch    21 | loss: 5.5202417Losses:  4.555852890014648 0.20727823674678802
CurrentTrain: epoch  2, batch    22 | loss: 4.7631311Losses:  4.419530868530273 0.09615351259708405
CurrentTrain: epoch  2, batch    23 | loss: 4.5156846Losses:  4.541816711425781 0.1745375245809555
CurrentTrain: epoch  2, batch    24 | loss: 4.7163544Losses:  4.208873748779297 0.23654410243034363
CurrentTrain: epoch  2, batch    25 | loss: 4.4454179Losses:  4.414247512817383 0.22795522212982178
CurrentTrain: epoch  2, batch    26 | loss: 4.6422029Losses:  4.687602996826172 0.1576417088508606
CurrentTrain: epoch  2, batch    27 | loss: 4.8452449Losses:  4.565360069274902 0.18692484498023987
CurrentTrain: epoch  2, batch    28 | loss: 4.7522850Losses:  4.783585071563721 0.19089043140411377
CurrentTrain: epoch  2, batch    29 | loss: 4.9744754Losses:  4.387269496917725 0.20139986276626587
CurrentTrain: epoch  2, batch    30 | loss: 4.5886693Losses:  4.316657066345215 0.12209925800561905
CurrentTrain: epoch  2, batch    31 | loss: 4.4387565Losses:  4.403845310211182 0.22921356558799744
CurrentTrain: epoch  2, batch    32 | loss: 4.6330590Losses:  4.51751708984375 0.21928830444812775
CurrentTrain: epoch  2, batch    33 | loss: 4.7368054Losses:  4.183026313781738 0.09659970551729202
CurrentTrain: epoch  2, batch    34 | loss: 4.2796259Losses:  4.400088310241699 0.18851837515830994
CurrentTrain: epoch  2, batch    35 | loss: 4.5886068Losses:  4.547183036804199 0.22963672876358032
CurrentTrain: epoch  2, batch    36 | loss: 4.7768197Losses:  4.443750381469727 0.25583550333976746
CurrentTrain: epoch  2, batch    37 | loss: 4.6995859Losses:  4.637556552886963 0.21238470077514648
CurrentTrain: epoch  2, batch    38 | loss: 4.8499413Losses:  4.370632171630859 0.12173376977443695
CurrentTrain: epoch  2, batch    39 | loss: 4.4923658Losses:  4.321922779083252 0.23933109641075134
CurrentTrain: epoch  2, batch    40 | loss: 4.5612540Losses:  5.611608505249023 0.4864620268344879
CurrentTrain: epoch  2, batch    41 | loss: 6.0980706Losses:  4.73655891418457 0.20008888840675354
CurrentTrain: epoch  2, batch    42 | loss: 4.9366479Losses:  4.948030948638916 0.3833119571208954
CurrentTrain: epoch  2, batch    43 | loss: 5.3313427Losses:  4.450599670410156 0.2648884057998657
CurrentTrain: epoch  2, batch    44 | loss: 4.7154880Losses:  4.951120376586914 0.5443772673606873
CurrentTrain: epoch  2, batch    45 | loss: 5.4954977Losses:  4.459637641906738 0.20588615536689758
CurrentTrain: epoch  2, batch    46 | loss: 4.6655240Losses:  4.423333644866943 0.142017662525177
CurrentTrain: epoch  2, batch    47 | loss: 4.5653515Losses:  4.336477279663086 0.1938517689704895
CurrentTrain: epoch  2, batch    48 | loss: 4.5303292Losses:  4.238633155822754 0.22349905967712402
CurrentTrain: epoch  2, batch    49 | loss: 4.4621325Losses:  4.56638765335083 0.29040467739105225
CurrentTrain: epoch  2, batch    50 | loss: 4.8567924Losses:  4.186356544494629 0.18765419721603394
CurrentTrain: epoch  2, batch    51 | loss: 4.3740106Losses:  4.2552032470703125 0.20851744711399078
CurrentTrain: epoch  2, batch    52 | loss: 4.4637208Losses:  4.4911298751831055 0.2254052460193634
CurrentTrain: epoch  2, batch    53 | loss: 4.7165351Losses:  4.219995021820068 0.19490933418273926
CurrentTrain: epoch  2, batch    54 | loss: 4.4149046Losses:  4.496207237243652 0.2516818046569824
CurrentTrain: epoch  2, batch    55 | loss: 4.7478890Losses:  4.342840194702148 0.1183277815580368
CurrentTrain: epoch  2, batch    56 | loss: 4.4611678Losses:  4.565127372741699 0.13477090001106262
CurrentTrain: epoch  2, batch    57 | loss: 4.6998982Losses:  4.194446086883545 0.1324973702430725
CurrentTrain: epoch  2, batch    58 | loss: 4.3269434Losses:  4.771091461181641 0.14252281188964844
CurrentTrain: epoch  2, batch    59 | loss: 4.9136143Losses:  4.4279022216796875 0.2688501477241516
CurrentTrain: epoch  2, batch    60 | loss: 4.6967525Losses:  4.20102596282959 0.24484744668006897
CurrentTrain: epoch  2, batch    61 | loss: 4.4458733Losses:  5.482382774353027 0.453124076128006
CurrentTrain: epoch  2, batch    62 | loss: 5.9355068Losses:  4.369326591491699 0.18275651335716248
CurrentTrain: epoch  2, batch    63 | loss: 4.5520830Losses:  4.346278190612793 0.10719183832406998
CurrentTrain: epoch  2, batch    64 | loss: 4.4534702Losses:  4.372087001800537 0.17370909452438354
CurrentTrain: epoch  2, batch    65 | loss: 4.5457959Losses:  4.382214069366455 0.1818741112947464
CurrentTrain: epoch  2, batch    66 | loss: 4.5640883Losses:  4.377347946166992 0.19812540709972382
CurrentTrain: epoch  2, batch    67 | loss: 4.5754733Losses:  4.8796916007995605 0.1950588822364807
CurrentTrain: epoch  2, batch    68 | loss: 5.0747504Losses:  5.219915390014648 0.3895919620990753
CurrentTrain: epoch  2, batch    69 | loss: 5.6095076Losses:  4.353318214416504 0.17418453097343445
CurrentTrain: epoch  2, batch    70 | loss: 4.5275025Losses:  4.3724894523620605 0.13566406071186066
CurrentTrain: epoch  2, batch    71 | loss: 4.5081534Losses:  4.385772705078125 0.13781490921974182
CurrentTrain: epoch  2, batch    72 | loss: 4.5235877Losses:  4.29941463470459 0.11090251058340073
CurrentTrain: epoch  2, batch    73 | loss: 4.4103169Losses:  4.2210469245910645 0.12914256751537323
CurrentTrain: epoch  2, batch    74 | loss: 4.3501897Losses:  4.429597854614258 0.17165252566337585
CurrentTrain: epoch  2, batch    75 | loss: 4.6012502Losses:  4.334553241729736 0.14046470820903778
CurrentTrain: epoch  2, batch    76 | loss: 4.4750180Losses:  4.432811737060547 0.10149534046649933
CurrentTrain: epoch  2, batch    77 | loss: 4.5343070Losses:  4.317222595214844 0.17042404413223267
CurrentTrain: epoch  2, batch    78 | loss: 4.4876466Losses:  4.567651748657227 0.24574357271194458
CurrentTrain: epoch  2, batch    79 | loss: 4.8133955Losses:  4.495654106140137 0.18582741916179657
CurrentTrain: epoch  2, batch    80 | loss: 4.6814814Losses:  4.305206298828125 0.1230483129620552
CurrentTrain: epoch  2, batch    81 | loss: 4.4282546Losses:  4.257360935211182 0.23268099129199982
CurrentTrain: epoch  2, batch    82 | loss: 4.4900417Losses:  4.020115852355957 0.13979913294315338
CurrentTrain: epoch  2, batch    83 | loss: 4.1599150Losses:  4.514537334442139 0.23880738019943237
CurrentTrain: epoch  2, batch    84 | loss: 4.7533445Losses:  4.799314975738525 0.0901385098695755
CurrentTrain: epoch  2, batch    85 | loss: 4.8894534Losses:  4.281059265136719 0.10999931395053864
CurrentTrain: epoch  2, batch    86 | loss: 4.3910584Losses:  4.352045059204102 0.21320101618766785
CurrentTrain: epoch  2, batch    87 | loss: 4.5652461Losses:  4.30836296081543 0.13428160548210144
CurrentTrain: epoch  2, batch    88 | loss: 4.4426446Losses:  4.169946670532227 0.13255277276039124
CurrentTrain: epoch  2, batch    89 | loss: 4.3024993Losses:  4.405932426452637 0.1257377564907074
CurrentTrain: epoch  2, batch    90 | loss: 4.5316701Losses:  4.264983177185059 0.18282555043697357
CurrentTrain: epoch  2, batch    91 | loss: 4.4478087Losses:  5.00424337387085 0.40736496448516846
CurrentTrain: epoch  2, batch    92 | loss: 5.4116082Losses:  4.294309616088867 0.18958359956741333
CurrentTrain: epoch  2, batch    93 | loss: 4.4838934Losses:  4.287464618682861 0.12477008253335953
CurrentTrain: epoch  2, batch    94 | loss: 4.4122348Losses:  4.329675674438477 0.12425622344017029
CurrentTrain: epoch  2, batch    95 | loss: 4.4539318Losses:  4.06309700012207 0.12700174748897552
CurrentTrain: epoch  2, batch    96 | loss: 4.1900988Losses:  4.1256184577941895 0.09052692353725433
CurrentTrain: epoch  2, batch    97 | loss: 4.2161455Losses:  4.543702125549316 0.11975978314876556
CurrentTrain: epoch  2, batch    98 | loss: 4.6634617Losses:  4.148806571960449 0.24433207511901855
CurrentTrain: epoch  2, batch    99 | loss: 4.3931389Losses:  4.21835470199585 0.10208785533905029
CurrentTrain: epoch  2, batch   100 | loss: 4.3204427Losses:  4.198946952819824 0.17189863324165344
CurrentTrain: epoch  2, batch   101 | loss: 4.3708458Losses:  4.194470405578613 0.07716283202171326
CurrentTrain: epoch  2, batch   102 | loss: 4.2716331Losses:  4.145627498626709 0.1833702027797699
CurrentTrain: epoch  2, batch   103 | loss: 4.3289976Losses:  4.339066505432129 0.15439879894256592
CurrentTrain: epoch  2, batch   104 | loss: 4.4934654Losses:  4.232181549072266 0.13740143179893494
CurrentTrain: epoch  2, batch   105 | loss: 4.3695831Losses:  4.515925884246826 0.19006773829460144
CurrentTrain: epoch  2, batch   106 | loss: 4.7059937Losses:  4.33115816116333 0.14695008099079132
CurrentTrain: epoch  2, batch   107 | loss: 4.4781084Losses:  4.223664283752441 0.13433966040611267
CurrentTrain: epoch  2, batch   108 | loss: 4.3580041Losses:  4.297447204589844 0.11264218389987946
CurrentTrain: epoch  2, batch   109 | loss: 4.4100895Losses:  4.2798004150390625 0.17999780178070068
CurrentTrain: epoch  2, batch   110 | loss: 4.4597983Losses:  4.124370574951172 0.11319075524806976
CurrentTrain: epoch  2, batch   111 | loss: 4.2375612Losses:  4.227632522583008 0.2004982829093933
CurrentTrain: epoch  2, batch   112 | loss: 4.4281306Losses:  4.0942487716674805 0.1297774612903595
CurrentTrain: epoch  2, batch   113 | loss: 4.2240262Losses:  4.300544738769531 0.158868670463562
CurrentTrain: epoch  2, batch   114 | loss: 4.4594135Losses:  4.228392601013184 0.188250333070755
CurrentTrain: epoch  2, batch   115 | loss: 4.4166431Losses:  4.356395244598389 0.12809285521507263
CurrentTrain: epoch  2, batch   116 | loss: 4.4844880Losses:  4.8173933029174805 0.4417125880718231
CurrentTrain: epoch  2, batch   117 | loss: 5.2591057Losses:  4.175660610198975 0.15088766813278198
CurrentTrain: epoch  2, batch   118 | loss: 4.3265481Losses:  4.167976379394531 0.10504832863807678
CurrentTrain: epoch  2, batch   119 | loss: 4.2730246Losses:  4.1905717849731445 0.1309492290019989
CurrentTrain: epoch  2, batch   120 | loss: 4.3215208Losses:  4.275820255279541 0.1916356384754181
CurrentTrain: epoch  2, batch   121 | loss: 4.4674559Losses:  4.20848274230957 0.12113726139068604
CurrentTrain: epoch  2, batch   122 | loss: 4.3296199Losses:  4.234313488006592 0.17677772045135498
CurrentTrain: epoch  2, batch   123 | loss: 4.4110913Losses:  4.221867084503174 0.15904054045677185
CurrentTrain: epoch  2, batch   124 | loss: 4.3809075Losses:  4.269406318664551 0.09698639810085297
CurrentTrain: epoch  3, batch     0 | loss: 4.3663926Losses:  4.078244209289551 0.11644613742828369
CurrentTrain: epoch  3, batch     1 | loss: 4.1946902Losses:  4.1723737716674805 0.14011552929878235
CurrentTrain: epoch  3, batch     2 | loss: 4.3124895Losses:  4.211255073547363 0.09413067996501923
CurrentTrain: epoch  3, batch     3 | loss: 4.3053856Losses:  5.366586685180664 0.4993922710418701
CurrentTrain: epoch  3, batch     4 | loss: 5.8659792Losses:  4.145198822021484 0.12113898992538452
CurrentTrain: epoch  3, batch     5 | loss: 4.2663379Losses:  4.369419097900391 0.15541169047355652
CurrentTrain: epoch  3, batch     6 | loss: 4.5248308Losses:  4.1555023193359375 0.20250007510185242
CurrentTrain: epoch  3, batch     7 | loss: 4.3580022Losses:  4.320572853088379 0.18916404247283936
CurrentTrain: epoch  3, batch     8 | loss: 4.5097370Losses:  4.4162421226501465 0.13709914684295654
CurrentTrain: epoch  3, batch     9 | loss: 4.5533414Losses:  4.084414958953857 0.08118720352649689
CurrentTrain: epoch  3, batch    10 | loss: 4.1656022Losses:  4.2553863525390625 0.13547232747077942
CurrentTrain: epoch  3, batch    11 | loss: 4.3908587Losses:  4.412992000579834 0.16381442546844482
CurrentTrain: epoch  3, batch    12 | loss: 4.5768065Losses:  4.1001129150390625 0.12122954428195953
CurrentTrain: epoch  3, batch    13 | loss: 4.2213426Losses:  4.440134048461914 0.09505227953195572
CurrentTrain: epoch  3, batch    14 | loss: 4.5351863Losses:  4.16430139541626 0.10137052834033966
CurrentTrain: epoch  3, batch    15 | loss: 4.2656717Losses:  4.276504039764404 0.12546367943286896
CurrentTrain: epoch  3, batch    16 | loss: 4.4019675Losses:  4.523918151855469 0.11906065791845322
CurrentTrain: epoch  3, batch    17 | loss: 4.6429787Losses:  4.325295448303223 0.14328482747077942
CurrentTrain: epoch  3, batch    18 | loss: 4.4685802Losses:  4.196978569030762 0.1093718409538269
CurrentTrain: epoch  3, batch    19 | loss: 4.3063502Losses:  4.175254821777344 0.14694520831108093
CurrentTrain: epoch  3, batch    20 | loss: 4.3221998Losses:  4.61459493637085 0.1807192862033844
CurrentTrain: epoch  3, batch    21 | loss: 4.7953143Losses:  4.180397033691406 0.05266113579273224
CurrentTrain: epoch  3, batch    22 | loss: 4.2330580Losses:  4.082620620727539 0.14344772696495056
CurrentTrain: epoch  3, batch    23 | loss: 4.2260685Losses:  4.269960403442383 0.07881215214729309
CurrentTrain: epoch  3, batch    24 | loss: 4.3487725Losses:  4.144707202911377 0.050134025514125824
CurrentTrain: epoch  3, batch    25 | loss: 4.1948414Losses:  4.0836944580078125 0.17433354258537292
CurrentTrain: epoch  3, batch    26 | loss: 4.2580280Losses:  4.235591888427734 0.13419213891029358
CurrentTrain: epoch  3, batch    27 | loss: 4.3697839Losses:  4.320623397827148 0.0880679339170456
CurrentTrain: epoch  3, batch    28 | loss: 4.4086914Losses:  4.098062992095947 0.08895312249660492
CurrentTrain: epoch  3, batch    29 | loss: 4.1870160Losses:  4.311103343963623 0.13030534982681274
CurrentTrain: epoch  3, batch    30 | loss: 4.4414086Losses:  4.103276252746582 0.16919934749603271
CurrentTrain: epoch  3, batch    31 | loss: 4.2724757Losses:  4.605962753295898 0.19206435978412628
CurrentTrain: epoch  3, batch    32 | loss: 4.7980270Losses:  4.241284370422363 0.1731019914150238
CurrentTrain: epoch  3, batch    33 | loss: 4.4143863Losses:  4.013715744018555 0.10695205628871918
CurrentTrain: epoch  3, batch    34 | loss: 4.1206679Losses:  4.197023391723633 0.07244328409433365
CurrentTrain: epoch  3, batch    35 | loss: 4.2694669Losses:  4.232575416564941 0.10724224150180817
CurrentTrain: epoch  3, batch    36 | loss: 4.3398175Losses:  4.134552001953125 0.2004525065422058
CurrentTrain: epoch  3, batch    37 | loss: 4.3350043Losses:  4.187533378601074 0.030558882281184196
CurrentTrain: epoch  3, batch    38 | loss: 4.2180924Losses:  4.235239028930664 0.07046742737293243
CurrentTrain: epoch  3, batch    39 | loss: 4.3057065Losses:  4.220297336578369 0.1473430097103119
CurrentTrain: epoch  3, batch    40 | loss: 4.3676405Losses:  3.9313647747039795 0.07346297800540924
CurrentTrain: epoch  3, batch    41 | loss: 4.0048280Losses:  4.156826972961426 0.1034955382347107
CurrentTrain: epoch  3, batch    42 | loss: 4.2603226Losses:  4.0647430419921875 0.03530225157737732
CurrentTrain: epoch  3, batch    43 | loss: 4.1000452Losses:  4.208629608154297 0.13868050277233124
CurrentTrain: epoch  3, batch    44 | loss: 4.3473101Losses:  4.127669334411621 0.1666475236415863
CurrentTrain: epoch  3, batch    45 | loss: 4.2943168Losses:  4.188165664672852 0.0957823246717453
CurrentTrain: epoch  3, batch    46 | loss: 4.2839479Losses:  4.2162580490112305 0.21048733592033386
CurrentTrain: epoch  3, batch    47 | loss: 4.4267454Losses:  4.094227313995361 0.14417433738708496
CurrentTrain: epoch  3, batch    48 | loss: 4.2384014Losses:  4.127119064331055 0.14010019600391388
CurrentTrain: epoch  3, batch    49 | loss: 4.2672191Losses:  4.136140823364258 0.20393478870391846
CurrentTrain: epoch  3, batch    50 | loss: 4.3400755Losses:  4.087301254272461 0.14594565331935883
CurrentTrain: epoch  3, batch    51 | loss: 4.2332468Losses:  4.247392654418945 0.0974334180355072
CurrentTrain: epoch  3, batch    52 | loss: 4.3448262Losses:  4.128495216369629 0.0802774578332901
CurrentTrain: epoch  3, batch    53 | loss: 4.2087727Losses:  4.095869541168213 0.13485464453697205
CurrentTrain: epoch  3, batch    54 | loss: 4.2307243Losses:  4.2019572257995605 0.14746007323265076
CurrentTrain: epoch  3, batch    55 | loss: 4.3494172Losses:  4.216413497924805 0.16797654330730438
CurrentTrain: epoch  3, batch    56 | loss: 4.3843899Losses:  4.243278503417969 0.13851043581962585
CurrentTrain: epoch  3, batch    57 | loss: 4.3817887Losses:  4.282175064086914 0.15330353379249573
CurrentTrain: epoch  3, batch    58 | loss: 4.4354787Losses:  4.051668167114258 0.11542929708957672
CurrentTrain: epoch  3, batch    59 | loss: 4.1670976Losses:  4.132303714752197 0.18318507075309753
CurrentTrain: epoch  3, batch    60 | loss: 4.3154888Losses:  4.201712608337402 0.1348700225353241
CurrentTrain: epoch  3, batch    61 | loss: 4.3365827Losses:  4.180914402008057 0.18636471033096313
CurrentTrain: epoch  3, batch    62 | loss: 4.3672791Losses:  4.0935516357421875 0.10402686148881912
CurrentTrain: epoch  3, batch    63 | loss: 4.1975784Losses:  4.113690376281738 0.08662664890289307
CurrentTrain: epoch  3, batch    64 | loss: 4.2003169Losses:  4.188667297363281 0.11495696753263474
CurrentTrain: epoch  3, batch    65 | loss: 4.3036242Losses:  4.176499366760254 0.075229711830616
CurrentTrain: epoch  3, batch    66 | loss: 4.2517290Losses:  4.22244930267334 0.18812890350818634
CurrentTrain: epoch  3, batch    67 | loss: 4.4105783Losses:  4.168549537658691 0.19836238026618958
CurrentTrain: epoch  3, batch    68 | loss: 4.3669119Losses:  4.090850830078125 0.10383866727352142
CurrentTrain: epoch  3, batch    69 | loss: 4.1946893Losses:  4.095746040344238 0.15205630660057068
CurrentTrain: epoch  3, batch    70 | loss: 4.2478023Losses:  4.362072944641113 0.1523813158273697
CurrentTrain: epoch  3, batch    71 | loss: 4.5144544Losses:  4.113323211669922 0.13514910638332367
CurrentTrain: epoch  3, batch    72 | loss: 4.2484722Losses:  5.462953567504883 0.3597294092178345
CurrentTrain: epoch  3, batch    73 | loss: 5.8226829Losses:  4.289802074432373 0.13395282626152039
CurrentTrain: epoch  3, batch    74 | loss: 4.4237547Losses:  4.041306495666504 0.10682899504899979
CurrentTrain: epoch  3, batch    75 | loss: 4.1481357Losses:  4.004061698913574 0.11531311273574829
CurrentTrain: epoch  3, batch    76 | loss: 4.1193748Losses:  4.296807765960693 0.09977337718009949
CurrentTrain: epoch  3, batch    77 | loss: 4.3965812Losses:  4.142854690551758 0.12058188021183014
CurrentTrain: epoch  3, batch    78 | loss: 4.2634368Losses:  4.181309700012207 0.10857094824314117
CurrentTrain: epoch  3, batch    79 | loss: 4.2898808Losses:  4.132745742797852 0.046318359673023224
CurrentTrain: epoch  3, batch    80 | loss: 4.1790643Losses:  4.018467903137207 0.06834091246128082
CurrentTrain: epoch  3, batch    81 | loss: 4.0868087Losses:  4.344125747680664 0.11833269894123077
CurrentTrain: epoch  3, batch    82 | loss: 4.4624586Losses:  4.014605522155762 0.09346263855695724
CurrentTrain: epoch  3, batch    83 | loss: 4.1080680Losses:  4.190607070922852 0.12616416811943054
CurrentTrain: epoch  3, batch    84 | loss: 4.3167710Losses:  4.169668197631836 0.10441014915704727
CurrentTrain: epoch  3, batch    85 | loss: 4.2740784Losses:  4.052932262420654 0.10666635632514954
CurrentTrain: epoch  3, batch    86 | loss: 4.1595988Losses:  4.004403591156006 0.04558185860514641
CurrentTrain: epoch  3, batch    87 | loss: 4.0499854Losses:  4.487199306488037 0.17022705078125
CurrentTrain: epoch  3, batch    88 | loss: 4.6574264Losses:  4.520488262176514 0.08301140367984772
CurrentTrain: epoch  3, batch    89 | loss: 4.6034999Losses:  4.0501203536987305 0.06523051857948303
CurrentTrain: epoch  3, batch    90 | loss: 4.1153507Losses:  4.27718448638916 0.11609599739313126
CurrentTrain: epoch  3, batch    91 | loss: 4.3932805Losses:  4.288824081420898 0.08989077806472778
CurrentTrain: epoch  3, batch    92 | loss: 4.3787150Losses:  4.040312767028809 0.09525598585605621
CurrentTrain: epoch  3, batch    93 | loss: 4.1355686Losses:  4.187931060791016 0.15659426152706146
CurrentTrain: epoch  3, batch    94 | loss: 4.3445253Losses:  4.0723876953125 0.09300363808870316
CurrentTrain: epoch  3, batch    95 | loss: 4.1653914Losses:  4.052107810974121 0.12096862494945526
CurrentTrain: epoch  3, batch    96 | loss: 4.1730766Losses:  4.546833038330078 0.13992087543010712
CurrentTrain: epoch  3, batch    97 | loss: 4.6867537Losses:  4.1495795249938965 0.10715862363576889
CurrentTrain: epoch  3, batch    98 | loss: 4.2567382Losses:  4.227665901184082 0.13680769503116608
CurrentTrain: epoch  3, batch    99 | loss: 4.3644738Losses:  4.107350826263428 0.06995342671871185
CurrentTrain: epoch  3, batch   100 | loss: 4.1773043Losses:  4.186008453369141 0.059387825429439545
CurrentTrain: epoch  3, batch   101 | loss: 4.2453961Losses:  4.169985771179199 0.1443473845720291
CurrentTrain: epoch  3, batch   102 | loss: 4.3143330Losses:  4.032093048095703 0.0941426157951355
CurrentTrain: epoch  3, batch   103 | loss: 4.1262355Losses:  4.236721992492676 0.1342972069978714
CurrentTrain: epoch  3, batch   104 | loss: 4.3710194Losses:  4.114201545715332 0.12162581831216812
CurrentTrain: epoch  3, batch   105 | loss: 4.2358274Losses:  4.148050785064697 0.09221391379833221
CurrentTrain: epoch  3, batch   106 | loss: 4.2402649Losses:  4.137312889099121 0.06099041551351547
CurrentTrain: epoch  3, batch   107 | loss: 4.1983032Losses:  4.02058219909668 0.04504236578941345
CurrentTrain: epoch  3, batch   108 | loss: 4.0656247Losses:  4.265124320983887 0.12613171339035034
CurrentTrain: epoch  3, batch   109 | loss: 4.3912559Losses:  4.222963333129883 0.1771710366010666
CurrentTrain: epoch  3, batch   110 | loss: 4.4001346Losses:  3.952687978744507 0.11038853973150253
CurrentTrain: epoch  3, batch   111 | loss: 4.0630765Losses:  3.9942896366119385 0.05397690087556839
CurrentTrain: epoch  3, batch   112 | loss: 4.0482664Losses:  4.090939044952393 0.12698854506015778
CurrentTrain: epoch  3, batch   113 | loss: 4.2179275Losses:  4.044521331787109 0.17318832874298096
CurrentTrain: epoch  3, batch   114 | loss: 4.2177095Losses:  4.1773481369018555 0.13199490308761597
CurrentTrain: epoch  3, batch   115 | loss: 4.3093429Losses:  4.179594993591309 0.1476454734802246
CurrentTrain: epoch  3, batch   116 | loss: 4.3272405Losses:  4.0506439208984375 0.11392167210578918
CurrentTrain: epoch  3, batch   117 | loss: 4.1645656Losses:  3.9504904747009277 0.05392257869243622
CurrentTrain: epoch  3, batch   118 | loss: 4.0044131Losses:  4.068708896636963 0.1481354981660843
CurrentTrain: epoch  3, batch   119 | loss: 4.2168446Losses:  4.058082103729248 0.15562817454338074
CurrentTrain: epoch  3, batch   120 | loss: 4.2137103Losses:  3.9885365962982178 0.08483472466468811
CurrentTrain: epoch  3, batch   121 | loss: 4.0733714Losses:  4.008265495300293 0.17299965023994446
CurrentTrain: epoch  3, batch   122 | loss: 4.1812654Losses:  4.096864700317383 0.1328217089176178
CurrentTrain: epoch  3, batch   123 | loss: 4.2296863Losses:  4.033392429351807 0.09484504908323288
CurrentTrain: epoch  3, batch   124 | loss: 4.1282372Losses:  4.143265724182129 0.11185425519943237
CurrentTrain: epoch  4, batch     0 | loss: 4.2551198Losses:  4.083203315734863 0.12035642564296722
CurrentTrain: epoch  4, batch     1 | loss: 4.2035599Losses:  4.036898612976074 0.09589200466871262
CurrentTrain: epoch  4, batch     2 | loss: 4.1327906Losses:  4.0778632164001465 0.13852688670158386
CurrentTrain: epoch  4, batch     3 | loss: 4.2163901Losses:  4.150444984436035 0.08683523535728455
CurrentTrain: epoch  4, batch     4 | loss: 4.2372804Losses:  4.153542995452881 0.09903130680322647
CurrentTrain: epoch  4, batch     5 | loss: 4.2525744Losses:  4.030724048614502 0.11951614916324615
CurrentTrain: epoch  4, batch     6 | loss: 4.1502404Losses:  4.063631057739258 0.0870017409324646
CurrentTrain: epoch  4, batch     7 | loss: 4.1506329Losses:  4.025900840759277 0.0917614996433258
CurrentTrain: epoch  4, batch     8 | loss: 4.1176624Losses:  4.094353675842285 0.13039252161979675
CurrentTrain: epoch  4, batch     9 | loss: 4.2247462Losses:  4.020161151885986 0.07949614524841309
CurrentTrain: epoch  4, batch    10 | loss: 4.0996571Losses:  4.026050567626953 0.11819727718830109
CurrentTrain: epoch  4, batch    11 | loss: 4.1442480Losses:  4.247318267822266 0.10467830300331116
CurrentTrain: epoch  4, batch    12 | loss: 4.3519964Losses:  3.915651798248291 0.04689272120594978
CurrentTrain: epoch  4, batch    13 | loss: 3.9625444Losses:  4.067448139190674 0.20743349194526672
CurrentTrain: epoch  4, batch    14 | loss: 4.2748818Losses:  4.203158378601074 0.11360182613134384
CurrentTrain: epoch  4, batch    15 | loss: 4.3167601Losses:  4.109635829925537 0.06062803789973259
CurrentTrain: epoch  4, batch    16 | loss: 4.1702638Losses:  4.025961875915527 0.09699451923370361
CurrentTrain: epoch  4, batch    17 | loss: 4.1229563Losses:  4.253536224365234 0.08514223992824554
CurrentTrain: epoch  4, batch    18 | loss: 4.3386784Losses:  4.042293548583984 0.07298097759485245
CurrentTrain: epoch  4, batch    19 | loss: 4.1152744Losses:  4.039559364318848 0.10482558608055115
CurrentTrain: epoch  4, batch    20 | loss: 4.1443849Losses:  4.051898956298828 0.168167844414711
CurrentTrain: epoch  4, batch    21 | loss: 4.2200670Losses:  4.032187461853027 0.10582791268825531
CurrentTrain: epoch  4, batch    22 | loss: 4.1380153Losses:  4.051202774047852 0.10988172888755798
CurrentTrain: epoch  4, batch    23 | loss: 4.1610847Losses:  4.015923976898193 0.1178523525595665
CurrentTrain: epoch  4, batch    24 | loss: 4.1337762Losses:  3.9787654876708984 0.12410666048526764
CurrentTrain: epoch  4, batch    25 | loss: 4.1028724Losses:  4.071211338043213 0.07393252849578857
CurrentTrain: epoch  4, batch    26 | loss: 4.1451440Losses:  4.073145866394043 0.13261815905570984
CurrentTrain: epoch  4, batch    27 | loss: 4.2057638Losses:  3.9906668663024902 0.06560607254505157
CurrentTrain: epoch  4, batch    28 | loss: 4.0562730Losses:  4.095308303833008 0.12073652446269989
CurrentTrain: epoch  4, batch    29 | loss: 4.2160449Losses:  4.0664286613464355 0.09521250426769257
CurrentTrain: epoch  4, batch    30 | loss: 4.1616411Losses:  4.026208400726318 0.09402170777320862
CurrentTrain: epoch  4, batch    31 | loss: 4.1202302Losses:  4.025568008422852 0.07510265707969666
CurrentTrain: epoch  4, batch    32 | loss: 4.1006708Losses:  4.967618942260742 0.1383964866399765
CurrentTrain: epoch  4, batch    33 | loss: 5.1060152Losses:  4.029544830322266 0.06949254125356674
CurrentTrain: epoch  4, batch    34 | loss: 4.0990372Losses:  4.046210289001465 0.051650263369083405
CurrentTrain: epoch  4, batch    35 | loss: 4.0978603Losses:  4.137042045593262 0.06872715055942535
CurrentTrain: epoch  4, batch    36 | loss: 4.2057691Losses:  4.015712738037109 0.11813554912805557
CurrentTrain: epoch  4, batch    37 | loss: 4.1338482Losses:  4.0039873123168945 0.07710109651088715
CurrentTrain: epoch  4, batch    38 | loss: 4.0810885Losses:  4.22987174987793 0.13756908476352692
CurrentTrain: epoch  4, batch    39 | loss: 4.3674407Losses:  4.148507118225098 0.11265882104635239
CurrentTrain: epoch  4, batch    40 | loss: 4.2611661Losses:  3.9477391242980957 0.11414096504449844
CurrentTrain: epoch  4, batch    41 | loss: 4.0618801Losses:  4.010148525238037 0.07478976249694824
CurrentTrain: epoch  4, batch    42 | loss: 4.0849380Losses:  3.972557306289673 0.1277989149093628
CurrentTrain: epoch  4, batch    43 | loss: 4.1003561Losses:  4.272630214691162 0.09490205347537994
CurrentTrain: epoch  4, batch    44 | loss: 4.3675323Losses:  4.043483734130859 0.11086101084947586
CurrentTrain: epoch  4, batch    45 | loss: 4.1543446Losses:  4.2177228927612305 0.12811142206192017
CurrentTrain: epoch  4, batch    46 | loss: 4.3458343Losses:  4.0355753898620605 0.10252842307090759
CurrentTrain: epoch  4, batch    47 | loss: 4.1381040Losses:  4.031666278839111 0.09468822181224823
CurrentTrain: epoch  4, batch    48 | loss: 4.1263547Losses:  3.9748752117156982 0.07211536914110184
CurrentTrain: epoch  4, batch    49 | loss: 4.0469904Losses:  4.06348991394043 0.13413438200950623
CurrentTrain: epoch  4, batch    50 | loss: 4.1976242Losses:  4.206096172332764 0.12160103768110275
CurrentTrain: epoch  4, batch    51 | loss: 4.3276973Losses:  4.0872368812561035 0.1270972192287445
CurrentTrain: epoch  4, batch    52 | loss: 4.2143340Losses:  4.133089542388916 0.13351891934871674
CurrentTrain: epoch  4, batch    53 | loss: 4.2666082Losses:  4.399066925048828 0.13331685960292816
CurrentTrain: epoch  4, batch    54 | loss: 4.5323839Losses:  4.010507583618164 0.05319245159626007
CurrentTrain: epoch  4, batch    55 | loss: 4.0637002Losses:  4.117345809936523 0.08547079563140869
CurrentTrain: epoch  4, batch    56 | loss: 4.2028165Losses:  4.03749942779541 0.0927513837814331
CurrentTrain: epoch  4, batch    57 | loss: 4.1302509Losses:  4.0919365882873535 0.1021299660205841
CurrentTrain: epoch  4, batch    58 | loss: 4.1940665Losses:  3.925039291381836 0.06654810160398483
CurrentTrain: epoch  4, batch    59 | loss: 3.9915874Losses:  3.9911599159240723 0.07800246775150299
CurrentTrain: epoch  4, batch    60 | loss: 4.0691624Losses:  5.427760124206543 0.22381678223609924
CurrentTrain: epoch  4, batch    61 | loss: 5.6515770Losses:  4.193376541137695 0.08522126823663712
CurrentTrain: epoch  4, batch    62 | loss: 4.2785978Losses:  4.435523509979248 0.15035904943943024
CurrentTrain: epoch  4, batch    63 | loss: 4.5858827Losses:  4.184182643890381 0.11106445640325546
CurrentTrain: epoch  4, batch    64 | loss: 4.2952471Losses:  4.229424476623535 0.06728322803974152
CurrentTrain: epoch  4, batch    65 | loss: 4.2967076Losses:  3.9897944927215576 0.055637113749980927
CurrentTrain: epoch  4, batch    66 | loss: 4.0454316Losses:  4.024014472961426 0.09501390904188156
CurrentTrain: epoch  4, batch    67 | loss: 4.1190286Losses:  4.321273326873779 0.1454349160194397
CurrentTrain: epoch  4, batch    68 | loss: 4.4667082Losses:  4.057957172393799 0.08875815570354462
CurrentTrain: epoch  4, batch    69 | loss: 4.1467152Losses:  4.041617393493652 0.07858575135469437
CurrentTrain: epoch  4, batch    70 | loss: 4.1202030Losses:  4.061152935028076 0.11021972447633743
CurrentTrain: epoch  4, batch    71 | loss: 4.1713729Losses:  4.390662670135498 0.11198793351650238
CurrentTrain: epoch  4, batch    72 | loss: 4.5026507Losses:  4.05031156539917 0.045136768370866776
CurrentTrain: epoch  4, batch    73 | loss: 4.0954485Losses:  4.014875411987305 0.1144915223121643
CurrentTrain: epoch  4, batch    74 | loss: 4.1293669Losses:  4.077031135559082 0.06895209848880768
CurrentTrain: epoch  4, batch    75 | loss: 4.1459832Losses:  4.4886698722839355 0.21608251333236694
CurrentTrain: epoch  4, batch    76 | loss: 4.7047524Losses:  4.338013172149658 0.15619099140167236
CurrentTrain: epoch  4, batch    77 | loss: 4.4942040Losses:  4.057262420654297 0.0832999050617218
CurrentTrain: epoch  4, batch    78 | loss: 4.1405625Losses:  4.03788423538208 0.07108709961175919
CurrentTrain: epoch  4, batch    79 | loss: 4.1089711Losses:  4.0783796310424805 0.06255154311656952
CurrentTrain: epoch  4, batch    80 | loss: 4.1409311Losses:  4.08022403717041 0.08184434473514557
CurrentTrain: epoch  4, batch    81 | loss: 4.1620684Losses:  4.22216796875 0.11883258819580078
CurrentTrain: epoch  4, batch    82 | loss: 4.3410006Losses:  4.004251480102539 0.09211008995771408
CurrentTrain: epoch  4, batch    83 | loss: 4.0963616Losses:  4.247433662414551 0.06759709119796753
CurrentTrain: epoch  4, batch    84 | loss: 4.3150306Losses:  4.023621559143066 0.0791480541229248
CurrentTrain: epoch  4, batch    85 | loss: 4.1027699Losses:  4.5941667556762695 0.11274275928735733
CurrentTrain: epoch  4, batch    86 | loss: 4.7069097Losses:  4.417266368865967 0.1935783326625824
CurrentTrain: epoch  4, batch    87 | loss: 4.6108446Losses:  3.9545493125915527 0.06570656597614288
CurrentTrain: epoch  4, batch    88 | loss: 4.0202560Losses:  4.1514434814453125 0.06621596217155457
CurrentTrain: epoch  4, batch    89 | loss: 4.2176595Losses:  4.023090839385986 0.041280850768089294
CurrentTrain: epoch  4, batch    90 | loss: 4.0643716Losses:  3.8837294578552246 0.02506731078028679
CurrentTrain: epoch  4, batch    91 | loss: 3.9087968Losses:  3.8978233337402344 0.042145952582359314
CurrentTrain: epoch  4, batch    92 | loss: 3.9399693Losses:  3.9577746391296387 0.11072491109371185
CurrentTrain: epoch  4, batch    93 | loss: 4.0684996Losses:  3.9113965034484863 0.07808032631874084
CurrentTrain: epoch  4, batch    94 | loss: 3.9894769Losses:  4.247612953186035 0.10315994918346405
CurrentTrain: epoch  4, batch    95 | loss: 4.3507729Losses:  4.0511250495910645 0.05980467051267624
CurrentTrain: epoch  4, batch    96 | loss: 4.1109295Losses:  4.280242919921875 0.18932054936885834
CurrentTrain: epoch  4, batch    97 | loss: 4.4695635Losses:  4.144359588623047 0.08388876914978027
CurrentTrain: epoch  4, batch    98 | loss: 4.2282486Losses:  3.9844858646392822 0.1111079752445221
CurrentTrain: epoch  4, batch    99 | loss: 4.0955939Losses:  3.9956023693084717 0.0785544365644455
CurrentTrain: epoch  4, batch   100 | loss: 4.0741568Losses:  4.1736907958984375 0.1264786273241043
CurrentTrain: epoch  4, batch   101 | loss: 4.3001695Losses:  4.056344509124756 0.12261410057544708
CurrentTrain: epoch  4, batch   102 | loss: 4.1789584Losses:  4.273842811584473 0.08484502881765366
CurrentTrain: epoch  4, batch   103 | loss: 4.3586879Losses:  4.297327995300293 0.08294789493083954
CurrentTrain: epoch  4, batch   104 | loss: 4.3802757Losses:  3.993563652038574 0.07862207293510437
CurrentTrain: epoch  4, batch   105 | loss: 4.0721855Losses:  3.9514870643615723 0.037867072969675064
CurrentTrain: epoch  4, batch   106 | loss: 3.9893541Losses:  3.9749815464019775 0.05407194048166275
CurrentTrain: epoch  4, batch   107 | loss: 4.0290537Losses:  4.343618869781494 0.14310264587402344
CurrentTrain: epoch  4, batch   108 | loss: 4.4867215Losses:  3.996128559112549 0.10182300209999084
CurrentTrain: epoch  4, batch   109 | loss: 4.0979514Losses:  4.331420421600342 0.11514978855848312
CurrentTrain: epoch  4, batch   110 | loss: 4.4465704Losses:  4.011836051940918 0.08953148126602173
CurrentTrain: epoch  4, batch   111 | loss: 4.1013675Losses:  4.24862003326416 0.06730207800865173
CurrentTrain: epoch  4, batch   112 | loss: 4.3159223Losses:  4.010733604431152 0.08669745177030563
CurrentTrain: epoch  4, batch   113 | loss: 4.0974312Losses:  3.999457359313965 0.09944847971200943
CurrentTrain: epoch  4, batch   114 | loss: 4.0989060Losses:  4.017495155334473 0.09026399999856949
CurrentTrain: epoch  4, batch   115 | loss: 4.1077590Losses:  4.002842903137207 0.055381037294864655
CurrentTrain: epoch  4, batch   116 | loss: 4.0582237Losses:  4.058206081390381 0.14652612805366516
CurrentTrain: epoch  4, batch   117 | loss: 4.2047324Losses:  4.034481525421143 0.06650572270154953
CurrentTrain: epoch  4, batch   118 | loss: 4.1009874Losses:  4.023252487182617 0.06111598014831543
CurrentTrain: epoch  4, batch   119 | loss: 4.0843687Losses:  4.028149604797363 0.055223189294338226
CurrentTrain: epoch  4, batch   120 | loss: 4.0833726Losses:  4.309819221496582 0.12043297290802002
CurrentTrain: epoch  4, batch   121 | loss: 4.4302521Losses:  3.9988956451416016 0.06092701852321625
CurrentTrain: epoch  4, batch   122 | loss: 4.0598226Losses:  4.059633255004883 0.052494682371616364
CurrentTrain: epoch  4, batch   123 | loss: 4.1121278Losses:  4.065791130065918 0.11021549999713898
CurrentTrain: epoch  4, batch   124 | loss: 4.1760068Losses:  3.9997172355651855 0.0839168056845665
CurrentTrain: epoch  5, batch     0 | loss: 4.0836339Losses:  4.064669132232666 0.11846524477005005
CurrentTrain: epoch  5, batch     1 | loss: 4.1831346Losses:  4.122769832611084 0.08291785418987274
CurrentTrain: epoch  5, batch     2 | loss: 4.2056875Losses:  4.085902690887451 0.11536365002393723
CurrentTrain: epoch  5, batch     3 | loss: 4.2012663Losses:  4.0584259033203125 0.11945400387048721
CurrentTrain: epoch  5, batch     4 | loss: 4.1778798Losses:  4.154412269592285 0.12122838944196701
CurrentTrain: epoch  5, batch     5 | loss: 4.2756405Losses:  4.10970401763916 0.12803532183170319
CurrentTrain: epoch  5, batch     6 | loss: 4.2377396Losses:  4.042040824890137 0.12452422082424164
CurrentTrain: epoch  5, batch     7 | loss: 4.1665649Losses:  4.04325008392334 0.06937944889068604
CurrentTrain: epoch  5, batch     8 | loss: 4.1126294Losses:  4.09282922744751 0.1208726167678833
CurrentTrain: epoch  5, batch     9 | loss: 4.2137017Losses:  4.049739837646484 0.07972852885723114
CurrentTrain: epoch  5, batch    10 | loss: 4.1294684Losses:  4.041378498077393 0.10805150121450424
CurrentTrain: epoch  5, batch    11 | loss: 4.1494298Losses:  3.980806589126587 0.11750500649213791
CurrentTrain: epoch  5, batch    12 | loss: 4.0983114Losses:  4.069124221801758 0.09509900212287903
CurrentTrain: epoch  5, batch    13 | loss: 4.1642232Losses:  4.164029121398926 0.1513681709766388
CurrentTrain: epoch  5, batch    14 | loss: 4.3153973Losses:  3.985886812210083 0.06952275335788727
CurrentTrain: epoch  5, batch    15 | loss: 4.0554094Losses:  3.997109889984131 0.06961190700531006
CurrentTrain: epoch  5, batch    16 | loss: 4.0667219Losses:  3.989450693130493 0.0419306717813015
CurrentTrain: epoch  5, batch    17 | loss: 4.0313811Losses:  4.021622657775879 0.06269931048154831
CurrentTrain: epoch  5, batch    18 | loss: 4.0843220Losses:  4.05257511138916 0.09373565763235092
CurrentTrain: epoch  5, batch    19 | loss: 4.1463108Losses:  4.050408363342285 0.06343050301074982
CurrentTrain: epoch  5, batch    20 | loss: 4.1138387Losses:  3.99092960357666 0.02760244905948639
CurrentTrain: epoch  5, batch    21 | loss: 4.0185323Losses:  4.017407417297363 0.08497422933578491
CurrentTrain: epoch  5, batch    22 | loss: 4.1023817Losses:  4.049274444580078 0.0979117751121521
CurrentTrain: epoch  5, batch    23 | loss: 4.1471863Losses:  4.082474231719971 0.11000281572341919
CurrentTrain: epoch  5, batch    24 | loss: 4.1924772Losses:  3.985037326812744 0.044173549860715866
CurrentTrain: epoch  5, batch    25 | loss: 4.0292110Losses:  4.030252933502197 0.059498533606529236
CurrentTrain: epoch  5, batch    26 | loss: 4.0897512Losses:  3.9753360748291016 0.0740690678358078
CurrentTrain: epoch  5, batch    27 | loss: 4.0494051Losses:  4.010476589202881 0.07245581597089767
CurrentTrain: epoch  5, batch    28 | loss: 4.0829325Losses:  4.031424045562744 0.0705108493566513
CurrentTrain: epoch  5, batch    29 | loss: 4.1019349Losses:  4.213809967041016 0.08465492725372314
CurrentTrain: epoch  5, batch    30 | loss: 4.2984648Losses:  4.03033447265625 0.1384851634502411
CurrentTrain: epoch  5, batch    31 | loss: 4.1688194Losses:  4.029191493988037 0.07732071727514267
CurrentTrain: epoch  5, batch    32 | loss: 4.1065121Losses:  4.085054397583008 0.11587119102478027
CurrentTrain: epoch  5, batch    33 | loss: 4.2009258Losses:  4.0276198387146 0.10629425942897797
CurrentTrain: epoch  5, batch    34 | loss: 4.1339140Losses:  3.9172816276550293 0.06708534806966782
CurrentTrain: epoch  5, batch    35 | loss: 3.9843669Losses:  4.025670528411865 0.06998001784086227
CurrentTrain: epoch  5, batch    36 | loss: 4.0956507Losses:  4.00369119644165 0.06864753365516663
CurrentTrain: epoch  5, batch    37 | loss: 4.0723386Losses:  4.071706771850586 0.09946943819522858
CurrentTrain: epoch  5, batch    38 | loss: 4.1711764Losses:  4.077842712402344 0.08086097985506058
CurrentTrain: epoch  5, batch    39 | loss: 4.1587038Losses:  4.107447147369385 0.10076979547739029
CurrentTrain: epoch  5, batch    40 | loss: 4.2082171Losses:  3.9372787475585938 0.04555179923772812
CurrentTrain: epoch  5, batch    41 | loss: 3.9828305Losses:  3.9467594623565674 0.07430361956357956
CurrentTrain: epoch  5, batch    42 | loss: 4.0210629Losses:  3.9537694454193115 0.04590750113129616
CurrentTrain: epoch  5, batch    43 | loss: 3.9996769Losses:  4.005244731903076 0.0524136908352375
CurrentTrain: epoch  5, batch    44 | loss: 4.0576582Losses:  3.993800163269043 0.09197758883237839
CurrentTrain: epoch  5, batch    45 | loss: 4.0857778Losses:  3.9532885551452637 0.06630216538906097
CurrentTrain: epoch  5, batch    46 | loss: 4.0195909Losses:  3.972960948944092 0.0887143686413765
CurrentTrain: epoch  5, batch    47 | loss: 4.0616755Losses:  4.123658180236816 0.12253858149051666
CurrentTrain: epoch  5, batch    48 | loss: 4.2461967Losses:  3.959817409515381 0.052015624940395355
CurrentTrain: epoch  5, batch    49 | loss: 4.0118332Losses:  4.094263076782227 0.04088277369737625
CurrentTrain: epoch  5, batch    50 | loss: 4.1351457Losses:  4.020251750946045 0.08672168850898743
CurrentTrain: epoch  5, batch    51 | loss: 4.1069736Losses:  4.132882118225098 0.11671409755945206
CurrentTrain: epoch  5, batch    52 | loss: 4.2495961Losses:  3.9558422565460205 0.04040776938199997
CurrentTrain: epoch  5, batch    53 | loss: 3.9962499Losses:  3.9931325912475586 0.10309141874313354
CurrentTrain: epoch  5, batch    54 | loss: 4.0962238Losses:  3.9630069732666016 0.10521286725997925
CurrentTrain: epoch  5, batch    55 | loss: 4.0682197Losses:  4.06845760345459 0.14773736894130707
CurrentTrain: epoch  5, batch    56 | loss: 4.2161951Losses:  4.02463960647583 0.05405681952834129
CurrentTrain: epoch  5, batch    57 | loss: 4.0786963Losses:  4.130366325378418 0.11511459946632385
CurrentTrain: epoch  5, batch    58 | loss: 4.2454810Losses:  4.054752349853516 0.09397818148136139
CurrentTrain: epoch  5, batch    59 | loss: 4.1487308Losses:  4.025441646575928 0.07388616353273392
CurrentTrain: epoch  5, batch    60 | loss: 4.0993280Losses:  3.9355900287628174 0.09340137243270874
CurrentTrain: epoch  5, batch    61 | loss: 4.0289912Losses:  3.9750423431396484 0.06902249157428741
CurrentTrain: epoch  5, batch    62 | loss: 4.0440650Losses:  4.078307151794434 0.09529419988393784
CurrentTrain: epoch  5, batch    63 | loss: 4.1736012Losses:  4.00819206237793 0.07138264924287796
CurrentTrain: epoch  5, batch    64 | loss: 4.0795746Losses:  3.9702999591827393 0.05435001850128174
CurrentTrain: epoch  5, batch    65 | loss: 4.0246501Losses:  4.015693664550781 0.06209347024559975
CurrentTrain: epoch  5, batch    66 | loss: 4.0777869Losses:  3.9973208904266357 0.02398841828107834
CurrentTrain: epoch  5, batch    67 | loss: 4.0213094Losses:  3.913668155670166 0.06898476183414459
CurrentTrain: epoch  5, batch    68 | loss: 3.9826529Losses:  4.025178909301758 0.04823017492890358
CurrentTrain: epoch  5, batch    69 | loss: 4.0734091Losses:  3.9546470642089844 0.09421367943286896
CurrentTrain: epoch  5, batch    70 | loss: 4.0488605Losses:  3.9604532718658447 0.09919486939907074
CurrentTrain: epoch  5, batch    71 | loss: 4.0596480Losses:  4.068939208984375 0.05828482657670975
CurrentTrain: epoch  5, batch    72 | loss: 4.1272240Losses:  3.989778518676758 0.05129633843898773
CurrentTrain: epoch  5, batch    73 | loss: 4.0410748Losses:  3.949685573577881 0.08480501174926758
CurrentTrain: epoch  5, batch    74 | loss: 4.0344906Losses:  4.079229831695557 0.0707225501537323
CurrentTrain: epoch  5, batch    75 | loss: 4.1499524Losses:  4.002475738525391 0.07264041155576706
CurrentTrain: epoch  5, batch    76 | loss: 4.0751162Losses:  4.044050693511963 0.06244148313999176
CurrentTrain: epoch  5, batch    77 | loss: 4.1064920Losses:  4.063138961791992 0.09028363972902298
CurrentTrain: epoch  5, batch    78 | loss: 4.1534228Losses:  3.992328405380249 0.03650607913732529
CurrentTrain: epoch  5, batch    79 | loss: 4.0288343Losses:  5.404368877410889 0.2802179455757141
CurrentTrain: epoch  5, batch    80 | loss: 5.6845870Losses:  4.161807060241699 0.07783389091491699
CurrentTrain: epoch  5, batch    81 | loss: 4.2396412Losses:  4.002593040466309 0.033387601375579834
CurrentTrain: epoch  5, batch    82 | loss: 4.0359807Losses:  4.134169101715088 0.10525424778461456
CurrentTrain: epoch  5, batch    83 | loss: 4.2394233Losses:  3.9836788177490234 0.07961511611938477
CurrentTrain: epoch  5, batch    84 | loss: 4.0632939Losses:  4.016605377197266 0.061601147055625916
CurrentTrain: epoch  5, batch    85 | loss: 4.0782065Losses:  3.949930191040039 0.06211470067501068
CurrentTrain: epoch  5, batch    86 | loss: 4.0120449Losses:  3.9835023880004883 0.14170733094215393
CurrentTrain: epoch  5, batch    87 | loss: 4.1252098Losses:  4.010288238525391 0.08928678929805756
CurrentTrain: epoch  5, batch    88 | loss: 4.0995750Losses:  3.9286088943481445 0.08992308378219604
CurrentTrain: epoch  5, batch    89 | loss: 4.0185318Losses:  3.9640941619873047 0.089263416826725
CurrentTrain: epoch  5, batch    90 | loss: 4.0533576Losses:  4.044407367706299 0.08441247791051865
CurrentTrain: epoch  5, batch    91 | loss: 4.1288199Losses:  3.9991636276245117 0.07234670966863632
CurrentTrain: epoch  5, batch    92 | loss: 4.0715103Losses:  4.291592597961426 0.10044093430042267
CurrentTrain: epoch  5, batch    93 | loss: 4.3920336Losses:  3.972379684448242 0.05944950878620148
CurrentTrain: epoch  5, batch    94 | loss: 4.0318294Losses:  4.13394021987915 0.07824017852544785
CurrentTrain: epoch  5, batch    95 | loss: 4.2121806Losses:  4.090253829956055 0.058834951370954514
CurrentTrain: epoch  5, batch    96 | loss: 4.1490889Losses:  3.961585283279419 0.06564430892467499
CurrentTrain: epoch  5, batch    97 | loss: 4.0272298Losses:  3.9217281341552734 0.05446535348892212
CurrentTrain: epoch  5, batch    98 | loss: 3.9761934Losses:  3.9922289848327637 0.09967869520187378
CurrentTrain: epoch  5, batch    99 | loss: 4.0919075Losses:  4.296998977661133 0.10999594628810883
CurrentTrain: epoch  5, batch   100 | loss: 4.4069948Losses:  4.100364685058594 0.0696839690208435
CurrentTrain: epoch  5, batch   101 | loss: 4.1700487Losses:  3.8751392364501953 0.030840016901493073
CurrentTrain: epoch  5, batch   102 | loss: 3.9059792Losses:  3.9618406295776367 0.06355871260166168
CurrentTrain: epoch  5, batch   103 | loss: 4.0253992Losses:  3.9614858627319336 0.08842294663190842
CurrentTrain: epoch  5, batch   104 | loss: 4.0499086Losses:  3.956315755844116 0.06809115409851074
CurrentTrain: epoch  5, batch   105 | loss: 4.0244069Losses:  4.001989841461182 0.09824591875076294
CurrentTrain: epoch  5, batch   106 | loss: 4.1002359Losses:  4.017556190490723 0.06816911697387695
CurrentTrain: epoch  5, batch   107 | loss: 4.0857253Losses:  3.9891085624694824 0.091386578977108
CurrentTrain: epoch  5, batch   108 | loss: 4.0804954Losses:  3.983550548553467 0.06872506439685822
CurrentTrain: epoch  5, batch   109 | loss: 4.0522757Losses:  4.044515609741211 0.06903348118066788
CurrentTrain: epoch  5, batch   110 | loss: 4.1135492Losses:  4.077548503875732 0.10484597086906433
CurrentTrain: epoch  5, batch   111 | loss: 4.1823945Losses:  3.9607269763946533 0.042498089373111725
CurrentTrain: epoch  5, batch   112 | loss: 4.0032248Losses:  3.968106508255005 0.11619619280099869
CurrentTrain: epoch  5, batch   113 | loss: 4.0843029Losses:  4.005281925201416 0.06286416947841644
CurrentTrain: epoch  5, batch   114 | loss: 4.0681462Losses:  4.018857002258301 0.03688504174351692
CurrentTrain: epoch  5, batch   115 | loss: 4.0557423Losses:  4.079111576080322 0.07410863786935806
CurrentTrain: epoch  5, batch   116 | loss: 4.1532202Losses:  3.9514389038085938 0.0763595923781395
CurrentTrain: epoch  5, batch   117 | loss: 4.0277987Losses:  4.004025459289551 0.09669312834739685
CurrentTrain: epoch  5, batch   118 | loss: 4.1007185Losses:  4.039076805114746 0.038051970303058624
CurrentTrain: epoch  5, batch   119 | loss: 4.0771289Losses:  4.079519748687744 0.08137360215187073
CurrentTrain: epoch  5, batch   120 | loss: 4.1608934Losses:  4.100405216217041 0.11751440167427063
CurrentTrain: epoch  5, batch   121 | loss: 4.2179198Losses:  3.8779826164245605 0.05107971280813217
CurrentTrain: epoch  5, batch   122 | loss: 3.9290624Losses:  3.9935433864593506 0.05755692720413208
CurrentTrain: epoch  5, batch   123 | loss: 4.0511003Losses:  3.9284510612487793 0.04508848488330841
CurrentTrain: epoch  5, batch   124 | loss: 3.9735396Losses:  4.046372890472412 0.04537464678287506
CurrentTrain: epoch  6, batch     0 | loss: 4.0917478Losses:  3.9884934425354004 0.05921133980154991
CurrentTrain: epoch  6, batch     1 | loss: 4.0477047Losses:  3.948760986328125 0.034138113260269165
CurrentTrain: epoch  6, batch     2 | loss: 3.9828992Losses:  4.1573286056518555 0.06988182663917542
CurrentTrain: epoch  6, batch     3 | loss: 4.2272105Losses:  4.053213119506836 0.0613795667886734
CurrentTrain: epoch  6, batch     4 | loss: 4.1145926Losses:  4.098186492919922 0.08702629059553146
CurrentTrain: epoch  6, batch     5 | loss: 4.1852126Losses:  3.9710259437561035 0.04511074349284172
CurrentTrain: epoch  6, batch     6 | loss: 4.0161366Losses:  3.923027753829956 0.04404941573739052
CurrentTrain: epoch  6, batch     7 | loss: 3.9670773Losses:  4.0362114906311035 0.07474187761545181
CurrentTrain: epoch  6, batch     8 | loss: 4.1109533Losses:  3.9800662994384766 0.03620195388793945
CurrentTrain: epoch  6, batch     9 | loss: 4.0162683Losses:  4.064701080322266 0.12514466047286987
CurrentTrain: epoch  6, batch    10 | loss: 4.1898456Losses:  3.906043767929077 0.06188572570681572
CurrentTrain: epoch  6, batch    11 | loss: 3.9679296Losses:  4.076129913330078 0.079083651304245
CurrentTrain: epoch  6, batch    12 | loss: 4.1552134Losses:  4.071032524108887 0.07729820907115936
CurrentTrain: epoch  6, batch    13 | loss: 4.1483307Losses:  3.991191864013672 0.05306436866521835
CurrentTrain: epoch  6, batch    14 | loss: 4.0442562Losses:  3.9673259258270264 0.035336412489414215
CurrentTrain: epoch  6, batch    15 | loss: 4.0026622Losses:  4.025733947753906 0.11415055394172668
CurrentTrain: epoch  6, batch    16 | loss: 4.1398845Losses:  4.072210311889648 0.10220348834991455
CurrentTrain: epoch  6, batch    17 | loss: 4.1744137Losses:  3.9801487922668457 0.034727439284324646
CurrentTrain: epoch  6, batch    18 | loss: 4.0148764Losses:  3.988015651702881 0.057256560772657394
CurrentTrain: epoch  6, batch    19 | loss: 4.0452724Losses:  3.9605557918548584 0.06489011645317078
CurrentTrain: epoch  6, batch    20 | loss: 4.0254459Losses:  3.9395151138305664 0.043846458196640015
CurrentTrain: epoch  6, batch    21 | loss: 3.9833615Losses:  3.9771339893341064 0.06035584211349487
CurrentTrain: epoch  6, batch    22 | loss: 4.0374899Losses:  4.032029151916504 0.047177642583847046
CurrentTrain: epoch  6, batch    23 | loss: 4.0792069Losses:  4.013488292694092 0.03780244663357735
CurrentTrain: epoch  6, batch    24 | loss: 4.0512905Losses:  4.001008033752441 0.04588693752884865
CurrentTrain: epoch  6, batch    25 | loss: 4.0468950Losses:  3.98128080368042 0.042439721524715424
CurrentTrain: epoch  6, batch    26 | loss: 4.0237207Losses:  4.032314300537109 0.06515607237815857
CurrentTrain: epoch  6, batch    27 | loss: 4.0974703Losses:  3.9761126041412354 0.03578067570924759
CurrentTrain: epoch  6, batch    28 | loss: 4.0118933Losses:  3.949249744415283 0.08671209216117859
CurrentTrain: epoch  6, batch    29 | loss: 4.0359616Losses:  3.9512033462524414 0.05190776288509369
CurrentTrain: epoch  6, batch    30 | loss: 4.0031109Losses:  4.1044816970825195 0.09900620579719543
CurrentTrain: epoch  6, batch    31 | loss: 4.2034879Losses:  3.9196858406066895 0.047169044613838196
CurrentTrain: epoch  6, batch    32 | loss: 3.9668548Losses:  4.048425674438477 0.09749492257833481
CurrentTrain: epoch  6, batch    33 | loss: 4.1459208Losses:  3.9839086532592773 0.05766744539141655
CurrentTrain: epoch  6, batch    34 | loss: 4.0415759Losses:  4.175258636474609 0.0977911576628685
CurrentTrain: epoch  6, batch    35 | loss: 4.2730498Losses:  3.98526930809021 0.04832587391138077
CurrentTrain: epoch  6, batch    36 | loss: 4.0335951Losses:  4.0934224128723145 0.0853576585650444
CurrentTrain: epoch  6, batch    37 | loss: 4.1787801Losses:  3.959571599960327 0.0570056177675724
CurrentTrain: epoch  6, batch    38 | loss: 4.0165772Losses:  4.063610076904297 0.05669095367193222
CurrentTrain: epoch  6, batch    39 | loss: 4.1203012Losses:  3.975979804992676 0.07491202652454376
CurrentTrain: epoch  6, batch    40 | loss: 4.0508919Losses:  4.124663352966309 0.11547336727380753
CurrentTrain: epoch  6, batch    41 | loss: 4.2401366Losses:  4.099615097045898 0.06898868083953857
CurrentTrain: epoch  6, batch    42 | loss: 4.1686039Losses:  3.955919027328491 0.09019085764884949
CurrentTrain: epoch  6, batch    43 | loss: 4.0461097Losses:  3.9134697914123535 0.06623336672782898
CurrentTrain: epoch  6, batch    44 | loss: 3.9797032Losses:  3.9515485763549805 0.08817674964666367
CurrentTrain: epoch  6, batch    45 | loss: 4.0397253Losses:  4.021058082580566 0.05329247564077377
CurrentTrain: epoch  6, batch    46 | loss: 4.0743504Losses:  4.026161193847656 0.05970141291618347
CurrentTrain: epoch  6, batch    47 | loss: 4.0858626Losses:  4.069650650024414 0.1017179861664772
CurrentTrain: epoch  6, batch    48 | loss: 4.1713686Losses:  3.9939451217651367 0.057009607553482056
CurrentTrain: epoch  6, batch    49 | loss: 4.0509548Losses:  4.00169038772583 0.08779202401638031
CurrentTrain: epoch  6, batch    50 | loss: 4.0894823Losses:  4.006301403045654 0.10113652050495148
CurrentTrain: epoch  6, batch    51 | loss: 4.1074381Losses:  5.348268985748291 0.2797488570213318
CurrentTrain: epoch  6, batch    52 | loss: 5.6280179Losses:  3.9697346687316895 0.04693906009197235
CurrentTrain: epoch  6, batch    53 | loss: 4.0166736Losses:  4.019896507263184 0.07239541411399841
CurrentTrain: epoch  6, batch    54 | loss: 4.0922918Losses:  3.943006992340088 0.04061776027083397
CurrentTrain: epoch  6, batch    55 | loss: 3.9836247Losses:  4.308779716491699 0.1204896792769432
CurrentTrain: epoch  6, batch    56 | loss: 4.4292693Losses:  3.9533917903900146 0.07632338255643845
CurrentTrain: epoch  6, batch    57 | loss: 4.0297151Losses:  4.402097702026367 0.0817319005727768
CurrentTrain: epoch  6, batch    58 | loss: 4.4838295Losses:  3.8764467239379883 0.029957640916109085
CurrentTrain: epoch  6, batch    59 | loss: 3.9064043Losses:  3.958405017852783 0.0842231810092926
CurrentTrain: epoch  6, batch    60 | loss: 4.0426283Losses:  3.9854238033294678 0.08910706639289856
CurrentTrain: epoch  6, batch    61 | loss: 4.0745311Losses:  3.9799678325653076 0.04770046845078468
CurrentTrain: epoch  6, batch    62 | loss: 4.0276685Losses:  3.933712959289551 0.0460260771214962
CurrentTrain: epoch  6, batch    63 | loss: 3.9797390Losses:  4.085972785949707 0.058622535318136215
CurrentTrain: epoch  6, batch    64 | loss: 4.1445951Losses:  4.152948379516602 0.07807236164808273
CurrentTrain: epoch  6, batch    65 | loss: 4.2310209Losses:  4.171023368835449 0.1108655035495758
CurrentTrain: epoch  6, batch    66 | loss: 4.2818890Losses:  3.9087657928466797 0.029884353280067444
CurrentTrain: epoch  6, batch    67 | loss: 3.9386501Losses:  3.903916597366333 0.03948736935853958
CurrentTrain: epoch  6, batch    68 | loss: 3.9434040Losses:  3.970167636871338 0.08125723898410797
CurrentTrain: epoch  6, batch    69 | loss: 4.0514250Losses:  4.065178871154785 0.07338232547044754
CurrentTrain: epoch  6, batch    70 | loss: 4.1385612Losses:  3.9874157905578613 0.07676762342453003
CurrentTrain: epoch  6, batch    71 | loss: 4.0641832Losses:  3.970952033996582 0.05507560074329376
CurrentTrain: epoch  6, batch    72 | loss: 4.0260277Losses:  3.947631359100342 0.028759757056832314
CurrentTrain: epoch  6, batch    73 | loss: 3.9763911Losses:  3.970046281814575 0.07648804783821106
CurrentTrain: epoch  6, batch    74 | loss: 4.0465345Losses:  3.972872734069824 0.07916565984487534
CurrentTrain: epoch  6, batch    75 | loss: 4.0520382Losses:  4.0393548011779785 0.10850873589515686
CurrentTrain: epoch  6, batch    76 | loss: 4.1478634Losses:  3.946843385696411 0.10479351878166199
CurrentTrain: epoch  6, batch    77 | loss: 4.0516367Losses:  4.029691219329834 0.04814997315406799
CurrentTrain: epoch  6, batch    78 | loss: 4.0778413Losses:  3.9810433387756348 0.052317939698696136
CurrentTrain: epoch  6, batch    79 | loss: 4.0333614Losses:  3.9316093921661377 0.09656748175621033
CurrentTrain: epoch  6, batch    80 | loss: 4.0281768Losses:  3.9504599571228027 0.05536596104502678
CurrentTrain: epoch  6, batch    81 | loss: 4.0058260Losses:  3.8677351474761963 0.04009383171796799
CurrentTrain: epoch  6, batch    82 | loss: 3.9078290Losses:  4.081793785095215 0.07283677160739899
CurrentTrain: epoch  6, batch    83 | loss: 4.1546307Losses:  4.13580846786499 0.060483820736408234
CurrentTrain: epoch  6, batch    84 | loss: 4.1962924Losses:  3.9652953147888184 0.07832295447587967
CurrentTrain: epoch  6, batch    85 | loss: 4.0436182Losses:  3.8859498500823975 0.05810316652059555
CurrentTrain: epoch  6, batch    86 | loss: 3.9440529Losses:  3.9334335327148438 0.036446306854486465
CurrentTrain: epoch  6, batch    87 | loss: 3.9698799Losses:  3.94271183013916 0.05662481486797333
CurrentTrain: epoch  6, batch    88 | loss: 3.9993367Losses:  3.969412326812744 0.09144074469804764
CurrentTrain: epoch  6, batch    89 | loss: 4.0608530Losses:  3.9256978034973145 0.07147859781980515
CurrentTrain: epoch  6, batch    90 | loss: 3.9971764Losses:  4.0531182289123535 0.05002130568027496
CurrentTrain: epoch  6, batch    91 | loss: 4.1031394Losses:  4.043725967407227 0.10676484555006027
CurrentTrain: epoch  6, batch    92 | loss: 4.1504908Losses:  3.912787437438965 0.09245032072067261
CurrentTrain: epoch  6, batch    93 | loss: 4.0052376Losses:  3.9777281284332275 0.06937500834465027
CurrentTrain: epoch  6, batch    94 | loss: 4.0471029Losses:  3.9495277404785156 0.06675848364830017
CurrentTrain: epoch  6, batch    95 | loss: 4.0162864Losses:  3.995807409286499 0.05190889537334442
CurrentTrain: epoch  6, batch    96 | loss: 4.0477161Losses:  3.981583595275879 0.09522637724876404
CurrentTrain: epoch  6, batch    97 | loss: 4.0768099Losses:  3.9887142181396484 0.07667869329452515
CurrentTrain: epoch  6, batch    98 | loss: 4.0653930Losses:  4.099757671356201 0.08541962504386902
CurrentTrain: epoch  6, batch    99 | loss: 4.1851773Losses:  4.011587142944336 0.0926322415471077
CurrentTrain: epoch  6, batch   100 | loss: 4.1042194Losses:  4.025452613830566 0.09077107906341553
CurrentTrain: epoch  6, batch   101 | loss: 4.1162238Losses:  4.036174774169922 0.03773334622383118
CurrentTrain: epoch  6, batch   102 | loss: 4.0739083Losses:  3.9159443378448486 0.04300428926944733
CurrentTrain: epoch  6, batch   103 | loss: 3.9589486Losses:  3.923466920852661 0.07620380818843842
CurrentTrain: epoch  6, batch   104 | loss: 3.9996707Losses:  4.035416126251221 0.03847083821892738
CurrentTrain: epoch  6, batch   105 | loss: 4.0738869Losses:  4.056002616882324 0.10255402326583862
CurrentTrain: epoch  6, batch   106 | loss: 4.1585565Losses:  4.011010646820068 0.07132197916507721
CurrentTrain: epoch  6, batch   107 | loss: 4.0823326Losses:  3.9894399642944336 0.02936159260571003
CurrentTrain: epoch  6, batch   108 | loss: 4.0188017Losses:  4.018826484680176 0.03629794344305992
CurrentTrain: epoch  6, batch   109 | loss: 4.0551243Losses:  3.922428846359253 0.04150202125310898
CurrentTrain: epoch  6, batch   110 | loss: 3.9639308Losses:  3.9761791229248047 0.05063052475452423
CurrentTrain: epoch  6, batch   111 | loss: 4.0268097Losses:  3.9548418521881104 0.03971337154507637
CurrentTrain: epoch  6, batch   112 | loss: 3.9945552Losses:  4.013271808624268 0.05750099569559097
CurrentTrain: epoch  6, batch   113 | loss: 4.0707726Losses:  3.950430154800415 0.06787899136543274
CurrentTrain: epoch  6, batch   114 | loss: 4.0183091Losses:  3.931874990463257 0.10688702762126923
CurrentTrain: epoch  6, batch   115 | loss: 4.0387621Losses:  4.01296854019165 0.08259102702140808
CurrentTrain: epoch  6, batch   116 | loss: 4.0955596Losses:  3.9819788932800293 0.06617365777492523
CurrentTrain: epoch  6, batch   117 | loss: 4.0481524Losses:  3.9470224380493164 0.04629101976752281
CurrentTrain: epoch  6, batch   118 | loss: 3.9933136Losses:  4.060399532318115 0.03226850554347038
CurrentTrain: epoch  6, batch   119 | loss: 4.0926681Losses:  4.058455944061279 0.08177022635936737
CurrentTrain: epoch  6, batch   120 | loss: 4.1402264Losses:  3.9727883338928223 0.04263525828719139
CurrentTrain: epoch  6, batch   121 | loss: 4.0154238Losses:  4.044666290283203 0.051057375967502594
CurrentTrain: epoch  6, batch   122 | loss: 4.0957236Losses:  3.927760124206543 0.04814072325825691
CurrentTrain: epoch  6, batch   123 | loss: 3.9759009Losses:  4.004555702209473 0.05298907309770584
CurrentTrain: epoch  6, batch   124 | loss: 4.0575447Losses:  3.9640965461730957 0.06124882027506828
CurrentTrain: epoch  7, batch     0 | loss: 4.0253453Losses:  3.945065975189209 0.06048291176557541
CurrentTrain: epoch  7, batch     1 | loss: 4.0055490Losses:  3.935558319091797 0.0521901473402977
CurrentTrain: epoch  7, batch     2 | loss: 3.9877484Losses:  4.021376609802246 0.038401663303375244
CurrentTrain: epoch  7, batch     3 | loss: 4.0597782Losses:  3.9602904319763184 0.038757652044296265
CurrentTrain: epoch  7, batch     4 | loss: 3.9990480Losses:  3.8874759674072266 0.030563609674572945
CurrentTrain: epoch  7, batch     5 | loss: 3.9180396Losses:  4.008732795715332 0.04543192684650421
CurrentTrain: epoch  7, batch     6 | loss: 4.0541649Losses:  3.8862948417663574 0.03622825816273689
CurrentTrain: epoch  7, batch     7 | loss: 3.9225230Losses:  3.9391841888427734 0.07384169846773148
CurrentTrain: epoch  7, batch     8 | loss: 4.0130258Losses:  4.056329727172852 0.09132417291402817
CurrentTrain: epoch  7, batch     9 | loss: 4.1476541Losses:  3.9512109756469727 0.019308332353830338
CurrentTrain: epoch  7, batch    10 | loss: 3.9705193Losses:  3.9734153747558594 0.03899025171995163
CurrentTrain: epoch  7, batch    11 | loss: 4.0124054Losses:  3.964099884033203 0.08148710429668427
CurrentTrain: epoch  7, batch    12 | loss: 4.0455871Losses:  3.91648268699646 0.07304030656814575
CurrentTrain: epoch  7, batch    13 | loss: 3.9895229Losses:  4.02207088470459 0.055826738476753235
CurrentTrain: epoch  7, batch    14 | loss: 4.0778975Losses:  3.9760894775390625 0.026657119393348694
CurrentTrain: epoch  7, batch    15 | loss: 4.0027466Losses:  4.048631191253662 0.05083610862493515
CurrentTrain: epoch  7, batch    16 | loss: 4.0994673Losses:  3.938429832458496 0.03876456245779991
CurrentTrain: epoch  7, batch    17 | loss: 3.9771943Losses:  4.057826995849609 0.04554037004709244
CurrentTrain: epoch  7, batch    18 | loss: 4.1033673Losses:  3.959575653076172 0.08757553994655609
CurrentTrain: epoch  7, batch    19 | loss: 4.0471511Losses:  3.944422721862793 0.09233680367469788
CurrentTrain: epoch  7, batch    20 | loss: 4.0367594Losses:  3.9691319465637207 0.0778341069817543
CurrentTrain: epoch  7, batch    21 | loss: 4.0469661Losses:  3.9706008434295654 0.04598045349121094
CurrentTrain: epoch  7, batch    22 | loss: 4.0165815Losses:  3.9152750968933105 0.07810284942388535
CurrentTrain: epoch  7, batch    23 | loss: 3.9933779Losses:  3.913351535797119 0.04166277498006821
CurrentTrain: epoch  7, batch    24 | loss: 3.9550142Losses:  4.014975547790527 0.052900828421115875
CurrentTrain: epoch  7, batch    25 | loss: 4.0678763Losses:  3.956833600997925 0.019280292093753815
CurrentTrain: epoch  7, batch    26 | loss: 3.9761138Losses:  3.9686174392700195 0.0695420652627945
CurrentTrain: epoch  7, batch    27 | loss: 4.0381594Losses:  4.063518524169922 0.08196163177490234
CurrentTrain: epoch  7, batch    28 | loss: 4.1454802Losses:  3.9273552894592285 0.04819631576538086
CurrentTrain: epoch  7, batch    29 | loss: 3.9755516Losses:  4.000240325927734 0.06126182898879051
CurrentTrain: epoch  7, batch    30 | loss: 4.0615020Losses:  3.8916893005371094 0.024545207619667053
CurrentTrain: epoch  7, batch    31 | loss: 3.9162345Losses:  3.994349479675293 0.08868058025836945
CurrentTrain: epoch  7, batch    32 | loss: 4.0830302Losses:  4.00800085067749 0.10045450180768967
CurrentTrain: epoch  7, batch    33 | loss: 4.1084552Losses:  3.9571704864501953 0.03387344628572464
CurrentTrain: epoch  7, batch    34 | loss: 3.9910440Losses:  3.994687080383301 0.042361609637737274
CurrentTrain: epoch  7, batch    35 | loss: 4.0370488Losses:  3.999413013458252 0.08556631952524185
CurrentTrain: epoch  7, batch    36 | loss: 4.0849795Losses:  3.936687707901001 0.04710431396961212
CurrentTrain: epoch  7, batch    37 | loss: 3.9837921Losses:  3.972412586212158 0.0658835843205452
CurrentTrain: epoch  7, batch    38 | loss: 4.0382962Losses:  3.9817752838134766 0.05304078757762909
CurrentTrain: epoch  7, batch    39 | loss: 4.0348163Losses:  3.9638900756835938 0.047079239040613174
CurrentTrain: epoch  7, batch    40 | loss: 4.0109692Losses:  4.002427577972412 0.06896807253360748
CurrentTrain: epoch  7, batch    41 | loss: 4.0713959Losses:  3.976069450378418 0.05346670746803284
CurrentTrain: epoch  7, batch    42 | loss: 4.0295362Losses:  3.947700023651123 0.03196446970105171
CurrentTrain: epoch  7, batch    43 | loss: 3.9796646Losses:  3.9792680740356445 0.05583282560110092
CurrentTrain: epoch  7, batch    44 | loss: 4.0351009Losses:  3.9386215209960938 0.036336466670036316
CurrentTrain: epoch  7, batch    45 | loss: 3.9749579Losses:  4.085521221160889 0.04746771976351738
CurrentTrain: epoch  7, batch    46 | loss: 4.1329889Losses:  3.9453344345092773 0.06657849997282028
CurrentTrain: epoch  7, batch    47 | loss: 4.0119128Losses:  3.987717628479004 0.025400329381227493
CurrentTrain: epoch  7, batch    48 | loss: 4.0131178Losses:  3.922178268432617 0.055294789373874664
CurrentTrain: epoch  7, batch    49 | loss: 3.9774730Losses:  3.906595230102539 0.06955553591251373
CurrentTrain: epoch  7, batch    50 | loss: 3.9761508Losses:  3.963470458984375 0.09703949838876724
CurrentTrain: epoch  7, batch    51 | loss: 4.0605102Losses:  3.9421005249023438 0.013857864774763584
CurrentTrain: epoch  7, batch    52 | loss: 3.9559584Losses:  3.903211832046509 0.023832041770219803
CurrentTrain: epoch  7, batch    53 | loss: 3.9270439Losses:  3.9607291221618652 0.057090841233730316
CurrentTrain: epoch  7, batch    54 | loss: 4.0178199Losses:  4.111299991607666 0.026605425402522087
CurrentTrain: epoch  7, batch    55 | loss: 4.1379056Losses:  3.9412264823913574 0.05914066731929779
CurrentTrain: epoch  7, batch    56 | loss: 4.0003672Losses:  3.989417791366577 0.06047926843166351
CurrentTrain: epoch  7, batch    57 | loss: 4.0498972Losses:  3.998577117919922 0.10072687268257141
CurrentTrain: epoch  7, batch    58 | loss: 4.0993042Losses:  4.0312724113464355 0.06699677556753159
CurrentTrain: epoch  7, batch    59 | loss: 4.0982690Losses:  3.9690396785736084 0.03417326509952545
CurrentTrain: epoch  7, batch    60 | loss: 4.0032129Losses:  4.051169395446777 0.0767010822892189
CurrentTrain: epoch  7, batch    61 | loss: 4.1278706Losses:  3.9537405967712402 0.059140823781490326
CurrentTrain: epoch  7, batch    62 | loss: 4.0128813Losses:  3.9611759185791016 0.02645929902791977
CurrentTrain: epoch  7, batch    63 | loss: 3.9876351Losses:  3.9612786769866943 0.0803666040301323
CurrentTrain: epoch  7, batch    64 | loss: 4.0416451Losses:  3.9699549674987793 0.06653586775064468
CurrentTrain: epoch  7, batch    65 | loss: 4.0364909Losses:  3.8971920013427734 0.04333516210317612
CurrentTrain: epoch  7, batch    66 | loss: 3.9405272Losses:  3.9894447326660156 0.06883761286735535
CurrentTrain: epoch  7, batch    67 | loss: 4.0582824Losses:  3.977564811706543 0.03746659681200981
CurrentTrain: epoch  7, batch    68 | loss: 4.0150313Losses:  3.9512569904327393 0.050998754799366
CurrentTrain: epoch  7, batch    69 | loss: 4.0022559Losses:  3.94183349609375 0.0522988885641098
CurrentTrain: epoch  7, batch    70 | loss: 3.9941323Losses:  4.111067295074463 0.056190475821495056
CurrentTrain: epoch  7, batch    71 | loss: 4.1672578Losses:  3.9903650283813477 0.04091339558362961
CurrentTrain: epoch  7, batch    72 | loss: 4.0312786Losses:  3.9836719036102295 0.05288579314947128
CurrentTrain: epoch  7, batch    73 | loss: 4.0365577Losses:  3.8945584297180176 0.0568191334605217
CurrentTrain: epoch  7, batch    74 | loss: 3.9513776Losses:  3.9733288288116455 0.06816747784614563
CurrentTrain: epoch  7, batch    75 | loss: 4.0414963Losses:  3.994912624359131 0.049942195415496826
CurrentTrain: epoch  7, batch    76 | loss: 4.0448546Losses:  3.9755783081054688 0.04390719532966614
CurrentTrain: epoch  7, batch    77 | loss: 4.0194855Losses:  3.929779529571533 0.07357452809810638
CurrentTrain: epoch  7, batch    78 | loss: 4.0033541Losses:  3.9593353271484375 0.06441468000411987
CurrentTrain: epoch  7, batch    79 | loss: 4.0237498Losses:  3.9436283111572266 0.031922608613967896
CurrentTrain: epoch  7, batch    80 | loss: 3.9755509Losses:  3.999497890472412 0.11223936080932617
CurrentTrain: epoch  7, batch    81 | loss: 4.1117373Losses:  3.916301727294922 0.0522371307015419
CurrentTrain: epoch  7, batch    82 | loss: 3.9685388Losses:  3.893211603164673 0.043547920882701874
CurrentTrain: epoch  7, batch    83 | loss: 3.9367595Losses:  3.951998472213745 0.07271389663219452
CurrentTrain: epoch  7, batch    84 | loss: 4.0247126Losses:  4.008521556854248 0.09990331530570984
CurrentTrain: epoch  7, batch    85 | loss: 4.1084247Losses:  3.9218332767486572 0.06701885163784027
CurrentTrain: epoch  7, batch    86 | loss: 3.9888520Losses:  3.9360334873199463 0.04252096638083458
CurrentTrain: epoch  7, batch    87 | loss: 3.9785545Losses:  3.9592442512512207 0.03310048207640648
CurrentTrain: epoch  7, batch    88 | loss: 3.9923446Losses:  3.9901394844055176 0.060500651597976685
CurrentTrain: epoch  7, batch    89 | loss: 4.0506401Losses:  3.9321062564849854 0.03352363780140877
CurrentTrain: epoch  7, batch    90 | loss: 3.9656298Losses:  4.026333808898926 0.0657241940498352
CurrentTrain: epoch  7, batch    91 | loss: 4.0920582Losses:  4.004727363586426 0.08388378471136093
CurrentTrain: epoch  7, batch    92 | loss: 4.0886111Losses:  3.958456039428711 0.02833586186170578
CurrentTrain: epoch  7, batch    93 | loss: 3.9867918Losses:  3.892261028289795 0.04847690463066101
CurrentTrain: epoch  7, batch    94 | loss: 3.9407380Losses:  4.007619857788086 0.03825129196047783
CurrentTrain: epoch  7, batch    95 | loss: 4.0458713Losses:  3.925293207168579 0.06553930789232254
CurrentTrain: epoch  7, batch    96 | loss: 3.9908326Losses:  3.9617972373962402 0.06261057406663895
CurrentTrain: epoch  7, batch    97 | loss: 4.0244079Losses:  3.930469036102295 0.07252383977174759
CurrentTrain: epoch  7, batch    98 | loss: 4.0029931Losses:  3.917076587677002 0.07570163905620575
CurrentTrain: epoch  7, batch    99 | loss: 3.9927783Losses:  3.9464354515075684 0.06151070073246956
CurrentTrain: epoch  7, batch   100 | loss: 4.0079460Losses:  3.94889760017395 0.02448364906013012
CurrentTrain: epoch  7, batch   101 | loss: 3.9733813Losses:  3.987083911895752 0.0732758492231369
CurrentTrain: epoch  7, batch   102 | loss: 4.0603600Losses:  3.943758010864258 0.06844139099121094
CurrentTrain: epoch  7, batch   103 | loss: 4.0121994Losses:  3.9736077785491943 0.052861399948596954
CurrentTrain: epoch  7, batch   104 | loss: 4.0264692Losses:  4.048656463623047 0.02898191288113594
CurrentTrain: epoch  7, batch   105 | loss: 4.0776381Losses:  3.9572954177856445 0.0437031090259552
CurrentTrain: epoch  7, batch   106 | loss: 4.0009985Losses:  3.9564003944396973 0.055364370346069336
CurrentTrain: epoch  7, batch   107 | loss: 4.0117645Losses:  3.944315195083618 0.09537944942712784
CurrentTrain: epoch  7, batch   108 | loss: 4.0396948Losses:  3.9855518341064453 0.061840057373046875
CurrentTrain: epoch  7, batch   109 | loss: 4.0473919Losses:  3.934131622314453 0.05383412539958954
CurrentTrain: epoch  7, batch   110 | loss: 3.9879658Losses:  3.9915671348571777 0.08783338218927383
CurrentTrain: epoch  7, batch   111 | loss: 4.0794005Losses:  3.9391183853149414 0.04283162206411362
CurrentTrain: epoch  7, batch   112 | loss: 3.9819500Losses:  3.954066753387451 0.04361988604068756
CurrentTrain: epoch  7, batch   113 | loss: 3.9976866Losses:  3.9737389087677 0.09997626394033432
CurrentTrain: epoch  7, batch   114 | loss: 4.0737152Losses:  3.958587646484375 0.06895729899406433
CurrentTrain: epoch  7, batch   115 | loss: 4.0275450Losses:  3.9771509170532227 0.08450084924697876
CurrentTrain: epoch  7, batch   116 | loss: 4.0616517Losses:  3.9811248779296875 0.06873460859060287
CurrentTrain: epoch  7, batch   117 | loss: 4.0498595Losses:  3.911165475845337 0.04293446987867355
CurrentTrain: epoch  7, batch   118 | loss: 3.9540999Losses:  4.001254081726074 0.04748699814081192
CurrentTrain: epoch  7, batch   119 | loss: 4.0487409Losses:  3.9709277153015137 0.07071106880903244
CurrentTrain: epoch  7, batch   120 | loss: 4.0416389Losses:  3.892256259918213 0.07347938418388367
CurrentTrain: epoch  7, batch   121 | loss: 3.9657357Losses:  3.9647583961486816 0.03309732675552368
CurrentTrain: epoch  7, batch   122 | loss: 3.9978557Losses:  3.955606698989868 0.0716693252325058
CurrentTrain: epoch  7, batch   123 | loss: 4.0272760Losses:  3.9641988277435303 0.050303082913160324
CurrentTrain: epoch  7, batch   124 | loss: 4.0145020Losses:  3.977450370788574 0.07593492418527603
CurrentTrain: epoch  8, batch     0 | loss: 4.0533853Losses:  3.9665415287017822 0.06543462723493576
CurrentTrain: epoch  8, batch     1 | loss: 4.0319762Losses:  3.9318885803222656 0.09255094081163406
CurrentTrain: epoch  8, batch     2 | loss: 4.0244393Losses:  3.936324119567871 0.035749707370996475
CurrentTrain: epoch  8, batch     3 | loss: 3.9720738Losses:  3.9824604988098145 0.07110017538070679
CurrentTrain: epoch  8, batch     4 | loss: 4.0535607Losses:  3.969176769256592 0.0669110044836998
CurrentTrain: epoch  8, batch     5 | loss: 4.0360880Losses:  3.9634993076324463 0.06947116553783417
CurrentTrain: epoch  8, batch     6 | loss: 4.0329704Losses:  4.030777454376221 0.08433471620082855
CurrentTrain: epoch  8, batch     7 | loss: 4.1151123Losses:  3.9736273288726807 0.02327748015522957
CurrentTrain: epoch  8, batch     8 | loss: 3.9969049Losses:  3.961536407470703 0.064959816634655
CurrentTrain: epoch  8, batch     9 | loss: 4.0264964Losses:  3.914050817489624 0.09762822091579437
CurrentTrain: epoch  8, batch    10 | loss: 4.0116792Losses:  3.897874355316162 0.03863024339079857
CurrentTrain: epoch  8, batch    11 | loss: 3.9365046Losses:  3.987104892730713 0.022998768836259842
CurrentTrain: epoch  8, batch    12 | loss: 4.0101037Losses:  3.9681942462921143 0.06150791794061661
CurrentTrain: epoch  8, batch    13 | loss: 4.0297022Losses:  3.9343106746673584 0.04608539491891861
CurrentTrain: epoch  8, batch    14 | loss: 3.9803960Losses:  3.9279117584228516 0.06284120678901672
CurrentTrain: epoch  8, batch    15 | loss: 3.9907529Losses:  3.9403278827667236 0.09280143678188324
CurrentTrain: epoch  8, batch    16 | loss: 4.0331292Losses:  3.8872883319854736 0.07125933468341827
CurrentTrain: epoch  8, batch    17 | loss: 3.9585476Losses:  3.941258430480957 0.07771659642457962
CurrentTrain: epoch  8, batch    18 | loss: 4.0189753Losses:  3.9730162620544434 0.03633718937635422
CurrentTrain: epoch  8, batch    19 | loss: 4.0093536Losses:  3.919266939163208 0.05836871266365051
CurrentTrain: epoch  8, batch    20 | loss: 3.9776356Losses:  3.9370431900024414 0.04524247720837593
CurrentTrain: epoch  8, batch    21 | loss: 3.9822857Losses:  3.9395241737365723 0.03698005527257919
CurrentTrain: epoch  8, batch    22 | loss: 3.9765043Losses:  3.9410548210144043 0.10199427604675293
CurrentTrain: epoch  8, batch    23 | loss: 4.0430489Losses:  3.90744686126709 0.03588955104351044
CurrentTrain: epoch  8, batch    24 | loss: 3.9433365Losses:  3.987043857574463 0.02660278230905533
CurrentTrain: epoch  8, batch    25 | loss: 4.0136466Losses:  3.882513999938965 0.06737081706523895
CurrentTrain: epoch  8, batch    26 | loss: 3.9498849Losses:  3.9521584510803223 0.059595659375190735
CurrentTrain: epoch  8, batch    27 | loss: 4.0117540Losses:  3.972963333129883 0.02707008272409439
CurrentTrain: epoch  8, batch    28 | loss: 4.0000334Losses:  3.9790804386138916 0.07932648062705994
CurrentTrain: epoch  8, batch    29 | loss: 4.0584068Losses:  3.8778316974639893 0.019440077245235443
CurrentTrain: epoch  8, batch    30 | loss: 3.8972719Losses:  4.002065658569336 0.06414611637592316
CurrentTrain: epoch  8, batch    31 | loss: 4.0662117Losses:  3.9556610584259033 0.058118999004364014
CurrentTrain: epoch  8, batch    32 | loss: 4.0137801Losses:  3.980987787246704 0.04197758436203003
CurrentTrain: epoch  8, batch    33 | loss: 4.0229654Losses:  3.928847074508667 0.02381942979991436
CurrentTrain: epoch  8, batch    34 | loss: 3.9526665Losses:  3.8835625648498535 0.04278026893734932
CurrentTrain: epoch  8, batch    35 | loss: 3.9263427Losses:  3.879424571990967 0.016805680468678474
CurrentTrain: epoch  8, batch    36 | loss: 3.8962302Losses:  3.906893253326416 0.05233093351125717
CurrentTrain: epoch  8, batch    37 | loss: 3.9592242Losses:  3.9178590774536133 0.05707261711359024
CurrentTrain: epoch  8, batch    38 | loss: 3.9749317Losses:  3.9729080200195312 0.0652044415473938
CurrentTrain: epoch  8, batch    39 | loss: 4.0381126Losses:  3.99582839012146 0.06996284425258636
CurrentTrain: epoch  8, batch    40 | loss: 4.0657911Losses:  3.956455945968628 0.05156522989273071
CurrentTrain: epoch  8, batch    41 | loss: 4.0080214Losses:  3.9309065341949463 0.023813487961888313
CurrentTrain: epoch  8, batch    42 | loss: 3.9547200Losses:  3.9710988998413086 0.052989255636930466
CurrentTrain: epoch  8, batch    43 | loss: 4.0240884Losses:  3.919356346130371 0.04278118908405304
CurrentTrain: epoch  8, batch    44 | loss: 3.9621375Losses:  3.9095826148986816 0.0335351824760437
CurrentTrain: epoch  8, batch    45 | loss: 3.9431179Losses:  3.9113194942474365 0.05741438269615173
CurrentTrain: epoch  8, batch    46 | loss: 3.9687338Losses:  3.9321305751800537 0.0792936235666275
CurrentTrain: epoch  8, batch    47 | loss: 4.0114241Losses:  3.933210849761963 0.04689765349030495
CurrentTrain: epoch  8, batch    48 | loss: 3.9801085Losses:  3.9397594928741455 0.04131694883108139
CurrentTrain: epoch  8, batch    49 | loss: 3.9810765Losses:  3.9298181533813477 0.0576167032122612
CurrentTrain: epoch  8, batch    50 | loss: 3.9874349Losses:  3.9449880123138428 0.04200541600584984
CurrentTrain: epoch  8, batch    51 | loss: 3.9869933Losses:  3.9358935356140137 0.06472955644130707
CurrentTrain: epoch  8, batch    52 | loss: 4.0006232Losses:  3.887533187866211 0.054273247718811035
CurrentTrain: epoch  8, batch    53 | loss: 3.9418063Losses:  3.9257752895355225 0.08255085349082947
CurrentTrain: epoch  8, batch    54 | loss: 4.0083261Losses:  3.947436809539795 0.054342303425073624
CurrentTrain: epoch  8, batch    55 | loss: 4.0017791Losses:  3.9161179065704346 0.021069709211587906
CurrentTrain: epoch  8, batch    56 | loss: 3.9371877Losses:  3.9541375637054443 0.07215116173028946
CurrentTrain: epoch  8, batch    57 | loss: 4.0262885Losses:  3.9219858646392822 0.0826343223452568
CurrentTrain: epoch  8, batch    58 | loss: 4.0046201Losses:  3.915452480316162 0.062150582671165466
CurrentTrain: epoch  8, batch    59 | loss: 3.9776030Losses:  3.9577085971832275 0.05271453782916069
CurrentTrain: epoch  8, batch    60 | loss: 4.0104232Losses:  3.9242730140686035 0.05685223266482353
CurrentTrain: epoch  8, batch    61 | loss: 3.9811254Losses:  3.9412055015563965 0.04673069715499878
CurrentTrain: epoch  8, batch    62 | loss: 3.9879363Losses:  3.950662136077881 0.06834886968135834
CurrentTrain: epoch  8, batch    63 | loss: 4.0190110Losses:  3.914472818374634 0.06906330585479736
CurrentTrain: epoch  8, batch    64 | loss: 3.9835362Losses:  3.9215571880340576 0.06509625911712646
CurrentTrain: epoch  8, batch    65 | loss: 3.9866533Losses:  3.919095039367676 0.05448712036013603
CurrentTrain: epoch  8, batch    66 | loss: 3.9735823Losses:  3.9129536151885986 0.05272514373064041
CurrentTrain: epoch  8, batch    67 | loss: 3.9656787Losses:  3.934727430343628 0.08834625780582428
CurrentTrain: epoch  8, batch    68 | loss: 4.0230737Losses:  3.921079158782959 0.05447380989789963
CurrentTrain: epoch  8, batch    69 | loss: 3.9755530Losses:  3.9040045738220215 0.06335410475730896
CurrentTrain: epoch  8, batch    70 | loss: 3.9673586Losses:  3.9731850624084473 0.02788383699953556
CurrentTrain: epoch  8, batch    71 | loss: 4.0010691Losses:  3.8931047916412354 0.03780783340334892
CurrentTrain: epoch  8, batch    72 | loss: 3.9309127Losses:  3.932054042816162 0.018007928505539894
CurrentTrain: epoch  8, batch    73 | loss: 3.9500620Losses:  3.9487619400024414 0.040734704583883286
CurrentTrain: epoch  8, batch    74 | loss: 3.9894967Losses:  3.9295401573181152 0.06758099794387817
CurrentTrain: epoch  8, batch    75 | loss: 3.9971211Losses:  3.896066188812256 0.07718902081251144
CurrentTrain: epoch  8, batch    76 | loss: 3.9732552Losses:  3.979337453842163 0.020051702857017517
CurrentTrain: epoch  8, batch    77 | loss: 3.9993892Losses:  3.924294948577881 0.0184418186545372
CurrentTrain: epoch  8, batch    78 | loss: 3.9427369Losses:  3.9148449897766113 0.053299665451049805
CurrentTrain: epoch  8, batch    79 | loss: 3.9681447Losses:  3.9325013160705566 0.06360366195440292
CurrentTrain: epoch  8, batch    80 | loss: 3.9961050Losses:  3.916616201400757 0.04861992225050926
CurrentTrain: epoch  8, batch    81 | loss: 3.9652362Losses:  3.95957612991333 0.030587591230869293
CurrentTrain: epoch  8, batch    82 | loss: 3.9901638Losses:  4.053417205810547 0.05235215276479721
CurrentTrain: epoch  8, batch    83 | loss: 4.1057692Losses:  3.918060779571533 0.022688278928399086
CurrentTrain: epoch  8, batch    84 | loss: 3.9407492Losses:  3.9798240661621094 0.027433324605226517
CurrentTrain: epoch  8, batch    85 | loss: 4.0072575Losses:  3.947162389755249 0.02495744451880455
CurrentTrain: epoch  8, batch    86 | loss: 3.9721198Losses:  3.920163631439209 0.059209391474723816
CurrentTrain: epoch  8, batch    87 | loss: 3.9793730Losses:  3.912541389465332 0.05059698596596718
CurrentTrain: epoch  8, batch    88 | loss: 3.9631383Losses:  3.921290397644043 0.06537530571222305
CurrentTrain: epoch  8, batch    89 | loss: 3.9866657Losses:  3.9424889087677 0.03868592157959938
CurrentTrain: epoch  8, batch    90 | loss: 3.9811749Losses:  3.9695072174072266 0.06065557897090912
CurrentTrain: epoch  8, batch    91 | loss: 4.0301628Losses:  3.93406081199646 0.035624220967292786
CurrentTrain: epoch  8, batch    92 | loss: 3.9696851Losses:  3.9845001697540283 0.04020457714796066
CurrentTrain: epoch  8, batch    93 | loss: 4.0247049Losses:  3.9482507705688477 0.04890560731291771
CurrentTrain: epoch  8, batch    94 | loss: 3.9971564Losses:  3.9885363578796387 0.03753170371055603
CurrentTrain: epoch  8, batch    95 | loss: 4.0260682Losses:  3.944918155670166 0.041190795600414276
CurrentTrain: epoch  8, batch    96 | loss: 3.9861090Losses:  3.9530298709869385 0.06333591043949127
CurrentTrain: epoch  8, batch    97 | loss: 4.0163660Losses:  3.9228413105010986 0.028477124869823456
CurrentTrain: epoch  8, batch    98 | loss: 3.9513185Losses:  3.9133756160736084 0.03397612273693085
CurrentTrain: epoch  8, batch    99 | loss: 3.9473517Losses:  3.9303770065307617 0.03951490670442581
CurrentTrain: epoch  8, batch   100 | loss: 3.9698920Losses:  3.9604926109313965 0.05310560762882233
CurrentTrain: epoch  8, batch   101 | loss: 4.0135984Losses:  3.8868565559387207 0.04204479977488518
CurrentTrain: epoch  8, batch   102 | loss: 3.9289014Losses:  3.9172608852386475 0.02511577680706978
CurrentTrain: epoch  8, batch   103 | loss: 3.9423766Losses:  3.9580283164978027 0.07842612266540527
CurrentTrain: epoch  8, batch   104 | loss: 4.0364542Losses:  3.9320459365844727 0.06340856850147247
CurrentTrain: epoch  8, batch   105 | loss: 3.9954545Losses:  3.9294967651367188 0.053494542837142944
CurrentTrain: epoch  8, batch   106 | loss: 3.9829912Losses:  3.92777419090271 0.017315035685896873
CurrentTrain: epoch  8, batch   107 | loss: 3.9450893Losses:  3.9686660766601562 0.06743764877319336
CurrentTrain: epoch  8, batch   108 | loss: 4.0361037Losses:  3.9526188373565674 0.04133085906505585
CurrentTrain: epoch  8, batch   109 | loss: 3.9939497Losses:  3.877988576889038 0.05638338252902031
CurrentTrain: epoch  8, batch   110 | loss: 3.9343719Losses:  3.8919382095336914 0.050088778138160706
CurrentTrain: epoch  8, batch   111 | loss: 3.9420271Losses:  3.8913216590881348 0.06631895899772644
CurrentTrain: epoch  8, batch   112 | loss: 3.9576406Losses:  3.9405670166015625 0.018940631300210953
CurrentTrain: epoch  8, batch   113 | loss: 3.9595077Losses:  3.91860294342041 0.04515266418457031
CurrentTrain: epoch  8, batch   114 | loss: 3.9637556Losses:  3.9788854122161865 0.05149102956056595
CurrentTrain: epoch  8, batch   115 | loss: 4.0303764Losses:  3.8751773834228516 0.05096437782049179
CurrentTrain: epoch  8, batch   116 | loss: 3.9261417Losses:  3.938387393951416 0.04470273479819298
CurrentTrain: epoch  8, batch   117 | loss: 3.9830902Losses:  3.9790780544281006 0.02516116015613079
CurrentTrain: epoch  8, batch   118 | loss: 4.0042391Losses:  3.9794809818267822 0.03939438611268997
CurrentTrain: epoch  8, batch   119 | loss: 4.0188756Losses:  3.945474147796631 0.04804866388440132
CurrentTrain: epoch  8, batch   120 | loss: 3.9935229Losses:  3.962941884994507 0.057010188698768616
CurrentTrain: epoch  8, batch   121 | loss: 4.0199523Losses:  3.936880111694336 0.057251784950494766
CurrentTrain: epoch  8, batch   122 | loss: 3.9941318Losses:  3.941582202911377 0.06710740178823471
CurrentTrain: epoch  8, batch   123 | loss: 4.0086894Losses:  3.9109692573547363 0.044021666049957275
CurrentTrain: epoch  8, batch   124 | loss: 3.9549909Losses:  3.909785270690918 0.03936908394098282
CurrentTrain: epoch  9, batch     0 | loss: 3.9491544Losses:  3.9476637840270996 0.015959490090608597
CurrentTrain: epoch  9, batch     1 | loss: 3.9636233Losses:  3.9172208309173584 0.04257315397262573
CurrentTrain: epoch  9, batch     2 | loss: 3.9597940Losses:  3.959395408630371 0.08324647694826126
CurrentTrain: epoch  9, batch     3 | loss: 4.0426421Losses:  3.9990077018737793 0.05880028381943703
CurrentTrain: epoch  9, batch     4 | loss: 4.0578079Losses:  3.9917290210723877 0.04963422566652298
CurrentTrain: epoch  9, batch     5 | loss: 4.0413632Losses:  3.933396816253662 0.04936246573925018
CurrentTrain: epoch  9, batch     6 | loss: 3.9827592Losses:  3.990461587905884 0.03304630517959595
CurrentTrain: epoch  9, batch     7 | loss: 4.0235081Losses:  3.912147283554077 0.04238785430788994
CurrentTrain: epoch  9, batch     8 | loss: 3.9545352Losses:  3.992353677749634 0.03468812629580498
CurrentTrain: epoch  9, batch     9 | loss: 4.0270419Losses:  3.9053964614868164 0.03950688987970352
CurrentTrain: epoch  9, batch    10 | loss: 3.9449034Losses:  3.961397647857666 0.0413481630384922
CurrentTrain: epoch  9, batch    11 | loss: 4.0027456Losses:  3.873159885406494 0.030947726219892502
CurrentTrain: epoch  9, batch    12 | loss: 3.9041076Losses:  3.9469547271728516 0.037844959646463394
CurrentTrain: epoch  9, batch    13 | loss: 3.9847996Losses:  3.963529109954834 0.08990710973739624
CurrentTrain: epoch  9, batch    14 | loss: 4.0534363Losses:  3.903031349182129 0.03589409217238426
CurrentTrain: epoch  9, batch    15 | loss: 3.9389255Losses:  3.960794687271118 0.06825882196426392
CurrentTrain: epoch  9, batch    16 | loss: 4.0290537Losses:  3.961977481842041 0.03963130712509155
CurrentTrain: epoch  9, batch    17 | loss: 4.0016088Losses:  3.972355842590332 0.032932303845882416
CurrentTrain: epoch  9, batch    18 | loss: 4.0052881Losses:  3.9318270683288574 0.06311062723398209
CurrentTrain: epoch  9, batch    19 | loss: 3.9949377Losses:  3.983438014984131 0.0529821440577507
CurrentTrain: epoch  9, batch    20 | loss: 4.0364203Losses:  3.968254327774048 0.05448208749294281
CurrentTrain: epoch  9, batch    21 | loss: 4.0227365Losses:  3.9840362071990967 0.03868250176310539
CurrentTrain: epoch  9, batch    22 | loss: 4.0227189Losses:  3.949958324432373 0.05213945358991623
CurrentTrain: epoch  9, batch    23 | loss: 4.0020976Losses:  3.9483065605163574 0.05328547582030296
CurrentTrain: epoch  9, batch    24 | loss: 4.0015922Losses:  3.9311842918395996 0.027251791208982468
CurrentTrain: epoch  9, batch    25 | loss: 3.9584360Losses:  3.8998332023620605 0.05194173380732536
CurrentTrain: epoch  9, batch    26 | loss: 3.9517748Losses:  3.927377223968506 0.06816752254962921
CurrentTrain: epoch  9, batch    27 | loss: 3.9955447Losses:  3.9498963356018066 0.03939149156212807
CurrentTrain: epoch  9, batch    28 | loss: 3.9892879Losses:  3.9147634506225586 0.07587417960166931
CurrentTrain: epoch  9, batch    29 | loss: 3.9906375Losses:  3.9387264251708984 0.035613156855106354
CurrentTrain: epoch  9, batch    30 | loss: 3.9743395Losses:  3.8722009658813477 0.042915232479572296
CurrentTrain: epoch  9, batch    31 | loss: 3.9151163Losses:  3.9487667083740234 0.05015496909618378
CurrentTrain: epoch  9, batch    32 | loss: 3.9989216Losses:  3.9085116386413574 0.08422280848026276
CurrentTrain: epoch  9, batch    33 | loss: 3.9927344Losses:  3.880765438079834 0.04090462625026703
CurrentTrain: epoch  9, batch    34 | loss: 3.9216700Losses:  3.954807758331299 0.050164587795734406
CurrentTrain: epoch  9, batch    35 | loss: 4.0049725Losses:  3.911224842071533 0.03252946957945824
CurrentTrain: epoch  9, batch    36 | loss: 3.9437542Losses:  3.939706325531006 0.01854744926095009
CurrentTrain: epoch  9, batch    37 | loss: 3.9582539Losses:  3.9263343811035156 0.032679688185453415
CurrentTrain: epoch  9, batch    38 | loss: 3.9590142Losses:  3.987884044647217 0.020347222685813904
CurrentTrain: epoch  9, batch    39 | loss: 4.0082312Losses:  3.87471604347229 0.06489292532205582
CurrentTrain: epoch  9, batch    40 | loss: 3.9396091Losses:  3.88559627532959 0.04632376879453659
CurrentTrain: epoch  9, batch    41 | loss: 3.9319201Losses:  3.9123377799987793 0.05723210424184799
CurrentTrain: epoch  9, batch    42 | loss: 3.9695699Losses:  3.9426980018615723 0.04024645686149597
CurrentTrain: epoch  9, batch    43 | loss: 3.9829445Losses:  3.973405361175537 0.02597411721944809
CurrentTrain: epoch  9, batch    44 | loss: 3.9993794Losses:  3.8756837844848633 0.029393792152404785
CurrentTrain: epoch  9, batch    45 | loss: 3.9050775Losses:  3.911703586578369 0.028814975172281265
CurrentTrain: epoch  9, batch    46 | loss: 3.9405186Losses:  3.93719482421875 0.02899245172739029
CurrentTrain: epoch  9, batch    47 | loss: 3.9661872Losses:  3.9126229286193848 0.04020950198173523
CurrentTrain: epoch  9, batch    48 | loss: 3.9528325Losses:  3.9799442291259766 0.03603297844529152
CurrentTrain: epoch  9, batch    49 | loss: 4.0159774Losses:  3.9093565940856934 0.02448788657784462
CurrentTrain: epoch  9, batch    50 | loss: 3.9338446Losses:  3.9167447090148926 0.026856176555156708
CurrentTrain: epoch  9, batch    51 | loss: 3.9436009Losses:  3.9194626808166504 0.03570663928985596
CurrentTrain: epoch  9, batch    52 | loss: 3.9551692Losses:  3.842184066772461 0.014852700755000114
CurrentTrain: epoch  9, batch    53 | loss: 3.8570368Losses:  3.9617981910705566 0.08133012056350708
CurrentTrain: epoch  9, batch    54 | loss: 4.0431285Losses:  3.945436477661133 0.03334823250770569
CurrentTrain: epoch  9, batch    55 | loss: 3.9787848Losses:  3.914905071258545 0.03223676234483719
CurrentTrain: epoch  9, batch    56 | loss: 3.9471419Losses:  3.9333863258361816 0.061903245747089386
CurrentTrain: epoch  9, batch    57 | loss: 3.9952896Losses:  3.9521586894989014 0.03881154954433441
CurrentTrain: epoch  9, batch    58 | loss: 3.9909701Losses:  3.863766670227051 0.05540311336517334
CurrentTrain: epoch  9, batch    59 | loss: 3.9191699Losses:  4.006058216094971 0.04264827072620392
CurrentTrain: epoch  9, batch    60 | loss: 4.0487065Losses:  3.9934802055358887 0.033664148300886154
CurrentTrain: epoch  9, batch    61 | loss: 4.0271444Losses:  3.965562343597412 0.07935212552547455
CurrentTrain: epoch  9, batch    62 | loss: 4.0449142Losses:  3.913447141647339 0.045323073863983154
CurrentTrain: epoch  9, batch    63 | loss: 3.9587703Losses:  3.9283366203308105 0.03016401268541813
CurrentTrain: epoch  9, batch    64 | loss: 3.9585006Losses:  3.92767333984375 0.0278319101780653
CurrentTrain: epoch  9, batch    65 | loss: 3.9555051Losses:  3.954923152923584 0.06965483725070953
CurrentTrain: epoch  9, batch    66 | loss: 4.0245781Losses:  3.913410186767578 0.05539917200803757
CurrentTrain: epoch  9, batch    67 | loss: 3.9688094Losses:  3.9770846366882324 0.02296803519129753
CurrentTrain: epoch  9, batch    68 | loss: 4.0000525Losses:  3.8721108436584473 0.03895185515284538
CurrentTrain: epoch  9, batch    69 | loss: 3.9110627Losses:  3.915590524673462 0.05711963400244713
CurrentTrain: epoch  9, batch    70 | loss: 3.9727101Losses:  3.9351086616516113 0.035482607781887054
CurrentTrain: epoch  9, batch    71 | loss: 3.9705913Losses:  3.966324806213379 0.030227376148104668
CurrentTrain: epoch  9, batch    72 | loss: 3.9965522Losses:  3.8782949447631836 0.017242316156625748
CurrentTrain: epoch  9, batch    73 | loss: 3.8955374Losses:  3.9161016941070557 0.02574027143418789
CurrentTrain: epoch  9, batch    74 | loss: 3.9418421Losses:  3.897195339202881 0.062409088015556335
CurrentTrain: epoch  9, batch    75 | loss: 3.9596045Losses:  3.93031644821167 0.04657478258013725
CurrentTrain: epoch  9, batch    76 | loss: 3.9768913Losses:  3.929990530014038 0.05011213570833206
CurrentTrain: epoch  9, batch    77 | loss: 3.9801028Losses:  3.9175825119018555 0.057516537606716156
CurrentTrain: epoch  9, batch    78 | loss: 3.9750991Losses:  3.923048973083496 0.047277793288230896
CurrentTrain: epoch  9, batch    79 | loss: 3.9703267Losses:  3.945905923843384 0.02874235063791275
CurrentTrain: epoch  9, batch    80 | loss: 3.9746482Losses:  3.915766716003418 0.07144913077354431
CurrentTrain: epoch  9, batch    81 | loss: 3.9872158Losses:  3.8707823753356934 0.06842255592346191
CurrentTrain: epoch  9, batch    82 | loss: 3.9392049Losses:  3.947336196899414 0.04569278657436371
CurrentTrain: epoch  9, batch    83 | loss: 3.9930289Losses:  3.923513889312744 0.0538204088807106
CurrentTrain: epoch  9, batch    84 | loss: 3.9773343Losses:  3.9184036254882812 0.06474754214286804
CurrentTrain: epoch  9, batch    85 | loss: 3.9831512Losses:  3.957434892654419 0.06500569730997086
CurrentTrain: epoch  9, batch    86 | loss: 4.0224404Losses:  3.8879852294921875 0.04526105523109436
CurrentTrain: epoch  9, batch    87 | loss: 3.9332464Losses:  3.947277069091797 0.07009054720401764
CurrentTrain: epoch  9, batch    88 | loss: 4.0173678Losses:  3.9869186878204346 0.03679272532463074
CurrentTrain: epoch  9, batch    89 | loss: 4.0237112Losses:  3.998595714569092 0.037670426070690155
CurrentTrain: epoch  9, batch    90 | loss: 4.0362663Losses:  3.970154047012329 0.06517490744590759
CurrentTrain: epoch  9, batch    91 | loss: 4.0353289Losses:  3.9276344776153564 0.05966412276029587
CurrentTrain: epoch  9, batch    92 | loss: 3.9872985Losses:  3.9467430114746094 0.02169055864214897
CurrentTrain: epoch  9, batch    93 | loss: 3.9684336Losses:  3.91837215423584 0.05826503783464432
CurrentTrain: epoch  9, batch    94 | loss: 3.9766371Losses:  3.922694683074951 0.03565172478556633
CurrentTrain: epoch  9, batch    95 | loss: 3.9583464Losses:  3.9142959117889404 0.05755085125565529
CurrentTrain: epoch  9, batch    96 | loss: 3.9718468Losses:  3.930739402770996 0.033431265503168106
CurrentTrain: epoch  9, batch    97 | loss: 3.9641707Losses:  3.9660658836364746 0.04947803169488907
CurrentTrain: epoch  9, batch    98 | loss: 4.0155439Losses:  3.95269775390625 0.05014856159687042
CurrentTrain: epoch  9, batch    99 | loss: 4.0028462Losses:  3.907414674758911 0.08052662014961243
CurrentTrain: epoch  9, batch   100 | loss: 3.9879413Losses:  3.9161014556884766 0.032465726137161255
CurrentTrain: epoch  9, batch   101 | loss: 3.9485672Losses:  3.8712496757507324 0.04228018596768379
CurrentTrain: epoch  9, batch   102 | loss: 3.9135299Losses:  3.9828011989593506 0.0379815474152565
CurrentTrain: epoch  9, batch   103 | loss: 4.0207829Losses:  3.9506232738494873 0.08247368037700653
CurrentTrain: epoch  9, batch   104 | loss: 4.0330968Losses:  3.8411474227905273 0.031582728028297424
CurrentTrain: epoch  9, batch   105 | loss: 3.8727303Losses:  3.9925127029418945 0.055213674902915955
CurrentTrain: epoch  9, batch   106 | loss: 4.0477262Losses:  3.9930217266082764 0.046718940138816833
CurrentTrain: epoch  9, batch   107 | loss: 4.0397406Losses:  3.932986259460449 0.04175030067563057
CurrentTrain: epoch  9, batch   108 | loss: 3.9747365Losses:  3.885505199432373 0.047749113291502
CurrentTrain: epoch  9, batch   109 | loss: 3.9332542Losses:  3.9400157928466797 0.058987826108932495
CurrentTrain: epoch  9, batch   110 | loss: 3.9990036Losses:  3.946821451187134 0.027303636074066162
CurrentTrain: epoch  9, batch   111 | loss: 3.9741251Losses:  3.875540256500244 0.04562922939658165
CurrentTrain: epoch  9, batch   112 | loss: 3.9211695Losses:  3.9359660148620605 0.028973670676350594
CurrentTrain: epoch  9, batch   113 | loss: 3.9649396Losses:  3.9729981422424316 0.05365234613418579
CurrentTrain: epoch  9, batch   114 | loss: 4.0266504Losses:  3.8918356895446777 0.045092061161994934
CurrentTrain: epoch  9, batch   115 | loss: 3.9369278Losses:  3.9536609649658203 0.034993164241313934
CurrentTrain: epoch  9, batch   116 | loss: 3.9886541Losses:  3.975344181060791 0.051236629486083984
CurrentTrain: epoch  9, batch   117 | loss: 4.0265808Losses:  3.9442834854125977 0.05763234943151474
CurrentTrain: epoch  9, batch   118 | loss: 4.0019159Losses:  3.959503173828125 0.03238051012158394
CurrentTrain: epoch  9, batch   119 | loss: 3.9918838Losses:  3.964531898498535 0.04299945384263992
CurrentTrain: epoch  9, batch   120 | loss: 4.0075312Losses:  3.901096820831299 0.015713151544332504
CurrentTrain: epoch  9, batch   121 | loss: 3.9168100Losses:  3.9370245933532715 0.05718120187520981
CurrentTrain: epoch  9, batch   122 | loss: 3.9942057Losses:  3.906402349472046 0.030149850994348526
CurrentTrain: epoch  9, batch   123 | loss: 3.9365523Losses:  3.99041485786438 0.026076357811689377
CurrentTrain: epoch  9, batch   124 | loss: 4.0164914
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [0 5 4 0 0 0 0 0 8 7 0 3 0 0 0 6 0 1 0 2]
Losses:  8.968300819396973 1.2875289916992188
CurrentTrain: epoch  0, batch     0 | loss: 10.2558298Losses:  9.8094482421875 0.9992969036102295
CurrentTrain: epoch  0, batch     1 | loss: 10.8087454Losses:  10.949840545654297 1.5352284908294678
CurrentTrain: epoch  0, batch     2 | loss: 12.4850693Losses:  10.059404373168945 1.8861134052276611
CurrentTrain: epoch  0, batch     3 | loss: 11.9455175Losses:  7.918939590454102 1.3690035343170166
CurrentTrain: epoch  0, batch     4 | loss: 9.2879429Losses:  7.5372796058654785 1.3300666809082031
CurrentTrain: epoch  0, batch     5 | loss: 8.8673458Losses:  5.539238929748535 0.3074066638946533
CurrentTrain: epoch  0, batch     6 | loss: 5.8466454Losses:  5.0645270347595215 1.4820680618286133
CurrentTrain: epoch  1, batch     0 | loss: 6.5465951Losses:  3.867664098739624 1.3187875747680664
CurrentTrain: epoch  1, batch     1 | loss: 5.1864519Losses:  4.342798233032227 0.7839298248291016
CurrentTrain: epoch  1, batch     2 | loss: 5.1267281Losses:  5.11455774307251 1.0904755592346191
CurrentTrain: epoch  1, batch     3 | loss: 6.2050333Losses:  4.350639820098877 1.556348204612732
CurrentTrain: epoch  1, batch     4 | loss: 5.9069881Losses:  4.1727294921875 0.7786732912063599
CurrentTrain: epoch  1, batch     5 | loss: 4.9514027Losses:  7.277935981750488 0.3355829119682312
CurrentTrain: epoch  1, batch     6 | loss: 7.6135187Losses:  3.7719287872314453 1.203880786895752
CurrentTrain: epoch  2, batch     0 | loss: 4.9758096Losses:  4.0489912033081055 0.9888447523117065
CurrentTrain: epoch  2, batch     1 | loss: 5.0378361Losses:  3.199582576751709 0.9764646291732788
CurrentTrain: epoch  2, batch     2 | loss: 4.1760473Losses:  3.3096301555633545 1.2141900062561035
CurrentTrain: epoch  2, batch     3 | loss: 4.5238199Losses:  4.962811470031738 0.8552080392837524
CurrentTrain: epoch  2, batch     4 | loss: 5.8180194Losses:  4.344602584838867 1.3109675645828247
CurrentTrain: epoch  2, batch     5 | loss: 5.6555700Losses:  3.243462562561035 0.30185914039611816
CurrentTrain: epoch  2, batch     6 | loss: 3.5453217Losses:  2.498377561569214 0.8924020528793335
CurrentTrain: epoch  3, batch     0 | loss: 3.3907795Losses:  3.586698055267334 1.171534776687622
CurrentTrain: epoch  3, batch     1 | loss: 4.7582331Losses:  3.4399218559265137 0.8470085263252258
CurrentTrain: epoch  3, batch     2 | loss: 4.2869306Losses:  3.5959510803222656 1.0475335121154785
CurrentTrain: epoch  3, batch     3 | loss: 4.6434846Losses:  4.67500638961792 0.5160209536552429
CurrentTrain: epoch  3, batch     4 | loss: 5.1910272Losses:  3.0459136962890625 1.0826722383499146
CurrentTrain: epoch  3, batch     5 | loss: 4.1285858Losses:  2.972835063934326 0.3834940493106842
CurrentTrain: epoch  3, batch     6 | loss: 3.3563292Losses:  2.867055892944336 0.8569295406341553
CurrentTrain: epoch  4, batch     0 | loss: 3.7239854Losses:  3.754270553588867 1.0566538572311401
CurrentTrain: epoch  4, batch     1 | loss: 4.8109245Losses:  3.3936800956726074 1.337497591972351
CurrentTrain: epoch  4, batch     2 | loss: 4.7311778Losses:  3.5834970474243164 1.263047456741333
CurrentTrain: epoch  4, batch     3 | loss: 4.8465443Losses:  2.459029197692871 0.7831714153289795
CurrentTrain: epoch  4, batch     4 | loss: 3.2422006Losses:  2.985952138900757 0.7128604054450989
CurrentTrain: epoch  4, batch     5 | loss: 3.6988125Losses:  3.8597564697265625 0.19847321510314941
CurrentTrain: epoch  4, batch     6 | loss: 4.0582294Losses:  3.723282814025879 1.1539803743362427
CurrentTrain: epoch  5, batch     0 | loss: 4.8772631Losses:  2.8895339965820312 0.9064348936080933
CurrentTrain: epoch  5, batch     1 | loss: 3.7959690Losses:  2.7427265644073486 0.971064567565918
CurrentTrain: epoch  5, batch     2 | loss: 3.7137911Losses:  3.456533193588257 1.12185537815094
CurrentTrain: epoch  5, batch     3 | loss: 4.5783887Losses:  3.0575122833251953 0.9173508286476135
CurrentTrain: epoch  5, batch     4 | loss: 3.9748631Losses:  2.5116939544677734 0.8274133205413818
CurrentTrain: epoch  5, batch     5 | loss: 3.3391073Losses:  1.7692210674285889 8.94069742685133e-08
CurrentTrain: epoch  5, batch     6 | loss: 1.7692212Losses:  3.15623140335083 0.9566166400909424
CurrentTrain: epoch  6, batch     0 | loss: 4.1128483Losses:  1.9536259174346924 0.5326730012893677
CurrentTrain: epoch  6, batch     1 | loss: 2.4862990Losses:  2.8740577697753906 0.7985146641731262
CurrentTrain: epoch  6, batch     2 | loss: 3.6725724Losses:  2.9999794960021973 0.9548900127410889
CurrentTrain: epoch  6, batch     3 | loss: 3.9548695Losses:  2.793443202972412 0.7842890620231628
CurrentTrain: epoch  6, batch     4 | loss: 3.5777323Losses:  3.1114444732666016 0.8645898103713989
CurrentTrain: epoch  6, batch     5 | loss: 3.9760342Losses:  2.9548192024230957 0.28057661652565
CurrentTrain: epoch  6, batch     6 | loss: 3.2353959Losses:  1.9177052974700928 0.31250622868537903
CurrentTrain: epoch  7, batch     0 | loss: 2.2302115Losses:  2.3533318042755127 0.512688159942627
CurrentTrain: epoch  7, batch     1 | loss: 2.8660200Losses:  2.7553932666778564 0.7239641547203064
CurrentTrain: epoch  7, batch     2 | loss: 3.4793575Losses:  3.054989814758301 0.8595672249794006
CurrentTrain: epoch  7, batch     3 | loss: 3.9145570Losses:  3.111462116241455 0.8107640743255615
CurrentTrain: epoch  7, batch     4 | loss: 3.9222262Losses:  3.00089693069458 0.967326283454895
CurrentTrain: epoch  7, batch     5 | loss: 3.9682231Losses:  2.3725194931030273 0.12489618360996246
CurrentTrain: epoch  7, batch     6 | loss: 2.4974158Losses:  2.6306352615356445 0.7987232208251953
CurrentTrain: epoch  8, batch     0 | loss: 3.4293585Losses:  2.503094434738159 0.5928702354431152
CurrentTrain: epoch  8, batch     1 | loss: 3.0959647Losses:  2.905399799346924 0.6342848539352417
CurrentTrain: epoch  8, batch     2 | loss: 3.5396848Losses:  2.2442240715026855 0.5982720851898193
CurrentTrain: epoch  8, batch     3 | loss: 2.8424962Losses:  2.150554895401001 0.5302007794380188
CurrentTrain: epoch  8, batch     4 | loss: 2.6807556Losses:  2.9462223052978516 0.8028976917266846
CurrentTrain: epoch  8, batch     5 | loss: 3.7491200Losses:  1.6753220558166504 8.94069742685133e-08
CurrentTrain: epoch  8, batch     6 | loss: 1.6753222Losses:  2.0511107444763184 0.442554771900177
CurrentTrain: epoch  9, batch     0 | loss: 2.4936655Losses:  2.2123427391052246 0.47309672832489014
CurrentTrain: epoch  9, batch     1 | loss: 2.6854396Losses:  2.364933729171753 0.9021702408790588
CurrentTrain: epoch  9, batch     2 | loss: 3.2671039Losses:  2.881524085998535 0.6777350306510925
CurrentTrain: epoch  9, batch     3 | loss: 3.5592592Losses:  2.386742115020752 0.5563232898712158
CurrentTrain: epoch  9, batch     4 | loss: 2.9430654Losses:  2.5614609718322754 0.5731985569000244
CurrentTrain: epoch  9, batch     5 | loss: 3.1346595Losses:  2.6043217182159424 0.05531581863760948
CurrentTrain: epoch  9, batch     6 | loss: 2.6596375
Losses:  5.392615795135498 0.7179444432258606
MemoryTrain:  epoch  0, batch     0 | loss: 6.1105604Losses:  8.534210205078125 0.3495689630508423
MemoryTrain:  epoch  0, batch     1 | loss: 8.8837795Losses:  10.453143119812012 0.1263492852449417
MemoryTrain:  epoch  0, batch     2 | loss: 10.5794926Losses:  0.48429256677627563 0.5449860692024231
MemoryTrain:  epoch  1, batch     0 | loss: 1.0292786Losses:  0.42758458852767944 0.4200316369533539
MemoryTrain:  epoch  1, batch     1 | loss: 0.8476162Losses:  1.4176805019378662 0.2164706438779831
MemoryTrain:  epoch  1, batch     2 | loss: 1.6341511Losses:  0.56611168384552 0.2505037188529968
MemoryTrain:  epoch  2, batch     0 | loss: 0.8166154Losses:  0.5535280704498291 0.43131184577941895
MemoryTrain:  epoch  2, batch     1 | loss: 0.9848399Losses:  0.16108310222625732 0.25581350922584534
MemoryTrain:  epoch  2, batch     2 | loss: 0.4168966Losses:  0.3199804425239563 0.5548754930496216
MemoryTrain:  epoch  3, batch     0 | loss: 0.8748559Losses:  0.5686814785003662 0.3116249740123749
MemoryTrain:  epoch  3, batch     1 | loss: 0.8803065Losses:  0.08516673743724823 0.15786461532115936
MemoryTrain:  epoch  3, batch     2 | loss: 0.2430314Losses:  0.2345041036605835 0.4480360746383667
MemoryTrain:  epoch  4, batch     0 | loss: 0.6825402Losses:  0.1632770597934723 0.3964179754257202
MemoryTrain:  epoch  4, batch     1 | loss: 0.5596950Losses:  0.13062268495559692 0.1008540540933609
MemoryTrain:  epoch  4, batch     2 | loss: 0.2314767Losses:  0.1629592627286911 0.47412189841270447
MemoryTrain:  epoch  5, batch     0 | loss: 0.6370811Losses:  0.173095241189003 0.40011969208717346
MemoryTrain:  epoch  5, batch     1 | loss: 0.5732149Losses:  0.12037281692028046 0.0630388855934143
MemoryTrain:  epoch  5, batch     2 | loss: 0.1834117Losses:  0.09898969531059265 0.33238929510116577
MemoryTrain:  epoch  6, batch     0 | loss: 0.4313790Losses:  0.1662256419658661 0.46097344160079956
MemoryTrain:  epoch  6, batch     1 | loss: 0.6271991Losses:  0.12536047399044037 0.10243386030197144
MemoryTrain:  epoch  6, batch     2 | loss: 0.2277943Losses:  0.08569853007793427 0.28227174282073975
MemoryTrain:  epoch  7, batch     0 | loss: 0.3679703Losses:  0.15721479058265686 0.4201178252696991
MemoryTrain:  epoch  7, batch     1 | loss: 0.5773326Losses:  0.1834004819393158 0.22993586957454681
MemoryTrain:  epoch  7, batch     2 | loss: 0.4133363Losses:  0.10843874514102936 0.2843495011329651
MemoryTrain:  epoch  8, batch     0 | loss: 0.3927882Losses:  0.14827370643615723 0.5016047954559326
MemoryTrain:  epoch  8, batch     1 | loss: 0.6498785Losses:  0.06726367026567459 0.11584912240505219
MemoryTrain:  epoch  8, batch     2 | loss: 0.1831128Losses:  0.13273049890995026 0.3780672550201416
MemoryTrain:  epoch  9, batch     0 | loss: 0.5107977Losses:  0.10565109550952911 0.24865765869617462
MemoryTrain:  epoch  9, batch     1 | loss: 0.3543088Losses:  0.08306488394737244 0.07284864783287048
MemoryTrain:  epoch  9, batch     2 | loss: 0.1559135
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 94.72%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 94.70%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 94.16%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.15%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 94.15%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 93.46%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 92.98%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.99%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 92.44%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 92.10%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 92.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 92.08%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 92.10%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.21%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.23%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 91.53%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 90.75%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 89.90%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 89.16%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 88.67%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 87.89%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.73%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 88.23%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 87.93%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 87.08%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 86.18%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 85.51%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 84.71%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 83.80%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 83.24%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 83.36%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 83.44%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 83.48%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 84.40%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 84.20%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 83.69%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 83.45%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 83.13%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 82.95%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 82.93%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.96%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 82.94%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 83.06%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 82.91%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 82.90%   
cur_acc:  ['0.9464', '0.7103']
his_acc:  ['0.9464', '0.8290']
Clustering into  14  clusters
Clusters:  [ 2 11  9  2  2  2  0  2  8  7  0 13 10  2  2  6  2  1  2 12  2  5  2  2
  1  2  2  2  3  4]
Losses:  7.318719387054443 1.0114398002624512
CurrentTrain: epoch  0, batch     0 | loss: 8.3301592Losses:  8.467216491699219 1.1493799686431885
CurrentTrain: epoch  0, batch     1 | loss: 9.6165962Losses:  10.016851425170898 1.0821375846862793
CurrentTrain: epoch  0, batch     2 | loss: 11.0989895Losses:  8.391570091247559 1.0994585752487183
CurrentTrain: epoch  0, batch     3 | loss: 9.4910288Losses:  8.30219841003418 1.3800406455993652
CurrentTrain: epoch  0, batch     4 | loss: 9.6822395Losses:  7.054997444152832 1.2119250297546387
CurrentTrain: epoch  0, batch     5 | loss: 8.2669220Losses:  5.268379211425781 0.5544978976249695
CurrentTrain: epoch  0, batch     6 | loss: 5.8228769Losses:  4.827937126159668 1.0538690090179443
CurrentTrain: epoch  1, batch     0 | loss: 5.8818064Losses:  4.7686591148376465 1.097578525543213
CurrentTrain: epoch  1, batch     1 | loss: 5.8662376Losses:  4.611180305480957 0.9181036949157715
CurrentTrain: epoch  1, batch     2 | loss: 5.5292840Losses:  3.7138142585754395 1.0912935733795166
CurrentTrain: epoch  1, batch     3 | loss: 4.8051081Losses:  3.357879161834717 0.7910955548286438
CurrentTrain: epoch  1, batch     4 | loss: 4.1489749Losses:  3.4606995582580566 0.9948826432228088
CurrentTrain: epoch  1, batch     5 | loss: 4.4555821Losses:  3.454550266265869 5.960464477539063e-08
CurrentTrain: epoch  1, batch     6 | loss: 3.4545503Losses:  3.810805320739746 1.0090253353118896
CurrentTrain: epoch  2, batch     0 | loss: 4.8198309Losses:  3.3991899490356445 1.1033916473388672
CurrentTrain: epoch  2, batch     1 | loss: 4.5025816Losses:  3.938880443572998 1.0819437503814697
CurrentTrain: epoch  2, batch     2 | loss: 5.0208244Losses:  3.703465461730957 1.1554417610168457
CurrentTrain: epoch  2, batch     3 | loss: 4.8589072Losses:  3.220515727996826 1.173028826713562
CurrentTrain: epoch  2, batch     4 | loss: 4.3935447Losses:  3.9185121059417725 1.0662527084350586
CurrentTrain: epoch  2, batch     5 | loss: 4.9847651Losses:  2.0831351280212402 0.2545389235019684
CurrentTrain: epoch  2, batch     6 | loss: 2.3376741Losses:  4.030153274536133 1.0976331233978271
CurrentTrain: epoch  3, batch     0 | loss: 5.1277866Losses:  2.4139392375946045 0.5216749310493469
CurrentTrain: epoch  3, batch     1 | loss: 2.9356141Losses:  3.117676019668579 0.9912496209144592
CurrentTrain: epoch  3, batch     2 | loss: 4.1089258Losses:  3.348579168319702 0.997119128704071
CurrentTrain: epoch  3, batch     3 | loss: 4.3456984Losses:  3.7994463443756104 0.8824103474617004
CurrentTrain: epoch  3, batch     4 | loss: 4.6818566Losses:  3.0178189277648926 0.5895267724990845
CurrentTrain: epoch  3, batch     5 | loss: 3.6073456Losses:  3.5205235481262207 0.3688505291938782
CurrentTrain: epoch  3, batch     6 | loss: 3.8893740Losses:  2.9203994274139404 0.8129218220710754
CurrentTrain: epoch  4, batch     0 | loss: 3.7333212Losses:  3.3773975372314453 0.8310364484786987
CurrentTrain: epoch  4, batch     1 | loss: 4.2084341Losses:  3.5899908542633057 0.7583591938018799
CurrentTrain: epoch  4, batch     2 | loss: 4.3483500Losses:  2.2712035179138184 0.6466947793960571
CurrentTrain: epoch  4, batch     3 | loss: 2.9178982Losses:  3.0082249641418457 1.0435645580291748
CurrentTrain: epoch  4, batch     4 | loss: 4.0517893Losses:  2.7999510765075684 0.4723665714263916
CurrentTrain: epoch  4, batch     5 | loss: 3.2723176Losses:  3.932389497756958 0.36235079169273376
CurrentTrain: epoch  4, batch     6 | loss: 4.2947402Losses:  2.274921417236328 0.6331693530082703
CurrentTrain: epoch  5, batch     0 | loss: 2.9080908Losses:  3.4457192420959473 0.7670889496803284
CurrentTrain: epoch  5, batch     1 | loss: 4.2128081Losses:  2.9237778186798096 0.9121102690696716
CurrentTrain: epoch  5, batch     2 | loss: 3.8358881Losses:  3.1458041667938232 1.0354397296905518
CurrentTrain: epoch  5, batch     3 | loss: 4.1812439Losses:  2.10115385055542 0.43189722299575806
CurrentTrain: epoch  5, batch     4 | loss: 2.5330510Losses:  2.7291793823242188 0.4269615411758423
CurrentTrain: epoch  5, batch     5 | loss: 3.1561408Losses:  2.629652500152588 0.1214134469628334
CurrentTrain: epoch  5, batch     6 | loss: 2.7510660Losses:  2.348825454711914 0.6213153004646301
CurrentTrain: epoch  6, batch     0 | loss: 2.9701407Losses:  2.594855546951294 0.5434470176696777
CurrentTrain: epoch  6, batch     1 | loss: 3.1383026Losses:  2.532162666320801 0.7908532619476318
CurrentTrain: epoch  6, batch     2 | loss: 3.3230159Losses:  2.446956157684326 0.7037918567657471
CurrentTrain: epoch  6, batch     3 | loss: 3.1507480Losses:  2.7733914852142334 0.8306179046630859
CurrentTrain: epoch  6, batch     4 | loss: 3.6040094Losses:  2.2937798500061035 0.4638948142528534
CurrentTrain: epoch  6, batch     5 | loss: 2.7576747Losses:  1.8722950220108032 0.14036956429481506
CurrentTrain: epoch  6, batch     6 | loss: 2.0126646Losses:  2.6595659255981445 0.7667427659034729
CurrentTrain: epoch  7, batch     0 | loss: 3.4263086Losses:  2.559159755706787 0.7968335151672363
CurrentTrain: epoch  7, batch     1 | loss: 3.3559933Losses:  2.602484703063965 0.8555974364280701
CurrentTrain: epoch  7, batch     2 | loss: 3.4580822Losses:  2.346745014190674 0.5869475603103638
CurrentTrain: epoch  7, batch     3 | loss: 2.9336925Losses:  1.827303171157837 0.35887613892555237
CurrentTrain: epoch  7, batch     4 | loss: 2.1861794Losses:  1.936227798461914 0.5289373397827148
CurrentTrain: epoch  7, batch     5 | loss: 2.4651651Losses:  2.2517447471618652 1.1920928955078125e-07
CurrentTrain: epoch  7, batch     6 | loss: 2.2517447Losses:  2.0622286796569824 0.5593398809432983
CurrentTrain: epoch  8, batch     0 | loss: 2.6215687Losses:  2.7952961921691895 0.7845799326896667
CurrentTrain: epoch  8, batch     1 | loss: 3.5798762Losses:  2.171635389328003 0.5186505317687988
CurrentTrain: epoch  8, batch     2 | loss: 2.6902859Losses:  2.093000888824463 0.35024532675743103
CurrentTrain: epoch  8, batch     3 | loss: 2.4432461Losses:  2.476706027984619 0.637721061706543
CurrentTrain: epoch  8, batch     4 | loss: 3.1144271Losses:  1.948028802871704 0.45676589012145996
CurrentTrain: epoch  8, batch     5 | loss: 2.4047947Losses:  1.809074878692627 0.13349846005439758
CurrentTrain: epoch  8, batch     6 | loss: 1.9425733Losses:  2.5640006065368652 0.5271283388137817
CurrentTrain: epoch  9, batch     0 | loss: 3.0911288Losses:  2.3434386253356934 0.49908190965652466
CurrentTrain: epoch  9, batch     1 | loss: 2.8425205Losses:  1.8651033639907837 0.4805563986301422
CurrentTrain: epoch  9, batch     2 | loss: 2.3456597Losses:  1.9712865352630615 0.4874502420425415
CurrentTrain: epoch  9, batch     3 | loss: 2.4587369Losses:  2.1294453144073486 0.40855565667152405
CurrentTrain: epoch  9, batch     4 | loss: 2.5380011Losses:  1.890971064567566 0.49432966113090515
CurrentTrain: epoch  9, batch     5 | loss: 2.3853006Losses:  1.9814715385437012 1.1920930376163597e-07
CurrentTrain: epoch  9, batch     6 | loss: 1.9814717
Losses:  5.659559726715088 0.5879124402999878
MemoryTrain:  epoch  0, batch     0 | loss: 6.2474723Losses:  8.787318229675293 0.5900009870529175
MemoryTrain:  epoch  0, batch     1 | loss: 9.3773193Losses:  9.733339309692383 0.29165908694267273
MemoryTrain:  epoch  0, batch     2 | loss: 10.0249987Losses:  10.56569766998291 0.28912487626075745
MemoryTrain:  epoch  0, batch     3 | loss: 10.8548222Losses:  0.8665464520454407 0.48401737213134766
MemoryTrain:  epoch  1, batch     0 | loss: 1.3505638Losses:  0.6772092580795288 0.5334688425064087
MemoryTrain:  epoch  1, batch     1 | loss: 1.2106781Losses:  0.6393133401870728 0.32467585802078247
MemoryTrain:  epoch  1, batch     2 | loss: 0.9639892Losses:  0.6884931921958923 0.3707923889160156
MemoryTrain:  epoch  1, batch     3 | loss: 1.0592856Losses:  0.41911953687667847 0.7354236841201782
MemoryTrain:  epoch  2, batch     0 | loss: 1.1545432Losses:  0.5372356176376343 0.26707810163497925
MemoryTrain:  epoch  2, batch     1 | loss: 0.8043137Losses:  0.7077502012252808 0.4670603275299072
MemoryTrain:  epoch  2, batch     2 | loss: 1.1748105Losses:  0.4913616180419922 0.39713573455810547
MemoryTrain:  epoch  2, batch     3 | loss: 0.8884974Losses:  0.2932511270046234 0.4498334527015686
MemoryTrain:  epoch  3, batch     0 | loss: 0.7430845Losses:  0.3958472013473511 0.3106091618537903
MemoryTrain:  epoch  3, batch     1 | loss: 0.7064564Losses:  0.2834128141403198 0.37083297967910767
MemoryTrain:  epoch  3, batch     2 | loss: 0.6542458Losses:  0.4983420968055725 0.4451579451560974
MemoryTrain:  epoch  3, batch     3 | loss: 0.9435000Losses:  0.20188026130199432 0.5045862793922424
MemoryTrain:  epoch  4, batch     0 | loss: 0.7064666Losses:  0.3310662508010864 0.4775503873825073
MemoryTrain:  epoch  4, batch     1 | loss: 0.8086166Losses:  0.35206928849220276 0.47954848408699036
MemoryTrain:  epoch  4, batch     2 | loss: 0.8316178Losses:  0.1805465668439865 0.2623167037963867
MemoryTrain:  epoch  4, batch     3 | loss: 0.4428633Losses:  0.1832900047302246 0.3370881676673889
MemoryTrain:  epoch  5, batch     0 | loss: 0.5203782Losses:  0.21732565760612488 0.2723621129989624
MemoryTrain:  epoch  5, batch     1 | loss: 0.4896878Losses:  0.2254643440246582 0.5748090744018555
MemoryTrain:  epoch  5, batch     2 | loss: 0.8002734Losses:  0.28635135293006897 0.3994433283805847
MemoryTrain:  epoch  5, batch     3 | loss: 0.6857947Losses:  0.22646021842956543 0.40091443061828613
MemoryTrain:  epoch  6, batch     0 | loss: 0.6273746Losses:  0.21073685586452484 0.4891369044780731
MemoryTrain:  epoch  6, batch     1 | loss: 0.6998737Losses:  0.20577211678028107 0.34573256969451904
MemoryTrain:  epoch  6, batch     2 | loss: 0.5515047Losses:  0.2555280327796936 0.24854803085327148
MemoryTrain:  epoch  6, batch     3 | loss: 0.5040761Losses:  0.20723044872283936 0.40852734446525574
MemoryTrain:  epoch  7, batch     0 | loss: 0.6157578Losses:  0.24598917365074158 0.41222941875457764
MemoryTrain:  epoch  7, batch     1 | loss: 0.6582186Losses:  0.24902677536010742 0.3704735040664673
MemoryTrain:  epoch  7, batch     2 | loss: 0.6195003Losses:  0.23715460300445557 0.2708921432495117
MemoryTrain:  epoch  7, batch     3 | loss: 0.5080467Losses:  0.21524593234062195 0.3657522201538086
MemoryTrain:  epoch  8, batch     0 | loss: 0.5809982Losses:  0.20946253836154938 0.4300839304924011
MemoryTrain:  epoch  8, batch     1 | loss: 0.6395465Losses:  0.32688307762145996 0.3663405776023865
MemoryTrain:  epoch  8, batch     2 | loss: 0.6932237Losses:  0.19370168447494507 0.3237766921520233
MemoryTrain:  epoch  8, batch     3 | loss: 0.5174783Losses:  0.2777649164199829 0.3391585350036621
MemoryTrain:  epoch  9, batch     0 | loss: 0.6169235Losses:  0.1475435346364975 0.220316082239151
MemoryTrain:  epoch  9, batch     1 | loss: 0.3678596Losses:  0.22763961553573608 0.3879595398902893
MemoryTrain:  epoch  9, batch     2 | loss: 0.6155992Losses:  0.27824369072914124 0.3354153633117676
MemoryTrain:  epoch  9, batch     3 | loss: 0.6136590
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 81.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 79.63%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 78.18%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.98%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.90%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.75%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 92.89%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 92.37%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 91.39%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 91.03%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 90.77%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 89.52%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 88.77%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.91%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 89.25%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 88.49%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 87.66%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 86.86%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 86.23%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 85.70%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 84.95%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 85.09%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 84.48%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.54%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 82.83%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 82.07%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.25%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.72%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 80.79%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 80.80%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.19%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 81.37%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 81.66%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.68%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.17%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.67%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 79.07%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.82%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.97%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 79.15%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 79.27%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 79.07%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 78.79%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 78.42%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 78.20%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 77.69%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 77.41%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 78.22%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 77.97%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 77.66%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 77.55%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.58%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 78.59%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 79.54%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 79.56%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 79.53%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 79.32%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 79.45%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 79.90%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 79.73%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 79.47%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 79.34%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 79.18%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 79.02%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 78.92%   
cur_acc:  ['0.9464', '0.7103', '0.7798']
his_acc:  ['0.9464', '0.8290', '0.7892']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15 17 16  2  1  1 12  1 18  1  8  1 14  1  1
  7  1  1  1  3  6  2  1  5  1 10  0  4  1  1  1]
Losses:  6.921492099761963 1.4553884267807007
CurrentTrain: epoch  0, batch     0 | loss: 8.3768806Losses:  9.110766410827637 1.070122241973877
CurrentTrain: epoch  0, batch     1 | loss: 10.1808891Losses:  8.529077529907227 1.1333539485931396
CurrentTrain: epoch  0, batch     2 | loss: 9.6624317Losses:  6.665660858154297 1.0193867683410645
CurrentTrain: epoch  0, batch     3 | loss: 7.6850476Losses:  9.27079963684082 1.1926891803741455
CurrentTrain: epoch  0, batch     4 | loss: 10.4634886Losses:  8.258460998535156 1.2427562475204468
CurrentTrain: epoch  0, batch     5 | loss: 9.5012169Losses:  3.8185782432556152 0.30780497193336487
CurrentTrain: epoch  0, batch     6 | loss: 4.1263833Losses:  4.193376541137695 0.7993195056915283
CurrentTrain: epoch  1, batch     0 | loss: 4.9926958Losses:  3.8883564472198486 1.264643907546997
CurrentTrain: epoch  1, batch     1 | loss: 5.1530004Losses:  3.8016607761383057 1.1062685251235962
CurrentTrain: epoch  1, batch     2 | loss: 4.9079294Losses:  3.278421640396118 1.1198747158050537
CurrentTrain: epoch  1, batch     3 | loss: 4.3982964Losses:  3.8654441833496094 0.9154706001281738
CurrentTrain: epoch  1, batch     4 | loss: 4.7809148Losses:  3.7123987674713135 1.1519356966018677
CurrentTrain: epoch  1, batch     5 | loss: 4.8643346Losses:  3.9710516929626465 0.14270344376564026
CurrentTrain: epoch  1, batch     6 | loss: 4.1137552Losses:  3.6143813133239746 0.921177089214325
CurrentTrain: epoch  2, batch     0 | loss: 4.5355582Losses:  3.8325417041778564 0.9607489705085754
CurrentTrain: epoch  2, batch     1 | loss: 4.7932906Losses:  3.55413818359375 1.2586108446121216
CurrentTrain: epoch  2, batch     2 | loss: 4.8127489Losses:  3.6157422065734863 0.7016907930374146
CurrentTrain: epoch  2, batch     3 | loss: 4.3174329Losses:  3.5563056468963623 0.9813699722290039
CurrentTrain: epoch  2, batch     4 | loss: 4.5376759Losses:  2.787623405456543 0.5265752077102661
CurrentTrain: epoch  2, batch     5 | loss: 3.3141985Losses:  2.990199089050293 0.2535198926925659
CurrentTrain: epoch  2, batch     6 | loss: 3.2437191Losses:  3.2502708435058594 0.7889004945755005
CurrentTrain: epoch  3, batch     0 | loss: 4.0391712Losses:  3.908353090286255 0.962131142616272
CurrentTrain: epoch  3, batch     1 | loss: 4.8704844Losses:  3.1842803955078125 0.9081496000289917
CurrentTrain: epoch  3, batch     2 | loss: 4.0924301Losses:  2.8249306678771973 0.7156923413276672
CurrentTrain: epoch  3, batch     3 | loss: 3.5406229Losses:  3.745972156524658 0.8271175622940063
CurrentTrain: epoch  3, batch     4 | loss: 4.5730896Losses:  2.0600242614746094 0.450177937746048
CurrentTrain: epoch  3, batch     5 | loss: 2.5102022Losses:  3.1346933841705322 0.4143279194831848
CurrentTrain: epoch  3, batch     6 | loss: 3.5490212Losses:  3.330498695373535 0.865742564201355
CurrentTrain: epoch  4, batch     0 | loss: 4.1962414Losses:  3.6794676780700684 0.8658472299575806
CurrentTrain: epoch  4, batch     1 | loss: 4.5453148Losses:  2.6318581104278564 0.4859086871147156
CurrentTrain: epoch  4, batch     2 | loss: 3.1177669Losses:  2.858982563018799 0.709136426448822
CurrentTrain: epoch  4, batch     3 | loss: 3.5681190Losses:  2.620339870452881 0.5927141904830933
CurrentTrain: epoch  4, batch     4 | loss: 3.2130542Losses:  2.8079848289489746 0.8121812343597412
CurrentTrain: epoch  4, batch     5 | loss: 3.6201661Losses:  1.6816734075546265 0.0
CurrentTrain: epoch  4, batch     6 | loss: 1.6816734Losses:  3.6917364597320557 0.6425644755363464
CurrentTrain: epoch  5, batch     0 | loss: 4.3343010Losses:  1.9301666021347046 0.4158518612384796
CurrentTrain: epoch  5, batch     1 | loss: 2.3460186Losses:  3.0395874977111816 0.7846343517303467
CurrentTrain: epoch  5, batch     2 | loss: 3.8242218Losses:  3.1583218574523926 0.6070403456687927
CurrentTrain: epoch  5, batch     3 | loss: 3.7653623Losses:  1.7910661697387695 0.3914068341255188
CurrentTrain: epoch  5, batch     4 | loss: 2.1824729Losses:  2.718623161315918 0.5374324917793274
CurrentTrain: epoch  5, batch     5 | loss: 3.2560556Losses:  3.935084104537964 1.788139485370266e-07
CurrentTrain: epoch  5, batch     6 | loss: 3.9350843Losses:  2.5334391593933105 0.5324897766113281
CurrentTrain: epoch  6, batch     0 | loss: 3.0659289Losses:  2.440095901489258 0.5142509937286377
CurrentTrain: epoch  6, batch     1 | loss: 2.9543469Losses:  2.430704116821289 0.7275441884994507
CurrentTrain: epoch  6, batch     2 | loss: 3.1582484Losses:  2.1668293476104736 0.42538541555404663
CurrentTrain: epoch  6, batch     3 | loss: 2.5922148Losses:  3.219320297241211 0.6117329597473145
CurrentTrain: epoch  6, batch     4 | loss: 3.8310533Losses:  2.8119802474975586 0.8655462265014648
CurrentTrain: epoch  6, batch     5 | loss: 3.6775265Losses:  1.821624517440796 0.11978040635585785
CurrentTrain: epoch  6, batch     6 | loss: 1.9414049Losses:  3.3966875076293945 0.41496020555496216
CurrentTrain: epoch  7, batch     0 | loss: 3.8116477Losses:  2.191986560821533 0.5937230587005615
CurrentTrain: epoch  7, batch     1 | loss: 2.7857096Losses:  2.070655345916748 0.4965476989746094
CurrentTrain: epoch  7, batch     2 | loss: 2.5672030Losses:  2.228626251220703 0.406656950712204
CurrentTrain: epoch  7, batch     3 | loss: 2.6352832Losses:  2.8927907943725586 0.6464712023735046
CurrentTrain: epoch  7, batch     4 | loss: 3.5392621Losses:  2.0916810035705566 0.46044906973838806
CurrentTrain: epoch  7, batch     5 | loss: 2.5521300Losses:  1.7565298080444336 0.10863238573074341
CurrentTrain: epoch  7, batch     6 | loss: 1.8651621Losses:  2.4519805908203125 0.6722216606140137
CurrentTrain: epoch  8, batch     0 | loss: 3.1242023Losses:  2.1368448734283447 0.4247809648513794
CurrentTrain: epoch  8, batch     1 | loss: 2.5616260Losses:  2.121371030807495 0.45168745517730713
CurrentTrain: epoch  8, batch     2 | loss: 2.5730586Losses:  2.284166097640991 0.5719824433326721
CurrentTrain: epoch  8, batch     3 | loss: 2.8561485Losses:  2.0156514644622803 0.540684700012207
CurrentTrain: epoch  8, batch     4 | loss: 2.5563362Losses:  2.071483850479126 0.36252328753471375
CurrentTrain: epoch  8, batch     5 | loss: 2.4340072Losses:  1.7667644023895264 0.07650809735059738
CurrentTrain: epoch  8, batch     6 | loss: 1.8432724Losses:  1.978419303894043 0.3991277813911438
CurrentTrain: epoch  9, batch     0 | loss: 2.3775470Losses:  2.1685781478881836 0.6447129249572754
CurrentTrain: epoch  9, batch     1 | loss: 2.8132911Losses:  1.923000693321228 0.4350966811180115
CurrentTrain: epoch  9, batch     2 | loss: 2.3580973Losses:  1.8072245121002197 0.34728991985321045
CurrentTrain: epoch  9, batch     3 | loss: 2.1545143Losses:  2.303226947784424 0.544427216053009
CurrentTrain: epoch  9, batch     4 | loss: 2.8476541Losses:  1.7805917263031006 0.520075798034668
CurrentTrain: epoch  9, batch     5 | loss: 2.3006675Losses:  1.7162153720855713 0.07980963587760925
CurrentTrain: epoch  9, batch     6 | loss: 1.7960250
Losses:  5.836007118225098 0.5841765403747559
MemoryTrain:  epoch  0, batch     0 | loss: 6.4201837Losses:  9.224576950073242 0.5762488842010498
MemoryTrain:  epoch  0, batch     1 | loss: 9.8008261Losses:  9.905473709106445 0.35500162839889526
MemoryTrain:  epoch  0, batch     2 | loss: 10.2604752Losses:  11.060979843139648 0.7046931982040405
MemoryTrain:  epoch  0, batch     3 | loss: 11.7656727Losses:  10.315593719482422 0.23284010589122772
MemoryTrain:  epoch  0, batch     4 | loss: 10.5484343Losses:  0.6312958598136902 0.4987086355686188
MemoryTrain:  epoch  1, batch     0 | loss: 1.1300045Losses:  0.7864401340484619 0.41412413120269775
MemoryTrain:  epoch  1, batch     1 | loss: 1.2005643Losses:  0.8394088745117188 0.3021092712879181
MemoryTrain:  epoch  1, batch     2 | loss: 1.1415181Losses:  1.0400068759918213 0.4422973096370697
MemoryTrain:  epoch  1, batch     3 | loss: 1.4823042Losses:  0.996715784072876 0.560707688331604
MemoryTrain:  epoch  1, batch     4 | loss: 1.5574235Losses:  0.5270183086395264 0.4367484450340271
MemoryTrain:  epoch  2, batch     0 | loss: 0.9637668Losses:  0.45047399401664734 0.5675279498100281
MemoryTrain:  epoch  2, batch     1 | loss: 1.0180019Losses:  0.42421483993530273 0.3414919376373291
MemoryTrain:  epoch  2, batch     2 | loss: 0.7657068Losses:  0.6500784158706665 0.3952198624610901
MemoryTrain:  epoch  2, batch     3 | loss: 1.0452983Losses:  0.9335410594940186 0.43610817193984985
MemoryTrain:  epoch  2, batch     4 | loss: 1.3696492Losses:  0.5822649598121643 0.3006829023361206
MemoryTrain:  epoch  3, batch     0 | loss: 0.8829479Losses:  0.4183063805103302 0.4499243497848511
MemoryTrain:  epoch  3, batch     1 | loss: 0.8682307Losses:  0.4320255517959595 0.47135835886001587
MemoryTrain:  epoch  3, batch     2 | loss: 0.9033839Losses:  0.7737846374511719 0.7268342971801758
MemoryTrain:  epoch  3, batch     3 | loss: 1.5006189Losses:  0.32112836837768555 0.27736973762512207
MemoryTrain:  epoch  3, batch     4 | loss: 0.5984981Losses:  0.5106415748596191 0.3539462387561798
MemoryTrain:  epoch  4, batch     0 | loss: 0.8645878Losses:  0.42545926570892334 0.4462267756462097
MemoryTrain:  epoch  4, batch     1 | loss: 0.8716860Losses:  0.43116509914398193 0.4668482840061188
MemoryTrain:  epoch  4, batch     2 | loss: 0.8980134Losses:  0.37386587262153625 0.41290318965911865
MemoryTrain:  epoch  4, batch     3 | loss: 0.7867690Losses:  0.42182794213294983 0.6300804018974304
MemoryTrain:  epoch  4, batch     4 | loss: 1.0519084Losses:  0.3631970286369324 0.46324360370635986
MemoryTrain:  epoch  5, batch     0 | loss: 0.8264406Losses:  0.3323989808559418 0.4352785050868988
MemoryTrain:  epoch  5, batch     1 | loss: 0.7676775Losses:  0.25101229548454285 0.2501599192619324
MemoryTrain:  epoch  5, batch     2 | loss: 0.5011722Losses:  0.38885071873664856 0.48069924116134644
MemoryTrain:  epoch  5, batch     3 | loss: 0.8695500Losses:  0.31421613693237305 0.49741867184638977
MemoryTrain:  epoch  5, batch     4 | loss: 0.8116348Losses:  0.25684189796447754 0.3508129119873047
MemoryTrain:  epoch  6, batch     0 | loss: 0.6076548Losses:  0.4526010751724243 0.5275188088417053
MemoryTrain:  epoch  6, batch     1 | loss: 0.9801199Losses:  0.2468811720609665 0.33974167704582214
MemoryTrain:  epoch  6, batch     2 | loss: 0.5866228Losses:  0.3355569839477539 0.42334309220314026
MemoryTrain:  epoch  6, batch     3 | loss: 0.7589000Losses:  0.3610842227935791 0.31076303124427795
MemoryTrain:  epoch  6, batch     4 | loss: 0.6718472Losses:  0.28682073950767517 0.4459041953086853
MemoryTrain:  epoch  7, batch     0 | loss: 0.7327249Losses:  0.28031477332115173 0.3185526728630066
MemoryTrain:  epoch  7, batch     1 | loss: 0.5988674Losses:  0.23945334553718567 0.31564587354660034
MemoryTrain:  epoch  7, batch     2 | loss: 0.5550992Losses:  0.35606691241264343 0.5657267570495605
MemoryTrain:  epoch  7, batch     3 | loss: 0.9217937Losses:  0.27571362257003784 0.33281242847442627
MemoryTrain:  epoch  7, batch     4 | loss: 0.6085261Losses:  0.23583823442459106 0.21380916237831116
MemoryTrain:  epoch  8, batch     0 | loss: 0.4496474Losses:  0.264303982257843 0.4275638461112976
MemoryTrain:  epoch  8, batch     1 | loss: 0.6918678Losses:  0.3201252818107605 0.4398458003997803
MemoryTrain:  epoch  8, batch     2 | loss: 0.7599711Losses:  0.26496800780296326 0.27194005250930786
MemoryTrain:  epoch  8, batch     3 | loss: 0.5369080Losses:  0.4476045072078705 0.5552612543106079
MemoryTrain:  epoch  8, batch     4 | loss: 1.0028658Losses:  0.2427135705947876 0.24302791059017181
MemoryTrain:  epoch  9, batch     0 | loss: 0.4857415Losses:  0.24677085876464844 0.31860172748565674
MemoryTrain:  epoch  9, batch     1 | loss: 0.5653726Losses:  0.3272988796234131 0.5643466711044312
MemoryTrain:  epoch  9, batch     2 | loss: 0.8916456Losses:  0.5263354778289795 0.4637284278869629
MemoryTrain:  epoch  9, batch     3 | loss: 0.9900639Losses:  0.23435018956661224 0.27347445487976074
MemoryTrain:  epoch  9, batch     4 | loss: 0.5078247
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 55.40%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 52.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 54.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 56.02%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 57.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 58.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 61.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 67.39%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 66.62%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 66.21%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 64.86%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 63.59%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 92.24%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 89.82%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 89.48%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 87.88%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 87.69%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 87.41%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 87.32%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.06%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 86.89%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 86.27%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 85.47%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 84.54%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 83.70%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 82.97%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 82.10%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 82.09%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 82.74%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 81.95%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 81.04%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 80.22%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 79.55%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 78.70%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 79.31%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 78.73%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 78.35%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 77.93%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 77.34%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 76.94%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 76.68%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 76.60%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 76.54%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 76.43%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 75.98%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 75.68%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 75.14%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 74.76%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 75.18%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 74.91%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 74.69%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 75.53%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 75.77%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 76.57%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 76.37%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 76.13%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 75.94%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 75.74%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 75.67%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 76.24%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.13%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 75.87%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 75.53%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 75.17%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 74.84%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 74.51%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 74.28%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 74.03%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.81%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 74.17%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 74.24%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 74.33%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 74.09%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.89%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.74%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 73.51%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 73.06%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 73.38%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 74.51%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 74.38%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 74.20%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 74.07%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 73.83%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 73.55%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 73.67%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 73.75%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 73.55%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 73.33%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 73.14%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 72.94%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 72.82%   
cur_acc:  ['0.9464', '0.7103', '0.7798', '0.6359']
his_acc:  ['0.9464', '0.8290', '0.7892', '0.7282']
Clustering into  24  clusters
Clusters:  [ 4 23 19  4  4  4  0  4 15 16  1 21  3  4  4 12  4 22  4 10  4 17  4  4
 11  4  4  4 20 18  3  4 13  4 14  0  6  4  4  4  7  8  4  4  5  1  9  4
  4  2]
Losses:  7.114831447601318 1.348902702331543
CurrentTrain: epoch  0, batch     0 | loss: 8.4637337Losses:  8.765923500061035 1.3791277408599854
CurrentTrain: epoch  0, batch     1 | loss: 10.1450510Losses:  8.589897155761719 1.0975267887115479
CurrentTrain: epoch  0, batch     2 | loss: 9.6874237Losses:  6.62917947769165 1.057232141494751
CurrentTrain: epoch  0, batch     3 | loss: 7.6864119Losses:  7.433155059814453 1.3521848917007446
CurrentTrain: epoch  0, batch     4 | loss: 8.7853403Losses:  5.307008743286133 1.4575217962265015
CurrentTrain: epoch  0, batch     5 | loss: 6.7645307Losses:  3.7866733074188232 0.3564991354942322
CurrentTrain: epoch  0, batch     6 | loss: 4.1431723Losses:  3.340496301651001 1.2826884984970093
CurrentTrain: epoch  1, batch     0 | loss: 4.6231847Losses:  2.9019174575805664 0.7596907019615173
CurrentTrain: epoch  1, batch     1 | loss: 3.6616082Losses:  2.4167277812957764 0.6816222667694092
CurrentTrain: epoch  1, batch     2 | loss: 3.0983500Losses:  2.6412901878356934 0.6453312039375305
CurrentTrain: epoch  1, batch     3 | loss: 3.2866213Losses:  3.1912269592285156 0.9112743139266968
CurrentTrain: epoch  1, batch     4 | loss: 4.1025014Losses:  2.3809220790863037 1.099618911743164
CurrentTrain: epoch  1, batch     5 | loss: 3.4805410Losses:  3.5846569538116455 0.4687449336051941
CurrentTrain: epoch  1, batch     6 | loss: 4.0534019Losses:  2.7533092498779297 1.0206881761550903
CurrentTrain: epoch  2, batch     0 | loss: 3.7739973Losses:  2.4221835136413574 1.04695463180542
CurrentTrain: epoch  2, batch     1 | loss: 3.4691381Losses:  2.6467642784118652 0.8526252508163452
CurrentTrain: epoch  2, batch     2 | loss: 3.4993896Losses:  2.660111904144287 0.9787423014640808
CurrentTrain: epoch  2, batch     3 | loss: 3.6388543Losses:  2.38761043548584 1.0377793312072754
CurrentTrain: epoch  2, batch     4 | loss: 3.4253898Losses:  1.999384880065918 0.5909238457679749
CurrentTrain: epoch  2, batch     5 | loss: 2.5903087Losses:  1.9166254997253418 0.12588055431842804
CurrentTrain: epoch  2, batch     6 | loss: 2.0425060Losses:  2.2060585021972656 0.9980243444442749
CurrentTrain: epoch  3, batch     0 | loss: 3.2040830Losses:  2.313084125518799 0.8534908890724182
CurrentTrain: epoch  3, batch     1 | loss: 3.1665750Losses:  2.385866403579712 1.0119874477386475
CurrentTrain: epoch  3, batch     2 | loss: 3.3978539Losses:  2.338064193725586 0.5440546274185181
CurrentTrain: epoch  3, batch     3 | loss: 2.8821187Losses:  2.1929407119750977 0.44460177421569824
CurrentTrain: epoch  3, batch     4 | loss: 2.6375425Losses:  2.0345191955566406 0.6409090757369995
CurrentTrain: epoch  3, batch     5 | loss: 2.6754284Losses:  2.0116991996765137 0.24036628007888794
CurrentTrain: epoch  3, batch     6 | loss: 2.2520654Losses:  2.332866668701172 0.5581682920455933
CurrentTrain: epoch  4, batch     0 | loss: 2.8910351Losses:  1.9537980556488037 0.7818468809127808
CurrentTrain: epoch  4, batch     1 | loss: 2.7356448Losses:  1.9521210193634033 0.6040849685668945
CurrentTrain: epoch  4, batch     2 | loss: 2.5562060Losses:  1.9305886030197144 0.4341559410095215
CurrentTrain: epoch  4, batch     3 | loss: 2.3647447Losses:  2.081831693649292 0.5078481435775757
CurrentTrain: epoch  4, batch     4 | loss: 2.5896797Losses:  2.220263957977295 0.3712209463119507
CurrentTrain: epoch  4, batch     5 | loss: 2.5914850Losses:  2.038710594177246 0.1123887300491333
CurrentTrain: epoch  4, batch     6 | loss: 2.1510992Losses:  2.2729084491729736 0.4612988829612732
CurrentTrain: epoch  5, batch     0 | loss: 2.7342074Losses:  1.9710575342178345 0.3211769461631775
CurrentTrain: epoch  5, batch     1 | loss: 2.2922344Losses:  1.9496909379959106 0.5680221915245056
CurrentTrain: epoch  5, batch     2 | loss: 2.5177131Losses:  1.8433728218078613 0.22477984428405762
CurrentTrain: epoch  5, batch     3 | loss: 2.0681527Losses:  1.7217576503753662 0.3449629545211792
CurrentTrain: epoch  5, batch     4 | loss: 2.0667205Losses:  2.0119211673736572 0.7726141214370728
CurrentTrain: epoch  5, batch     5 | loss: 2.7845354Losses:  1.7051072120666504 0.10448705404996872
CurrentTrain: epoch  5, batch     6 | loss: 1.8095943Losses:  1.7343628406524658 0.30740559101104736
CurrentTrain: epoch  6, batch     0 | loss: 2.0417686Losses:  1.903860330581665 0.4793255031108856
CurrentTrain: epoch  6, batch     1 | loss: 2.3831859Losses:  1.8069355487823486 0.4705968201160431
CurrentTrain: epoch  6, batch     2 | loss: 2.2775323Losses:  1.9593101739883423 0.416004478931427
CurrentTrain: epoch  6, batch     3 | loss: 2.3753147Losses:  1.8093397617340088 0.3363385796546936
CurrentTrain: epoch  6, batch     4 | loss: 2.1456783Losses:  2.0622544288635254 0.3019479215145111
CurrentTrain: epoch  6, batch     5 | loss: 2.3642023Losses:  2.0376787185668945 0.08829811215400696
CurrentTrain: epoch  6, batch     6 | loss: 2.1259768Losses:  1.742605447769165 0.3474743962287903
CurrentTrain: epoch  7, batch     0 | loss: 2.0900798Losses:  1.8706763982772827 0.4489096999168396
CurrentTrain: epoch  7, batch     1 | loss: 2.3195860Losses:  1.7486435174942017 0.566416323184967
CurrentTrain: epoch  7, batch     2 | loss: 2.3150599Losses:  1.8347022533416748 0.4351174831390381
CurrentTrain: epoch  7, batch     3 | loss: 2.2698197Losses:  1.7924120426177979 0.33917030692100525
CurrentTrain: epoch  7, batch     4 | loss: 2.1315823Losses:  1.7379354238510132 0.2607848644256592
CurrentTrain: epoch  7, batch     5 | loss: 1.9987203Losses:  1.7215759754180908 0.10269095748662949
CurrentTrain: epoch  7, batch     6 | loss: 1.8242669Losses:  1.7557623386383057 0.4736591875553131
CurrentTrain: epoch  8, batch     0 | loss: 2.2294216Losses:  1.8219659328460693 0.3864603638648987
CurrentTrain: epoch  8, batch     1 | loss: 2.2084262Losses:  1.7270126342773438 0.19921019673347473
CurrentTrain: epoch  8, batch     2 | loss: 1.9262228Losses:  1.7445924282073975 0.4076775908470154
CurrentTrain: epoch  8, batch     3 | loss: 2.1522701Losses:  1.7317659854888916 0.36811375617980957
CurrentTrain: epoch  8, batch     4 | loss: 2.0998797Losses:  1.7428350448608398 0.36239081621170044
CurrentTrain: epoch  8, batch     5 | loss: 2.1052258Losses:  1.6712265014648438 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.6712265Losses:  1.7410643100738525 0.3712618350982666
CurrentTrain: epoch  9, batch     0 | loss: 2.1123261Losses:  1.7216410636901855 0.19004012644290924
CurrentTrain: epoch  9, batch     1 | loss: 1.9116812Losses:  1.682159185409546 0.23337459564208984
CurrentTrain: epoch  9, batch     2 | loss: 1.9155338Losses:  1.7090083360671997 0.2714999318122864
CurrentTrain: epoch  9, batch     3 | loss: 1.9805083Losses:  1.7073051929473877 0.37627777457237244
CurrentTrain: epoch  9, batch     4 | loss: 2.0835829Losses:  1.7050570249557495 0.21656683087348938
CurrentTrain: epoch  9, batch     5 | loss: 1.9216238Losses:  1.7198070287704468 0.03455566242337227
CurrentTrain: epoch  9, batch     6 | loss: 1.7543627
Losses:  5.568901062011719 0.49610435962677
MemoryTrain:  epoch  0, batch     0 | loss: 6.0650053Losses:  8.038911819458008 0.31579872965812683
MemoryTrain:  epoch  0, batch     1 | loss: 8.3547106Losses:  9.489463806152344 0.5027710199356079
MemoryTrain:  epoch  0, batch     2 | loss: 9.9922352Losses:  9.745649337768555 0.5394030213356018
MemoryTrain:  epoch  0, batch     3 | loss: 10.2850523Losses:  10.417105674743652 0.38292837142944336
MemoryTrain:  epoch  0, batch     4 | loss: 10.8000336Losses:  10.249216079711914 0.4194509983062744
MemoryTrain:  epoch  0, batch     5 | loss: 10.6686668Losses:  11.776917457580566 0.015193464234471321
MemoryTrain:  epoch  0, batch     6 | loss: 11.7921114Losses:  0.6327037811279297 0.2930556535720825
MemoryTrain:  epoch  1, batch     0 | loss: 0.9257594Losses:  0.5463641285896301 0.275749146938324
MemoryTrain:  epoch  1, batch     1 | loss: 0.8221133Losses:  1.0375516414642334 0.6733394861221313
MemoryTrain:  epoch  1, batch     2 | loss: 1.7108911Losses:  0.8845048546791077 0.41547149419784546
MemoryTrain:  epoch  1, batch     3 | loss: 1.2999763Losses:  0.9973831176757812 0.48176756501197815
MemoryTrain:  epoch  1, batch     4 | loss: 1.4791507Losses:  0.5735121369361877 0.2951623201370239
MemoryTrain:  epoch  1, batch     5 | loss: 0.8686745Losses:  0.49986040592193604 0.06591267138719559
MemoryTrain:  epoch  1, batch     6 | loss: 0.5657731Losses:  0.6201421022415161 0.45260974764823914
MemoryTrain:  epoch  2, batch     0 | loss: 1.0727519Losses:  0.3945814371109009 0.3475489616394043
MemoryTrain:  epoch  2, batch     1 | loss: 0.7421304Losses:  0.31202688813209534 0.31800180673599243
MemoryTrain:  epoch  2, batch     2 | loss: 0.6300287Losses:  0.7117757797241211 0.42013970017433167
MemoryTrain:  epoch  2, batch     3 | loss: 1.1319155Losses:  0.7669405341148376 0.44101256132125854
MemoryTrain:  epoch  2, batch     4 | loss: 1.2079531Losses:  0.43586114048957825 0.47310835123062134
MemoryTrain:  epoch  2, batch     5 | loss: 0.9089695Losses:  0.3388357162475586 0.07482650876045227
MemoryTrain:  epoch  2, batch     6 | loss: 0.4136622Losses:  0.8087641596794128 0.43519043922424316
MemoryTrain:  epoch  3, batch     0 | loss: 1.2439547Losses:  0.42336899042129517 0.46514561772346497
MemoryTrain:  epoch  3, batch     1 | loss: 0.8885146Losses:  0.3667392432689667 0.44081059098243713
MemoryTrain:  epoch  3, batch     2 | loss: 0.8075498Losses:  0.26810553669929504 0.21289995312690735
MemoryTrain:  epoch  3, batch     3 | loss: 0.4810055Losses:  0.38838866353034973 0.3510391116142273
MemoryTrain:  epoch  3, batch     4 | loss: 0.7394278Losses:  0.3298407793045044 0.431733101606369
MemoryTrain:  epoch  3, batch     5 | loss: 0.7615739Losses:  0.4519307613372803 0.06550540775060654
MemoryTrain:  epoch  3, batch     6 | loss: 0.5174361Losses:  0.4894683361053467 0.4667908549308777
MemoryTrain:  epoch  4, batch     0 | loss: 0.9562592Losses:  0.3665812015533447 0.46787697076797485
MemoryTrain:  epoch  4, batch     1 | loss: 0.8344582Losses:  0.3079281449317932 0.3344529867172241
MemoryTrain:  epoch  4, batch     2 | loss: 0.6423811Losses:  0.39546239376068115 0.2804528474807739
MemoryTrain:  epoch  4, batch     3 | loss: 0.6759152Losses:  0.345403254032135 0.38873326778411865
MemoryTrain:  epoch  4, batch     4 | loss: 0.7341365Losses:  0.40053826570510864 0.2864301800727844
MemoryTrain:  epoch  4, batch     5 | loss: 0.6869684Losses:  0.28643327951431274 0.09031549841165543
MemoryTrain:  epoch  4, batch     6 | loss: 0.3767488Losses:  0.2621309161186218 0.2672339081764221
MemoryTrain:  epoch  5, batch     0 | loss: 0.5293648Losses:  0.28726616501808167 0.37783724069595337
MemoryTrain:  epoch  5, batch     1 | loss: 0.6651034Losses:  0.39850056171417236 0.3744710683822632
MemoryTrain:  epoch  5, batch     2 | loss: 0.7729716Losses:  0.32534876465797424 0.3637600541114807
MemoryTrain:  epoch  5, batch     3 | loss: 0.6891088Losses:  0.22116559743881226 0.27405625581741333
MemoryTrain:  epoch  5, batch     4 | loss: 0.4952219Losses:  0.3896932899951935 0.4451749920845032
MemoryTrain:  epoch  5, batch     5 | loss: 0.8348683Losses:  0.5589504837989807 0.09822793304920197
MemoryTrain:  epoch  5, batch     6 | loss: 0.6571784Losses:  0.27349480986595154 0.32712340354919434
MemoryTrain:  epoch  6, batch     0 | loss: 0.6006182Losses:  0.22796319425106049 0.3359026312828064
MemoryTrain:  epoch  6, batch     1 | loss: 0.5638658Losses:  0.32326382398605347 0.31429269909858704
MemoryTrain:  epoch  6, batch     2 | loss: 0.6375566Losses:  0.31409770250320435 0.3345845937728882
MemoryTrain:  epoch  6, batch     3 | loss: 0.6486823Losses:  0.29425761103630066 0.3610605001449585
MemoryTrain:  epoch  6, batch     4 | loss: 0.6553181Losses:  0.30598145723342896 0.4666745662689209
MemoryTrain:  epoch  6, batch     5 | loss: 0.7726560Losses:  0.3465847373008728 0.12142477929592133
MemoryTrain:  epoch  6, batch     6 | loss: 0.4680095Losses:  0.24125342071056366 0.2784496545791626
MemoryTrain:  epoch  7, batch     0 | loss: 0.5197031Losses:  0.27664685249328613 0.21285496652126312
MemoryTrain:  epoch  7, batch     1 | loss: 0.4895018Losses:  0.31948262453079224 0.3491237163543701
MemoryTrain:  epoch  7, batch     2 | loss: 0.6686063Losses:  0.31474947929382324 0.4054024815559387
MemoryTrain:  epoch  7, batch     3 | loss: 0.7201520Losses:  0.3228573501110077 0.42851418256759644
MemoryTrain:  epoch  7, batch     4 | loss: 0.7513715Losses:  0.25781017541885376 0.3017076253890991
MemoryTrain:  epoch  7, batch     5 | loss: 0.5595178Losses:  0.4136989414691925 0.04164600744843483
MemoryTrain:  epoch  7, batch     6 | loss: 0.4553449Losses:  0.3875739574432373 0.49727967381477356
MemoryTrain:  epoch  8, batch     0 | loss: 0.8848536Losses:  0.31011056900024414 0.3370811343193054
MemoryTrain:  epoch  8, batch     1 | loss: 0.6471917Losses:  0.2552676796913147 0.3120172917842865
MemoryTrain:  epoch  8, batch     2 | loss: 0.5672849Losses:  0.24293285608291626 0.22646600008010864
MemoryTrain:  epoch  8, batch     3 | loss: 0.4693989Losses:  0.23269057273864746 0.25258806347846985
MemoryTrain:  epoch  8, batch     4 | loss: 0.4852786Losses:  0.2560742497444153 0.3647608160972595
MemoryTrain:  epoch  8, batch     5 | loss: 0.6208351Losses:  0.41173142194747925 0.04599710553884506
MemoryTrain:  epoch  8, batch     6 | loss: 0.4577285Losses:  0.2743287682533264 0.39096125960350037
MemoryTrain:  epoch  9, batch     0 | loss: 0.6652900Losses:  0.2529803514480591 0.29564428329467773
MemoryTrain:  epoch  9, batch     1 | loss: 0.5486246Losses:  0.28323808312416077 0.2863169312477112
MemoryTrain:  epoch  9, batch     2 | loss: 0.5695550Losses:  0.2936166524887085 0.5538679361343384
MemoryTrain:  epoch  9, batch     3 | loss: 0.8474846Losses:  0.28899359703063965 0.3217662572860718
MemoryTrain:  epoch  9, batch     4 | loss: 0.6107599Losses:  0.29460522532463074 0.33001214265823364
MemoryTrain:  epoch  9, batch     5 | loss: 0.6246173Losses:  0.1171962320804596 0.007359690964221954
MemoryTrain:  epoch  9, batch     6 | loss: 0.1245559
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.19%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 81.50%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.91%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.80%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.14%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 82.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.55%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 89.58%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.40%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.10%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 86.90%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 86.33%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 84.89%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 84.65%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 84.42%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.51%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 84.59%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.63%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.67%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 83.96%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 83.20%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 82.21%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 81.41%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 80.70%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 79.86%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 80.61%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 79.71%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 78.82%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.95%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 77.24%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 76.41%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 75.86%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 76.05%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 75.42%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 74.94%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 74.25%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 73.98%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 73.65%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 73.10%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 72.68%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 72.23%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 71.98%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 71.79%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 71.61%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.95%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 72.42%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 71.58%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 71.32%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 70.82%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 70.42%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.85%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 71.51%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 71.31%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 70.92%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 70.67%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 72.34%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 73.11%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 72.87%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 72.69%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.67%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.38%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 73.31%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 73.29%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 73.26%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 73.10%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 72.97%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 72.98%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 72.91%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 72.62%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 72.43%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 72.15%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 72.04%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 71.83%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 71.68%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 71.80%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 72.23%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 72.10%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 71.91%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 71.68%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 71.46%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.30%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 71.14%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 71.04%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.56%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 72.54%   [EVAL] batch:  231 | acc: 37.50%,  total acc: 72.39%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 72.21%   [EVAL] batch:  233 | acc: 18.75%,  total acc: 71.98%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 71.76%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 71.66%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 71.49%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.51%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 71.76%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 71.36%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 71.17%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 71.84%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 71.90%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 71.91%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 71.60%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 71.43%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 71.23%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 70.97%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 70.80%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 70.64%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 72.70%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:  303 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.04%   
cur_acc:  ['0.9464', '0.7103', '0.7798', '0.6359', '0.8155']
his_acc:  ['0.9464', '0.8290', '0.7892', '0.7282', '0.7304']
Clustering into  28  clusters
Clusters:  [ 5 19 22  5  5  5 26  5 18  1  0 24  4  5  5 15  5 25  5 27  5 20  5  5
  2  5  5  5 17 21  4  5 16  5  8 14  7  5  5  5  3 10  5  5 13  0 23  5
  5  9  5  5  5 12  5  1  5  6 11  2]
Losses:  6.810383319854736 1.1907587051391602
CurrentTrain: epoch  0, batch     0 | loss: 8.0011425Losses:  7.989411354064941 0.8303899765014648
CurrentTrain: epoch  0, batch     1 | loss: 8.8198013Losses:  9.110086441040039 0.871658205986023
CurrentTrain: epoch  0, batch     2 | loss: 9.9817448Losses:  8.005558013916016 1.0805895328521729
CurrentTrain: epoch  0, batch     3 | loss: 9.0861473Losses:  8.142579078674316 1.1723637580871582
CurrentTrain: epoch  0, batch     4 | loss: 9.3149433Losses:  6.611171722412109 0.42599746584892273
CurrentTrain: epoch  0, batch     5 | loss: 7.0371690Losses:  5.937490940093994 0.3998255729675293
CurrentTrain: epoch  0, batch     6 | loss: 6.3373165Losses:  4.290112495422363 0.8156470060348511
CurrentTrain: epoch  1, batch     0 | loss: 5.1057596Losses:  3.2046873569488525 0.9057711362838745
CurrentTrain: epoch  1, batch     1 | loss: 4.1104584Losses:  3.672666549682617 0.8424007892608643
CurrentTrain: epoch  1, batch     2 | loss: 4.5150671Losses:  4.031641960144043 0.7932130098342896
CurrentTrain: epoch  1, batch     3 | loss: 4.8248549Losses:  2.6742889881134033 0.4391040503978729
CurrentTrain: epoch  1, batch     4 | loss: 3.1133931Losses:  3.5699286460876465 0.777640700340271
CurrentTrain: epoch  1, batch     5 | loss: 4.3475695Losses:  2.7079386711120605 0.07846727967262268
CurrentTrain: epoch  1, batch     6 | loss: 2.7864060Losses:  3.2696421146392822 0.8116717338562012
CurrentTrain: epoch  2, batch     0 | loss: 4.0813141Losses:  3.532846212387085 0.48290279507637024
CurrentTrain: epoch  2, batch     1 | loss: 4.0157490Losses:  3.2268929481506348 0.6054337024688721
CurrentTrain: epoch  2, batch     2 | loss: 3.8323267Losses:  2.7271103858947754 0.5069375038146973
CurrentTrain: epoch  2, batch     3 | loss: 3.2340479Losses:  2.4257071018218994 0.57674241065979
CurrentTrain: epoch  2, batch     4 | loss: 3.0024495Losses:  2.291609764099121 0.4425606429576874
CurrentTrain: epoch  2, batch     5 | loss: 2.7341704Losses:  3.172006607055664 0.3751421272754669
CurrentTrain: epoch  2, batch     6 | loss: 3.5471487Losses:  2.8335273265838623 0.7331942915916443
CurrentTrain: epoch  3, batch     0 | loss: 3.5667217Losses:  2.5819156169891357 0.7115484476089478
CurrentTrain: epoch  3, batch     1 | loss: 3.2934642Losses:  2.1967613697052 0.33166539669036865
CurrentTrain: epoch  3, batch     2 | loss: 2.5284266Losses:  2.6034648418426514 0.4520249366760254
CurrentTrain: epoch  3, batch     3 | loss: 3.0554898Losses:  2.516589641571045 0.519641101360321
CurrentTrain: epoch  3, batch     4 | loss: 3.0362308Losses:  2.302194595336914 0.4715501666069031
CurrentTrain: epoch  3, batch     5 | loss: 2.7737448Losses:  2.783262252807617 0.2047966867685318
CurrentTrain: epoch  3, batch     6 | loss: 2.9880590Losses:  2.2306973934173584 0.53846275806427
CurrentTrain: epoch  4, batch     0 | loss: 2.7691603Losses:  2.5898854732513428 0.5672047138214111
CurrentTrain: epoch  4, batch     1 | loss: 3.1570902Losses:  2.011439323425293 0.42632409930229187
CurrentTrain: epoch  4, batch     2 | loss: 2.4377635Losses:  2.460604667663574 0.6658462882041931
CurrentTrain: epoch  4, batch     3 | loss: 3.1264510Losses:  2.284231662750244 0.33102428913116455
CurrentTrain: epoch  4, batch     4 | loss: 2.6152558Losses:  2.294323444366455 0.313101589679718
CurrentTrain: epoch  4, batch     5 | loss: 2.6074250Losses:  1.8242921829223633 0.028877245262265205
CurrentTrain: epoch  4, batch     6 | loss: 1.8531694Losses:  2.304581642150879 0.3503758907318115
CurrentTrain: epoch  5, batch     0 | loss: 2.6549575Losses:  1.9445449113845825 0.32765981554985046
CurrentTrain: epoch  5, batch     1 | loss: 2.2722046Losses:  2.0723912715911865 0.4675515294075012
CurrentTrain: epoch  5, batch     2 | loss: 2.5399427Losses:  2.4602863788604736 0.4998354911804199
CurrentTrain: epoch  5, batch     3 | loss: 2.9601219Losses:  2.050961494445801 0.4010489583015442
CurrentTrain: epoch  5, batch     4 | loss: 2.4520104Losses:  2.072350263595581 0.38771525025367737
CurrentTrain: epoch  5, batch     5 | loss: 2.4600656Losses:  1.8951225280761719 0.06301116943359375
CurrentTrain: epoch  5, batch     6 | loss: 1.9581337Losses:  2.1851918697357178 0.3980657458305359
CurrentTrain: epoch  6, batch     0 | loss: 2.5832577Losses:  2.4774298667907715 0.376189649105072
CurrentTrain: epoch  6, batch     1 | loss: 2.8536196Losses:  1.8568198680877686 0.3402600884437561
CurrentTrain: epoch  6, batch     2 | loss: 2.1970799Losses:  1.820059061050415 0.32904893159866333
CurrentTrain: epoch  6, batch     3 | loss: 2.1491079Losses:  1.7755753993988037 0.32122600078582764
CurrentTrain: epoch  6, batch     4 | loss: 2.0968013Losses:  1.808631420135498 0.246923565864563
CurrentTrain: epoch  6, batch     5 | loss: 2.0555549Losses:  1.7697958946228027 0.1616305112838745
CurrentTrain: epoch  6, batch     6 | loss: 1.9314264Losses:  1.8568201065063477 0.4008053243160248
CurrentTrain: epoch  7, batch     0 | loss: 2.2576253Losses:  1.9335436820983887 0.5731164813041687
CurrentTrain: epoch  7, batch     1 | loss: 2.5066602Losses:  1.9899601936340332 0.18190601468086243
CurrentTrain: epoch  7, batch     2 | loss: 2.1718662Losses:  2.0253260135650635 0.3119465708732605
CurrentTrain: epoch  7, batch     3 | loss: 2.3372726Losses:  1.763161540031433 0.39720144867897034
CurrentTrain: epoch  7, batch     4 | loss: 2.1603630Losses:  1.7677826881408691 0.2001141607761383
CurrentTrain: epoch  7, batch     5 | loss: 1.9678968Losses:  1.6922527551651 0.15334846079349518
CurrentTrain: epoch  7, batch     6 | loss: 1.8456012Losses:  1.7509541511535645 0.18664714694023132
CurrentTrain: epoch  8, batch     0 | loss: 1.9376013Losses:  1.8067548274993896 0.28987985849380493
CurrentTrain: epoch  8, batch     1 | loss: 2.0966346Losses:  2.05141019821167 0.4572048783302307
CurrentTrain: epoch  8, batch     2 | loss: 2.5086150Losses:  1.7102744579315186 0.35541918873786926
CurrentTrain: epoch  8, batch     3 | loss: 2.0656936Losses:  1.7860945463180542 0.33770182728767395
CurrentTrain: epoch  8, batch     4 | loss: 2.1237965Losses:  1.769989252090454 0.319186806678772
CurrentTrain: epoch  8, batch     5 | loss: 2.0891762Losses:  1.763140082359314 0.06845565140247345
CurrentTrain: epoch  8, batch     6 | loss: 1.8315958Losses:  1.8045587539672852 0.24469640851020813
CurrentTrain: epoch  9, batch     0 | loss: 2.0492551Losses:  1.6824836730957031 0.1877354383468628
CurrentTrain: epoch  9, batch     1 | loss: 1.8702191Losses:  1.7607148885726929 0.28342628479003906
CurrentTrain: epoch  9, batch     2 | loss: 2.0441413Losses:  1.881484031677246 0.19190828502178192
CurrentTrain: epoch  9, batch     3 | loss: 2.0733924Losses:  1.9400650262832642 0.4337211847305298
CurrentTrain: epoch  9, batch     4 | loss: 2.3737862Losses:  1.730665683746338 0.24250809848308563
CurrentTrain: epoch  9, batch     5 | loss: 1.9731737Losses:  1.655957579612732 0.015141796320676804
CurrentTrain: epoch  9, batch     6 | loss: 1.6710994
Losses:  6.095193386077881 0.35514116287231445
MemoryTrain:  epoch  0, batch     0 | loss: 6.4503345Losses:  8.34683609008789 0.40064555406570435
MemoryTrain:  epoch  0, batch     1 | loss: 8.7474813Losses:  8.744470596313477 0.5053482055664062
MemoryTrain:  epoch  0, batch     2 | loss: 9.2498188Losses:  9.792535781860352 0.4258045256137848
MemoryTrain:  epoch  0, batch     3 | loss: 10.2183399Losses:  10.301692008972168 0.33633700013160706
MemoryTrain:  epoch  0, batch     4 | loss: 10.6380291Losses:  10.165666580200195 0.3440067172050476
MemoryTrain:  epoch  0, batch     5 | loss: 10.5096731Losses:  11.06173324584961 0.3312898278236389
MemoryTrain:  epoch  0, batch     6 | loss: 11.3930235Losses:  10.926838874816895 0.20363424718379974
MemoryTrain:  epoch  0, batch     7 | loss: 11.1304731Losses:  1.1493754386901855 0.386807382106781
MemoryTrain:  epoch  1, batch     0 | loss: 1.5361829Losses:  2.3009397983551025 0.48027393221855164
MemoryTrain:  epoch  1, batch     1 | loss: 2.7812138Losses:  0.8621127009391785 0.2751784920692444
MemoryTrain:  epoch  1, batch     2 | loss: 1.1372912Losses:  0.4842195212841034 0.29542216658592224
MemoryTrain:  epoch  1, batch     3 | loss: 0.7796417Losses:  0.9062020778656006 0.3202914595603943
MemoryTrain:  epoch  1, batch     4 | loss: 1.2264936Losses:  0.8367053270339966 0.3267916440963745
MemoryTrain:  epoch  1, batch     5 | loss: 1.1634970Losses:  0.8354562520980835 0.48049575090408325
MemoryTrain:  epoch  1, batch     6 | loss: 1.3159521Losses:  0.5265852212905884 0.2009461224079132
MemoryTrain:  epoch  1, batch     7 | loss: 0.7275313Losses:  0.5244737267494202 0.3063557744026184
MemoryTrain:  epoch  2, batch     0 | loss: 0.8308295Losses:  1.1654386520385742 0.3733508884906769
MemoryTrain:  epoch  2, batch     1 | loss: 1.5387895Losses:  0.7315991520881653 0.3158213794231415
MemoryTrain:  epoch  2, batch     2 | loss: 1.0474205Losses:  0.6070443391799927 0.3694628179073334
MemoryTrain:  epoch  2, batch     3 | loss: 0.9765072Losses:  0.5114688873291016 0.25193750858306885
MemoryTrain:  epoch  2, batch     4 | loss: 0.7634064Losses:  0.8554320335388184 0.33188554644584656
MemoryTrain:  epoch  2, batch     5 | loss: 1.1873176Losses:  0.7907372713088989 0.33769842982292175
MemoryTrain:  epoch  2, batch     6 | loss: 1.1284357Losses:  0.7405871152877808 0.24452246725559235
MemoryTrain:  epoch  2, batch     7 | loss: 0.9851096Losses:  0.35257649421691895 0.3474581837654114
MemoryTrain:  epoch  3, batch     0 | loss: 0.7000347Losses:  0.9497766494750977 0.40069207549095154
MemoryTrain:  epoch  3, batch     1 | loss: 1.3504688Losses:  0.8150506615638733 0.271589457988739
MemoryTrain:  epoch  3, batch     2 | loss: 1.0866401Losses:  0.4250822067260742 0.32712507247924805
MemoryTrain:  epoch  3, batch     3 | loss: 0.7522073Losses:  0.5821407437324524 0.46581554412841797
MemoryTrain:  epoch  3, batch     4 | loss: 1.0479562Losses:  0.5625358819961548 0.4495803117752075
MemoryTrain:  epoch  3, batch     5 | loss: 1.0121162Losses:  0.4575303792953491 0.3362482786178589
MemoryTrain:  epoch  3, batch     6 | loss: 0.7937787Losses:  0.35950392484664917 0.12346448004245758
MemoryTrain:  epoch  3, batch     7 | loss: 0.4829684Losses:  0.41452378034591675 0.3420390486717224
MemoryTrain:  epoch  4, batch     0 | loss: 0.7565628Losses:  0.4538400173187256 0.33262407779693604
MemoryTrain:  epoch  4, batch     1 | loss: 0.7864641Losses:  0.47729694843292236 0.2721822261810303
MemoryTrain:  epoch  4, batch     2 | loss: 0.7494792Losses:  0.31862393021583557 0.39637434482574463
MemoryTrain:  epoch  4, batch     3 | loss: 0.7149982Losses:  0.45662736892700195 0.4118592143058777
MemoryTrain:  epoch  4, batch     4 | loss: 0.8684866Losses:  0.6828427314758301 0.47378236055374146
MemoryTrain:  epoch  4, batch     5 | loss: 1.1566250Losses:  0.47452878952026367 0.22931097447872162
MemoryTrain:  epoch  4, batch     6 | loss: 0.7038398Losses:  0.20054930448532104 0.12498839199542999
MemoryTrain:  epoch  4, batch     7 | loss: 0.3255377Losses:  0.4567010998725891 0.26379668712615967
MemoryTrain:  epoch  5, batch     0 | loss: 0.7204978Losses:  0.39146721363067627 0.3394352197647095
MemoryTrain:  epoch  5, batch     1 | loss: 0.7309024Losses:  0.39791619777679443 0.48056069016456604
MemoryTrain:  epoch  5, batch     2 | loss: 0.8784769Losses:  0.35718846321105957 0.3632334768772125
MemoryTrain:  epoch  5, batch     3 | loss: 0.7204219Losses:  0.29121413826942444 0.24793429672718048
MemoryTrain:  epoch  5, batch     4 | loss: 0.5391484Losses:  0.4454347491264343 0.33575835824012756
MemoryTrain:  epoch  5, batch     5 | loss: 0.7811931Losses:  0.298919141292572 0.32408246397972107
MemoryTrain:  epoch  5, batch     6 | loss: 0.6230016Losses:  0.4046160876750946 0.17328056693077087
MemoryTrain:  epoch  5, batch     7 | loss: 0.5778967Losses:  0.3292657136917114 0.3079831600189209
MemoryTrain:  epoch  6, batch     0 | loss: 0.6372489Losses:  0.4879252314567566 0.6751848459243774
MemoryTrain:  epoch  6, batch     1 | loss: 1.1631100Losses:  0.3814846873283386 0.23810335993766785
MemoryTrain:  epoch  6, batch     2 | loss: 0.6195880Losses:  0.28833043575286865 0.2013714611530304
MemoryTrain:  epoch  6, batch     3 | loss: 0.4897019Losses:  0.3209885358810425 0.2459474503993988
MemoryTrain:  epoch  6, batch     4 | loss: 0.5669360Losses:  0.4834093451499939 0.35051029920578003
MemoryTrain:  epoch  6, batch     5 | loss: 0.8339196Losses:  0.30394065380096436 0.2852381765842438
MemoryTrain:  epoch  6, batch     6 | loss: 0.5891788Losses:  0.46762388944625854 0.28784501552581787
MemoryTrain:  epoch  6, batch     7 | loss: 0.7554689Losses:  0.38559895753860474 0.3748839199542999
MemoryTrain:  epoch  7, batch     0 | loss: 0.7604829Losses:  0.2281171977519989 0.19328872859477997
MemoryTrain:  epoch  7, batch     1 | loss: 0.4214059Losses:  0.37068018317222595 0.44650012254714966
MemoryTrain:  epoch  7, batch     2 | loss: 0.8171803Losses:  0.33690014481544495 0.3997310400009155
MemoryTrain:  epoch  7, batch     3 | loss: 0.7366312Losses:  0.300706148147583 0.3420133590698242
MemoryTrain:  epoch  7, batch     4 | loss: 0.6427195Losses:  0.3424483835697174 0.3970371186733246
MemoryTrain:  epoch  7, batch     5 | loss: 0.7394855Losses:  0.2816392779350281 0.23485860228538513
MemoryTrain:  epoch  7, batch     6 | loss: 0.5164979Losses:  0.51689213514328 0.15394070744514465
MemoryTrain:  epoch  7, batch     7 | loss: 0.6708329Losses:  0.29485464096069336 0.27885690331459045
MemoryTrain:  epoch  8, batch     0 | loss: 0.5737115Losses:  0.3235040605068207 0.2543467879295349
MemoryTrain:  epoch  8, batch     1 | loss: 0.5778508Losses:  0.29028475284576416 0.24085009098052979
MemoryTrain:  epoch  8, batch     2 | loss: 0.5311348Losses:  0.41351836919784546 0.4389858841896057
MemoryTrain:  epoch  8, batch     3 | loss: 0.8525043Losses:  0.30762016773223877 0.31790104508399963
MemoryTrain:  epoch  8, batch     4 | loss: 0.6255212Losses:  0.3640556335449219 0.49276939034461975
MemoryTrain:  epoch  8, batch     5 | loss: 0.8568250Losses:  0.3024597764015198 0.265350878238678
MemoryTrain:  epoch  8, batch     6 | loss: 0.5678107Losses:  0.2508770227432251 0.12217546999454498
MemoryTrain:  epoch  8, batch     7 | loss: 0.3730525Losses:  0.3414935767650604 0.3992178738117218
MemoryTrain:  epoch  9, batch     0 | loss: 0.7407115Losses:  0.325392484664917 0.33135786652565
MemoryTrain:  epoch  9, batch     1 | loss: 0.6567503Losses:  0.24770407378673553 0.2616423964500427
MemoryTrain:  epoch  9, batch     2 | loss: 0.5093465Losses:  0.2742072343826294 0.27241501212120056
MemoryTrain:  epoch  9, batch     3 | loss: 0.5466223Losses:  0.26534491777420044 0.24046370387077332
MemoryTrain:  epoch  9, batch     4 | loss: 0.5058086Losses:  0.3940567672252655 0.5109447836875916
MemoryTrain:  epoch  9, batch     5 | loss: 0.9050015Losses:  0.241819366812706 0.22802579402923584
MemoryTrain:  epoch  9, batch     6 | loss: 0.4698452Losses:  0.30552563071250916 0.1452789008617401
MemoryTrain:  epoch  9, batch     7 | loss: 0.4508045
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 73.14%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 67.95%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 66.93%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 65.94%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.73%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.21%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 88.27%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 87.18%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 86.55%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 86.04%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 85.66%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 85.18%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 84.72%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 83.46%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 82.74%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 82.25%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 82.31%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 82.33%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 81.41%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 80.52%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 79.57%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 78.80%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 77.31%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 77.36%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 78.59%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 78.27%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 77.39%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 76.53%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.69%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 74.19%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.21%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 73.98%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 73.94%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 73.71%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 73.22%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 72.95%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 72.75%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 72.54%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 71.98%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 71.34%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 70.89%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.38%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 70.96%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 70.51%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.78%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 70.55%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 70.59%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 70.17%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 70.49%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 70.30%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 69.97%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 69.44%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.36%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 69.21%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 69.08%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 68.72%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 68.62%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 68.37%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 68.39%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 67.89%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 67.71%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 67.57%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 67.49%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 69.30%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 69.37%   [EVAL] batch:  231 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:  233 | acc: 18.75%,  total acc: 68.80%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 68.59%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 68.62%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 68.39%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 68.12%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 67.89%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 67.67%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 67.97%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 67.74%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 67.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:  301 | acc: 56.25%,  total acc: 69.70%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 69.72%   [EVAL] batch:  303 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 69.65%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 69.65%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:  314 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:  316 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 70.74%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.76%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 70.84%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 70.76%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 70.33%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 70.36%   [EVAL] batch:  345 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.43%   [EVAL] batch:  347 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 70.38%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 70.36%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 70.36%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 70.27%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 70.11%   [EVAL] batch:  358 | acc: 12.50%,  total acc: 69.95%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 69.79%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 69.67%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 69.56%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 70.10%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 70.35%   
cur_acc:  ['0.9464', '0.7103', '0.7798', '0.6359', '0.8155', '0.7073']
his_acc:  ['0.9464', '0.8290', '0.7892', '0.7282', '0.7304', '0.7035']
Clustering into  34  clusters
Clusters:  [ 0  9 20  3  0  0 29  0 22  1 31 27  4  0  0 17  0 28  0 33  0 24  0  0
 16  0  0  0 21 25  4  0 18  0 19 26  8  0  0  0  3 13  0  0 30 14 23  0
  0 32  0  0  0 10  0  1  0  6 12 15  9 11  0  0  0  5  0  0  2  7]
Losses:  7.476261138916016 1.0413086414337158
CurrentTrain: epoch  0, batch     0 | loss: 8.5175695Losses:  7.827677249908447 0.7896290421485901
CurrentTrain: epoch  0, batch     1 | loss: 8.6173067Losses:  8.443948745727539 1.1698176860809326
CurrentTrain: epoch  0, batch     2 | loss: 9.6137667Losses:  6.894700527191162 1.2534809112548828
CurrentTrain: epoch  0, batch     3 | loss: 8.1481819Losses:  7.977025508880615 0.8717721700668335
CurrentTrain: epoch  0, batch     4 | loss: 8.8487978Losses:  5.046245098114014 0.9490289688110352
CurrentTrain: epoch  0, batch     5 | loss: 5.9952741Losses:  5.6320695877075195 0.5834153294563293
CurrentTrain: epoch  0, batch     6 | loss: 6.2154851Losses:  4.219814300537109 0.931135892868042
CurrentTrain: epoch  1, batch     0 | loss: 5.1509504Losses:  3.3397984504699707 0.8209494352340698
CurrentTrain: epoch  1, batch     1 | loss: 4.1607480Losses:  2.9240505695343018 0.5787702798843384
CurrentTrain: epoch  1, batch     2 | loss: 3.5028210Losses:  3.265110731124878 0.651433527469635
CurrentTrain: epoch  1, batch     3 | loss: 3.9165442Losses:  3.3226962089538574 0.8014163970947266
CurrentTrain: epoch  1, batch     4 | loss: 4.1241126Losses:  2.8254570960998535 0.9016033411026001
CurrentTrain: epoch  1, batch     5 | loss: 3.7270603Losses:  1.7873756885528564 0.15470336377620697
CurrentTrain: epoch  1, batch     6 | loss: 1.9420791Losses:  2.699944019317627 0.7202265858650208
CurrentTrain: epoch  2, batch     0 | loss: 3.4201705Losses:  2.5897693634033203 0.5860943794250488
CurrentTrain: epoch  2, batch     1 | loss: 3.1758637Losses:  2.486342430114746 0.6468690037727356
CurrentTrain: epoch  2, batch     2 | loss: 3.1332114Losses:  3.338313102722168 0.8123935461044312
CurrentTrain: epoch  2, batch     3 | loss: 4.1507068Losses:  2.166653871536255 0.43026620149612427
CurrentTrain: epoch  2, batch     4 | loss: 2.5969200Losses:  3.7082486152648926 0.6148602962493896
CurrentTrain: epoch  2, batch     5 | loss: 4.3231087Losses:  2.0646753311157227 0.14520427584648132
CurrentTrain: epoch  2, batch     6 | loss: 2.2098796Losses:  2.647369384765625 0.7653343677520752
CurrentTrain: epoch  3, batch     0 | loss: 3.4127038Losses:  2.7843284606933594 0.7168353199958801
CurrentTrain: epoch  3, batch     1 | loss: 3.5011637Losses:  2.4096016883850098 0.39964696764945984
CurrentTrain: epoch  3, batch     2 | loss: 2.8092487Losses:  2.8941683769226074 0.63676917552948
CurrentTrain: epoch  3, batch     3 | loss: 3.5309377Losses:  2.246605157852173 0.5949433445930481
CurrentTrain: epoch  3, batch     4 | loss: 2.8415484Losses:  2.2024166584014893 0.45324623584747314
CurrentTrain: epoch  3, batch     5 | loss: 2.6556630Losses:  2.2018890380859375 0.06932581961154938
CurrentTrain: epoch  3, batch     6 | loss: 2.2712150Losses:  2.8749961853027344 0.6390701532363892
CurrentTrain: epoch  4, batch     0 | loss: 3.5140662Losses:  2.644247531890869 0.7066521644592285
CurrentTrain: epoch  4, batch     1 | loss: 3.3508997Losses:  1.8723552227020264 0.3663609027862549
CurrentTrain: epoch  4, batch     2 | loss: 2.2387161Losses:  2.1953673362731934 0.6040374040603638
CurrentTrain: epoch  4, batch     3 | loss: 2.7994046Losses:  2.134716510772705 0.5023507475852966
CurrentTrain: epoch  4, batch     4 | loss: 2.6370673Losses:  1.9605720043182373 0.34312713146209717
CurrentTrain: epoch  4, batch     5 | loss: 2.3036990Losses:  2.2667312622070312 0.08258084952831268
CurrentTrain: epoch  4, batch     6 | loss: 2.3493121Losses:  1.8979427814483643 0.3942407965660095
CurrentTrain: epoch  5, batch     0 | loss: 2.2921836Losses:  2.1739015579223633 0.4443967342376709
CurrentTrain: epoch  5, batch     1 | loss: 2.6182983Losses:  1.8523643016815186 0.4041355550289154
CurrentTrain: epoch  5, batch     2 | loss: 2.2564998Losses:  2.0838356018066406 0.43920233845710754
CurrentTrain: epoch  5, batch     3 | loss: 2.5230379Losses:  2.2003581523895264 0.6755656003952026
CurrentTrain: epoch  5, batch     4 | loss: 2.8759236Losses:  1.867401361465454 0.347898006439209
CurrentTrain: epoch  5, batch     5 | loss: 2.2152994Losses:  3.5897436141967773 8.94069742685133e-08
CurrentTrain: epoch  5, batch     6 | loss: 3.5897436Losses:  1.9809328317642212 0.522339940071106
CurrentTrain: epoch  6, batch     0 | loss: 2.5032728Losses:  2.1408843994140625 0.5309476256370544
CurrentTrain: epoch  6, batch     1 | loss: 2.6718321Losses:  1.851893663406372 0.41024065017700195
CurrentTrain: epoch  6, batch     2 | loss: 2.2621343Losses:  1.8203754425048828 0.42032748460769653
CurrentTrain: epoch  6, batch     3 | loss: 2.2407029Losses:  1.8901729583740234 0.46837517619132996
CurrentTrain: epoch  6, batch     4 | loss: 2.3585482Losses:  1.8224701881408691 0.37639790773391724
CurrentTrain: epoch  6, batch     5 | loss: 2.1988680Losses:  2.971548080444336 0.31182199716567993
CurrentTrain: epoch  6, batch     6 | loss: 3.2833700Losses:  1.9622267484664917 0.4231160879135132
CurrentTrain: epoch  7, batch     0 | loss: 2.3853428Losses:  1.7441444396972656 0.32718661427497864
CurrentTrain: epoch  7, batch     1 | loss: 2.0713310Losses:  1.9448883533477783 0.3985898494720459
CurrentTrain: epoch  7, batch     2 | loss: 2.3434782Losses:  2.0159335136413574 0.5665009617805481
CurrentTrain: epoch  7, batch     3 | loss: 2.5824344Losses:  1.8094334602355957 0.3336217403411865
CurrentTrain: epoch  7, batch     4 | loss: 2.1430552Losses:  1.7405693531036377 0.33683791756629944
CurrentTrain: epoch  7, batch     5 | loss: 2.0774074Losses:  1.7088038921356201 0.052771467715501785
CurrentTrain: epoch  7, batch     6 | loss: 1.7615753Losses:  1.8414133787155151 0.3041974902153015
CurrentTrain: epoch  8, batch     0 | loss: 2.1456108Losses:  1.804667592048645 0.4020085334777832
CurrentTrain: epoch  8, batch     1 | loss: 2.2066760Losses:  1.7898008823394775 0.3368094563484192
CurrentTrain: epoch  8, batch     2 | loss: 2.1266103Losses:  1.7907078266143799 0.2868531048297882
CurrentTrain: epoch  8, batch     3 | loss: 2.0775609Losses:  1.7822446823120117 0.49920475482940674
CurrentTrain: epoch  8, batch     4 | loss: 2.2814493Losses:  1.753063678741455 0.3074970245361328
CurrentTrain: epoch  8, batch     5 | loss: 2.0605607Losses:  1.761364459991455 0.08855652809143066
CurrentTrain: epoch  8, batch     6 | loss: 1.8499210Losses:  1.7482154369354248 0.43343275785446167
CurrentTrain: epoch  9, batch     0 | loss: 2.1816483Losses:  1.7482714653015137 0.3675084114074707
CurrentTrain: epoch  9, batch     1 | loss: 2.1157799Losses:  1.727051019668579 0.35945481061935425
CurrentTrain: epoch  9, batch     2 | loss: 2.0865059Losses:  1.7304598093032837 0.23803558945655823
CurrentTrain: epoch  9, batch     3 | loss: 1.9684954Losses:  1.6886954307556152 0.10777534544467926
CurrentTrain: epoch  9, batch     4 | loss: 1.7964708Losses:  1.8107316493988037 0.37648311257362366
CurrentTrain: epoch  9, batch     5 | loss: 2.1872149Losses:  1.6993625164031982 0.049773864448070526
CurrentTrain: epoch  9, batch     6 | loss: 1.7491363
Losses:  5.862804412841797 0.30098822712898254
MemoryTrain:  epoch  0, batch     0 | loss: 6.1637926Losses:  8.290571212768555 0.4748336374759674
MemoryTrain:  epoch  0, batch     1 | loss: 8.7654047Losses:  8.615753173828125 0.2960306406021118
MemoryTrain:  epoch  0, batch     2 | loss: 8.9117842Losses:  9.182964324951172 0.3272404074668884
MemoryTrain:  epoch  0, batch     3 | loss: 9.5102043Losses:  10.79841136932373 0.37820035219192505
MemoryTrain:  epoch  0, batch     4 | loss: 11.1766119Losses:  10.57247543334961 0.6070365905761719
MemoryTrain:  epoch  0, batch     5 | loss: 11.1795120Losses:  12.292383193969727 0.37376511096954346
MemoryTrain:  epoch  0, batch     6 | loss: 12.6661482Losses:  11.415351867675781 0.3068928122520447
MemoryTrain:  epoch  0, batch     7 | loss: 11.7222443Losses:  10.448286056518555 0.23740026354789734
MemoryTrain:  epoch  0, batch     8 | loss: 10.6856861Losses:  0.7322598695755005 0.30705344676971436
MemoryTrain:  epoch  1, batch     0 | loss: 1.0393133Losses:  0.7018157839775085 0.35088101029396057
MemoryTrain:  epoch  1, batch     1 | loss: 1.0526968Losses:  1.6398632526397705 0.34052470326423645
MemoryTrain:  epoch  1, batch     2 | loss: 1.9803879Losses:  0.3416396379470825 0.2372315675020218
MemoryTrain:  epoch  1, batch     3 | loss: 0.5788712Losses:  1.282551884651184 0.44603002071380615
MemoryTrain:  epoch  1, batch     4 | loss: 1.7285819Losses:  0.9628946781158447 0.4692835807800293
MemoryTrain:  epoch  1, batch     5 | loss: 1.4321783Losses:  1.4318807125091553 0.43260878324508667
MemoryTrain:  epoch  1, batch     6 | loss: 1.8644896Losses:  1.7248731851577759 0.32301533222198486
MemoryTrain:  epoch  1, batch     7 | loss: 2.0478885Losses:  1.294413447380066 0.23732122778892517
MemoryTrain:  epoch  1, batch     8 | loss: 1.5317347Losses:  0.3840661942958832 0.3580601215362549
MemoryTrain:  epoch  2, batch     0 | loss: 0.7421263Losses:  1.2518662214279175 0.4720692038536072
MemoryTrain:  epoch  2, batch     1 | loss: 1.7239354Losses:  0.7886199951171875 0.32533231377601624
MemoryTrain:  epoch  2, batch     2 | loss: 1.1139523Losses:  0.5730150938034058 0.3420773446559906
MemoryTrain:  epoch  2, batch     3 | loss: 0.9150925Losses:  1.4376904964447021 0.5499339699745178
MemoryTrain:  epoch  2, batch     4 | loss: 1.9876244Losses:  0.6637140512466431 0.25648409128189087
MemoryTrain:  epoch  2, batch     5 | loss: 0.9201981Losses:  0.384279727935791 0.28284507989883423
MemoryTrain:  epoch  2, batch     6 | loss: 0.6671248Losses:  0.7872746586799622 0.27507510781288147
MemoryTrain:  epoch  2, batch     7 | loss: 1.0623498Losses:  0.8373392820358276 0.5589523315429688
MemoryTrain:  epoch  2, batch     8 | loss: 1.3962916Losses:  0.6487811803817749 0.41617846488952637
MemoryTrain:  epoch  3, batch     0 | loss: 1.0649596Losses:  1.0371891260147095 0.3069126009941101
MemoryTrain:  epoch  3, batch     1 | loss: 1.3441017Losses:  0.5098864436149597 0.47070547938346863
MemoryTrain:  epoch  3, batch     2 | loss: 0.9805919Losses:  0.34120500087738037 0.24736754596233368
MemoryTrain:  epoch  3, batch     3 | loss: 0.5885726Losses:  0.506111204624176 0.42581814527511597
MemoryTrain:  epoch  3, batch     4 | loss: 0.9319293Losses:  0.6350123882293701 0.5810708999633789
MemoryTrain:  epoch  3, batch     5 | loss: 1.2160833Losses:  0.7255929112434387 0.350285142660141
MemoryTrain:  epoch  3, batch     6 | loss: 1.0758780Losses:  0.3787941336631775 0.32945168018341064
MemoryTrain:  epoch  3, batch     7 | loss: 0.7082458Losses:  0.6848880648612976 0.20890340209007263
MemoryTrain:  epoch  3, batch     8 | loss: 0.8937914Losses:  0.5094020962715149 0.46767425537109375
MemoryTrain:  epoch  4, batch     0 | loss: 0.9770764Losses:  0.39798834919929504 0.3146827220916748
MemoryTrain:  epoch  4, batch     1 | loss: 0.7126710Losses:  0.7845134735107422 0.3936372697353363
MemoryTrain:  epoch  4, batch     2 | loss: 1.1781508Losses:  0.3760085105895996 0.35170888900756836
MemoryTrain:  epoch  4, batch     3 | loss: 0.7277174Losses:  0.49565058946609497 0.2755477726459503
MemoryTrain:  epoch  4, batch     4 | loss: 0.7711984Losses:  0.32719701528549194 0.2126598060131073
MemoryTrain:  epoch  4, batch     5 | loss: 0.5398568Losses:  0.2544742822647095 0.31170082092285156
MemoryTrain:  epoch  4, batch     6 | loss: 0.5661751Losses:  0.686192512512207 0.40901345014572144
MemoryTrain:  epoch  4, batch     7 | loss: 1.0952060Losses:  0.49309396743774414 0.3043661117553711
MemoryTrain:  epoch  4, batch     8 | loss: 0.7974601Losses:  0.33086034655570984 0.2688833475112915
MemoryTrain:  epoch  5, batch     0 | loss: 0.5997437Losses:  0.5978929996490479 0.4641619026660919
MemoryTrain:  epoch  5, batch     1 | loss: 1.0620549Losses:  0.43630552291870117 0.39843040704727173
MemoryTrain:  epoch  5, batch     2 | loss: 0.8347359Losses:  0.34736019372940063 0.24510681629180908
MemoryTrain:  epoch  5, batch     3 | loss: 0.5924670Losses:  0.3755117952823639 0.27508237957954407
MemoryTrain:  epoch  5, batch     4 | loss: 0.6505942Losses:  0.510588526725769 0.37162894010543823
MemoryTrain:  epoch  5, batch     5 | loss: 0.8822175Losses:  0.41939395666122437 0.3290235698223114
MemoryTrain:  epoch  5, batch     6 | loss: 0.7484175Losses:  0.3623506724834442 0.2803940176963806
MemoryTrain:  epoch  5, batch     7 | loss: 0.6427447Losses:  0.3848972022533417 0.35564494132995605
MemoryTrain:  epoch  5, batch     8 | loss: 0.7405422Losses:  0.43496185541152954 0.3674408197402954
MemoryTrain:  epoch  6, batch     0 | loss: 0.8024027Losses:  0.4164542853832245 0.30875059962272644
MemoryTrain:  epoch  6, batch     1 | loss: 0.7252049Losses:  0.2837514281272888 0.18342706561088562
MemoryTrain:  epoch  6, batch     2 | loss: 0.4671785Losses:  0.42398369312286377 0.34055495262145996
MemoryTrain:  epoch  6, batch     3 | loss: 0.7645386Losses:  0.35333818197250366 0.32373449206352234
MemoryTrain:  epoch  6, batch     4 | loss: 0.6770726Losses:  0.38912639021873474 0.49044910073280334
MemoryTrain:  epoch  6, batch     5 | loss: 0.8795755Losses:  0.37445229291915894 0.42485105991363525
MemoryTrain:  epoch  6, batch     6 | loss: 0.7993034Losses:  0.3876410126686096 0.3141884505748749
MemoryTrain:  epoch  6, batch     7 | loss: 0.7018294Losses:  0.41868382692337036 0.24720220267772675
MemoryTrain:  epoch  6, batch     8 | loss: 0.6658860Losses:  0.37586861848831177 0.3466040790081024
MemoryTrain:  epoch  7, batch     0 | loss: 0.7224727Losses:  0.353828489780426 0.2814188599586487
MemoryTrain:  epoch  7, batch     1 | loss: 0.6352473Losses:  0.32872989773750305 0.2998749911785126
MemoryTrain:  epoch  7, batch     2 | loss: 0.6286049Losses:  0.35316604375839233 0.3172208070755005
MemoryTrain:  epoch  7, batch     3 | loss: 0.6703869Losses:  0.3415444791316986 0.28446775674819946
MemoryTrain:  epoch  7, batch     4 | loss: 0.6260122Losses:  0.3443940579891205 0.3529193103313446
MemoryTrain:  epoch  7, batch     5 | loss: 0.6973134Losses:  0.35222262144088745 0.2456190139055252
MemoryTrain:  epoch  7, batch     6 | loss: 0.5978416Losses:  0.48499858379364014 0.3285563588142395
MemoryTrain:  epoch  7, batch     7 | loss: 0.8135549Losses:  0.37459254264831543 0.2991832196712494
MemoryTrain:  epoch  7, batch     8 | loss: 0.6737758Losses:  0.2922590374946594 0.3420497179031372
MemoryTrain:  epoch  8, batch     0 | loss: 0.6343088Losses:  0.3647478222846985 0.3196292221546173
MemoryTrain:  epoch  8, batch     1 | loss: 0.6843771Losses:  0.27990758419036865 0.2696644067764282
MemoryTrain:  epoch  8, batch     2 | loss: 0.5495720Losses:  0.3497880697250366 0.22633937001228333
MemoryTrain:  epoch  8, batch     3 | loss: 0.5761274Losses:  0.38055306673049927 0.4163360595703125
MemoryTrain:  epoch  8, batch     4 | loss: 0.7968891Losses:  0.335176944732666 0.30534589290618896
MemoryTrain:  epoch  8, batch     5 | loss: 0.6405228Losses:  0.26586076617240906 0.28620755672454834
MemoryTrain:  epoch  8, batch     6 | loss: 0.5520684Losses:  0.4227716326713562 0.30684971809387207
MemoryTrain:  epoch  8, batch     7 | loss: 0.7296214Losses:  0.5959727168083191 0.4757211208343506
MemoryTrain:  epoch  8, batch     8 | loss: 1.0716939Losses:  0.3230966627597809 0.3389153480529785
MemoryTrain:  epoch  9, batch     0 | loss: 0.6620120Losses:  0.357635498046875 0.293667197227478
MemoryTrain:  epoch  9, batch     1 | loss: 0.6513027Losses:  0.35182154178619385 0.2752654552459717
MemoryTrain:  epoch  9, batch     2 | loss: 0.6270870Losses:  0.24214857816696167 0.178598552942276
MemoryTrain:  epoch  9, batch     3 | loss: 0.4207471Losses:  0.30541864037513733 0.34046781063079834
MemoryTrain:  epoch  9, batch     4 | loss: 0.6458864Losses:  0.4320054054260254 0.5617886781692505
MemoryTrain:  epoch  9, batch     5 | loss: 0.9937941Losses:  0.3170556128025055 0.24563705921173096
MemoryTrain:  epoch  9, batch     6 | loss: 0.5626926Losses:  0.30698299407958984 0.23673580586910248
MemoryTrain:  epoch  9, batch     7 | loss: 0.5437188Losses:  0.4177475869655609 0.38974839448928833
MemoryTrain:  epoch  9, batch     8 | loss: 0.8074960
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 61.49%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 61.35%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 61.70%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 63.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 84.49%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 82.79%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 81.79%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 80.33%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 79.94%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 79.56%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 79.10%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 78.46%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 77.89%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 77.85%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 77.45%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.41%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 77.17%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 76.94%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 76.67%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 75.82%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 74.12%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 73.42%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 72.73%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 71.91%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 73.22%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 72.40%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 71.60%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 70.88%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 69.56%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 69.08%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 70.53%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 70.15%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 69.91%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 69.09%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 67.83%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 67.41%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 67.17%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 68.20%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 67.86%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 67.48%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 66.73%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 67.36%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 66.77%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 69.71%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 69.36%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 68.90%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 68.60%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.30%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 68.93%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 68.61%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 68.47%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 68.33%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 68.00%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 68.07%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 67.96%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 67.58%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 67.42%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 67.30%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 67.17%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 67.07%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 66.93%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 66.73%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 66.56%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 66.42%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 66.34%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 68.31%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.51%   [EVAL] batch:  231 | acc: 37.50%,  total acc: 68.37%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 67.85%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 67.62%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.03%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 67.88%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 67.38%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 67.16%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 66.77%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 67.64%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 67.84%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 67.55%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 67.44%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 66.75%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 69.67%   [EVAL] batch:  314 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 69.98%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.06%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 70.03%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 69.96%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  340 | acc: 50.00%,  total acc: 69.74%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:  342 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 69.46%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  345 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 69.61%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:  358 | acc: 12.50%,  total acc: 69.17%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 69.01%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 68.78%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:  375 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  376 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 69.28%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 69.36%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 69.37%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 69.78%   [EVAL] batch:  402 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:  404 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 69.54%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 69.49%   [EVAL] batch:  407 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 69.25%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 68.99%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 68.86%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  421 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 68.93%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.59%   
cur_acc:  ['0.9464', '0.7103', '0.7798', '0.6359', '0.8155', '0.7073', '0.6964']
his_acc:  ['0.9464', '0.8290', '0.7892', '0.7282', '0.7304', '0.7035', '0.6959']
Clustering into  38  clusters
Clusters:  [ 1  5 24  1  1  1 33  1 21  0 32 27  3  1  1 37  1 29  1 34  2 25  1  1
 36  1  1  1 20 23 26  1 22  1 19 31 35  1  1  1 17 30  1  1 28 15 11  1
  1 18  1  1  1 12  1  0  1 13 16 14  5 10  1  1  1  2  1  8  6  9  7  1
  1  3  3  4  1  1  1  1]
Losses:  7.0614728927612305 1.1630703210830688
CurrentTrain: epoch  0, batch     0 | loss: 8.2245436Losses:  9.37801742553711 1.5077338218688965
CurrentTrain: epoch  0, batch     1 | loss: 10.8857517Losses:  8.182408332824707 1.2693495750427246
CurrentTrain: epoch  0, batch     2 | loss: 9.4517574Losses:  8.611639022827148 1.4556012153625488
CurrentTrain: epoch  0, batch     3 | loss: 10.0672398Losses:  6.845273017883301 1.0153433084487915
CurrentTrain: epoch  0, batch     4 | loss: 7.8606162Losses:  6.333767890930176 1.2488046884536743
CurrentTrain: epoch  0, batch     5 | loss: 7.5825725Losses:  7.379072189331055 0.38464176654815674
CurrentTrain: epoch  0, batch     6 | loss: 7.7637138Losses:  4.300214767456055 1.1902697086334229
CurrentTrain: epoch  1, batch     0 | loss: 5.4904842Losses:  4.460698127746582 1.1374578475952148
CurrentTrain: epoch  1, batch     1 | loss: 5.5981560Losses:  3.9204955101013184 1.1882197856903076
CurrentTrain: epoch  1, batch     2 | loss: 5.1087151Losses:  3.3400979042053223 0.9036905765533447
CurrentTrain: epoch  1, batch     3 | loss: 4.2437887Losses:  4.0618896484375 1.0478885173797607
CurrentTrain: epoch  1, batch     4 | loss: 5.1097784Losses:  2.9744935035705566 1.1446146965026855
CurrentTrain: epoch  1, batch     5 | loss: 4.1191082Losses:  3.8463330268859863 0.32778337597846985
CurrentTrain: epoch  1, batch     6 | loss: 4.1741166Losses:  3.260406970977783 0.8827614188194275
CurrentTrain: epoch  2, batch     0 | loss: 4.1431684Losses:  4.273384094238281 0.9359297752380371
CurrentTrain: epoch  2, batch     1 | loss: 5.2093139Losses:  3.2683041095733643 0.9674900770187378
CurrentTrain: epoch  2, batch     2 | loss: 4.2357941Losses:  3.5943331718444824 1.0563392639160156
CurrentTrain: epoch  2, batch     3 | loss: 4.6506724Losses:  3.177776336669922 1.00100839138031
CurrentTrain: epoch  2, batch     4 | loss: 4.1787848Losses:  2.9113378524780273 0.8774317502975464
CurrentTrain: epoch  2, batch     5 | loss: 3.7887697Losses:  2.4019086360931396 0.16313427686691284
CurrentTrain: epoch  2, batch     6 | loss: 2.5650430Losses:  2.926330327987671 0.7943241000175476
CurrentTrain: epoch  3, batch     0 | loss: 3.7206545Losses:  2.7526140213012695 1.1447702646255493
CurrentTrain: epoch  3, batch     1 | loss: 3.8973842Losses:  3.288832902908325 1.1972204446792603
CurrentTrain: epoch  3, batch     2 | loss: 4.4860535Losses:  3.1403393745422363 1.0382862091064453
CurrentTrain: epoch  3, batch     3 | loss: 4.1786256Losses:  2.8493480682373047 0.7499037384986877
CurrentTrain: epoch  3, batch     4 | loss: 3.5992517Losses:  3.0907745361328125 0.7286385297775269
CurrentTrain: epoch  3, batch     5 | loss: 3.8194132Losses:  2.5018787384033203 0.17005665600299835
CurrentTrain: epoch  3, batch     6 | loss: 2.6719353Losses:  2.452918529510498 0.8929131031036377
CurrentTrain: epoch  4, batch     0 | loss: 3.3458316Losses:  2.290001392364502 0.7785176038742065
CurrentTrain: epoch  4, batch     1 | loss: 3.0685191Losses:  2.7402825355529785 0.9516233801841736
CurrentTrain: epoch  4, batch     2 | loss: 3.6919060Losses:  3.0293455123901367 1.2356879711151123
CurrentTrain: epoch  4, batch     3 | loss: 4.2650337Losses:  3.2874295711517334 0.914268970489502
CurrentTrain: epoch  4, batch     4 | loss: 4.2016983Losses:  2.5295186042785645 0.5152191519737244
CurrentTrain: epoch  4, batch     5 | loss: 3.0447378Losses:  2.101789951324463 0.10127899050712585
CurrentTrain: epoch  4, batch     6 | loss: 2.2030690Losses:  2.4528117179870605 0.7909762859344482
CurrentTrain: epoch  5, batch     0 | loss: 3.2437880Losses:  2.2286839485168457 0.4884183406829834
CurrentTrain: epoch  5, batch     1 | loss: 2.7171023Losses:  2.3843610286712646 0.6150996685028076
CurrentTrain: epoch  5, batch     2 | loss: 2.9994607Losses:  2.614071846008301 0.7528221011161804
CurrentTrain: epoch  5, batch     3 | loss: 3.3668940Losses:  3.373055934906006 0.847740113735199
CurrentTrain: epoch  5, batch     4 | loss: 4.2207961Losses:  2.472682476043701 0.8592737317085266
CurrentTrain: epoch  5, batch     5 | loss: 3.3319561Losses:  1.8243319988250732 0.13833636045455933
CurrentTrain: epoch  5, batch     6 | loss: 1.9626684Losses:  2.3341054916381836 0.5992753505706787
CurrentTrain: epoch  6, batch     0 | loss: 2.9333808Losses:  2.6468324661254883 0.7860231399536133
CurrentTrain: epoch  6, batch     1 | loss: 3.4328556Losses:  3.1046924591064453 0.5289146304130554
CurrentTrain: epoch  6, batch     2 | loss: 3.6336071Losses:  1.9901529550552368 0.559797465801239
CurrentTrain: epoch  6, batch     3 | loss: 2.5499504Losses:  1.994441270828247 0.6529252529144287
CurrentTrain: epoch  6, batch     4 | loss: 2.6473665Losses:  2.133881092071533 0.508010745048523
CurrentTrain: epoch  6, batch     5 | loss: 2.6418920Losses:  1.936397671699524 0.07352851331233978
CurrentTrain: epoch  6, batch     6 | loss: 2.0099261Losses:  2.276134490966797 0.8656948804855347
CurrentTrain: epoch  7, batch     0 | loss: 3.1418295Losses:  2.4223098754882812 0.5698294043540955
CurrentTrain: epoch  7, batch     1 | loss: 2.9921393Losses:  1.9302687644958496 0.3999783992767334
CurrentTrain: epoch  7, batch     2 | loss: 2.3302472Losses:  1.951793909072876 0.6555490493774414
CurrentTrain: epoch  7, batch     3 | loss: 2.6073430Losses:  3.0417304039001465 0.30211007595062256
CurrentTrain: epoch  7, batch     4 | loss: 3.3438406Losses:  1.8094093799591064 0.25851184129714966
CurrentTrain: epoch  7, batch     5 | loss: 2.0679212Losses:  1.7503924369812012 0.31455111503601074
CurrentTrain: epoch  7, batch     6 | loss: 2.0649436Losses:  2.1979236602783203 0.560653567314148
CurrentTrain: epoch  8, batch     0 | loss: 2.7585773Losses:  2.4295408725738525 0.5605582594871521
CurrentTrain: epoch  8, batch     1 | loss: 2.9900992Losses:  1.9548187255859375 0.6535872220993042
CurrentTrain: epoch  8, batch     2 | loss: 2.6084061Losses:  1.7687616348266602 0.6622499227523804
CurrentTrain: epoch  8, batch     3 | loss: 2.4310117Losses:  2.213437557220459 0.6982043385505676
CurrentTrain: epoch  8, batch     4 | loss: 2.9116418Losses:  2.130795478820801 0.6306442618370056
CurrentTrain: epoch  8, batch     5 | loss: 2.7614398Losses:  2.0367140769958496 0.10613378882408142
CurrentTrain: epoch  8, batch     6 | loss: 2.1428478Losses:  2.154109477996826 0.7385733723640442
CurrentTrain: epoch  9, batch     0 | loss: 2.8926828Losses:  2.2302608489990234 0.6906030178070068
CurrentTrain: epoch  9, batch     1 | loss: 2.9208639Losses:  1.8083908557891846 0.6958791017532349
CurrentTrain: epoch  9, batch     2 | loss: 2.5042701Losses:  1.8262829780578613 0.4195965826511383
CurrentTrain: epoch  9, batch     3 | loss: 2.2458797Losses:  1.9629563093185425 0.5696053504943848
CurrentTrain: epoch  9, batch     4 | loss: 2.5325618Losses:  1.9958728551864624 0.6397117972373962
CurrentTrain: epoch  9, batch     5 | loss: 2.6355846Losses:  3.0017080307006836 0.1890418529510498
CurrentTrain: epoch  9, batch     6 | loss: 3.1907499
Losses:  6.153646469116211 0.3660275340080261
MemoryTrain:  epoch  0, batch     0 | loss: 6.5196738Losses:  7.738677978515625 0.2757095694541931
MemoryTrain:  epoch  0, batch     1 | loss: 8.0143871Losses:  8.919916152954102 0.4010159969329834
MemoryTrain:  epoch  0, batch     2 | loss: 9.3209324Losses:  9.536347389221191 0.5167961716651917
MemoryTrain:  epoch  0, batch     3 | loss: 10.0531435Losses:  9.80468463897705 0.4950631856918335
MemoryTrain:  epoch  0, batch     4 | loss: 10.2997475Losses:  10.073892593383789 0.3388814926147461
MemoryTrain:  epoch  0, batch     5 | loss: 10.4127741Losses:  11.1270751953125 0.4709497392177582
MemoryTrain:  epoch  0, batch     6 | loss: 11.5980253Losses:  10.553560256958008 0.23122328519821167
MemoryTrain:  epoch  0, batch     7 | loss: 10.7847834Losses:  11.14892292022705 0.33918100595474243
MemoryTrain:  epoch  0, batch     8 | loss: 11.4881039Losses:  11.027290344238281 0.3378353416919708
MemoryTrain:  epoch  0, batch     9 | loss: 11.3651257Losses:  1.26082181930542 0.5688390731811523
MemoryTrain:  epoch  1, batch     0 | loss: 1.8296609Losses:  0.35806816816329956 0.21979884803295135
MemoryTrain:  epoch  1, batch     1 | loss: 0.5778670Losses:  0.7514477372169495 0.2715669870376587
MemoryTrain:  epoch  1, batch     2 | loss: 1.0230148Losses:  0.8038570880889893 0.3630412220954895
MemoryTrain:  epoch  1, batch     3 | loss: 1.1668983Losses:  0.9490634202957153 0.46355363726615906
MemoryTrain:  epoch  1, batch     4 | loss: 1.4126171Losses:  1.1827434301376343 0.26448115706443787
MemoryTrain:  epoch  1, batch     5 | loss: 1.4472246Losses:  1.0280146598815918 0.38877761363983154
MemoryTrain:  epoch  1, batch     6 | loss: 1.4167923Losses:  0.8463965654373169 0.39515459537506104
MemoryTrain:  epoch  1, batch     7 | loss: 1.2415512Losses:  1.0290870666503906 0.3844262659549713
MemoryTrain:  epoch  1, batch     8 | loss: 1.4135133Losses:  1.1596941947937012 0.40805932879447937
MemoryTrain:  epoch  1, batch     9 | loss: 1.5677536Losses:  0.7888142466545105 0.4177864193916321
MemoryTrain:  epoch  2, batch     0 | loss: 1.2066007Losses:  0.8329053521156311 0.5059229731559753
MemoryTrain:  epoch  2, batch     1 | loss: 1.3388283Losses:  0.8967065811157227 0.29659855365753174
MemoryTrain:  epoch  2, batch     2 | loss: 1.1933051Losses:  0.8416246771812439 0.26295483112335205
MemoryTrain:  epoch  2, batch     3 | loss: 1.1045794Losses:  0.46755632758140564 0.32971256971359253
MemoryTrain:  epoch  2, batch     4 | loss: 0.7972689Losses:  0.3975377380847931 0.3766138553619385
MemoryTrain:  epoch  2, batch     5 | loss: 0.7741516Losses:  1.199069619178772 0.3397410809993744
MemoryTrain:  epoch  2, batch     6 | loss: 1.5388107Losses:  0.6348294615745544 0.30330464243888855
MemoryTrain:  epoch  2, batch     7 | loss: 0.9381341Losses:  0.4864977300167084 0.4168778359889984
MemoryTrain:  epoch  2, batch     8 | loss: 0.9033756Losses:  0.5350115299224854 0.5469775795936584
MemoryTrain:  epoch  2, batch     9 | loss: 1.0819890Losses:  0.34032949805259705 0.2890135645866394
MemoryTrain:  epoch  3, batch     0 | loss: 0.6293430Losses:  0.6750867366790771 0.26164335012435913
MemoryTrain:  epoch  3, batch     1 | loss: 0.9367301Losses:  0.4100068211555481 0.30754947662353516
MemoryTrain:  epoch  3, batch     2 | loss: 0.7175563Losses:  0.5520334839820862 0.4658389687538147
MemoryTrain:  epoch  3, batch     3 | loss: 1.0178725Losses:  0.7264472246170044 0.5082776546478271
MemoryTrain:  epoch  3, batch     4 | loss: 1.2347249Losses:  0.5383150577545166 0.360484778881073
MemoryTrain:  epoch  3, batch     5 | loss: 0.8987998Losses:  0.6219742298126221 0.44052591919898987
MemoryTrain:  epoch  3, batch     6 | loss: 1.0625001Losses:  0.8443655967712402 0.29213273525238037
MemoryTrain:  epoch  3, batch     7 | loss: 1.1364983Losses:  0.3788588047027588 0.25624799728393555
MemoryTrain:  epoch  3, batch     8 | loss: 0.6351068Losses:  0.7540528774261475 0.44995415210723877
MemoryTrain:  epoch  3, batch     9 | loss: 1.2040070Losses:  0.6507644653320312 0.5232112407684326
MemoryTrain:  epoch  4, batch     0 | loss: 1.1739757Losses:  0.7618088722229004 0.4904455244541168
MemoryTrain:  epoch  4, batch     1 | loss: 1.2522544Losses:  0.3320525884628296 0.30498969554901123
MemoryTrain:  epoch  4, batch     2 | loss: 0.6370423Losses:  0.45562928915023804 0.3294375240802765
MemoryTrain:  epoch  4, batch     3 | loss: 0.7850668Losses:  0.5522125959396362 0.4138384461402893
MemoryTrain:  epoch  4, batch     4 | loss: 0.9660510Losses:  0.44786322116851807 0.5057077407836914
MemoryTrain:  epoch  4, batch     5 | loss: 0.9535710Losses:  0.5996630191802979 0.3701520562171936
MemoryTrain:  epoch  4, batch     6 | loss: 0.9698151Losses:  0.3629148006439209 0.30657655000686646
MemoryTrain:  epoch  4, batch     7 | loss: 0.6694914Losses:  0.34468787908554077 0.19936031103134155
MemoryTrain:  epoch  4, batch     8 | loss: 0.5440482Losses:  0.45360660552978516 0.3054649829864502
MemoryTrain:  epoch  4, batch     9 | loss: 0.7590716Losses:  0.3665103614330292 0.33433520793914795
MemoryTrain:  epoch  5, batch     0 | loss: 0.7008456Losses:  0.5644005537033081 0.34465745091438293
MemoryTrain:  epoch  5, batch     1 | loss: 0.9090580Losses:  0.4367225170135498 0.3597865104675293
MemoryTrain:  epoch  5, batch     2 | loss: 0.7965090Losses:  0.46768197417259216 0.33990681171417236
MemoryTrain:  epoch  5, batch     3 | loss: 0.8075888Losses:  0.32204294204711914 0.24010659754276276
MemoryTrain:  epoch  5, batch     4 | loss: 0.5621495Losses:  0.627643346786499 0.6324501037597656
MemoryTrain:  epoch  5, batch     5 | loss: 1.2600935Losses:  0.40906310081481934 0.24389997124671936
MemoryTrain:  epoch  5, batch     6 | loss: 0.6529630Losses:  0.4503806531429291 0.42272794246673584
MemoryTrain:  epoch  5, batch     7 | loss: 0.8731086Losses:  0.5297955870628357 0.4266894459724426
MemoryTrain:  epoch  5, batch     8 | loss: 0.9564850Losses:  0.3408893346786499 0.25322404503822327
MemoryTrain:  epoch  5, batch     9 | loss: 0.5941133Losses:  0.4278979003429413 0.30812960863113403
MemoryTrain:  epoch  6, batch     0 | loss: 0.7360275Losses:  0.4226638674736023 0.3156326711177826
MemoryTrain:  epoch  6, batch     1 | loss: 0.7382965Losses:  0.41871076822280884 0.42299556732177734
MemoryTrain:  epoch  6, batch     2 | loss: 0.8417063Losses:  0.34307661652565 0.22779905796051025
MemoryTrain:  epoch  6, batch     3 | loss: 0.5708756Losses:  0.37408703565597534 0.393183171749115
MemoryTrain:  epoch  6, batch     4 | loss: 0.7672702Losses:  0.3504331111907959 0.2843630611896515
MemoryTrain:  epoch  6, batch     5 | loss: 0.6347961Losses:  0.42949360609054565 0.4777334928512573
MemoryTrain:  epoch  6, batch     6 | loss: 0.9072271Losses:  0.5229256749153137 0.4398648738861084
MemoryTrain:  epoch  6, batch     7 | loss: 0.9627905Losses:  0.42483922839164734 0.3160995543003082
MemoryTrain:  epoch  6, batch     8 | loss: 0.7409388Losses:  0.4990192651748657 0.30534613132476807
MemoryTrain:  epoch  6, batch     9 | loss: 0.8043654Losses:  0.3542710542678833 0.3032865524291992
MemoryTrain:  epoch  7, batch     0 | loss: 0.6575576Losses:  0.477809876203537 0.41524016857147217
MemoryTrain:  epoch  7, batch     1 | loss: 0.8930501Losses:  0.3460536003112793 0.30627357959747314
MemoryTrain:  epoch  7, batch     2 | loss: 0.6523272Losses:  0.40215960144996643 0.38928014039993286
MemoryTrain:  epoch  7, batch     3 | loss: 0.7914398Losses:  0.460269570350647 0.3803554177284241
MemoryTrain:  epoch  7, batch     4 | loss: 0.8406250Losses:  0.413958877325058 0.3151102662086487
MemoryTrain:  epoch  7, batch     5 | loss: 0.7290691Losses:  0.3915022611618042 0.32937684655189514
MemoryTrain:  epoch  7, batch     6 | loss: 0.7208791Losses:  0.32699984312057495 0.2399187684059143
MemoryTrain:  epoch  7, batch     7 | loss: 0.5669186Losses:  0.4517633616924286 0.37928348779678345
MemoryTrain:  epoch  7, batch     8 | loss: 0.8310468Losses:  0.34161442518234253 0.28040388226509094
MemoryTrain:  epoch  7, batch     9 | loss: 0.6220183Losses:  0.37685590982437134 0.2731955945491791
MemoryTrain:  epoch  8, batch     0 | loss: 0.6500515Losses:  0.40416938066482544 0.33813226222991943
MemoryTrain:  epoch  8, batch     1 | loss: 0.7423016Losses:  0.42559516429901123 0.4048160910606384
MemoryTrain:  epoch  8, batch     2 | loss: 0.8304113Losses:  0.4527985453605652 0.38493746519088745
MemoryTrain:  epoch  8, batch     3 | loss: 0.8377360Losses:  0.34381765127182007 0.29831966757774353
MemoryTrain:  epoch  8, batch     4 | loss: 0.6421373Losses:  0.33545050024986267 0.2798466682434082
MemoryTrain:  epoch  8, batch     5 | loss: 0.6152972Losses:  0.4797632694244385 0.485431969165802
MemoryTrain:  epoch  8, batch     6 | loss: 0.9651952Losses:  0.4074923098087311 0.348895400762558
MemoryTrain:  epoch  8, batch     7 | loss: 0.7563877Losses:  0.25637710094451904 0.2262243628501892
MemoryTrain:  epoch  8, batch     8 | loss: 0.4826015Losses:  0.360079288482666 0.3610377907752991
MemoryTrain:  epoch  8, batch     9 | loss: 0.7211171Losses:  0.30398643016815186 0.19987091422080994
MemoryTrain:  epoch  9, batch     0 | loss: 0.5038574Losses:  0.3950508236885071 0.2858174741268158
MemoryTrain:  epoch  9, batch     1 | loss: 0.6808683Losses:  0.41906440258026123 0.42632952332496643
MemoryTrain:  epoch  9, batch     2 | loss: 0.8453939Losses:  0.38195693492889404 0.3820320963859558
MemoryTrain:  epoch  9, batch     3 | loss: 0.7639890Losses:  0.36018818616867065 0.25387829542160034
MemoryTrain:  epoch  9, batch     4 | loss: 0.6140665Losses:  0.4155283570289612 0.3957332670688629
MemoryTrain:  epoch  9, batch     5 | loss: 0.8112617Losses:  0.41784340143203735 0.35785263776779175
MemoryTrain:  epoch  9, batch     6 | loss: 0.7756960Losses:  0.38163816928863525 0.37231773138046265
MemoryTrain:  epoch  9, batch     7 | loss: 0.7539559Losses:  0.3638174533843994 0.2480345219373703
MemoryTrain:  epoch  9, batch     8 | loss: 0.6118520Losses:  0.259990930557251 0.19507551193237305
MemoryTrain:  epoch  9, batch     9 | loss: 0.4550664
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 57.61%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 0.00%,  total acc: 53.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 51.44%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 49.77%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.99%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 46.34%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 45.00%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 44.34%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 45.45%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 46.32%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 48.78%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 49.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 50.99%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 51.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 54.12%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 54.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 56.53%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 55.97%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 54.89%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 54.26%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 53.39%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 52.81%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 52.12%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 51.96%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 52.40%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 52.71%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 53.36%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 53.41%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 53.91%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 53.73%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 53.88%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 53.81%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 54.20%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 54.54%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 54.07%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.43%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 82.99%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 81.80%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 80.93%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.40%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 77.98%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 77.43%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 76.48%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 76.10%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 75.92%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 75.08%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 0.00%,  total acc: 73.32%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 72.55%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 71.80%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 70.99%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.56%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.99%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 69.36%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 68.62%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 68.15%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 69.00%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 68.44%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 66.79%   [EVAL] batch:  105 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 66.06%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 65.43%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 64.46%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 63.68%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 63.51%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 63.45%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 63.70%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 64.47%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 64.21%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 63.61%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 64.46%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 64.32%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 64.13%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 64.19%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 67.23%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 66.79%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 66.26%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 66.27%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 66.98%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 66.60%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 66.37%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 66.14%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 66.16%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 65.77%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 65.69%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 65.51%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 65.37%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 65.32%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 65.04%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 64.87%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.40%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 64.24%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  231 | acc: 37.50%,  total acc: 66.38%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 66.23%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 65.66%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 65.49%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 65.27%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 65.06%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 65.78%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 65.54%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 64.84%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 64.61%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 65.31%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  314 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 67.83%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 67.74%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 67.60%   [EVAL] batch:  338 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  340 | acc: 31.25%,  total acc: 67.36%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 67.20%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 67.11%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 67.12%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 67.09%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  348 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 67.02%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 66.97%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 66.97%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 66.51%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 66.40%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  375 | acc: 50.00%,  total acc: 67.12%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 67.00%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 66.97%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 66.87%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  384 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 66.89%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 67.44%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 67.40%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 67.10%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 67.00%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 66.85%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 66.73%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  419 | acc: 56.25%,  total acc: 66.85%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  438 | acc: 37.50%,  total acc: 67.45%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 67.37%   [EVAL] batch:  440 | acc: 12.50%,  total acc: 67.25%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 67.12%   [EVAL] batch:  442 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 66.92%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  445 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  446 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 67.34%   [EVAL] batch:  452 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  456 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  457 | acc: 0.00%,  total acc: 67.29%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  459 | acc: 31.25%,  total acc: 67.13%   [EVAL] batch:  460 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:  461 | acc: 6.25%,  total acc: 66.87%   [EVAL] batch:  462 | acc: 6.25%,  total acc: 66.74%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 66.59%   [EVAL] batch:  464 | acc: 6.25%,  total acc: 66.47%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 66.32%   [EVAL] batch:  466 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 66.07%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:  469 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  473 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 66.52%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  483 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  484 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  485 | acc: 12.50%,  total acc: 66.13%   [EVAL] batch:  486 | acc: 18.75%,  total acc: 66.03%   [EVAL] batch:  487 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  489 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  494 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  495 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  498 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 65.94%   
cur_acc:  ['0.9464', '0.7103', '0.7798', '0.6359', '0.8155', '0.7073', '0.6964', '0.5407']
his_acc:  ['0.9464', '0.8290', '0.7892', '0.7282', '0.7304', '0.7035', '0.6959', '0.6594']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.713980674743652 1.4164063930511475
CurrentTrain: epoch  0, batch     0 | loss: 13.1303873Losses:  13.358515739440918 1.3497560024261475
CurrentTrain: epoch  0, batch     1 | loss: 14.7082720Losses:  13.194415092468262 1.504921555519104
CurrentTrain: epoch  0, batch     2 | loss: 14.6993370Losses:  13.463445663452148 1.6892757415771484
CurrentTrain: epoch  0, batch     3 | loss: 15.1527214Losses:  13.149042129516602 1.8366838693618774
CurrentTrain: epoch  0, batch     4 | loss: 14.9857264Losses:  13.271011352539062 1.4383301734924316
CurrentTrain: epoch  0, batch     5 | loss: 14.7093410Losses:  12.792929649353027 1.4225966930389404
CurrentTrain: epoch  0, batch     6 | loss: 14.2155266Losses:  13.242396354675293 1.4181914329528809
CurrentTrain: epoch  0, batch     7 | loss: 14.6605873Losses:  12.915163040161133 1.3731249570846558
CurrentTrain: epoch  0, batch     8 | loss: 14.2882881Losses:  13.145233154296875 1.3654981851577759
CurrentTrain: epoch  0, batch     9 | loss: 14.5107317Losses:  12.857967376708984 1.7850439548492432
CurrentTrain: epoch  0, batch    10 | loss: 14.6430111Losses:  12.930181503295898 1.5141961574554443
CurrentTrain: epoch  0, batch    11 | loss: 14.4443779Losses:  12.547120094299316 1.0466090440750122
CurrentTrain: epoch  0, batch    12 | loss: 13.5937290Losses:  12.185117721557617 1.0854443311691284
CurrentTrain: epoch  0, batch    13 | loss: 13.2705622Losses:  11.977578163146973 1.2698009014129639
CurrentTrain: epoch  0, batch    14 | loss: 13.2473793Losses:  11.717985153198242 1.3291983604431152
CurrentTrain: epoch  0, batch    15 | loss: 13.0471840Losses:  12.213479995727539 1.1548693180084229
CurrentTrain: epoch  0, batch    16 | loss: 13.3683491Losses:  11.824353218078613 1.4357308149337769
CurrentTrain: epoch  0, batch    17 | loss: 13.2600842Losses:  11.878780364990234 1.4809739589691162
CurrentTrain: epoch  0, batch    18 | loss: 13.3597546Losses:  11.88094711303711 1.6871418952941895
CurrentTrain: epoch  0, batch    19 | loss: 13.5680885Losses:  12.461652755737305 1.6812878847122192
CurrentTrain: epoch  0, batch    20 | loss: 14.1429405Losses:  12.161120414733887 1.5719760656356812
CurrentTrain: epoch  0, batch    21 | loss: 13.7330961Losses:  12.28851318359375 1.1582088470458984
CurrentTrain: epoch  0, batch    22 | loss: 13.4467220Losses:  12.254412651062012 1.5597811937332153
CurrentTrain: epoch  0, batch    23 | loss: 13.8141937Losses:  11.8624849319458 1.3008936643600464
CurrentTrain: epoch  0, batch    24 | loss: 13.1633787Losses:  11.809598922729492 1.6103670597076416
CurrentTrain: epoch  0, batch    25 | loss: 13.4199657Losses:  12.118368148803711 0.7112118005752563
CurrentTrain: epoch  0, batch    26 | loss: 12.8295803Losses:  11.642383575439453 1.3095180988311768
CurrentTrain: epoch  0, batch    27 | loss: 12.9519014Losses:  11.340553283691406 1.1881399154663086
CurrentTrain: epoch  0, batch    28 | loss: 12.5286932Losses:  12.074434280395508 1.253861904144287
CurrentTrain: epoch  0, batch    29 | loss: 13.3282967Losses:  10.953714370727539 0.9039924740791321
CurrentTrain: epoch  0, batch    30 | loss: 11.8577070Losses:  11.500048637390137 1.0850473642349243
CurrentTrain: epoch  0, batch    31 | loss: 12.5850964Losses:  11.254766464233398 0.9698734879493713
CurrentTrain: epoch  0, batch    32 | loss: 12.2246399Losses:  11.408231735229492 1.094606637954712
CurrentTrain: epoch  0, batch    33 | loss: 12.5028381Losses:  11.280917167663574 1.4923851490020752
CurrentTrain: epoch  0, batch    34 | loss: 12.7733021Losses:  11.542898178100586 1.3751168251037598
CurrentTrain: epoch  0, batch    35 | loss: 12.9180145Losses:  10.845311164855957 1.012729525566101
CurrentTrain: epoch  0, batch    36 | loss: 11.8580408Losses:  11.03783130645752 1.1999828815460205
CurrentTrain: epoch  0, batch    37 | loss: 12.2378139Losses:  11.323968887329102 0.8749147057533264
CurrentTrain: epoch  0, batch    38 | loss: 12.1988840Losses:  11.237013816833496 1.410169243812561
CurrentTrain: epoch  0, batch    39 | loss: 12.6471834Losses:  10.506245613098145 0.6853235960006714
CurrentTrain: epoch  0, batch    40 | loss: 11.1915693Losses:  10.731120109558105 1.0206513404846191
CurrentTrain: epoch  0, batch    41 | loss: 11.7517719Losses:  10.831113815307617 0.758531928062439
CurrentTrain: epoch  0, batch    42 | loss: 11.5896454Losses:  10.452733993530273 0.9774854779243469
CurrentTrain: epoch  0, batch    43 | loss: 11.4302197Losses:  11.512900352478027 1.094781517982483
CurrentTrain: epoch  0, batch    44 | loss: 12.6076822Losses:  10.499746322631836 0.860647439956665
CurrentTrain: epoch  0, batch    45 | loss: 11.3603935Losses:  10.442636489868164 0.9540181756019592
CurrentTrain: epoch  0, batch    46 | loss: 11.3966551Losses:  10.887781143188477 1.0927085876464844
CurrentTrain: epoch  0, batch    47 | loss: 11.9804897Losses:  9.874288558959961 0.896176815032959
CurrentTrain: epoch  0, batch    48 | loss: 10.7704659Losses:  10.78360366821289 0.7771451473236084
CurrentTrain: epoch  0, batch    49 | loss: 11.5607491Losses:  10.487143516540527 0.9480018019676208
CurrentTrain: epoch  0, batch    50 | loss: 11.4351454Losses:  10.069406509399414 0.9209641218185425
CurrentTrain: epoch  0, batch    51 | loss: 10.9903708Losses:  10.608709335327148 0.9075128436088562
CurrentTrain: epoch  0, batch    52 | loss: 11.5162220Losses:  9.719482421875 0.9920763969421387
CurrentTrain: epoch  0, batch    53 | loss: 10.7115593Losses:  9.824444770812988 1.1136399507522583
CurrentTrain: epoch  0, batch    54 | loss: 10.9380846Losses:  9.911596298217773 1.1893293857574463
CurrentTrain: epoch  0, batch    55 | loss: 11.1009254Losses:  9.624382019042969 1.0456788539886475
CurrentTrain: epoch  0, batch    56 | loss: 10.6700611Losses:  9.787436485290527 0.661902666091919
CurrentTrain: epoch  0, batch    57 | loss: 10.4493389Losses:  10.195499420166016 1.0057650804519653
CurrentTrain: epoch  0, batch    58 | loss: 11.2012644Losses:  9.607890129089355 0.9982107877731323
CurrentTrain: epoch  0, batch    59 | loss: 10.6061010Losses:  9.242782592773438 0.8366132378578186
CurrentTrain: epoch  0, batch    60 | loss: 10.0793962Losses:  9.366053581237793 0.7697080969810486
CurrentTrain: epoch  0, batch    61 | loss: 10.1357613Losses:  9.711666107177734 0.9203420281410217
CurrentTrain: epoch  0, batch    62 | loss: 10.6320086Losses:  9.430145263671875 1.091372013092041
CurrentTrain: epoch  0, batch    63 | loss: 10.5215168Losses:  9.37399673461914 0.9538495540618896
CurrentTrain: epoch  0, batch    64 | loss: 10.3278465Losses:  9.174222946166992 0.5094907283782959
CurrentTrain: epoch  0, batch    65 | loss: 9.6837139Losses:  9.310890197753906 1.2149014472961426
CurrentTrain: epoch  0, batch    66 | loss: 10.5257912Losses:  9.239578247070312 0.7750111818313599
CurrentTrain: epoch  0, batch    67 | loss: 10.0145893Losses:  8.946413040161133 0.6647902727127075
CurrentTrain: epoch  0, batch    68 | loss: 9.6112032Losses:  9.088943481445312 0.9812572002410889
CurrentTrain: epoch  0, batch    69 | loss: 10.0702009Losses:  9.233979225158691 0.6576035618782043
CurrentTrain: epoch  0, batch    70 | loss: 9.8915825Losses:  8.598394393920898 0.7578085660934448
CurrentTrain: epoch  0, batch    71 | loss: 9.3562031Losses:  8.57754135131836 0.8359819054603577
CurrentTrain: epoch  0, batch    72 | loss: 9.4135237Losses:  8.941871643066406 0.8169252276420593
CurrentTrain: epoch  0, batch    73 | loss: 9.7587967Losses:  8.864750862121582 0.9124852418899536
CurrentTrain: epoch  0, batch    74 | loss: 9.7772360Losses:  8.38648796081543 0.9002717733383179
CurrentTrain: epoch  0, batch    75 | loss: 9.2867594Losses:  8.623128890991211 0.576711118221283
CurrentTrain: epoch  0, batch    76 | loss: 9.1998396Losses:  8.683012962341309 0.8419805765151978
CurrentTrain: epoch  0, batch    77 | loss: 9.5249939Losses:  9.014564514160156 1.0420644283294678
CurrentTrain: epoch  0, batch    78 | loss: 10.0566292Losses:  8.539302825927734 0.9361854791641235
CurrentTrain: epoch  0, batch    79 | loss: 9.4754887Losses:  8.128748893737793 0.3797885775566101
CurrentTrain: epoch  0, batch    80 | loss: 8.5085373Losses:  8.618354797363281 0.6990050077438354
CurrentTrain: epoch  0, batch    81 | loss: 9.3173599Losses:  8.416903495788574 0.8947492837905884
CurrentTrain: epoch  0, batch    82 | loss: 9.3116531Losses:  8.087425231933594 0.6187106370925903
CurrentTrain: epoch  0, batch    83 | loss: 8.7061357Losses:  8.409573554992676 0.8295489549636841
CurrentTrain: epoch  0, batch    84 | loss: 9.2391224Losses:  8.009567260742188 0.7007686495780945
CurrentTrain: epoch  0, batch    85 | loss: 8.7103357Losses:  8.123126029968262 0.7571803331375122
CurrentTrain: epoch  0, batch    86 | loss: 8.8803062Losses:  7.988269329071045 0.7039534449577332
CurrentTrain: epoch  0, batch    87 | loss: 8.6922226Losses:  7.779611587524414 0.6019881367683411
CurrentTrain: epoch  0, batch    88 | loss: 8.3815994Losses:  8.162214279174805 0.8434561491012573
CurrentTrain: epoch  0, batch    89 | loss: 9.0056705Losses:  7.191165924072266 0.6357436776161194
CurrentTrain: epoch  0, batch    90 | loss: 7.8269095Losses:  7.674793720245361 0.6052826642990112
CurrentTrain: epoch  0, batch    91 | loss: 8.2800760Losses:  7.225428581237793 0.8207526206970215
CurrentTrain: epoch  0, batch    92 | loss: 8.0461807Losses:  7.393004417419434 0.6081825494766235
CurrentTrain: epoch  0, batch    93 | loss: 8.0011873Losses:  7.540191650390625 0.6887944936752319
CurrentTrain: epoch  0, batch    94 | loss: 8.2289858Losses:  7.697696685791016 0.3209953308105469
CurrentTrain: epoch  0, batch    95 | loss: 8.0186920Losses:  7.536612033843994 0.7629284858703613
CurrentTrain: epoch  0, batch    96 | loss: 8.2995405Losses:  7.4913458824157715 0.6142804026603699
CurrentTrain: epoch  0, batch    97 | loss: 8.1056261Losses:  7.008477210998535 0.40444713830947876
CurrentTrain: epoch  0, batch    98 | loss: 7.4129243Losses:  7.553037166595459 0.6681962609291077
CurrentTrain: epoch  0, batch    99 | loss: 8.2212334Losses:  7.4018096923828125 0.8578615188598633
CurrentTrain: epoch  0, batch   100 | loss: 8.2596712Losses:  6.954005241394043 0.7198805809020996
CurrentTrain: epoch  0, batch   101 | loss: 7.6738858Losses:  6.788986682891846 0.511526882648468
CurrentTrain: epoch  0, batch   102 | loss: 7.3005137Losses:  6.678987503051758 0.6130590438842773
CurrentTrain: epoch  0, batch   103 | loss: 7.2920465Losses:  6.494110584259033 0.589371919631958
CurrentTrain: epoch  0, batch   104 | loss: 7.0834827Losses:  6.5246782302856445 0.6867846250534058
CurrentTrain: epoch  0, batch   105 | loss: 7.2114630Losses:  6.638959884643555 0.8928002715110779
CurrentTrain: epoch  0, batch   106 | loss: 7.5317602Losses:  6.294848918914795 0.4803413450717926
CurrentTrain: epoch  0, batch   107 | loss: 6.7751904Losses:  7.26803731918335 0.8813201785087585
CurrentTrain: epoch  0, batch   108 | loss: 8.1493578Losses:  6.308926105499268 0.6010180711746216
CurrentTrain: epoch  0, batch   109 | loss: 6.9099441Losses:  6.693455696105957 0.7472749948501587
CurrentTrain: epoch  0, batch   110 | loss: 7.4407306Losses:  6.3269500732421875 0.5191303491592407
CurrentTrain: epoch  0, batch   111 | loss: 6.8460803Losses:  6.302170753479004 0.4264046847820282
CurrentTrain: epoch  0, batch   112 | loss: 6.7285752Losses:  6.096041679382324 0.5937358736991882
CurrentTrain: epoch  0, batch   113 | loss: 6.6897774Losses:  5.83917236328125 0.32752537727355957
CurrentTrain: epoch  0, batch   114 | loss: 6.1666975Losses:  6.075967788696289 0.5677690505981445
CurrentTrain: epoch  0, batch   115 | loss: 6.6437368Losses:  6.055205345153809 0.7646637558937073
CurrentTrain: epoch  0, batch   116 | loss: 6.8198690Losses:  5.82846212387085 0.6948263049125671
CurrentTrain: epoch  0, batch   117 | loss: 6.5232882Losses:  5.906669616699219 0.45858234167099
CurrentTrain: epoch  0, batch   118 | loss: 6.3652520Losses:  6.205741882324219 0.4754452407360077
CurrentTrain: epoch  0, batch   119 | loss: 6.6811872Losses:  5.982088088989258 0.7274742722511292
CurrentTrain: epoch  0, batch   120 | loss: 6.7095623Losses:  6.4314866065979 0.5102365016937256
CurrentTrain: epoch  0, batch   121 | loss: 6.9417229Losses:  5.847562789916992 0.5937410593032837
CurrentTrain: epoch  0, batch   122 | loss: 6.4413037Losses:  5.0987629890441895 0.21391555666923523
CurrentTrain: epoch  0, batch   123 | loss: 5.3126783Losses:  6.025888919830322 0.33650270104408264
CurrentTrain: epoch  0, batch   124 | loss: 6.3623915Losses:  5.712080001831055 0.47348400950431824
CurrentTrain: epoch  1, batch     0 | loss: 6.1855640Losses:  5.581658363342285 0.6513018608093262
CurrentTrain: epoch  1, batch     1 | loss: 6.2329602Losses:  5.0780487060546875 0.44384366273880005
CurrentTrain: epoch  1, batch     2 | loss: 5.5218925Losses:  5.531432628631592 0.5214004516601562
CurrentTrain: epoch  1, batch     3 | loss: 6.0528331Losses:  5.710817337036133 0.6542032957077026
CurrentTrain: epoch  1, batch     4 | loss: 6.3650208Losses:  5.5136613845825195 0.5973070859909058
CurrentTrain: epoch  1, batch     5 | loss: 6.1109686Losses:  5.716002941131592 0.3850893974304199
CurrentTrain: epoch  1, batch     6 | loss: 6.1010923Losses:  5.078699111938477 0.48928818106651306
CurrentTrain: epoch  1, batch     7 | loss: 5.5679874Losses:  5.49072790145874 0.49152979254722595
CurrentTrain: epoch  1, batch     8 | loss: 5.9822578Losses:  5.091038703918457 0.21083416044712067
CurrentTrain: epoch  1, batch     9 | loss: 5.3018727Losses:  5.099326133728027 0.37741780281066895
CurrentTrain: epoch  1, batch    10 | loss: 5.4767437Losses:  5.498629093170166 0.4559178948402405
CurrentTrain: epoch  1, batch    11 | loss: 5.9545469Losses:  5.281599044799805 0.37907570600509644
CurrentTrain: epoch  1, batch    12 | loss: 5.6606746Losses:  5.109667778015137 0.503732442855835
CurrentTrain: epoch  1, batch    13 | loss: 5.6134005Losses:  5.513184547424316 0.413519024848938
CurrentTrain: epoch  1, batch    14 | loss: 5.9267035Losses:  5.555082321166992 0.5053688287734985
CurrentTrain: epoch  1, batch    15 | loss: 6.0604510Losses:  5.031512260437012 0.2223317176103592
CurrentTrain: epoch  1, batch    16 | loss: 5.2538438Losses:  5.197861671447754 0.4654945135116577
CurrentTrain: epoch  1, batch    17 | loss: 5.6633563Losses:  5.26088809967041 0.29779052734375
CurrentTrain: epoch  1, batch    18 | loss: 5.5586786Losses:  5.433104038238525 0.3824639618396759
CurrentTrain: epoch  1, batch    19 | loss: 5.8155680Losses:  5.702926158905029 0.4588097035884857
CurrentTrain: epoch  1, batch    20 | loss: 6.1617360Losses:  5.226369857788086 0.4925817847251892
CurrentTrain: epoch  1, batch    21 | loss: 5.7189517Losses:  5.362255573272705 0.35843396186828613
CurrentTrain: epoch  1, batch    22 | loss: 5.7206898Losses:  5.0908966064453125 0.3987152874469757
CurrentTrain: epoch  1, batch    23 | loss: 5.4896121Losses:  5.923773288726807 0.7077541351318359
CurrentTrain: epoch  1, batch    24 | loss: 6.6315274Losses:  5.34525728225708 0.49357569217681885
CurrentTrain: epoch  1, batch    25 | loss: 5.8388329Losses:  5.298674583435059 0.390201210975647
CurrentTrain: epoch  1, batch    26 | loss: 5.6888757Losses:  5.410502910614014 0.4423131048679352
CurrentTrain: epoch  1, batch    27 | loss: 5.8528161Losses:  5.546481132507324 0.5595468282699585
CurrentTrain: epoch  1, batch    28 | loss: 6.1060281Losses:  4.975376129150391 0.3984995484352112
CurrentTrain: epoch  1, batch    29 | loss: 5.3738756Losses:  4.915999412536621 0.19800056517124176
CurrentTrain: epoch  1, batch    30 | loss: 5.1139998Losses:  5.2646918296813965 0.30522409081459045
CurrentTrain: epoch  1, batch    31 | loss: 5.5699158Losses:  5.690412998199463 0.3899107575416565
CurrentTrain: epoch  1, batch    32 | loss: 6.0803237Losses:  5.6356201171875 0.5036718845367432
CurrentTrain: epoch  1, batch    33 | loss: 6.1392918Losses:  5.162271976470947 0.4072617292404175
CurrentTrain: epoch  1, batch    34 | loss: 5.5695338Losses:  5.890151500701904 0.9605138301849365
CurrentTrain: epoch  1, batch    35 | loss: 6.8506651Losses:  6.3613080978393555 0.7760124802589417
CurrentTrain: epoch  1, batch    36 | loss: 7.1373205Losses:  5.377236843109131 0.4914647340774536
CurrentTrain: epoch  1, batch    37 | loss: 5.8687015Losses:  5.593014717102051 0.31428396701812744
CurrentTrain: epoch  1, batch    38 | loss: 5.9072986Losses:  5.4427313804626465 0.43918293714523315
CurrentTrain: epoch  1, batch    39 | loss: 5.8819141Losses:  4.906903266906738 0.27315860986709595
CurrentTrain: epoch  1, batch    40 | loss: 5.1800618Losses:  5.2325825691223145 0.2531433701515198
CurrentTrain: epoch  1, batch    41 | loss: 5.4857259Losses:  5.352694511413574 0.42391860485076904
CurrentTrain: epoch  1, batch    42 | loss: 5.7766132Losses:  5.293426036834717 0.6352417469024658
CurrentTrain: epoch  1, batch    43 | loss: 5.9286680Losses:  5.187363624572754 0.3817555010318756
CurrentTrain: epoch  1, batch    44 | loss: 5.5691190Losses:  4.851864814758301 0.18412348628044128
CurrentTrain: epoch  1, batch    45 | loss: 5.0359883Losses:  5.045098304748535 0.35798436403274536
CurrentTrain: epoch  1, batch    46 | loss: 5.4030828Losses:  5.228693962097168 0.4930271804332733
CurrentTrain: epoch  1, batch    47 | loss: 5.7217212Losses:  5.167304992675781 0.27580228447914124
CurrentTrain: epoch  1, batch    48 | loss: 5.4431071Losses:  5.443298816680908 0.5453972816467285
CurrentTrain: epoch  1, batch    49 | loss: 5.9886961Losses:  5.096627235412598 0.3391415774822235
CurrentTrain: epoch  1, batch    50 | loss: 5.4357686Losses:  5.228230953216553 0.4978978633880615
CurrentTrain: epoch  1, batch    51 | loss: 5.7261286Losses:  5.648405075073242 0.4742452800273895
CurrentTrain: epoch  1, batch    52 | loss: 6.1226501Losses:  4.693365097045898 0.20008882880210876
CurrentTrain: epoch  1, batch    53 | loss: 4.8934541Losses:  5.2571258544921875 0.5212609171867371
CurrentTrain: epoch  1, batch    54 | loss: 5.7783866Losses:  4.679971694946289 0.421394407749176
CurrentTrain: epoch  1, batch    55 | loss: 5.1013660Losses:  4.878626346588135 0.2570834755897522
CurrentTrain: epoch  1, batch    56 | loss: 5.1357098Losses:  4.942411422729492 0.3477432429790497
CurrentTrain: epoch  1, batch    57 | loss: 5.2901545Losses:  5.017838478088379 0.24495671689510345
CurrentTrain: epoch  1, batch    58 | loss: 5.2627950Losses:  5.452276229858398 0.45186686515808105
CurrentTrain: epoch  1, batch    59 | loss: 5.9041433Losses:  5.087225914001465 0.3292948007583618
CurrentTrain: epoch  1, batch    60 | loss: 5.4165206Losses:  5.116013526916504 0.24954897165298462
CurrentTrain: epoch  1, batch    61 | loss: 5.3655624Losses:  4.710514068603516 0.18107208609580994
CurrentTrain: epoch  1, batch    62 | loss: 4.8915863Losses:  5.205288410186768 0.423217236995697
CurrentTrain: epoch  1, batch    63 | loss: 5.6285057Losses:  4.941503047943115 0.3255012631416321
CurrentTrain: epoch  1, batch    64 | loss: 5.2670045Losses:  4.874062538146973 0.3135563135147095
CurrentTrain: epoch  1, batch    65 | loss: 5.1876187Losses:  5.003349304199219 0.4028581976890564
CurrentTrain: epoch  1, batch    66 | loss: 5.4062076Losses:  4.554470062255859 0.2240709513425827
CurrentTrain: epoch  1, batch    67 | loss: 4.7785411Losses:  4.551323890686035 0.37776702642440796
CurrentTrain: epoch  1, batch    68 | loss: 4.9290910Losses:  4.681596755981445 0.16621506214141846
CurrentTrain: epoch  1, batch    69 | loss: 4.8478117Losses:  4.899408340454102 0.24003763496875763
CurrentTrain: epoch  1, batch    70 | loss: 5.1394458Losses:  4.886221885681152 0.3654300570487976
CurrentTrain: epoch  1, batch    71 | loss: 5.2516518Losses:  4.94126558303833 0.3879636228084564
CurrentTrain: epoch  1, batch    72 | loss: 5.3292294Losses:  4.906269073486328 0.349385142326355
CurrentTrain: epoch  1, batch    73 | loss: 5.2556543Losses:  5.281191825866699 0.26244744658470154
CurrentTrain: epoch  1, batch    74 | loss: 5.5436392Losses:  5.482316017150879 0.3302852511405945
CurrentTrain: epoch  1, batch    75 | loss: 5.8126011Losses:  4.795846939086914 0.2611442804336548
CurrentTrain: epoch  1, batch    76 | loss: 5.0569911Losses:  4.850887298583984 0.19727584719657898
CurrentTrain: epoch  1, batch    77 | loss: 5.0481629Losses:  4.944742202758789 0.1948312222957611
CurrentTrain: epoch  1, batch    78 | loss: 5.1395736Losses:  4.843574523925781 0.23297138512134552
CurrentTrain: epoch  1, batch    79 | loss: 5.0765457Losses:  5.061096668243408 0.36705678701400757
CurrentTrain: epoch  1, batch    80 | loss: 5.4281535Losses:  5.3585968017578125 0.35590797662734985
CurrentTrain: epoch  1, batch    81 | loss: 5.7145047Losses:  5.523140907287598 0.39888307452201843
CurrentTrain: epoch  1, batch    82 | loss: 5.9220238Losses:  4.839201927185059 0.26675406098365784
CurrentTrain: epoch  1, batch    83 | loss: 5.1059561Losses:  5.548491477966309 0.6781672835350037
CurrentTrain: epoch  1, batch    84 | loss: 6.2266588Losses:  5.003696441650391 0.31168293952941895
CurrentTrain: epoch  1, batch    85 | loss: 5.3153791Losses:  5.07602596282959 0.27498236298561096
CurrentTrain: epoch  1, batch    86 | loss: 5.3510084Losses:  4.494403839111328 0.24967722594738007
CurrentTrain: epoch  1, batch    87 | loss: 4.7440810Losses:  5.054270267486572 0.36277055740356445
CurrentTrain: epoch  1, batch    88 | loss: 5.4170408Losses:  4.8343400955200195 0.3660021424293518
CurrentTrain: epoch  1, batch    89 | loss: 5.2003422Losses:  4.638850212097168 0.1978173702955246
CurrentTrain: epoch  1, batch    90 | loss: 4.8366675Losses:  4.893784999847412 0.21503613889217377
CurrentTrain: epoch  1, batch    91 | loss: 5.1088209Losses:  5.293121337890625 0.4593883156776428
CurrentTrain: epoch  1, batch    92 | loss: 5.7525096Losses:  4.690096855163574 0.33795681595802307
CurrentTrain: epoch  1, batch    93 | loss: 5.0280538Losses:  4.397606372833252 0.14182429015636444
CurrentTrain: epoch  1, batch    94 | loss: 4.5394306Losses:  4.710634708404541 0.28606465458869934
CurrentTrain: epoch  1, batch    95 | loss: 4.9966993Losses:  4.601681709289551 0.32307979464530945
CurrentTrain: epoch  1, batch    96 | loss: 4.9247613Losses:  4.756994247436523 0.3613028824329376
CurrentTrain: epoch  1, batch    97 | loss: 5.1182971Losses:  4.800014019012451 0.4290549159049988
CurrentTrain: epoch  1, batch    98 | loss: 5.2290688Losses:  4.58419942855835 0.28996771574020386
CurrentTrain: epoch  1, batch    99 | loss: 4.8741670Losses:  4.440716743469238 0.2609725594520569
CurrentTrain: epoch  1, batch   100 | loss: 4.7016892Losses:  4.611861705780029 0.2575305700302124
CurrentTrain: epoch  1, batch   101 | loss: 4.8693924Losses:  4.62790060043335 0.31092363595962524
CurrentTrain: epoch  1, batch   102 | loss: 4.9388242Losses:  5.022343635559082 0.23294036090373993
CurrentTrain: epoch  1, batch   103 | loss: 5.2552838Losses:  4.508740425109863 0.15878474712371826
CurrentTrain: epoch  1, batch   104 | loss: 4.6675253Losses:  4.853927135467529 0.4814017415046692
CurrentTrain: epoch  1, batch   105 | loss: 5.3353291Losses:  4.63983154296875 0.36575669050216675
CurrentTrain: epoch  1, batch   106 | loss: 5.0055881Losses:  4.7600836753845215 0.2361431121826172
CurrentTrain: epoch  1, batch   107 | loss: 4.9962268Losses:  4.512389183044434 0.22585609555244446
CurrentTrain: epoch  1, batch   108 | loss: 4.7382455Losses:  4.606374740600586 0.2775077223777771
CurrentTrain: epoch  1, batch   109 | loss: 4.8838825Losses:  4.331809997558594 0.20650021731853485
CurrentTrain: epoch  1, batch   110 | loss: 4.5383101Losses:  5.171084403991699 0.3569612205028534
CurrentTrain: epoch  1, batch   111 | loss: 5.5280457Losses:  4.699835777282715 0.3591073155403137
CurrentTrain: epoch  1, batch   112 | loss: 5.0589433Losses:  4.9795050621032715 0.42038989067077637
CurrentTrain: epoch  1, batch   113 | loss: 5.3998947Losses:  5.119881629943848 0.22212716937065125
CurrentTrain: epoch  1, batch   114 | loss: 5.3420086Losses:  4.613636016845703 0.29448336362838745
CurrentTrain: epoch  1, batch   115 | loss: 4.9081192Losses:  4.89313268661499 0.23989492654800415
CurrentTrain: epoch  1, batch   116 | loss: 5.1330276Losses:  4.721403121948242 0.28385037183761597
CurrentTrain: epoch  1, batch   117 | loss: 5.0052533Losses:  4.366809844970703 0.23217086493968964
CurrentTrain: epoch  1, batch   118 | loss: 4.5989809Losses:  4.525338172912598 0.2905859053134918
CurrentTrain: epoch  1, batch   119 | loss: 4.8159242Losses:  4.768856048583984 0.21417073905467987
CurrentTrain: epoch  1, batch   120 | loss: 4.9830270Losses:  4.581892490386963 0.15187838673591614
CurrentTrain: epoch  1, batch   121 | loss: 4.7337708Losses:  4.640362739562988 0.20015490055084229
CurrentTrain: epoch  1, batch   122 | loss: 4.8405175Losses:  4.64017391204834 0.40481728315353394
CurrentTrain: epoch  1, batch   123 | loss: 5.0449910Losses:  4.481797695159912 0.35872530937194824
CurrentTrain: epoch  1, batch   124 | loss: 4.8405228Losses:  4.584918022155762 0.25601518154144287
CurrentTrain: epoch  2, batch     0 | loss: 4.8409333Losses:  4.4008684158325195 0.18132160604000092
CurrentTrain: epoch  2, batch     1 | loss: 4.5821900Losses:  4.374748706817627 0.17836642265319824
CurrentTrain: epoch  2, batch     2 | loss: 4.5531149Losses:  4.465008735656738 0.17731183767318726
CurrentTrain: epoch  2, batch     3 | loss: 4.6423206Losses:  4.571957588195801 0.15926674008369446
CurrentTrain: epoch  2, batch     4 | loss: 4.7312245Losses:  4.846637725830078 0.2858629524707794
CurrentTrain: epoch  2, batch     5 | loss: 5.1325006Losses:  4.204350471496582 0.17406736314296722
CurrentTrain: epoch  2, batch     6 | loss: 4.3784180Losses:  4.389230728149414 0.12329144775867462
CurrentTrain: epoch  2, batch     7 | loss: 4.5125222Losses:  4.31142520904541 0.24597209692001343
CurrentTrain: epoch  2, batch     8 | loss: 4.5573974Losses:  4.754454612731934 0.24844516813755035
CurrentTrain: epoch  2, batch     9 | loss: 5.0028996Losses:  4.1921868324279785 0.2189558893442154
CurrentTrain: epoch  2, batch    10 | loss: 4.4111428Losses:  4.508995056152344 0.23961122334003448
CurrentTrain: epoch  2, batch    11 | loss: 4.7486062Losses:  5.215011119842529 0.27502113580703735
CurrentTrain: epoch  2, batch    12 | loss: 5.4900322Losses:  4.157962322235107 0.12289300560951233
CurrentTrain: epoch  2, batch    13 | loss: 4.2808552Losses:  4.539068698883057 0.229160875082016
CurrentTrain: epoch  2, batch    14 | loss: 4.7682295Losses:  4.234901428222656 0.13560019433498383
CurrentTrain: epoch  2, batch    15 | loss: 4.3705015Losses:  4.882626533508301 0.4468992352485657
CurrentTrain: epoch  2, batch    16 | loss: 5.3295259Losses:  4.192185401916504 0.17009082436561584
CurrentTrain: epoch  2, batch    17 | loss: 4.3622761Losses:  4.404527187347412 0.3287598490715027
CurrentTrain: epoch  2, batch    18 | loss: 4.7332869Losses:  4.539217948913574 0.1459861695766449
CurrentTrain: epoch  2, batch    19 | loss: 4.6852040Losses:  4.378689765930176 0.18424077332019806
CurrentTrain: epoch  2, batch    20 | loss: 4.5629306Losses:  4.386022567749023 0.21767032146453857
CurrentTrain: epoch  2, batch    21 | loss: 4.6036930Losses:  5.038167476654053 0.3093443512916565
CurrentTrain: epoch  2, batch    22 | loss: 5.3475118Losses:  4.237400054931641 0.26297616958618164
CurrentTrain: epoch  2, batch    23 | loss: 4.5003762Losses:  4.317233085632324 0.15099141001701355
CurrentTrain: epoch  2, batch    24 | loss: 4.4682245Losses:  4.332649230957031 0.1884985715150833
CurrentTrain: epoch  2, batch    25 | loss: 4.5211477Losses:  4.294106483459473 0.12861311435699463
CurrentTrain: epoch  2, batch    26 | loss: 4.4227195Losses:  4.320188522338867 0.2033892571926117
CurrentTrain: epoch  2, batch    27 | loss: 4.5235777Losses:  4.380252838134766 0.11607811599969864
CurrentTrain: epoch  2, batch    28 | loss: 4.4963307Losses:  4.3338165283203125 0.1207367330789566
CurrentTrain: epoch  2, batch    29 | loss: 4.4545531Losses:  4.476365089416504 0.1907590627670288
CurrentTrain: epoch  2, batch    30 | loss: 4.6671243Losses:  4.348593711853027 0.2547265887260437
CurrentTrain: epoch  2, batch    31 | loss: 4.6033201Losses:  4.400329113006592 0.23989790678024292
CurrentTrain: epoch  2, batch    32 | loss: 4.6402268Losses:  4.48938512802124 0.20333272218704224
CurrentTrain: epoch  2, batch    33 | loss: 4.6927180Losses:  4.327134609222412 0.15246841311454773
CurrentTrain: epoch  2, batch    34 | loss: 4.4796028Losses:  4.424300193786621 0.2030775249004364
CurrentTrain: epoch  2, batch    35 | loss: 4.6273775Losses:  4.335190773010254 0.17205943167209625
CurrentTrain: epoch  2, batch    36 | loss: 4.5072503Losses:  4.438987731933594 0.17539793252944946
CurrentTrain: epoch  2, batch    37 | loss: 4.6143856Losses:  4.480175971984863 0.24232080578804016
CurrentTrain: epoch  2, batch    38 | loss: 4.7224970Losses:  4.373398303985596 0.11805136501789093
CurrentTrain: epoch  2, batch    39 | loss: 4.4914498Losses:  4.198725700378418 0.12361408770084381
CurrentTrain: epoch  2, batch    40 | loss: 4.3223400Losses:  4.248339653015137 0.17976418137550354
CurrentTrain: epoch  2, batch    41 | loss: 4.4281039Losses:  4.26442813873291 0.1690565049648285
CurrentTrain: epoch  2, batch    42 | loss: 4.4334846Losses:  4.388234615325928 0.2517377436161041
CurrentTrain: epoch  2, batch    43 | loss: 4.6399722Losses:  4.480241298675537 0.18134137988090515
CurrentTrain: epoch  2, batch    44 | loss: 4.6615825Losses:  4.287538528442383 0.2633274495601654
CurrentTrain: epoch  2, batch    45 | loss: 4.5508661Losses:  4.631341934204102 0.22850051522254944
CurrentTrain: epoch  2, batch    46 | loss: 4.8598423Losses:  4.240555763244629 0.08045557141304016
CurrentTrain: epoch  2, batch    47 | loss: 4.3210115Losses:  4.321215629577637 0.1519370973110199
CurrentTrain: epoch  2, batch    48 | loss: 4.4731526Losses:  4.483277320861816 0.1684418022632599
CurrentTrain: epoch  2, batch    49 | loss: 4.6517191Losses:  4.351202487945557 0.1532910317182541
CurrentTrain: epoch  2, batch    50 | loss: 4.5044937Losses:  4.19127082824707 0.21799518167972565
CurrentTrain: epoch  2, batch    51 | loss: 4.4092660Losses:  4.250561237335205 0.21775861084461212
CurrentTrain: epoch  2, batch    52 | loss: 4.4683199Losses:  4.3837995529174805 0.1445167064666748
CurrentTrain: epoch  2, batch    53 | loss: 4.5283165Losses:  4.246211051940918 0.1728636920452118
CurrentTrain: epoch  2, batch    54 | loss: 4.4190745Losses:  5.545141220092773 0.1500292718410492
CurrentTrain: epoch  2, batch    55 | loss: 5.6951704Losses:  4.351325988769531 0.16141945123672485
CurrentTrain: epoch  2, batch    56 | loss: 4.5127454Losses:  4.149285793304443 0.229336217045784
CurrentTrain: epoch  2, batch    57 | loss: 4.3786221Losses:  4.237971782684326 0.23384401202201843
CurrentTrain: epoch  2, batch    58 | loss: 4.4718156Losses:  4.228367805480957 0.15825948119163513
CurrentTrain: epoch  2, batch    59 | loss: 4.3866272Losses:  5.016537189483643 0.10913770645856857
CurrentTrain: epoch  2, batch    60 | loss: 5.1256747Losses:  4.314310073852539 0.12143190205097198
CurrentTrain: epoch  2, batch    61 | loss: 4.4357419Losses:  4.411895751953125 0.18867895007133484
CurrentTrain: epoch  2, batch    62 | loss: 4.6005745Losses:  4.441835403442383 0.24148797988891602
CurrentTrain: epoch  2, batch    63 | loss: 4.6833234Losses:  4.385863304138184 0.13979828357696533
CurrentTrain: epoch  2, batch    64 | loss: 4.5256615Losses:  4.417532920837402 0.17564229667186737
CurrentTrain: epoch  2, batch    65 | loss: 4.5931754Losses:  4.280207633972168 0.08096084743738174
CurrentTrain: epoch  2, batch    66 | loss: 4.3611684Losses:  4.321362495422363 0.17671798169612885
CurrentTrain: epoch  2, batch    67 | loss: 4.4980803Losses:  4.478803634643555 0.12810495495796204
CurrentTrain: epoch  2, batch    68 | loss: 4.6069088Losses:  4.267743110656738 0.20308679342269897
CurrentTrain: epoch  2, batch    69 | loss: 4.4708300Losses:  4.5482330322265625 0.2791844308376312
CurrentTrain: epoch  2, batch    70 | loss: 4.8274174Losses:  4.474301815032959 0.2103378027677536
CurrentTrain: epoch  2, batch    71 | loss: 4.6846395Losses:  4.259518146514893 0.1799708902835846
CurrentTrain: epoch  2, batch    72 | loss: 4.4394889Losses:  4.330521583557129 0.21845906972885132
CurrentTrain: epoch  2, batch    73 | loss: 4.5489807Losses:  4.610771656036377 0.302428275346756
CurrentTrain: epoch  2, batch    74 | loss: 4.9131999Losses:  4.17944860458374 0.17124046385288239
CurrentTrain: epoch  2, batch    75 | loss: 4.3506889Losses:  4.31559944152832 0.2029159963130951
CurrentTrain: epoch  2, batch    76 | loss: 4.5185156Losses:  4.2070770263671875 0.29143938422203064
CurrentTrain: epoch  2, batch    77 | loss: 4.4985166Losses:  4.359893798828125 0.17579728364944458
CurrentTrain: epoch  2, batch    78 | loss: 4.5356913Losses:  4.269059181213379 0.14168301224708557
CurrentTrain: epoch  2, batch    79 | loss: 4.4107423Losses:  4.147231578826904 0.1103324443101883
CurrentTrain: epoch  2, batch    80 | loss: 4.2575641Losses:  4.276193618774414 0.11199159920215607
CurrentTrain: epoch  2, batch    81 | loss: 4.3881850Losses:  4.434597492218018 0.1862417757511139
CurrentTrain: epoch  2, batch    82 | loss: 4.6208391Losses:  4.276571273803711 0.14968733489513397
CurrentTrain: epoch  2, batch    83 | loss: 4.4262586Losses:  4.238064765930176 0.07814587652683258
CurrentTrain: epoch  2, batch    84 | loss: 4.3162107Losses:  4.139116287231445 0.1122344583272934
CurrentTrain: epoch  2, batch    85 | loss: 4.2513509Losses:  4.2284746170043945 0.10085851699113846
CurrentTrain: epoch  2, batch    86 | loss: 4.3293333Losses:  4.293920993804932 0.2243908792734146
CurrentTrain: epoch  2, batch    87 | loss: 4.5183120Losses:  4.153329372406006 0.11142392456531525
CurrentTrain: epoch  2, batch    88 | loss: 4.2647533Losses:  4.035336971282959 0.1652337908744812
CurrentTrain: epoch  2, batch    89 | loss: 4.2005706Losses:  4.392106056213379 0.11160317063331604
CurrentTrain: epoch  2, batch    90 | loss: 4.5037093Losses:  4.226570129394531 0.11098900437355042
CurrentTrain: epoch  2, batch    91 | loss: 4.3375592Losses:  4.225874423980713 0.19278484582901
CurrentTrain: epoch  2, batch    92 | loss: 4.4186592Losses:  4.648287296295166 0.2912787199020386
CurrentTrain: epoch  2, batch    93 | loss: 4.9395661Losses:  4.4225969314575195 0.2250012457370758
CurrentTrain: epoch  2, batch    94 | loss: 4.6475983Losses:  4.58205509185791 0.16999676823616028
CurrentTrain: epoch  2, batch    95 | loss: 4.7520518Losses:  4.281367778778076 0.17583391070365906
CurrentTrain: epoch  2, batch    96 | loss: 4.4572015Losses:  4.3137712478637695 0.205698162317276
CurrentTrain: epoch  2, batch    97 | loss: 4.5194693Losses:  4.363083362579346 0.1626579463481903
CurrentTrain: epoch  2, batch    98 | loss: 4.5257411Losses:  4.399585723876953 0.1862475872039795
CurrentTrain: epoch  2, batch    99 | loss: 4.5858335Losses:  4.553266525268555 0.1620597243309021
CurrentTrain: epoch  2, batch   100 | loss: 4.7153263Losses:  4.292299270629883 0.11082252860069275
CurrentTrain: epoch  2, batch   101 | loss: 4.4031219Losses:  4.50651216506958 0.18652155995368958
CurrentTrain: epoch  2, batch   102 | loss: 4.6930337Losses:  4.236551284790039 0.12679149210453033
CurrentTrain: epoch  2, batch   103 | loss: 4.3633428Losses:  4.537571430206299 0.2247670739889145
CurrentTrain: epoch  2, batch   104 | loss: 4.7623386Losses:  4.277783393859863 0.10565890371799469
CurrentTrain: epoch  2, batch   105 | loss: 4.3834424Losses:  4.182635307312012 0.12431430071592331
CurrentTrain: epoch  2, batch   106 | loss: 4.3069496Losses:  4.3229899406433105 0.11790670454502106
CurrentTrain: epoch  2, batch   107 | loss: 4.4408965Losses:  4.188880443572998 0.06821886450052261
CurrentTrain: epoch  2, batch   108 | loss: 4.2570992Losses:  4.150714874267578 0.11449775844812393
CurrentTrain: epoch  2, batch   109 | loss: 4.2652125Losses:  4.67616081237793 0.26831555366516113
CurrentTrain: epoch  2, batch   110 | loss: 4.9444761Losses:  4.260478496551514 0.1663302183151245
CurrentTrain: epoch  2, batch   111 | loss: 4.4268088Losses:  4.212102890014648 0.11800587177276611
CurrentTrain: epoch  2, batch   112 | loss: 4.3301086Losses:  4.111869812011719 0.12073875218629837
CurrentTrain: epoch  2, batch   113 | loss: 4.2326088Losses:  4.11236572265625 0.1320163905620575
CurrentTrain: epoch  2, batch   114 | loss: 4.2443819Losses:  4.634162902832031 0.1617957055568695
CurrentTrain: epoch  2, batch   115 | loss: 4.7959585Losses:  4.048764228820801 0.19705329835414886
CurrentTrain: epoch  2, batch   116 | loss: 4.2458177Losses:  5.076990127563477 0.5058983564376831
CurrentTrain: epoch  2, batch   117 | loss: 5.5828886Losses:  4.35965633392334 0.1937953382730484
CurrentTrain: epoch  2, batch   118 | loss: 4.5534515Losses:  4.093290328979492 0.08588361740112305
CurrentTrain: epoch  2, batch   119 | loss: 4.1791739Losses:  4.154988765716553 0.17116409540176392
CurrentTrain: epoch  2, batch   120 | loss: 4.3261528Losses:  4.184009075164795 0.23171404004096985
CurrentTrain: epoch  2, batch   121 | loss: 4.4157233Losses:  4.100516319274902 0.1436648666858673
CurrentTrain: epoch  2, batch   122 | loss: 4.2441812Losses:  4.259281158447266 0.10515902936458588
CurrentTrain: epoch  2, batch   123 | loss: 4.3644400Losses:  4.255598068237305 0.12924128770828247
CurrentTrain: epoch  2, batch   124 | loss: 4.3848395Losses:  4.090600490570068 0.08259622752666473
CurrentTrain: epoch  3, batch     0 | loss: 4.1731968Losses:  4.163550853729248 0.1492127627134323
CurrentTrain: epoch  3, batch     1 | loss: 4.3127637Losses:  4.362401008605957 0.21825459599494934
CurrentTrain: epoch  3, batch     2 | loss: 4.5806556Losses:  4.231392860412598 0.15757668018341064
CurrentTrain: epoch  3, batch     3 | loss: 4.3889694Losses:  4.160304546356201 0.10776882618665695
CurrentTrain: epoch  3, batch     4 | loss: 4.2680736Losses:  4.282258987426758 0.10899179428815842
CurrentTrain: epoch  3, batch     5 | loss: 4.3912506Losses:  4.251347541809082 0.1796465814113617
CurrentTrain: epoch  3, batch     6 | loss: 4.4309940Losses:  4.185120105743408 0.09675706177949905
CurrentTrain: epoch  3, batch     7 | loss: 4.2818770Losses:  4.121707916259766 0.1172650158405304
CurrentTrain: epoch  3, batch     8 | loss: 4.2389731Losses:  4.021393299102783 0.12781856954097748
CurrentTrain: epoch  3, batch     9 | loss: 4.1492119Losses:  4.158744812011719 0.1980200707912445
CurrentTrain: epoch  3, batch    10 | loss: 4.3567648Losses:  4.112532615661621 0.11078458279371262
CurrentTrain: epoch  3, batch    11 | loss: 4.2233171Losses:  4.0825581550598145 0.09802614897489548
CurrentTrain: epoch  3, batch    12 | loss: 4.1805844Losses:  4.192785739898682 0.11617735028266907
CurrentTrain: epoch  3, batch    13 | loss: 4.3089633Losses:  4.150330543518066 0.12848415970802307
CurrentTrain: epoch  3, batch    14 | loss: 4.2788148Losses:  4.146420001983643 0.18287378549575806
CurrentTrain: epoch  3, batch    15 | loss: 4.3292937Losses:  4.113457679748535 0.2026093304157257
CurrentTrain: epoch  3, batch    16 | loss: 4.3160672Losses:  4.069687843322754 0.14881634712219238
CurrentTrain: epoch  3, batch    17 | loss: 4.2185040Losses:  4.1286845207214355 0.14119742810726166
CurrentTrain: epoch  3, batch    18 | loss: 4.2698817Losses:  4.132509231567383 0.1976747363805771
CurrentTrain: epoch  3, batch    19 | loss: 4.3301840Losses:  4.048662185668945 0.10067272931337357
CurrentTrain: epoch  3, batch    20 | loss: 4.1493349Losses:  4.1733551025390625 0.13975784182548523
CurrentTrain: epoch  3, batch    21 | loss: 4.3131127Losses:  4.070606231689453 0.19878220558166504
CurrentTrain: epoch  3, batch    22 | loss: 4.2693882Losses:  4.259607791900635 0.2164137363433838
CurrentTrain: epoch  3, batch    23 | loss: 4.4760218Losses:  5.289544105529785 0.22185321152210236
CurrentTrain: epoch  3, batch    24 | loss: 5.5113974Losses:  4.0685272216796875 0.0935872346162796
CurrentTrain: epoch  3, batch    25 | loss: 4.1621146Losses:  4.2706098556518555 0.10443995893001556
CurrentTrain: epoch  3, batch    26 | loss: 4.3750496Losses:  4.2102155685424805 0.09063202887773514
CurrentTrain: epoch  3, batch    27 | loss: 4.3008475Losses:  4.163826942443848 0.1439162641763687
CurrentTrain: epoch  3, batch    28 | loss: 4.3077431Losses:  4.135738849639893 0.16746598482131958
CurrentTrain: epoch  3, batch    29 | loss: 4.3032050Losses:  4.08753776550293 0.11309131979942322
CurrentTrain: epoch  3, batch    30 | loss: 4.2006292Losses:  4.568380832672119 0.17472052574157715
CurrentTrain: epoch  3, batch    31 | loss: 4.7431011Losses:  4.11293888092041 0.1279115378856659
CurrentTrain: epoch  3, batch    32 | loss: 4.2408504Losses:  4.068909168243408 0.07474097609519958
CurrentTrain: epoch  3, batch    33 | loss: 4.1436501Losses:  4.273925304412842 0.13235020637512207
CurrentTrain: epoch  3, batch    34 | loss: 4.4062757Losses:  4.092597484588623 0.20033803582191467
CurrentTrain: epoch  3, batch    35 | loss: 4.2929354Losses:  4.076526641845703 0.12324050813913345
CurrentTrain: epoch  3, batch    36 | loss: 4.1997671Losses:  4.122467994689941 0.15239693224430084
CurrentTrain: epoch  3, batch    37 | loss: 4.2748652Losses:  4.106919288635254 0.08551532030105591
CurrentTrain: epoch  3, batch    38 | loss: 4.1924348Losses:  3.984196186065674 0.08653837442398071
CurrentTrain: epoch  3, batch    39 | loss: 4.0707345Losses:  4.318094253540039 0.24753832817077637
CurrentTrain: epoch  3, batch    40 | loss: 4.5656328Losses:  4.161445617675781 0.15874938666820526
CurrentTrain: epoch  3, batch    41 | loss: 4.3201952Losses:  4.02412748336792 0.15150386095046997
CurrentTrain: epoch  3, batch    42 | loss: 4.1756315Losses:  4.073337554931641 0.1127551943063736
CurrentTrain: epoch  3, batch    43 | loss: 4.1860929Losses:  4.4763383865356445 0.12844988703727722
CurrentTrain: epoch  3, batch    44 | loss: 4.6047883Losses:  4.069741725921631 0.09969057142734528
CurrentTrain: epoch  3, batch    45 | loss: 4.1694322Losses:  4.060361862182617 0.07461801171302795
CurrentTrain: epoch  3, batch    46 | loss: 4.1349797Losses:  4.063884735107422 0.08704838901758194
CurrentTrain: epoch  3, batch    47 | loss: 4.1509333Losses:  4.058666229248047 0.1318531185388565
CurrentTrain: epoch  3, batch    48 | loss: 4.1905193Losses:  4.10776948928833 0.13705655932426453
CurrentTrain: epoch  3, batch    49 | loss: 4.2448258Losses:  4.069842338562012 0.07887265086174011
CurrentTrain: epoch  3, batch    50 | loss: 4.1487150Losses:  4.085720062255859 0.07479528337717056
CurrentTrain: epoch  3, batch    51 | loss: 4.1605153Losses:  4.192741870880127 0.1054118424654007
CurrentTrain: epoch  3, batch    52 | loss: 4.2981539Losses:  4.118488311767578 0.12052594125270844
CurrentTrain: epoch  3, batch    53 | loss: 4.2390141Losses:  4.0906829833984375 0.10138025879859924
CurrentTrain: epoch  3, batch    54 | loss: 4.1920633Losses:  4.109016418457031 0.12586301565170288
CurrentTrain: epoch  3, batch    55 | loss: 4.2348795Losses:  4.126589775085449 0.1301150768995285
CurrentTrain: epoch  3, batch    56 | loss: 4.2567048Losses:  4.050281524658203 0.0827600359916687
CurrentTrain: epoch  3, batch    57 | loss: 4.1330414Losses:  4.17172908782959 0.0959988459944725
CurrentTrain: epoch  3, batch    58 | loss: 4.2677279Losses:  4.072925090789795 0.13289734721183777
CurrentTrain: epoch  3, batch    59 | loss: 4.2058225Losses:  4.025701999664307 0.11207739263772964
CurrentTrain: epoch  3, batch    60 | loss: 4.1377792Losses:  4.1633782386779785 0.08193373680114746
CurrentTrain: epoch  3, batch    61 | loss: 4.2453117Losses:  4.128161430358887 0.20886382460594177
CurrentTrain: epoch  3, batch    62 | loss: 4.3370252Losses:  4.258677005767822 0.06969162821769714
CurrentTrain: epoch  3, batch    63 | loss: 4.3283687Losses:  4.0460405349731445 0.07691113650798798
CurrentTrain: epoch  3, batch    64 | loss: 4.1229515Losses:  4.0309906005859375 0.09501328319311142
CurrentTrain: epoch  3, batch    65 | loss: 4.1260037Losses:  4.1740288734436035 0.09073005616664886
CurrentTrain: epoch  3, batch    66 | loss: 4.2647591Losses:  4.034635066986084 0.1731109768152237
CurrentTrain: epoch  3, batch    67 | loss: 4.2077460Losses:  4.097553730010986 0.07289473712444305
CurrentTrain: epoch  3, batch    68 | loss: 4.1704483Losses:  4.043882846832275 0.05651090666651726
CurrentTrain: epoch  3, batch    69 | loss: 4.1003938Losses:  4.138848304748535 0.07414815574884415
CurrentTrain: epoch  3, batch    70 | loss: 4.2129965Losses:  4.086462020874023 0.05648917704820633
CurrentTrain: epoch  3, batch    71 | loss: 4.1429510Losses:  4.010761737823486 0.11681737750768661
CurrentTrain: epoch  3, batch    72 | loss: 4.1275792Losses:  4.293531894683838 0.14489850401878357
CurrentTrain: epoch  3, batch    73 | loss: 4.4384303Losses:  4.026689529418945 0.11050516366958618
CurrentTrain: epoch  3, batch    74 | loss: 4.1371946Losses:  4.060530185699463 0.11384317278862
CurrentTrain: epoch  3, batch    75 | loss: 4.1743731Losses:  4.032681465148926 0.1526377648115158
CurrentTrain: epoch  3, batch    76 | loss: 4.1853194Losses:  3.948009490966797 0.10037636756896973
CurrentTrain: epoch  3, batch    77 | loss: 4.0483856Losses:  4.027825355529785 0.09105400741100311
CurrentTrain: epoch  3, batch    78 | loss: 4.1188793Losses:  4.278584957122803 0.25109899044036865
CurrentTrain: epoch  3, batch    79 | loss: 4.5296841Losses:  4.3857269287109375 0.09972003102302551
CurrentTrain: epoch  3, batch    80 | loss: 4.4854469Losses:  4.306577205657959 0.12204715609550476
CurrentTrain: epoch  3, batch    81 | loss: 4.4286242Losses:  4.092101097106934 0.13159571588039398
CurrentTrain: epoch  3, batch    82 | loss: 4.2236967Losses:  4.027925968170166 0.07201816141605377
CurrentTrain: epoch  3, batch    83 | loss: 4.0999441Losses:  4.265432834625244 0.1365043669939041
CurrentTrain: epoch  3, batch    84 | loss: 4.4019370Losses:  3.947535991668701 0.03851810097694397
CurrentTrain: epoch  3, batch    85 | loss: 3.9860542Losses:  3.932957172393799 0.0419367253780365
CurrentTrain: epoch  3, batch    86 | loss: 3.9748938Losses:  4.071301460266113 0.07503144443035126
CurrentTrain: epoch  3, batch    87 | loss: 4.1463327Losses:  4.179122447967529 0.12199914455413818
CurrentTrain: epoch  3, batch    88 | loss: 4.3011217Losses:  4.192087173461914 0.10862770676612854
CurrentTrain: epoch  3, batch    89 | loss: 4.3007150Losses:  4.000774383544922 0.0963154211640358
CurrentTrain: epoch  3, batch    90 | loss: 4.0970898Losses:  4.113831996917725 0.09599791467189789
CurrentTrain: epoch  3, batch    91 | loss: 4.2098298Losses:  4.024425029754639 0.08547629415988922
CurrentTrain: epoch  3, batch    92 | loss: 4.1099014Losses:  4.240119457244873 0.11351973563432693
CurrentTrain: epoch  3, batch    93 | loss: 4.3536391Losses:  4.057178497314453 0.09450899809598923
CurrentTrain: epoch  3, batch    94 | loss: 4.1516876Losses:  3.9911723136901855 0.1569245159626007
CurrentTrain: epoch  3, batch    95 | loss: 4.1480970Losses:  4.158505439758301 0.109990693628788
CurrentTrain: epoch  3, batch    96 | loss: 4.2684960Losses:  4.056906700134277 0.12477494031190872
CurrentTrain: epoch  3, batch    97 | loss: 4.1816816Losses:  4.113790035247803 0.08380361646413803
CurrentTrain: epoch  3, batch    98 | loss: 4.1975937Losses:  4.065544605255127 0.047173697501420975
CurrentTrain: epoch  3, batch    99 | loss: 4.1127181Losses:  4.025473117828369 0.0533493310213089
CurrentTrain: epoch  3, batch   100 | loss: 4.0788226Losses:  4.108198165893555 0.08978447318077087
CurrentTrain: epoch  3, batch   101 | loss: 4.1979828Losses:  4.1304545402526855 0.11923717707395554
CurrentTrain: epoch  3, batch   102 | loss: 4.2496915Losses:  4.083810806274414 0.09273325651884079
CurrentTrain: epoch  3, batch   103 | loss: 4.1765442Losses:  4.042242527008057 0.11100433021783829
CurrentTrain: epoch  3, batch   104 | loss: 4.1532469Losses:  3.9862735271453857 0.09366917610168457
CurrentTrain: epoch  3, batch   105 | loss: 4.0799427Losses:  4.010477066040039 0.0910835787653923
CurrentTrain: epoch  3, batch   106 | loss: 4.1015606Losses:  4.00977897644043 0.08377505838871002
CurrentTrain: epoch  3, batch   107 | loss: 4.0935540Losses:  4.082635879516602 0.1426440179347992
CurrentTrain: epoch  3, batch   108 | loss: 4.2252798Losses:  4.14505672454834 0.16091971099376678
CurrentTrain: epoch  3, batch   109 | loss: 4.3059764Losses:  4.139066696166992 0.17596909403800964
CurrentTrain: epoch  3, batch   110 | loss: 4.3150358Losses:  4.039681434631348 0.1588546484708786
CurrentTrain: epoch  3, batch   111 | loss: 4.1985359Losses:  4.153398513793945 0.09345740079879761
CurrentTrain: epoch  3, batch   112 | loss: 4.2468557Losses:  4.110476970672607 0.10505592077970505
CurrentTrain: epoch  3, batch   113 | loss: 4.2155328Losses:  4.118035316467285 0.030963359400629997
CurrentTrain: epoch  3, batch   114 | loss: 4.1489987Losses:  4.183860778808594 0.2258569747209549
CurrentTrain: epoch  3, batch   115 | loss: 4.4097176Losses:  4.14964485168457 0.1464117467403412
CurrentTrain: epoch  3, batch   116 | loss: 4.2960567Losses:  4.043320178985596 0.09793838858604431
CurrentTrain: epoch  3, batch   117 | loss: 4.1412587Losses:  4.063766002655029 0.08803431689739227
CurrentTrain: epoch  3, batch   118 | loss: 4.1518002Losses:  4.172365665435791 0.20864666998386383
CurrentTrain: epoch  3, batch   119 | loss: 4.3810124Losses:  4.019794464111328 0.10950440913438797
CurrentTrain: epoch  3, batch   120 | loss: 4.1292987Losses:  4.080616474151611 0.11977890133857727
CurrentTrain: epoch  3, batch   121 | loss: 4.2003956Losses:  4.121253967285156 0.09317994117736816
CurrentTrain: epoch  3, batch   122 | loss: 4.2144337Losses:  4.005453586578369 0.08773212134838104
CurrentTrain: epoch  3, batch   123 | loss: 4.0931859Losses:  4.115800857543945 0.06424172967672348
CurrentTrain: epoch  3, batch   124 | loss: 4.1800427Losses:  4.119767189025879 0.09083425998687744
CurrentTrain: epoch  4, batch     0 | loss: 4.2106013Losses:  3.9329824447631836 0.09971282631158829
CurrentTrain: epoch  4, batch     1 | loss: 4.0326953Losses:  4.064807891845703 0.17817997932434082
CurrentTrain: epoch  4, batch     2 | loss: 4.2429876Losses:  4.117844581604004 0.1516740620136261
CurrentTrain: epoch  4, batch     3 | loss: 4.2695189Losses:  4.47743034362793 0.3530186414718628
CurrentTrain: epoch  4, batch     4 | loss: 4.8304491Losses:  4.056389808654785 0.14109213650226593
CurrentTrain: epoch  4, batch     5 | loss: 4.1974821Losses:  4.05440092086792 0.15625815093517303
CurrentTrain: epoch  4, batch     6 | loss: 4.2106590Losses:  4.073085784912109 0.08688335120677948
CurrentTrain: epoch  4, batch     7 | loss: 4.1599693Losses:  4.024221897125244 0.12192811071872711
CurrentTrain: epoch  4, batch     8 | loss: 4.1461501Losses:  4.037965297698975 0.12359345704317093
CurrentTrain: epoch  4, batch     9 | loss: 4.1615586Losses:  4.061476230621338 0.14127995073795319
CurrentTrain: epoch  4, batch    10 | loss: 4.2027564Losses:  3.9617836475372314 0.12021199613809586
CurrentTrain: epoch  4, batch    11 | loss: 4.0819955Losses:  4.1220550537109375 0.07168784737586975
CurrentTrain: epoch  4, batch    12 | loss: 4.1937428Losses:  4.025513648986816 0.10552610456943512
CurrentTrain: epoch  4, batch    13 | loss: 4.1310396Losses:  4.084615707397461 0.08164625614881516
CurrentTrain: epoch  4, batch    14 | loss: 4.1662621Losses:  4.005982875823975 0.10731945931911469
CurrentTrain: epoch  4, batch    15 | loss: 4.1133022Losses:  4.19072151184082 0.08454931527376175
CurrentTrain: epoch  4, batch    16 | loss: 4.2752709Losses:  4.004985332489014 0.08885949850082397
CurrentTrain: epoch  4, batch    17 | loss: 4.0938449Losses:  4.0309672355651855 0.10438612848520279
CurrentTrain: epoch  4, batch    18 | loss: 4.1353536Losses:  4.009529113769531 0.06408403813838959
CurrentTrain: epoch  4, batch    19 | loss: 4.0736132Losses:  4.06721830368042 0.09603951871395111
CurrentTrain: epoch  4, batch    20 | loss: 4.1632576Losses:  4.042267799377441 0.11131155490875244
CurrentTrain: epoch  4, batch    21 | loss: 4.1535792Losses:  4.003211498260498 0.11453483998775482
CurrentTrain: epoch  4, batch    22 | loss: 4.1177464Losses:  4.003775596618652 0.06139947474002838
CurrentTrain: epoch  4, batch    23 | loss: 4.0651751Losses:  4.034087181091309 0.08081212639808655
CurrentTrain: epoch  4, batch    24 | loss: 4.1148992Losses:  4.097985744476318 0.08093134313821793
CurrentTrain: epoch  4, batch    25 | loss: 4.1789169Losses:  4.032388210296631 0.11052156239748001
CurrentTrain: epoch  4, batch    26 | loss: 4.1429100Losses:  4.105895042419434 0.1291874647140503
CurrentTrain: epoch  4, batch    27 | loss: 4.2350826Losses:  4.052815914154053 0.1040010005235672
CurrentTrain: epoch  4, batch    28 | loss: 4.1568170Losses:  4.001286029815674 0.060008034110069275
CurrentTrain: epoch  4, batch    29 | loss: 4.0612941Losses:  3.9779579639434814 0.06117109954357147
CurrentTrain: epoch  4, batch    30 | loss: 4.0391293Losses:  4.048244953155518 0.1619219183921814
CurrentTrain: epoch  4, batch    31 | loss: 4.2101669Losses:  4.078536033630371 0.15624172985553741
CurrentTrain: epoch  4, batch    32 | loss: 4.2347779Losses:  4.033910751342773 0.0851137563586235
CurrentTrain: epoch  4, batch    33 | loss: 4.1190243Losses:  3.965134620666504 0.06308538466691971
CurrentTrain: epoch  4, batch    34 | loss: 4.0282202Losses:  4.015920162200928 0.050165943801403046
CurrentTrain: epoch  4, batch    35 | loss: 4.0660863Losses:  4.052743434906006 0.09894225746393204
CurrentTrain: epoch  4, batch    36 | loss: 4.1516857Losses:  3.984382152557373 0.15227937698364258
CurrentTrain: epoch  4, batch    37 | loss: 4.1366615Losses:  4.047403812408447 0.07805613428354263
CurrentTrain: epoch  4, batch    38 | loss: 4.1254601Losses:  4.101064205169678 0.10434483736753464
CurrentTrain: epoch  4, batch    39 | loss: 4.2054090Losses:  4.0241851806640625 0.11778786033391953
CurrentTrain: epoch  4, batch    40 | loss: 4.1419730Losses:  3.9896469116210938 0.06952480971813202
CurrentTrain: epoch  4, batch    41 | loss: 4.0591717Losses:  4.036221504211426 0.0851849615573883
CurrentTrain: epoch  4, batch    42 | loss: 4.1214066Losses:  4.011834621429443 0.06854771077632904
CurrentTrain: epoch  4, batch    43 | loss: 4.0803823Losses:  3.9822888374328613 0.10161416232585907
CurrentTrain: epoch  4, batch    44 | loss: 4.0839028Losses:  3.9400484561920166 0.07040206342935562
CurrentTrain: epoch  4, batch    45 | loss: 4.0104504Losses:  4.018285751342773 0.09053117781877518
CurrentTrain: epoch  4, batch    46 | loss: 4.1088171Losses:  4.0800395011901855 0.060559362173080444
CurrentTrain: epoch  4, batch    47 | loss: 4.1405988Losses:  4.046133041381836 0.07166492938995361
CurrentTrain: epoch  4, batch    48 | loss: 4.1177979Losses:  4.075496673583984 0.1414455771446228
CurrentTrain: epoch  4, batch    49 | loss: 4.2169423Losses:  4.023532867431641 0.06148707494139671
CurrentTrain: epoch  4, batch    50 | loss: 4.0850201Losses:  4.013567924499512 0.05495420843362808
CurrentTrain: epoch  4, batch    51 | loss: 4.0685220Losses:  4.003778457641602 0.06916406005620956
CurrentTrain: epoch  4, batch    52 | loss: 4.0729427Losses:  4.0852460861206055 0.10976694524288177
CurrentTrain: epoch  4, batch    53 | loss: 4.1950130Losses:  4.081944942474365 0.09396416693925858
CurrentTrain: epoch  4, batch    54 | loss: 4.1759090Losses:  4.000801086425781 0.1143537312746048
CurrentTrain: epoch  4, batch    55 | loss: 4.1151547Losses:  3.995130777359009 0.07157665491104126
CurrentTrain: epoch  4, batch    56 | loss: 4.0667076Losses:  4.0801191329956055 0.09639571607112885
CurrentTrain: epoch  4, batch    57 | loss: 4.1765146Losses:  4.010634899139404 0.06855440139770508
CurrentTrain: epoch  4, batch    58 | loss: 4.0791893Losses:  4.07534122467041 0.09037919342517853
CurrentTrain: epoch  4, batch    59 | loss: 4.1657205Losses:  4.052443504333496 0.057049185037612915
CurrentTrain: epoch  4, batch    60 | loss: 4.1094928Losses:  3.9535818099975586 0.10958266258239746
CurrentTrain: epoch  4, batch    61 | loss: 4.0631647Losses:  4.016757965087891 0.11528153717517853
CurrentTrain: epoch  4, batch    62 | loss: 4.1320395Losses:  4.025276184082031 0.08119417726993561
CurrentTrain: epoch  4, batch    63 | loss: 4.1064706Losses:  3.992662191390991 0.07583160698413849
CurrentTrain: epoch  4, batch    64 | loss: 4.0684938Losses:  4.023298740386963 0.07343312352895737
CurrentTrain: epoch  4, batch    65 | loss: 4.0967317Losses:  4.014511585235596 0.12388771772384644
CurrentTrain: epoch  4, batch    66 | loss: 4.1383991Losses:  3.994204521179199 0.07958219945430756
CurrentTrain: epoch  4, batch    67 | loss: 4.0737867Losses:  4.022951126098633 0.075585275888443
CurrentTrain: epoch  4, batch    68 | loss: 4.0985365Losses:  4.016554832458496 0.07946017384529114
CurrentTrain: epoch  4, batch    69 | loss: 4.0960150Losses:  3.9838478565216064 0.09459380805492401
CurrentTrain: epoch  4, batch    70 | loss: 4.0784416Losses:  4.019997596740723 0.06359679996967316
CurrentTrain: epoch  4, batch    71 | loss: 4.0835943Losses:  4.000386714935303 0.04784754663705826
CurrentTrain: epoch  4, batch    72 | loss: 4.0482345Losses:  3.9814653396606445 0.11982665210962296
CurrentTrain: epoch  4, batch    73 | loss: 4.1012921Losses:  4.019791603088379 0.05135193467140198
CurrentTrain: epoch  4, batch    74 | loss: 4.0711436Losses:  4.006645679473877 0.05532178282737732
CurrentTrain: epoch  4, batch    75 | loss: 4.0619674Losses:  4.138449668884277 0.09356798231601715
CurrentTrain: epoch  4, batch    76 | loss: 4.2320175Losses:  4.001099586486816 0.0957694947719574
CurrentTrain: epoch  4, batch    77 | loss: 4.0968690Losses:  4.025331497192383 0.10732509940862656
CurrentTrain: epoch  4, batch    78 | loss: 4.1326566Losses:  4.023963451385498 0.10124535858631134
CurrentTrain: epoch  4, batch    79 | loss: 4.1252089Losses:  4.040037155151367 0.0835912749171257
CurrentTrain: epoch  4, batch    80 | loss: 4.1236286Losses:  3.987370729446411 0.08914566785097122
CurrentTrain: epoch  4, batch    81 | loss: 4.0765166Losses:  4.124665260314941 0.08162913471460342
CurrentTrain: epoch  4, batch    82 | loss: 4.2062945Losses:  4.016632556915283 0.04833615571260452
CurrentTrain: epoch  4, batch    83 | loss: 4.0649686Losses:  4.03383731842041 0.06959345191717148
CurrentTrain: epoch  4, batch    84 | loss: 4.1034307Losses:  4.045714378356934 0.13747210800647736
CurrentTrain: epoch  4, batch    85 | loss: 4.1831865Losses:  4.005572319030762 0.13730639219284058
CurrentTrain: epoch  4, batch    86 | loss: 4.1428785Losses:  4.0121541023254395 0.1048334538936615
CurrentTrain: epoch  4, batch    87 | loss: 4.1169877Losses:  3.9987356662750244 0.04498696327209473
CurrentTrain: epoch  4, batch    88 | loss: 4.0437226Losses:  3.987692356109619 0.07063525915145874
CurrentTrain: epoch  4, batch    89 | loss: 4.0583277Losses:  3.959022283554077 0.06675629317760468
CurrentTrain: epoch  4, batch    90 | loss: 4.0257788Losses:  3.939180850982666 0.022587794810533524
CurrentTrain: epoch  4, batch    91 | loss: 3.9617686Losses:  4.000762939453125 0.057372450828552246
CurrentTrain: epoch  4, batch    92 | loss: 4.0581355Losses:  3.966723918914795 0.09545674175024033
CurrentTrain: epoch  4, batch    93 | loss: 4.0621805Losses:  4.000774383544922 0.07108201086521149
CurrentTrain: epoch  4, batch    94 | loss: 4.0718565Losses:  3.991337776184082 0.09053827077150345
CurrentTrain: epoch  4, batch    95 | loss: 4.0818763Losses:  3.990201950073242 0.0695999413728714
CurrentTrain: epoch  4, batch    96 | loss: 4.0598021Losses:  4.030348777770996 0.10237279534339905
CurrentTrain: epoch  4, batch    97 | loss: 4.1327214Losses:  3.999265670776367 0.09307833015918732
CurrentTrain: epoch  4, batch    98 | loss: 4.0923438Losses:  3.9813790321350098 0.09652441740036011
CurrentTrain: epoch  4, batch    99 | loss: 4.0779033Losses:  4.025805473327637 0.061459194868803024
CurrentTrain: epoch  4, batch   100 | loss: 4.0872645Losses:  3.9724173545837402 0.09201136231422424
CurrentTrain: epoch  4, batch   101 | loss: 4.0644288Losses:  4.0255208015441895 0.09072458744049072
CurrentTrain: epoch  4, batch   102 | loss: 4.1162453Losses:  4.019059181213379 0.14234624803066254
CurrentTrain: epoch  4, batch   103 | loss: 4.1614056Losses:  3.9938950538635254 0.11159912496805191
CurrentTrain: epoch  4, batch   104 | loss: 4.1054940Losses:  4.0252299308776855 0.0822056382894516
CurrentTrain: epoch  4, batch   105 | loss: 4.1074357Losses:  3.992706060409546 0.080023854970932
CurrentTrain: epoch  4, batch   106 | loss: 4.0727301Losses:  4.06049919128418 0.05738772824406624
CurrentTrain: epoch  4, batch   107 | loss: 4.1178870Losses:  3.951479911804199 0.0753956064581871
CurrentTrain: epoch  4, batch   108 | loss: 4.0268755Losses:  3.9965872764587402 0.08935117721557617
CurrentTrain: epoch  4, batch   109 | loss: 4.0859385Losses:  4.037965774536133 0.05588729307055473
CurrentTrain: epoch  4, batch   110 | loss: 4.0938530Losses:  4.144787788391113 0.05745813250541687
CurrentTrain: epoch  4, batch   111 | loss: 4.2022457Losses:  4.00804328918457 0.09771450608968735
CurrentTrain: epoch  4, batch   112 | loss: 4.1057577Losses:  4.0220947265625 0.05047135800123215
CurrentTrain: epoch  4, batch   113 | loss: 4.0725660Losses:  3.9609146118164062 0.0592268742620945
CurrentTrain: epoch  4, batch   114 | loss: 4.0201416Losses:  3.9756271839141846 0.11039736121892929
CurrentTrain: epoch  4, batch   115 | loss: 4.0860248Losses:  3.998623847961426 0.08630745857954025
CurrentTrain: epoch  4, batch   116 | loss: 4.0849314Losses:  4.046974182128906 0.07416026294231415
CurrentTrain: epoch  4, batch   117 | loss: 4.1211343Losses:  3.989630699157715 0.06691175699234009
CurrentTrain: epoch  4, batch   118 | loss: 4.0565424Losses:  4.010016441345215 0.09627677500247955
CurrentTrain: epoch  4, batch   119 | loss: 4.1062932Losses:  3.9901297092437744 0.05970241501927376
CurrentTrain: epoch  4, batch   120 | loss: 4.0498323Losses:  3.898320436477661 0.05214976519346237
CurrentTrain: epoch  4, batch   121 | loss: 3.9504702Losses:  3.920830011367798 0.07166972011327744
CurrentTrain: epoch  4, batch   122 | loss: 3.9924998Losses:  3.9563229084014893 0.09934435039758682
CurrentTrain: epoch  4, batch   123 | loss: 4.0556674Losses:  3.931718587875366 0.06444353610277176
CurrentTrain: epoch  4, batch   124 | loss: 3.9961622Losses:  3.9627676010131836 0.07597479224205017
CurrentTrain: epoch  5, batch     0 | loss: 4.0387425Losses:  3.9831461906433105 0.036647897213697433
CurrentTrain: epoch  5, batch     1 | loss: 4.0197940Losses:  3.926452398300171 0.05578593537211418
CurrentTrain: epoch  5, batch     2 | loss: 3.9822383Losses:  3.984111785888672 0.09214384853839874
CurrentTrain: epoch  5, batch     3 | loss: 4.0762558Losses:  3.9170713424682617 0.09669006615877151
CurrentTrain: epoch  5, batch     4 | loss: 4.0137615Losses:  3.9311540126800537 0.08232265710830688
CurrentTrain: epoch  5, batch     5 | loss: 4.0134768Losses:  4.014402389526367 0.04624354466795921
CurrentTrain: epoch  5, batch     6 | loss: 4.0606461Losses:  4.01674222946167 0.08667544275522232
CurrentTrain: epoch  5, batch     7 | loss: 4.1034179Losses:  3.9942526817321777 0.08396224677562714
CurrentTrain: epoch  5, batch     8 | loss: 4.0782151Losses:  3.9698784351348877 0.08480502665042877
CurrentTrain: epoch  5, batch     9 | loss: 4.0546837Losses:  4.0064191818237305 0.11042474210262299
CurrentTrain: epoch  5, batch    10 | loss: 4.1168437Losses:  3.977278709411621 0.065171018242836
CurrentTrain: epoch  5, batch    11 | loss: 4.0424500Losses:  3.906914710998535 0.10843690484762192
CurrentTrain: epoch  5, batch    12 | loss: 4.0153518Losses:  3.9603095054626465 0.05321007966995239
CurrentTrain: epoch  5, batch    13 | loss: 4.0135198Losses:  3.992457628250122 0.06326015293598175
CurrentTrain: epoch  5, batch    14 | loss: 4.0557179Losses:  4.025993347167969 0.0506066158413887
CurrentTrain: epoch  5, batch    15 | loss: 4.0766001Losses:  4.001544952392578 0.037689607590436935
CurrentTrain: epoch  5, batch    16 | loss: 4.0392346Losses:  3.991488218307495 0.12179706245660782
CurrentTrain: epoch  5, batch    17 | loss: 4.1132851Losses:  3.919754981994629 0.09684061259031296
CurrentTrain: epoch  5, batch    18 | loss: 4.0165954Losses:  3.987283229827881 0.09704916179180145
CurrentTrain: epoch  5, batch    19 | loss: 4.0843325Losses:  3.975113868713379 0.14040324091911316
CurrentTrain: epoch  5, batch    20 | loss: 4.1155171Losses:  4.02235221862793 0.09935258328914642
CurrentTrain: epoch  5, batch    21 | loss: 4.1217046Losses:  3.9368653297424316 0.05847923457622528
CurrentTrain: epoch  5, batch    22 | loss: 3.9953446Losses:  3.959618091583252 0.11238294839859009
CurrentTrain: epoch  5, batch    23 | loss: 4.0720010Losses:  3.9666337966918945 0.06338688731193542
CurrentTrain: epoch  5, batch    24 | loss: 4.0300207Losses:  3.953160285949707 0.13255925476551056
CurrentTrain: epoch  5, batch    25 | loss: 4.0857196Losses:  3.970470905303955 0.07869642972946167
CurrentTrain: epoch  5, batch    26 | loss: 4.0491672Losses:  4.0190324783325195 0.08572930097579956
CurrentTrain: epoch  5, batch    27 | loss: 4.1047616Losses:  4.013258457183838 0.13835734128952026
CurrentTrain: epoch  5, batch    28 | loss: 4.1516156Losses:  3.941023826599121 0.07387073338031769
CurrentTrain: epoch  5, batch    29 | loss: 4.0148945Losses:  3.989100694656372 0.0968538224697113
CurrentTrain: epoch  5, batch    30 | loss: 4.0859547Losses:  4.011824607849121 0.058277372270822525
CurrentTrain: epoch  5, batch    31 | loss: 4.0701022Losses:  3.9988789558410645 0.09368687868118286
CurrentTrain: epoch  5, batch    32 | loss: 4.0925660Losses:  4.005182266235352 0.11781813204288483
CurrentTrain: epoch  5, batch    33 | loss: 4.1230006Losses:  3.975731372833252 0.056213103234767914
CurrentTrain: epoch  5, batch    34 | loss: 4.0319443Losses:  3.945331573486328 0.08071812987327576
CurrentTrain: epoch  5, batch    35 | loss: 4.0260496Losses:  4.0034966468811035 0.08444322645664215
CurrentTrain: epoch  5, batch    36 | loss: 4.0879397Losses:  3.9486141204833984 0.09277552366256714
CurrentTrain: epoch  5, batch    37 | loss: 4.0413895Losses:  3.885256290435791 0.07226883620023727
CurrentTrain: epoch  5, batch    38 | loss: 3.9575250Losses:  3.981044292449951 0.020859677344560623
CurrentTrain: epoch  5, batch    39 | loss: 4.0019040Losses:  4.006638526916504 0.11386467516422272
CurrentTrain: epoch  5, batch    40 | loss: 4.1205034Losses:  3.9182093143463135 0.037818849086761475
CurrentTrain: epoch  5, batch    41 | loss: 3.9560282Losses:  3.978036880493164 0.09476646780967712
CurrentTrain: epoch  5, batch    42 | loss: 4.0728035Losses:  3.9155030250549316 0.058629170060157776
CurrentTrain: epoch  5, batch    43 | loss: 3.9741323Losses:  4.00266170501709 0.08834060281515121
CurrentTrain: epoch  5, batch    44 | loss: 4.0910025Losses:  3.9652583599090576 0.06767638027667999
CurrentTrain: epoch  5, batch    45 | loss: 4.0329347Losses:  3.9689669609069824 0.06215757131576538
CurrentTrain: epoch  5, batch    46 | loss: 4.0311246Losses:  3.9539334774017334 0.054694924503564835
CurrentTrain: epoch  5, batch    47 | loss: 4.0086284Losses:  4.008245944976807 0.0681246668100357
CurrentTrain: epoch  5, batch    48 | loss: 4.0763707Losses:  4.004180908203125 0.08294437825679779
CurrentTrain: epoch  5, batch    49 | loss: 4.0871253Losses:  4.00261116027832 0.06831075996160507
CurrentTrain: epoch  5, batch    50 | loss: 4.0709219Losses:  4.017266273498535 0.11382504552602768
CurrentTrain: epoch  5, batch    51 | loss: 4.1310911Losses:  3.962792158126831 0.1192716434597969
CurrentTrain: epoch  5, batch    52 | loss: 4.0820637Losses:  3.932615280151367 0.0697801411151886
CurrentTrain: epoch  5, batch    53 | loss: 4.0023956Losses:  3.9841651916503906 0.055898718535900116
CurrentTrain: epoch  5, batch    54 | loss: 4.0400639Losses:  3.987539768218994 0.07420812547206879
CurrentTrain: epoch  5, batch    55 | loss: 4.0617480Losses:  3.934072494506836 0.026950940489768982
CurrentTrain: epoch  5, batch    56 | loss: 3.9610233Losses:  3.963118553161621 0.0853661596775055
CurrentTrain: epoch  5, batch    57 | loss: 4.0484848Losses:  3.9325239658355713 0.057514891028404236
CurrentTrain: epoch  5, batch    58 | loss: 3.9900389Losses:  4.011234760284424 0.08475319296121597
CurrentTrain: epoch  5, batch    59 | loss: 4.0959878Losses:  3.942911148071289 0.03310975432395935
CurrentTrain: epoch  5, batch    60 | loss: 3.9760208Losses:  3.943084239959717 0.08317220211029053
CurrentTrain: epoch  5, batch    61 | loss: 4.0262566Losses:  3.9883928298950195 0.0421370193362236
CurrentTrain: epoch  5, batch    62 | loss: 4.0305300Losses:  3.9906363487243652 0.060996294021606445
CurrentTrain: epoch  5, batch    63 | loss: 4.0516329Losses:  3.968029022216797 0.09831146150827408
CurrentTrain: epoch  5, batch    64 | loss: 4.0663404Losses:  3.9813666343688965 0.03802022337913513
CurrentTrain: epoch  5, batch    65 | loss: 4.0193868Losses:  3.986149549484253 0.04258493706583977
CurrentTrain: epoch  5, batch    66 | loss: 4.0287347Losses:  4.041200637817383 0.03849340230226517
CurrentTrain: epoch  5, batch    67 | loss: 4.0796943Losses:  3.9278547763824463 0.06310504674911499
CurrentTrain: epoch  5, batch    68 | loss: 3.9909599Losses:  3.989745616912842 0.10744623839855194
CurrentTrain: epoch  5, batch    69 | loss: 4.0971918Losses:  4.003292083740234 0.0452914834022522
CurrentTrain: epoch  5, batch    70 | loss: 4.0485835Losses:  3.975534200668335 0.08370496332645416
CurrentTrain: epoch  5, batch    71 | loss: 4.0592394Losses:  3.981555700302124 0.035142019391059875
CurrentTrain: epoch  5, batch    72 | loss: 4.0166979Losses:  3.9329400062561035 0.10396469384431839
CurrentTrain: epoch  5, batch    73 | loss: 4.0369048Losses:  3.994168758392334 0.049326419830322266
CurrentTrain: epoch  5, batch    74 | loss: 4.0434952Losses:  3.8671441078186035 0.03429251164197922
CurrentTrain: epoch  5, batch    75 | loss: 3.9014366Losses:  3.922626495361328 0.07573798298835754
CurrentTrain: epoch  5, batch    76 | loss: 3.9983644Losses:  3.9843268394470215 0.08090011775493622
CurrentTrain: epoch  5, batch    77 | loss: 4.0652270Losses:  3.9341373443603516 0.05185691639780998
CurrentTrain: epoch  5, batch    78 | loss: 3.9859943Losses:  3.9533257484436035 0.11065013706684113
CurrentTrain: epoch  5, batch    79 | loss: 4.0639758Losses:  3.922553300857544 0.062380101531744
CurrentTrain: epoch  5, batch    80 | loss: 3.9849334Losses:  3.9451842308044434 0.03504408895969391
CurrentTrain: epoch  5, batch    81 | loss: 3.9802284Losses:  3.91766357421875 0.060808662325143814
CurrentTrain: epoch  5, batch    82 | loss: 3.9784722Losses:  3.9528958797454834 0.06696152687072754
CurrentTrain: epoch  5, batch    83 | loss: 4.0198574Losses:  3.9963908195495605 0.0850747600197792
CurrentTrain: epoch  5, batch    84 | loss: 4.0814657Losses:  3.885011911392212 0.07382216304540634
CurrentTrain: epoch  5, batch    85 | loss: 3.9588342Losses:  3.9410033226013184 0.07752086222171783
CurrentTrain: epoch  5, batch    86 | loss: 4.0185242Losses:  3.9301226139068604 0.056235894560813904
CurrentTrain: epoch  5, batch    87 | loss: 3.9863584Losses:  3.958156108856201 0.0478527769446373
CurrentTrain: epoch  5, batch    88 | loss: 4.0060091Losses:  3.9483962059020996 0.04332257807254791
CurrentTrain: epoch  5, batch    89 | loss: 3.9917188Losses:  3.964649200439453 0.07459421455860138
CurrentTrain: epoch  5, batch    90 | loss: 4.0392432Losses:  3.9279773235321045 0.040065281093120575
CurrentTrain: epoch  5, batch    91 | loss: 3.9680426Losses:  3.9991605281829834 0.04691071808338165
CurrentTrain: epoch  5, batch    92 | loss: 4.0460711Losses:  3.9399192333221436 0.06953373551368713
CurrentTrain: epoch  5, batch    93 | loss: 4.0094528Losses:  3.9313721656799316 0.026615042239427567
CurrentTrain: epoch  5, batch    94 | loss: 3.9579873Losses:  3.977200984954834 0.10724177956581116
CurrentTrain: epoch  5, batch    95 | loss: 4.0844426Losses:  4.02000093460083 0.044777728617191315
CurrentTrain: epoch  5, batch    96 | loss: 4.0647788Losses:  3.919793128967285 0.10040177404880524
CurrentTrain: epoch  5, batch    97 | loss: 4.0201950Losses:  3.9738430976867676 0.07072386145591736
CurrentTrain: epoch  5, batch    98 | loss: 4.0445671Losses:  3.924165725708008 0.06902344524860382
CurrentTrain: epoch  5, batch    99 | loss: 3.9931891Losses:  3.960911750793457 0.05475141108036041
CurrentTrain: epoch  5, batch   100 | loss: 4.0156631Losses:  3.938405990600586 0.07324515283107758
CurrentTrain: epoch  5, batch   101 | loss: 4.0116510Losses:  3.895216703414917 0.03816236928105354
CurrentTrain: epoch  5, batch   102 | loss: 3.9333792Losses:  3.9076459407806396 0.05810248851776123
CurrentTrain: epoch  5, batch   103 | loss: 3.9657483Losses:  3.9621706008911133 0.09319624304771423
CurrentTrain: epoch  5, batch   104 | loss: 4.0553670Losses:  3.942469835281372 0.07827110588550568
CurrentTrain: epoch  5, batch   105 | loss: 4.0207410Losses:  3.9735045433044434 0.07121644914150238
CurrentTrain: epoch  5, batch   106 | loss: 4.0447211Losses:  3.9364118576049805 0.06376722455024719
CurrentTrain: epoch  5, batch   107 | loss: 4.0001793Losses:  3.9838719367980957 0.06043129414319992
CurrentTrain: epoch  5, batch   108 | loss: 4.0443034Losses:  3.9176793098449707 0.05804679915308952
CurrentTrain: epoch  5, batch   109 | loss: 3.9757261Losses:  3.95717191696167 0.06667071580886841
CurrentTrain: epoch  5, batch   110 | loss: 4.0238428Losses:  3.9526727199554443 0.07320119440555573
CurrentTrain: epoch  5, batch   111 | loss: 4.0258741Losses:  4.010607719421387 0.06119294837117195
CurrentTrain: epoch  5, batch   112 | loss: 4.0718007Losses:  3.9708709716796875 0.06446877121925354
CurrentTrain: epoch  5, batch   113 | loss: 4.0353398Losses:  3.924893379211426 0.05929248407483101
CurrentTrain: epoch  5, batch   114 | loss: 3.9841859Losses:  3.9915359020233154 0.06157083809375763
CurrentTrain: epoch  5, batch   115 | loss: 4.0531068Losses:  3.928455114364624 0.06917613744735718
CurrentTrain: epoch  5, batch   116 | loss: 3.9976313Losses:  3.925227642059326 0.017272915691137314
CurrentTrain: epoch  5, batch   117 | loss: 3.9425006Losses:  3.9219231605529785 0.09067659080028534
CurrentTrain: epoch  5, batch   118 | loss: 4.0125999Losses:  3.9237141609191895 0.05584342032670975
CurrentTrain: epoch  5, batch   119 | loss: 3.9795575Losses:  3.9451162815093994 0.025067340582609177
CurrentTrain: epoch  5, batch   120 | loss: 3.9701836Losses:  3.9108030796051025 0.09796193242073059
CurrentTrain: epoch  5, batch   121 | loss: 4.0087652Losses:  4.00809907913208 0.06516421586275101
CurrentTrain: epoch  5, batch   122 | loss: 4.0732632Losses:  3.9705376625061035 0.05180106312036514
CurrentTrain: epoch  5, batch   123 | loss: 4.0223389Losses:  3.9332387447357178 0.08050329238176346
CurrentTrain: epoch  5, batch   124 | loss: 4.0137420Losses:  3.9646973609924316 0.08497868478298187
CurrentTrain: epoch  6, batch     0 | loss: 4.0496759Losses:  3.923109769821167 0.09026707708835602
CurrentTrain: epoch  6, batch     1 | loss: 4.0133767Losses:  3.9480881690979004 0.04784156382083893
CurrentTrain: epoch  6, batch     2 | loss: 3.9959297Losses:  3.9452335834503174 0.06541011482477188
CurrentTrain: epoch  6, batch     3 | loss: 4.0106435Losses:  3.9649884700775146 0.0699172243475914
CurrentTrain: epoch  6, batch     4 | loss: 4.0349059Losses:  3.9473042488098145 0.034108906984329224
CurrentTrain: epoch  6, batch     5 | loss: 3.9814131Losses:  3.9346885681152344 0.05427900701761246
CurrentTrain: epoch  6, batch     6 | loss: 3.9889677Losses:  3.9879775047302246 0.041505470871925354
CurrentTrain: epoch  6, batch     7 | loss: 4.0294828Losses:  3.9744420051574707 0.05308609455823898
CurrentTrain: epoch  6, batch     8 | loss: 4.0275283Losses:  3.975040912628174 0.08174271136522293
CurrentTrain: epoch  6, batch     9 | loss: 4.0567837Losses:  4.005992889404297 0.03906424343585968
CurrentTrain: epoch  6, batch    10 | loss: 4.0450573Losses:  4.000369071960449 0.05391237512230873
CurrentTrain: epoch  6, batch    11 | loss: 4.0542812Losses:  3.9891767501831055 0.07809106260538101
CurrentTrain: epoch  6, batch    12 | loss: 4.0672679Losses:  3.906024932861328 0.08764396607875824
CurrentTrain: epoch  6, batch    13 | loss: 3.9936688Losses:  3.9507274627685547 0.10572095215320587
CurrentTrain: epoch  6, batch    14 | loss: 4.0564485Losses:  3.9642605781555176 0.054885994642972946
CurrentTrain: epoch  6, batch    15 | loss: 4.0191464Losses:  3.9372916221618652 0.11422913521528244
CurrentTrain: epoch  6, batch    16 | loss: 4.0515208Losses:  3.9914846420288086 0.053130049258470535
CurrentTrain: epoch  6, batch    17 | loss: 4.0446148Losses:  3.9726319313049316 0.03775372356176376
CurrentTrain: epoch  6, batch    18 | loss: 4.0103855Losses:  4.005009651184082 0.06903508305549622
CurrentTrain: epoch  6, batch    19 | loss: 4.0740447Losses:  3.9227800369262695 0.0538024976849556
CurrentTrain: epoch  6, batch    20 | loss: 3.9765825Losses:  3.9383885860443115 0.07280868291854858
CurrentTrain: epoch  6, batch    21 | loss: 4.0111971Losses:  3.9611384868621826 0.06020020321011543
CurrentTrain: epoch  6, batch    22 | loss: 4.0213385Losses:  3.9164161682128906 0.07015852630138397
CurrentTrain: epoch  6, batch    23 | loss: 3.9865746Losses:  3.898420810699463 0.0674525573849678
CurrentTrain: epoch  6, batch    24 | loss: 3.9658735Losses:  3.9623284339904785 0.06891333311796188
CurrentTrain: epoch  6, batch    25 | loss: 4.0312419Losses:  3.9317760467529297 0.06472370028495789
CurrentTrain: epoch  6, batch    26 | loss: 3.9964998Losses:  4.0872650146484375 0.04562075063586235
CurrentTrain: epoch  6, batch    27 | loss: 4.1328859Losses:  3.9366374015808105 0.09114953875541687
CurrentTrain: epoch  6, batch    28 | loss: 4.0277867Losses:  3.9376869201660156 0.06364449113607407
CurrentTrain: epoch  6, batch    29 | loss: 4.0013313Losses:  3.940599203109741 0.0381515771150589
CurrentTrain: epoch  6, batch    30 | loss: 3.9787507Losses:  3.956937789916992 0.032925210893154144
CurrentTrain: epoch  6, batch    31 | loss: 3.9898629Losses:  3.917780876159668 0.0865749642252922
CurrentTrain: epoch  6, batch    32 | loss: 4.0043559Losses:  3.932551860809326 0.08391311019659042
CurrentTrain: epoch  6, batch    33 | loss: 4.0164652Losses:  3.947152614593506 0.08627042919397354
CurrentTrain: epoch  6, batch    34 | loss: 4.0334229Losses:  3.90403413772583 0.06837063282728195
CurrentTrain: epoch  6, batch    35 | loss: 3.9724047Losses:  3.964046001434326 0.06733369827270508
CurrentTrain: epoch  6, batch    36 | loss: 4.0313797Losses:  3.874502658843994 0.04480486735701561
CurrentTrain: epoch  6, batch    37 | loss: 3.9193075Losses:  3.902851104736328 0.04409976303577423
CurrentTrain: epoch  6, batch    38 | loss: 3.9469509Losses:  3.9205756187438965 0.07194057106971741
CurrentTrain: epoch  6, batch    39 | loss: 3.9925163Losses:  3.9323153495788574 0.07856811583042145
CurrentTrain: epoch  6, batch    40 | loss: 4.0108833Losses:  3.8955674171447754 0.09701718389987946
CurrentTrain: epoch  6, batch    41 | loss: 3.9925847Losses:  3.9246950149536133 0.05612559616565704
CurrentTrain: epoch  6, batch    42 | loss: 3.9808207Losses:  3.954606771469116 0.08927744626998901
CurrentTrain: epoch  6, batch    43 | loss: 4.0438843Losses:  3.9155921936035156 0.07266515493392944
CurrentTrain: epoch  6, batch    44 | loss: 3.9882574Losses:  3.9124035835266113 0.03249349817633629
CurrentTrain: epoch  6, batch    45 | loss: 3.9448972Losses:  3.927638053894043 0.08529660850763321
CurrentTrain: epoch  6, batch    46 | loss: 4.0129347Losses:  3.9478368759155273 0.05307779088616371
CurrentTrain: epoch  6, batch    47 | loss: 4.0009146Losses:  3.979332447052002 0.0841226652264595
CurrentTrain: epoch  6, batch    48 | loss: 4.0634551Losses:  3.8857839107513428 0.07384312152862549
CurrentTrain: epoch  6, batch    49 | loss: 3.9596272Losses:  3.9052929878234863 0.032256439328193665
CurrentTrain: epoch  6, batch    50 | loss: 3.9375494Losses:  3.953701972961426 0.043820593506097794
CurrentTrain: epoch  6, batch    51 | loss: 3.9975226Losses:  3.958662748336792 0.04474196583032608
CurrentTrain: epoch  6, batch    52 | loss: 4.0034046Losses:  3.9460530281066895 0.030276697129011154
CurrentTrain: epoch  6, batch    53 | loss: 3.9763298Losses:  3.937053680419922 0.11695381999015808
CurrentTrain: epoch  6, batch    54 | loss: 4.0540075Losses:  3.9267120361328125 0.05656542256474495
CurrentTrain: epoch  6, batch    55 | loss: 3.9832776Losses:  3.949241876602173 0.02380465343594551
CurrentTrain: epoch  6, batch    56 | loss: 3.9730465Losses:  3.907721996307373 0.05363829433917999
CurrentTrain: epoch  6, batch    57 | loss: 3.9613602Losses:  3.9181928634643555 0.045697759836912155
CurrentTrain: epoch  6, batch    58 | loss: 3.9638906Losses:  3.9284415245056152 0.07152209430932999
CurrentTrain: epoch  6, batch    59 | loss: 3.9999635Losses:  3.9181125164031982 0.04931667447090149
CurrentTrain: epoch  6, batch    60 | loss: 3.9674292Losses:  3.9270761013031006 0.05596977472305298
CurrentTrain: epoch  6, batch    61 | loss: 3.9830458Losses:  3.995237350463867 0.07775013148784637
CurrentTrain: epoch  6, batch    62 | loss: 4.0729876Losses:  3.9242706298828125 0.08713377267122269
CurrentTrain: epoch  6, batch    63 | loss: 4.0114045Losses:  3.918548107147217 0.07890155166387558
CurrentTrain: epoch  6, batch    64 | loss: 3.9974496Losses:  3.9887852668762207 0.04048639535903931
CurrentTrain: epoch  6, batch    65 | loss: 4.0292716Losses:  3.947573184967041 0.06427773088216782
CurrentTrain: epoch  6, batch    66 | loss: 4.0118508Losses:  3.9524288177490234 0.03697771206498146
CurrentTrain: epoch  6, batch    67 | loss: 3.9894066Losses:  3.96700382232666 0.0452684611082077
CurrentTrain: epoch  6, batch    68 | loss: 4.0122724Losses:  4.0101518630981445 0.07021205127239227
CurrentTrain: epoch  6, batch    69 | loss: 4.0803638Losses:  3.9626576900482178 0.03404448181390762
CurrentTrain: epoch  6, batch    70 | loss: 3.9967022Losses:  3.949856758117676 0.03567175194621086
CurrentTrain: epoch  6, batch    71 | loss: 3.9855285Losses:  3.9301302433013916 0.046564992517232895
CurrentTrain: epoch  6, batch    72 | loss: 3.9766953Losses:  3.961836338043213 0.05705907195806503
CurrentTrain: epoch  6, batch    73 | loss: 4.0188956Losses:  3.9735124111175537 0.05471765995025635
CurrentTrain: epoch  6, batch    74 | loss: 4.0282302Losses:  3.950843572616577 0.04170845448970795
CurrentTrain: epoch  6, batch    75 | loss: 3.9925520Losses:  3.9505627155303955 0.05741221830248833
CurrentTrain: epoch  6, batch    76 | loss: 4.0079751Losses:  3.9353413581848145 0.05931731313467026
CurrentTrain: epoch  6, batch    77 | loss: 3.9946587Losses:  4.015610694885254 0.018722129985690117
CurrentTrain: epoch  6, batch    78 | loss: 4.0343328Losses:  3.943222999572754 0.06471902132034302
CurrentTrain: epoch  6, batch    79 | loss: 4.0079422Losses:  4.039636611938477 0.05798961967229843
CurrentTrain: epoch  6, batch    80 | loss: 4.0976262Losses:  3.9892735481262207 0.05851415917277336
CurrentTrain: epoch  6, batch    81 | loss: 4.0477877Losses:  3.938291549682617 0.02114257961511612
CurrentTrain: epoch  6, batch    82 | loss: 3.9594340Losses:  3.9322938919067383 0.06825336068868637
CurrentTrain: epoch  6, batch    83 | loss: 4.0005474Losses:  3.966279983520508 0.08043757826089859
CurrentTrain: epoch  6, batch    84 | loss: 4.0467176Losses:  3.9264559745788574 0.06404456496238708
CurrentTrain: epoch  6, batch    85 | loss: 3.9905005Losses:  4.008062362670898 0.041786640882492065
CurrentTrain: epoch  6, batch    86 | loss: 4.0498490Losses:  3.9515552520751953 0.08141148090362549
CurrentTrain: epoch  6, batch    87 | loss: 4.0329666Losses:  3.944082260131836 0.03403315320611
CurrentTrain: epoch  6, batch    88 | loss: 3.9781153Losses:  3.9778246879577637 0.06157149374485016
CurrentTrain: epoch  6, batch    89 | loss: 4.0393963Losses:  3.9595375061035156 0.02287374995648861
CurrentTrain: epoch  6, batch    90 | loss: 3.9824111Losses:  3.9199604988098145 0.05306229740381241
CurrentTrain: epoch  6, batch    91 | loss: 3.9730227Losses:  3.9084279537200928 0.07146859169006348
CurrentTrain: epoch  6, batch    92 | loss: 3.9798965Losses:  3.929295539855957 0.0857420489192009
CurrentTrain: epoch  6, batch    93 | loss: 4.0150375Losses:  3.975416898727417 0.08581490069627762
CurrentTrain: epoch  6, batch    94 | loss: 4.0612316Losses:  3.9643101692199707 0.07970348745584488
CurrentTrain: epoch  6, batch    95 | loss: 4.0440135Losses:  3.96970796585083 0.10222125053405762
CurrentTrain: epoch  6, batch    96 | loss: 4.0719290Losses:  3.9675822257995605 0.043601974844932556
CurrentTrain: epoch  6, batch    97 | loss: 4.0111842Losses:  4.0099639892578125 0.05292042717337608
CurrentTrain: epoch  6, batch    98 | loss: 4.0628843Losses:  3.876865863800049 0.04968049377202988
CurrentTrain: epoch  6, batch    99 | loss: 3.9265463Losses:  3.9878945350646973 0.08081384003162384
CurrentTrain: epoch  6, batch   100 | loss: 4.0687084Losses:  3.9600296020507812 0.06346164643764496
CurrentTrain: epoch  6, batch   101 | loss: 4.0234914Losses:  3.9031729698181152 0.06199737638235092
CurrentTrain: epoch  6, batch   102 | loss: 3.9651704Losses:  3.8851561546325684 0.050469473004341125
CurrentTrain: epoch  6, batch   103 | loss: 3.9356256Losses:  3.964235544204712 0.05984308570623398
CurrentTrain: epoch  6, batch   104 | loss: 4.0240788Losses:  3.915445566177368 0.08117087930440903
CurrentTrain: epoch  6, batch   105 | loss: 3.9966164Losses:  3.97906494140625 0.08434484899044037
CurrentTrain: epoch  6, batch   106 | loss: 4.0634098Losses:  3.9210052490234375 0.08890500664710999
CurrentTrain: epoch  6, batch   107 | loss: 4.0099101Losses:  3.951648712158203 0.05417640879750252
CurrentTrain: epoch  6, batch   108 | loss: 4.0058250Losses:  3.9399099349975586 0.06627902388572693
CurrentTrain: epoch  6, batch   109 | loss: 4.0061889Losses:  3.93677020072937 0.044596098363399506
CurrentTrain: epoch  6, batch   110 | loss: 3.9813664Losses:  3.9487099647521973 0.04236634075641632
CurrentTrain: epoch  6, batch   111 | loss: 3.9910762Losses:  3.9543609619140625 0.04957738518714905
CurrentTrain: epoch  6, batch   112 | loss: 4.0039382Losses:  3.9550588130950928 0.05925503000617027
CurrentTrain: epoch  6, batch   113 | loss: 4.0143137Losses:  3.989757776260376 0.03672920539975166
CurrentTrain: epoch  6, batch   114 | loss: 4.0264869Losses:  3.9284191131591797 0.05047721043229103
CurrentTrain: epoch  6, batch   115 | loss: 3.9788964Losses:  3.8880345821380615 0.028647849336266518
CurrentTrain: epoch  6, batch   116 | loss: 3.9166825Losses:  3.9393696784973145 0.06498908996582031
CurrentTrain: epoch  6, batch   117 | loss: 4.0043588Losses:  3.9659926891326904 0.04379226639866829
CurrentTrain: epoch  6, batch   118 | loss: 4.0097852Losses:  3.9152917861938477 0.07757040858268738
CurrentTrain: epoch  6, batch   119 | loss: 3.9928622Losses:  3.9560325145721436 0.044741082936525345
CurrentTrain: epoch  6, batch   120 | loss: 4.0007734Losses:  3.9739174842834473 0.06015103682875633
CurrentTrain: epoch  6, batch   121 | loss: 4.0340686Losses:  3.982452869415283 0.05315036699175835
CurrentTrain: epoch  6, batch   122 | loss: 4.0356030Losses:  3.9895412921905518 0.05035839229822159
CurrentTrain: epoch  6, batch   123 | loss: 4.0398998Losses:  3.8474717140197754 0.05776568129658699
CurrentTrain: epoch  6, batch   124 | loss: 3.9052374Losses:  3.9083924293518066 0.046884097158908844
CurrentTrain: epoch  7, batch     0 | loss: 3.9552765Losses:  3.919088840484619 0.05897628888487816
CurrentTrain: epoch  7, batch     1 | loss: 3.9780650Losses:  3.9674882888793945 0.012492690235376358
CurrentTrain: epoch  7, batch     2 | loss: 3.9799809Losses:  3.937375068664551 0.06503815948963165
CurrentTrain: epoch  7, batch     3 | loss: 4.0024133Losses:  3.904283046722412 0.03493843972682953
CurrentTrain: epoch  7, batch     4 | loss: 3.9392214Losses:  3.971210479736328 0.008788540959358215
CurrentTrain: epoch  7, batch     5 | loss: 3.9799991Losses:  3.9427542686462402 0.05445122718811035
CurrentTrain: epoch  7, batch     6 | loss: 3.9972055Losses:  3.9560399055480957 0.06622280180454254
CurrentTrain: epoch  7, batch     7 | loss: 4.0222626Losses:  3.953908920288086 0.07918742299079895
CurrentTrain: epoch  7, batch     8 | loss: 4.0330963Losses:  3.9449615478515625 0.06733977794647217
CurrentTrain: epoch  7, batch     9 | loss: 4.0123014Losses:  3.9064831733703613 0.058061882853507996
CurrentTrain: epoch  7, batch    10 | loss: 3.9645450Losses:  3.977182388305664 0.04696585237979889
CurrentTrain: epoch  7, batch    11 | loss: 4.0241485Losses:  3.994324207305908 0.025199148803949356
CurrentTrain: epoch  7, batch    12 | loss: 4.0195231Losses:  3.887787342071533 0.0565035380423069
CurrentTrain: epoch  7, batch    13 | loss: 3.9442909Losses:  3.9473228454589844 0.07302101701498032
CurrentTrain: epoch  7, batch    14 | loss: 4.0203438Losses:  3.9656667709350586 0.036384690552949905
CurrentTrain: epoch  7, batch    15 | loss: 4.0020514Losses:  3.911369562149048 0.05294167995452881
CurrentTrain: epoch  7, batch    16 | loss: 3.9643111Losses:  3.9085187911987305 0.029282426461577415
CurrentTrain: epoch  7, batch    17 | loss: 3.9378011Losses:  3.977085590362549 0.08675913512706757
CurrentTrain: epoch  7, batch    18 | loss: 4.0638447Losses:  3.9432778358459473 0.046502307057380676
CurrentTrain: epoch  7, batch    19 | loss: 3.9897802Losses:  3.9570095539093018 0.051172588020563126
CurrentTrain: epoch  7, batch    20 | loss: 4.0081820Losses:  3.9438493251800537 0.0934453010559082
CurrentTrain: epoch  7, batch    21 | loss: 4.0372944Losses:  3.9560861587524414 0.03548254072666168
CurrentTrain: epoch  7, batch    22 | loss: 3.9915688Losses:  3.967775344848633 0.07363949716091156
CurrentTrain: epoch  7, batch    23 | loss: 4.0414147Losses:  3.942847967147827 0.031418491154909134
CurrentTrain: epoch  7, batch    24 | loss: 3.9742665Losses:  3.9223008155822754 0.07505866885185242
CurrentTrain: epoch  7, batch    25 | loss: 3.9973595Losses:  3.906430721282959 0.05566485971212387
CurrentTrain: epoch  7, batch    26 | loss: 3.9620955Losses:  3.920778751373291 0.062099821865558624
CurrentTrain: epoch  7, batch    27 | loss: 3.9828787Losses:  3.989715099334717 0.07303396612405777
CurrentTrain: epoch  7, batch    28 | loss: 4.0627489Losses:  3.87387752532959 0.06202332675457001
CurrentTrain: epoch  7, batch    29 | loss: 3.9359009Losses:  3.9537034034729004 0.03767494112253189
CurrentTrain: epoch  7, batch    30 | loss: 3.9913783Losses:  3.922579050064087 0.06605257093906403
CurrentTrain: epoch  7, batch    31 | loss: 3.9886317Losses:  3.9900174140930176 0.025129083544015884
CurrentTrain: epoch  7, batch    32 | loss: 4.0151467Losses:  3.919797420501709 0.07263591885566711
CurrentTrain: epoch  7, batch    33 | loss: 3.9924333Losses:  3.959493637084961 0.06537789106369019
CurrentTrain: epoch  7, batch    34 | loss: 4.0248713Losses:  3.888274669647217 0.05811757594347
CurrentTrain: epoch  7, batch    35 | loss: 3.9463923Losses:  3.948138952255249 0.07048426568508148
CurrentTrain: epoch  7, batch    36 | loss: 4.0186234Losses:  3.9678092002868652 0.047415539622306824
CurrentTrain: epoch  7, batch    37 | loss: 4.0152249Losses:  3.9402658939361572 0.060995861887931824
CurrentTrain: epoch  7, batch    38 | loss: 4.0012617Losses:  3.927151679992676 0.07964755594730377
CurrentTrain: epoch  7, batch    39 | loss: 4.0067992Losses:  3.9107584953308105 0.04676678031682968
CurrentTrain: epoch  7, batch    40 | loss: 3.9575253Losses:  3.9375317096710205 0.055084798485040665
CurrentTrain: epoch  7, batch    41 | loss: 3.9926164Losses:  3.9531211853027344 0.07942435145378113
CurrentTrain: epoch  7, batch    42 | loss: 4.0325456Losses:  3.9185001850128174 0.07546092569828033
CurrentTrain: epoch  7, batch    43 | loss: 3.9939611Losses:  3.9718732833862305 0.051301080733537674
CurrentTrain: epoch  7, batch    44 | loss: 4.0231743Losses:  3.969547748565674 0.038262739777565
CurrentTrain: epoch  7, batch    45 | loss: 4.0078106Losses:  3.903114080429077 0.04947175085544586
CurrentTrain: epoch  7, batch    46 | loss: 3.9525859Losses:  3.9395461082458496 0.05262216180562973
CurrentTrain: epoch  7, batch    47 | loss: 3.9921682Losses:  3.9017860889434814 0.049850985407829285
CurrentTrain: epoch  7, batch    48 | loss: 3.9516370Losses:  3.916699171066284 0.041142068803310394
CurrentTrain: epoch  7, batch    49 | loss: 3.9578412Losses:  3.958186626434326 0.04810498654842377
CurrentTrain: epoch  7, batch    50 | loss: 4.0062914Losses:  3.9244630336761475 0.06888536363840103
CurrentTrain: epoch  7, batch    51 | loss: 3.9933484Losses:  3.937206506729126 0.0321364551782608
CurrentTrain: epoch  7, batch    52 | loss: 3.9693429Losses:  3.9910480976104736 0.03676959499716759
CurrentTrain: epoch  7, batch    53 | loss: 4.0278177Losses:  3.9374444484710693 0.0636495053768158
CurrentTrain: epoch  7, batch    54 | loss: 4.0010939Losses:  3.925861358642578 0.07235873490571976
CurrentTrain: epoch  7, batch    55 | loss: 3.9982202Losses:  3.9589548110961914 0.04957636073231697
CurrentTrain: epoch  7, batch    56 | loss: 4.0085311Losses:  3.897193193435669 0.06179938092827797
CurrentTrain: epoch  7, batch    57 | loss: 3.9589925Losses:  3.927675724029541 0.061435215175151825
CurrentTrain: epoch  7, batch    58 | loss: 3.9891109Losses:  3.9378600120544434 0.06357672810554504
CurrentTrain: epoch  7, batch    59 | loss: 4.0014367Losses:  3.9327821731567383 0.0636330172419548
CurrentTrain: epoch  7, batch    60 | loss: 3.9964151Losses:  3.973294734954834 0.04450281709432602
CurrentTrain: epoch  7, batch    61 | loss: 4.0177975Losses:  3.974142551422119 0.04966519773006439
CurrentTrain: epoch  7, batch    62 | loss: 4.0238075Losses:  3.9468955993652344 0.039548084139823914
CurrentTrain: epoch  7, batch    63 | loss: 3.9864438Losses:  3.9789695739746094 0.051430247724056244
CurrentTrain: epoch  7, batch    64 | loss: 4.0303998Losses:  3.958818197250366 0.05720560625195503
CurrentTrain: epoch  7, batch    65 | loss: 4.0160236Losses:  3.9666876792907715 0.042394958436489105
CurrentTrain: epoch  7, batch    66 | loss: 4.0090828Losses:  3.9544219970703125 0.04453940689563751
CurrentTrain: epoch  7, batch    67 | loss: 3.9989614Losses:  3.9334211349487305 0.04361175745725632
CurrentTrain: epoch  7, batch    68 | loss: 3.9770329Losses:  3.9420266151428223 0.053862668573856354
CurrentTrain: epoch  7, batch    69 | loss: 3.9958892Losses:  3.962371587753296 0.050243571400642395
CurrentTrain: epoch  7, batch    70 | loss: 4.0126152Losses:  3.8960118293762207 0.05112536996603012
CurrentTrain: epoch  7, batch    71 | loss: 3.9471371Losses:  3.958062171936035 0.05983990058302879
CurrentTrain: epoch  7, batch    72 | loss: 4.0179019Losses:  3.9461545944213867 0.055822912603616714
CurrentTrain: epoch  7, batch    73 | loss: 4.0019774Losses:  3.9098262786865234 0.02537851594388485
CurrentTrain: epoch  7, batch    74 | loss: 3.9352047Losses:  3.89694881439209 0.05921344459056854
CurrentTrain: epoch  7, batch    75 | loss: 3.9561622Losses:  3.85660982131958 0.05216158181428909
CurrentTrain: epoch  7, batch    76 | loss: 3.9087715Losses:  3.9773621559143066 0.057653412222862244
CurrentTrain: epoch  7, batch    77 | loss: 4.0350156Losses:  3.9453251361846924 0.061708685010671616
CurrentTrain: epoch  7, batch    78 | loss: 4.0070338Losses:  3.962846517562866 0.01866026408970356
CurrentTrain: epoch  7, batch    79 | loss: 3.9815068Losses:  3.9629316329956055 0.07823847234249115
CurrentTrain: epoch  7, batch    80 | loss: 4.0411701Losses:  3.9383888244628906 0.038734320551157
CurrentTrain: epoch  7, batch    81 | loss: 3.9771233Losses:  3.9060301780700684 0.0742211565375328
CurrentTrain: epoch  7, batch    82 | loss: 3.9802513Losses:  3.9681687355041504 0.04583805799484253
CurrentTrain: epoch  7, batch    83 | loss: 4.0140066Losses:  3.9930527210235596 0.05909379571676254
CurrentTrain: epoch  7, batch    84 | loss: 4.0521464Losses:  3.9483351707458496 0.050544217228889465
CurrentTrain: epoch  7, batch    85 | loss: 3.9988794Losses:  3.918001174926758 0.030971726402640343
CurrentTrain: epoch  7, batch    86 | loss: 3.9489729Losses:  3.938732147216797 0.044725947082042694
CurrentTrain: epoch  7, batch    87 | loss: 3.9834580Losses:  3.944383144378662 0.05677501857280731
CurrentTrain: epoch  7, batch    88 | loss: 4.0011582Losses:  3.919443130493164 0.06908464431762695
CurrentTrain: epoch  7, batch    89 | loss: 3.9885278Losses:  3.9838995933532715 0.03490815684199333
CurrentTrain: epoch  7, batch    90 | loss: 4.0188079Losses:  3.89686918258667 0.05626395344734192
CurrentTrain: epoch  7, batch    91 | loss: 3.9531331Losses:  3.937886953353882 0.026638057082891464
CurrentTrain: epoch  7, batch    92 | loss: 3.9645250Losses:  3.9382781982421875 0.07170478254556656
CurrentTrain: epoch  7, batch    93 | loss: 4.0099831Losses:  3.9568862915039062 0.031053971499204636
CurrentTrain: epoch  7, batch    94 | loss: 3.9879403Losses:  3.9146764278411865 0.07159023731946945
CurrentTrain: epoch  7, batch    95 | loss: 3.9862666Losses:  3.9034457206726074 0.02039383538067341
CurrentTrain: epoch  7, batch    96 | loss: 3.9238396Losses:  3.9662692546844482 0.03745419532060623
CurrentTrain: epoch  7, batch    97 | loss: 4.0037236Losses:  3.9231104850769043 0.03577995300292969
CurrentTrain: epoch  7, batch    98 | loss: 3.9588904Losses:  3.922640800476074 0.066460981965065
CurrentTrain: epoch  7, batch    99 | loss: 3.9891019Losses:  3.947118043899536 0.049212489277124405
CurrentTrain: epoch  7, batch   100 | loss: 3.9963305Losses:  3.928101062774658 0.03602231293916702
CurrentTrain: epoch  7, batch   101 | loss: 3.9641235Losses:  3.9138479232788086 0.06798983365297318
CurrentTrain: epoch  7, batch   102 | loss: 3.9818377Losses:  3.931027412414551 0.06535926461219788
CurrentTrain: epoch  7, batch   103 | loss: 3.9963868Losses:  3.968867778778076 0.0618663914501667
CurrentTrain: epoch  7, batch   104 | loss: 4.0307341Losses:  3.851957082748413 0.05901668965816498
CurrentTrain: epoch  7, batch   105 | loss: 3.9109738Losses:  3.89619779586792 0.06199623644351959
CurrentTrain: epoch  7, batch   106 | loss: 3.9581940Losses:  3.8849337100982666 0.03714405745267868
CurrentTrain: epoch  7, batch   107 | loss: 3.9220777Losses:  3.9797675609588623 0.05367152392864227
CurrentTrain: epoch  7, batch   108 | loss: 4.0334392Losses:  3.9221177101135254 0.04574739933013916
CurrentTrain: epoch  7, batch   109 | loss: 3.9678650Losses:  3.921041965484619 0.04606323689222336
CurrentTrain: epoch  7, batch   110 | loss: 3.9671052Losses:  3.9410240650177 0.07224085927009583
CurrentTrain: epoch  7, batch   111 | loss: 4.0132651Losses:  3.9243826866149902 0.07284079492092133
CurrentTrain: epoch  7, batch   112 | loss: 3.9972234Losses:  3.914107084274292 0.06055217981338501
CurrentTrain: epoch  7, batch   113 | loss: 3.9746592Losses:  3.960505723953247 0.05149777606129646
CurrentTrain: epoch  7, batch   114 | loss: 4.0120034Losses:  3.9259450435638428 0.030413493514060974
CurrentTrain: epoch  7, batch   115 | loss: 3.9563584Losses:  3.9439001083374023 0.050617195665836334
CurrentTrain: epoch  7, batch   116 | loss: 3.9945173Losses:  3.9384913444519043 0.0329800583422184
CurrentTrain: epoch  7, batch   117 | loss: 3.9714713Losses:  3.878033399581909 0.04372074455022812
CurrentTrain: epoch  7, batch   118 | loss: 3.9217541Losses:  3.9904158115386963 0.013692702166736126
CurrentTrain: epoch  7, batch   119 | loss: 4.0041084Losses:  3.9920835494995117 0.048297300934791565
CurrentTrain: epoch  7, batch   120 | loss: 4.0403810Losses:  3.9044947624206543 0.03872360289096832
CurrentTrain: epoch  7, batch   121 | loss: 3.9432185Losses:  3.961186170578003 0.04102395474910736
CurrentTrain: epoch  7, batch   122 | loss: 4.0022101Losses:  3.9774351119995117 0.03857063129544258
CurrentTrain: epoch  7, batch   123 | loss: 4.0160055Losses:  3.928497791290283 0.05369219183921814
CurrentTrain: epoch  7, batch   124 | loss: 3.9821899Losses:  3.946743965148926 0.06647400557994843
CurrentTrain: epoch  8, batch     0 | loss: 4.0132179Losses:  3.9458703994750977 0.05658509582281113
CurrentTrain: epoch  8, batch     1 | loss: 4.0024557Losses:  3.9550704956054688 0.05589033663272858
CurrentTrain: epoch  8, batch     2 | loss: 4.0109611Losses:  3.9326186180114746 0.04497841000556946
CurrentTrain: epoch  8, batch     3 | loss: 3.9775970Losses:  3.9197583198547363 0.026392724364995956
CurrentTrain: epoch  8, batch     4 | loss: 3.9461510Losses:  3.907850742340088 0.039337947964668274
CurrentTrain: epoch  8, batch     5 | loss: 3.9471886Losses:  3.8704028129577637 0.025830809026956558
CurrentTrain: epoch  8, batch     6 | loss: 3.8962336Losses:  3.99287748336792 0.03309853374958038
CurrentTrain: epoch  8, batch     7 | loss: 4.0259762Losses:  3.971062660217285 0.047949329018592834
CurrentTrain: epoch  8, batch     8 | loss: 4.0190120Losses:  3.9180006980895996 0.047466859221458435
CurrentTrain: epoch  8, batch     9 | loss: 3.9654675Losses:  3.932783603668213 0.057493656873703
CurrentTrain: epoch  8, batch    10 | loss: 3.9902773Losses:  3.947209119796753 0.051758311688899994
CurrentTrain: epoch  8, batch    11 | loss: 3.9989674Losses:  3.960496425628662 0.04403290897607803
CurrentTrain: epoch  8, batch    12 | loss: 4.0045295Losses:  3.92917799949646 0.06977154314517975
CurrentTrain: epoch  8, batch    13 | loss: 3.9989495Losses:  3.939988613128662 0.07055801898241043
CurrentTrain: epoch  8, batch    14 | loss: 4.0105467Losses:  3.8796074390411377 0.0510953851044178
CurrentTrain: epoch  8, batch    15 | loss: 3.9307029Losses:  3.967029571533203 0.04033917933702469
CurrentTrain: epoch  8, batch    16 | loss: 4.0073686Losses:  3.9090287685394287 0.04630487412214279
CurrentTrain: epoch  8, batch    17 | loss: 3.9553337Losses:  3.93503999710083 0.03252130746841431
CurrentTrain: epoch  8, batch    18 | loss: 3.9675612Losses:  3.9777660369873047 0.03998628258705139
CurrentTrain: epoch  8, batch    19 | loss: 4.0177522Losses:  3.933218479156494 0.03276361525058746
CurrentTrain: epoch  8, batch    20 | loss: 3.9659822Losses:  3.9090399742126465 0.025454264134168625
CurrentTrain: epoch  8, batch    21 | loss: 3.9344943Losses:  3.9459667205810547 0.05903090536594391
CurrentTrain: epoch  8, batch    22 | loss: 4.0049977Losses:  4.012197494506836 0.032488927245140076
CurrentTrain: epoch  8, batch    23 | loss: 4.0446863Losses:  3.9678540229797363 0.0346425361931324
CurrentTrain: epoch  8, batch    24 | loss: 4.0024967Losses:  3.958466053009033 0.03497873246669769
CurrentTrain: epoch  8, batch    25 | loss: 3.9934447Losses:  3.940896511077881 0.06089849770069122
CurrentTrain: epoch  8, batch    26 | loss: 4.0017948Losses:  3.9199252128601074 0.038839492946863174
CurrentTrain: epoch  8, batch    27 | loss: 3.9587648Losses:  3.8966939449310303 0.02502491883933544
CurrentTrain: epoch  8, batch    28 | loss: 3.9217188Losses:  3.9491500854492188 0.034336596727371216
CurrentTrain: epoch  8, batch    29 | loss: 3.9834867Losses:  3.9531235694885254 0.04889291524887085
CurrentTrain: epoch  8, batch    30 | loss: 4.0020165Losses:  3.873138904571533 0.043033406138420105
CurrentTrain: epoch  8, batch    31 | loss: 3.9161723Losses:  3.89814829826355 0.045947033911943436
CurrentTrain: epoch  8, batch    32 | loss: 3.9440954Losses:  3.938293933868408 0.06278636306524277
CurrentTrain: epoch  8, batch    33 | loss: 4.0010805Losses:  3.9228932857513428 0.04789779707789421
CurrentTrain: epoch  8, batch    34 | loss: 3.9707911Losses:  3.945037364959717 0.01869834028184414
CurrentTrain: epoch  8, batch    35 | loss: 3.9637358Losses:  3.973287582397461 0.06429538875818253
CurrentTrain: epoch  8, batch    36 | loss: 4.0375829Losses:  3.904160499572754 0.03827501833438873
CurrentTrain: epoch  8, batch    37 | loss: 3.9424355Losses:  3.9513463973999023 0.07135023176670074
CurrentTrain: epoch  8, batch    38 | loss: 4.0226965Losses:  3.9366133213043213 0.057886213064193726
CurrentTrain: epoch  8, batch    39 | loss: 3.9944994Losses:  3.911423683166504 0.04702884703874588
CurrentTrain: epoch  8, batch    40 | loss: 3.9584525Losses:  3.939883232116699 0.07295213639736176
CurrentTrain: epoch  8, batch    41 | loss: 4.0128355Losses:  3.9162158966064453 0.02750745788216591
CurrentTrain: epoch  8, batch    42 | loss: 3.9437234Losses:  4.006748199462891 0.0462428517639637
CurrentTrain: epoch  8, batch    43 | loss: 4.0529909Losses:  3.8877220153808594 0.024199679493904114
CurrentTrain: epoch  8, batch    44 | loss: 3.9119217Losses:  3.937044143676758 0.060993291437625885
CurrentTrain: epoch  8, batch    45 | loss: 3.9980373Losses:  3.8771843910217285 0.055371545255184174
CurrentTrain: epoch  8, batch    46 | loss: 3.9325559Losses:  3.9155776500701904 0.03987931087613106
CurrentTrain: epoch  8, batch    47 | loss: 3.9554570Losses:  3.9471709728240967 0.057179197669029236
CurrentTrain: epoch  8, batch    48 | loss: 4.0043502Losses:  3.937117576599121 0.049367547035217285
CurrentTrain: epoch  8, batch    49 | loss: 3.9864850Losses:  3.93565034866333 0.069181427359581
CurrentTrain: epoch  8, batch    50 | loss: 4.0048318Losses:  3.8800947666168213 0.039902716875076294
CurrentTrain: epoch  8, batch    51 | loss: 3.9199975Losses:  3.960932970046997 0.033421292901039124
CurrentTrain: epoch  8, batch    52 | loss: 3.9943542Losses:  3.900690793991089 0.05997258797287941
CurrentTrain: epoch  8, batch    53 | loss: 3.9606633Losses:  3.9491140842437744 0.06466144323348999
CurrentTrain: epoch  8, batch    54 | loss: 4.0137753Losses:  3.974148750305176 0.018623024225234985
CurrentTrain: epoch  8, batch    55 | loss: 3.9927719Losses:  3.86788272857666 0.039110857993364334
CurrentTrain: epoch  8, batch    56 | loss: 3.9069936Losses:  3.9358057975769043 0.060486145317554474
CurrentTrain: epoch  8, batch    57 | loss: 3.9962919Losses:  3.9240496158599854 0.05767522752285004
CurrentTrain: epoch  8, batch    58 | loss: 3.9817247Losses:  3.9596259593963623 0.03027103841304779
CurrentTrain: epoch  8, batch    59 | loss: 3.9898970Losses:  3.879810094833374 0.03397967666387558
CurrentTrain: epoch  8, batch    60 | loss: 3.9137897Losses:  3.9813995361328125 0.05878110229969025
CurrentTrain: epoch  8, batch    61 | loss: 4.0401807Losses:  3.944885730743408 0.025428367778658867
CurrentTrain: epoch  8, batch    62 | loss: 3.9703140Losses:  3.928201198577881 0.06706569343805313
CurrentTrain: epoch  8, batch    63 | loss: 3.9952669Losses:  3.9541635513305664 0.030757077038288116
CurrentTrain: epoch  8, batch    64 | loss: 3.9849207Losses:  3.9915125370025635 0.041796550154685974
CurrentTrain: epoch  8, batch    65 | loss: 4.0333090Losses:  3.9478578567504883 0.031870149075984955
CurrentTrain: epoch  8, batch    66 | loss: 3.9797280Losses:  3.9839534759521484 0.0564059317111969
CurrentTrain: epoch  8, batch    67 | loss: 4.0403595Losses:  3.952119827270508 0.04483596235513687
CurrentTrain: epoch  8, batch    68 | loss: 3.9969559Losses:  3.9190683364868164 0.04903874546289444
CurrentTrain: epoch  8, batch    69 | loss: 3.9681070Losses:  3.9707579612731934 0.0563492551445961
CurrentTrain: epoch  8, batch    70 | loss: 4.0271072Losses:  3.933091878890991 0.030457669869065285
CurrentTrain: epoch  8, batch    71 | loss: 3.9635496Losses:  3.932246685028076 0.05382890999317169
CurrentTrain: epoch  8, batch    72 | loss: 3.9860756Losses:  3.965801954269409 0.04601237177848816
CurrentTrain: epoch  8, batch    73 | loss: 4.0118141Losses:  3.949392318725586 0.04032908007502556
CurrentTrain: epoch  8, batch    74 | loss: 3.9897213Losses:  3.9528727531433105 0.05555512756109238
CurrentTrain: epoch  8, batch    75 | loss: 4.0084281Losses:  3.9722161293029785 0.04448094964027405
CurrentTrain: epoch  8, batch    76 | loss: 4.0166969Losses:  3.898210048675537 0.05762273073196411
CurrentTrain: epoch  8, batch    77 | loss: 3.9558327Losses:  3.932018280029297 0.04520503804087639
CurrentTrain: epoch  8, batch    78 | loss: 3.9772234Losses:  3.936279773712158 0.028937309980392456
CurrentTrain: epoch  8, batch    79 | loss: 3.9652171Losses:  3.961164951324463 0.025447923690080643
CurrentTrain: epoch  8, batch    80 | loss: 3.9866128Losses:  3.9259865283966064 0.03416789695620537
CurrentTrain: epoch  8, batch    81 | loss: 3.9601545Losses:  3.968275547027588 0.03076615184545517
CurrentTrain: epoch  8, batch    82 | loss: 3.9990418Losses:  3.920644998550415 0.037449587136507034
CurrentTrain: epoch  8, batch    83 | loss: 3.9580946Losses:  3.9749677181243896 0.02657456323504448
CurrentTrain: epoch  8, batch    84 | loss: 4.0015421Losses:  3.965989351272583 0.05819900706410408
CurrentTrain: epoch  8, batch    85 | loss: 4.0241885Losses:  3.944182872772217 0.04933790862560272
CurrentTrain: epoch  8, batch    86 | loss: 3.9935207Losses:  3.913543224334717 0.028713103383779526
CurrentTrain: epoch  8, batch    87 | loss: 3.9422562Losses:  3.9002249240875244 0.05788451060652733
CurrentTrain: epoch  8, batch    88 | loss: 3.9581094Losses:  3.9354796409606934 0.04240426421165466
CurrentTrain: epoch  8, batch    89 | loss: 3.9778838Losses:  3.894460439682007 0.04908350110054016
CurrentTrain: epoch  8, batch    90 | loss: 3.9435439Losses:  3.9740195274353027 0.04089082032442093
CurrentTrain: epoch  8, batch    91 | loss: 4.0149102Losses:  3.905601739883423 0.06688167154788971
CurrentTrain: epoch  8, batch    92 | loss: 3.9724834Losses:  3.951714038848877 0.042015191167593
CurrentTrain: epoch  8, batch    93 | loss: 3.9937291Losses:  3.9071907997131348 0.0481853112578392
CurrentTrain: epoch  8, batch    94 | loss: 3.9553761Losses:  4.003719329833984 0.039167627692222595
CurrentTrain: epoch  8, batch    95 | loss: 4.0428867Losses:  3.879281520843506 0.029555153101682663
CurrentTrain: epoch  8, batch    96 | loss: 3.9088366Losses:  3.955188751220703 0.03704313933849335
CurrentTrain: epoch  8, batch    97 | loss: 3.9922318Losses:  3.972135543823242 0.035317521542310715
CurrentTrain: epoch  8, batch    98 | loss: 4.0074530Losses:  3.890336275100708 0.028727369382977486
CurrentTrain: epoch  8, batch    99 | loss: 3.9190636Losses:  3.944207191467285 0.03849030286073685
CurrentTrain: epoch  8, batch   100 | loss: 3.9826975Losses:  3.9430172443389893 0.03780224919319153
CurrentTrain: epoch  8, batch   101 | loss: 3.9808195Losses:  3.9233336448669434 0.04580302909016609
CurrentTrain: epoch  8, batch   102 | loss: 3.9691367Losses:  3.904144287109375 0.024578526616096497
CurrentTrain: epoch  8, batch   103 | loss: 3.9287229Losses:  3.9218766689300537 0.04021575674414635
CurrentTrain: epoch  8, batch   104 | loss: 3.9620924Losses:  3.9327242374420166 0.03840463608503342
CurrentTrain: epoch  8, batch   105 | loss: 3.9711289Losses:  3.9560065269470215 0.03358365222811699
CurrentTrain: epoch  8, batch   106 | loss: 3.9895902Losses:  3.9298253059387207 0.07048648595809937
CurrentTrain: epoch  8, batch   107 | loss: 4.0003119Losses:  3.89212965965271 0.049476880580186844
CurrentTrain: epoch  8, batch   108 | loss: 3.9416065Losses:  3.942631959915161 0.039544880390167236
CurrentTrain: epoch  8, batch   109 | loss: 3.9821768Losses:  3.862443447113037 0.016882257536053658
CurrentTrain: epoch  8, batch   110 | loss: 3.8793256Losses:  3.943718433380127 0.052293047308921814
CurrentTrain: epoch  8, batch   111 | loss: 3.9960115Losses:  3.8897972106933594 0.035114165395498276
CurrentTrain: epoch  8, batch   112 | loss: 3.9249113Losses:  3.9153761863708496 0.05939720198512077
CurrentTrain: epoch  8, batch   113 | loss: 3.9747734Losses:  3.928699016571045 0.05418325960636139
CurrentTrain: epoch  8, batch   114 | loss: 3.9828823Losses:  3.9350132942199707 0.031796008348464966
CurrentTrain: epoch  8, batch   115 | loss: 3.9668093Losses:  3.8761582374572754 0.051885657012462616
CurrentTrain: epoch  8, batch   116 | loss: 3.9280438Losses:  3.907677173614502 0.06327072530984879
CurrentTrain: epoch  8, batch   117 | loss: 3.9709480Losses:  3.9815878868103027 0.06765326857566833
CurrentTrain: epoch  8, batch   118 | loss: 4.0492411Losses:  3.9626011848449707 0.0511910654604435
CurrentTrain: epoch  8, batch   119 | loss: 4.0137920Losses:  3.9309160709381104 0.07957342267036438
CurrentTrain: epoch  8, batch   120 | loss: 4.0104895Losses:  3.937429189682007 0.0243337694555521
CurrentTrain: epoch  8, batch   121 | loss: 3.9617629Losses:  3.9078311920166016 0.0645226463675499
CurrentTrain: epoch  8, batch   122 | loss: 3.9723539Losses:  4.000789642333984 0.026266852393746376
CurrentTrain: epoch  8, batch   123 | loss: 4.0270567Losses:  3.915435314178467 0.037914302200078964
CurrentTrain: epoch  8, batch   124 | loss: 3.9533496Losses:  3.8874611854553223 0.05521602928638458
CurrentTrain: epoch  9, batch     0 | loss: 3.9426773Losses:  3.937704563140869 0.040470853447914124
CurrentTrain: epoch  9, batch     1 | loss: 3.9781754Losses:  3.8997185230255127 0.028101246803998947
CurrentTrain: epoch  9, batch     2 | loss: 3.9278197Losses:  3.911123037338257 0.0762050598859787
CurrentTrain: epoch  9, batch     3 | loss: 3.9873281Losses:  3.9438247680664062 0.012867750599980354
CurrentTrain: epoch  9, batch     4 | loss: 3.9566925Losses:  3.9488911628723145 0.04314267635345459
CurrentTrain: epoch  9, batch     5 | loss: 3.9920340Losses:  3.9135313034057617 0.07019263505935669
CurrentTrain: epoch  9, batch     6 | loss: 3.9837239Losses:  3.9278881549835205 0.04392728954553604
CurrentTrain: epoch  9, batch     7 | loss: 3.9718153Losses:  3.933694362640381 0.04026667773723602
CurrentTrain: epoch  9, batch     8 | loss: 3.9739611Losses:  3.9392430782318115 0.06352944672107697
CurrentTrain: epoch  9, batch     9 | loss: 4.0027723Losses:  3.9397621154785156 0.026414040476083755
CurrentTrain: epoch  9, batch    10 | loss: 3.9661763Losses:  3.900740146636963 0.04179626703262329
CurrentTrain: epoch  9, batch    11 | loss: 3.9425364Losses:  3.943010091781616 0.05120617523789406
CurrentTrain: epoch  9, batch    12 | loss: 3.9942162Losses:  3.936457395553589 0.06425876915454865
CurrentTrain: epoch  9, batch    13 | loss: 4.0007162Losses:  3.8834993839263916 0.025431375950574875
CurrentTrain: epoch  9, batch    14 | loss: 3.9089308Losses:  3.969550609588623 0.03959014639258385
CurrentTrain: epoch  9, batch    15 | loss: 4.0091410Losses:  4.009036064147949 0.03218384459614754
CurrentTrain: epoch  9, batch    16 | loss: 4.0412197Losses:  4.019095420837402 0.02306068316102028
CurrentTrain: epoch  9, batch    17 | loss: 4.0421562Losses:  3.9422519207000732 0.03809089958667755
CurrentTrain: epoch  9, batch    18 | loss: 3.9803429Losses:  3.949131965637207 0.03605484217405319
CurrentTrain: epoch  9, batch    19 | loss: 3.9851868Losses:  3.892007827758789 0.01968422345817089
CurrentTrain: epoch  9, batch    20 | loss: 3.9116921Losses:  3.931044578552246 0.046626798808574677
CurrentTrain: epoch  9, batch    21 | loss: 3.9776714Losses:  3.9320273399353027 0.04858949035406113
CurrentTrain: epoch  9, batch    22 | loss: 3.9806168Losses:  3.9403481483459473 0.03983227163553238
CurrentTrain: epoch  9, batch    23 | loss: 3.9801805Losses:  3.9024391174316406 0.01929350383579731
CurrentTrain: epoch  9, batch    24 | loss: 3.9217327Losses:  3.9615910053253174 0.042079564183950424
CurrentTrain: epoch  9, batch    25 | loss: 4.0036707Losses:  3.9335198402404785 0.03817742317914963
CurrentTrain: epoch  9, batch    26 | loss: 3.9716973Losses:  3.902764081954956 0.06541400402784348
CurrentTrain: epoch  9, batch    27 | loss: 3.9681780Losses:  3.8921313285827637 0.046055592596530914
CurrentTrain: epoch  9, batch    28 | loss: 3.9381869Losses:  3.9451651573181152 0.04780719429254532
CurrentTrain: epoch  9, batch    29 | loss: 3.9929724Losses:  3.938504219055176 0.047185055911540985
CurrentTrain: epoch  9, batch    30 | loss: 3.9856892Losses:  3.9039411544799805 0.04153440147638321
CurrentTrain: epoch  9, batch    31 | loss: 3.9454756Losses:  3.887521266937256 0.02030838280916214
CurrentTrain: epoch  9, batch    32 | loss: 3.9078298Losses:  3.9184350967407227 0.053641919046640396
CurrentTrain: epoch  9, batch    33 | loss: 3.9720771Losses:  3.9465627670288086 0.03359222412109375
CurrentTrain: epoch  9, batch    34 | loss: 3.9801550Losses:  3.954432487487793 0.04666037857532501
CurrentTrain: epoch  9, batch    35 | loss: 4.0010929Losses:  3.925102949142456 0.047436803579330444
CurrentTrain: epoch  9, batch    36 | loss: 3.9725397Losses:  3.916637897491455 0.044333603233098984
CurrentTrain: epoch  9, batch    37 | loss: 3.9609716Losses:  3.919414520263672 0.03994718939065933
CurrentTrain: epoch  9, batch    38 | loss: 3.9593618Losses:  3.935682773590088 0.026367057114839554
CurrentTrain: epoch  9, batch    39 | loss: 3.9620497Losses:  3.955543279647827 0.023642444983124733
CurrentTrain: epoch  9, batch    40 | loss: 3.9791858Losses:  3.9457287788391113 0.04466031491756439
CurrentTrain: epoch  9, batch    41 | loss: 3.9903891Losses:  3.9578237533569336 0.053902000188827515
CurrentTrain: epoch  9, batch    42 | loss: 4.0117259Losses:  3.9657437801361084 0.04233741760253906
CurrentTrain: epoch  9, batch    43 | loss: 4.0080814Losses:  3.9680092334747314 0.030832702293992043
CurrentTrain: epoch  9, batch    44 | loss: 3.9988420Losses:  3.959604501724243 0.03608721122145653
CurrentTrain: epoch  9, batch    45 | loss: 3.9956918Losses:  3.9434266090393066 0.029844406992197037
CurrentTrain: epoch  9, batch    46 | loss: 3.9732711Losses:  3.9366936683654785 0.057904068380594254
CurrentTrain: epoch  9, batch    47 | loss: 3.9945977Losses:  3.920073986053467 0.0525665245950222
CurrentTrain: epoch  9, batch    48 | loss: 3.9726405Losses:  3.81710147857666 0.05056951195001602
CurrentTrain: epoch  9, batch    49 | loss: 3.8676710Losses:  3.901366949081421 0.026729175820946693
CurrentTrain: epoch  9, batch    50 | loss: 3.9280961Losses:  3.920501708984375 0.047418661415576935
CurrentTrain: epoch  9, batch    51 | loss: 3.9679203Losses:  3.8694210052490234 0.028022905811667442
CurrentTrain: epoch  9, batch    52 | loss: 3.8974440Losses:  3.94321870803833 0.0396278090775013
CurrentTrain: epoch  9, batch    53 | loss: 3.9828465Losses:  3.9132461547851562 0.039381809532642365
CurrentTrain: epoch  9, batch    54 | loss: 3.9526279Losses:  3.9886789321899414 0.0433521568775177
CurrentTrain: epoch  9, batch    55 | loss: 4.0320311Losses:  3.9226250648498535 0.055785130709409714
CurrentTrain: epoch  9, batch    56 | loss: 3.9784102Losses:  3.902284622192383 0.04077018052339554
CurrentTrain: epoch  9, batch    57 | loss: 3.9430549Losses:  3.924135208129883 0.045791711658239365
CurrentTrain: epoch  9, batch    58 | loss: 3.9699268Losses:  3.9470486640930176 0.05076407641172409
CurrentTrain: epoch  9, batch    59 | loss: 3.9978127Losses:  3.8945837020874023 0.05727046728134155
CurrentTrain: epoch  9, batch    60 | loss: 3.9518542Losses:  3.9475173950195312 0.0223118644207716
CurrentTrain: epoch  9, batch    61 | loss: 3.9698293Losses:  3.951594829559326 0.06731736660003662
CurrentTrain: epoch  9, batch    62 | loss: 4.0189123Losses:  3.9452414512634277 0.031711749732494354
CurrentTrain: epoch  9, batch    63 | loss: 3.9769533Losses:  3.9021265506744385 0.03603854775428772
CurrentTrain: epoch  9, batch    64 | loss: 3.9381652Losses:  3.9259328842163086 0.05918523669242859
CurrentTrain: epoch  9, batch    65 | loss: 3.9851182Losses:  3.9682209491729736 0.03239467740058899
CurrentTrain: epoch  9, batch    66 | loss: 4.0006156Losses:  3.914609670639038 0.0359916090965271
CurrentTrain: epoch  9, batch    67 | loss: 3.9506013Losses:  3.975032091140747 0.02292279340326786
CurrentTrain: epoch  9, batch    68 | loss: 3.9979548Losses:  3.9177870750427246 0.05345489829778671
CurrentTrain: epoch  9, batch    69 | loss: 3.9712420Losses:  3.958414316177368 0.03229936212301254
CurrentTrain: epoch  9, batch    70 | loss: 3.9907136Losses:  3.9199674129486084 0.03821750730276108
CurrentTrain: epoch  9, batch    71 | loss: 3.9581850Losses:  3.920027732849121 0.045391395688056946
CurrentTrain: epoch  9, batch    72 | loss: 3.9654191Losses:  3.945387363433838 0.03649953752756119
CurrentTrain: epoch  9, batch    73 | loss: 3.9818869Losses:  3.9889976978302 0.03144422918558121
CurrentTrain: epoch  9, batch    74 | loss: 4.0204420Losses:  3.9796345233917236 0.045109592378139496
CurrentTrain: epoch  9, batch    75 | loss: 4.0247440Losses:  3.8804402351379395 0.058122098445892334
CurrentTrain: epoch  9, batch    76 | loss: 3.9385624Losses:  3.919808864593506 0.014559595845639706
CurrentTrain: epoch  9, batch    77 | loss: 3.9343684Losses:  3.9231536388397217 0.047467559576034546
CurrentTrain: epoch  9, batch    78 | loss: 3.9706211Losses:  3.9072463512420654 0.05002698302268982
CurrentTrain: epoch  9, batch    79 | loss: 3.9572732Losses:  3.94989013671875 0.02738424763083458
CurrentTrain: epoch  9, batch    80 | loss: 3.9772744Losses:  3.9172210693359375 0.0604875385761261
CurrentTrain: epoch  9, batch    81 | loss: 3.9777086Losses:  3.9181735515594482 0.030162181705236435
CurrentTrain: epoch  9, batch    82 | loss: 3.9483356Losses:  3.8617405891418457 0.039352238178253174
CurrentTrain: epoch  9, batch    83 | loss: 3.9010928Losses:  3.9817934036254883 0.04645572230219841
CurrentTrain: epoch  9, batch    84 | loss: 4.0282493Losses:  3.910292863845825 0.0713045597076416
CurrentTrain: epoch  9, batch    85 | loss: 3.9815974Losses:  3.9307498931884766 0.022335920482873917
CurrentTrain: epoch  9, batch    86 | loss: 3.9530859Losses:  3.9322524070739746 0.04129687324166298
CurrentTrain: epoch  9, batch    87 | loss: 3.9735494Losses:  3.946897268295288 0.04428229480981827
CurrentTrain: epoch  9, batch    88 | loss: 3.9911795Losses:  3.852097511291504 0.06339399516582489
CurrentTrain: epoch  9, batch    89 | loss: 3.9154916Losses:  3.9250259399414062 0.0308857262134552
CurrentTrain: epoch  9, batch    90 | loss: 3.9559116Losses:  3.9127485752105713 0.0383613184094429
CurrentTrain: epoch  9, batch    91 | loss: 3.9511099Losses:  3.9371724128723145 0.03382096067070961
CurrentTrain: epoch  9, batch    92 | loss: 3.9709933Losses:  3.9286627769470215 0.0313660204410553
CurrentTrain: epoch  9, batch    93 | loss: 3.9600289Losses:  3.9497737884521484 0.05573071539402008
CurrentTrain: epoch  9, batch    94 | loss: 4.0055046Losses:  3.8698055744171143 0.02681421861052513
CurrentTrain: epoch  9, batch    95 | loss: 3.8966198Losses:  3.9605507850646973 0.06880450993776321
CurrentTrain: epoch  9, batch    96 | loss: 4.0293555Losses:  3.9099292755126953 0.07862169295549393
CurrentTrain: epoch  9, batch    97 | loss: 3.9885509Losses:  3.9381847381591797 0.04915532469749451
CurrentTrain: epoch  9, batch    98 | loss: 3.9873400Losses:  3.9317846298217773 0.05363892763853073
CurrentTrain: epoch  9, batch    99 | loss: 3.9854236Losses:  3.9501500129699707 0.01956946961581707
CurrentTrain: epoch  9, batch   100 | loss: 3.9697194Losses:  3.932839870452881 0.04150943085551262
CurrentTrain: epoch  9, batch   101 | loss: 3.9743493Losses:  3.92911434173584 0.038273345679044724
CurrentTrain: epoch  9, batch   102 | loss: 3.9673877Losses:  3.946913242340088 0.055128928273916245
CurrentTrain: epoch  9, batch   103 | loss: 4.0020423Losses:  3.902891159057617 0.029394831508398056
CurrentTrain: epoch  9, batch   104 | loss: 3.9322860Losses:  3.931607484817505 0.02662087231874466
CurrentTrain: epoch  9, batch   105 | loss: 3.9582283Losses:  3.9747815132141113 0.026396803557872772
CurrentTrain: epoch  9, batch   106 | loss: 4.0011783Losses:  3.955860137939453 0.035101816058158875
CurrentTrain: epoch  9, batch   107 | loss: 3.9909620Losses:  3.897798538208008 0.048832543194293976
CurrentTrain: epoch  9, batch   108 | loss: 3.9466312Losses:  3.9513649940490723 0.03950344771146774
CurrentTrain: epoch  9, batch   109 | loss: 3.9908683Losses:  3.9530839920043945 0.04685773327946663
CurrentTrain: epoch  9, batch   110 | loss: 3.9999418Losses:  3.9352920055389404 0.031922824680805206
CurrentTrain: epoch  9, batch   111 | loss: 3.9672148Losses:  3.9536685943603516 0.03148046135902405
CurrentTrain: epoch  9, batch   112 | loss: 3.9851491Losses:  4.039660453796387 0.03897297382354736
CurrentTrain: epoch  9, batch   113 | loss: 4.0786333Losses:  3.950143337249756 0.03915194422006607
CurrentTrain: epoch  9, batch   114 | loss: 3.9892952Losses:  3.9490034580230713 0.03147280216217041
CurrentTrain: epoch  9, batch   115 | loss: 3.9804764Losses:  3.959627628326416 0.0267984289675951
CurrentTrain: epoch  9, batch   116 | loss: 3.9864261Losses:  3.9541237354278564 0.03154831379652023
CurrentTrain: epoch  9, batch   117 | loss: 3.9856720Losses:  3.8964600563049316 0.04630200192332268
CurrentTrain: epoch  9, batch   118 | loss: 3.9427621Losses:  3.8831727504730225 0.022594310343265533
CurrentTrain: epoch  9, batch   119 | loss: 3.9057670Losses:  3.9008395671844482 0.06117735803127289
CurrentTrain: epoch  9, batch   120 | loss: 3.9620168Losses:  3.9275460243225098 0.04993361607193947
CurrentTrain: epoch  9, batch   121 | loss: 3.9774797Losses:  3.9078562259674072 0.048392750322818756
CurrentTrain: epoch  9, batch   122 | loss: 3.9562490Losses:  3.9231865406036377 0.03089769370853901
CurrentTrain: epoch  9, batch   123 | loss: 3.9540842Losses:  3.8927903175354004 0.0356290377676487
CurrentTrain: epoch  9, batch   124 | loss: 3.9284194
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [2 7 8 1 2 2 2 2 0 5 1 6 2 2 2 0 4 2 2 3]
Losses:  8.224337577819824 1.355475664138794
CurrentTrain: epoch  0, batch     0 | loss: 9.5798130Losses:  10.201004028320312 1.0652694702148438
CurrentTrain: epoch  0, batch     1 | loss: 11.2662735Losses:  9.957172393798828 1.1419165134429932
CurrentTrain: epoch  0, batch     2 | loss: 11.0990887Losses:  8.979714393615723 1.2425875663757324
CurrentTrain: epoch  0, batch     3 | loss: 10.2223015Losses:  8.046683311462402 1.5323742628097534
CurrentTrain: epoch  0, batch     4 | loss: 9.5790577Losses:  7.454219818115234 1.573357105255127
CurrentTrain: epoch  0, batch     5 | loss: 9.0275764Losses:  4.49709415435791 0.3950168490409851
CurrentTrain: epoch  0, batch     6 | loss: 4.8921108Losses:  4.0749311447143555 1.2229406833648682
CurrentTrain: epoch  1, batch     0 | loss: 5.2978716Losses:  4.281476020812988 1.4055101871490479
CurrentTrain: epoch  1, batch     1 | loss: 5.6869860Losses:  3.4782519340515137 1.197701096534729
CurrentTrain: epoch  1, batch     2 | loss: 4.6759529Losses:  4.211061000823975 1.3482961654663086
CurrentTrain: epoch  1, batch     3 | loss: 5.5593572Losses:  2.9709506034851074 0.9987163543701172
CurrentTrain: epoch  1, batch     4 | loss: 3.9696670Losses:  3.2048192024230957 1.0524500608444214
CurrentTrain: epoch  1, batch     5 | loss: 4.2572694Losses:  2.9040207862854004 0.17666786909103394
CurrentTrain: epoch  1, batch     6 | loss: 3.0806887Losses:  3.6833019256591797 1.0531415939331055
CurrentTrain: epoch  2, batch     0 | loss: 4.7364435Losses:  2.7346601486206055 0.3560023307800293
CurrentTrain: epoch  2, batch     1 | loss: 3.0906625Losses:  2.962465286254883 1.3934597969055176
CurrentTrain: epoch  2, batch     2 | loss: 4.3559251Losses:  2.6441898345947266 0.8167438507080078
CurrentTrain: epoch  2, batch     3 | loss: 3.4609337Losses:  2.023935317993164 0.8434371948242188
CurrentTrain: epoch  2, batch     4 | loss: 2.8673725Losses:  2.9104697704315186 1.0709631443023682
CurrentTrain: epoch  2, batch     5 | loss: 3.9814329Losses:  4.96972131729126 0.3405296802520752
CurrentTrain: epoch  2, batch     6 | loss: 5.3102512Losses:  2.5419058799743652 0.5985524654388428
CurrentTrain: epoch  3, batch     0 | loss: 3.1404583Losses:  2.457340717315674 0.7363106608390808
CurrentTrain: epoch  3, batch     1 | loss: 3.1936514Losses:  3.3924551010131836 1.4188600778579712
CurrentTrain: epoch  3, batch     2 | loss: 4.8113151Losses:  1.9259536266326904 0.6453166007995605
CurrentTrain: epoch  3, batch     3 | loss: 2.5712702Losses:  2.187309741973877 0.8211562633514404
CurrentTrain: epoch  3, batch     4 | loss: 3.0084660Losses:  1.9418902397155762 0.614159107208252
CurrentTrain: epoch  3, batch     5 | loss: 2.5560493Losses:  2.3075947761535645 0.17146560549736023
CurrentTrain: epoch  3, batch     6 | loss: 2.4790604Losses:  2.3634426593780518 0.9828590154647827
CurrentTrain: epoch  4, batch     0 | loss: 3.3463016Losses:  2.289095878601074 1.0076428651809692
CurrentTrain: epoch  4, batch     1 | loss: 3.2967386Losses:  1.713326096534729 0.5083025693893433
CurrentTrain: epoch  4, batch     2 | loss: 2.2216287Losses:  2.537745475769043 0.9879474639892578
CurrentTrain: epoch  4, batch     3 | loss: 3.5256929Losses:  1.9507142305374146 0.4647945761680603
CurrentTrain: epoch  4, batch     4 | loss: 2.4155087Losses:  2.148020029067993 0.6189775466918945
CurrentTrain: epoch  4, batch     5 | loss: 2.7669976Losses:  2.0625646114349365 0.31295037269592285
CurrentTrain: epoch  4, batch     6 | loss: 2.3755150Losses:  2.081967353820801 0.6646595597267151
CurrentTrain: epoch  5, batch     0 | loss: 2.7466269Losses:  2.0998268127441406 0.6530736684799194
CurrentTrain: epoch  5, batch     1 | loss: 2.7529006Losses:  1.8271129131317139 0.562890887260437
CurrentTrain: epoch  5, batch     2 | loss: 2.3900037Losses:  1.9296607971191406 0.691910982131958
CurrentTrain: epoch  5, batch     3 | loss: 2.6215718Losses:  1.9966952800750732 0.6732138395309448
CurrentTrain: epoch  5, batch     4 | loss: 2.6699090Losses:  2.38419771194458 0.656017541885376
CurrentTrain: epoch  5, batch     5 | loss: 3.0402153Losses:  1.9522022008895874 0.14413464069366455
CurrentTrain: epoch  5, batch     6 | loss: 2.0963368Losses:  2.06251859664917 0.41122883558273315
CurrentTrain: epoch  6, batch     0 | loss: 2.4737475Losses:  2.3278648853302 0.8339817523956299
CurrentTrain: epoch  6, batch     1 | loss: 3.1618466Losses:  1.7594290971755981 0.4326697885990143
CurrentTrain: epoch  6, batch     2 | loss: 2.1920989Losses:  2.0258805751800537 0.3797622323036194
CurrentTrain: epoch  6, batch     3 | loss: 2.4056427Losses:  1.8256175518035889 0.5033446550369263
CurrentTrain: epoch  6, batch     4 | loss: 2.3289623Losses:  2.066127300262451 0.5254946947097778
CurrentTrain: epoch  6, batch     5 | loss: 2.5916219Losses:  1.8474743366241455 0.1693742722272873
CurrentTrain: epoch  6, batch     6 | loss: 2.0168486Losses:  1.941165804862976 0.5276789665222168
CurrentTrain: epoch  7, batch     0 | loss: 2.4688449Losses:  1.9008266925811768 0.7231132984161377
CurrentTrain: epoch  7, batch     1 | loss: 2.6239400Losses:  1.8753836154937744 0.447879433631897
CurrentTrain: epoch  7, batch     2 | loss: 2.3232632Losses:  1.7627516984939575 0.42042237520217896
CurrentTrain: epoch  7, batch     3 | loss: 2.1831741Losses:  2.488536834716797 0.5630302429199219
CurrentTrain: epoch  7, batch     4 | loss: 3.0515671Losses:  1.7322704792022705 0.4814192056655884
CurrentTrain: epoch  7, batch     5 | loss: 2.2136898Losses:  1.7628508806228638 0.04421348124742508
CurrentTrain: epoch  7, batch     6 | loss: 1.8070644Losses:  1.9596914052963257 0.5130784511566162
CurrentTrain: epoch  8, batch     0 | loss: 2.4727697Losses:  1.791211724281311 0.41274207830429077
CurrentTrain: epoch  8, batch     1 | loss: 2.2039537Losses:  1.7626690864562988 0.33357590436935425
CurrentTrain: epoch  8, batch     2 | loss: 2.0962451Losses:  1.7742035388946533 0.4048093557357788
CurrentTrain: epoch  8, batch     3 | loss: 2.1790128Losses:  1.8289947509765625 0.39399564266204834
CurrentTrain: epoch  8, batch     4 | loss: 2.2229905Losses:  1.8950226306915283 0.3328382670879364
CurrentTrain: epoch  8, batch     5 | loss: 2.2278609Losses:  3.4366068840026855 0.11343620717525482
CurrentTrain: epoch  8, batch     6 | loss: 3.5500431Losses:  2.1199169158935547 0.48330187797546387
CurrentTrain: epoch  9, batch     0 | loss: 2.6032188Losses:  1.9106389284133911 0.23938067257404327
CurrentTrain: epoch  9, batch     1 | loss: 2.1500196Losses:  1.7269915342330933 0.4386030435562134
CurrentTrain: epoch  9, batch     2 | loss: 2.1655946Losses:  1.7936925888061523 0.41266459226608276
CurrentTrain: epoch  9, batch     3 | loss: 2.2063572Losses:  1.8039830923080444 0.24605639278888702
CurrentTrain: epoch  9, batch     4 | loss: 2.0500395Losses:  1.816011905670166 0.30665719509124756
CurrentTrain: epoch  9, batch     5 | loss: 2.1226692Losses:  1.6463398933410645 0.06941093504428864
CurrentTrain: epoch  9, batch     6 | loss: 1.7157508
Losses:  5.222397804260254 0.22008010745048523
MemoryTrain:  epoch  0, batch     0 | loss: 5.4424777Losses:  9.140254020690918 0.4694103002548218
MemoryTrain:  epoch  0, batch     1 | loss: 9.6096640Losses:  11.713949203491211 0.06781060993671417
MemoryTrain:  epoch  0, batch     2 | loss: 11.7817602Losses:  2.194755792617798 0.6289993524551392
MemoryTrain:  epoch  1, batch     0 | loss: 2.8237553Losses:  0.7449860572814941 0.3466716706752777
MemoryTrain:  epoch  1, batch     1 | loss: 1.0916578Losses:  0.35379093885421753 0.03373302146792412
MemoryTrain:  epoch  1, batch     2 | loss: 0.3875239Losses:  0.5276046991348267 0.5155193209648132
MemoryTrain:  epoch  2, batch     0 | loss: 1.0431240Losses:  1.4488002061843872 0.4113790690898895
MemoryTrain:  epoch  2, batch     1 | loss: 1.8601793Losses:  1.0200413465499878 0.17799216508865356
MemoryTrain:  epoch  2, batch     2 | loss: 1.1980336Losses:  0.3448535203933716 0.4102713465690613
MemoryTrain:  epoch  3, batch     0 | loss: 0.7551249Losses:  1.1559969186782837 0.32191234827041626
MemoryTrain:  epoch  3, batch     1 | loss: 1.4779093Losses:  0.40299203991889954 0.13391798734664917
MemoryTrain:  epoch  3, batch     2 | loss: 0.5369101Losses:  0.2229921966791153 0.32722538709640503
MemoryTrain:  epoch  4, batch     0 | loss: 0.5502176Losses:  0.5210780501365662 0.671581506729126
MemoryTrain:  epoch  4, batch     1 | loss: 1.1926596Losses:  0.10698103904724121 0.02663673833012581
MemoryTrain:  epoch  4, batch     2 | loss: 0.1336178Losses:  0.1961306482553482 0.4556500315666199
MemoryTrain:  epoch  5, batch     0 | loss: 0.6517807Losses:  0.20958605408668518 0.2343575805425644
MemoryTrain:  epoch  5, batch     1 | loss: 0.4439436Losses:  0.3244543671607971 0.34270212054252625
MemoryTrain:  epoch  5, batch     2 | loss: 0.6671565Losses:  0.17138774693012238 0.4726119935512543
MemoryTrain:  epoch  6, batch     0 | loss: 0.6439998Losses:  0.18054550886154175 0.2657490372657776
MemoryTrain:  epoch  6, batch     1 | loss: 0.4462945Losses:  0.10610519349575043 0.22882002592086792
MemoryTrain:  epoch  6, batch     2 | loss: 0.3349252Losses:  0.12413491308689117 0.17314618825912476
MemoryTrain:  epoch  7, batch     0 | loss: 0.2972811Losses:  0.18272070586681366 0.4649795889854431
MemoryTrain:  epoch  7, batch     1 | loss: 0.6477003Losses:  0.14199897646903992 0.11775997281074524
MemoryTrain:  epoch  7, batch     2 | loss: 0.2597589Losses:  0.10888393968343735 0.31417423486709595
MemoryTrain:  epoch  8, batch     0 | loss: 0.4230582Losses:  0.13327676057815552 0.29248684644699097
MemoryTrain:  epoch  8, batch     1 | loss: 0.4257636Losses:  0.20793285965919495 0.23476767539978027
MemoryTrain:  epoch  8, batch     2 | loss: 0.4427005Losses:  0.2061995267868042 0.2886715531349182
MemoryTrain:  epoch  9, batch     0 | loss: 0.4948711Losses:  0.11302361637353897 0.3065299689769745
MemoryTrain:  epoch  9, batch     1 | loss: 0.4195536Losses:  0.11858196556568146 0.1377953141927719
MemoryTrain:  epoch  9, batch     2 | loss: 0.2563773
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 81.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 81.36%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 81.14%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 80.06%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 92.26%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.68%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 92.50%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 92.35%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.89%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.91%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 92.60%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 92.47%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 91.93%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 91.41%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 91.05%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 90.24%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 89.31%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 88.39%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 87.50%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 86.70%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.85%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 86.38%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 86.39%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 86.29%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 86.17%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 86.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 86.15%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.62%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 87.34%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 87.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.45%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 87.39%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 87.34%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 87.34%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 87.19%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 86.99%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 86.75%   
cur_acc:  ['0.9464', '0.8006']
his_acc:  ['0.9464', '0.8675']
Clustering into  14  clusters
Clusters:  [ 0  7  9  0  0  0 11  0 10 12  8  3  0  0  5 13  4  0  0  6  2  0  0  0
  0  1  0  0  0  0]
Losses:  7.414428234100342 0.9827308654785156
CurrentTrain: epoch  0, batch     0 | loss: 8.3971596Losses:  10.90859603881836 1.719336986541748
CurrentTrain: epoch  0, batch     1 | loss: 12.6279335Losses:  9.25271224975586 1.5043550729751587
CurrentTrain: epoch  0, batch     2 | loss: 10.7570677Losses:  9.395597457885742 1.1877715587615967
CurrentTrain: epoch  0, batch     3 | loss: 10.5833693Losses:  8.283289909362793 1.5283584594726562
CurrentTrain: epoch  0, batch     4 | loss: 9.8116484Losses:  7.079808235168457 1.9909694194793701
CurrentTrain: epoch  0, batch     5 | loss: 9.0707779Losses:  4.667647361755371 0.4688951075077057
CurrentTrain: epoch  0, batch     6 | loss: 5.1365423Losses:  3.9486708641052246 1.3921618461608887
CurrentTrain: epoch  1, batch     0 | loss: 5.3408327Losses:  4.565555572509766 1.6865051984786987
CurrentTrain: epoch  1, batch     1 | loss: 6.2520609Losses:  3.6583566665649414 1.56796133518219
CurrentTrain: epoch  1, batch     2 | loss: 5.2263179Losses:  3.8301455974578857 1.2887259721755981
CurrentTrain: epoch  1, batch     3 | loss: 5.1188717Losses:  3.609487533569336 1.5072540044784546
CurrentTrain: epoch  1, batch     4 | loss: 5.1167417Losses:  3.273582935333252 1.4121153354644775
CurrentTrain: epoch  1, batch     5 | loss: 4.6856985Losses:  3.574615001678467 0.17887862026691437
CurrentTrain: epoch  1, batch     6 | loss: 3.7534935Losses:  3.7279653549194336 1.0108389854431152
CurrentTrain: epoch  2, batch     0 | loss: 4.7388043Losses:  3.4718661308288574 1.443327784538269
CurrentTrain: epoch  2, batch     1 | loss: 4.9151940Losses:  3.616420269012451 1.4219517707824707
CurrentTrain: epoch  2, batch     2 | loss: 5.0383720Losses:  2.83711576461792 1.3289144039154053
CurrentTrain: epoch  2, batch     3 | loss: 4.1660299Losses:  3.0424416065216064 1.193373441696167
CurrentTrain: epoch  2, batch     4 | loss: 4.2358150Losses:  2.6946053504943848 1.1314690113067627
CurrentTrain: epoch  2, batch     5 | loss: 3.8260744Losses:  2.764859914779663 0.4475589692592621
CurrentTrain: epoch  2, batch     6 | loss: 3.2124188Losses:  3.7085328102111816 1.1262470483779907
CurrentTrain: epoch  3, batch     0 | loss: 4.8347797Losses:  2.4537501335144043 0.9158064723014832
CurrentTrain: epoch  3, batch     1 | loss: 3.3695567Losses:  3.211376667022705 1.2331483364105225
CurrentTrain: epoch  3, batch     2 | loss: 4.4445248Losses:  3.0149636268615723 1.4656388759613037
CurrentTrain: epoch  3, batch     3 | loss: 4.4806023Losses:  2.4130425453186035 1.1734662055969238
CurrentTrain: epoch  3, batch     4 | loss: 3.5865088Losses:  2.9549503326416016 1.4692673683166504
CurrentTrain: epoch  3, batch     5 | loss: 4.4242177Losses:  2.9532222747802734 0.45352211594581604
CurrentTrain: epoch  3, batch     6 | loss: 3.4067445Losses:  3.559201240539551 0.9610963463783264
CurrentTrain: epoch  4, batch     0 | loss: 4.5202975Losses:  2.949705123901367 1.0882164239883423
CurrentTrain: epoch  4, batch     1 | loss: 4.0379214Losses:  2.1974844932556152 0.5050182342529297
CurrentTrain: epoch  4, batch     2 | loss: 2.7025027Losses:  3.1696653366088867 1.288112759590149
CurrentTrain: epoch  4, batch     3 | loss: 4.4577780Losses:  2.6996912956237793 1.263216495513916
CurrentTrain: epoch  4, batch     4 | loss: 3.9629078Losses:  2.326536178588867 1.111040711402893
CurrentTrain: epoch  4, batch     5 | loss: 3.4375768Losses:  2.2337594032287598 0.41733038425445557
CurrentTrain: epoch  4, batch     6 | loss: 2.6510897Losses:  2.5109682083129883 1.073350191116333
CurrentTrain: epoch  5, batch     0 | loss: 3.5843184Losses:  2.8886659145355225 1.05794095993042
CurrentTrain: epoch  5, batch     1 | loss: 3.9466069Losses:  2.0864124298095703 1.0198216438293457
CurrentTrain: epoch  5, batch     2 | loss: 3.1062341Losses:  2.7761075496673584 0.8651305437088013
CurrentTrain: epoch  5, batch     3 | loss: 3.6412382Losses:  2.4621386528015137 0.8934371471405029
CurrentTrain: epoch  5, batch     4 | loss: 3.3555758Losses:  2.6520073413848877 1.1734256744384766
CurrentTrain: epoch  5, batch     5 | loss: 3.8254330Losses:  2.0042319297790527 0.21453243494033813
CurrentTrain: epoch  5, batch     6 | loss: 2.2187643Losses:  2.469752788543701 0.9997392892837524
CurrentTrain: epoch  6, batch     0 | loss: 3.4694920Losses:  2.1419055461883545 0.8397254347801208
CurrentTrain: epoch  6, batch     1 | loss: 2.9816310Losses:  2.4964852333068848 0.8302313089370728
CurrentTrain: epoch  6, batch     2 | loss: 3.3267164Losses:  2.3325226306915283 0.9915142059326172
CurrentTrain: epoch  6, batch     3 | loss: 3.3240368Losses:  3.1614584922790527 0.7828826308250427
CurrentTrain: epoch  6, batch     4 | loss: 3.9443412Losses:  1.9415552616119385 0.571734607219696
CurrentTrain: epoch  6, batch     5 | loss: 2.5132899Losses:  1.7822840213775635 0.2931387424468994
CurrentTrain: epoch  6, batch     6 | loss: 2.0754228Losses:  2.3452811241149902 1.014681339263916
CurrentTrain: epoch  7, batch     0 | loss: 3.3599625Losses:  2.182694911956787 0.8980031609535217
CurrentTrain: epoch  7, batch     1 | loss: 3.0806980Losses:  1.9133543968200684 0.8873821496963501
CurrentTrain: epoch  7, batch     2 | loss: 2.8007364Losses:  2.7502198219299316 0.7892975211143494
CurrentTrain: epoch  7, batch     3 | loss: 3.5395174Losses:  2.0646438598632812 0.8960325717926025
CurrentTrain: epoch  7, batch     4 | loss: 2.9606764Losses:  2.1623904705047607 0.9857807755470276
CurrentTrain: epoch  7, batch     5 | loss: 3.1481712Losses:  3.15735125541687 0.2504587769508362
CurrentTrain: epoch  7, batch     6 | loss: 3.4078100Losses:  2.1881728172302246 0.7074499130249023
CurrentTrain: epoch  8, batch     0 | loss: 2.8956227Losses:  2.247192859649658 0.7462525367736816
CurrentTrain: epoch  8, batch     1 | loss: 2.9934454Losses:  2.3953793048858643 0.5400047898292542
CurrentTrain: epoch  8, batch     2 | loss: 2.9353840Losses:  2.3874564170837402 1.0331013202667236
CurrentTrain: epoch  8, batch     3 | loss: 3.4205577Losses:  2.0203394889831543 0.6580405235290527
CurrentTrain: epoch  8, batch     4 | loss: 2.6783800Losses:  1.948457956314087 0.8897842168807983
CurrentTrain: epoch  8, batch     5 | loss: 2.8382421Losses:  1.7914048433303833 0.2902598977088928
CurrentTrain: epoch  8, batch     6 | loss: 2.0816648Losses:  1.8499963283538818 0.4627700448036194
CurrentTrain: epoch  9, batch     0 | loss: 2.3127663Losses:  2.2419700622558594 0.8408104777336121
CurrentTrain: epoch  9, batch     1 | loss: 3.0827806Losses:  1.9108589887619019 0.8133708238601685
CurrentTrain: epoch  9, batch     2 | loss: 2.7242298Losses:  2.403688430786133 0.6324827671051025
CurrentTrain: epoch  9, batch     3 | loss: 3.0361712Losses:  2.2417078018188477 0.6859347820281982
CurrentTrain: epoch  9, batch     4 | loss: 2.9276426Losses:  2.0293712615966797 0.6617435216903687
CurrentTrain: epoch  9, batch     5 | loss: 2.6911149Losses:  1.8852753639221191 0.06275895982980728
CurrentTrain: epoch  9, batch     6 | loss: 1.9480343
Losses:  5.695586204528809 0.5241968035697937
MemoryTrain:  epoch  0, batch     0 | loss: 6.2197828Losses:  9.67697525024414 0.8178775906562805
MemoryTrain:  epoch  0, batch     1 | loss: 10.4948530Losses:  8.42017936706543 0.3050440549850464
MemoryTrain:  epoch  0, batch     2 | loss: 8.7252235Losses:  10.709012985229492 0.3139379024505615
MemoryTrain:  epoch  0, batch     3 | loss: 11.0229511Losses:  1.0503661632537842 0.4423670172691345
MemoryTrain:  epoch  1, batch     0 | loss: 1.4927332Losses:  1.5539939403533936 0.6992185115814209
MemoryTrain:  epoch  1, batch     1 | loss: 2.2532125Losses:  0.6820608377456665 0.4785103499889374
MemoryTrain:  epoch  1, batch     2 | loss: 1.1605712Losses:  0.5280457139015198 0.44003960490226746
MemoryTrain:  epoch  1, batch     3 | loss: 0.9680853Losses:  0.5996251106262207 0.4776500463485718
MemoryTrain:  epoch  2, batch     0 | loss: 1.0772752Losses:  0.9570177793502808 0.4294282793998718
MemoryTrain:  epoch  2, batch     1 | loss: 1.3864460Losses:  0.4621833264827728 0.4470558762550354
MemoryTrain:  epoch  2, batch     2 | loss: 0.9092392Losses:  0.6262637972831726 0.5040329694747925
MemoryTrain:  epoch  2, batch     3 | loss: 1.1302967Losses:  0.592174768447876 0.46254831552505493
MemoryTrain:  epoch  3, batch     0 | loss: 1.0547230Losses:  0.4541107714176178 0.5831597447395325
MemoryTrain:  epoch  3, batch     1 | loss: 1.0372705Losses:  0.3207102119922638 0.3578323721885681
MemoryTrain:  epoch  3, batch     2 | loss: 0.6785426Losses:  0.7691394686698914 0.5287553071975708
MemoryTrain:  epoch  3, batch     3 | loss: 1.2978947Losses:  0.24914704263210297 0.46753281354904175
MemoryTrain:  epoch  4, batch     0 | loss: 0.7166799Losses:  0.4574313759803772 0.5627943277359009
MemoryTrain:  epoch  4, batch     1 | loss: 1.0202258Losses:  0.3119276762008667 0.5904871225357056
MemoryTrain:  epoch  4, batch     2 | loss: 0.9024148Losses:  0.33095264434814453 0.3566005229949951
MemoryTrain:  epoch  4, batch     3 | loss: 0.6875532Losses:  0.24276399612426758 0.6656163334846497
MemoryTrain:  epoch  5, batch     0 | loss: 0.9083803Losses:  0.3408518433570862 0.4548943042755127
MemoryTrain:  epoch  5, batch     1 | loss: 0.7957461Losses:  0.251574844121933 0.6163402199745178
MemoryTrain:  epoch  5, batch     2 | loss: 0.8679150Losses:  0.2761040925979614 0.14633594453334808
MemoryTrain:  epoch  5, batch     3 | loss: 0.4224401Losses:  0.30405566096305847 0.4348503351211548
MemoryTrain:  epoch  6, batch     0 | loss: 0.7389060Losses:  0.33148154616355896 0.5521594882011414
MemoryTrain:  epoch  6, batch     1 | loss: 0.8836410Losses:  0.173762246966362 0.3490298390388489
MemoryTrain:  epoch  6, batch     2 | loss: 0.5227921Losses:  0.2337113320827484 0.4259967803955078
MemoryTrain:  epoch  6, batch     3 | loss: 0.6597081Losses:  0.16099752485752106 0.30739209055900574
MemoryTrain:  epoch  7, batch     0 | loss: 0.4683896Losses:  0.24991795420646667 0.4325307309627533
MemoryTrain:  epoch  7, batch     1 | loss: 0.6824487Losses:  0.21120557188987732 0.5502183437347412
MemoryTrain:  epoch  7, batch     2 | loss: 0.7614239Losses:  0.42706701159477234 0.5290285348892212
MemoryTrain:  epoch  7, batch     3 | loss: 0.9560956Losses:  0.20771096646785736 0.4522002339363098
MemoryTrain:  epoch  8, batch     0 | loss: 0.6599112Losses:  0.22399061918258667 0.40900278091430664
MemoryTrain:  epoch  8, batch     1 | loss: 0.6329934Losses:  0.21233616769313812 0.5369357466697693
MemoryTrain:  epoch  8, batch     2 | loss: 0.7492719Losses:  0.137624591588974 0.4194108843803406
MemoryTrain:  epoch  8, batch     3 | loss: 0.5570354Losses:  0.21142356097698212 0.46593719720840454
MemoryTrain:  epoch  9, batch     0 | loss: 0.6773608Losses:  0.2790161371231079 0.5804701447486877
MemoryTrain:  epoch  9, batch     1 | loss: 0.8594863Losses:  0.19830222427845 0.32379767298698425
MemoryTrain:  epoch  9, batch     2 | loss: 0.5220999Losses:  0.15206310153007507 0.23872503638267517
MemoryTrain:  epoch  9, batch     3 | loss: 0.3907881
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.28%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 58.54%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 56.85%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 57.03%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 58.14%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 58.82%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 60.59%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 61.32%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 69.08%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 67.92%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 67.42%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 66.57%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.84%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.00%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 91.09%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.17%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.11%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 91.14%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 91.21%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 90.80%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 90.58%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 90.12%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 89.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 89.56%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.37%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 88.29%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 87.58%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 86.89%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 85.99%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 84.97%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 84.04%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 83.14%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 82.26%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 83.18%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 83.16%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.27%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 83.25%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 83.16%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 83.08%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 83.53%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 83.94%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 84.65%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 84.57%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 84.43%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 84.35%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 84.19%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 83.83%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 83.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 84.05%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 83.68%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 83.22%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 82.81%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 82.46%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 82.12%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 81.63%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 81.98%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 82.02%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:  139 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 82.52%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 82.51%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 82.33%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 82.06%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 81.88%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 81.67%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 81.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.08%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 80.59%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 80.07%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 79.55%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 79.11%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 78.65%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 78.54%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.64%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 78.86%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.93%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 79.14%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 79.19%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 79.52%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.53%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 79.54%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 79.52%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.64%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 79.56%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 79.56%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.51%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 79.56%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 79.36%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 79.13%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 78.94%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 78.82%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 78.60%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 78.51%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 78.19%   
cur_acc:  ['0.9464', '0.8006', '0.6657']
his_acc:  ['0.9464', '0.8675', '0.7819']
Clustering into  19  clusters
Clusters:  [ 0  3 12  1  0  0 16  0 13 11 10 17  1  0 14  6 15  0  0 18  7  1  0  0
  0  9  0  0  0  0  3  8  1  1  0  5  0  0  4  2]
Losses:  6.771491050720215 0.7535035610198975
CurrentTrain: epoch  0, batch     0 | loss: 7.5249949Losses:  7.007171154022217 1.0175065994262695
CurrentTrain: epoch  0, batch     1 | loss: 8.0246773Losses:  9.591556549072266 1.5259380340576172
CurrentTrain: epoch  0, batch     2 | loss: 11.1174946Losses:  8.60302734375 1.2382500171661377
CurrentTrain: epoch  0, batch     3 | loss: 9.8412771Losses:  7.391997814178467 1.0266196727752686
CurrentTrain: epoch  0, batch     4 | loss: 8.4186172Losses:  7.04582405090332 1.387457013130188
CurrentTrain: epoch  0, batch     5 | loss: 8.4332809Losses:  5.17815637588501 0.4895439147949219
CurrentTrain: epoch  0, batch     6 | loss: 5.6677003Losses:  3.210759162902832 1.414961338043213
CurrentTrain: epoch  1, batch     0 | loss: 4.6257205Losses:  3.622736930847168 0.8904509544372559
CurrentTrain: epoch  1, batch     1 | loss: 4.5131879Losses:  3.953396797180176 0.6714440584182739
CurrentTrain: epoch  1, batch     2 | loss: 4.6248407Losses:  4.326157569885254 1.1637232303619385
CurrentTrain: epoch  1, batch     3 | loss: 5.4898806Losses:  3.106977939605713 0.9094164371490479
CurrentTrain: epoch  1, batch     4 | loss: 4.0163946Losses:  3.3886635303497314 0.8099334836006165
CurrentTrain: epoch  1, batch     5 | loss: 4.1985970Losses:  2.014488697052002 0.1018831878900528
CurrentTrain: epoch  1, batch     6 | loss: 2.1163719Losses:  2.7278759479522705 0.9537822604179382
CurrentTrain: epoch  2, batch     0 | loss: 3.6816583Losses:  2.221433639526367 0.6877255439758301
CurrentTrain: epoch  2, batch     1 | loss: 2.9091592Losses:  3.038926601409912 0.874083399772644
CurrentTrain: epoch  2, batch     2 | loss: 3.9130101Losses:  3.960505723953247 0.6709601283073425
CurrentTrain: epoch  2, batch     3 | loss: 4.6314659Losses:  3.441316843032837 0.9860748052597046
CurrentTrain: epoch  2, batch     4 | loss: 4.4273915Losses:  3.4771785736083984 0.8657691478729248
CurrentTrain: epoch  2, batch     5 | loss: 4.3429480Losses:  2.326582431793213 0.19047445058822632
CurrentTrain: epoch  2, batch     6 | loss: 2.5170569Losses:  3.6634926795959473 0.7011326551437378
CurrentTrain: epoch  3, batch     0 | loss: 4.3646255Losses:  2.1812357902526855 0.6563842296600342
CurrentTrain: epoch  3, batch     1 | loss: 2.8376200Losses:  2.37381649017334 0.8461171388626099
CurrentTrain: epoch  3, batch     2 | loss: 3.2199335Losses:  2.6964993476867676 0.6923186779022217
CurrentTrain: epoch  3, batch     3 | loss: 3.3888180Losses:  3.3858909606933594 0.5419700145721436
CurrentTrain: epoch  3, batch     4 | loss: 3.9278610Losses:  2.724224090576172 0.64642333984375
CurrentTrain: epoch  3, batch     5 | loss: 3.3706474Losses:  3.664670705795288 0.2536471486091614
CurrentTrain: epoch  3, batch     6 | loss: 3.9183178Losses:  2.224658489227295 0.5147783756256104
CurrentTrain: epoch  4, batch     0 | loss: 2.7394369Losses:  2.3988375663757324 0.6859242916107178
CurrentTrain: epoch  4, batch     1 | loss: 3.0847619Losses:  3.430117607116699 0.6589820384979248
CurrentTrain: epoch  4, batch     2 | loss: 4.0890999Losses:  2.124952793121338 0.7145863771438599
CurrentTrain: epoch  4, batch     3 | loss: 2.8395391Losses:  2.2127323150634766 0.5472127795219421
CurrentTrain: epoch  4, batch     4 | loss: 2.7599452Losses:  3.0102829933166504 0.9415296316146851
CurrentTrain: epoch  4, batch     5 | loss: 3.9518127Losses:  2.4495580196380615 0.08676766604185104
CurrentTrain: epoch  4, batch     6 | loss: 2.5363257Losses:  2.058631420135498 0.658208429813385
CurrentTrain: epoch  5, batch     0 | loss: 2.7168398Losses:  2.2396254539489746 0.5834933519363403
CurrentTrain: epoch  5, batch     1 | loss: 2.8231187Losses:  2.2880702018737793 0.47713857889175415
CurrentTrain: epoch  5, batch     2 | loss: 2.7652087Losses:  2.3141884803771973 0.6734107136726379
CurrentTrain: epoch  5, batch     3 | loss: 2.9875991Losses:  2.7983195781707764 0.5361237525939941
CurrentTrain: epoch  5, batch     4 | loss: 3.3344433Losses:  2.306337833404541 0.5460342168807983
CurrentTrain: epoch  5, batch     5 | loss: 2.8523722Losses:  2.264376640319824 0.3141268789768219
CurrentTrain: epoch  5, batch     6 | loss: 2.5785036Losses:  2.324824333190918 0.6664714813232422
CurrentTrain: epoch  6, batch     0 | loss: 2.9912958Losses:  1.9332211017608643 0.4233846962451935
CurrentTrain: epoch  6, batch     1 | loss: 2.3566058Losses:  2.183814287185669 0.6462599039077759
CurrentTrain: epoch  6, batch     2 | loss: 2.8300743Losses:  2.1391615867614746 0.6626160740852356
CurrentTrain: epoch  6, batch     3 | loss: 2.8017776Losses:  2.3883018493652344 0.6135221719741821
CurrentTrain: epoch  6, batch     4 | loss: 3.0018239Losses:  1.9665864706039429 0.3851512670516968
CurrentTrain: epoch  6, batch     5 | loss: 2.3517377Losses:  2.405601978302002 0.1876760721206665
CurrentTrain: epoch  6, batch     6 | loss: 2.5932779Losses:  2.2809958457946777 0.6141952276229858
CurrentTrain: epoch  7, batch     0 | loss: 2.8951912Losses:  2.1366772651672363 0.5853695273399353
CurrentTrain: epoch  7, batch     1 | loss: 2.7220469Losses:  1.910557508468628 0.36193761229515076
CurrentTrain: epoch  7, batch     2 | loss: 2.2724950Losses:  2.100492477416992 0.4390784204006195
CurrentTrain: epoch  7, batch     3 | loss: 2.5395708Losses:  1.9177589416503906 0.6203014850616455
CurrentTrain: epoch  7, batch     4 | loss: 2.5380604Losses:  1.7887828350067139 0.39246293902397156
CurrentTrain: epoch  7, batch     5 | loss: 2.1812458Losses:  1.8625431060791016 0.17160238325595856
CurrentTrain: epoch  7, batch     6 | loss: 2.0341456Losses:  2.054616689682007 0.383508563041687
CurrentTrain: epoch  8, batch     0 | loss: 2.4381251Losses:  1.896938681602478 0.4236244559288025
CurrentTrain: epoch  8, batch     1 | loss: 2.3205631Losses:  2.0472726821899414 0.3955461382865906
CurrentTrain: epoch  8, batch     2 | loss: 2.4428189Losses:  1.7199046611785889 0.165601447224617
CurrentTrain: epoch  8, batch     3 | loss: 1.8855062Losses:  1.7468693256378174 0.37998729944229126
CurrentTrain: epoch  8, batch     4 | loss: 2.1268566Losses:  1.7785115242004395 0.39185434579849243
CurrentTrain: epoch  8, batch     5 | loss: 2.1703658Losses:  2.6028242111206055 1.1920928955078125e-07
CurrentTrain: epoch  8, batch     6 | loss: 2.6028242Losses:  1.8892021179199219 0.40307146310806274
CurrentTrain: epoch  9, batch     0 | loss: 2.2922735Losses:  1.8148421049118042 0.4108821153640747
CurrentTrain: epoch  9, batch     1 | loss: 2.2257242Losses:  1.846951961517334 0.4523763358592987
CurrentTrain: epoch  9, batch     2 | loss: 2.2993283Losses:  1.831237554550171 0.4663040041923523
CurrentTrain: epoch  9, batch     3 | loss: 2.2975416Losses:  1.8818564414978027 0.5040258169174194
CurrentTrain: epoch  9, batch     4 | loss: 2.3858824Losses:  1.7364904880523682 0.418957382440567
CurrentTrain: epoch  9, batch     5 | loss: 2.1554480Losses:  1.8580783605575562 0.07063759118318558
CurrentTrain: epoch  9, batch     6 | loss: 1.9287159
Losses:  5.681024551391602 0.6002238392829895
MemoryTrain:  epoch  0, batch     0 | loss: 6.2812486Losses:  8.493547439575195 0.5853528380393982
MemoryTrain:  epoch  0, batch     1 | loss: 9.0789003Losses:  9.929167747497559 0.305754154920578
MemoryTrain:  epoch  0, batch     2 | loss: 10.2349215Losses:  10.433452606201172 0.5151780843734741
MemoryTrain:  epoch  0, batch     3 | loss: 10.9486303Losses:  10.356966972351074 0.4894616901874542
MemoryTrain:  epoch  0, batch     4 | loss: 10.8464289Losses:  1.7579723596572876 0.49175313115119934
MemoryTrain:  epoch  1, batch     0 | loss: 2.2497256Losses:  0.7885010838508606 0.46103379130363464
MemoryTrain:  epoch  1, batch     1 | loss: 1.2495348Losses:  1.0675839185714722 0.4387909770011902
MemoryTrain:  epoch  1, batch     2 | loss: 1.5063748Losses:  0.6010989546775818 0.4372463822364807
MemoryTrain:  epoch  1, batch     3 | loss: 1.0383453Losses:  0.5504728555679321 0.5938684940338135
MemoryTrain:  epoch  1, batch     4 | loss: 1.1443413Losses:  0.7792927622795105 0.31026843190193176
MemoryTrain:  epoch  2, batch     0 | loss: 1.0895612Losses:  0.760596752166748 0.5461821556091309
MemoryTrain:  epoch  2, batch     1 | loss: 1.3067789Losses:  0.8782711029052734 0.5948736071586609
MemoryTrain:  epoch  2, batch     2 | loss: 1.4731448Losses:  0.8387396335601807 0.5415806770324707
MemoryTrain:  epoch  2, batch     3 | loss: 1.3803203Losses:  0.46039220690727234 0.44914597272872925
MemoryTrain:  epoch  2, batch     4 | loss: 0.9095381Losses:  0.7651654481887817 0.4744302034378052
MemoryTrain:  epoch  3, batch     0 | loss: 1.2395957Losses:  0.5785297155380249 0.34289300441741943
MemoryTrain:  epoch  3, batch     1 | loss: 0.9214227Losses:  0.3625977039337158 0.4091660976409912
MemoryTrain:  epoch  3, batch     2 | loss: 0.7717638Losses:  0.530860960483551 0.4537736773490906
MemoryTrain:  epoch  3, batch     3 | loss: 0.9846346Losses:  0.6617244482040405 0.5461207628250122
MemoryTrain:  epoch  3, batch     4 | loss: 1.2078452Losses:  0.8653905391693115 0.4612433910369873
MemoryTrain:  epoch  4, batch     0 | loss: 1.3266339Losses:  0.3332158923149109 0.5787156820297241
MemoryTrain:  epoch  4, batch     1 | loss: 0.9119316Losses:  0.47234904766082764 0.37630391120910645
MemoryTrain:  epoch  4, batch     2 | loss: 0.8486530Losses:  0.24235127866268158 0.3499552607536316
MemoryTrain:  epoch  4, batch     3 | loss: 0.5923066Losses:  0.3237345218658447 0.5720696449279785
MemoryTrain:  epoch  4, batch     4 | loss: 0.8958042Losses:  0.48664164543151855 0.18031203746795654
MemoryTrain:  epoch  5, batch     0 | loss: 0.6669537Losses:  0.7771861553192139 0.5741048455238342
MemoryTrain:  epoch  5, batch     1 | loss: 1.3512909Losses:  0.2947080135345459 0.46690237522125244
MemoryTrain:  epoch  5, batch     2 | loss: 0.7616104Losses:  0.5404723882675171 0.5261761546134949
MemoryTrain:  epoch  5, batch     3 | loss: 1.0666485Losses:  0.30297625064849854 0.39621222019195557
MemoryTrain:  epoch  5, batch     4 | loss: 0.6991885Losses:  0.2308560609817505 0.5146905779838562
MemoryTrain:  epoch  6, batch     0 | loss: 0.7455466Losses:  0.44816288352012634 0.4788038730621338
MemoryTrain:  epoch  6, batch     1 | loss: 0.9269668Losses:  0.4716835618019104 0.5315518379211426
MemoryTrain:  epoch  6, batch     2 | loss: 1.0032353Losses:  0.2757379412651062 0.3206312954425812
MemoryTrain:  epoch  6, batch     3 | loss: 0.5963693Losses:  0.24128732085227966 0.3338840901851654
MemoryTrain:  epoch  6, batch     4 | loss: 0.5751714Losses:  0.3709120452404022 0.5679084062576294
MemoryTrain:  epoch  7, batch     0 | loss: 0.9388205Losses:  0.313244104385376 0.42758429050445557
MemoryTrain:  epoch  7, batch     1 | loss: 0.7408284Losses:  0.22515442967414856 0.3080747127532959
MemoryTrain:  epoch  7, batch     2 | loss: 0.5332291Losses:  0.20150873064994812 0.28997403383255005
MemoryTrain:  epoch  7, batch     3 | loss: 0.4914828Losses:  0.22422319650650024 0.4239078164100647
MemoryTrain:  epoch  7, batch     4 | loss: 0.6481310Losses:  0.3141942620277405 0.4470673203468323
MemoryTrain:  epoch  8, batch     0 | loss: 0.7612616Losses:  0.3811812996864319 0.5082055330276489
MemoryTrain:  epoch  8, batch     1 | loss: 0.8893868Losses:  0.22724324464797974 0.3096729516983032
MemoryTrain:  epoch  8, batch     2 | loss: 0.5369162Losses:  0.21580979228019714 0.32617512345314026
MemoryTrain:  epoch  8, batch     3 | loss: 0.5419849Losses:  0.249740332365036 0.4051585793495178
MemoryTrain:  epoch  8, batch     4 | loss: 0.6548989Losses:  0.2058367133140564 0.3482273817062378
MemoryTrain:  epoch  9, batch     0 | loss: 0.5540641Losses:  0.20920386910438538 0.5093591213226318
MemoryTrain:  epoch  9, batch     1 | loss: 0.7185630Losses:  0.2074362337589264 0.42140111327171326
MemoryTrain:  epoch  9, batch     2 | loss: 0.6288373Losses:  0.2865976095199585 0.5189157128334045
MemoryTrain:  epoch  9, batch     3 | loss: 0.8055133Losses:  0.2698630392551422 0.4923659563064575
MemoryTrain:  epoch  9, batch     4 | loss: 0.7622290
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 77.04%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.06%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 87.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.40%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.60%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 87.40%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.41%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 87.31%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 87.23%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 87.06%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 86.82%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 86.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 86.35%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 86.12%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 85.82%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 85.28%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 84.65%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 83.92%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 82.98%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 81.99%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 81.10%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 80.23%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 79.31%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 80.33%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 80.29%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 80.18%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 79.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 82.02%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 82.02%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 82.26%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.31%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 82.86%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 82.49%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 81.99%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 81.59%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 81.30%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 80.96%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 80.63%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 80.64%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.02%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:  142 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 81.47%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 81.34%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 81.04%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 80.99%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 80.91%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 80.66%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 80.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 80.05%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 79.56%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 79.04%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 78.53%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 78.10%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 77.60%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 77.51%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 77.67%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 78.59%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 78.20%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 77.74%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 77.36%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 76.95%   [EVAL] batch:  173 | acc: 12.50%,  total acc: 76.58%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 76.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 76.17%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 76.26%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 76.24%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 76.09%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 75.92%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 75.81%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 75.60%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 75.39%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 75.36%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 75.29%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 75.26%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 75.03%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 75.25%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 75.37%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 75.55%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 76.08%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:  216 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  217 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 75.91%   [EVAL] batch:  220 | acc: 50.00%,  total acc: 75.79%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 75.70%   [EVAL] batch:  222 | acc: 43.75%,  total acc: 75.56%   [EVAL] batch:  223 | acc: 50.00%,  total acc: 75.45%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 75.31%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 75.36%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 75.56%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 75.69%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 75.79%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 76.98%   
cur_acc:  ['0.9464', '0.8006', '0.6657', '0.8095']
his_acc:  ['0.9464', '0.8675', '0.7819', '0.7698']
Clustering into  24  clusters
Clusters:  [ 0  1 17  2  0  0 21  0 18 23 12 11  2  0 19 14 15  0  0 22  8  2  0  0
  0 20  0  0  0  0  1 13  2  0  0  6  0  0 10  9 16  0  4  0  5  7  3  0
  0  0]
Losses:  7.418510437011719 1.1350619792938232
CurrentTrain: epoch  0, batch     0 | loss: 8.5535727Losses:  10.351264953613281 0.6557856798171997
CurrentTrain: epoch  0, batch     1 | loss: 11.0070505Losses:  7.8176727294921875 1.1871836185455322
CurrentTrain: epoch  0, batch     2 | loss: 9.0048561Losses:  9.174928665161133 1.2814626693725586
CurrentTrain: epoch  0, batch     3 | loss: 10.4563913Losses:  9.974575996398926 0.9358301162719727
CurrentTrain: epoch  0, batch     4 | loss: 10.9104061Losses:  5.916232109069824 0.993028998374939
CurrentTrain: epoch  0, batch     5 | loss: 6.9092612Losses:  5.583120346069336 0.23847723007202148
CurrentTrain: epoch  0, batch     6 | loss: 5.8215976Losses:  5.322164058685303 0.8516466617584229
CurrentTrain: epoch  1, batch     0 | loss: 6.1738110Losses:  3.7613260746002197 1.0314443111419678
CurrentTrain: epoch  1, batch     1 | loss: 4.7927704Losses:  2.870323896408081 1.0205978155136108
CurrentTrain: epoch  1, batch     2 | loss: 3.8909216Losses:  3.6340255737304688 0.9046745896339417
CurrentTrain: epoch  1, batch     3 | loss: 4.5387001Losses:  3.6536967754364014 1.1308953762054443
CurrentTrain: epoch  1, batch     4 | loss: 4.7845922Losses:  4.528533935546875 1.255723237991333
CurrentTrain: epoch  1, batch     5 | loss: 5.7842569Losses:  2.5472145080566406 0.21294543147087097
CurrentTrain: epoch  1, batch     6 | loss: 2.7601600Losses:  3.367070436477661 0.967232346534729
CurrentTrain: epoch  2, batch     0 | loss: 4.3343029Losses:  3.662337303161621 1.1908282041549683
CurrentTrain: epoch  2, batch     1 | loss: 4.8531656Losses:  4.758561134338379 0.8493492603302002
CurrentTrain: epoch  2, batch     2 | loss: 5.6079102Losses:  2.973825216293335 0.8591967225074768
CurrentTrain: epoch  2, batch     3 | loss: 3.8330219Losses:  3.0919277667999268 0.8141533136367798
CurrentTrain: epoch  2, batch     4 | loss: 3.9060812Losses:  3.3623600006103516 0.7121654152870178
CurrentTrain: epoch  2, batch     5 | loss: 4.0745254Losses:  2.0074195861816406 0.22696124017238617
CurrentTrain: epoch  2, batch     6 | loss: 2.2343807Losses:  2.086616039276123 0.609550416469574
CurrentTrain: epoch  3, batch     0 | loss: 2.6961665Losses:  3.426734447479248 0.548845112323761
CurrentTrain: epoch  3, batch     1 | loss: 3.9755795Losses:  3.220668315887451 1.0118939876556396
CurrentTrain: epoch  3, batch     2 | loss: 4.2325621Losses:  3.494335174560547 0.8860024213790894
CurrentTrain: epoch  3, batch     3 | loss: 4.3803377Losses:  4.1904616355896 0.9126558303833008
CurrentTrain: epoch  3, batch     4 | loss: 5.1031175Losses:  2.672642230987549 1.023727297782898
CurrentTrain: epoch  3, batch     5 | loss: 3.6963696Losses:  3.4649744033813477 0.8027015924453735
CurrentTrain: epoch  3, batch     6 | loss: 4.2676759Losses:  3.05983304977417 0.6198375821113586
CurrentTrain: epoch  4, batch     0 | loss: 3.6796706Losses:  3.9399495124816895 0.8962885141372681
CurrentTrain: epoch  4, batch     1 | loss: 4.8362379Losses:  2.2055513858795166 0.7002352476119995
CurrentTrain: epoch  4, batch     2 | loss: 2.9057865Losses:  2.970053195953369 0.7538721561431885
CurrentTrain: epoch  4, batch     3 | loss: 3.7239254Losses:  2.795516014099121 0.9513016939163208
CurrentTrain: epoch  4, batch     4 | loss: 3.7468176Losses:  2.9818711280822754 0.8447108268737793
CurrentTrain: epoch  4, batch     5 | loss: 3.8265820Losses:  3.297210693359375 0.059641700237989426
CurrentTrain: epoch  4, batch     6 | loss: 3.3568523Losses:  3.9312517642974854 0.8730221390724182
CurrentTrain: epoch  5, batch     0 | loss: 4.8042741Losses:  2.8140525817871094 0.6456562280654907
CurrentTrain: epoch  5, batch     1 | loss: 3.4597087Losses:  2.806143045425415 0.6560586094856262
CurrentTrain: epoch  5, batch     2 | loss: 3.4622016Losses:  2.3737974166870117 0.67319655418396
CurrentTrain: epoch  5, batch     3 | loss: 3.0469940Losses:  1.8392919301986694 0.43480604887008667
CurrentTrain: epoch  5, batch     4 | loss: 2.2740979Losses:  2.3610596656799316 0.6990103125572205
CurrentTrain: epoch  5, batch     5 | loss: 3.0600700Losses:  4.0171661376953125 0.2406114935874939
CurrentTrain: epoch  5, batch     6 | loss: 4.2577777Losses:  2.3661465644836426 0.4671395421028137
CurrentTrain: epoch  6, batch     0 | loss: 2.8332860Losses:  2.2798919677734375 0.5810306072235107
CurrentTrain: epoch  6, batch     1 | loss: 2.8609226Losses:  2.612069606781006 0.5802431702613831
CurrentTrain: epoch  6, batch     2 | loss: 3.1923127Losses:  1.9526766538619995 0.4342508018016815
CurrentTrain: epoch  6, batch     3 | loss: 2.3869274Losses:  2.3355462551116943 0.6155506372451782
CurrentTrain: epoch  6, batch     4 | loss: 2.9510970Losses:  3.0614213943481445 0.7694714665412903
CurrentTrain: epoch  6, batch     5 | loss: 3.8308928Losses:  2.2426199913024902 0.12140616774559021
CurrentTrain: epoch  6, batch     6 | loss: 2.3640261Losses:  2.501376152038574 0.745336651802063
CurrentTrain: epoch  7, batch     0 | loss: 3.2467127Losses:  1.865882158279419 0.44606924057006836
CurrentTrain: epoch  7, batch     1 | loss: 2.3119514Losses:  3.023289680480957 0.6428171396255493
CurrentTrain: epoch  7, batch     2 | loss: 3.6661067Losses:  2.2123842239379883 0.6546527743339539
CurrentTrain: epoch  7, batch     3 | loss: 2.8670371Losses:  2.174121379852295 0.5255972743034363
CurrentTrain: epoch  7, batch     4 | loss: 2.6997187Losses:  2.474653720855713 0.6346920728683472
CurrentTrain: epoch  7, batch     5 | loss: 3.1093459Losses:  1.7274549007415771 0.08160671591758728
CurrentTrain: epoch  7, batch     6 | loss: 1.8090616Losses:  2.5791430473327637 0.6945881843566895
CurrentTrain: epoch  8, batch     0 | loss: 3.2737312Losses:  1.8856773376464844 0.2716398239135742
CurrentTrain: epoch  8, batch     1 | loss: 2.1573172Losses:  2.1294684410095215 0.34101006388664246
CurrentTrain: epoch  8, batch     2 | loss: 2.4704785Losses:  1.8417067527770996 0.4014989137649536
CurrentTrain: epoch  8, batch     3 | loss: 2.2432055Losses:  2.277402400970459 0.6007272005081177
CurrentTrain: epoch  8, batch     4 | loss: 2.8781295Losses:  2.321751594543457 0.42297619581222534
CurrentTrain: epoch  8, batch     5 | loss: 2.7447278Losses:  2.162144899368286 0.07105895131826401
CurrentTrain: epoch  8, batch     6 | loss: 2.2332039Losses:  1.9621374607086182 0.23952434957027435
CurrentTrain: epoch  9, batch     0 | loss: 2.2016618Losses:  1.757172703742981 0.3731822073459625
CurrentTrain: epoch  9, batch     1 | loss: 2.1303549Losses:  2.222757339477539 0.38605037331581116
CurrentTrain: epoch  9, batch     2 | loss: 2.6088078Losses:  2.0487380027770996 0.4005235433578491
CurrentTrain: epoch  9, batch     3 | loss: 2.4492617Losses:  2.0425705909729004 0.5243932604789734
CurrentTrain: epoch  9, batch     4 | loss: 2.5669639Losses:  1.9180288314819336 0.458084374666214
CurrentTrain: epoch  9, batch     5 | loss: 2.3761132Losses:  2.30928373336792 0.15475447475910187
CurrentTrain: epoch  9, batch     6 | loss: 2.4640381
Losses:  5.545974254608154 0.33013930916786194
MemoryTrain:  epoch  0, batch     0 | loss: 5.8761134Losses:  8.00019645690918 0.3384286165237427
MemoryTrain:  epoch  0, batch     1 | loss: 8.3386250Losses:  8.697896957397461 0.5409752130508423
MemoryTrain:  epoch  0, batch     2 | loss: 9.2388725Losses:  9.123392105102539 0.5096274614334106
MemoryTrain:  epoch  0, batch     3 | loss: 9.6330194Losses:  10.429553985595703 0.4044388234615326
MemoryTrain:  epoch  0, batch     4 | loss: 10.8339930Losses:  10.442721366882324 0.6145380735397339
MemoryTrain:  epoch  0, batch     5 | loss: 11.0572596Losses:  13.306859970092773 0.08763963729143143
MemoryTrain:  epoch  0, batch     6 | loss: 13.3944998Losses:  1.1771793365478516 0.44378358125686646
MemoryTrain:  epoch  1, batch     0 | loss: 1.6209629Losses:  0.7718873620033264 0.36764252185821533
MemoryTrain:  epoch  1, batch     1 | loss: 1.1395299Losses:  0.6904429197311401 0.5453983545303345
MemoryTrain:  epoch  1, batch     2 | loss: 1.2358413Losses:  0.4730694591999054 0.30971625447273254
MemoryTrain:  epoch  1, batch     3 | loss: 0.7827857Losses:  1.068575143814087 0.5402131676673889
MemoryTrain:  epoch  1, batch     4 | loss: 1.6087883Losses:  0.7968648672103882 0.5316461324691772
MemoryTrain:  epoch  1, batch     5 | loss: 1.3285110Losses:  0.2761097848415375 0.04550011456012726
MemoryTrain:  epoch  1, batch     6 | loss: 0.3216099Losses:  0.7696672081947327 0.4378010630607605
MemoryTrain:  epoch  2, batch     0 | loss: 1.2074683Losses:  0.6611541509628296 0.5803234577178955
MemoryTrain:  epoch  2, batch     1 | loss: 1.2414776Losses:  0.4850029945373535 0.2654743790626526
MemoryTrain:  epoch  2, batch     2 | loss: 0.7504774Losses:  0.596219539642334 0.743902325630188
MemoryTrain:  epoch  2, batch     3 | loss: 1.3401219Losses:  0.5436803102493286 0.43425148725509644
MemoryTrain:  epoch  2, batch     4 | loss: 0.9779318Losses:  0.36516764760017395 0.27946406602859497
MemoryTrain:  epoch  2, batch     5 | loss: 0.6446317Losses:  1.1088221073150635 0.16252459585666656
MemoryTrain:  epoch  2, batch     6 | loss: 1.2713467Losses:  0.49063611030578613 0.5045232176780701
MemoryTrain:  epoch  3, batch     0 | loss: 0.9951593Losses:  0.5697641372680664 0.38972195982933044
MemoryTrain:  epoch  3, batch     1 | loss: 0.9594861Losses:  0.7210656404495239 0.5425980091094971
MemoryTrain:  epoch  3, batch     2 | loss: 1.2636636Losses:  0.36424729228019714 0.4490680992603302
MemoryTrain:  epoch  3, batch     3 | loss: 0.8133154Losses:  0.35078874230384827 0.6018195748329163
MemoryTrain:  epoch  3, batch     4 | loss: 0.9526083Losses:  0.43125367164611816 0.3765789568424225
MemoryTrain:  epoch  3, batch     5 | loss: 0.8078326Losses:  0.42557451128959656 0.0658622607588768
MemoryTrain:  epoch  3, batch     6 | loss: 0.4914368Losses:  0.27798497676849365 0.3543933033943176
MemoryTrain:  epoch  4, batch     0 | loss: 0.6323783Losses:  0.41491812467575073 0.5722855925559998
MemoryTrain:  epoch  4, batch     1 | loss: 0.9872037Losses:  0.3306546211242676 0.30451253056526184
MemoryTrain:  epoch  4, batch     2 | loss: 0.6351671Losses:  0.3713682293891907 0.3773157000541687
MemoryTrain:  epoch  4, batch     3 | loss: 0.7486839Losses:  0.3630262613296509 0.40216511487960815
MemoryTrain:  epoch  4, batch     4 | loss: 0.7651914Losses:  0.5247893929481506 0.5942989587783813
MemoryTrain:  epoch  4, batch     5 | loss: 1.1190884Losses:  0.4284431040287018 0.117694191634655
MemoryTrain:  epoch  4, batch     6 | loss: 0.5461373Losses:  0.5112197399139404 0.4204629957675934
MemoryTrain:  epoch  5, batch     0 | loss: 0.9316827Losses:  0.3156161308288574 0.323533296585083
MemoryTrain:  epoch  5, batch     1 | loss: 0.6391494Losses:  0.234154611825943 0.29033222794532776
MemoryTrain:  epoch  5, batch     2 | loss: 0.5244868Losses:  0.3910456597805023 0.3827911615371704
MemoryTrain:  epoch  5, batch     3 | loss: 0.7738369Losses:  0.3031814396381378 0.2952629029750824
MemoryTrain:  epoch  5, batch     4 | loss: 0.5984443Losses:  0.5469533205032349 0.6443547010421753
MemoryTrain:  epoch  5, batch     5 | loss: 1.1913080Losses:  0.2501612901687622 0.036065176129341125
MemoryTrain:  epoch  5, batch     6 | loss: 0.2862265Losses:  0.3773040771484375 0.5280055999755859
MemoryTrain:  epoch  6, batch     0 | loss: 0.9053097Losses:  0.3332529366016388 0.47950536012649536
MemoryTrain:  epoch  6, batch     1 | loss: 0.8127583Losses:  0.37771177291870117 0.46581628918647766
MemoryTrain:  epoch  6, batch     2 | loss: 0.8435280Losses:  0.2892805337905884 0.33713316917419434
MemoryTrain:  epoch  6, batch     3 | loss: 0.6264137Losses:  0.3428426682949066 0.3899106979370117
MemoryTrain:  epoch  6, batch     4 | loss: 0.7327534Losses:  0.31781864166259766 0.3289492130279541
MemoryTrain:  epoch  6, batch     5 | loss: 0.6467679Losses:  0.3380342125892639 0.06992378830909729
MemoryTrain:  epoch  6, batch     6 | loss: 0.4079580Losses:  0.270846426486969 0.42857375741004944
MemoryTrain:  epoch  7, batch     0 | loss: 0.6994202Losses:  0.3948689103126526 0.44613611698150635
MemoryTrain:  epoch  7, batch     1 | loss: 0.8410050Losses:  0.3860248923301697 0.4197596311569214
MemoryTrain:  epoch  7, batch     2 | loss: 0.8057845Losses:  0.41057950258255005 0.4095555245876312
MemoryTrain:  epoch  7, batch     3 | loss: 0.8201350Losses:  0.2375936061143875 0.3733595907688141
MemoryTrain:  epoch  7, batch     4 | loss: 0.6109532Losses:  0.2905231714248657 0.34612172842025757
MemoryTrain:  epoch  7, batch     5 | loss: 0.6366449Losses:  0.20163017511367798 0.024060411378741264
MemoryTrain:  epoch  7, batch     6 | loss: 0.2256906Losses:  0.39012953639030457 0.4675127863883972
MemoryTrain:  epoch  8, batch     0 | loss: 0.8576423Losses:  0.3305741250514984 0.3654620945453644
MemoryTrain:  epoch  8, batch     1 | loss: 0.6960362Losses:  0.26798850297927856 0.2824540138244629
MemoryTrain:  epoch  8, batch     2 | loss: 0.5504425Losses:  0.23172450065612793 0.27467888593673706
MemoryTrain:  epoch  8, batch     3 | loss: 0.5064034Losses:  0.3137800097465515 0.4980255365371704
MemoryTrain:  epoch  8, batch     4 | loss: 0.8118055Losses:  0.34670063853263855 0.45983028411865234
MemoryTrain:  epoch  8, batch     5 | loss: 0.8065310Losses:  0.36548179388046265 0.054833125323057175
MemoryTrain:  epoch  8, batch     6 | loss: 0.4203149Losses:  0.2982413172721863 0.43387091159820557
MemoryTrain:  epoch  9, batch     0 | loss: 0.7321122Losses:  0.31462305784225464 0.40967631340026855
MemoryTrain:  epoch  9, batch     1 | loss: 0.7242994Losses:  0.2513786852359772 0.24753394722938538
MemoryTrain:  epoch  9, batch     2 | loss: 0.4989126Losses:  0.31787431240081787 0.486755907535553
MemoryTrain:  epoch  9, batch     3 | loss: 0.8046302Losses:  0.23814791440963745 0.3982190489768982
MemoryTrain:  epoch  9, batch     4 | loss: 0.6363670Losses:  0.30307069420814514 0.4020809531211853
MemoryTrain:  epoch  9, batch     5 | loss: 0.7051517Losses:  0.4704127311706543 0.05276571214199066
MemoryTrain:  epoch  9, batch     6 | loss: 0.5231785
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 77.31%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 77.96%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 77.19%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.09%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.13%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.83%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 86.73%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 86.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.09%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.30%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 87.30%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 87.21%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 87.23%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 86.88%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 86.18%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 85.85%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 85.79%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 85.47%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 85.17%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 84.66%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 83.78%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 83.44%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 83.02%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 82.32%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 81.33%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 80.36%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 79.49%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 78.63%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 77.73%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 77.49%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 78.95%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 79.04%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 79.15%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 79.10%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 78.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.01%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 80.20%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 81.03%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 80.81%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 80.61%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.84%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 80.36%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 79.98%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 79.70%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 79.42%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 79.10%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 79.07%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 79.14%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 79.48%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 79.54%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 79.60%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:  142 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 79.70%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 79.37%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 79.29%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 79.18%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 78.94%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.35%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.88%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 77.37%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 76.87%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 76.45%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 75.96%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 75.88%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 76.02%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 76.27%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 76.37%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 76.54%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 76.10%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 75.73%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 75.36%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 75.04%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 74.68%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 74.47%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 74.41%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 74.24%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 74.15%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 73.89%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 73.83%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 73.74%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.45%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 73.24%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 73.26%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 73.27%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 73.22%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 73.14%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 73.71%   [EVAL] batch:  214 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:  216 | acc: 50.00%,  total acc: 73.47%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 73.42%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 73.13%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 73.00%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 72.79%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 72.52%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 72.33%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 72.36%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 72.46%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 72.39%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 72.25%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 72.20%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 73.53%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 73.33%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 73.27%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.28%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 73.22%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 73.23%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 73.42%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 73.68%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 73.64%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 73.58%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 74.48%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 74.44%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 74.36%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 74.34%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 74.16%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 74.10%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.10%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 74.02%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 74.26%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  306 | acc: 75.00%,  total acc: 74.39%   [EVAL] batch:  307 | acc: 68.75%,  total acc: 74.37%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  309 | acc: 43.75%,  total acc: 74.27%   [EVAL] batch:  310 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:  311 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 74.08%   
cur_acc:  ['0.9464', '0.8006', '0.6657', '0.8095', '0.7609']
his_acc:  ['0.9464', '0.8675', '0.7819', '0.7698', '0.7408']
Clustering into  29  clusters
Clusters:  [ 0  2 19  0  0  0 24  0 20 15 25 23  0  0 11 27  9  0  0 26 22  0  0  0
  0 17  0  0  0  0  2 21  0  0  0  1  0 28 10 12 18  0 14  0 16  8 13  0
  0  0  1  4  0  6  7  0  0  0  5  3]
Losses:  7.341886520385742 1.2105128765106201
CurrentTrain: epoch  0, batch     0 | loss: 8.5523996Losses:  8.868663787841797 1.0753958225250244
CurrentTrain: epoch  0, batch     1 | loss: 9.9440594Losses:  9.391005516052246 0.9828630685806274
CurrentTrain: epoch  0, batch     2 | loss: 10.3738689Losses:  9.490787506103516 1.4268898963928223
CurrentTrain: epoch  0, batch     3 | loss: 10.9176769Losses:  7.69730806350708 0.8883443474769592
CurrentTrain: epoch  0, batch     4 | loss: 8.5856524Losses:  5.919253826141357 1.0911383628845215
CurrentTrain: epoch  0, batch     5 | loss: 7.0103922Losses:  4.088150978088379 0.44260406494140625
CurrentTrain: epoch  0, batch     6 | loss: 4.5307550Losses:  5.078702449798584 1.2728995084762573
CurrentTrain: epoch  1, batch     0 | loss: 6.3516021Losses:  3.701103687286377 0.9577890634536743
CurrentTrain: epoch  1, batch     1 | loss: 4.6588926Losses:  4.026415824890137 1.1675288677215576
CurrentTrain: epoch  1, batch     2 | loss: 5.1939449Losses:  3.323904037475586 0.9942129850387573
CurrentTrain: epoch  1, batch     3 | loss: 4.3181171Losses:  4.376623630523682 0.7690767049789429
CurrentTrain: epoch  1, batch     4 | loss: 5.1457005Losses:  3.8517165184020996 0.8757473826408386
CurrentTrain: epoch  1, batch     5 | loss: 4.7274637Losses:  1.8693313598632812 5.960464477539063e-08
CurrentTrain: epoch  1, batch     6 | loss: 1.8693314Losses:  4.080418586730957 1.0862418413162231
CurrentTrain: epoch  2, batch     0 | loss: 5.1666603Losses:  3.4697322845458984 0.4691983461380005
CurrentTrain: epoch  2, batch     1 | loss: 3.9389305Losses:  4.268665790557861 1.3051185607910156
CurrentTrain: epoch  2, batch     2 | loss: 5.5737844Losses:  2.879819869995117 0.6942388415336609
CurrentTrain: epoch  2, batch     3 | loss: 3.5740588Losses:  4.445821762084961 0.9715646505355835
CurrentTrain: epoch  2, batch     4 | loss: 5.4173865Losses:  2.4155564308166504 0.4590182900428772
CurrentTrain: epoch  2, batch     5 | loss: 2.8745747Losses:  2.319187641143799 0.28807586431503296
CurrentTrain: epoch  2, batch     6 | loss: 2.6072636Losses:  3.8405816555023193 0.8715578317642212
CurrentTrain: epoch  3, batch     0 | loss: 4.7121396Losses:  2.4458887577056885 0.5491493344306946
CurrentTrain: epoch  3, batch     1 | loss: 2.9950380Losses:  2.912889003753662 0.8948876261711121
CurrentTrain: epoch  3, batch     2 | loss: 3.8077767Losses:  3.6992714405059814 1.040297031402588
CurrentTrain: epoch  3, batch     3 | loss: 4.7395687Losses:  2.692842721939087 0.8031798601150513
CurrentTrain: epoch  3, batch     4 | loss: 3.4960227Losses:  2.9988105297088623 0.8504313230514526
CurrentTrain: epoch  3, batch     5 | loss: 3.8492417Losses:  1.8296864032745361 0.12055007368326187
CurrentTrain: epoch  3, batch     6 | loss: 1.9502364Losses:  2.6675233840942383 0.7453928589820862
CurrentTrain: epoch  4, batch     0 | loss: 3.4129162Losses:  3.0264346599578857 0.9351722002029419
CurrentTrain: epoch  4, batch     1 | loss: 3.9616070Losses:  2.9662046432495117 0.5840798616409302
CurrentTrain: epoch  4, batch     2 | loss: 3.5502844Losses:  2.4001669883728027 0.6864138841629028
CurrentTrain: epoch  4, batch     3 | loss: 3.0865808Losses:  3.065410852432251 0.7107127904891968
CurrentTrain: epoch  4, batch     4 | loss: 3.7761235Losses:  2.5177230834960938 0.6037404537200928
CurrentTrain: epoch  4, batch     5 | loss: 3.1214635Losses:  2.2230751514434814 0.450674444437027
CurrentTrain: epoch  4, batch     6 | loss: 2.6737497Losses:  3.1223597526550293 0.7698395848274231
CurrentTrain: epoch  5, batch     0 | loss: 3.8921993Losses:  2.6009507179260254 0.8275644779205322
CurrentTrain: epoch  5, batch     1 | loss: 3.4285152Losses:  2.293544054031372 0.6060154438018799
CurrentTrain: epoch  5, batch     2 | loss: 2.8995595Losses:  2.5825181007385254 0.4870695471763611
CurrentTrain: epoch  5, batch     3 | loss: 3.0695877Losses:  2.2517902851104736 0.5619069337844849
CurrentTrain: epoch  5, batch     4 | loss: 2.8136973Losses:  2.4390292167663574 0.6195635199546814
CurrentTrain: epoch  5, batch     5 | loss: 3.0585928Losses:  1.7672955989837646 0.06368391215801239
CurrentTrain: epoch  5, batch     6 | loss: 1.8309795Losses:  3.075087547302246 0.850954532623291
CurrentTrain: epoch  6, batch     0 | loss: 3.9260421Losses:  2.463942050933838 0.8208886384963989
CurrentTrain: epoch  6, batch     1 | loss: 3.2848306Losses:  1.9492813348770142 0.48544561862945557
CurrentTrain: epoch  6, batch     2 | loss: 2.4347270Losses:  2.3222274780273438 0.4907853603363037
CurrentTrain: epoch  6, batch     3 | loss: 2.8130128Losses:  2.4188146591186523 0.5269284844398499
CurrentTrain: epoch  6, batch     4 | loss: 2.9457431Losses:  2.1279544830322266 0.39424121379852295
CurrentTrain: epoch  6, batch     5 | loss: 2.5221958Losses:  2.200709342956543 0.21502599120140076
CurrentTrain: epoch  6, batch     6 | loss: 2.4157352Losses:  2.0739152431488037 0.5778136849403381
CurrentTrain: epoch  7, batch     0 | loss: 2.6517289Losses:  2.0199074745178223 0.27941447496414185
CurrentTrain: epoch  7, batch     1 | loss: 2.2993219Losses:  2.7187485694885254 0.4025399088859558
CurrentTrain: epoch  7, batch     2 | loss: 3.1212885Losses:  2.2694942951202393 0.6683988571166992
CurrentTrain: epoch  7, batch     3 | loss: 2.9378932Losses:  2.302772283554077 0.4844207763671875
CurrentTrain: epoch  7, batch     4 | loss: 2.7871931Losses:  2.106861114501953 0.35330355167388916
CurrentTrain: epoch  7, batch     5 | loss: 2.4601645Losses:  1.7339905500411987 0.11094385385513306
CurrentTrain: epoch  7, batch     6 | loss: 1.8449345Losses:  2.1676387786865234 0.6425331830978394
CurrentTrain: epoch  8, batch     0 | loss: 2.8101721Losses:  1.834491491317749 0.3724743723869324
CurrentTrain: epoch  8, batch     1 | loss: 2.2069659Losses:  1.9309487342834473 0.4035455584526062
CurrentTrain: epoch  8, batch     2 | loss: 2.3344944Losses:  2.0294957160949707 0.4940412938594818
CurrentTrain: epoch  8, batch     3 | loss: 2.5235369Losses:  2.318558931350708 0.6627119183540344
CurrentTrain: epoch  8, batch     4 | loss: 2.9812708Losses:  2.2110342979431152 0.625143826007843
CurrentTrain: epoch  8, batch     5 | loss: 2.8361781Losses:  1.7922066450119019 0.1417711079120636
CurrentTrain: epoch  8, batch     6 | loss: 1.9339777Losses:  2.198880672454834 0.5868748426437378
CurrentTrain: epoch  9, batch     0 | loss: 2.7857556Losses:  1.9930706024169922 0.5501772165298462
CurrentTrain: epoch  9, batch     1 | loss: 2.5432477Losses:  1.967576026916504 0.39335259795188904
CurrentTrain: epoch  9, batch     2 | loss: 2.3609285Losses:  1.833327293395996 0.41473132371902466
CurrentTrain: epoch  9, batch     3 | loss: 2.2480586Losses:  2.081467866897583 0.6102160811424255
CurrentTrain: epoch  9, batch     4 | loss: 2.6916840Losses:  1.7768886089324951 0.3102598190307617
CurrentTrain: epoch  9, batch     5 | loss: 2.0871484Losses:  1.7253978252410889 0.07456836849451065
CurrentTrain: epoch  9, batch     6 | loss: 1.7999662
Losses:  5.785706520080566 0.3712545335292816
MemoryTrain:  epoch  0, batch     0 | loss: 6.1569610Losses:  7.876090049743652 0.35443413257598877
MemoryTrain:  epoch  0, batch     1 | loss: 8.2305241Losses:  9.519770622253418 0.37231722474098206
MemoryTrain:  epoch  0, batch     2 | loss: 9.8920879Losses:  9.496651649475098 0.43701112270355225
MemoryTrain:  epoch  0, batch     3 | loss: 9.9336624Losses:  9.888696670532227 0.49296510219573975
MemoryTrain:  epoch  0, batch     4 | loss: 10.3816614Losses:  9.714509963989258 0.6707557439804077
MemoryTrain:  epoch  0, batch     5 | loss: 10.3852654Losses:  10.941544532775879 0.49004581570625305
MemoryTrain:  epoch  0, batch     6 | loss: 11.4315901Losses:  11.646722793579102 0.2647385001182556
MemoryTrain:  epoch  0, batch     7 | loss: 11.9114609Losses:  0.7781249284744263 0.5054233074188232
MemoryTrain:  epoch  1, batch     0 | loss: 1.2835482Losses:  1.0865249633789062 0.35071825981140137
MemoryTrain:  epoch  1, batch     1 | loss: 1.4372432Losses:  1.167488694190979 0.5286729335784912
MemoryTrain:  epoch  1, batch     2 | loss: 1.6961616Losses:  0.9668408632278442 0.3727027475833893
MemoryTrain:  epoch  1, batch     3 | loss: 1.3395436Losses:  0.7402873039245605 0.3909777104854584
MemoryTrain:  epoch  1, batch     4 | loss: 1.1312650Losses:  0.6801937818527222 0.34518012404441833
MemoryTrain:  epoch  1, batch     5 | loss: 1.0253739Losses:  0.6944285035133362 0.3744945526123047
MemoryTrain:  epoch  1, batch     6 | loss: 1.0689230Losses:  0.3546324372291565 0.13955028355121613
MemoryTrain:  epoch  1, batch     7 | loss: 0.4941827Losses:  1.0451762676239014 0.47658079862594604
MemoryTrain:  epoch  2, batch     0 | loss: 1.5217571Losses:  0.5091592669487 0.5476256608963013
MemoryTrain:  epoch  2, batch     1 | loss: 1.0567849Losses:  0.5470201373100281 0.5424764752388
MemoryTrain:  epoch  2, batch     2 | loss: 1.0894966Losses:  0.45113879442214966 0.3531193733215332
MemoryTrain:  epoch  2, batch     3 | loss: 0.8042582Losses:  0.44822996854782104 0.45555755496025085
MemoryTrain:  epoch  2, batch     4 | loss: 0.9037875Losses:  0.9727523326873779 0.49231618642807007
MemoryTrain:  epoch  2, batch     5 | loss: 1.4650686Losses:  0.540345311164856 0.3293434977531433
MemoryTrain:  epoch  2, batch     6 | loss: 0.8696888Losses:  0.37720730900764465 0.1861797720193863
MemoryTrain:  epoch  2, batch     7 | loss: 0.5633871Losses:  0.43736374378204346 0.38874173164367676
MemoryTrain:  epoch  3, batch     0 | loss: 0.8261055Losses:  0.5838016271591187 0.44681793451309204
MemoryTrain:  epoch  3, batch     1 | loss: 1.0306196Losses:  0.43311914801597595 0.277742475271225
MemoryTrain:  epoch  3, batch     2 | loss: 0.7108616Losses:  0.6657728552818298 0.3487224578857422
MemoryTrain:  epoch  3, batch     3 | loss: 1.0144954Losses:  0.4656100869178772 0.4086204767227173
MemoryTrain:  epoch  3, batch     4 | loss: 0.8742306Losses:  0.38367339968681335 0.4793585240840912
MemoryTrain:  epoch  3, batch     5 | loss: 0.8630319Losses:  0.7864041924476624 0.4435039162635803
MemoryTrain:  epoch  3, batch     6 | loss: 1.2299081Losses:  0.4236913025379181 0.2404363453388214
MemoryTrain:  epoch  3, batch     7 | loss: 0.6641276Losses:  0.6031025648117065 0.275380939245224
MemoryTrain:  epoch  4, batch     0 | loss: 0.8784835Losses:  0.3268604874610901 0.4446064531803131
MemoryTrain:  epoch  4, batch     1 | loss: 0.7714670Losses:  0.5872644782066345 0.4072330594062805
MemoryTrain:  epoch  4, batch     2 | loss: 0.9944975Losses:  0.4992590844631195 0.7274692058563232
MemoryTrain:  epoch  4, batch     3 | loss: 1.2267283Losses:  0.3693771958351135 0.27812865376472473
MemoryTrain:  epoch  4, batch     4 | loss: 0.6475059Losses:  0.405538409948349 0.530287504196167
MemoryTrain:  epoch  4, batch     5 | loss: 0.9358259Losses:  0.2889592945575714 0.3419651389122009
MemoryTrain:  epoch  4, batch     6 | loss: 0.6309245Losses:  0.37171751260757446 0.1620628535747528
MemoryTrain:  epoch  4, batch     7 | loss: 0.5337803Losses:  0.5146914720535278 0.3894115388393402
MemoryTrain:  epoch  5, batch     0 | loss: 0.9041030Losses:  0.3538818955421448 0.4021769165992737
MemoryTrain:  epoch  5, batch     1 | loss: 0.7560588Losses:  0.405911386013031 0.43885400891304016
MemoryTrain:  epoch  5, batch     2 | loss: 0.8447654Losses:  0.35151541233062744 0.2910034954547882
MemoryTrain:  epoch  5, batch     3 | loss: 0.6425189Losses:  0.47470587491989136 0.45392483472824097
MemoryTrain:  epoch  5, batch     4 | loss: 0.9286307Losses:  0.28907760977745056 0.28683602809906006
MemoryTrain:  epoch  5, batch     5 | loss: 0.5759137Losses:  0.33511894941329956 0.5369736552238464
MemoryTrain:  epoch  5, batch     6 | loss: 0.8720926Losses:  0.6805167198181152 0.2952846884727478
MemoryTrain:  epoch  5, batch     7 | loss: 0.9758014Losses:  0.2924001216888428 0.2532265782356262
MemoryTrain:  epoch  6, batch     0 | loss: 0.5456267Losses:  0.5671712160110474 0.47000306844711304
MemoryTrain:  epoch  6, batch     1 | loss: 1.0371742Losses:  0.30506205558776855 0.5571314692497253
MemoryTrain:  epoch  6, batch     2 | loss: 0.8621935Losses:  0.4044521749019623 0.32222890853881836
MemoryTrain:  epoch  6, batch     3 | loss: 0.7266811Losses:  0.4217967689037323 0.6295680403709412
MemoryTrain:  epoch  6, batch     4 | loss: 1.0513648Losses:  0.30091947317123413 0.2639956772327423
MemoryTrain:  epoch  6, batch     5 | loss: 0.5649152Losses:  0.5474901795387268 0.4007420539855957
MemoryTrain:  epoch  6, batch     6 | loss: 0.9482322Losses:  0.2970237731933594 0.08017659187316895
MemoryTrain:  epoch  6, batch     7 | loss: 0.3772004Losses:  0.43357446789741516 0.37610924243927
MemoryTrain:  epoch  7, batch     0 | loss: 0.8096837Losses:  0.4685879349708557 0.5475715398788452
MemoryTrain:  epoch  7, batch     1 | loss: 1.0161595Losses:  0.3644414246082306 0.3707827627658844
MemoryTrain:  epoch  7, batch     2 | loss: 0.7352242Losses:  0.29562732577323914 0.24955463409423828
MemoryTrain:  epoch  7, batch     3 | loss: 0.5451820Losses:  0.36764732003211975 0.4560253620147705
MemoryTrain:  epoch  7, batch     4 | loss: 0.8236727Losses:  0.30665504932403564 0.36227789521217346
MemoryTrain:  epoch  7, batch     5 | loss: 0.6689329Losses:  0.2984411120414734 0.3185628652572632
MemoryTrain:  epoch  7, batch     6 | loss: 0.6170040Losses:  0.4966689646244049 0.21582242846488953
MemoryTrain:  epoch  7, batch     7 | loss: 0.7124914Losses:  0.369653582572937 0.2796502411365509
MemoryTrain:  epoch  8, batch     0 | loss: 0.6493038Losses:  0.42328375577926636 0.53622967004776
MemoryTrain:  epoch  8, batch     1 | loss: 0.9595134Losses:  0.35458797216415405 0.3718721866607666
MemoryTrain:  epoch  8, batch     2 | loss: 0.7264602Losses:  0.30354535579681396 0.41890034079551697
MemoryTrain:  epoch  8, batch     3 | loss: 0.7224457Losses:  0.4345515966415405 0.4941580891609192
MemoryTrain:  epoch  8, batch     4 | loss: 0.9287097Losses:  0.24695315957069397 0.2863737940788269
MemoryTrain:  epoch  8, batch     5 | loss: 0.5333270Losses:  0.36687344312667847 0.38939279317855835
MemoryTrain:  epoch  8, batch     6 | loss: 0.7562662Losses:  0.29230475425720215 0.28763487935066223
MemoryTrain:  epoch  8, batch     7 | loss: 0.5799396Losses:  0.2923300564289093 0.3218185305595398
MemoryTrain:  epoch  9, batch     0 | loss: 0.6141486Losses:  0.3111458420753479 0.32781535387039185
MemoryTrain:  epoch  9, batch     1 | loss: 0.6389612Losses:  0.3718423545360565 0.4243718385696411
MemoryTrain:  epoch  9, batch     2 | loss: 0.7962142Losses:  0.36929452419281006 0.4252462387084961
MemoryTrain:  epoch  9, batch     3 | loss: 0.7945408Losses:  0.4515751898288727 0.44476380944252014
MemoryTrain:  epoch  9, batch     4 | loss: 0.8963390Losses:  0.2443947196006775 0.2703206241130829
MemoryTrain:  epoch  9, batch     5 | loss: 0.5147153Losses:  0.38339734077453613 0.30208635330200195
MemoryTrain:  epoch  9, batch     6 | loss: 0.6854837Losses:  0.4196624755859375 0.13988091051578522
MemoryTrain:  epoch  9, batch     7 | loss: 0.5595434
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.67%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 70.94%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 71.01%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.59%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.76%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 86.81%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.59%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 85.86%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 84.53%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 83.73%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 84.14%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 83.75%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 83.01%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 82.02%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 81.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 79.51%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.06%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 78.70%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 78.05%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 77.18%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 76.26%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 75.44%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 74.71%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 73.85%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 73.65%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 75.70%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 77.85%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 77.83%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 77.80%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 77.62%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 77.93%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.56%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 77.25%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 77.03%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 76.78%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 76.48%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 76.55%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.77%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 76.80%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 76.90%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 76.58%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.57%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 76.48%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 76.26%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 75.79%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.33%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 74.88%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 74.39%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 73.99%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.52%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 74.45%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 74.16%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 73.87%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 73.48%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 73.24%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 72.93%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 72.69%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 72.71%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 72.35%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 72.33%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 72.07%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 72.11%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 71.80%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 71.81%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 71.74%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 72.43%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 72.25%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 72.12%   [EVAL] batch:  217 | acc: 56.25%,  total acc: 72.05%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 71.82%   [EVAL] batch:  220 | acc: 18.75%,  total acc: 71.58%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 71.05%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 70.56%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.55%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 70.54%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 70.42%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.02%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 69.91%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 71.19%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 71.11%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 71.03%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 71.00%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 71.29%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 71.26%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 72.31%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 72.26%   [EVAL] batch:  292 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 72.28%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 72.14%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 71.96%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 71.73%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 71.67%   [EVAL] batch:  308 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 71.27%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 71.18%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 70.84%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 70.38%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 70.28%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 70.57%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 70.37%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 70.18%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 70.06%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 69.77%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 71.14%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 70.99%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:  354 | acc: 31.25%,  total acc: 70.86%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 70.80%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 70.92%   
cur_acc:  ['0.9464', '0.8006', '0.6657', '0.8095', '0.7609', '0.7014']
his_acc:  ['0.9464', '0.8675', '0.7819', '0.7698', '0.7408', '0.7092']
Clustering into  34  clusters
Clusters:  [ 0  5 22  0  0  0 27  0 23 31 33 28  0  0 26 32 24  0  0 29 15  0  0  0
  0 19  0  0  0  0  5 21  0  0  0  2  0 16 25 13 20  0 17  0 18  9  8  0
  0  0  2 11  0  7 30  0  0  0 10  4  0  0  0 14  0 12  0  6  3  1]
Losses:  6.939218521118164 1.1579201221466064
CurrentTrain: epoch  0, batch     0 | loss: 8.0971384Losses:  8.883337020874023 0.9254526495933533
CurrentTrain: epoch  0, batch     1 | loss: 9.8087893Losses:  8.166999816894531 0.7832674980163574
CurrentTrain: epoch  0, batch     2 | loss: 8.9502678Losses:  10.117880821228027 0.9791983962059021
CurrentTrain: epoch  0, batch     3 | loss: 11.0970793Losses:  7.412553787231445 1.247691035270691
CurrentTrain: epoch  0, batch     4 | loss: 8.6602449Losses:  5.8302388191223145 0.955510139465332
CurrentTrain: epoch  0, batch     5 | loss: 6.7857490Losses:  5.043962478637695 2.9802322387695312e-08
CurrentTrain: epoch  0, batch     6 | loss: 5.0439625Losses:  2.8768928050994873 0.5468882322311401
CurrentTrain: epoch  1, batch     0 | loss: 3.4237809Losses:  3.8614654541015625 0.8173234462738037
CurrentTrain: epoch  1, batch     1 | loss: 4.6787891Losses:  3.7909178733825684 0.9653715491294861
CurrentTrain: epoch  1, batch     2 | loss: 4.7562895Losses:  3.1897382736206055 0.446888267993927
CurrentTrain: epoch  1, batch     3 | loss: 3.6366265Losses:  4.7407379150390625 0.8088923692703247
CurrentTrain: epoch  1, batch     4 | loss: 5.5496302Losses:  3.56260347366333 0.940654456615448
CurrentTrain: epoch  1, batch     5 | loss: 4.5032578Losses:  3.475956916809082 0.08853264153003693
CurrentTrain: epoch  1, batch     6 | loss: 3.5644896Losses:  3.4151573181152344 0.36602503061294556
CurrentTrain: epoch  2, batch     0 | loss: 3.7811823Losses:  3.3286385536193848 0.7651559114456177
CurrentTrain: epoch  2, batch     1 | loss: 4.0937943Losses:  3.184107780456543 0.6341203451156616
CurrentTrain: epoch  2, batch     2 | loss: 3.8182282Losses:  2.8590779304504395 0.37175774574279785
CurrentTrain: epoch  2, batch     3 | loss: 3.2308357Losses:  3.699763059616089 0.5482068657875061
CurrentTrain: epoch  2, batch     4 | loss: 4.2479701Losses:  3.070742607116699 0.5463117361068726
CurrentTrain: epoch  2, batch     5 | loss: 3.6170545Losses:  2.908742904663086 0.03699866309762001
CurrentTrain: epoch  2, batch     6 | loss: 2.9457417Losses:  3.80017352104187 0.6989579796791077
CurrentTrain: epoch  3, batch     0 | loss: 4.4991317Losses:  2.6299538612365723 0.677746593952179
CurrentTrain: epoch  3, batch     1 | loss: 3.3077004Losses:  2.3868536949157715 0.32933565974235535
CurrentTrain: epoch  3, batch     2 | loss: 2.7161894Losses:  3.000950336456299 0.6456977725028992
CurrentTrain: epoch  3, batch     3 | loss: 3.6466482Losses:  2.2011396884918213 0.37622198462486267
CurrentTrain: epoch  3, batch     4 | loss: 2.5773616Losses:  2.505126953125 0.5759592056274414
CurrentTrain: epoch  3, batch     5 | loss: 3.0810862Losses:  2.6868362426757812 8.94069742685133e-08
CurrentTrain: epoch  3, batch     6 | loss: 2.6868362Losses:  2.6734790802001953 0.5266584753990173
CurrentTrain: epoch  4, batch     0 | loss: 3.2001376Losses:  2.5144224166870117 0.544148862361908
CurrentTrain: epoch  4, batch     1 | loss: 3.0585713Losses:  2.324132204055786 0.5492146015167236
CurrentTrain: epoch  4, batch     2 | loss: 2.8733468Losses:  2.3940818309783936 0.49144795536994934
CurrentTrain: epoch  4, batch     3 | loss: 2.8855298Losses:  2.177844762802124 0.3832293152809143
CurrentTrain: epoch  4, batch     4 | loss: 2.5610740Losses:  2.2215261459350586 0.36264854669570923
CurrentTrain: epoch  4, batch     5 | loss: 2.5841746Losses:  1.9914579391479492 8.94069742685133e-08
CurrentTrain: epoch  4, batch     6 | loss: 1.9914581Losses:  2.1429359912872314 0.41684091091156006
CurrentTrain: epoch  5, batch     0 | loss: 2.5597768Losses:  2.307191848754883 0.4723623991012573
CurrentTrain: epoch  5, batch     1 | loss: 2.7795544Losses:  1.9411895275115967 0.494182288646698
CurrentTrain: epoch  5, batch     2 | loss: 2.4353719Losses:  1.8941491842269897 0.3917134404182434
CurrentTrain: epoch  5, batch     3 | loss: 2.2858627Losses:  2.146488904953003 0.4537259638309479
CurrentTrain: epoch  5, batch     4 | loss: 2.6002150Losses:  2.3082547187805176 0.4244815707206726
CurrentTrain: epoch  5, batch     5 | loss: 2.7327363Losses:  2.2370035648345947 0.061356671154499054
CurrentTrain: epoch  5, batch     6 | loss: 2.2983603Losses:  1.8783730268478394 0.3164459466934204
CurrentTrain: epoch  6, batch     0 | loss: 2.1948190Losses:  1.8395793437957764 0.4263584017753601
CurrentTrain: epoch  6, batch     1 | loss: 2.2659378Losses:  1.8716259002685547 0.38657906651496887
CurrentTrain: epoch  6, batch     2 | loss: 2.2582049Losses:  1.9522242546081543 0.23079684376716614
CurrentTrain: epoch  6, batch     3 | loss: 2.1830211Losses:  2.045712947845459 0.17983382940292358
CurrentTrain: epoch  6, batch     4 | loss: 2.2255468Losses:  2.0530357360839844 0.20259223878383636
CurrentTrain: epoch  6, batch     5 | loss: 2.2556279Losses:  1.8231960535049438 0.03583459556102753
CurrentTrain: epoch  6, batch     6 | loss: 1.8590306Losses:  1.8854308128356934 0.32332518696784973
CurrentTrain: epoch  7, batch     0 | loss: 2.2087560Losses:  1.912177324295044 0.35363686084747314
CurrentTrain: epoch  7, batch     1 | loss: 2.2658143Losses:  1.864308476448059 0.3990255296230316
CurrentTrain: epoch  7, batch     2 | loss: 2.2633340Losses:  1.7409690618515015 0.2706299424171448
CurrentTrain: epoch  7, batch     3 | loss: 2.0115991Losses:  1.7812354564666748 0.2547735273838043
CurrentTrain: epoch  7, batch     4 | loss: 2.0360091Losses:  1.782326102256775 0.1657082736492157
CurrentTrain: epoch  7, batch     5 | loss: 1.9480344Losses:  1.7434933185577393 0.023424407467246056
CurrentTrain: epoch  7, batch     6 | loss: 1.7669177Losses:  1.9056248664855957 0.4850884675979614
CurrentTrain: epoch  8, batch     0 | loss: 2.3907132Losses:  1.8350083827972412 0.3014856278896332
CurrentTrain: epoch  8, batch     1 | loss: 2.1364939Losses:  1.7545559406280518 0.2409786731004715
CurrentTrain: epoch  8, batch     2 | loss: 1.9955347Losses:  1.7323970794677734 0.2843935787677765
CurrentTrain: epoch  8, batch     3 | loss: 2.0167906Losses:  1.8473045825958252 0.26728057861328125
CurrentTrain: epoch  8, batch     4 | loss: 2.1145852Losses:  1.7271208763122559 0.3108155429363251
CurrentTrain: epoch  8, batch     5 | loss: 2.0379364Losses:  1.6702359914779663 0.03501342609524727
CurrentTrain: epoch  8, batch     6 | loss: 1.7052494Losses:  1.8351973295211792 0.22725170850753784
CurrentTrain: epoch  9, batch     0 | loss: 2.0624490Losses:  1.7202644348144531 0.22222475707530975
CurrentTrain: epoch  9, batch     1 | loss: 1.9424891Losses:  1.7274179458618164 0.23244443535804749
CurrentTrain: epoch  9, batch     2 | loss: 1.9598624Losses:  1.7525341510772705 0.28270334005355835
CurrentTrain: epoch  9, batch     3 | loss: 2.0352376Losses:  1.7332763671875 0.30648213624954224
CurrentTrain: epoch  9, batch     4 | loss: 2.0397584Losses:  1.704340934753418 0.24183890223503113
CurrentTrain: epoch  9, batch     5 | loss: 1.9461799Losses:  1.6696417331695557 0.03902148827910423
CurrentTrain: epoch  9, batch     6 | loss: 1.7086632
Losses:  5.882336616516113 0.45578888058662415
MemoryTrain:  epoch  0, batch     0 | loss: 6.3381257Losses:  8.246529579162598 0.6803582906723022
MemoryTrain:  epoch  0, batch     1 | loss: 8.9268875Losses:  9.36565113067627 0.425231009721756
MemoryTrain:  epoch  0, batch     2 | loss: 9.7908821Losses:  8.994707107543945 0.26383844017982483
MemoryTrain:  epoch  0, batch     3 | loss: 9.2585459Losses:  9.8995361328125 0.36671915650367737
MemoryTrain:  epoch  0, batch     4 | loss: 10.2662554Losses:  10.30677318572998 0.5236649513244629
MemoryTrain:  epoch  0, batch     5 | loss: 10.8304386Losses:  11.407665252685547 0.3195491135120392
MemoryTrain:  epoch  0, batch     6 | loss: 11.7272148Losses:  10.178143501281738 0.3429170250892639
MemoryTrain:  epoch  0, batch     7 | loss: 10.5210609Losses:  11.70950698852539 0.2556111514568329
MemoryTrain:  epoch  0, batch     8 | loss: 11.9651184Losses:  1.2942490577697754 0.4218946397304535
MemoryTrain:  epoch  1, batch     0 | loss: 1.7161437Losses:  1.6946301460266113 0.5316835641860962
MemoryTrain:  epoch  1, batch     1 | loss: 2.2263136Losses:  1.0537669658660889 0.41989776492118835
MemoryTrain:  epoch  1, batch     2 | loss: 1.4736648Losses:  0.5170750617980957 0.4700795114040375
MemoryTrain:  epoch  1, batch     3 | loss: 0.9871546Losses:  1.1824054718017578 0.3210848569869995
MemoryTrain:  epoch  1, batch     4 | loss: 1.5034903Losses:  0.9600961208343506 0.4792132079601288
MemoryTrain:  epoch  1, batch     5 | loss: 1.4393094Losses:  0.8236970901489258 0.4428918957710266
MemoryTrain:  epoch  1, batch     6 | loss: 1.2665889Losses:  0.6205413341522217 0.32993465662002563
MemoryTrain:  epoch  1, batch     7 | loss: 0.9504760Losses:  1.0487098693847656 0.22213761508464813
MemoryTrain:  epoch  1, batch     8 | loss: 1.2708474Losses:  0.6769863963127136 0.33073651790618896
MemoryTrain:  epoch  2, batch     0 | loss: 1.0077229Losses:  0.8622809052467346 0.3476157784461975
MemoryTrain:  epoch  2, batch     1 | loss: 1.2098967Losses:  0.5328684449195862 0.33107998967170715
MemoryTrain:  epoch  2, batch     2 | loss: 0.8639485Losses:  0.6002341508865356 0.3978261351585388
MemoryTrain:  epoch  2, batch     3 | loss: 0.9980603Losses:  1.680452823638916 0.483613520860672
MemoryTrain:  epoch  2, batch     4 | loss: 2.1640663Losses:  1.2250218391418457 0.4755549132823944
MemoryTrain:  epoch  2, batch     5 | loss: 1.7005768Losses:  0.5567348003387451 0.47480082511901855
MemoryTrain:  epoch  2, batch     6 | loss: 1.0315356Losses:  0.4847220480442047 0.34596800804138184
MemoryTrain:  epoch  2, batch     7 | loss: 0.8306900Losses:  0.663473904132843 0.23737430572509766
MemoryTrain:  epoch  2, batch     8 | loss: 0.9008482Losses:  0.6808897256851196 0.6554853320121765
MemoryTrain:  epoch  3, batch     0 | loss: 1.3363750Losses:  0.6356261968612671 0.3607381284236908
MemoryTrain:  epoch  3, batch     1 | loss: 0.9963644Losses:  0.8118930459022522 0.3221704959869385
MemoryTrain:  epoch  3, batch     2 | loss: 1.1340635Losses:  0.43612101674079895 0.3347300589084625
MemoryTrain:  epoch  3, batch     3 | loss: 0.7708511Losses:  1.3192789554595947 0.6377291679382324
MemoryTrain:  epoch  3, batch     4 | loss: 1.9570081Losses:  0.6107107400894165 0.26212838292121887
MemoryTrain:  epoch  3, batch     5 | loss: 0.8728391Losses:  0.36251527070999146 0.33525341749191284
MemoryTrain:  epoch  3, batch     6 | loss: 0.6977687Losses:  0.7455230951309204 0.3953273594379425
MemoryTrain:  epoch  3, batch     7 | loss: 1.1408504Losses:  0.4095097482204437 0.29302453994750977
MemoryTrain:  epoch  3, batch     8 | loss: 0.7025343Losses:  0.3162468671798706 0.38312602043151855
MemoryTrain:  epoch  4, batch     0 | loss: 0.6993729Losses:  0.48858514428138733 0.40295249223709106
MemoryTrain:  epoch  4, batch     1 | loss: 0.8915377Losses:  0.4478846490383148 0.3358911871910095
MemoryTrain:  epoch  4, batch     2 | loss: 0.7837758Losses:  0.8585415482521057 0.43157216906547546
MemoryTrain:  epoch  4, batch     3 | loss: 1.2901137Losses:  0.3311178684234619 0.3210154175758362
MemoryTrain:  epoch  4, batch     4 | loss: 0.6521333Losses:  0.5847196578979492 0.4007815718650818
MemoryTrain:  epoch  4, batch     5 | loss: 0.9855012Losses:  0.6909230351448059 0.5091769695281982
MemoryTrain:  epoch  4, batch     6 | loss: 1.2000999Losses:  0.9620956182479858 0.38248252868652344
MemoryTrain:  epoch  4, batch     7 | loss: 1.3445781Losses:  0.9032586812973022 0.410620778799057
MemoryTrain:  epoch  4, batch     8 | loss: 1.3138795Losses:  0.37577980756759644 0.3637670874595642
MemoryTrain:  epoch  5, batch     0 | loss: 0.7395469Losses:  0.664250373840332 0.4685041904449463
MemoryTrain:  epoch  5, batch     1 | loss: 1.1327546Losses:  0.769364595413208 0.4947564899921417
MemoryTrain:  epoch  5, batch     2 | loss: 1.2641211Losses:  0.47101491689682007 0.33246177434921265
MemoryTrain:  epoch  5, batch     3 | loss: 0.8034767Losses:  0.2972894608974457 0.2864978313446045
MemoryTrain:  epoch  5, batch     4 | loss: 0.5837873Losses:  0.8717784881591797 0.41515204310417175
MemoryTrain:  epoch  5, batch     5 | loss: 1.2869306Losses:  0.5182702541351318 0.4098048806190491
MemoryTrain:  epoch  5, batch     6 | loss: 0.9280751Losses:  0.35187527537345886 0.29170259833335876
MemoryTrain:  epoch  5, batch     7 | loss: 0.6435779Losses:  0.5049458146095276 0.2651214599609375
MemoryTrain:  epoch  5, batch     8 | loss: 0.7700673Losses:  0.32805442810058594 0.2826029658317566
MemoryTrain:  epoch  6, batch     0 | loss: 0.6106574Losses:  0.4909648895263672 0.2529790699481964
MemoryTrain:  epoch  6, batch     1 | loss: 0.7439439Losses:  0.2837011218070984 0.42630138993263245
MemoryTrain:  epoch  6, batch     2 | loss: 0.7100025Losses:  0.4979857802391052 0.5319439172744751
MemoryTrain:  epoch  6, batch     3 | loss: 1.0299296Losses:  0.5501044988632202 0.3621719777584076
MemoryTrain:  epoch  6, batch     4 | loss: 0.9122765Losses:  0.5629462003707886 0.28856974840164185
MemoryTrain:  epoch  6, batch     5 | loss: 0.8515159Losses:  0.4041062891483307 0.33414340019226074
MemoryTrain:  epoch  6, batch     6 | loss: 0.7382497Losses:  0.3653923273086548 0.3430301547050476
MemoryTrain:  epoch  6, batch     7 | loss: 0.7084225Losses:  0.68932044506073 0.3672986626625061
MemoryTrain:  epoch  6, batch     8 | loss: 1.0566192Losses:  0.40105703473091125 0.41580966114997864
MemoryTrain:  epoch  7, batch     0 | loss: 0.8168667Losses:  0.5750104188919067 0.5578077435493469
MemoryTrain:  epoch  7, batch     1 | loss: 1.1328182Losses:  0.4546739459037781 0.31050801277160645
MemoryTrain:  epoch  7, batch     2 | loss: 0.7651820Losses:  0.5294024348258972 0.44334185123443604
MemoryTrain:  epoch  7, batch     3 | loss: 0.9727443Losses:  0.416756808757782 0.34032928943634033
MemoryTrain:  epoch  7, batch     4 | loss: 0.7570861Losses:  0.4074685275554657 0.3333554267883301
MemoryTrain:  epoch  7, batch     5 | loss: 0.7408240Losses:  0.24737727642059326 0.216566264629364
MemoryTrain:  epoch  7, batch     6 | loss: 0.4639435Losses:  0.4807187020778656 0.41392701864242554
MemoryTrain:  epoch  7, batch     7 | loss: 0.8946457Losses:  0.36046406626701355 0.21214091777801514
MemoryTrain:  epoch  7, batch     8 | loss: 0.5726050Losses:  0.3892764449119568 0.25217974185943604
MemoryTrain:  epoch  8, batch     0 | loss: 0.6414562Losses:  0.32813337445259094 0.25393515825271606
MemoryTrain:  epoch  8, batch     1 | loss: 0.5820686Losses:  0.4670334458351135 0.36366087198257446
MemoryTrain:  epoch  8, batch     2 | loss: 0.8306943Losses:  0.4194057583808899 0.3741147518157959
MemoryTrain:  epoch  8, batch     3 | loss: 0.7935205Losses:  0.6351991295814514 0.42786362767219543
MemoryTrain:  epoch  8, batch     4 | loss: 1.0630628Losses:  0.5486252307891846 0.3225908875465393
MemoryTrain:  epoch  8, batch     5 | loss: 0.8712161Losses:  0.3318091630935669 0.35300612449645996
MemoryTrain:  epoch  8, batch     6 | loss: 0.6848153Losses:  0.5228188037872314 0.4512382745742798
MemoryTrain:  epoch  8, batch     7 | loss: 0.9740571Losses:  0.22344879806041718 0.16508331894874573
MemoryTrain:  epoch  8, batch     8 | loss: 0.3885321Losses:  0.35568374395370483 0.3956734836101532
MemoryTrain:  epoch  9, batch     0 | loss: 0.7513572Losses:  0.4250420928001404 0.2868415415287018
MemoryTrain:  epoch  9, batch     1 | loss: 0.7118837Losses:  0.2828648090362549 0.26309552788734436
MemoryTrain:  epoch  9, batch     2 | loss: 0.5459603Losses:  0.3857259452342987 0.34908488392829895
MemoryTrain:  epoch  9, batch     3 | loss: 0.7348108Losses:  0.4452889561653137 0.4748948812484741
MemoryTrain:  epoch  9, batch     4 | loss: 0.9201838Losses:  0.48084598779678345 0.45995408296585083
MemoryTrain:  epoch  9, batch     5 | loss: 0.9408001Losses:  0.4207534193992615 0.2972818613052368
MemoryTrain:  epoch  9, batch     6 | loss: 0.7180353Losses:  0.7085649967193604 0.30700403451919556
MemoryTrain:  epoch  9, batch     7 | loss: 1.0155690Losses:  0.33883553743362427 0.19479313492774963
MemoryTrain:  epoch  9, batch     8 | loss: 0.5336287
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 62.70%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 62.68%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 62.33%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 61.59%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 61.77%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 60.69%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 59.18%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 57.91%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 57.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 58.58%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 60.14%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 60.88%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 64.34%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 64.82%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 64.38%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.15%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 83.44%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.65%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 82.10%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 81.05%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 80.81%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 80.22%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 79.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 79.03%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.65%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 78.37%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 77.85%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 77.42%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 76.45%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 75.60%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 74.78%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 73.97%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.26%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.49%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 72.37%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 74.49%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 74.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 76.59%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 76.47%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 76.22%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 76.26%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 76.55%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 77.02%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 76.59%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 76.18%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 75.68%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 75.15%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 74.71%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 74.15%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 74.45%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 74.69%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 74.96%   [EVAL] batch:  142 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  144 | acc: 12.50%,  total acc: 74.57%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 74.06%   [EVAL] batch:  146 | acc: 0.00%,  total acc: 73.55%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 73.06%   [EVAL] batch:  148 | acc: 6.25%,  total acc: 72.61%   [EVAL] batch:  149 | acc: 0.00%,  total acc: 72.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.69%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 71.26%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 70.79%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 70.33%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 69.92%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.47%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 70.93%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 70.66%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 70.29%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 69.95%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 69.29%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 68.93%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 68.79%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 68.78%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 68.55%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 68.55%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 68.28%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 68.26%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 68.81%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 68.66%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 68.41%   [EVAL] batch:  218 | acc: 37.50%,  total acc: 68.26%   [EVAL] batch:  219 | acc: 31.25%,  total acc: 68.10%   [EVAL] batch:  220 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 67.54%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 67.32%   [EVAL] batch:  223 | acc: 0.00%,  total acc: 67.02%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 66.75%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 67.89%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 67.82%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.83%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 67.78%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 67.69%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 67.83%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 67.90%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:  291 | acc: 56.25%,  total acc: 69.07%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 68.96%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 68.77%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 68.67%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 68.63%   [EVAL] batch:  308 | acc: 12.50%,  total acc: 68.45%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 68.25%   [EVAL] batch:  310 | acc: 25.00%,  total acc: 68.11%   [EVAL] batch:  311 | acc: 12.50%,  total acc: 67.93%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 67.81%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 67.12%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  325 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  327 | acc: 0.00%,  total acc: 66.98%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 66.85%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 66.69%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.26%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 67.90%   [EVAL] batch:  354 | acc: 31.25%,  total acc: 67.80%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.14%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 68.00%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 67.96%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 67.95%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 67.89%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 67.81%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 67.74%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 67.61%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 67.49%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 67.76%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 67.83%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  396 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 68.09%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 68.11%   [EVAL] batch:  399 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:  400 | acc: 18.75%,  total acc: 67.99%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 67.94%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 67.60%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 67.38%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 67.30%   [EVAL] batch:  419 | acc: 18.75%,  total acc: 67.19%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 67.10%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 66.99%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 66.89%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 67.34%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 67.45%   
cur_acc:  ['0.9464', '0.8006', '0.6657', '0.8095', '0.7609', '0.7014', '0.6438']
his_acc:  ['0.9464', '0.8675', '0.7819', '0.7698', '0.7408', '0.7092', '0.6745']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 37 29  0  0 28 32 23  0  0 35 34  0  0  1
  1 19  0  0  0  0  5 22  0  0  0  2  0 18 27 20 26  0 10  0  9 25 36  0
  0  0  2 16  0  0 17  0  0  0 13 11  0  0  0 12  0 31  0 30 15 14  7  4
  1  0  0  8  0  6  0  3]
Losses:  6.867224216461182 1.0962635278701782
CurrentTrain: epoch  0, batch     0 | loss: 7.9634876Losses:  9.728778839111328 0.9793127775192261
CurrentTrain: epoch  0, batch     1 | loss: 10.7080917Losses:  7.830241680145264 0.9837614297866821
CurrentTrain: epoch  0, batch     2 | loss: 8.8140030Losses:  7.528879165649414 1.0392887592315674
CurrentTrain: epoch  0, batch     3 | loss: 8.5681677Losses:  6.636108875274658 0.914746880531311
CurrentTrain: epoch  0, batch     4 | loss: 7.5508556Losses:  6.4196295738220215 1.3052558898925781
CurrentTrain: epoch  0, batch     5 | loss: 7.7248855Losses:  3.822990894317627 0.2313874363899231
CurrentTrain: epoch  0, batch     6 | loss: 4.0543785Losses:  4.180027484893799 0.7371190786361694
CurrentTrain: epoch  1, batch     0 | loss: 4.9171467Losses:  3.3441693782806396 0.872292160987854
CurrentTrain: epoch  1, batch     1 | loss: 4.2164617Losses:  3.4555015563964844 0.8801284432411194
CurrentTrain: epoch  1, batch     2 | loss: 4.3356299Losses:  4.847657203674316 0.9680963158607483
CurrentTrain: epoch  1, batch     3 | loss: 5.8157535Losses:  2.9364283084869385 0.8385117650032043
CurrentTrain: epoch  1, batch     4 | loss: 3.7749400Losses:  3.1161317825317383 0.7014686465263367
CurrentTrain: epoch  1, batch     5 | loss: 3.8176005Losses:  3.99615478515625 0.4971214234828949
CurrentTrain: epoch  1, batch     6 | loss: 4.4932761Losses:  2.6343493461608887 0.4569048285484314
CurrentTrain: epoch  2, batch     0 | loss: 3.0912542Losses:  4.300689697265625 1.1299078464508057
CurrentTrain: epoch  2, batch     1 | loss: 5.4305973Losses:  3.282900094985962 0.9191141128540039
CurrentTrain: epoch  2, batch     2 | loss: 4.2020140Losses:  3.0005810260772705 0.7810863256454468
CurrentTrain: epoch  2, batch     3 | loss: 3.7816672Losses:  3.5564780235290527 0.6172854900360107
CurrentTrain: epoch  2, batch     4 | loss: 4.1737633Losses:  3.3555822372436523 0.6587280631065369
CurrentTrain: epoch  2, batch     5 | loss: 4.0143104Losses:  2.041471481323242 0.10885541141033173
CurrentTrain: epoch  2, batch     6 | loss: 2.1503270Losses:  3.432589054107666 0.5399880409240723
CurrentTrain: epoch  3, batch     0 | loss: 3.9725771Losses:  3.9310173988342285 0.8531453609466553
CurrentTrain: epoch  3, batch     1 | loss: 4.7841625Losses:  2.3546807765960693 0.4669210612773895
CurrentTrain: epoch  3, batch     2 | loss: 2.8216019Losses:  3.1209535598754883 0.6776158809661865
CurrentTrain: epoch  3, batch     3 | loss: 3.7985694Losses:  2.5954062938690186 0.6519013047218323
CurrentTrain: epoch  3, batch     4 | loss: 3.2473075Losses:  2.654042959213257 0.43007466197013855
CurrentTrain: epoch  3, batch     5 | loss: 3.0841177Losses:  1.8934643268585205 0.05798185616731644
CurrentTrain: epoch  3, batch     6 | loss: 1.9514462Losses:  2.977797508239746 0.4141717255115509
CurrentTrain: epoch  4, batch     0 | loss: 3.3919692Losses:  2.7400999069213867 0.6146893501281738
CurrentTrain: epoch  4, batch     1 | loss: 3.3547893Losses:  3.246366262435913 0.7131462097167969
CurrentTrain: epoch  4, batch     2 | loss: 3.9595125Losses:  2.3808023929595947 0.5557736158370972
CurrentTrain: epoch  4, batch     3 | loss: 2.9365759Losses:  2.22944974899292 0.3289831876754761
CurrentTrain: epoch  4, batch     4 | loss: 2.5584331Losses:  2.891209840774536 0.5530863404273987
CurrentTrain: epoch  4, batch     5 | loss: 3.4442961Losses:  2.8981966972351074 0.1080426573753357
CurrentTrain: epoch  4, batch     6 | loss: 3.0062394Losses:  2.481539726257324 0.44139426946640015
CurrentTrain: epoch  5, batch     0 | loss: 2.9229341Losses:  2.488672971725464 0.5998822450637817
CurrentTrain: epoch  5, batch     1 | loss: 3.0885553Losses:  2.978083610534668 0.4786456823348999
CurrentTrain: epoch  5, batch     2 | loss: 3.4567294Losses:  2.359869956970215 0.4653935432434082
CurrentTrain: epoch  5, batch     3 | loss: 2.8252635Losses:  2.372673273086548 0.35837841033935547
CurrentTrain: epoch  5, batch     4 | loss: 2.7310517Losses:  2.7406373023986816 0.3587450683116913
CurrentTrain: epoch  5, batch     5 | loss: 3.0993824Losses:  2.7190892696380615 0.09515880048274994
CurrentTrain: epoch  5, batch     6 | loss: 2.8142481Losses:  2.3707070350646973 0.33189257979393005
CurrentTrain: epoch  6, batch     0 | loss: 2.7025995Losses:  2.4958200454711914 0.6159664392471313
CurrentTrain: epoch  6, batch     1 | loss: 3.1117864Losses:  3.0476913452148438 0.7526975870132446
CurrentTrain: epoch  6, batch     2 | loss: 3.8003888Losses:  2.36592960357666 0.4400051236152649
CurrentTrain: epoch  6, batch     3 | loss: 2.8059347Losses:  2.1187355518341064 0.3129313588142395
CurrentTrain: epoch  6, batch     4 | loss: 2.4316669Losses:  2.09197735786438 0.39413222670555115
CurrentTrain: epoch  6, batch     5 | loss: 2.4861095Losses:  2.280261754989624 0.14686930179595947
CurrentTrain: epoch  6, batch     6 | loss: 2.4271312Losses:  2.531647205352783 0.6079413890838623
CurrentTrain: epoch  7, batch     0 | loss: 3.1395886Losses:  1.9357025623321533 0.3303004205226898
CurrentTrain: epoch  7, batch     1 | loss: 2.2660029Losses:  2.486926794052124 0.577924907207489
CurrentTrain: epoch  7, batch     2 | loss: 3.0648518Losses:  2.376643657684326 0.537273645401001
CurrentTrain: epoch  7, batch     3 | loss: 2.9139173Losses:  2.343526840209961 0.5364758372306824
CurrentTrain: epoch  7, batch     4 | loss: 2.8800027Losses:  2.225314140319824 0.46650028228759766
CurrentTrain: epoch  7, batch     5 | loss: 2.6918144Losses:  1.7059047222137451 0.0
CurrentTrain: epoch  7, batch     6 | loss: 1.7059047Losses:  2.0807905197143555 0.31574589014053345
CurrentTrain: epoch  8, batch     0 | loss: 2.3965364Losses:  1.9284228086471558 0.2825391888618469
CurrentTrain: epoch  8, batch     1 | loss: 2.2109621Losses:  2.772838830947876 0.3437127470970154
CurrentTrain: epoch  8, batch     2 | loss: 3.1165516Losses:  2.3266565799713135 0.48729485273361206
CurrentTrain: epoch  8, batch     3 | loss: 2.8139515Losses:  2.0345277786254883 0.34432730078697205
CurrentTrain: epoch  8, batch     4 | loss: 2.3788550Losses:  2.1542415618896484 0.2635784447193146
CurrentTrain: epoch  8, batch     5 | loss: 2.4178200Losses:  1.6911295652389526 0.03996012359857559
CurrentTrain: epoch  8, batch     6 | loss: 1.7310897Losses:  1.9905935525894165 0.35617637634277344
CurrentTrain: epoch  9, batch     0 | loss: 2.3467698Losses:  1.9973845481872559 0.3236048221588135
CurrentTrain: epoch  9, batch     1 | loss: 2.3209894Losses:  1.7519603967666626 0.248456209897995
CurrentTrain: epoch  9, batch     2 | loss: 2.0004165Losses:  2.0915212631225586 0.2431388795375824
CurrentTrain: epoch  9, batch     3 | loss: 2.3346601Losses:  2.2200729846954346 0.4044814109802246
CurrentTrain: epoch  9, batch     4 | loss: 2.6245544Losses:  2.5417513847351074 0.45957016944885254
CurrentTrain: epoch  9, batch     5 | loss: 3.0013216Losses:  1.6645283699035645 0.0
CurrentTrain: epoch  9, batch     6 | loss: 1.6645284
Losses:  6.212026596069336 0.34792953729629517
MemoryTrain:  epoch  0, batch     0 | loss: 6.5599561Losses:  8.503227233886719 0.4281284511089325
MemoryTrain:  epoch  0, batch     1 | loss: 8.9313555Losses:  9.102750778198242 0.4706527292728424
MemoryTrain:  epoch  0, batch     2 | loss: 9.5734034Losses:  10.199747085571289 0.49972331523895264
MemoryTrain:  epoch  0, batch     3 | loss: 10.6994705Losses:  10.559162139892578 0.26521095633506775
MemoryTrain:  epoch  0, batch     4 | loss: 10.8243732Losses:  10.072000503540039 0.24621932208538055
MemoryTrain:  epoch  0, batch     5 | loss: 10.3182201Losses:  9.814167022705078 0.29914307594299316
MemoryTrain:  epoch  0, batch     6 | loss: 10.1133099Losses:  10.162425994873047 0.5971152186393738
MemoryTrain:  epoch  0, batch     7 | loss: 10.7595415Losses:  10.308511734008789 0.5016111731529236
MemoryTrain:  epoch  0, batch     8 | loss: 10.8101225Losses:  10.858600616455078 0.31017982959747314
MemoryTrain:  epoch  0, batch     9 | loss: 11.1687803Losses:  1.0061768293380737 0.33209940791130066
MemoryTrain:  epoch  1, batch     0 | loss: 1.3382763Losses:  0.5220721960067749 0.35861921310424805
MemoryTrain:  epoch  1, batch     1 | loss: 0.8806914Losses:  1.7631202936172485 0.44678229093551636
MemoryTrain:  epoch  1, batch     2 | loss: 2.2099025Losses:  0.548448920249939 0.24461370706558228
MemoryTrain:  epoch  1, batch     3 | loss: 0.7930626Losses:  1.2159470319747925 0.5636670589447021
MemoryTrain:  epoch  1, batch     4 | loss: 1.7796141Losses:  0.4600147604942322 0.2730066180229187
MemoryTrain:  epoch  1, batch     5 | loss: 0.7330214Losses:  0.9393346905708313 0.4156758189201355
MemoryTrain:  epoch  1, batch     6 | loss: 1.3550105Losses:  0.7759568691253662 0.5175253748893738
MemoryTrain:  epoch  1, batch     7 | loss: 1.2934823Losses:  1.2045762538909912 0.44433191418647766
MemoryTrain:  epoch  1, batch     8 | loss: 1.6489081Losses:  1.3714015483856201 0.3737955093383789
MemoryTrain:  epoch  1, batch     9 | loss: 1.7451971Losses:  0.9764258861541748 0.26863551139831543
MemoryTrain:  epoch  2, batch     0 | loss: 1.2450614Losses:  0.5211899876594543 0.3246362507343292
MemoryTrain:  epoch  2, batch     1 | loss: 0.8458263Losses:  0.4203610420227051 0.2845005989074707
MemoryTrain:  epoch  2, batch     2 | loss: 0.7048616Losses:  0.5628429055213928 0.3867785930633545
MemoryTrain:  epoch  2, batch     3 | loss: 0.9496215Losses:  0.8059179782867432 0.32966935634613037
MemoryTrain:  epoch  2, batch     4 | loss: 1.1355873Losses:  0.761398196220398 0.2503938376903534
MemoryTrain:  epoch  2, batch     5 | loss: 1.0117921Losses:  0.6334996223449707 0.48194119334220886
MemoryTrain:  epoch  2, batch     6 | loss: 1.1154408Losses:  0.7580523490905762 0.5249166488647461
MemoryTrain:  epoch  2, batch     7 | loss: 1.2829690Losses:  1.2688369750976562 0.3782075047492981
MemoryTrain:  epoch  2, batch     8 | loss: 1.6470444Losses:  0.8366217017173767 0.32243025302886963
MemoryTrain:  epoch  2, batch     9 | loss: 1.1590519Losses:  0.28951674699783325 0.33583003282546997
MemoryTrain:  epoch  3, batch     0 | loss: 0.6253468Losses:  0.4914623498916626 0.40610891580581665
MemoryTrain:  epoch  3, batch     1 | loss: 0.8975713Losses:  0.37378284335136414 0.30244922637939453
MemoryTrain:  epoch  3, batch     2 | loss: 0.6762321Losses:  0.7938259840011597 0.4849448800086975
MemoryTrain:  epoch  3, batch     3 | loss: 1.2787709Losses:  0.9199222326278687 0.3115296959877014
MemoryTrain:  epoch  3, batch     4 | loss: 1.2314520Losses:  0.6658425331115723 0.5982538461685181
MemoryTrain:  epoch  3, batch     5 | loss: 1.2640964Losses:  0.5412722826004028 0.3766748607158661
MemoryTrain:  epoch  3, batch     6 | loss: 0.9179472Losses:  0.8557623624801636 0.28453028202056885
MemoryTrain:  epoch  3, batch     7 | loss: 1.1402926Losses:  0.5998618602752686 0.23337984085083008
MemoryTrain:  epoch  3, batch     8 | loss: 0.8332417Losses:  0.6346988081932068 0.5935201644897461
MemoryTrain:  epoch  3, batch     9 | loss: 1.2282190Losses:  0.458948016166687 0.4442322254180908
MemoryTrain:  epoch  4, batch     0 | loss: 0.9031802Losses:  0.4963134527206421 0.28235673904418945
MemoryTrain:  epoch  4, batch     1 | loss: 0.7786702Losses:  0.5258554220199585 0.3901100158691406
MemoryTrain:  epoch  4, batch     2 | loss: 0.9159654Losses:  0.49353188276290894 0.3282524645328522
MemoryTrain:  epoch  4, batch     3 | loss: 0.8217844Losses:  0.7114485502243042 0.46183234453201294
MemoryTrain:  epoch  4, batch     4 | loss: 1.1732810Losses:  0.6996641159057617 0.29442471265792847
MemoryTrain:  epoch  4, batch     5 | loss: 0.9940888Losses:  0.5732619762420654 0.4713963270187378
MemoryTrain:  epoch  4, batch     6 | loss: 1.0446583Losses:  0.6394526958465576 0.3306276798248291
MemoryTrain:  epoch  4, batch     7 | loss: 0.9700804Losses:  0.36351579427719116 0.32121527194976807
MemoryTrain:  epoch  4, batch     8 | loss: 0.6847311Losses:  0.5295145511627197 0.35848820209503174
MemoryTrain:  epoch  4, batch     9 | loss: 0.8880028Losses:  0.45377275347709656 0.25312936305999756
MemoryTrain:  epoch  5, batch     0 | loss: 0.7069021Losses:  0.45663702487945557 0.4768011271953583
MemoryTrain:  epoch  5, batch     1 | loss: 0.9334382Losses:  0.5077390670776367 0.35994118452072144
MemoryTrain:  epoch  5, batch     2 | loss: 0.8676803Losses:  0.5189948081970215 0.4920869469642639
MemoryTrain:  epoch  5, batch     3 | loss: 1.0110817Losses:  0.42278963327407837 0.3778986930847168
MemoryTrain:  epoch  5, batch     4 | loss: 0.8006883Losses:  0.47127097845077515 0.37772858142852783
MemoryTrain:  epoch  5, batch     5 | loss: 0.8489996Losses:  0.781943678855896 0.592106819152832
MemoryTrain:  epoch  5, batch     6 | loss: 1.3740505Losses:  0.5808751583099365 0.3899872601032257
MemoryTrain:  epoch  5, batch     7 | loss: 0.9708624Losses:  0.3833533823490143 0.2741938829421997
MemoryTrain:  epoch  5, batch     8 | loss: 0.6575472Losses:  0.3614908456802368 0.2916235625743866
MemoryTrain:  epoch  5, batch     9 | loss: 0.6531144Losses:  0.38077861070632935 0.31150510907173157
MemoryTrain:  epoch  6, batch     0 | loss: 0.6922837Losses:  0.45759114623069763 0.35317057371139526
MemoryTrain:  epoch  6, batch     1 | loss: 0.8107617Losses:  0.43946877121925354 0.3003109097480774
MemoryTrain:  epoch  6, batch     2 | loss: 0.7397797Losses:  0.4119245111942291 0.29527774453163147
MemoryTrain:  epoch  6, batch     3 | loss: 0.7072023Losses:  0.34618353843688965 0.31529897451400757
MemoryTrain:  epoch  6, batch     4 | loss: 0.6614825Losses:  0.37208718061447144 0.3107132315635681
MemoryTrain:  epoch  6, batch     5 | loss: 0.6828004Losses:  0.7149048447608948 0.5184574723243713
MemoryTrain:  epoch  6, batch     6 | loss: 1.2333623Losses:  0.434120774269104 0.3198806047439575
MemoryTrain:  epoch  6, batch     7 | loss: 0.7540014Losses:  0.5347251296043396 0.4264041483402252
MemoryTrain:  epoch  6, batch     8 | loss: 0.9611293Losses:  0.4909753203392029 0.3924749195575714
MemoryTrain:  epoch  6, batch     9 | loss: 0.8834503Losses:  0.36891990900039673 0.29997360706329346
MemoryTrain:  epoch  7, batch     0 | loss: 0.6688935Losses:  0.4457257390022278 0.28399235010147095
MemoryTrain:  epoch  7, batch     1 | loss: 0.7297181Losses:  0.5192539691925049 0.3743281364440918
MemoryTrain:  epoch  7, batch     2 | loss: 0.8935821Losses:  0.326763391494751 0.2910245656967163
MemoryTrain:  epoch  7, batch     3 | loss: 0.6177880Losses:  0.4258200526237488 0.29772427678108215
MemoryTrain:  epoch  7, batch     4 | loss: 0.7235444Losses:  0.40701472759246826 0.3428565561771393
MemoryTrain:  epoch  7, batch     5 | loss: 0.7498713Losses:  0.39724019169807434 0.2893746793270111
MemoryTrain:  epoch  7, batch     6 | loss: 0.6866149Losses:  0.4966997504234314 0.362565279006958
MemoryTrain:  epoch  7, batch     7 | loss: 0.8592650Losses:  0.4711044728755951 0.37582314014434814
MemoryTrain:  epoch  7, batch     8 | loss: 0.8469276Losses:  0.4794243276119232 0.4889976978302002
MemoryTrain:  epoch  7, batch     9 | loss: 0.9684221Losses:  0.27799728512763977 0.185649111866951
MemoryTrain:  epoch  8, batch     0 | loss: 0.4636464Losses:  0.44428059458732605 0.35926496982574463
MemoryTrain:  epoch  8, batch     1 | loss: 0.8035456Losses:  0.41605937480926514 0.2533377408981323
MemoryTrain:  epoch  8, batch     2 | loss: 0.6693971Losses:  0.36658963561058044 0.3353617191314697
MemoryTrain:  epoch  8, batch     3 | loss: 0.7019514Losses:  0.44986212253570557 0.4463505744934082
MemoryTrain:  epoch  8, batch     4 | loss: 0.8962127Losses:  0.4709962010383606 0.4143635034561157
MemoryTrain:  epoch  8, batch     5 | loss: 0.8853597Losses:  0.415447473526001 0.2829339802265167
MemoryTrain:  epoch  8, batch     6 | loss: 0.6983814Losses:  0.5793004035949707 0.35957324504852295
MemoryTrain:  epoch  8, batch     7 | loss: 0.9388736Losses:  0.3386401832103729 0.2629452347755432
MemoryTrain:  epoch  8, batch     8 | loss: 0.6015854Losses:  0.45114827156066895 0.45712554454803467
MemoryTrain:  epoch  8, batch     9 | loss: 0.9082738Losses:  0.3693559765815735 0.24996955692768097
MemoryTrain:  epoch  9, batch     0 | loss: 0.6193255Losses:  0.5079571604728699 0.37248319387435913
MemoryTrain:  epoch  9, batch     1 | loss: 0.8804404Losses:  0.40916064381599426 0.39851394295692444
MemoryTrain:  epoch  9, batch     2 | loss: 0.8076746Losses:  0.37423500418663025 0.3222336173057556
MemoryTrain:  epoch  9, batch     3 | loss: 0.6964686Losses:  0.2675950825214386 0.1769619584083557
MemoryTrain:  epoch  9, batch     4 | loss: 0.4445570Losses:  0.39991679787635803 0.3086463212966919
MemoryTrain:  epoch  9, batch     5 | loss: 0.7085631Losses:  0.3986224830150604 0.3483467698097229
MemoryTrain:  epoch  9, batch     6 | loss: 0.7469692Losses:  0.38583916425704956 0.25747883319854736
MemoryTrain:  epoch  9, batch     7 | loss: 0.6433180Losses:  0.43447738885879517 0.2578812539577484
MemoryTrain:  epoch  9, batch     8 | loss: 0.6923586Losses:  0.45589929819107056 0.47525933384895325
MemoryTrain:  epoch  9, batch     9 | loss: 0.9311587
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 55.21%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.43%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 54.10%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 60.69%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 60.98%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 61.05%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 61.22%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 59.18%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 58.59%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 57.91%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 57.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 58.14%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 59.04%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.32%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 59.70%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 60.17%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 60.96%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 61.11%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.64%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 83.79%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 83.44%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 82.66%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 83.36%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 81.69%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 81.08%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 79.56%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 78.92%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 78.54%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.17%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.96%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 77.37%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 76.95%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 76.62%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 75.91%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 75.08%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 73.38%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 72.67%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 71.91%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.11%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 75.71%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 75.70%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 75.53%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 75.32%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 75.46%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 74.95%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 74.37%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 73.84%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.27%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 72.71%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 72.63%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 72.65%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.71%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 72.78%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 72.86%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 72.96%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 73.06%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 72.95%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.70%   [EVAL] batch:  144 | acc: 12.50%,  total acc: 72.28%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 71.79%   [EVAL] batch:  146 | acc: 6.25%,  total acc: 71.34%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 70.86%   [EVAL] batch:  148 | acc: 0.00%,  total acc: 70.39%   [EVAL] batch:  149 | acc: 0.00%,  total acc: 69.92%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.50%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.63%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.18%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 67.78%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.35%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 67.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 68.64%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 68.27%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 67.56%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 66.93%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 66.60%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 66.50%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 66.60%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 66.43%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 66.34%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 66.18%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.81%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  218 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  219 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  220 | acc: 6.25%,  total acc: 65.72%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 65.25%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 64.72%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 64.74%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 64.83%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 64.91%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 64.90%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 64.79%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 66.14%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 66.05%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:  272 | acc: 12.50%,  total acc: 65.77%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 65.57%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  288 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  291 | acc: 50.00%,  total acc: 66.82%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 66.49%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 66.37%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 66.30%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 66.20%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 66.31%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 66.12%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 65.78%   [EVAL] batch:  311 | acc: 12.50%,  total acc: 65.60%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 65.41%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 65.26%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 65.11%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 64.88%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 64.75%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  325 | acc: 6.25%,  total acc: 65.15%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 64.95%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 64.77%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 64.41%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 65.12%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 65.67%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 65.61%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 66.08%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 65.96%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.92%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  375 | acc: 43.75%,  total acc: 65.81%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.73%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 65.66%   [EVAL] batch:  378 | acc: 6.25%,  total acc: 65.50%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  380 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 65.84%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  394 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  396 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  401 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  405 | acc: 56.25%,  total acc: 65.84%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  415 | acc: 56.25%,  total acc: 65.78%   [EVAL] batch:  416 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 65.76%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  420 | acc: 18.75%,  total acc: 65.54%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 65.43%   [EVAL] batch:  422 | acc: 12.50%,  total acc: 65.31%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 65.20%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 65.77%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  437 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  445 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  446 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 66.17%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  450 | acc: 12.50%,  total acc: 66.12%   [EVAL] batch:  451 | acc: 6.25%,  total acc: 65.98%   [EVAL] batch:  452 | acc: 0.00%,  total acc: 65.84%   [EVAL] batch:  453 | acc: 12.50%,  total acc: 65.72%   [EVAL] batch:  454 | acc: 6.25%,  total acc: 65.59%   [EVAL] batch:  455 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.63%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 65.67%   [EVAL] batch:  464 | acc: 6.25%,  total acc: 65.54%   [EVAL] batch:  465 | acc: 6.25%,  total acc: 65.41%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 65.18%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  475 | acc: 56.25%,  total acc: 65.48%   [EVAL] batch:  476 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  477 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  478 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  480 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  482 | acc: 37.50%,  total acc: 65.41%   [EVAL] batch:  483 | acc: 25.00%,  total acc: 65.33%   [EVAL] batch:  484 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:  485 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  486 | acc: 31.25%,  total acc: 65.11%   [EVAL] batch:  487 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  488 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 65.10%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  492 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 65.14%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.40%   
cur_acc:  ['0.9464', '0.8006', '0.6657', '0.8095', '0.7609', '0.7014', '0.6438', '0.6111']
his_acc:  ['0.9464', '0.8675', '0.7819', '0.7698', '0.7408', '0.7092', '0.6745', '0.6540']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.747373580932617 1.918205738067627
CurrentTrain: epoch  0, batch     0 | loss: 13.6655788Losses:  13.114652633666992 1.232893705368042
CurrentTrain: epoch  0, batch     1 | loss: 14.3475466Losses:  13.348538398742676 1.7379493713378906
CurrentTrain: epoch  0, batch     2 | loss: 15.0864878Losses:  13.450355529785156 1.6717140674591064
CurrentTrain: epoch  0, batch     3 | loss: 15.1220694Losses:  12.95094108581543 1.3932247161865234
CurrentTrain: epoch  0, batch     4 | loss: 14.3441658Losses:  13.385053634643555 1.5327359437942505
CurrentTrain: epoch  0, batch     5 | loss: 14.9177895Losses:  13.926403045654297 1.1405062675476074
CurrentTrain: epoch  0, batch     6 | loss: 15.0669098Losses:  13.3701171875 1.6883604526519775
CurrentTrain: epoch  0, batch     7 | loss: 15.0584774Losses:  13.509527206420898 1.1772902011871338
CurrentTrain: epoch  0, batch     8 | loss: 14.6868172Losses:  13.148028373718262 1.434882402420044
CurrentTrain: epoch  0, batch     9 | loss: 14.5829105Losses:  12.876260757446289 1.205664873123169
CurrentTrain: epoch  0, batch    10 | loss: 14.0819254Losses:  12.741983413696289 1.549410104751587
CurrentTrain: epoch  0, batch    11 | loss: 14.2913933Losses:  12.430351257324219 2.174752712249756
CurrentTrain: epoch  0, batch    12 | loss: 14.6051044Losses:  12.383437156677246 1.231369972229004
CurrentTrain: epoch  0, batch    13 | loss: 13.6148071Losses:  11.949732780456543 1.3864049911499023
CurrentTrain: epoch  0, batch    14 | loss: 13.3361378Losses:  12.056707382202148 1.356912612915039
CurrentTrain: epoch  0, batch    15 | loss: 13.4136200Losses:  12.314294815063477 1.2796545028686523
CurrentTrain: epoch  0, batch    16 | loss: 13.5939493Losses:  12.317741394042969 1.3224843740463257
CurrentTrain: epoch  0, batch    17 | loss: 13.6402254Losses:  12.108406066894531 1.0032610893249512
CurrentTrain: epoch  0, batch    18 | loss: 13.1116676Losses:  12.44393539428711 1.7851945161819458
CurrentTrain: epoch  0, batch    19 | loss: 14.2291298Losses:  12.056268692016602 1.3851289749145508
CurrentTrain: epoch  0, batch    20 | loss: 13.4413977Losses:  12.011861801147461 1.2491445541381836
CurrentTrain: epoch  0, batch    21 | loss: 13.2610064Losses:  12.368459701538086 1.1698055267333984
CurrentTrain: epoch  0, batch    22 | loss: 13.5382652Losses:  12.220094680786133 1.3661373853683472
CurrentTrain: epoch  0, batch    23 | loss: 13.5862322Losses:  11.287176132202148 1.094238519668579
CurrentTrain: epoch  0, batch    24 | loss: 12.3814144Losses:  11.471068382263184 1.0223183631896973
CurrentTrain: epoch  0, batch    25 | loss: 12.4933872Losses:  12.094442367553711 1.6084551811218262
CurrentTrain: epoch  0, batch    26 | loss: 13.7028980Losses:  11.983569145202637 1.003400444984436
CurrentTrain: epoch  0, batch    27 | loss: 12.9869699Losses:  11.572839736938477 0.9427793622016907
CurrentTrain: epoch  0, batch    28 | loss: 12.5156193Losses:  11.471912384033203 1.022244930267334
CurrentTrain: epoch  0, batch    29 | loss: 12.4941578Losses:  11.768316268920898 1.1332416534423828
CurrentTrain: epoch  0, batch    30 | loss: 12.9015579Losses:  11.79011344909668 1.1049296855926514
CurrentTrain: epoch  0, batch    31 | loss: 12.8950434Losses:  11.265536308288574 1.0215649604797363
CurrentTrain: epoch  0, batch    32 | loss: 12.2871017Losses:  11.42318058013916 1.545109748840332
CurrentTrain: epoch  0, batch    33 | loss: 12.9682903Losses:  11.486557006835938 1.1641080379486084
CurrentTrain: epoch  0, batch    34 | loss: 12.6506653Losses:  10.984683990478516 1.1542465686798096
CurrentTrain: epoch  0, batch    35 | loss: 12.1389303Losses:  11.230823516845703 1.038917899131775
CurrentTrain: epoch  0, batch    36 | loss: 12.2697411Losses:  10.58413028717041 1.0871217250823975
CurrentTrain: epoch  0, batch    37 | loss: 11.6712523Losses:  10.853443145751953 0.9121218919754028
CurrentTrain: epoch  0, batch    38 | loss: 11.7655649Losses:  11.774497985839844 1.0835950374603271
CurrentTrain: epoch  0, batch    39 | loss: 12.8580933Losses:  10.638320922851562 0.9804108738899231
CurrentTrain: epoch  0, batch    40 | loss: 11.6187315Losses:  10.833745956420898 0.7885314226150513
CurrentTrain: epoch  0, batch    41 | loss: 11.6222773Losses:  11.075960159301758 1.0622601509094238
CurrentTrain: epoch  0, batch    42 | loss: 12.1382198Losses:  10.97829818725586 1.254817247390747
CurrentTrain: epoch  0, batch    43 | loss: 12.2331152Losses:  11.143800735473633 0.9055455923080444
CurrentTrain: epoch  0, batch    44 | loss: 12.0493460Losses:  10.341442108154297 1.1147102117538452
CurrentTrain: epoch  0, batch    45 | loss: 11.4561520Losses:  10.277568817138672 0.5675030946731567
CurrentTrain: epoch  0, batch    46 | loss: 10.8450718Losses:  10.40243911743164 0.9207842350006104
CurrentTrain: epoch  0, batch    47 | loss: 11.3232231Losses:  10.39096450805664 0.8552905321121216
CurrentTrain: epoch  0, batch    48 | loss: 11.2462549Losses:  10.316136360168457 1.0821914672851562
CurrentTrain: epoch  0, batch    49 | loss: 11.3983278Losses:  10.22298812866211 0.9279567003250122
CurrentTrain: epoch  0, batch    50 | loss: 11.1509447Losses:  10.501724243164062 0.8826279640197754
CurrentTrain: epoch  0, batch    51 | loss: 11.3843517Losses:  10.474799156188965 1.0942374467849731
CurrentTrain: epoch  0, batch    52 | loss: 11.5690365Losses:  10.18802261352539 1.1087563037872314
CurrentTrain: epoch  0, batch    53 | loss: 11.2967787Losses:  9.424232482910156 0.9225314855575562
CurrentTrain: epoch  0, batch    54 | loss: 10.3467636Losses:  9.567503929138184 0.5874786376953125
CurrentTrain: epoch  0, batch    55 | loss: 10.1549826Losses:  9.618748664855957 0.9359989166259766
CurrentTrain: epoch  0, batch    56 | loss: 10.5547476Losses:  9.67037582397461 0.9913443922996521
CurrentTrain: epoch  0, batch    57 | loss: 10.6617203Losses:  9.265789031982422 0.8623905181884766
CurrentTrain: epoch  0, batch    58 | loss: 10.1281796Losses:  9.279411315917969 0.8900614976882935
CurrentTrain: epoch  0, batch    59 | loss: 10.1694727Losses:  9.375223159790039 0.9695955514907837
CurrentTrain: epoch  0, batch    60 | loss: 10.3448191Losses:  9.279125213623047 0.6337701082229614
CurrentTrain: epoch  0, batch    61 | loss: 9.9128952Losses:  9.25084114074707 0.9784150123596191
CurrentTrain: epoch  0, batch    62 | loss: 10.2292557Losses:  8.866576194763184 0.47687196731567383
CurrentTrain: epoch  0, batch    63 | loss: 9.3434486Losses:  9.513161659240723 1.2064087390899658
CurrentTrain: epoch  0, batch    64 | loss: 10.7195702Losses:  9.892654418945312 0.49324530363082886
CurrentTrain: epoch  0, batch    65 | loss: 10.3858995Losses:  9.157379150390625 1.0941386222839355
CurrentTrain: epoch  0, batch    66 | loss: 10.2515182Losses:  9.101224899291992 0.7865029573440552
CurrentTrain: epoch  0, batch    67 | loss: 9.8877277Losses:  9.0794038772583 0.9682997465133667
CurrentTrain: epoch  0, batch    68 | loss: 10.0477037Losses:  9.183591842651367 0.8130068778991699
CurrentTrain: epoch  0, batch    69 | loss: 9.9965992Losses:  9.210514068603516 0.5812997817993164
CurrentTrain: epoch  0, batch    70 | loss: 9.7918139Losses:  8.671518325805664 0.6246091723442078
CurrentTrain: epoch  0, batch    71 | loss: 9.2961273Losses:  9.103668212890625 0.6687214374542236
CurrentTrain: epoch  0, batch    72 | loss: 9.7723894Losses:  8.536018371582031 1.0261929035186768
CurrentTrain: epoch  0, batch    73 | loss: 9.5622110Losses:  8.335334777832031 0.7252604961395264
CurrentTrain: epoch  0, batch    74 | loss: 9.0605955Losses:  8.930065155029297 1.0927824974060059
CurrentTrain: epoch  0, batch    75 | loss: 10.0228481Losses:  8.998305320739746 1.0316178798675537
CurrentTrain: epoch  0, batch    76 | loss: 10.0299234Losses:  8.271687507629395 0.6025660634040833
CurrentTrain: epoch  0, batch    77 | loss: 8.8742533Losses:  8.360477447509766 0.4136395752429962
CurrentTrain: epoch  0, batch    78 | loss: 8.7741175Losses:  8.805963516235352 0.5839680433273315
CurrentTrain: epoch  0, batch    79 | loss: 9.3899317Losses:  8.56016731262207 0.8247195482254028
CurrentTrain: epoch  0, batch    80 | loss: 9.3848867Losses:  8.120383262634277 0.79323410987854
CurrentTrain: epoch  0, batch    81 | loss: 8.9136171Losses:  8.299264907836914 0.6323593854904175
CurrentTrain: epoch  0, batch    82 | loss: 8.9316244Losses:  8.084331512451172 0.40764397382736206
CurrentTrain: epoch  0, batch    83 | loss: 8.4919758Losses:  7.861103057861328 0.6121554970741272
CurrentTrain: epoch  0, batch    84 | loss: 8.4732590Losses:  7.716066837310791 0.4987676739692688
CurrentTrain: epoch  0, batch    85 | loss: 8.2148342Losses:  8.172712326049805 0.9491595029830933
CurrentTrain: epoch  0, batch    86 | loss: 9.1218719Losses:  7.850481986999512 0.676971435546875
CurrentTrain: epoch  0, batch    87 | loss: 8.5274534Losses:  8.188292503356934 0.9649035930633545
CurrentTrain: epoch  0, batch    88 | loss: 9.1531963Losses:  8.340702056884766 1.0099635124206543
CurrentTrain: epoch  0, batch    89 | loss: 9.3506660Losses:  7.978666305541992 0.7248467206954956
CurrentTrain: epoch  0, batch    90 | loss: 8.7035131Losses:  7.781878471374512 0.7390111684799194
CurrentTrain: epoch  0, batch    91 | loss: 8.5208893Losses:  7.688984394073486 0.6132552623748779
CurrentTrain: epoch  0, batch    92 | loss: 8.3022394Losses:  8.744330406188965 0.7645580768585205
CurrentTrain: epoch  0, batch    93 | loss: 9.5088882Losses:  7.720370292663574 0.49971863627433777
CurrentTrain: epoch  0, batch    94 | loss: 8.2200890Losses:  7.604755401611328 0.7691525816917419
CurrentTrain: epoch  0, batch    95 | loss: 8.3739080Losses:  7.733486652374268 0.5120023488998413
CurrentTrain: epoch  0, batch    96 | loss: 8.2454891Losses:  6.677707672119141 0.38856691122055054
CurrentTrain: epoch  0, batch    97 | loss: 7.0662746Losses:  7.254124641418457 0.6670522689819336
CurrentTrain: epoch  0, batch    98 | loss: 7.9211769Losses:  7.587884902954102 0.7897207736968994
CurrentTrain: epoch  0, batch    99 | loss: 8.3776054Losses:  6.721492767333984 0.5954928994178772
CurrentTrain: epoch  0, batch   100 | loss: 7.3169856Losses:  6.9652862548828125 0.4647420048713684
CurrentTrain: epoch  0, batch   101 | loss: 7.4300284Losses:  6.726711273193359 0.5415562391281128
CurrentTrain: epoch  0, batch   102 | loss: 7.2682676Losses:  7.069344520568848 0.5750985741615295
CurrentTrain: epoch  0, batch   103 | loss: 7.6444430Losses:  7.158278942108154 0.5819739699363708
CurrentTrain: epoch  0, batch   104 | loss: 7.7402530Losses:  6.830011367797852 0.6268519759178162
CurrentTrain: epoch  0, batch   105 | loss: 7.4568634Losses:  7.147104263305664 0.6405723094940186
CurrentTrain: epoch  0, batch   106 | loss: 7.7876768Losses:  6.9590911865234375 0.5684002041816711
CurrentTrain: epoch  0, batch   107 | loss: 7.5274916Losses:  6.213910102844238 0.6226774454116821
CurrentTrain: epoch  0, batch   108 | loss: 6.8365874Losses:  6.1930999755859375 0.6563326120376587
CurrentTrain: epoch  0, batch   109 | loss: 6.8494325Losses:  6.750200271606445 0.7817947864532471
CurrentTrain: epoch  0, batch   110 | loss: 7.5319948Losses:  6.013391971588135 0.6090109348297119
CurrentTrain: epoch  0, batch   111 | loss: 6.6224031Losses:  6.3904829025268555 0.6629519462585449
CurrentTrain: epoch  0, batch   112 | loss: 7.0534348Losses:  6.535013198852539 0.8178837895393372
CurrentTrain: epoch  0, batch   113 | loss: 7.3528972Losses:  6.569046974182129 0.655104398727417
CurrentTrain: epoch  0, batch   114 | loss: 7.2241516Losses:  6.312141418457031 0.797163724899292
CurrentTrain: epoch  0, batch   115 | loss: 7.1093054Losses:  6.071579933166504 0.7370893359184265
CurrentTrain: epoch  0, batch   116 | loss: 6.8086691Losses:  5.950032711029053 0.4029221534729004
CurrentTrain: epoch  0, batch   117 | loss: 6.3529549Losses:  5.8755621910095215 0.5701110363006592
CurrentTrain: epoch  0, batch   118 | loss: 6.4456730Losses:  5.697541236877441 0.35155603289604187
CurrentTrain: epoch  0, batch   119 | loss: 6.0490971Losses:  5.922536849975586 0.6192120313644409
CurrentTrain: epoch  0, batch   120 | loss: 6.5417490Losses:  5.552007675170898 0.6403805017471313
CurrentTrain: epoch  0, batch   121 | loss: 6.1923881Losses:  5.774445056915283 0.6806910037994385
CurrentTrain: epoch  0, batch   122 | loss: 6.4551363Losses:  5.9462995529174805 0.40871375799179077
CurrentTrain: epoch  0, batch   123 | loss: 6.3550134Losses:  5.4630632400512695 0.5788540840148926
CurrentTrain: epoch  0, batch   124 | loss: 6.0419173Losses:  5.025565147399902 0.20626214146614075
CurrentTrain: epoch  1, batch     0 | loss: 5.2318273Losses:  5.694667816162109 0.40745264291763306
CurrentTrain: epoch  1, batch     1 | loss: 6.1021204Losses:  5.345746994018555 0.5624903440475464
CurrentTrain: epoch  1, batch     2 | loss: 5.9082375Losses:  5.336979866027832 0.47552764415740967
CurrentTrain: epoch  1, batch     3 | loss: 5.8125076Losses:  5.351814270019531 0.3680092692375183
CurrentTrain: epoch  1, batch     4 | loss: 5.7198234Losses:  6.200446128845215 0.5191938281059265
CurrentTrain: epoch  1, batch     5 | loss: 6.7196398Losses:  5.507541179656982 0.4288337528705597
CurrentTrain: epoch  1, batch     6 | loss: 5.9363751Losses:  5.222954273223877 0.2732246518135071
CurrentTrain: epoch  1, batch     7 | loss: 5.4961791Losses:  6.086198806762695 0.730553925037384
CurrentTrain: epoch  1, batch     8 | loss: 6.8167529Losses:  5.471138000488281 0.5939635634422302
CurrentTrain: epoch  1, batch     9 | loss: 6.0651016Losses:  5.227481365203857 0.3785465359687805
CurrentTrain: epoch  1, batch    10 | loss: 5.6060281Losses:  5.515491485595703 0.4709468483924866
CurrentTrain: epoch  1, batch    11 | loss: 5.9864383Losses:  5.421395301818848 0.5212432146072388
CurrentTrain: epoch  1, batch    12 | loss: 5.9426384Losses:  5.584987640380859 0.41644179821014404
CurrentTrain: epoch  1, batch    13 | loss: 6.0014296Losses:  5.59975528717041 0.4205411970615387
CurrentTrain: epoch  1, batch    14 | loss: 6.0202966Losses:  5.403173446655273 0.46144235134124756
CurrentTrain: epoch  1, batch    15 | loss: 5.8646159Losses:  5.571737766265869 0.4196464419364929
CurrentTrain: epoch  1, batch    16 | loss: 5.9913840Losses:  5.829094886779785 0.5943937301635742
CurrentTrain: epoch  1, batch    17 | loss: 6.4234886Losses:  5.604942798614502 0.4672689437866211
CurrentTrain: epoch  1, batch    18 | loss: 6.0722117Losses:  5.284165382385254 0.532688558101654
CurrentTrain: epoch  1, batch    19 | loss: 5.8168540Losses:  5.237410545349121 0.28307604789733887
CurrentTrain: epoch  1, batch    20 | loss: 5.5204868Losses:  5.29948616027832 0.5378638505935669
CurrentTrain: epoch  1, batch    21 | loss: 5.8373499Losses:  5.313408851623535 0.376358300447464
CurrentTrain: epoch  1, batch    22 | loss: 5.6897674Losses:  5.217642784118652 0.3213885426521301
CurrentTrain: epoch  1, batch    23 | loss: 5.5390315Losses:  5.567410469055176 0.400706946849823
CurrentTrain: epoch  1, batch    24 | loss: 5.9681172Losses:  4.963115692138672 0.4119431972503662
CurrentTrain: epoch  1, batch    25 | loss: 5.3750591Losses:  5.214646339416504 0.35560011863708496
CurrentTrain: epoch  1, batch    26 | loss: 5.5702467Losses:  5.599853038787842 0.26430726051330566
CurrentTrain: epoch  1, batch    27 | loss: 5.8641605Losses:  5.01536750793457 0.2753544747829437
CurrentTrain: epoch  1, batch    28 | loss: 5.2907219Losses:  5.610052585601807 0.3984825909137726
CurrentTrain: epoch  1, batch    29 | loss: 6.0085354Losses:  5.558633804321289 0.2521117925643921
CurrentTrain: epoch  1, batch    30 | loss: 5.8107457Losses:  5.3614091873168945 0.3389836549758911
CurrentTrain: epoch  1, batch    31 | loss: 5.7003927Losses:  5.292888164520264 0.4158039689064026
CurrentTrain: epoch  1, batch    32 | loss: 5.7086921Losses:  5.605025768280029 0.5049199461936951
CurrentTrain: epoch  1, batch    33 | loss: 6.1099458Losses:  4.88926887512207 0.39575183391571045
CurrentTrain: epoch  1, batch    34 | loss: 5.2850208Losses:  4.81683349609375 0.34408482909202576
CurrentTrain: epoch  1, batch    35 | loss: 5.1609182Losses:  5.408872127532959 0.4835224151611328
CurrentTrain: epoch  1, batch    36 | loss: 5.8923945Losses:  5.131538391113281 0.2924637198448181
CurrentTrain: epoch  1, batch    37 | loss: 5.4240022Losses:  4.976943016052246 0.36329197883605957
CurrentTrain: epoch  1, batch    38 | loss: 5.3402348Losses:  5.392905235290527 0.34733814001083374
CurrentTrain: epoch  1, batch    39 | loss: 5.7402434Losses:  5.105456829071045 0.22282060980796814
CurrentTrain: epoch  1, batch    40 | loss: 5.3282776Losses:  5.417566299438477 0.4898407459259033
CurrentTrain: epoch  1, batch    41 | loss: 5.9074068Losses:  4.974092960357666 0.3216897249221802
CurrentTrain: epoch  1, batch    42 | loss: 5.2957826Losses:  4.762434959411621 0.14541955292224884
CurrentTrain: epoch  1, batch    43 | loss: 4.9078546Losses:  5.042438507080078 0.4117141366004944
CurrentTrain: epoch  1, batch    44 | loss: 5.4541526Losses:  5.113095283508301 0.4261086583137512
CurrentTrain: epoch  1, batch    45 | loss: 5.5392041Losses:  5.036927700042725 0.34175264835357666
CurrentTrain: epoch  1, batch    46 | loss: 5.3786802Losses:  5.397441864013672 0.3334308862686157
CurrentTrain: epoch  1, batch    47 | loss: 5.7308726Losses:  5.3144917488098145 0.5326343774795532
CurrentTrain: epoch  1, batch    48 | loss: 5.8471260Losses:  5.2580108642578125 0.4642830491065979
CurrentTrain: epoch  1, batch    49 | loss: 5.7222939Losses:  4.887889385223389 0.36061546206474304
CurrentTrain: epoch  1, batch    50 | loss: 5.2485046Losses:  5.044740200042725 0.4124244153499603
CurrentTrain: epoch  1, batch    51 | loss: 5.4571648Losses:  4.999449729919434 0.38848114013671875
CurrentTrain: epoch  1, batch    52 | loss: 5.3879309Losses:  5.1475701332092285 0.2897011339664459
CurrentTrain: epoch  1, batch    53 | loss: 5.4372711Losses:  4.965541839599609 0.20775900781154633
CurrentTrain: epoch  1, batch    54 | loss: 5.1733007Losses:  6.207771301269531 0.26702773571014404
CurrentTrain: epoch  1, batch    55 | loss: 6.4747992Losses:  5.315578937530518 0.4862351417541504
CurrentTrain: epoch  1, batch    56 | loss: 5.8018141Losses:  5.090719223022461 0.3921457827091217
CurrentTrain: epoch  1, batch    57 | loss: 5.4828649Losses:  5.683915138244629 0.5979708433151245
CurrentTrain: epoch  1, batch    58 | loss: 6.2818861Losses:  4.984856605529785 0.2546786963939667
CurrentTrain: epoch  1, batch    59 | loss: 5.2395353Losses:  4.70720100402832 0.1485956758260727
CurrentTrain: epoch  1, batch    60 | loss: 4.8557968Losses:  5.604640007019043 0.5872838497161865
CurrentTrain: epoch  1, batch    61 | loss: 6.1919241Losses:  5.320034980773926 0.3866328299045563
CurrentTrain: epoch  1, batch    62 | loss: 5.7066679Losses:  4.968905448913574 0.22088751196861267
CurrentTrain: epoch  1, batch    63 | loss: 5.1897931Losses:  5.0407257080078125 0.24515633285045624
CurrentTrain: epoch  1, batch    64 | loss: 5.2858820Losses:  4.864525318145752 0.2428169995546341
CurrentTrain: epoch  1, batch    65 | loss: 5.1073422Losses:  5.234499931335449 0.5078995823860168
CurrentTrain: epoch  1, batch    66 | loss: 5.7423997Losses:  5.187227725982666 0.40587788820266724
CurrentTrain: epoch  1, batch    67 | loss: 5.5931058Losses:  4.851075172424316 0.2564586400985718
CurrentTrain: epoch  1, batch    68 | loss: 5.1075339Losses:  4.72886848449707 0.37126240134239197
CurrentTrain: epoch  1, batch    69 | loss: 5.1001310Losses:  5.440417766571045 0.6441829800605774
CurrentTrain: epoch  1, batch    70 | loss: 6.0846009Losses:  5.216864585876465 0.38158172369003296
CurrentTrain: epoch  1, batch    71 | loss: 5.5984464Losses:  4.974039077758789 0.30644190311431885
CurrentTrain: epoch  1, batch    72 | loss: 5.2804809Losses:  4.6724700927734375 0.27549660205841064
CurrentTrain: epoch  1, batch    73 | loss: 4.9479666Losses:  4.993422508239746 0.31584614515304565
CurrentTrain: epoch  1, batch    74 | loss: 5.3092685Losses:  5.778365612030029 0.5337165594100952
CurrentTrain: epoch  1, batch    75 | loss: 6.3120823Losses:  4.9865288734436035 0.3056606352329254
CurrentTrain: epoch  1, batch    76 | loss: 5.2921896Losses:  5.480874538421631 0.3230487108230591
CurrentTrain: epoch  1, batch    77 | loss: 5.8039231Losses:  4.96561336517334 0.27312541007995605
CurrentTrain: epoch  1, batch    78 | loss: 5.2387390Losses:  4.801530838012695 0.17360489070415497
CurrentTrain: epoch  1, batch    79 | loss: 4.9751358Losses:  4.952847480773926 0.2582044005393982
CurrentTrain: epoch  1, batch    80 | loss: 5.2110519Losses:  4.8979573249816895 0.33214104175567627
CurrentTrain: epoch  1, batch    81 | loss: 5.2300982Losses:  5.204797744750977 0.37642988562583923
CurrentTrain: epoch  1, batch    82 | loss: 5.5812278Losses:  4.573728561401367 0.20030537247657776
CurrentTrain: epoch  1, batch    83 | loss: 4.7740340Losses:  5.184291839599609 0.4206332266330719
CurrentTrain: epoch  1, batch    84 | loss: 5.6049252Losses:  4.673264503479004 0.21995243430137634
CurrentTrain: epoch  1, batch    85 | loss: 4.8932171Losses:  4.998690128326416 0.277750164270401
CurrentTrain: epoch  1, batch    86 | loss: 5.2764401Losses:  4.861237525939941 0.2780478000640869
CurrentTrain: epoch  1, batch    87 | loss: 5.1392851Losses:  5.206846237182617 0.2810455560684204
CurrentTrain: epoch  1, batch    88 | loss: 5.4878917Losses:  5.187535285949707 0.5187953114509583
CurrentTrain: epoch  1, batch    89 | loss: 5.7063308Losses:  5.517072677612305 0.13902735710144043
CurrentTrain: epoch  1, batch    90 | loss: 5.6561003Losses:  4.630248069763184 0.32247403264045715
CurrentTrain: epoch  1, batch    91 | loss: 4.9527221Losses:  4.779437065124512 0.4307641386985779
CurrentTrain: epoch  1, batch    92 | loss: 5.2102013Losses:  4.756453514099121 0.2862016558647156
CurrentTrain: epoch  1, batch    93 | loss: 5.0426550Losses:  5.260844707489014 0.31879767775535583
CurrentTrain: epoch  1, batch    94 | loss: 5.5796423Losses:  5.483268737792969 0.5287202596664429
CurrentTrain: epoch  1, batch    95 | loss: 6.0119891Losses:  4.647814750671387 0.16776710748672485
CurrentTrain: epoch  1, batch    96 | loss: 4.8155818Losses:  4.890326023101807 0.3425285816192627
CurrentTrain: epoch  1, batch    97 | loss: 5.2328548Losses:  4.815445899963379 0.4051400423049927
CurrentTrain: epoch  1, batch    98 | loss: 5.2205858Losses:  4.932480335235596 0.37620675563812256
CurrentTrain: epoch  1, batch    99 | loss: 5.3086872Losses:  4.7775044441223145 0.2866877019405365
CurrentTrain: epoch  1, batch   100 | loss: 5.0641923Losses:  4.861782073974609 0.4311861991882324
CurrentTrain: epoch  1, batch   101 | loss: 5.2929683Losses:  4.569155693054199 0.29467636346817017
CurrentTrain: epoch  1, batch   102 | loss: 4.8638320Losses:  4.618185997009277 0.27511200308799744
CurrentTrain: epoch  1, batch   103 | loss: 4.8932981Losses:  4.470671653747559 0.25317907333374023
CurrentTrain: epoch  1, batch   104 | loss: 4.7238507Losses:  5.021296501159668 0.2583882510662079
CurrentTrain: epoch  1, batch   105 | loss: 5.2796845Losses:  4.570729732513428 0.15996018052101135
CurrentTrain: epoch  1, batch   106 | loss: 4.7306900Losses:  4.887071132659912 0.23312321305274963
CurrentTrain: epoch  1, batch   107 | loss: 5.1201944Losses:  5.0896453857421875 0.315970242023468
CurrentTrain: epoch  1, batch   108 | loss: 5.4056158Losses:  4.642459869384766 0.39660125970840454
CurrentTrain: epoch  1, batch   109 | loss: 5.0390611Losses:  4.624421119689941 0.2713571786880493
CurrentTrain: epoch  1, batch   110 | loss: 4.8957782Losses:  4.545742034912109 0.281654953956604
CurrentTrain: epoch  1, batch   111 | loss: 4.8273969Losses:  4.439024925231934 0.3361542820930481
CurrentTrain: epoch  1, batch   112 | loss: 4.7751794Losses:  5.023805141448975 0.34484124183654785
CurrentTrain: epoch  1, batch   113 | loss: 5.3686466Losses:  4.634934425354004 0.17059947550296783
CurrentTrain: epoch  1, batch   114 | loss: 4.8055339Losses:  4.431872367858887 0.2780458927154541
CurrentTrain: epoch  1, batch   115 | loss: 4.7099180Losses:  4.886966705322266 0.4881626069545746
CurrentTrain: epoch  1, batch   116 | loss: 5.3751292Losses:  4.870574951171875 0.15453192591667175
CurrentTrain: epoch  1, batch   117 | loss: 5.0251069Losses:  5.327824592590332 0.3656660318374634
CurrentTrain: epoch  1, batch   118 | loss: 5.6934905Losses:  4.6606221199035645 0.2761116623878479
CurrentTrain: epoch  1, batch   119 | loss: 4.9367337Losses:  4.239352226257324 0.14914964139461517
CurrentTrain: epoch  1, batch   120 | loss: 4.3885016Losses:  4.415876865386963 0.3193148076534271
CurrentTrain: epoch  1, batch   121 | loss: 4.7351918Losses:  4.462764739990234 0.1129375696182251
CurrentTrain: epoch  1, batch   122 | loss: 4.5757022Losses:  4.354182243347168 0.26223164796829224
CurrentTrain: epoch  1, batch   123 | loss: 4.6164141Losses:  4.215095043182373 0.14649632573127747
CurrentTrain: epoch  1, batch   124 | loss: 4.3615913Losses:  4.71018123626709 0.31856417655944824
CurrentTrain: epoch  2, batch     0 | loss: 5.0287457Losses:  4.468380451202393 0.24495641887187958
CurrentTrain: epoch  2, batch     1 | loss: 4.7133369Losses:  4.430988788604736 0.34358441829681396
CurrentTrain: epoch  2, batch     2 | loss: 4.7745733Losses:  5.001273155212402 0.3903499245643616
CurrentTrain: epoch  2, batch     3 | loss: 5.3916230Losses:  4.686892509460449 0.27112478017807007
CurrentTrain: epoch  2, batch     4 | loss: 4.9580173Losses:  4.4446306228637695 0.23587562143802643
CurrentTrain: epoch  2, batch     5 | loss: 4.6805062Losses:  4.446126937866211 0.19808512926101685
CurrentTrain: epoch  2, batch     6 | loss: 4.6442122Losses:  4.915175437927246 0.211463063955307
CurrentTrain: epoch  2, batch     7 | loss: 5.1266384Losses:  4.240390777587891 0.22327712178230286
CurrentTrain: epoch  2, batch     8 | loss: 4.4636679Losses:  4.3037919998168945 0.25315749645233154
CurrentTrain: epoch  2, batch     9 | loss: 4.5569496Losses:  5.582779884338379 0.17258217930793762
CurrentTrain: epoch  2, batch    10 | loss: 5.7553620Losses:  4.49686861038208 0.1891898810863495
CurrentTrain: epoch  2, batch    11 | loss: 4.6860585Losses:  4.598506927490234 0.26282358169555664
CurrentTrain: epoch  2, batch    12 | loss: 4.8613305Losses:  4.5331220626831055 0.2544696033000946
CurrentTrain: epoch  2, batch    13 | loss: 4.7875915Losses:  4.288500785827637 0.1550237089395523
CurrentTrain: epoch  2, batch    14 | loss: 4.4435244Losses:  4.317710876464844 0.24152827262878418
CurrentTrain: epoch  2, batch    15 | loss: 4.5592394Losses:  4.342899799346924 0.19329825043678284
CurrentTrain: epoch  2, batch    16 | loss: 4.5361981Losses:  4.123037815093994 0.10705466568470001
CurrentTrain: epoch  2, batch    17 | loss: 4.2300925Losses:  4.310703277587891 0.0997694581747055
CurrentTrain: epoch  2, batch    18 | loss: 4.4104729Losses:  4.346383094787598 0.16516628861427307
CurrentTrain: epoch  2, batch    19 | loss: 4.5115495Losses:  4.277310371398926 0.12319088727235794
CurrentTrain: epoch  2, batch    20 | loss: 4.4005013Losses:  4.4355692863464355 0.13018706440925598
CurrentTrain: epoch  2, batch    21 | loss: 4.5657563Losses:  4.197922706604004 0.15882804989814758
CurrentTrain: epoch  2, batch    22 | loss: 4.3567510Losses:  4.821292877197266 0.32107144594192505
CurrentTrain: epoch  2, batch    23 | loss: 5.1423645Losses:  4.632730484008789 0.18567563593387604
CurrentTrain: epoch  2, batch    24 | loss: 4.8184061Losses:  4.749853134155273 0.22932803630828857
CurrentTrain: epoch  2, batch    25 | loss: 4.9791813Losses:  4.627276420593262 0.1752839982509613
CurrentTrain: epoch  2, batch    26 | loss: 4.8025603Losses:  4.413901329040527 0.21681784093379974
CurrentTrain: epoch  2, batch    27 | loss: 4.6307192Losses:  4.645596504211426 0.1860601156949997
CurrentTrain: epoch  2, batch    28 | loss: 4.8316565Losses:  4.0853376388549805 0.13261497020721436
CurrentTrain: epoch  2, batch    29 | loss: 4.2179527Losses:  4.465724945068359 0.22871920466423035
CurrentTrain: epoch  2, batch    30 | loss: 4.6944442Losses:  4.345195770263672 0.19087925553321838
CurrentTrain: epoch  2, batch    31 | loss: 4.5360751Losses:  4.582211494445801 0.276489794254303
CurrentTrain: epoch  2, batch    32 | loss: 4.8587012Losses:  4.499929904937744 0.23138783872127533
CurrentTrain: epoch  2, batch    33 | loss: 4.7313175Losses:  4.278379440307617 0.22997432947158813
CurrentTrain: epoch  2, batch    34 | loss: 4.5083537Losses:  4.417333602905273 0.26839834451675415
CurrentTrain: epoch  2, batch    35 | loss: 4.6857319Losses:  4.330813884735107 0.22902314364910126
CurrentTrain: epoch  2, batch    36 | loss: 4.5598369Losses:  4.28873348236084 0.1890874207019806
CurrentTrain: epoch  2, batch    37 | loss: 4.4778209Losses:  4.336400985717773 0.18742568790912628
CurrentTrain: epoch  2, batch    38 | loss: 4.5238266Losses:  4.693173885345459 0.3137705624103546
CurrentTrain: epoch  2, batch    39 | loss: 5.0069447Losses:  4.738245964050293 0.2995870113372803
CurrentTrain: epoch  2, batch    40 | loss: 5.0378332Losses:  4.847622394561768 0.24167335033416748
CurrentTrain: epoch  2, batch    41 | loss: 5.0892959Losses:  4.354959487915039 0.20035645365715027
CurrentTrain: epoch  2, batch    42 | loss: 4.5553160Losses:  4.274068355560303 0.22926589846611023
CurrentTrain: epoch  2, batch    43 | loss: 4.5033340Losses:  4.517683982849121 0.13657715916633606
CurrentTrain: epoch  2, batch    44 | loss: 4.6542611Losses:  4.40135383605957 0.17946571111679077
CurrentTrain: epoch  2, batch    45 | loss: 4.5808196Losses:  4.540604591369629 0.2538270652294159
CurrentTrain: epoch  2, batch    46 | loss: 4.7944317Losses:  4.360271453857422 0.1107586994767189
CurrentTrain: epoch  2, batch    47 | loss: 4.4710302Losses:  4.248218059539795 0.11643381416797638
CurrentTrain: epoch  2, batch    48 | loss: 4.3646517Losses:  4.546268463134766 0.13277685642242432
CurrentTrain: epoch  2, batch    49 | loss: 4.6790452Losses:  4.678391456604004 0.23772674798965454
CurrentTrain: epoch  2, batch    50 | loss: 4.9161181Losses:  4.142951011657715 0.1001194417476654
CurrentTrain: epoch  2, batch    51 | loss: 4.2430706Losses:  4.647651672363281 0.1525936722755432
CurrentTrain: epoch  2, batch    52 | loss: 4.8002453Losses:  4.443193435668945 0.1589069366455078
CurrentTrain: epoch  2, batch    53 | loss: 4.6021004Losses:  4.366728782653809 0.18409636616706848
CurrentTrain: epoch  2, batch    54 | loss: 4.5508251Losses:  4.149596214294434 0.1485564410686493
CurrentTrain: epoch  2, batch    55 | loss: 4.2981524Losses:  4.392067909240723 0.1426149606704712
CurrentTrain: epoch  2, batch    56 | loss: 4.5346828Losses:  4.47717809677124 0.2689889073371887
CurrentTrain: epoch  2, batch    57 | loss: 4.7461672Losses:  4.656464576721191 0.25716155767440796
CurrentTrain: epoch  2, batch    58 | loss: 4.9136262Losses:  4.2046661376953125 0.14574211835861206
CurrentTrain: epoch  2, batch    59 | loss: 4.3504081Losses:  4.369755744934082 0.1339036524295807
CurrentTrain: epoch  2, batch    60 | loss: 4.5036592Losses:  4.452545166015625 0.24819475412368774
CurrentTrain: epoch  2, batch    61 | loss: 4.7007399Losses:  4.705705642700195 0.1612287312746048
CurrentTrain: epoch  2, batch    62 | loss: 4.8669343Losses:  4.308078765869141 0.20484092831611633
CurrentTrain: epoch  2, batch    63 | loss: 4.5129199Losses:  4.350366592407227 0.09323199838399887
CurrentTrain: epoch  2, batch    64 | loss: 4.4435987Losses:  4.38131046295166 0.09148715436458588
CurrentTrain: epoch  2, batch    65 | loss: 4.4727974Losses:  4.37437105178833 0.18713022768497467
CurrentTrain: epoch  2, batch    66 | loss: 4.5615015Losses:  4.268211364746094 0.1433030068874359
CurrentTrain: epoch  2, batch    67 | loss: 4.4115143Losses:  4.455244064331055 0.2048705667257309
CurrentTrain: epoch  2, batch    68 | loss: 4.6601148Losses:  4.168751239776611 0.12132874876260757
CurrentTrain: epoch  2, batch    69 | loss: 4.2900801Losses:  4.318425178527832 0.21213334798812866
CurrentTrain: epoch  2, batch    70 | loss: 4.5305586Losses:  4.252714157104492 0.1542549431324005
CurrentTrain: epoch  2, batch    71 | loss: 4.4069691Losses:  4.415805816650391 0.20046302676200867
CurrentTrain: epoch  2, batch    72 | loss: 4.6162686Losses:  4.2693281173706055 0.16392871737480164
CurrentTrain: epoch  2, batch    73 | loss: 4.4332566Losses:  4.21801233291626 0.23105746507644653
CurrentTrain: epoch  2, batch    74 | loss: 4.4490700Losses:  4.264118194580078 0.10162794589996338
CurrentTrain: epoch  2, batch    75 | loss: 4.3657460Losses:  4.367430686950684 0.22950199246406555
CurrentTrain: epoch  2, batch    76 | loss: 4.5969329Losses:  4.377340793609619 0.22865591943264008
CurrentTrain: epoch  2, batch    77 | loss: 4.6059966Losses:  4.272539138793945 0.13369759917259216
CurrentTrain: epoch  2, batch    78 | loss: 4.4062366Losses:  4.193759918212891 0.09893914312124252
CurrentTrain: epoch  2, batch    79 | loss: 4.2926989Losses:  4.4109954833984375 0.21619722247123718
CurrentTrain: epoch  2, batch    80 | loss: 4.6271925Losses:  4.381771564483643 0.2364654839038849
CurrentTrain: epoch  2, batch    81 | loss: 4.6182370Losses:  4.3139472007751465 0.15256769955158234
CurrentTrain: epoch  2, batch    82 | loss: 4.4665151Losses:  4.413453102111816 0.15836767852306366
CurrentTrain: epoch  2, batch    83 | loss: 4.5718207Losses:  4.509121894836426 0.2186344712972641
CurrentTrain: epoch  2, batch    84 | loss: 4.7277565Losses:  4.125190734863281 0.1610822081565857
CurrentTrain: epoch  2, batch    85 | loss: 4.2862730Losses:  4.335358619689941 0.12215743958950043
CurrentTrain: epoch  2, batch    86 | loss: 4.4575162Losses:  4.318507194519043 0.11373087763786316
CurrentTrain: epoch  2, batch    87 | loss: 4.4322381Losses:  4.955938339233398 0.14967744052410126
CurrentTrain: epoch  2, batch    88 | loss: 5.1056156Losses:  4.126994609832764 0.13325010240077972
CurrentTrain: epoch  2, batch    89 | loss: 4.2602448Losses:  4.5297346115112305 0.22234852612018585
CurrentTrain: epoch  2, batch    90 | loss: 4.7520833Losses:  4.3239336013793945 0.12667852640151978
CurrentTrain: epoch  2, batch    91 | loss: 4.4506121Losses:  4.294538974761963 0.23452557623386383
CurrentTrain: epoch  2, batch    92 | loss: 4.5290647Losses:  4.417182922363281 0.26690050959587097
CurrentTrain: epoch  2, batch    93 | loss: 4.6840835Losses:  4.159373760223389 0.09545911103487015
CurrentTrain: epoch  2, batch    94 | loss: 4.2548327Losses:  4.178656101226807 0.17526975274085999
CurrentTrain: epoch  2, batch    95 | loss: 4.3539257Losses:  4.2946977615356445 0.11201965063810349
CurrentTrain: epoch  2, batch    96 | loss: 4.4067173Losses:  4.187143325805664 0.16515956819057465
CurrentTrain: epoch  2, batch    97 | loss: 4.3523030Losses:  4.161233425140381 0.10248047113418579
CurrentTrain: epoch  2, batch    98 | loss: 4.2637138Losses:  4.5063276290893555 0.2176390439271927
CurrentTrain: epoch  2, batch    99 | loss: 4.7239666Losses:  5.090249538421631 0.37597572803497314
CurrentTrain: epoch  2, batch   100 | loss: 5.4662251Losses:  4.748156547546387 0.12572255730628967
CurrentTrain: epoch  2, batch   101 | loss: 4.8738790Losses:  5.187277793884277 0.16842928528785706
CurrentTrain: epoch  2, batch   102 | loss: 5.3557072Losses:  4.123394012451172 0.1102856770157814
CurrentTrain: epoch  2, batch   103 | loss: 4.2336798Losses:  4.190568923950195 0.141700878739357
CurrentTrain: epoch  2, batch   104 | loss: 4.3322697Losses:  4.223992347717285 0.15849652886390686
CurrentTrain: epoch  2, batch   105 | loss: 4.3824887Losses:  4.8167219161987305 0.1849285066127777
CurrentTrain: epoch  2, batch   106 | loss: 5.0016503Losses:  4.356722831726074 0.12909960746765137
CurrentTrain: epoch  2, batch   107 | loss: 4.4858227Losses:  4.514062881469727 0.18168948590755463
CurrentTrain: epoch  2, batch   108 | loss: 4.6957521Losses:  4.25140380859375 0.12958523631095886
CurrentTrain: epoch  2, batch   109 | loss: 4.3809891Losses:  4.205071926116943 0.10256941616535187
CurrentTrain: epoch  2, batch   110 | loss: 4.3076415Losses:  4.1728105545043945 0.14319393038749695
CurrentTrain: epoch  2, batch   111 | loss: 4.3160043Losses:  4.409774303436279 0.18999305367469788
CurrentTrain: epoch  2, batch   112 | loss: 4.5997672Losses:  4.16865873336792 0.0885164886713028
CurrentTrain: epoch  2, batch   113 | loss: 4.2571754Losses:  4.203470706939697 0.15487465262413025
CurrentTrain: epoch  2, batch   114 | loss: 4.3583455Losses:  4.30218505859375 0.1815670132637024
CurrentTrain: epoch  2, batch   115 | loss: 4.4837523Losses:  4.2304558753967285 0.0752527117729187
CurrentTrain: epoch  2, batch   116 | loss: 4.3057084Losses:  4.145122528076172 0.1670294553041458
CurrentTrain: epoch  2, batch   117 | loss: 4.3121519Losses:  4.058385372161865 0.07820656895637512
CurrentTrain: epoch  2, batch   118 | loss: 4.1365919Losses:  4.202945232391357 0.16701894998550415
CurrentTrain: epoch  2, batch   119 | loss: 4.3699641Losses:  4.32999849319458 0.15794068574905396
CurrentTrain: epoch  2, batch   120 | loss: 4.4879394Losses:  4.24260139465332 0.1292126178741455
CurrentTrain: epoch  2, batch   121 | loss: 4.3718138Losses:  4.311955451965332 0.14666613936424255
CurrentTrain: epoch  2, batch   122 | loss: 4.4586215Losses:  4.2361602783203125 0.16220209002494812
CurrentTrain: epoch  2, batch   123 | loss: 4.3983622Losses:  4.193116188049316 0.196755513548851
CurrentTrain: epoch  2, batch   124 | loss: 4.3898716Losses:  4.156110763549805 0.11828465759754181
CurrentTrain: epoch  3, batch     0 | loss: 4.2743955Losses:  4.117405414581299 0.17586183547973633
CurrentTrain: epoch  3, batch     1 | loss: 4.2932673Losses:  4.275847434997559 0.18046468496322632
CurrentTrain: epoch  3, batch     2 | loss: 4.4563122Losses:  4.185074806213379 0.14512985944747925
CurrentTrain: epoch  3, batch     3 | loss: 4.3302045Losses:  4.176508903503418 0.056261200457811356
CurrentTrain: epoch  3, batch     4 | loss: 4.2327700Losses:  4.222365856170654 0.15407001972198486
CurrentTrain: epoch  3, batch     5 | loss: 4.3764358Losses:  4.228034973144531 0.09305396676063538
CurrentTrain: epoch  3, batch     6 | loss: 4.3210888Losses:  4.128123760223389 0.12898586690425873
CurrentTrain: epoch  3, batch     7 | loss: 4.2571096Losses:  4.2371721267700195 0.2612592279911041
CurrentTrain: epoch  3, batch     8 | loss: 4.4984312Losses:  4.037250518798828 0.08017296344041824
CurrentTrain: epoch  3, batch     9 | loss: 4.1174235Losses:  3.980776071548462 0.08522814512252808
CurrentTrain: epoch  3, batch    10 | loss: 4.0660043Losses:  4.149357795715332 0.13530151546001434
CurrentTrain: epoch  3, batch    11 | loss: 4.2846594Losses:  4.115623474121094 0.15524537861347198
CurrentTrain: epoch  3, batch    12 | loss: 4.2708688Losses:  4.287540435791016 0.14665251970291138
CurrentTrain: epoch  3, batch    13 | loss: 4.4341931Losses:  4.222517013549805 0.1287466436624527
CurrentTrain: epoch  3, batch    14 | loss: 4.3512635Losses:  4.17819356918335 0.14449827373027802
CurrentTrain: epoch  3, batch    15 | loss: 4.3226919Losses:  4.1820068359375 0.18415862321853638
CurrentTrain: epoch  3, batch    16 | loss: 4.3661656Losses:  4.248776435852051 0.1268637478351593
CurrentTrain: epoch  3, batch    17 | loss: 4.3756404Losses:  4.068882942199707 0.17163506150245667
CurrentTrain: epoch  3, batch    18 | loss: 4.2405181Losses:  4.247758865356445 0.23221305012702942
CurrentTrain: epoch  3, batch    19 | loss: 4.4799719Losses:  4.120368003845215 0.11058729887008667
CurrentTrain: epoch  3, batch    20 | loss: 4.2309551Losses:  4.111044406890869 0.12231858819723129
CurrentTrain: epoch  3, batch    21 | loss: 4.2333632Losses:  4.233675956726074 0.16700848937034607
CurrentTrain: epoch  3, batch    22 | loss: 4.4006844Losses:  4.198861122131348 0.0892583429813385
CurrentTrain: epoch  3, batch    23 | loss: 4.2881193Losses:  4.12564754486084 0.1739691197872162
CurrentTrain: epoch  3, batch    24 | loss: 4.2996168Losses:  4.17117977142334 0.12239161133766174
CurrentTrain: epoch  3, batch    25 | loss: 4.2935715Losses:  4.228568077087402 0.053474001586437225
CurrentTrain: epoch  3, batch    26 | loss: 4.2820420Losses:  4.165667533874512 0.16094501316547394
CurrentTrain: epoch  3, batch    27 | loss: 4.3266125Losses:  4.095468521118164 0.10302107036113739
CurrentTrain: epoch  3, batch    28 | loss: 4.1984897Losses:  4.144774913787842 0.10572870075702667
CurrentTrain: epoch  3, batch    29 | loss: 4.2505035Losses:  4.117875099182129 0.10835465043783188
CurrentTrain: epoch  3, batch    30 | loss: 4.2262297Losses:  4.222395896911621 0.0937102809548378
CurrentTrain: epoch  3, batch    31 | loss: 4.3161063Losses:  4.2436676025390625 0.15473777055740356
CurrentTrain: epoch  3, batch    32 | loss: 4.3984056Losses:  4.114755153656006 0.03727427124977112
CurrentTrain: epoch  3, batch    33 | loss: 4.1520295Losses:  4.152651786804199 0.08558085560798645
CurrentTrain: epoch  3, batch    34 | loss: 4.2382326Losses:  4.087210655212402 0.1296703815460205
CurrentTrain: epoch  3, batch    35 | loss: 4.2168808Losses:  4.036982536315918 0.14821040630340576
CurrentTrain: epoch  3, batch    36 | loss: 4.1851931Losses:  5.463810920715332 0.5408246517181396
CurrentTrain: epoch  3, batch    37 | loss: 6.0046358Losses:  4.026218891143799 0.15843519568443298
CurrentTrain: epoch  3, batch    38 | loss: 4.1846542Losses:  4.106365203857422 0.16537535190582275
CurrentTrain: epoch  3, batch    39 | loss: 4.2717404Losses:  4.081013202667236 0.16248425841331482
CurrentTrain: epoch  3, batch    40 | loss: 4.2434974Losses:  4.254195690155029 0.1773812174797058
CurrentTrain: epoch  3, batch    41 | loss: 4.4315767Losses:  4.493519306182861 0.20887035131454468
CurrentTrain: epoch  3, batch    42 | loss: 4.7023897Losses:  4.056053161621094 0.1413263976573944
CurrentTrain: epoch  3, batch    43 | loss: 4.1973796Losses:  4.106889247894287 0.1296233981847763
CurrentTrain: epoch  3, batch    44 | loss: 4.2365127Losses:  4.148569107055664 0.19672539830207825
CurrentTrain: epoch  3, batch    45 | loss: 4.3452945Losses:  4.052257537841797 0.07241535931825638
CurrentTrain: epoch  3, batch    46 | loss: 4.1246729Losses:  4.072871208190918 0.09456802904605865
CurrentTrain: epoch  3, batch    47 | loss: 4.1674395Losses:  4.164442539215088 0.1402556598186493
CurrentTrain: epoch  3, batch    48 | loss: 4.3046980Losses:  4.204944610595703 0.19906800985336304
CurrentTrain: epoch  3, batch    49 | loss: 4.4040127Losses:  4.0478973388671875 0.05007870867848396
CurrentTrain: epoch  3, batch    50 | loss: 4.0979762Losses:  4.011621475219727 0.08969196677207947
CurrentTrain: epoch  3, batch    51 | loss: 4.1013136Losses:  4.282536506652832 0.171556293964386
CurrentTrain: epoch  3, batch    52 | loss: 4.4540930Losses:  4.180249214172363 0.19200806319713593
CurrentTrain: epoch  3, batch    53 | loss: 4.3722572Losses:  4.473309516906738 0.16381773352622986
CurrentTrain: epoch  3, batch    54 | loss: 4.6371274Losses:  4.046686172485352 0.0935223251581192
CurrentTrain: epoch  3, batch    55 | loss: 4.1402087Losses:  4.2099103927612305 0.1532392054796219
CurrentTrain: epoch  3, batch    56 | loss: 4.3631496Losses:  4.204744338989258 0.1829306185245514
CurrentTrain: epoch  3, batch    57 | loss: 4.3876748Losses:  4.133089542388916 0.19313453137874603
CurrentTrain: epoch  3, batch    58 | loss: 4.3262239Losses:  4.109381675720215 0.1671222448348999
CurrentTrain: epoch  3, batch    59 | loss: 4.2765040Losses:  4.218328952789307 0.19279369711875916
CurrentTrain: epoch  3, batch    60 | loss: 4.4111228Losses:  4.0745768547058105 0.14975491166114807
CurrentTrain: epoch  3, batch    61 | loss: 4.2243319Losses:  4.201781749725342 0.09571516513824463
CurrentTrain: epoch  3, batch    62 | loss: 4.2974968Losses:  4.123564720153809 0.07654617726802826
CurrentTrain: epoch  3, batch    63 | loss: 4.2001109Losses:  4.205117702484131 0.12737154960632324
CurrentTrain: epoch  3, batch    64 | loss: 4.3324890Losses:  4.097826957702637 0.07348684966564178
CurrentTrain: epoch  3, batch    65 | loss: 4.1713138Losses:  4.066242218017578 0.09247693419456482
CurrentTrain: epoch  3, batch    66 | loss: 4.1587191Losses:  4.07352352142334 0.10530555993318558
CurrentTrain: epoch  3, batch    67 | loss: 4.1788292Losses:  4.27653169631958 0.10256591439247131
CurrentTrain: epoch  3, batch    68 | loss: 4.3790975Losses:  4.073108673095703 0.17959292232990265
CurrentTrain: epoch  3, batch    69 | loss: 4.2527018Losses:  4.063909530639648 0.07407589256763458
CurrentTrain: epoch  3, batch    70 | loss: 4.1379852Losses:  4.13325834274292 0.07819455116987228
CurrentTrain: epoch  3, batch    71 | loss: 4.2114530Losses:  4.32629919052124 0.19166089594364166
CurrentTrain: epoch  3, batch    72 | loss: 4.5179601Losses:  4.150490760803223 0.1427614539861679
CurrentTrain: epoch  3, batch    73 | loss: 4.2932520Losses:  4.08137321472168 0.11173765361309052
CurrentTrain: epoch  3, batch    74 | loss: 4.1931109Losses:  4.247633934020996 0.08373594284057617
CurrentTrain: epoch  3, batch    75 | loss: 4.3313699Losses:  4.153701305389404 0.15098534524440765
CurrentTrain: epoch  3, batch    76 | loss: 4.3046865Losses:  4.1707987785339355 0.0982070043683052
CurrentTrain: epoch  3, batch    77 | loss: 4.2690058Losses:  4.169557571411133 0.1951403021812439
CurrentTrain: epoch  3, batch    78 | loss: 4.3646979Losses:  4.192132949829102 0.1392514407634735
CurrentTrain: epoch  3, batch    79 | loss: 4.3313842Losses:  4.515812397003174 0.21911388635635376
CurrentTrain: epoch  3, batch    80 | loss: 4.7349262Losses:  4.11911678314209 0.22577303647994995
CurrentTrain: epoch  3, batch    81 | loss: 4.3448896Losses:  4.025783538818359 0.0798945501446724
CurrentTrain: epoch  3, batch    82 | loss: 4.1056781Losses:  4.14216947555542 0.0863378494977951
CurrentTrain: epoch  3, batch    83 | loss: 4.2285075Losses:  4.202602386474609 0.1053999811410904
CurrentTrain: epoch  3, batch    84 | loss: 4.3080025Losses:  4.066268444061279 0.15553081035614014
CurrentTrain: epoch  3, batch    85 | loss: 4.2217994Losses:  4.209289073944092 0.13701286911964417
CurrentTrain: epoch  3, batch    86 | loss: 4.3463020Losses:  4.0154194831848145 0.07473745942115784
CurrentTrain: epoch  3, batch    87 | loss: 4.0901570Losses:  4.092254161834717 0.06172969192266464
CurrentTrain: epoch  3, batch    88 | loss: 4.1539841Losses:  4.1039204597473145 0.09793378412723541
CurrentTrain: epoch  3, batch    89 | loss: 4.2018542Losses:  4.100710868835449 0.07046373188495636
CurrentTrain: epoch  3, batch    90 | loss: 4.1711745Losses:  4.221547603607178 0.16126248240470886
CurrentTrain: epoch  3, batch    91 | loss: 4.3828101Losses:  4.080801963806152 0.06656238436698914
CurrentTrain: epoch  3, batch    92 | loss: 4.1473641Losses:  4.144598960876465 0.1255015730857849
CurrentTrain: epoch  3, batch    93 | loss: 4.2701006Losses:  4.141702651977539 0.173139750957489
CurrentTrain: epoch  3, batch    94 | loss: 4.3148422Losses:  4.117300033569336 0.14581337571144104
CurrentTrain: epoch  3, batch    95 | loss: 4.2631135Losses:  4.099855422973633 0.08640606701374054
CurrentTrain: epoch  3, batch    96 | loss: 4.1862617Losses:  4.030127048492432 0.07854510843753815
CurrentTrain: epoch  3, batch    97 | loss: 4.1086721Losses:  4.066344261169434 0.13969269394874573
CurrentTrain: epoch  3, batch    98 | loss: 4.2060370Losses:  4.218404769897461 0.16062086820602417
CurrentTrain: epoch  3, batch    99 | loss: 4.3790255Losses:  4.1464433670043945 0.08920441567897797
CurrentTrain: epoch  3, batch   100 | loss: 4.2356477Losses:  4.043787002563477 0.12417145073413849
CurrentTrain: epoch  3, batch   101 | loss: 4.1679583Losses:  4.190866947174072 0.12071535736322403
CurrentTrain: epoch  3, batch   102 | loss: 4.3115821Losses:  4.116581439971924 0.10278701782226562
CurrentTrain: epoch  3, batch   103 | loss: 4.2193685Losses:  4.061407089233398 0.1931343972682953
CurrentTrain: epoch  3, batch   104 | loss: 4.2545414Losses:  3.948608160018921 0.09484843909740448
CurrentTrain: epoch  3, batch   105 | loss: 4.0434566Losses:  4.122097969055176 0.05611342191696167
CurrentTrain: epoch  3, batch   106 | loss: 4.1782112Losses:  4.0516157150268555 0.1395784616470337
CurrentTrain: epoch  3, batch   107 | loss: 4.1911941Losses:  4.1998724937438965 0.0750252828001976
CurrentTrain: epoch  3, batch   108 | loss: 4.2748976Losses:  4.119112491607666 0.1004057228565216
CurrentTrain: epoch  3, batch   109 | loss: 4.2195182Losses:  4.040541648864746 0.07062734663486481
CurrentTrain: epoch  3, batch   110 | loss: 4.1111689Losses:  4.028228759765625 0.058249302208423615
CurrentTrain: epoch  3, batch   111 | loss: 4.0864782Losses:  4.039335250854492 0.08784039318561554
CurrentTrain: epoch  3, batch   112 | loss: 4.1271758Losses:  4.072147846221924 0.03267969563603401
CurrentTrain: epoch  3, batch   113 | loss: 4.1048274Losses:  4.031338691711426 0.052103228867053986
CurrentTrain: epoch  3, batch   114 | loss: 4.0834417Losses:  4.025649547576904 0.11050502955913544
CurrentTrain: epoch  3, batch   115 | loss: 4.1361547Losses:  3.9348108768463135 0.0377781018614769
CurrentTrain: epoch  3, batch   116 | loss: 3.9725890Losses:  4.030827522277832 0.1300487518310547
CurrentTrain: epoch  3, batch   117 | loss: 4.1608763Losses:  3.958057403564453 0.030204787850379944
CurrentTrain: epoch  3, batch   118 | loss: 3.9882622Losses:  4.0103912353515625 0.1782102733850479
CurrentTrain: epoch  3, batch   119 | loss: 4.1886015Losses:  4.049041748046875 0.128927543759346
CurrentTrain: epoch  3, batch   120 | loss: 4.1779695Losses:  3.9623022079467773 0.07479920238256454
CurrentTrain: epoch  3, batch   121 | loss: 4.0371013Losses:  4.101717948913574 0.08879552781581879
CurrentTrain: epoch  3, batch   122 | loss: 4.1905136Losses:  4.192770481109619 0.09547274559736252
CurrentTrain: epoch  3, batch   123 | loss: 4.2882433Losses:  4.057500839233398 0.11764006316661835
CurrentTrain: epoch  3, batch   124 | loss: 4.1751409Losses:  4.008386611938477 0.07265892624855042
CurrentTrain: epoch  4, batch     0 | loss: 4.0810456Losses:  4.065618515014648 0.15061886608600616
CurrentTrain: epoch  4, batch     1 | loss: 4.2162375Losses:  4.095869541168213 0.07554784417152405
CurrentTrain: epoch  4, batch     2 | loss: 4.1714172Losses:  4.050686359405518 0.07066845148801804
CurrentTrain: epoch  4, batch     3 | loss: 4.1213546Losses:  4.037357807159424 0.054708532989025116
CurrentTrain: epoch  4, batch     4 | loss: 4.0920663Losses:  4.075376510620117 0.09867522120475769
CurrentTrain: epoch  4, batch     5 | loss: 4.1740518Losses:  4.091465473175049 0.08016062527894974
CurrentTrain: epoch  4, batch     6 | loss: 4.1716261Losses:  4.052243232727051 0.12489888817071915
CurrentTrain: epoch  4, batch     7 | loss: 4.1771421Losses:  4.023613929748535 0.10516930371522903
CurrentTrain: epoch  4, batch     8 | loss: 4.1287832Losses:  3.954488515853882 0.07928114384412766
CurrentTrain: epoch  4, batch     9 | loss: 4.0337696Losses:  3.9925661087036133 0.14806391298770905
CurrentTrain: epoch  4, batch    10 | loss: 4.1406302Losses:  4.004060745239258 0.05242179334163666
CurrentTrain: epoch  4, batch    11 | loss: 4.0564823Losses:  4.055293560028076 0.12469833344221115
CurrentTrain: epoch  4, batch    12 | loss: 4.1799917Losses:  4.018150806427002 0.08196312189102173
CurrentTrain: epoch  4, batch    13 | loss: 4.1001139Losses:  4.066354274749756 0.10100213438272476
CurrentTrain: epoch  4, batch    14 | loss: 4.1673565Losses:  4.073705673217773 0.12398117780685425
CurrentTrain: epoch  4, batch    15 | loss: 4.1976867Losses:  4.049768447875977 0.08653078973293304
CurrentTrain: epoch  4, batch    16 | loss: 4.1362991Losses:  4.072339057922363 0.14311107993125916
CurrentTrain: epoch  4, batch    17 | loss: 4.2154503Losses:  4.046895980834961 0.12431753426790237
CurrentTrain: epoch  4, batch    18 | loss: 4.1712136Losses:  4.030872344970703 0.12964105606079102
CurrentTrain: epoch  4, batch    19 | loss: 4.1605134Losses:  4.0181989669799805 0.08468268066644669
CurrentTrain: epoch  4, batch    20 | loss: 4.1028814Losses:  4.07269287109375 0.05939542129635811
CurrentTrain: epoch  4, batch    21 | loss: 4.1320882Losses:  4.043121337890625 0.10750660300254822
CurrentTrain: epoch  4, batch    22 | loss: 4.1506281Losses:  4.092531204223633 0.12766285240650177
CurrentTrain: epoch  4, batch    23 | loss: 4.2201939Losses:  4.1177544593811035 0.12366856634616852
CurrentTrain: epoch  4, batch    24 | loss: 4.2414231Losses:  4.0180206298828125 0.08567409217357635
CurrentTrain: epoch  4, batch    25 | loss: 4.1036949Losses:  4.0290069580078125 0.07835574448108673
CurrentTrain: epoch  4, batch    26 | loss: 4.1073627Losses:  4.012018203735352 0.08686664700508118
CurrentTrain: epoch  4, batch    27 | loss: 4.0988851Losses:  4.076148509979248 0.153889462351799
CurrentTrain: epoch  4, batch    28 | loss: 4.2300382Losses:  4.059412956237793 0.12276805937290192
CurrentTrain: epoch  4, batch    29 | loss: 4.1821809Losses:  4.032232284545898 0.1277853548526764
CurrentTrain: epoch  4, batch    30 | loss: 4.1600175Losses:  4.303386211395264 0.1294543445110321
CurrentTrain: epoch  4, batch    31 | loss: 4.4328403Losses:  4.05160665512085 0.11007596552371979
CurrentTrain: epoch  4, batch    32 | loss: 4.1616826Losses:  4.002039909362793 0.10657607018947601
CurrentTrain: epoch  4, batch    33 | loss: 4.1086159Losses:  4.026257514953613 0.07166788727045059
CurrentTrain: epoch  4, batch    34 | loss: 4.0979252Losses:  4.015744209289551 0.05241815373301506
CurrentTrain: epoch  4, batch    35 | loss: 4.0681624Losses:  4.108180046081543 0.12625858187675476
CurrentTrain: epoch  4, batch    36 | loss: 4.2344384Losses:  4.1121344566345215 0.10490074753761292
CurrentTrain: epoch  4, batch    37 | loss: 4.2170353Losses:  4.043292045593262 0.12318149954080582
CurrentTrain: epoch  4, batch    38 | loss: 4.1664734Losses:  4.021925926208496 0.15545639395713806
CurrentTrain: epoch  4, batch    39 | loss: 4.1773825Losses:  4.010163307189941 0.11093427985906601
CurrentTrain: epoch  4, batch    40 | loss: 4.1210976Losses:  4.0021162033081055 0.05629957839846611
CurrentTrain: epoch  4, batch    41 | loss: 4.0584159Losses:  4.034943103790283 0.05431085824966431
CurrentTrain: epoch  4, batch    42 | loss: 4.0892539Losses:  4.056304931640625 0.06312495470046997
CurrentTrain: epoch  4, batch    43 | loss: 4.1194301Losses:  3.990217685699463 0.0862501934170723
CurrentTrain: epoch  4, batch    44 | loss: 4.0764680Losses:  4.001896858215332 0.04023180902004242
CurrentTrain: epoch  4, batch    45 | loss: 4.0421286Losses:  4.10909366607666 0.10278543829917908
CurrentTrain: epoch  4, batch    46 | loss: 4.2118793Losses:  4.024497032165527 0.09408016502857208
CurrentTrain: epoch  4, batch    47 | loss: 4.1185770Losses:  4.046091079711914 0.10369130969047546
CurrentTrain: epoch  4, batch    48 | loss: 4.1497822Losses:  4.071238994598389 0.09031225740909576
CurrentTrain: epoch  4, batch    49 | loss: 4.1615515Losses:  4.014449119567871 0.06625425070524216
CurrentTrain: epoch  4, batch    50 | loss: 4.0807033Losses:  4.091137409210205 0.13691812753677368
CurrentTrain: epoch  4, batch    51 | loss: 4.2280555Losses:  4.099687576293945 0.05300972983241081
CurrentTrain: epoch  4, batch    52 | loss: 4.1526971Losses:  4.017152786254883 0.044918276369571686
CurrentTrain: epoch  4, batch    53 | loss: 4.0620708Losses:  3.933443069458008 0.12146294116973877
CurrentTrain: epoch  4, batch    54 | loss: 4.0549059Losses:  4.002831935882568 0.11875836551189423
CurrentTrain: epoch  4, batch    55 | loss: 4.1215901Losses:  4.015006065368652 0.10007412731647491
CurrentTrain: epoch  4, batch    56 | loss: 4.1150804Losses:  4.043997287750244 0.12599042057991028
CurrentTrain: epoch  4, batch    57 | loss: 4.1699877Losses:  3.9687414169311523 0.09057468175888062
CurrentTrain: epoch  4, batch    58 | loss: 4.0593162Losses:  3.9833273887634277 0.03151611611247063
CurrentTrain: epoch  4, batch    59 | loss: 4.0148435Losses:  3.967068672180176 0.07849107682704926
CurrentTrain: epoch  4, batch    60 | loss: 4.0455599Losses:  4.050241947174072 0.08818522095680237
CurrentTrain: epoch  4, batch    61 | loss: 4.1384273Losses:  5.359475612640381 0.5289202332496643
CurrentTrain: epoch  4, batch    62 | loss: 5.8883958Losses:  4.046853542327881 0.08992858231067657
CurrentTrain: epoch  4, batch    63 | loss: 4.1367822Losses:  3.9317104816436768 0.07532810419797897
CurrentTrain: epoch  4, batch    64 | loss: 4.0070386Losses:  4.174759387969971 0.07340806722640991
CurrentTrain: epoch  4, batch    65 | loss: 4.2481675Losses:  4.170888900756836 0.0648171454668045
CurrentTrain: epoch  4, batch    66 | loss: 4.2357059Losses:  3.992225408554077 0.10795827209949493
CurrentTrain: epoch  4, batch    67 | loss: 4.1001835Losses:  3.984445571899414 0.09987202286720276
CurrentTrain: epoch  4, batch    68 | loss: 4.0843177Losses:  4.203172206878662 0.11629094928503036
CurrentTrain: epoch  4, batch    69 | loss: 4.3194633Losses:  3.9931411743164062 0.038813307881355286
CurrentTrain: epoch  4, batch    70 | loss: 4.0319543Losses:  4.05343770980835 0.09891656786203384
CurrentTrain: epoch  4, batch    71 | loss: 4.1523542Losses:  4.027493000030518 0.07911307364702225
CurrentTrain: epoch  4, batch    72 | loss: 4.1066060Losses:  3.955115795135498 0.13478338718414307
CurrentTrain: epoch  4, batch    73 | loss: 4.0898991Losses:  3.9547839164733887 0.06530256569385529
CurrentTrain: epoch  4, batch    74 | loss: 4.0200863Losses:  3.9769961833953857 0.05545409023761749
CurrentTrain: epoch  4, batch    75 | loss: 4.0324502Losses:  3.9900403022766113 0.056786276400089264
CurrentTrain: epoch  4, batch    76 | loss: 4.0468264Losses:  4.038512229919434 0.13512861728668213
CurrentTrain: epoch  4, batch    77 | loss: 4.1736407Losses:  3.9639134407043457 0.07623282074928284
CurrentTrain: epoch  4, batch    78 | loss: 4.0401464Losses:  4.162374496459961 0.10125678777694702
CurrentTrain: epoch  4, batch    79 | loss: 4.2636313Losses:  4.105441093444824 0.08562853932380676
CurrentTrain: epoch  4, batch    80 | loss: 4.1910696Losses:  4.013487815856934 0.059039123356342316
CurrentTrain: epoch  4, batch    81 | loss: 4.0725269Losses:  3.993124008178711 0.04744292050600052
CurrentTrain: epoch  4, batch    82 | loss: 4.0405669Losses:  4.033535957336426 0.06723833829164505
CurrentTrain: epoch  4, batch    83 | loss: 4.1007743Losses:  3.95278263092041 0.09558911621570587
CurrentTrain: epoch  4, batch    84 | loss: 4.0483718Losses:  4.092878341674805 0.10626939684152603
CurrentTrain: epoch  4, batch    85 | loss: 4.1991477Losses:  4.071036338806152 0.03356476128101349
CurrentTrain: epoch  4, batch    86 | loss: 4.1046009Losses:  3.945230007171631 0.05629296600818634
CurrentTrain: epoch  4, batch    87 | loss: 4.0015230Losses:  4.013515949249268 0.050022050738334656
CurrentTrain: epoch  4, batch    88 | loss: 4.0635381Losses:  4.035184383392334 0.12262772023677826
CurrentTrain: epoch  4, batch    89 | loss: 4.1578121Losses:  3.983247756958008 0.08015583455562592
CurrentTrain: epoch  4, batch    90 | loss: 4.0634036Losses:  4.000181674957275 0.05255879461765289
CurrentTrain: epoch  4, batch    91 | loss: 4.0527406Losses:  3.991528034210205 0.05549875274300575
CurrentTrain: epoch  4, batch    92 | loss: 4.0470266Losses:  3.9835855960845947 0.08029664307832718
CurrentTrain: epoch  4, batch    93 | loss: 4.0638824Losses:  4.013393402099609 0.06888040155172348
CurrentTrain: epoch  4, batch    94 | loss: 4.0822740Losses:  4.102250099182129 0.11781076341867447
CurrentTrain: epoch  4, batch    95 | loss: 4.2200608Losses:  4.07414436340332 0.09501782059669495
CurrentTrain: epoch  4, batch    96 | loss: 4.1691623Losses:  4.022430419921875 0.08793982863426208
CurrentTrain: epoch  4, batch    97 | loss: 4.1103702Losses:  3.92987060546875 0.06615370512008667
CurrentTrain: epoch  4, batch    98 | loss: 3.9960244Losses:  3.9353575706481934 0.045750319957733154
CurrentTrain: epoch  4, batch    99 | loss: 3.9811080Losses:  3.9772660732269287 0.07392463833093643
CurrentTrain: epoch  4, batch   100 | loss: 4.0511909Losses:  4.0428056716918945 0.10752023011445999
CurrentTrain: epoch  4, batch   101 | loss: 4.1503258Losses:  3.983428955078125 0.0667881891131401
CurrentTrain: epoch  4, batch   102 | loss: 4.0502172Losses:  3.954220771789551 0.0676894336938858
CurrentTrain: epoch  4, batch   103 | loss: 4.0219102Losses:  4.37033224105835 0.15556704998016357
CurrentTrain: epoch  4, batch   104 | loss: 4.5258994Losses:  4.004162311553955 0.04896027594804764
CurrentTrain: epoch  4, batch   105 | loss: 4.0531225Losses:  3.958639621734619 0.03187435492873192
CurrentTrain: epoch  4, batch   106 | loss: 3.9905140Losses:  3.970956563949585 0.03526662290096283
CurrentTrain: epoch  4, batch   107 | loss: 4.0062232Losses:  3.943021774291992 0.06246494874358177
CurrentTrain: epoch  4, batch   108 | loss: 4.0054865Losses:  4.077787399291992 0.05136513710021973
CurrentTrain: epoch  4, batch   109 | loss: 4.1291523Losses:  4.008517265319824 0.10617932677268982
CurrentTrain: epoch  4, batch   110 | loss: 4.1146965Losses:  3.955561399459839 0.09064755588769913
CurrentTrain: epoch  4, batch   111 | loss: 4.0462089Losses:  3.9874749183654785 0.11017510294914246
CurrentTrain: epoch  4, batch   112 | loss: 4.0976501Losses:  3.9904603958129883 0.0671047791838646
CurrentTrain: epoch  4, batch   113 | loss: 4.0575652Losses:  4.022393226623535 0.07634973526000977
CurrentTrain: epoch  4, batch   114 | loss: 4.0987430Losses:  3.965973377227783 0.09515298157930374
CurrentTrain: epoch  4, batch   115 | loss: 4.0611262Losses:  3.9652059078216553 0.06988692283630371
CurrentTrain: epoch  4, batch   116 | loss: 4.0350928Losses:  4.070530891418457 0.11980539560317993
CurrentTrain: epoch  4, batch   117 | loss: 4.1903362Losses:  3.996523857116699 0.07359427958726883
CurrentTrain: epoch  4, batch   118 | loss: 4.0701180Losses:  4.175382614135742 0.047882165759801865
CurrentTrain: epoch  4, batch   119 | loss: 4.2232647Losses:  4.006498336791992 0.03581290692090988
CurrentTrain: epoch  4, batch   120 | loss: 4.0423112Losses:  3.967698812484741 0.0563914030790329
CurrentTrain: epoch  4, batch   121 | loss: 4.0240903Losses:  3.94431209564209 0.07804957032203674
CurrentTrain: epoch  4, batch   122 | loss: 4.0223618Losses:  3.9216856956481934 0.03188060224056244
CurrentTrain: epoch  4, batch   123 | loss: 3.9535663Losses:  4.022468090057373 0.07444252073764801
CurrentTrain: epoch  4, batch   124 | loss: 4.0969105Losses:  4.115965843200684 0.08089381456375122
CurrentTrain: epoch  5, batch     0 | loss: 4.1968598Losses:  3.981828212738037 0.11425390839576721
CurrentTrain: epoch  5, batch     1 | loss: 4.0960822Losses:  3.964169979095459 0.07202503830194473
CurrentTrain: epoch  5, batch     2 | loss: 4.0361948Losses:  3.8953559398651123 0.08254870772361755
CurrentTrain: epoch  5, batch     3 | loss: 3.9779046Losses:  4.001210689544678 0.09441374242305756
CurrentTrain: epoch  5, batch     4 | loss: 4.0956244Losses:  3.93315052986145 0.03718659654259682
CurrentTrain: epoch  5, batch     5 | loss: 3.9703372Losses:  4.010871887207031 0.04243249446153641
CurrentTrain: epoch  5, batch     6 | loss: 4.0533042Losses:  4.000296115875244 0.06489802896976471
CurrentTrain: epoch  5, batch     7 | loss: 4.0651941Losses:  3.957200288772583 0.07420705258846283
CurrentTrain: epoch  5, batch     8 | loss: 4.0314074Losses:  3.937952995300293 0.09039910137653351
CurrentTrain: epoch  5, batch     9 | loss: 4.0283523Losses:  3.987654685974121 0.06969549506902695
CurrentTrain: epoch  5, batch    10 | loss: 4.0573502Losses:  3.9972407817840576 0.06541913747787476
CurrentTrain: epoch  5, batch    11 | loss: 4.0626597Losses:  4.06112003326416 0.044230785220861435
CurrentTrain: epoch  5, batch    12 | loss: 4.1053510Losses:  4.022490501403809 0.05183771997690201
CurrentTrain: epoch  5, batch    13 | loss: 4.0743284Losses:  3.940983772277832 0.0567418597638607
CurrentTrain: epoch  5, batch    14 | loss: 3.9977257Losses:  4.03056001663208 0.05106893926858902
CurrentTrain: epoch  5, batch    15 | loss: 4.0816288Losses:  4.007396221160889 0.06753379851579666
CurrentTrain: epoch  5, batch    16 | loss: 4.0749302Losses:  4.002785682678223 0.13139952719211578
CurrentTrain: epoch  5, batch    17 | loss: 4.1341853Losses:  3.976019859313965 0.057344987988471985
CurrentTrain: epoch  5, batch    18 | loss: 4.0333648Losses:  4.006198406219482 0.0428900271654129
CurrentTrain: epoch  5, batch    19 | loss: 4.0490885Losses:  4.012706279754639 0.044830840080976486
CurrentTrain: epoch  5, batch    20 | loss: 4.0575371Losses:  3.940443515777588 0.07198680937290192
CurrentTrain: epoch  5, batch    21 | loss: 4.0124302Losses:  4.016471862792969 0.05212288349866867
CurrentTrain: epoch  5, batch    22 | loss: 4.0685949Losses:  4.091940879821777 0.054786041378974915
CurrentTrain: epoch  5, batch    23 | loss: 4.1467271Losses:  4.040981292724609 0.09300842881202698
CurrentTrain: epoch  5, batch    24 | loss: 4.1339898Losses:  3.9951863288879395 0.05232863500714302
CurrentTrain: epoch  5, batch    25 | loss: 4.0475149Losses:  4.088892936706543 0.10131628811359406
CurrentTrain: epoch  5, batch    26 | loss: 4.1902094Losses:  3.9552745819091797 0.08976730704307556
CurrentTrain: epoch  5, batch    27 | loss: 4.0450420Losses:  4.064276695251465 0.07837764918804169
CurrentTrain: epoch  5, batch    28 | loss: 4.1426544Losses:  4.010212421417236 0.05869290605187416
CurrentTrain: epoch  5, batch    29 | loss: 4.0689054Losses:  4.018825531005859 0.0943639874458313
CurrentTrain: epoch  5, batch    30 | loss: 4.1131897Losses:  3.9798922538757324 0.1305829882621765
CurrentTrain: epoch  5, batch    31 | loss: 4.1104751Losses:  4.089648723602295 0.1153036504983902
CurrentTrain: epoch  5, batch    32 | loss: 4.2049522Losses:  3.9959216117858887 0.0857466608285904
CurrentTrain: epoch  5, batch    33 | loss: 4.0816684Losses:  4.014382839202881 0.0915127545595169
CurrentTrain: epoch  5, batch    34 | loss: 4.1058955Losses:  3.9654197692871094 0.07794082909822464
CurrentTrain: epoch  5, batch    35 | loss: 4.0433607Losses:  3.966862916946411 0.08359468728303909
CurrentTrain: epoch  5, batch    36 | loss: 4.0504575Losses:  3.9525253772735596 0.11404508352279663
CurrentTrain: epoch  5, batch    37 | loss: 4.0665703Losses:  3.9880547523498535 0.07004272937774658
CurrentTrain: epoch  5, batch    38 | loss: 4.0580974Losses:  4.0109639167785645 0.08001924306154251
CurrentTrain: epoch  5, batch    39 | loss: 4.0909834Losses:  4.012301445007324 0.08010649681091309
CurrentTrain: epoch  5, batch    40 | loss: 4.0924082Losses:  4.021312713623047 0.0682837963104248
CurrentTrain: epoch  5, batch    41 | loss: 4.0895967Losses:  4.102209091186523 0.13277429342269897
CurrentTrain: epoch  5, batch    42 | loss: 4.2349834Losses:  3.9682555198669434 0.09999863058328629
CurrentTrain: epoch  5, batch    43 | loss: 4.0682540Losses:  3.98822283744812 0.06617452204227448
CurrentTrain: epoch  5, batch    44 | loss: 4.0543976Losses:  4.005064487457275 0.0500689372420311
CurrentTrain: epoch  5, batch    45 | loss: 4.0551333Losses:  3.9688620567321777 0.07269614189863205
CurrentTrain: epoch  5, batch    46 | loss: 4.0415583Losses:  3.934030532836914 0.117130346596241
CurrentTrain: epoch  5, batch    47 | loss: 4.0511608Losses:  4.007304668426514 0.03247314319014549
CurrentTrain: epoch  5, batch    48 | loss: 4.0397778Losses:  3.9915778636932373 0.06681138277053833
CurrentTrain: epoch  5, batch    49 | loss: 4.0583892Losses:  3.9893908500671387 0.0896092876791954
CurrentTrain: epoch  5, batch    50 | loss: 4.0790000Losses:  3.9495341777801514 0.09673287719488144
CurrentTrain: epoch  5, batch    51 | loss: 4.0462670Losses:  4.028293609619141 0.1065346896648407
CurrentTrain: epoch  5, batch    52 | loss: 4.1348281Losses:  3.9461779594421387 0.036010369658470154
CurrentTrain: epoch  5, batch    53 | loss: 3.9821882Losses:  3.9709136486053467 0.11633429676294327
CurrentTrain: epoch  5, batch    54 | loss: 4.0872478Losses:  4.006567001342773 0.05150653421878815
CurrentTrain: epoch  5, batch    55 | loss: 4.0580735Losses:  3.892047882080078 0.040627945214509964
CurrentTrain: epoch  5, batch    56 | loss: 3.9326758Losses:  3.9063773155212402 0.06825929880142212
CurrentTrain: epoch  5, batch    57 | loss: 3.9746366Losses:  3.992382049560547 0.04818357899785042
CurrentTrain: epoch  5, batch    58 | loss: 4.0405655Losses:  3.9972691535949707 0.08709223568439484
CurrentTrain: epoch  5, batch    59 | loss: 4.0843616Losses:  3.967444896697998 0.05449000000953674
CurrentTrain: epoch  5, batch    60 | loss: 4.0219350Losses:  3.966336727142334 0.049757543951272964
CurrentTrain: epoch  5, batch    61 | loss: 4.0160942Losses:  4.0674543380737305 0.1121106669306755
CurrentTrain: epoch  5, batch    62 | loss: 4.1795650Losses:  4.004665851593018 0.09884735941886902
CurrentTrain: epoch  5, batch    63 | loss: 4.1035132Losses:  3.9936304092407227 0.08876841515302658
CurrentTrain: epoch  5, batch    64 | loss: 4.0823989Losses:  3.9617700576782227 0.11629517376422882
CurrentTrain: epoch  5, batch    65 | loss: 4.0780654Losses:  3.9846084117889404 0.06370145827531815
CurrentTrain: epoch  5, batch    66 | loss: 4.0483098Losses:  3.9409589767456055 0.09473276138305664
CurrentTrain: epoch  5, batch    67 | loss: 4.0356917Losses:  4.038067817687988 0.06941860914230347
CurrentTrain: epoch  5, batch    68 | loss: 4.1074862Losses:  3.9661474227905273 0.07521306723356247
CurrentTrain: epoch  5, batch    69 | loss: 4.0413604Losses:  3.966683864593506 0.03424602001905441
CurrentTrain: epoch  5, batch    70 | loss: 4.0009298Losses:  3.946455478668213 0.05670032650232315
CurrentTrain: epoch  5, batch    71 | loss: 4.0031557Losses:  3.930175304412842 0.05236324295401573
CurrentTrain: epoch  5, batch    72 | loss: 3.9825385Losses:  3.9547438621520996 0.08194497227668762
CurrentTrain: epoch  5, batch    73 | loss: 4.0366888Losses:  3.99345064163208 0.08780316263437271
CurrentTrain: epoch  5, batch    74 | loss: 4.0812540Losses:  3.986348867416382 0.047464519739151
CurrentTrain: epoch  5, batch    75 | loss: 4.0338135Losses:  4.128990173339844 0.13306693732738495
CurrentTrain: epoch  5, batch    76 | loss: 4.2620573Losses:  3.993476629257202 0.05730750784277916
CurrentTrain: epoch  5, batch    77 | loss: 4.0507841Losses:  3.9413745403289795 0.09423216432332993
CurrentTrain: epoch  5, batch    78 | loss: 4.0356069Losses:  3.994067668914795 0.12756767868995667
CurrentTrain: epoch  5, batch    79 | loss: 4.1216354Losses:  3.983948230743408 0.07035587728023529
CurrentTrain: epoch  5, batch    80 | loss: 4.0543041Losses:  3.9769673347473145 0.06531712412834167
CurrentTrain: epoch  5, batch    81 | loss: 4.0422845Losses:  3.9787182807922363 0.05302371084690094
CurrentTrain: epoch  5, batch    82 | loss: 4.0317421Losses:  4.0452046394348145 0.03875795006752014
CurrentTrain: epoch  5, batch    83 | loss: 4.0839624Losses:  3.950470209121704 0.04530642554163933
CurrentTrain: epoch  5, batch    84 | loss: 3.9957767Losses:  3.9192938804626465 0.09071782231330872
CurrentTrain: epoch  5, batch    85 | loss: 4.0100117Losses:  3.950356960296631 0.03392915427684784
CurrentTrain: epoch  5, batch    86 | loss: 3.9842861Losses:  3.9868359565734863 0.04952491074800491
CurrentTrain: epoch  5, batch    87 | loss: 4.0363607Losses:  3.9821629524230957 0.06963697075843811
CurrentTrain: epoch  5, batch    88 | loss: 4.0517998Losses:  3.9220805168151855 0.04450584203004837
CurrentTrain: epoch  5, batch    89 | loss: 3.9665864Losses:  3.9291164875030518 0.09593020379543304
CurrentTrain: epoch  5, batch    90 | loss: 4.0250468Losses:  3.9751839637756348 0.04509882628917694
CurrentTrain: epoch  5, batch    91 | loss: 4.0202827Losses:  3.9679741859436035 0.07874605059623718
CurrentTrain: epoch  5, batch    92 | loss: 4.0467200Losses:  3.976630687713623 0.08399002999067307
CurrentTrain: epoch  5, batch    93 | loss: 4.0606208Losses:  3.9422287940979004 0.04292735457420349
CurrentTrain: epoch  5, batch    94 | loss: 3.9851561Losses:  3.933910608291626 0.04169411584734917
CurrentTrain: epoch  5, batch    95 | loss: 3.9756048Losses:  3.9732728004455566 0.06264780461788177
CurrentTrain: epoch  5, batch    96 | loss: 4.0359206Losses:  3.9141123294830322 0.06351552903652191
CurrentTrain: epoch  5, batch    97 | loss: 3.9776278Losses:  3.9901742935180664 0.07946319878101349
CurrentTrain: epoch  5, batch    98 | loss: 4.0696373Losses:  4.005224227905273 0.09061963856220245
CurrentTrain: epoch  5, batch    99 | loss: 4.0958438Losses:  3.9294166564941406 0.058146845549345016
CurrentTrain: epoch  5, batch   100 | loss: 3.9875636Losses:  4.002713203430176 0.09282860159873962
CurrentTrain: epoch  5, batch   101 | loss: 4.0955420Losses:  3.988044261932373 0.07831750810146332
CurrentTrain: epoch  5, batch   102 | loss: 4.0663619Losses:  3.876094102859497 0.034480806440114975
CurrentTrain: epoch  5, batch   103 | loss: 3.9105749Losses:  3.96103835105896 0.10344173014163971
CurrentTrain: epoch  5, batch   104 | loss: 4.0644803Losses:  3.9989073276519775 0.07623140513896942
CurrentTrain: epoch  5, batch   105 | loss: 4.0751386Losses:  3.8996567726135254 0.06299852579832077
CurrentTrain: epoch  5, batch   106 | loss: 3.9626553Losses:  3.9703660011291504 0.08166594803333282
CurrentTrain: epoch  5, batch   107 | loss: 4.0520320Losses:  3.968575954437256 0.09023857116699219
CurrentTrain: epoch  5, batch   108 | loss: 4.0588145Losses:  3.914501667022705 0.05675601586699486
CurrentTrain: epoch  5, batch   109 | loss: 3.9712577Losses:  3.952683687210083 0.08478523045778275
CurrentTrain: epoch  5, batch   110 | loss: 4.0374689Losses:  3.957505226135254 0.09713263064622879
CurrentTrain: epoch  5, batch   111 | loss: 4.0546379Losses:  3.9238767623901367 0.06105039268732071
CurrentTrain: epoch  5, batch   112 | loss: 3.9849272Losses:  4.003629684448242 0.0528000146150589
CurrentTrain: epoch  5, batch   113 | loss: 4.0564299Losses:  3.9495370388031006 0.12716703116893768
CurrentTrain: epoch  5, batch   114 | loss: 4.0767040Losses:  3.9886655807495117 0.0906105786561966
CurrentTrain: epoch  5, batch   115 | loss: 4.0792761Losses:  3.9779915809631348 0.06467899680137634
CurrentTrain: epoch  5, batch   116 | loss: 4.0426707Losses:  3.954307794570923 0.11807407438755035
CurrentTrain: epoch  5, batch   117 | loss: 4.0723820Losses:  3.9828739166259766 0.03740273416042328
CurrentTrain: epoch  5, batch   118 | loss: 4.0202765Losses:  4.071325302124023 0.043512921780347824
CurrentTrain: epoch  5, batch   119 | loss: 4.1148381Losses:  4.013377666473389 0.05012989044189453
CurrentTrain: epoch  5, batch   120 | loss: 4.0635076Losses:  3.9621505737304688 0.0692729502916336
CurrentTrain: epoch  5, batch   121 | loss: 4.0314236Losses:  3.984032392501831 0.07043304294347763
CurrentTrain: epoch  5, batch   122 | loss: 4.0544653Losses:  3.9822769165039062 0.031029123812913895
CurrentTrain: epoch  5, batch   123 | loss: 4.0133061Losses:  3.9580302238464355 0.07161448895931244
CurrentTrain: epoch  5, batch   124 | loss: 4.0296445Losses:  3.9451608657836914 0.09946006536483765
CurrentTrain: epoch  6, batch     0 | loss: 4.0446210Losses:  4.011345863342285 0.09725084155797958
CurrentTrain: epoch  6, batch     1 | loss: 4.1085968Losses:  3.982628345489502 0.08222364634275436
CurrentTrain: epoch  6, batch     2 | loss: 4.0648518Losses:  3.9813385009765625 0.08493322134017944
CurrentTrain: epoch  6, batch     3 | loss: 4.0662718Losses:  4.022840976715088 0.05150606483221054
CurrentTrain: epoch  6, batch     4 | loss: 4.0743470Losses:  3.9439783096313477 0.040513694286346436
CurrentTrain: epoch  6, batch     5 | loss: 3.9844921Losses:  3.956841468811035 0.058913569897413254
CurrentTrain: epoch  6, batch     6 | loss: 4.0157552Losses:  3.964482307434082 0.07171735912561417
CurrentTrain: epoch  6, batch     7 | loss: 4.0361996Losses:  3.9348843097686768 0.05951597914099693
CurrentTrain: epoch  6, batch     8 | loss: 3.9944003Losses:  3.9654300212860107 0.08132234215736389
CurrentTrain: epoch  6, batch     9 | loss: 4.0467525Losses:  3.991140842437744 0.04843535274267197
CurrentTrain: epoch  6, batch    10 | loss: 4.0395761Losses:  3.9417953491210938 0.04604300111532211
CurrentTrain: epoch  6, batch    11 | loss: 3.9878383Losses:  3.9678890705108643 0.05572626739740372
CurrentTrain: epoch  6, batch    12 | loss: 4.0236154Losses:  3.9731602668762207 0.06539207696914673
CurrentTrain: epoch  6, batch    13 | loss: 4.0385523Losses:  3.9184012413024902 0.07697659730911255
CurrentTrain: epoch  6, batch    14 | loss: 3.9953778Losses:  3.9826107025146484 0.07020317018032074
CurrentTrain: epoch  6, batch    15 | loss: 4.0528140Losses:  3.928133726119995 0.05909816548228264
CurrentTrain: epoch  6, batch    16 | loss: 3.9872320Losses:  4.004435062408447 0.05411549657583237
CurrentTrain: epoch  6, batch    17 | loss: 4.0585504Losses:  3.9577345848083496 0.012188326567411423
CurrentTrain: epoch  6, batch    18 | loss: 3.9699230Losses:  4.030864715576172 0.08012660592794418
CurrentTrain: epoch  6, batch    19 | loss: 4.1109915Losses:  3.983274459838867 0.06145995482802391
CurrentTrain: epoch  6, batch    20 | loss: 4.0447345Losses:  3.935046434402466 0.040455128997564316
CurrentTrain: epoch  6, batch    21 | loss: 3.9755015Losses:  3.9471068382263184 0.12188341468572617
CurrentTrain: epoch  6, batch    22 | loss: 4.0689902Losses:  4.005499839782715 0.04016954079270363
CurrentTrain: epoch  6, batch    23 | loss: 4.0456696Losses:  3.991466999053955 0.05615920200943947
CurrentTrain: epoch  6, batch    24 | loss: 4.0476260Losses:  4.020809173583984 0.03281576186418533
CurrentTrain: epoch  6, batch    25 | loss: 4.0536251Losses:  3.899733543395996 0.10199984908103943
CurrentTrain: epoch  6, batch    26 | loss: 4.0017333Losses:  3.9692625999450684 0.0459437370300293
CurrentTrain: epoch  6, batch    27 | loss: 4.0152063Losses:  3.9472577571868896 0.06297716498374939
CurrentTrain: epoch  6, batch    28 | loss: 4.0102348Losses:  3.958200216293335 0.08460938185453415
CurrentTrain: epoch  6, batch    29 | loss: 4.0428095Losses:  3.990068197250366 0.04420456290245056
CurrentTrain: epoch  6, batch    30 | loss: 4.0342727Losses:  4.016992568969727 0.035425685346126556
CurrentTrain: epoch  6, batch    31 | loss: 4.0524182Losses:  3.974555253982544 0.09252340346574783
CurrentTrain: epoch  6, batch    32 | loss: 4.0670786Losses:  3.9650492668151855 0.08550120145082474
CurrentTrain: epoch  6, batch    33 | loss: 4.0505505Losses:  3.9558238983154297 0.062067706137895584
CurrentTrain: epoch  6, batch    34 | loss: 4.0178914Losses:  3.9699831008911133 0.06748434901237488
CurrentTrain: epoch  6, batch    35 | loss: 4.0374675Losses:  3.9537124633789062 0.05842963978648186
CurrentTrain: epoch  6, batch    36 | loss: 4.0121422Losses:  3.9504013061523438 0.06392479687929153
CurrentTrain: epoch  6, batch    37 | loss: 4.0143261Losses:  3.980100154876709 0.06288821995258331
CurrentTrain: epoch  6, batch    38 | loss: 4.0429883Losses:  3.925722122192383 0.07165324687957764
CurrentTrain: epoch  6, batch    39 | loss: 3.9973755Losses:  3.9460153579711914 0.08097970485687256
CurrentTrain: epoch  6, batch    40 | loss: 4.0269952Losses:  3.9436116218566895 0.030857160687446594
CurrentTrain: epoch  6, batch    41 | loss: 3.9744687Losses:  3.9484188556671143 0.07205603271722794
CurrentTrain: epoch  6, batch    42 | loss: 4.0204749Losses:  3.9563870429992676 0.08709171414375305
CurrentTrain: epoch  6, batch    43 | loss: 4.0434790Losses:  3.978947639465332 0.05847702920436859
CurrentTrain: epoch  6, batch    44 | loss: 4.0374246Losses:  3.9515380859375 0.06495741009712219
CurrentTrain: epoch  6, batch    45 | loss: 4.0164957Losses:  3.9804272651672363 0.05828291177749634
CurrentTrain: epoch  6, batch    46 | loss: 4.0387101Losses:  4.005465507507324 0.07096243649721146
CurrentTrain: epoch  6, batch    47 | loss: 4.0764279Losses:  3.902066707611084 0.02844391018152237
CurrentTrain: epoch  6, batch    48 | loss: 3.9305105Losses:  3.963081121444702 0.061845675110816956
CurrentTrain: epoch  6, batch    49 | loss: 4.0249267Losses:  3.961805582046509 0.047884777188301086
CurrentTrain: epoch  6, batch    50 | loss: 4.0096903Losses:  3.9464468955993652 0.04509648680686951
CurrentTrain: epoch  6, batch    51 | loss: 3.9915433Losses:  3.9929733276367188 0.05133155360817909
CurrentTrain: epoch  6, batch    52 | loss: 4.0443048Losses:  4.006691932678223 0.050585780292749405
CurrentTrain: epoch  6, batch    53 | loss: 4.0572777Losses:  3.9943807125091553 0.052286431193351746
CurrentTrain: epoch  6, batch    54 | loss: 4.0466671Losses:  3.959444999694824 0.06325734406709671
CurrentTrain: epoch  6, batch    55 | loss: 4.0227022Losses:  3.940458297729492 0.049412503838539124
CurrentTrain: epoch  6, batch    56 | loss: 3.9898708Losses:  3.9880430698394775 0.08170225471258163
CurrentTrain: epoch  6, batch    57 | loss: 4.0697455Losses:  3.9559173583984375 0.054520659148693085
CurrentTrain: epoch  6, batch    58 | loss: 4.0104380Losses:  3.9683234691619873 0.05577628314495087
CurrentTrain: epoch  6, batch    59 | loss: 4.0240998Losses:  3.987274408340454 0.04871512949466705
CurrentTrain: epoch  6, batch    60 | loss: 4.0359898Losses:  3.969658374786377 0.10423333942890167
CurrentTrain: epoch  6, batch    61 | loss: 4.0738916Losses:  3.854372501373291 0.031612321734428406
CurrentTrain: epoch  6, batch    62 | loss: 3.8859849Losses:  3.9351184368133545 0.05985349044203758
CurrentTrain: epoch  6, batch    63 | loss: 3.9949720Losses:  3.9877638816833496 0.07284591346979141
CurrentTrain: epoch  6, batch    64 | loss: 4.0606098Losses:  3.8817338943481445 0.028449416160583496
CurrentTrain: epoch  6, batch    65 | loss: 3.9101834Losses:  3.9671101570129395 0.05790194123983383
CurrentTrain: epoch  6, batch    66 | loss: 4.0250120Losses:  3.9591407775878906 0.04425239562988281
CurrentTrain: epoch  6, batch    67 | loss: 4.0033932Losses:  3.9318864345550537 0.0382780060172081
CurrentTrain: epoch  6, batch    68 | loss: 3.9701645Losses:  3.926116466522217 0.036359719932079315
CurrentTrain: epoch  6, batch    69 | loss: 3.9624763Losses:  3.899195909500122 0.042657520622015
CurrentTrain: epoch  6, batch    70 | loss: 3.9418535Losses:  3.957570791244507 0.08385155349969864
CurrentTrain: epoch  6, batch    71 | loss: 4.0414224Losses:  3.9827306270599365 0.056030452251434326
CurrentTrain: epoch  6, batch    72 | loss: 4.0387611Losses:  4.0220627784729 0.051433954387903214
CurrentTrain: epoch  6, batch    73 | loss: 4.0734968Losses:  3.9350404739379883 0.09184999018907547
CurrentTrain: epoch  6, batch    74 | loss: 4.0268903Losses:  3.9272866249084473 0.054515641182661057
CurrentTrain: epoch  6, batch    75 | loss: 3.9818022Losses:  3.935230016708374 0.06158824265003204
CurrentTrain: epoch  6, batch    76 | loss: 3.9968183Losses:  3.9388515949249268 0.03347589075565338
CurrentTrain: epoch  6, batch    77 | loss: 3.9723275Losses:  3.973355770111084 0.07933427393436432
CurrentTrain: epoch  6, batch    78 | loss: 4.0526900Losses:  3.9073710441589355 0.06003960967063904
CurrentTrain: epoch  6, batch    79 | loss: 3.9674106Losses:  3.975935697555542 0.05968223512172699
CurrentTrain: epoch  6, batch    80 | loss: 4.0356178Losses:  3.9629111289978027 0.04038894176483154
CurrentTrain: epoch  6, batch    81 | loss: 4.0033002Losses:  3.9607126712799072 0.06892870366573334
CurrentTrain: epoch  6, batch    82 | loss: 4.0296412Losses:  3.9447925090789795 0.08204607665538788
CurrentTrain: epoch  6, batch    83 | loss: 4.0268388Losses:  3.920621395111084 0.04739711433649063
CurrentTrain: epoch  6, batch    84 | loss: 3.9680185Losses:  3.9645118713378906 0.08704271912574768
CurrentTrain: epoch  6, batch    85 | loss: 4.0515547Losses:  3.972416639328003 0.04799947887659073
CurrentTrain: epoch  6, batch    86 | loss: 4.0204163Losses:  3.922276496887207 0.09300017356872559
CurrentTrain: epoch  6, batch    87 | loss: 4.0152769Losses:  3.9595017433166504 0.04168860241770744
CurrentTrain: epoch  6, batch    88 | loss: 4.0011902Losses:  3.9621613025665283 0.044802114367485046
CurrentTrain: epoch  6, batch    89 | loss: 4.0069633Losses:  3.9503746032714844 0.05004958063364029
CurrentTrain: epoch  6, batch    90 | loss: 4.0004244Losses:  3.9447476863861084 0.06933985650539398
CurrentTrain: epoch  6, batch    91 | loss: 4.0140877Losses:  3.9615283012390137 0.0240953266620636
CurrentTrain: epoch  6, batch    92 | loss: 3.9856236Losses:  3.931037425994873 0.0636814534664154
CurrentTrain: epoch  6, batch    93 | loss: 3.9947188Losses:  3.9425504207611084 0.08699078857898712
CurrentTrain: epoch  6, batch    94 | loss: 4.0295410Losses:  3.948373794555664 0.026025637984275818
CurrentTrain: epoch  6, batch    95 | loss: 3.9743993Losses:  3.959083318710327 0.04972619563341141
CurrentTrain: epoch  6, batch    96 | loss: 4.0088096Losses:  3.9052464962005615 0.04276265203952789
CurrentTrain: epoch  6, batch    97 | loss: 3.9480093Losses:  3.910660982131958 0.053986795246601105
CurrentTrain: epoch  6, batch    98 | loss: 3.9646478Losses:  3.908931016921997 0.08122783154249191
CurrentTrain: epoch  6, batch    99 | loss: 3.9901588Losses:  3.965426206588745 0.03544902056455612
CurrentTrain: epoch  6, batch   100 | loss: 4.0008750Losses:  3.954901933670044 0.08977971971035004
CurrentTrain: epoch  6, batch   101 | loss: 4.0446815Losses:  3.944497585296631 0.04763482138514519
CurrentTrain: epoch  6, batch   102 | loss: 3.9921324Losses:  4.028676986694336 0.02930617146193981
CurrentTrain: epoch  6, batch   103 | loss: 4.0579829Losses:  4.004275798797607 0.029310423880815506
CurrentTrain: epoch  6, batch   104 | loss: 4.0335860Losses:  3.932708978652954 0.03919728845357895
CurrentTrain: epoch  6, batch   105 | loss: 3.9719062Losses:  3.9275622367858887 0.07220408320426941
CurrentTrain: epoch  6, batch   106 | loss: 3.9997663Losses:  3.9348325729370117 0.061670586466789246
CurrentTrain: epoch  6, batch   107 | loss: 3.9965031Losses:  3.925692081451416 0.040703751146793365
CurrentTrain: epoch  6, batch   108 | loss: 3.9663959Losses:  4.005468845367432 0.05029899626970291
CurrentTrain: epoch  6, batch   109 | loss: 4.0557680Losses:  3.9656150341033936 0.06269969791173935
CurrentTrain: epoch  6, batch   110 | loss: 4.0283146Losses:  3.9634487628936768 0.06629304587841034
CurrentTrain: epoch  6, batch   111 | loss: 4.0297418Losses:  3.940908670425415 0.06864269077777863
CurrentTrain: epoch  6, batch   112 | loss: 4.0095515Losses:  3.9262003898620605 0.04849310219287872
CurrentTrain: epoch  6, batch   113 | loss: 3.9746935Losses:  3.946674346923828 0.0537603460252285
CurrentTrain: epoch  6, batch   114 | loss: 4.0004349Losses:  3.8700647354125977 0.03581101447343826
CurrentTrain: epoch  6, batch   115 | loss: 3.9058757Losses:  3.9114348888397217 0.0665130689740181
CurrentTrain: epoch  6, batch   116 | loss: 3.9779480Losses:  3.9323391914367676 0.06006920337677002
CurrentTrain: epoch  6, batch   117 | loss: 3.9924083Losses:  3.9073591232299805 0.060483451932668686
CurrentTrain: epoch  6, batch   118 | loss: 3.9678426Losses:  3.9741694927215576 0.04867292195558548
CurrentTrain: epoch  6, batch   119 | loss: 4.0228424Losses:  3.9596903324127197 0.03622131422162056
CurrentTrain: epoch  6, batch   120 | loss: 3.9959116Losses:  3.883133888244629 0.0362311527132988
CurrentTrain: epoch  6, batch   121 | loss: 3.9193649Losses:  3.951202869415283 0.09061895310878754
CurrentTrain: epoch  6, batch   122 | loss: 4.0418220Losses:  3.934523344039917 0.08090225607156754
CurrentTrain: epoch  6, batch   123 | loss: 4.0154257Losses:  3.9332528114318848 0.04954732954502106
CurrentTrain: epoch  6, batch   124 | loss: 3.9828002Losses:  3.943009376525879 0.10298512876033783
CurrentTrain: epoch  7, batch     0 | loss: 4.0459943Losses:  4.02087926864624 0.07488822191953659
CurrentTrain: epoch  7, batch     1 | loss: 4.0957675Losses:  3.90818452835083 0.06173308938741684
CurrentTrain: epoch  7, batch     2 | loss: 3.9699175Losses:  3.925081729888916 0.04464784264564514
CurrentTrain: epoch  7, batch     3 | loss: 3.9697297Losses:  3.973496913909912 0.08291161805391312
CurrentTrain: epoch  7, batch     4 | loss: 4.0564084Losses:  3.946091413497925 0.0422462522983551
CurrentTrain: epoch  7, batch     5 | loss: 3.9883378Losses:  4.024292945861816 0.052746474742889404
CurrentTrain: epoch  7, batch     6 | loss: 4.0770392Losses:  3.876352310180664 0.030792053788900375
CurrentTrain: epoch  7, batch     7 | loss: 3.9071443Losses:  3.96757435798645 0.052022457122802734
CurrentTrain: epoch  7, batch     8 | loss: 4.0195971Losses:  4.0229997634887695 0.04222002625465393
CurrentTrain: epoch  7, batch     9 | loss: 4.0652199Losses:  3.951702117919922 0.06486053764820099
CurrentTrain: epoch  7, batch    10 | loss: 4.0165625Losses:  3.9823246002197266 0.032914988696575165
CurrentTrain: epoch  7, batch    11 | loss: 4.0152397Losses:  3.990027904510498 0.030071690678596497
CurrentTrain: epoch  7, batch    12 | loss: 4.0200996Losses:  3.9688727855682373 0.0488491915166378
CurrentTrain: epoch  7, batch    13 | loss: 4.0177221Losses:  3.9818153381347656 0.04086471349000931
CurrentTrain: epoch  7, batch    14 | loss: 4.0226803Losses:  3.967924118041992 0.061678413301706314
CurrentTrain: epoch  7, batch    15 | loss: 4.0296025Losses:  3.9853944778442383 0.048512399196624756
CurrentTrain: epoch  7, batch    16 | loss: 4.0339069Losses:  3.967498302459717 0.04885608330368996
CurrentTrain: epoch  7, batch    17 | loss: 4.0163546Losses:  3.9774527549743652 0.07120086252689362
CurrentTrain: epoch  7, batch    18 | loss: 4.0486536Losses:  3.9301695823669434 0.050233833491802216
CurrentTrain: epoch  7, batch    19 | loss: 3.9804034Losses:  3.9537527561187744 0.07845044136047363
CurrentTrain: epoch  7, batch    20 | loss: 4.0322032Losses:  3.9143738746643066 0.04964949190616608
CurrentTrain: epoch  7, batch    21 | loss: 3.9640234Losses:  3.9068267345428467 0.07385478913784027
CurrentTrain: epoch  7, batch    22 | loss: 3.9806814Losses:  3.895153284072876 0.04564395546913147
CurrentTrain: epoch  7, batch    23 | loss: 3.9407973Losses:  3.9644837379455566 0.04092119261622429
CurrentTrain: epoch  7, batch    24 | loss: 4.0054049Losses:  3.9079060554504395 0.03296608477830887
CurrentTrain: epoch  7, batch    25 | loss: 3.9408722Losses:  3.8924756050109863 0.06775164604187012
CurrentTrain: epoch  7, batch    26 | loss: 3.9602273Losses:  4.023324966430664 0.061756111681461334
CurrentTrain: epoch  7, batch    27 | loss: 4.0850811Losses:  3.9747982025146484 0.06312514841556549
CurrentTrain: epoch  7, batch    28 | loss: 4.0379233Losses:  3.9630024433135986 0.044604845345020294
CurrentTrain: epoch  7, batch    29 | loss: 4.0076075Losses:  3.963948965072632 0.030002078041434288
CurrentTrain: epoch  7, batch    30 | loss: 3.9939511Losses:  3.91524076461792 0.044089481234550476
CurrentTrain: epoch  7, batch    31 | loss: 3.9593303Losses:  3.963799476623535 0.04518461972475052
CurrentTrain: epoch  7, batch    32 | loss: 4.0089841Losses:  4.029410362243652 0.05597512051463127
CurrentTrain: epoch  7, batch    33 | loss: 4.0853853Losses:  3.9099602699279785 0.08897136151790619
CurrentTrain: epoch  7, batch    34 | loss: 3.9989316Losses:  3.9973349571228027 0.05296114832162857
CurrentTrain: epoch  7, batch    35 | loss: 4.0502963Losses:  3.9694528579711914 0.029841311275959015
CurrentTrain: epoch  7, batch    36 | loss: 3.9992943Losses:  3.989673376083374 0.015201815403997898
CurrentTrain: epoch  7, batch    37 | loss: 4.0048752Losses:  3.9257874488830566 0.10987712442874908
CurrentTrain: epoch  7, batch    38 | loss: 4.0356646Losses:  3.918422222137451 0.05149830877780914
CurrentTrain: epoch  7, batch    39 | loss: 3.9699206Losses:  3.9558308124542236 0.011481916531920433
CurrentTrain: epoch  7, batch    40 | loss: 3.9673128Losses:  3.9240424633026123 0.059967365115880966
CurrentTrain: epoch  7, batch    41 | loss: 3.9840097Losses:  3.913376808166504 0.03732893988490105
CurrentTrain: epoch  7, batch    42 | loss: 3.9507058Losses:  3.8618898391723633 0.07853829115629196
CurrentTrain: epoch  7, batch    43 | loss: 3.9404280Losses:  3.9357974529266357 0.05964193493127823
CurrentTrain: epoch  7, batch    44 | loss: 3.9954393Losses:  3.948270082473755 0.07195299118757248
CurrentTrain: epoch  7, batch    45 | loss: 4.0202231Losses:  3.938720226287842 0.05275486037135124
CurrentTrain: epoch  7, batch    46 | loss: 3.9914751Losses:  3.931691884994507 0.04419710114598274
CurrentTrain: epoch  7, batch    47 | loss: 3.9758890Losses:  3.948923110961914 0.05559510737657547
CurrentTrain: epoch  7, batch    48 | loss: 4.0045180Losses:  3.9871649742126465 0.03483706712722778
CurrentTrain: epoch  7, batch    49 | loss: 4.0220022Losses:  3.9359474182128906 0.06981642544269562
CurrentTrain: epoch  7, batch    50 | loss: 4.0057640Losses:  3.9710206985473633 0.04222377389669418
CurrentTrain: epoch  7, batch    51 | loss: 4.0132446Losses:  3.9867401123046875 0.08591625094413757
CurrentTrain: epoch  7, batch    52 | loss: 4.0726562Losses:  3.874891757965088 0.05284205079078674
CurrentTrain: epoch  7, batch    53 | loss: 3.9277339Losses:  3.9052159786224365 0.046351607888936996
CurrentTrain: epoch  7, batch    54 | loss: 3.9515676Losses:  3.937133312225342 0.026862669736146927
CurrentTrain: epoch  7, batch    55 | loss: 3.9639959Losses:  3.92555570602417 0.050709210336208344
CurrentTrain: epoch  7, batch    56 | loss: 3.9762650Losses:  3.9113221168518066 0.0721890926361084
CurrentTrain: epoch  7, batch    57 | loss: 3.9835112Losses:  3.9133071899414062 0.047638293355703354
CurrentTrain: epoch  7, batch    58 | loss: 3.9609454Losses:  3.9404525756835938 0.03840146213769913
CurrentTrain: epoch  7, batch    59 | loss: 3.9788539Losses:  3.964195489883423 0.036141857504844666
CurrentTrain: epoch  7, batch    60 | loss: 4.0003371Losses:  3.9290146827697754 0.039482180029153824
CurrentTrain: epoch  7, batch    61 | loss: 3.9684968Losses:  3.9145712852478027 0.057815488427877426
CurrentTrain: epoch  7, batch    62 | loss: 3.9723868Losses:  3.937224864959717 0.07401259243488312
CurrentTrain: epoch  7, batch    63 | loss: 4.0112376Losses:  3.93414306640625 0.08566051721572876
CurrentTrain: epoch  7, batch    64 | loss: 4.0198035Losses:  3.8967907428741455 0.03706306219100952
CurrentTrain: epoch  7, batch    65 | loss: 3.9338539Losses:  3.9557371139526367 0.0819234624505043
CurrentTrain: epoch  7, batch    66 | loss: 4.0376606Losses:  3.9336161613464355 0.03449771925806999
CurrentTrain: epoch  7, batch    67 | loss: 3.9681139Losses:  3.9870715141296387 0.045195065438747406
CurrentTrain: epoch  7, batch    68 | loss: 4.0322666Losses:  3.949835777282715 0.045227035880088806
CurrentTrain: epoch  7, batch    69 | loss: 3.9950628Losses:  3.920231342315674 0.06935623288154602
CurrentTrain: epoch  7, batch    70 | loss: 3.9895875Losses:  3.9486684799194336 0.054438430815935135
CurrentTrain: epoch  7, batch    71 | loss: 4.0031071Losses:  3.9404401779174805 0.02515444904565811
CurrentTrain: epoch  7, batch    72 | loss: 3.9655945Losses:  3.9913008213043213 0.04625479876995087
CurrentTrain: epoch  7, batch    73 | loss: 4.0375557Losses:  3.9411230087280273 0.07748865336179733
CurrentTrain: epoch  7, batch    74 | loss: 4.0186114Losses:  3.9169111251831055 0.06409339606761932
CurrentTrain: epoch  7, batch    75 | loss: 3.9810045Losses:  3.9622488021850586 0.049315765500068665
CurrentTrain: epoch  7, batch    76 | loss: 4.0115647Losses:  3.9420089721679688 0.08724388480186462
CurrentTrain: epoch  7, batch    77 | loss: 4.0292530Losses:  3.9126687049865723 0.06906645745038986
CurrentTrain: epoch  7, batch    78 | loss: 3.9817352Losses:  3.886782169342041 0.04510638117790222
CurrentTrain: epoch  7, batch    79 | loss: 3.9318886Losses:  3.962446689605713 0.08093781769275665
CurrentTrain: epoch  7, batch    80 | loss: 4.0433846Losses:  3.951397657394409 0.08358210325241089
CurrentTrain: epoch  7, batch    81 | loss: 4.0349798Losses:  3.9212584495544434 0.05193416029214859
CurrentTrain: epoch  7, batch    82 | loss: 3.9731927Losses:  3.9030423164367676 0.06931361556053162
CurrentTrain: epoch  7, batch    83 | loss: 3.9723558Losses:  3.968090057373047 0.04793693870306015
CurrentTrain: epoch  7, batch    84 | loss: 4.0160270Losses:  3.91660737991333 0.03127475827932358
CurrentTrain: epoch  7, batch    85 | loss: 3.9478822Losses:  3.8685145378112793 0.03892045468091965
CurrentTrain: epoch  7, batch    86 | loss: 3.9074349Losses:  3.895604133605957 0.05267129838466644
CurrentTrain: epoch  7, batch    87 | loss: 3.9482753Losses:  3.899027109146118 0.0336775928735733
CurrentTrain: epoch  7, batch    88 | loss: 3.9327047Losses:  3.9940309524536133 0.07052730768918991
CurrentTrain: epoch  7, batch    89 | loss: 4.0645580Losses:  3.9857094287872314 0.03538873791694641
CurrentTrain: epoch  7, batch    90 | loss: 4.0210981Losses:  3.985903739929199 0.03917522728443146
CurrentTrain: epoch  7, batch    91 | loss: 4.0250788Losses:  3.947899341583252 0.03979690745472908
CurrentTrain: epoch  7, batch    92 | loss: 3.9876962Losses:  3.8808765411376953 0.0493343248963356
CurrentTrain: epoch  7, batch    93 | loss: 3.9302108Losses:  4.0170135498046875 0.05502692237496376
CurrentTrain: epoch  7, batch    94 | loss: 4.0720406Losses:  3.9595518112182617 0.06056591123342514
CurrentTrain: epoch  7, batch    95 | loss: 4.0201178Losses:  3.930145740509033 0.041650108993053436
CurrentTrain: epoch  7, batch    96 | loss: 3.9717958Losses:  3.9649925231933594 0.040963947772979736
CurrentTrain: epoch  7, batch    97 | loss: 4.0059566Losses:  3.962222099304199 0.04016968235373497
CurrentTrain: epoch  7, batch    98 | loss: 4.0023918Losses:  3.9709906578063965 0.03813955932855606
CurrentTrain: epoch  7, batch    99 | loss: 4.0091300Losses:  3.9001853466033936 0.043307580053806305
CurrentTrain: epoch  7, batch   100 | loss: 3.9434929Losses:  3.907940626144409 0.053719617426395416
CurrentTrain: epoch  7, batch   101 | loss: 3.9616601Losses:  3.936850070953369 0.040643323212862015
CurrentTrain: epoch  7, batch   102 | loss: 3.9774933Losses:  3.9835052490234375 0.05196042358875275
CurrentTrain: epoch  7, batch   103 | loss: 4.0354657Losses:  4.059950828552246 0.022472737357020378
CurrentTrain: epoch  7, batch   104 | loss: 4.0824237Losses:  3.9116079807281494 0.06177663803100586
CurrentTrain: epoch  7, batch   105 | loss: 3.9733846Losses:  3.904623508453369 0.060403719544410706
CurrentTrain: epoch  7, batch   106 | loss: 3.9650273Losses:  3.997769355773926 0.03041338175535202
CurrentTrain: epoch  7, batch   107 | loss: 4.0281825Losses:  3.9344263076782227 0.04227469861507416
CurrentTrain: epoch  7, batch   108 | loss: 3.9767010Losses:  3.9484195709228516 0.053092680871486664
CurrentTrain: epoch  7, batch   109 | loss: 4.0015121Losses:  3.9668540954589844 0.023169981315732002
CurrentTrain: epoch  7, batch   110 | loss: 3.9900241Losses:  3.9579877853393555 0.0663338303565979
CurrentTrain: epoch  7, batch   111 | loss: 4.0243216Losses:  3.9748549461364746 0.04003389924764633
CurrentTrain: epoch  7, batch   112 | loss: 4.0148888Losses:  4.0007195472717285 0.048812247812747955
CurrentTrain: epoch  7, batch   113 | loss: 4.0495319Losses:  3.94657564163208 0.04444793611764908
CurrentTrain: epoch  7, batch   114 | loss: 3.9910235Losses:  3.90112566947937 0.02914983406662941
CurrentTrain: epoch  7, batch   115 | loss: 3.9302754Losses:  3.9781670570373535 0.049606114625930786
CurrentTrain: epoch  7, batch   116 | loss: 4.0277734Losses:  3.9221129417419434 0.037268489599227905
CurrentTrain: epoch  7, batch   117 | loss: 3.9593813Losses:  4.02817440032959 0.050033994019031525
CurrentTrain: epoch  7, batch   118 | loss: 4.0782084Losses:  3.9227936267852783 0.0637722909450531
CurrentTrain: epoch  7, batch   119 | loss: 3.9865658Losses:  4.009794235229492 0.047616347670555115
CurrentTrain: epoch  7, batch   120 | loss: 4.0574107Losses:  3.9410061836242676 0.07468819618225098
CurrentTrain: epoch  7, batch   121 | loss: 4.0156946Losses:  3.9481253623962402 0.06368471682071686
CurrentTrain: epoch  7, batch   122 | loss: 4.0118103Losses:  3.95334529876709 0.027605483308434486
CurrentTrain: epoch  7, batch   123 | loss: 3.9809508Losses:  3.90910005569458 0.034850433468818665
CurrentTrain: epoch  7, batch   124 | loss: 3.9439504Losses:  3.9450721740722656 0.06066204980015755
CurrentTrain: epoch  8, batch     0 | loss: 4.0057344Losses:  3.954897880554199 0.046249933540821075
CurrentTrain: epoch  8, batch     1 | loss: 4.0011477Losses:  3.943617105484009 0.04303954541683197
CurrentTrain: epoch  8, batch     2 | loss: 3.9866567Losses:  3.972867965698242 0.024157827720046043
CurrentTrain: epoch  8, batch     3 | loss: 3.9970257Losses:  3.966986894607544 0.030227739363908768
CurrentTrain: epoch  8, batch     4 | loss: 3.9972146Losses:  3.93680477142334 0.05738859251141548
CurrentTrain: epoch  8, batch     5 | loss: 3.9941933Losses:  3.9767658710479736 0.06302319467067719
CurrentTrain: epoch  8, batch     6 | loss: 4.0397892Losses:  3.930135726928711 0.07479528337717056
CurrentTrain: epoch  8, batch     7 | loss: 4.0049310Losses:  3.9480462074279785 0.044006090611219406
CurrentTrain: epoch  8, batch     8 | loss: 3.9920523Losses:  3.919936180114746 0.04103173315525055
CurrentTrain: epoch  8, batch     9 | loss: 3.9609680Losses:  3.9159443378448486 0.07498413324356079
CurrentTrain: epoch  8, batch    10 | loss: 3.9909284Losses:  3.9013376235961914 0.060453370213508606
CurrentTrain: epoch  8, batch    11 | loss: 3.9617910Losses:  3.965674877166748 0.034765299409627914
CurrentTrain: epoch  8, batch    12 | loss: 4.0004401Losses:  3.937664031982422 0.08188693225383759
CurrentTrain: epoch  8, batch    13 | loss: 4.0195508Losses:  3.9286913871765137 0.05006437376141548
CurrentTrain: epoch  8, batch    14 | loss: 3.9787557Losses:  3.924360990524292 0.050524063408374786
CurrentTrain: epoch  8, batch    15 | loss: 3.9748850Losses:  3.920772075653076 0.04094516858458519
CurrentTrain: epoch  8, batch    16 | loss: 3.9617171Losses:  3.93087100982666 0.06539396941661835
CurrentTrain: epoch  8, batch    17 | loss: 3.9962649Losses:  3.9713549613952637 0.05061797797679901
CurrentTrain: epoch  8, batch    18 | loss: 4.0219731Losses:  3.953230381011963 0.058647770434617996
CurrentTrain: epoch  8, batch    19 | loss: 4.0118780Losses:  3.9239726066589355 0.05205830931663513
CurrentTrain: epoch  8, batch    20 | loss: 3.9760308Losses:  3.8900949954986572 0.04736486077308655
CurrentTrain: epoch  8, batch    21 | loss: 3.9374599Losses:  4.002614974975586 0.05789688602089882
CurrentTrain: epoch  8, batch    22 | loss: 4.0605121Losses:  3.9484524726867676 0.044040434062480927
CurrentTrain: epoch  8, batch    23 | loss: 3.9924929Losses:  3.9672703742980957 0.039835184812545776
CurrentTrain: epoch  8, batch    24 | loss: 4.0071054Losses:  3.925278663635254 0.04091478884220123
CurrentTrain: epoch  8, batch    25 | loss: 3.9661934Losses:  3.9589879512786865 0.04328080266714096
CurrentTrain: epoch  8, batch    26 | loss: 4.0022688Losses:  3.904712677001953 0.05194108188152313
CurrentTrain: epoch  8, batch    27 | loss: 3.9566538Losses:  3.8943843841552734 0.02932676300406456
CurrentTrain: epoch  8, batch    28 | loss: 3.9237111Losses:  3.934924840927124 0.04950019344687462
CurrentTrain: epoch  8, batch    29 | loss: 3.9844251Losses:  3.90950083732605 0.061093784868717194
CurrentTrain: epoch  8, batch    30 | loss: 3.9705946Losses:  3.939462423324585 0.040221430361270905
CurrentTrain: epoch  8, batch    31 | loss: 3.9796839Losses:  3.940622568130493 0.048229776322841644
CurrentTrain: epoch  8, batch    32 | loss: 3.9888523Losses:  3.9274628162384033 0.04489458352327347
CurrentTrain: epoch  8, batch    33 | loss: 3.9723575Losses:  3.9441304206848145 0.028250979259610176
CurrentTrain: epoch  8, batch    34 | loss: 3.9723814Losses:  3.9493050575256348 0.056812580674886703
CurrentTrain: epoch  8, batch    35 | loss: 4.0061178Losses:  3.98077392578125 0.03182707726955414
CurrentTrain: epoch  8, batch    36 | loss: 4.0126009Losses:  3.9770894050598145 0.06955154985189438
CurrentTrain: epoch  8, batch    37 | loss: 4.0466409Losses:  3.9267964363098145 0.04506857693195343
CurrentTrain: epoch  8, batch    38 | loss: 3.9718649Losses:  3.925095558166504 0.05558375269174576
CurrentTrain: epoch  8, batch    39 | loss: 3.9806793Losses:  3.9405951499938965 0.04995404928922653
CurrentTrain: epoch  8, batch    40 | loss: 3.9905491Losses:  3.9379849433898926 0.03522095829248428
CurrentTrain: epoch  8, batch    41 | loss: 3.9732058Losses:  3.9441421031951904 0.032845303416252136
CurrentTrain: epoch  8, batch    42 | loss: 3.9769874Losses:  3.9885244369506836 0.05261271446943283
CurrentTrain: epoch  8, batch    43 | loss: 4.0411372Losses:  3.9243526458740234 0.035949867218732834
CurrentTrain: epoch  8, batch    44 | loss: 3.9603026Losses:  3.9769339561462402 0.038260944187641144
CurrentTrain: epoch  8, batch    45 | loss: 4.0151949Losses:  3.9365487098693848 0.04421024024486542
CurrentTrain: epoch  8, batch    46 | loss: 3.9807589Losses:  3.9537270069122314 0.03720278665423393
CurrentTrain: epoch  8, batch    47 | loss: 3.9909298Losses:  3.913543462753296 0.023019729182124138
CurrentTrain: epoch  8, batch    48 | loss: 3.9365633Losses:  3.9418540000915527 0.043760113418102264
CurrentTrain: epoch  8, batch    49 | loss: 3.9856141Losses:  4.005514144897461 0.037297412753105164
CurrentTrain: epoch  8, batch    50 | loss: 4.0428114Losses:  3.962101459503174 0.03680340573191643
CurrentTrain: epoch  8, batch    51 | loss: 3.9989049Losses:  3.9446544647216797 0.036958567798137665
CurrentTrain: epoch  8, batch    52 | loss: 3.9816129Losses:  3.9078726768493652 0.038070790469646454
CurrentTrain: epoch  8, batch    53 | loss: 3.9459434Losses:  3.9010825157165527 0.030938483774662018
CurrentTrain: epoch  8, batch    54 | loss: 3.9320209Losses:  3.886770486831665 0.03133819252252579
CurrentTrain: epoch  8, batch    55 | loss: 3.9181087Losses:  3.962796449661255 0.02319682389497757
CurrentTrain: epoch  8, batch    56 | loss: 3.9859934Losses:  3.998077869415283 0.035667724907398224
CurrentTrain: epoch  8, batch    57 | loss: 4.0337458Losses:  3.984844923019409 0.039328861981630325
CurrentTrain: epoch  8, batch    58 | loss: 4.0241737Losses:  3.9154868125915527 0.057989515364170074
CurrentTrain: epoch  8, batch    59 | loss: 3.9734764Losses:  3.965884208679199 0.05334901809692383
CurrentTrain: epoch  8, batch    60 | loss: 4.0192332Losses:  3.914231538772583 0.04238048568367958
CurrentTrain: epoch  8, batch    61 | loss: 3.9566121Losses:  3.9202182292938232 0.062015365809202194
CurrentTrain: epoch  8, batch    62 | loss: 3.9822335Losses:  3.9511313438415527 0.04244677722454071
CurrentTrain: epoch  8, batch    63 | loss: 3.9935782Losses:  4.017301559448242 0.037550702691078186
CurrentTrain: epoch  8, batch    64 | loss: 4.0548525Losses:  3.9905638694763184 0.09019829332828522
CurrentTrain: epoch  8, batch    65 | loss: 4.0807624Losses:  3.933770179748535 0.03969946131110191
CurrentTrain: epoch  8, batch    66 | loss: 3.9734697Losses:  3.9558615684509277 0.02698461152613163
CurrentTrain: epoch  8, batch    67 | loss: 3.9828463Losses:  3.889744758605957 0.04961331933736801
CurrentTrain: epoch  8, batch    68 | loss: 3.9393580Losses:  3.906174659729004 0.05255565047264099
CurrentTrain: epoch  8, batch    69 | loss: 3.9587302Losses:  3.9733853340148926 0.027825772762298584
CurrentTrain: epoch  8, batch    70 | loss: 4.0012112Losses:  3.8924450874328613 0.042974840849637985
CurrentTrain: epoch  8, batch    71 | loss: 3.9354200Losses:  3.9187111854553223 0.05865192413330078
CurrentTrain: epoch  8, batch    72 | loss: 3.9773631Losses:  3.8796334266662598 0.08224834501743317
CurrentTrain: epoch  8, batch    73 | loss: 3.9618819Losses:  3.9311869144439697 0.07220171391963959
CurrentTrain: epoch  8, batch    74 | loss: 4.0033884Losses:  3.909855842590332 0.04259221628308296
CurrentTrain: epoch  8, batch    75 | loss: 3.9524481Losses:  3.940938711166382 0.0394701212644577
CurrentTrain: epoch  8, batch    76 | loss: 3.9804089Losses:  3.954406261444092 0.04100402817130089
CurrentTrain: epoch  8, batch    77 | loss: 3.9954102Losses:  3.8686017990112305 0.018308106809854507
CurrentTrain: epoch  8, batch    78 | loss: 3.8869100Losses:  3.974681854248047 0.03490530699491501
CurrentTrain: epoch  8, batch    79 | loss: 4.0095873Losses:  3.908503532409668 0.04379689693450928
CurrentTrain: epoch  8, batch    80 | loss: 3.9523005Losses:  3.97152042388916 0.03918588161468506
CurrentTrain: epoch  8, batch    81 | loss: 4.0107064Losses:  3.929647922515869 0.029605761170387268
CurrentTrain: epoch  8, batch    82 | loss: 3.9592538Losses:  3.961496114730835 0.07024353742599487
CurrentTrain: epoch  8, batch    83 | loss: 4.0317397Losses:  3.9373488426208496 0.03702205419540405
CurrentTrain: epoch  8, batch    84 | loss: 3.9743710Losses:  3.977282762527466 0.06613687425851822
CurrentTrain: epoch  8, batch    85 | loss: 4.0434198Losses:  3.943331718444824 0.042739734053611755
CurrentTrain: epoch  8, batch    86 | loss: 3.9860713Losses:  3.9815139770507812 0.038670122623443604
CurrentTrain: epoch  8, batch    87 | loss: 4.0201840Losses:  3.82893705368042 0.015002602711319923
CurrentTrain: epoch  8, batch    88 | loss: 3.8439395Losses:  3.9294700622558594 0.037376292049884796
CurrentTrain: epoch  8, batch    89 | loss: 3.9668465Losses:  3.786088466644287 0.029908202588558197
CurrentTrain: epoch  8, batch    90 | loss: 3.8159966Losses:  3.9462199211120605 0.04380862042307854
CurrentTrain: epoch  8, batch    91 | loss: 3.9900286Losses:  3.950347423553467 0.06128381937742233
CurrentTrain: epoch  8, batch    92 | loss: 4.0116310Losses:  3.9790635108947754 0.03949055075645447
CurrentTrain: epoch  8, batch    93 | loss: 4.0185542Losses:  3.9529471397399902 0.04591944068670273
CurrentTrain: epoch  8, batch    94 | loss: 3.9988666Losses:  3.891918182373047 0.05109584704041481
CurrentTrain: epoch  8, batch    95 | loss: 3.9430141Losses:  3.906337261199951 0.040191810578107834
CurrentTrain: epoch  8, batch    96 | loss: 3.9465292Losses:  3.895317554473877 0.045032478868961334
CurrentTrain: epoch  8, batch    97 | loss: 3.9403501Losses:  3.9493658542633057 0.06442451477050781
CurrentTrain: epoch  8, batch    98 | loss: 4.0137901Losses:  3.9524145126342773 0.06157431751489639
CurrentTrain: epoch  8, batch    99 | loss: 4.0139890Losses:  4.001940727233887 0.04633225500583649
CurrentTrain: epoch  8, batch   100 | loss: 4.0482731Losses:  3.8947086334228516 0.04280626401305199
CurrentTrain: epoch  8, batch   101 | loss: 3.9375148Losses:  3.9415242671966553 0.03199319913983345
CurrentTrain: epoch  8, batch   102 | loss: 3.9735174Losses:  3.963538646697998 0.06645847856998444
CurrentTrain: epoch  8, batch   103 | loss: 4.0299973Losses:  3.9928126335144043 0.02531968429684639
CurrentTrain: epoch  8, batch   104 | loss: 4.0181322Losses:  3.9832780361175537 0.04484377056360245
CurrentTrain: epoch  8, batch   105 | loss: 4.0281219Losses:  3.965871572494507 0.03768370300531387
CurrentTrain: epoch  8, batch   106 | loss: 4.0035553Losses:  3.9302234649658203 0.03380027785897255
CurrentTrain: epoch  8, batch   107 | loss: 3.9640238Losses:  3.944491386413574 0.02981826476752758
CurrentTrain: epoch  8, batch   108 | loss: 3.9743097Losses:  3.9863452911376953 0.03300897032022476
CurrentTrain: epoch  8, batch   109 | loss: 4.0193543Losses:  3.9532904624938965 0.03691365569829941
CurrentTrain: epoch  8, batch   110 | loss: 3.9902041Losses:  3.9438765048980713 0.03878675401210785
CurrentTrain: epoch  8, batch   111 | loss: 3.9826632Losses:  3.898153781890869 0.023211749270558357
CurrentTrain: epoch  8, batch   112 | loss: 3.9213655Losses:  3.892660140991211 0.02271723374724388
CurrentTrain: epoch  8, batch   113 | loss: 3.9153774Losses:  3.9629881381988525 0.04885876923799515
CurrentTrain: epoch  8, batch   114 | loss: 4.0118470Losses:  3.897036552429199 0.04014427959918976
CurrentTrain: epoch  8, batch   115 | loss: 3.9371808Losses:  3.9216718673706055 0.06374743580818176
CurrentTrain: epoch  8, batch   116 | loss: 3.9854193Losses:  3.9237704277038574 0.037837766110897064
CurrentTrain: epoch  8, batch   117 | loss: 3.9616082Losses:  3.9474544525146484 0.03834209591150284
CurrentTrain: epoch  8, batch   118 | loss: 3.9857965Losses:  3.8751325607299805 0.034041453152894974
CurrentTrain: epoch  8, batch   119 | loss: 3.9091740Losses:  3.923790693283081 0.06598185002803802
CurrentTrain: epoch  8, batch   120 | loss: 3.9897726Losses:  3.8638648986816406 0.027382006868720055
CurrentTrain: epoch  8, batch   121 | loss: 3.8912468Losses:  3.933474540710449 0.031344786286354065
CurrentTrain: epoch  8, batch   122 | loss: 3.9648194Losses:  3.957724094390869 0.06073833629488945
CurrentTrain: epoch  8, batch   123 | loss: 4.0184627Losses:  4.047183990478516 0.015222218818962574
CurrentTrain: epoch  8, batch   124 | loss: 4.0624061Losses:  4.028907299041748 0.03199505805969238
CurrentTrain: epoch  9, batch     0 | loss: 4.0609026Losses:  3.886286973953247 0.026589635759592056
CurrentTrain: epoch  9, batch     1 | loss: 3.9128766Losses:  3.9018211364746094 0.04608958214521408
CurrentTrain: epoch  9, batch     2 | loss: 3.9479108Losses:  3.968599319458008 0.052952900528907776
CurrentTrain: epoch  9, batch     3 | loss: 4.0215521Losses:  3.965095043182373 0.035226669162511826
CurrentTrain: epoch  9, batch     4 | loss: 4.0003219Losses:  3.9116814136505127 0.03862060233950615
CurrentTrain: epoch  9, batch     5 | loss: 3.9503021Losses:  3.928277015686035 0.05938881263136864
CurrentTrain: epoch  9, batch     6 | loss: 3.9876659Losses:  3.8212218284606934 0.017407922074198723
CurrentTrain: epoch  9, batch     7 | loss: 3.8386297Losses:  3.9525468349456787 0.06867985427379608
CurrentTrain: epoch  9, batch     8 | loss: 4.0212269Losses:  3.87310791015625 0.030509639531373978
CurrentTrain: epoch  9, batch     9 | loss: 3.9036176Losses:  3.9501101970672607 0.026947520673274994
CurrentTrain: epoch  9, batch    10 | loss: 3.9770577Losses:  3.9356794357299805 0.047342002391815186
CurrentTrain: epoch  9, batch    11 | loss: 3.9830215Losses:  3.920008659362793 0.03072849102318287
CurrentTrain: epoch  9, batch    12 | loss: 3.9507372Losses:  3.924464225769043 0.042066317051649094
CurrentTrain: epoch  9, batch    13 | loss: 3.9665306Losses:  3.9569976329803467 0.0454251766204834
CurrentTrain: epoch  9, batch    14 | loss: 4.0024228Losses:  3.94046688079834 0.03595799580216408
CurrentTrain: epoch  9, batch    15 | loss: 3.9764249Losses:  3.955626964569092 0.03780446946620941
CurrentTrain: epoch  9, batch    16 | loss: 3.9934313Losses:  3.9032540321350098 0.06534083187580109
CurrentTrain: epoch  9, batch    17 | loss: 3.9685948Losses:  3.910632371902466 0.052806369960308075
CurrentTrain: epoch  9, batch    18 | loss: 3.9634387Losses:  3.931619882583618 0.06481807678937912
CurrentTrain: epoch  9, batch    19 | loss: 3.9964380Losses:  3.917041301727295 0.0833899974822998
CurrentTrain: epoch  9, batch    20 | loss: 4.0004311Losses:  3.9759063720703125 0.04140275716781616
CurrentTrain: epoch  9, batch    21 | loss: 4.0173092Losses:  3.952956438064575 0.025256844237446785
CurrentTrain: epoch  9, batch    22 | loss: 3.9782133Losses:  3.966909885406494 0.043160486966371536
CurrentTrain: epoch  9, batch    23 | loss: 4.0100703Losses:  3.9598236083984375 0.022491253912448883
CurrentTrain: epoch  9, batch    24 | loss: 3.9823148Losses:  3.9682445526123047 0.06942090392112732
CurrentTrain: epoch  9, batch    25 | loss: 4.0376654Losses:  3.9591476917266846 0.05732791870832443
CurrentTrain: epoch  9, batch    26 | loss: 4.0164757Losses:  3.923532724380493 0.023758597671985626
CurrentTrain: epoch  9, batch    27 | loss: 3.9472914Losses:  3.9785239696502686 0.03779368847608566
CurrentTrain: epoch  9, batch    28 | loss: 4.0163178Losses:  3.937812089920044 0.02640734426677227
CurrentTrain: epoch  9, batch    29 | loss: 3.9642193Losses:  3.905649423599243 0.06094038486480713
CurrentTrain: epoch  9, batch    30 | loss: 3.9665899Losses:  3.976982593536377 0.033706434071063995
CurrentTrain: epoch  9, batch    31 | loss: 4.0106893Losses:  3.9189953804016113 0.054534029215574265
CurrentTrain: epoch  9, batch    32 | loss: 3.9735293Losses:  3.973398208618164 0.04117212072014809
CurrentTrain: epoch  9, batch    33 | loss: 4.0145702Losses:  3.9144837856292725 0.038835588842630386
CurrentTrain: epoch  9, batch    34 | loss: 3.9533193Losses:  3.926987648010254 0.03397279605269432
CurrentTrain: epoch  9, batch    35 | loss: 3.9609604Losses:  3.9152517318725586 0.03723836690187454
CurrentTrain: epoch  9, batch    36 | loss: 3.9524901Losses:  3.9484822750091553 0.03523925691843033
CurrentTrain: epoch  9, batch    37 | loss: 3.9837215Losses:  3.905977487564087 0.05990670993924141
CurrentTrain: epoch  9, batch    38 | loss: 3.9658842Losses:  3.9368529319763184 0.0566328689455986
CurrentTrain: epoch  9, batch    39 | loss: 3.9934857Losses:  3.952721357345581 0.04781784489750862
CurrentTrain: epoch  9, batch    40 | loss: 4.0005393Losses:  3.9141221046447754 0.035581864416599274
CurrentTrain: epoch  9, batch    41 | loss: 3.9497039Losses:  3.934161901473999 0.022679749876260757
CurrentTrain: epoch  9, batch    42 | loss: 3.9568417Losses:  3.923792600631714 0.04498833790421486
CurrentTrain: epoch  9, batch    43 | loss: 3.9687810Losses:  3.93760347366333 0.0464981272816658
CurrentTrain: epoch  9, batch    44 | loss: 3.9841015Losses:  3.950777530670166 0.025048676878213882
CurrentTrain: epoch  9, batch    45 | loss: 3.9758263Losses:  3.9296441078186035 0.026539070531725883
CurrentTrain: epoch  9, batch    46 | loss: 3.9561832Losses:  3.9110496044158936 0.025814171880483627
CurrentTrain: epoch  9, batch    47 | loss: 3.9368637Losses:  3.939760684967041 0.07110217213630676
CurrentTrain: epoch  9, batch    48 | loss: 4.0108628Losses:  3.9024834632873535 0.03996458649635315
CurrentTrain: epoch  9, batch    49 | loss: 3.9424481Losses:  3.897738456726074 0.07622136175632477
CurrentTrain: epoch  9, batch    50 | loss: 3.9739599Losses:  3.9085397720336914 0.04067085310816765
CurrentTrain: epoch  9, batch    51 | loss: 3.9492106Losses:  3.9221224784851074 0.023966122418642044
CurrentTrain: epoch  9, batch    52 | loss: 3.9460886Losses:  3.975210666656494 0.035658176988363266
CurrentTrain: epoch  9, batch    53 | loss: 4.0108690Losses:  3.9302215576171875 0.04789150133728981
CurrentTrain: epoch  9, batch    54 | loss: 3.9781132Losses:  3.8684725761413574 0.013199733570218086
CurrentTrain: epoch  9, batch    55 | loss: 3.8816724Losses:  3.91506290435791 0.03174091875553131
CurrentTrain: epoch  9, batch    56 | loss: 3.9468038Losses:  3.924513816833496 0.04550750181078911
CurrentTrain: epoch  9, batch    57 | loss: 3.9700212Losses:  4.009771347045898 0.02858458086848259
CurrentTrain: epoch  9, batch    58 | loss: 4.0383558Losses:  3.9469072818756104 0.048079125583171844
CurrentTrain: epoch  9, batch    59 | loss: 3.9949863Losses:  3.961785316467285 0.035080865025520325
CurrentTrain: epoch  9, batch    60 | loss: 3.9968662Losses:  3.9021658897399902 0.05455595254898071
CurrentTrain: epoch  9, batch    61 | loss: 3.9567218Losses:  3.9430348873138428 0.03687352314591408
CurrentTrain: epoch  9, batch    62 | loss: 3.9799085Losses:  3.9226107597351074 0.05140886455774307
CurrentTrain: epoch  9, batch    63 | loss: 3.9740195Losses:  3.8853440284729004 0.07081206142902374
CurrentTrain: epoch  9, batch    64 | loss: 3.9561560Losses:  3.939520835876465 0.05238298326730728
CurrentTrain: epoch  9, batch    65 | loss: 3.9919038Losses:  3.983907699584961 0.04059287905693054
CurrentTrain: epoch  9, batch    66 | loss: 4.0245004Losses:  3.965181827545166 0.035693444311618805
CurrentTrain: epoch  9, batch    67 | loss: 4.0008755Losses:  3.930434226989746 0.026057686656713486
CurrentTrain: epoch  9, batch    68 | loss: 3.9564919Losses:  3.932265043258667 0.06175094470381737
CurrentTrain: epoch  9, batch    69 | loss: 3.9940159Losses:  3.9225192070007324 0.055140864104032516
CurrentTrain: epoch  9, batch    70 | loss: 3.9776602Losses:  3.898712635040283 0.05180139094591141
CurrentTrain: epoch  9, batch    71 | loss: 3.9505141Losses:  3.9121780395507812 0.05370595306158066
CurrentTrain: epoch  9, batch    72 | loss: 3.9658840Losses:  3.9148106575012207 0.027952095493674278
CurrentTrain: epoch  9, batch    73 | loss: 3.9427629Losses:  3.9489312171936035 0.03928358107805252
CurrentTrain: epoch  9, batch    74 | loss: 3.9882147Losses:  3.986175537109375 0.05572175234556198
CurrentTrain: epoch  9, batch    75 | loss: 4.0418973Losses:  3.9260592460632324 0.06284325569868088
CurrentTrain: epoch  9, batch    76 | loss: 3.9889026Losses:  3.8979179859161377 0.0535140223801136
CurrentTrain: epoch  9, batch    77 | loss: 3.9514320Losses:  3.945206880569458 0.05021364241838455
CurrentTrain: epoch  9, batch    78 | loss: 3.9954205Losses:  3.897325038909912 0.05070668086409569
CurrentTrain: epoch  9, batch    79 | loss: 3.9480317Losses:  3.9652950763702393 0.05059729516506195
CurrentTrain: epoch  9, batch    80 | loss: 4.0158925Losses:  3.930833339691162 0.04372473433613777
CurrentTrain: epoch  9, batch    81 | loss: 3.9745581Losses:  3.9709999561309814 0.05174042284488678
CurrentTrain: epoch  9, batch    82 | loss: 4.0227404Losses:  3.9585256576538086 0.06642527133226395
CurrentTrain: epoch  9, batch    83 | loss: 4.0249510Losses:  3.9472126960754395 0.05160747468471527
CurrentTrain: epoch  9, batch    84 | loss: 3.9988201Losses:  3.9384384155273438 0.05009268969297409
CurrentTrain: epoch  9, batch    85 | loss: 3.9885311Losses:  3.887759208679199 0.04991402104496956
CurrentTrain: epoch  9, batch    86 | loss: 3.9376733Losses:  3.9568774700164795 0.03418795019388199
CurrentTrain: epoch  9, batch    87 | loss: 3.9910655Losses:  3.81319522857666 0.03789565712213516
CurrentTrain: epoch  9, batch    88 | loss: 3.8510909Losses:  3.9681191444396973 0.04647568613290787
CurrentTrain: epoch  9, batch    89 | loss: 4.0145950Losses:  3.8803515434265137 0.05247969180345535
CurrentTrain: epoch  9, batch    90 | loss: 3.9328313Losses:  3.953713893890381 0.046768657863140106
CurrentTrain: epoch  9, batch    91 | loss: 4.0004826Losses:  3.933805465698242 0.044208064675331116
CurrentTrain: epoch  9, batch    92 | loss: 3.9780135Losses:  3.8820912837982178 0.03579854965209961
CurrentTrain: epoch  9, batch    93 | loss: 3.9178898Losses:  3.9179091453552246 0.019800081849098206
CurrentTrain: epoch  9, batch    94 | loss: 3.9377093Losses:  3.9652204513549805 0.03447985276579857
CurrentTrain: epoch  9, batch    95 | loss: 3.9997003Losses:  4.00108528137207 0.026450954377651215
CurrentTrain: epoch  9, batch    96 | loss: 4.0275364Losses:  3.9396719932556152 0.04121674224734306
CurrentTrain: epoch  9, batch    97 | loss: 3.9808888Losses:  3.929220676422119 0.04399237781763077
CurrentTrain: epoch  9, batch    98 | loss: 3.9732130Losses:  3.888573169708252 0.021877851337194443
CurrentTrain: epoch  9, batch    99 | loss: 3.9104509Losses:  3.968567371368408 0.03201627358794212
CurrentTrain: epoch  9, batch   100 | loss: 4.0005836Losses:  3.984144449234009 0.02499386854469776
CurrentTrain: epoch  9, batch   101 | loss: 4.0091381Losses:  3.9223175048828125 0.0421871617436409
CurrentTrain: epoch  9, batch   102 | loss: 3.9645047Losses:  3.912562847137451 0.055937759578228
CurrentTrain: epoch  9, batch   103 | loss: 3.9685006Losses:  3.944267749786377 0.028609275817871094
CurrentTrain: epoch  9, batch   104 | loss: 3.9728770Losses:  3.923309326171875 0.07810062915086746
CurrentTrain: epoch  9, batch   105 | loss: 4.0014100Losses:  3.868192672729492 0.028180290013551712
CurrentTrain: epoch  9, batch   106 | loss: 3.8963730Losses:  3.9756522178649902 0.05204405263066292
CurrentTrain: epoch  9, batch   107 | loss: 4.0276961Losses:  3.907099723815918 0.03511934354901314
CurrentTrain: epoch  9, batch   108 | loss: 3.9422190Losses:  3.9565439224243164 0.04901698976755142
CurrentTrain: epoch  9, batch   109 | loss: 4.0055609Losses:  3.8863863945007324 0.01189676858484745
CurrentTrain: epoch  9, batch   110 | loss: 3.8982832Losses:  3.9751806259155273 0.03413083404302597
CurrentTrain: epoch  9, batch   111 | loss: 4.0093117Losses:  3.946575403213501 0.013267774134874344
CurrentTrain: epoch  9, batch   112 | loss: 3.9598432Losses:  3.918071746826172 0.048904988914728165
CurrentTrain: epoch  9, batch   113 | loss: 3.9669766Losses:  3.9134750366210938 0.05130128934979439
CurrentTrain: epoch  9, batch   114 | loss: 3.9647763Losses:  3.9341201782226562 0.04775233566761017
CurrentTrain: epoch  9, batch   115 | loss: 3.9818726Losses:  3.9396493434906006 0.03764038532972336
CurrentTrain: epoch  9, batch   116 | loss: 3.9772897Losses:  3.9373583793640137 0.012992536649107933
CurrentTrain: epoch  9, batch   117 | loss: 3.9503510Losses:  3.9805917739868164 0.028034502640366554
CurrentTrain: epoch  9, batch   118 | loss: 4.0086265Losses:  3.9603819847106934 0.0633915513753891
CurrentTrain: epoch  9, batch   119 | loss: 4.0237737Losses:  3.905252456665039 0.06870801746845245
CurrentTrain: epoch  9, batch   120 | loss: 3.9739604Losses:  3.9162073135375977 0.047770366072654724
CurrentTrain: epoch  9, batch   121 | loss: 3.9639776Losses:  3.968628168106079 0.031073614954948425
CurrentTrain: epoch  9, batch   122 | loss: 3.9997017Losses:  3.96505069732666 0.030890483409166336
CurrentTrain: epoch  9, batch   123 | loss: 3.9959412Losses:  3.9497926235198975 0.041031256318092346
CurrentTrain: epoch  9, batch   124 | loss: 3.9908240
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  8.428013801574707 1.4036176204681396
CurrentTrain: epoch  0, batch     0 | loss: 9.8316317Losses:  10.935344696044922 1.4091951847076416
CurrentTrain: epoch  0, batch     1 | loss: 12.3445396Losses:  11.515424728393555 1.8981348276138306
CurrentTrain: epoch  0, batch     2 | loss: 13.4135599Losses:  9.065069198608398 1.8616371154785156
CurrentTrain: epoch  0, batch     3 | loss: 10.9267063Losses:  9.155214309692383 1.3090336322784424
CurrentTrain: epoch  0, batch     4 | loss: 10.4642477Losses:  7.70955228805542 1.2006444931030273
CurrentTrain: epoch  0, batch     5 | loss: 8.9101963Losses:  6.9924540519714355 0.6099300384521484
CurrentTrain: epoch  0, batch     6 | loss: 7.6023841Losses:  3.793192148208618 1.0380496978759766
CurrentTrain: epoch  1, batch     0 | loss: 4.8312416Losses:  4.715088844299316 1.3451182842254639
CurrentTrain: epoch  1, batch     1 | loss: 6.0602074Losses:  4.410374641418457 1.2302463054656982
CurrentTrain: epoch  1, batch     2 | loss: 5.6406212Losses:  5.780303001403809 1.64973783493042
CurrentTrain: epoch  1, batch     3 | loss: 7.4300408Losses:  5.575387001037598 0.9529378414154053
CurrentTrain: epoch  1, batch     4 | loss: 6.5283251Losses:  4.13180685043335 1.1069238185882568
CurrentTrain: epoch  1, batch     5 | loss: 5.2387304Losses:  6.615118980407715 0.8131650686264038
CurrentTrain: epoch  1, batch     6 | loss: 7.4282842Losses:  4.255546569824219 1.3382810354232788
CurrentTrain: epoch  2, batch     0 | loss: 5.5938277Losses:  3.9953577518463135 0.794455349445343
CurrentTrain: epoch  2, batch     1 | loss: 4.7898130Losses:  3.898033618927002 1.2209879159927368
CurrentTrain: epoch  2, batch     2 | loss: 5.1190214Losses:  3.9573488235473633 1.3541195392608643
CurrentTrain: epoch  2, batch     3 | loss: 5.3114681Losses:  5.491412162780762 0.9616146087646484
CurrentTrain: epoch  2, batch     4 | loss: 6.4530268Losses:  3.2295899391174316 1.1840450763702393
CurrentTrain: epoch  2, batch     5 | loss: 4.4136353Losses:  4.435044288635254 0.2413361519575119
CurrentTrain: epoch  2, batch     6 | loss: 4.6763806Losses:  3.7896225452423096 0.7395890951156616
CurrentTrain: epoch  3, batch     0 | loss: 4.5292115Losses:  4.279820442199707 1.1906445026397705
CurrentTrain: epoch  3, batch     1 | loss: 5.4704647Losses:  2.954810857772827 0.7824060916900635
CurrentTrain: epoch  3, batch     2 | loss: 3.7372169Losses:  3.4796557426452637 0.8906012177467346
CurrentTrain: epoch  3, batch     3 | loss: 4.3702569Losses:  3.745945930480957 1.0852413177490234
CurrentTrain: epoch  3, batch     4 | loss: 4.8311872Losses:  4.526073455810547 0.8378450274467468
CurrentTrain: epoch  3, batch     5 | loss: 5.3639183Losses:  4.870279788970947 0.4260314106941223
CurrentTrain: epoch  3, batch     6 | loss: 5.2963114Losses:  3.7269721031188965 1.0525115728378296
CurrentTrain: epoch  4, batch     0 | loss: 4.7794838Losses:  3.0810434818267822 0.8969185948371887
CurrentTrain: epoch  4, batch     1 | loss: 3.9779620Losses:  3.9140920639038086 1.0635921955108643
CurrentTrain: epoch  4, batch     2 | loss: 4.9776840Losses:  3.071763515472412 1.0511479377746582
CurrentTrain: epoch  4, batch     3 | loss: 4.1229115Losses:  4.711652755737305 0.593201756477356
CurrentTrain: epoch  4, batch     4 | loss: 5.3048544Losses:  3.169954776763916 1.078658938407898
CurrentTrain: epoch  4, batch     5 | loss: 4.2486138Losses:  2.089374542236328 0.23723991215229034
CurrentTrain: epoch  4, batch     6 | loss: 2.3266144Losses:  3.6027870178222656 0.8837025761604309
CurrentTrain: epoch  5, batch     0 | loss: 4.4864898Losses:  3.7721242904663086 1.0285732746124268
CurrentTrain: epoch  5, batch     1 | loss: 4.8006973Losses:  3.3326003551483154 0.9057840704917908
CurrentTrain: epoch  5, batch     2 | loss: 4.2383842Losses:  3.4295506477355957 0.8551151156425476
CurrentTrain: epoch  5, batch     3 | loss: 4.2846656Losses:  2.772538423538208 0.7584601044654846
CurrentTrain: epoch  5, batch     4 | loss: 3.5309985Losses:  3.712144136428833 0.9332510828971863
CurrentTrain: epoch  5, batch     5 | loss: 4.6453953Losses:  1.95383882522583 0.1330127865076065
CurrentTrain: epoch  5, batch     6 | loss: 2.0868516Losses:  3.0438780784606934 0.8371108770370483
CurrentTrain: epoch  6, batch     0 | loss: 3.8809891Losses:  2.299001693725586 0.4873734712600708
CurrentTrain: epoch  6, batch     1 | loss: 2.7863750Losses:  3.9226508140563965 0.8647480010986328
CurrentTrain: epoch  6, batch     2 | loss: 4.7873988Losses:  2.388162612915039 0.7283369898796082
CurrentTrain: epoch  6, batch     3 | loss: 3.1164997Losses:  3.2849767208099365 0.9543758034706116
CurrentTrain: epoch  6, batch     4 | loss: 4.2393527Losses:  3.6100566387176514 0.8607938289642334
CurrentTrain: epoch  6, batch     5 | loss: 4.4708505Losses:  3.545135974884033 0.08908121287822723
CurrentTrain: epoch  6, batch     6 | loss: 3.6342173Losses:  3.60438871383667 1.0611348152160645
CurrentTrain: epoch  7, batch     0 | loss: 4.6655235Losses:  2.63480806350708 0.7041415572166443
CurrentTrain: epoch  7, batch     1 | loss: 3.3389497Losses:  3.4432220458984375 0.8737402558326721
CurrentTrain: epoch  7, batch     2 | loss: 4.3169622Losses:  2.7150678634643555 0.5158854722976685
CurrentTrain: epoch  7, batch     3 | loss: 3.2309532Losses:  2.231250286102295 0.5408108234405518
CurrentTrain: epoch  7, batch     4 | loss: 2.7720611Losses:  2.8062915802001953 1.0297696590423584
CurrentTrain: epoch  7, batch     5 | loss: 3.8360612Losses:  2.0715088844299316 0.13519474864006042
CurrentTrain: epoch  7, batch     6 | loss: 2.2067037Losses:  2.6073572635650635 0.6773573160171509
CurrentTrain: epoch  8, batch     0 | loss: 3.2847147Losses:  3.3190884590148926 0.803978443145752
CurrentTrain: epoch  8, batch     1 | loss: 4.1230669Losses:  2.611980676651001 0.7126222848892212
CurrentTrain: epoch  8, batch     2 | loss: 3.3246031Losses:  2.7812976837158203 0.6913630962371826
CurrentTrain: epoch  8, batch     3 | loss: 3.4726608Losses:  2.2341339588165283 0.5792656540870667
CurrentTrain: epoch  8, batch     4 | loss: 2.8133996Losses:  2.702725887298584 0.5378984808921814
CurrentTrain: epoch  8, batch     5 | loss: 3.2406244Losses:  1.6503829956054688 0.0
CurrentTrain: epoch  8, batch     6 | loss: 1.6503830Losses:  2.2753257751464844 0.4851415157318115
CurrentTrain: epoch  9, batch     0 | loss: 2.7604673Losses:  2.3022148609161377 0.5657526850700378
CurrentTrain: epoch  9, batch     1 | loss: 2.8679676Losses:  2.4835293292999268 0.876126229763031
CurrentTrain: epoch  9, batch     2 | loss: 3.3596556Losses:  2.2966160774230957 0.5904413461685181
CurrentTrain: epoch  9, batch     3 | loss: 2.8870573Losses:  2.6052887439727783 0.7205817103385925
CurrentTrain: epoch  9, batch     4 | loss: 3.3258705Losses:  2.157897472381592 0.5190548896789551
CurrentTrain: epoch  9, batch     5 | loss: 2.6769524Losses:  2.761728525161743 0.36976101994514465
CurrentTrain: epoch  9, batch     6 | loss: 3.1314895
Losses:  5.510899543762207 0.7188640832901001
MemoryTrain:  epoch  0, batch     0 | loss: 6.2297635Losses:  9.838628768920898 0.2952576279640198
MemoryTrain:  epoch  0, batch     1 | loss: 10.1338863Losses:  10.376067161560059 0.13854457437992096
MemoryTrain:  epoch  0, batch     2 | loss: 10.5146122Losses:  0.6936006546020508 0.6131969690322876
MemoryTrain:  epoch  1, batch     0 | loss: 1.3067976Losses:  0.8585871458053589 0.6057164669036865
MemoryTrain:  epoch  1, batch     1 | loss: 1.4643036Losses:  0.853559136390686 0.08927442878484726
MemoryTrain:  epoch  1, batch     2 | loss: 0.9428335Losses:  0.841833233833313 0.4151792526245117
MemoryTrain:  epoch  2, batch     0 | loss: 1.2570125Losses:  0.3708440363407135 0.37440985441207886
MemoryTrain:  epoch  2, batch     1 | loss: 0.7452539Losses:  0.3677492141723633 0.19683706760406494
MemoryTrain:  epoch  2, batch     2 | loss: 0.5645863Losses:  0.21618247032165527 0.4813448190689087
MemoryTrain:  epoch  3, batch     0 | loss: 0.6975273Losses:  0.2957516312599182 0.22061850130558014
MemoryTrain:  epoch  3, batch     1 | loss: 0.5163701Losses:  0.529893159866333 0.2967199683189392
MemoryTrain:  epoch  3, batch     2 | loss: 0.8266131Losses:  0.24563997983932495 0.5393954515457153
MemoryTrain:  epoch  4, batch     0 | loss: 0.7850354Losses:  0.239898681640625 0.35022783279418945
MemoryTrain:  epoch  4, batch     1 | loss: 0.5901265Losses:  0.154084250330925 0.20857585966587067
MemoryTrain:  epoch  4, batch     2 | loss: 0.3626601Losses:  0.3183576762676239 0.5292730331420898
MemoryTrain:  epoch  5, batch     0 | loss: 0.8476307Losses:  0.15313971042633057 0.3655877709388733
MemoryTrain:  epoch  5, batch     1 | loss: 0.5187275Losses:  0.20649567246437073 0.2652958333492279
MemoryTrain:  epoch  5, batch     2 | loss: 0.4717915Losses:  0.20587897300720215 0.46978405117988586
MemoryTrain:  epoch  6, batch     0 | loss: 0.6756630Losses:  0.1991494596004486 0.4136021137237549
MemoryTrain:  epoch  6, batch     1 | loss: 0.6127516Losses:  0.13555750250816345 0.19692011177539825
MemoryTrain:  epoch  6, batch     2 | loss: 0.3324776Losses:  0.15699531137943268 0.3301329016685486
MemoryTrain:  epoch  7, batch     0 | loss: 0.4871282Losses:  0.16906017065048218 0.3814751207828522
MemoryTrain:  epoch  7, batch     1 | loss: 0.5505353Losses:  0.2269519567489624 0.20327270030975342
MemoryTrain:  epoch  7, batch     2 | loss: 0.4302247Losses:  0.14209890365600586 0.32167789340019226
MemoryTrain:  epoch  8, batch     0 | loss: 0.4637768Losses:  0.17809805274009705 0.40172117948532104
MemoryTrain:  epoch  8, batch     1 | loss: 0.5798192Losses:  0.12872087955474854 0.1869887411594391
MemoryTrain:  epoch  8, batch     2 | loss: 0.3157096Losses:  0.12842828035354614 0.3426917791366577
MemoryTrain:  epoch  9, batch     0 | loss: 0.4711201Losses:  0.19010546803474426 0.4212381839752197
MemoryTrain:  epoch  9, batch     1 | loss: 0.6113436Losses:  0.10037563741207123 0.05790384113788605
MemoryTrain:  epoch  9, batch     2 | loss: 0.1582795
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.53%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.90%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.96%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 93.35%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 92.38%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.96%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 87.68%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.77%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 87.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.34%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 87.20%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 86.90%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 86.69%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.63%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 86.71%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.94%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 87.09%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 87.37%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 88.05%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 87.99%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.38%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 87.10%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 86.59%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 86.26%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 85.88%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 85.84%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.86%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 85.95%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 85.96%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 85.57%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 85.43%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 85.30%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 84.96%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 84.78%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 84.70%   
cur_acc:  ['0.9474', '0.7490']
his_acc:  ['0.9474', '0.8470']
Clustering into  14  clusters
Clusters:  [ 2 11 13  2  2  2  0  2  9  7  6  2  8  2 10  0  4  2  2  2  0 12  6  2
  2  3  2  1  2  5]
Losses:  7.5627336502075195 1.6071586608886719
CurrentTrain: epoch  0, batch     0 | loss: 9.1698923Losses:  8.464315414428711 1.1622965335845947
CurrentTrain: epoch  0, batch     1 | loss: 9.6266117Losses:  8.111093521118164 1.2329936027526855
CurrentTrain: epoch  0, batch     2 | loss: 9.3440876Losses:  8.003538131713867 1.4332408905029297
CurrentTrain: epoch  0, batch     3 | loss: 9.4367790Losses:  7.653031349182129 1.3056917190551758
CurrentTrain: epoch  0, batch     4 | loss: 8.9587231Losses:  6.374213218688965 1.5153024196624756
CurrentTrain: epoch  0, batch     5 | loss: 7.8895159Losses:  6.298464775085449 0.4286915957927704
CurrentTrain: epoch  0, batch     6 | loss: 6.7271562Losses:  4.039120674133301 1.2038965225219727
CurrentTrain: epoch  1, batch     0 | loss: 5.2430172Losses:  4.012936592102051 1.4744434356689453
CurrentTrain: epoch  1, batch     1 | loss: 5.4873800Losses:  4.626980781555176 1.2676608562469482
CurrentTrain: epoch  1, batch     2 | loss: 5.8946419Losses:  2.635976791381836 0.9145551919937134
CurrentTrain: epoch  1, batch     3 | loss: 3.5505319Losses:  4.034211158752441 1.0998492240905762
CurrentTrain: epoch  1, batch     4 | loss: 5.1340604Losses:  3.435007095336914 1.306705355644226
CurrentTrain: epoch  1, batch     5 | loss: 4.7417126Losses:  3.470846176147461 0.5124129056930542
CurrentTrain: epoch  1, batch     6 | loss: 3.9832592Losses:  3.6720426082611084 1.2535371780395508
CurrentTrain: epoch  2, batch     0 | loss: 4.9255800Losses:  3.493654251098633 0.8763408660888672
CurrentTrain: epoch  2, batch     1 | loss: 4.3699951Losses:  2.824514627456665 0.8913005590438843
CurrentTrain: epoch  2, batch     2 | loss: 3.7158151Losses:  3.847792387008667 1.2196218967437744
CurrentTrain: epoch  2, batch     3 | loss: 5.0674143Losses:  3.132174491882324 1.2434732913970947
CurrentTrain: epoch  2, batch     4 | loss: 4.3756475Losses:  3.7408347129821777 0.8903835415840149
CurrentTrain: epoch  2, batch     5 | loss: 4.6312184Losses:  2.423962116241455 0.16205894947052002
CurrentTrain: epoch  2, batch     6 | loss: 2.5860209Losses:  2.9436452388763428 0.6977945566177368
CurrentTrain: epoch  3, batch     0 | loss: 3.6414399Losses:  2.3361117839813232 0.7029162645339966
CurrentTrain: epoch  3, batch     1 | loss: 3.0390282Losses:  3.44606876373291 0.9027853012084961
CurrentTrain: epoch  3, batch     2 | loss: 4.3488541Losses:  3.092888593673706 0.4953543543815613
CurrentTrain: epoch  3, batch     3 | loss: 3.5882430Losses:  3.728031635284424 1.359297752380371
CurrentTrain: epoch  3, batch     4 | loss: 5.0873294Losses:  3.3882832527160645 1.0802863836288452
CurrentTrain: epoch  3, batch     5 | loss: 4.4685698Losses:  3.2433719635009766 0.1681915521621704
CurrentTrain: epoch  3, batch     6 | loss: 3.4115634Losses:  2.942655563354492 0.9051786065101624
CurrentTrain: epoch  4, batch     0 | loss: 3.8478341Losses:  3.4466350078582764 1.1621215343475342
CurrentTrain: epoch  4, batch     1 | loss: 4.6087565Losses:  3.0707743167877197 0.8383122682571411
CurrentTrain: epoch  4, batch     2 | loss: 3.9090867Losses:  2.6802988052368164 0.721490740776062
CurrentTrain: epoch  4, batch     3 | loss: 3.4017897Losses:  2.5453057289123535 0.6116694808006287
CurrentTrain: epoch  4, batch     4 | loss: 3.1569753Losses:  2.4948651790618896 0.8150153160095215
CurrentTrain: epoch  4, batch     5 | loss: 3.3098805Losses:  3.7367336750030518 0.2472931146621704
CurrentTrain: epoch  4, batch     6 | loss: 3.9840269Losses:  2.328092575073242 0.8363937139511108
CurrentTrain: epoch  5, batch     0 | loss: 3.1644864Losses:  3.1104302406311035 0.8367104530334473
CurrentTrain: epoch  5, batch     1 | loss: 3.9471407Losses:  2.9297497272491455 0.7799772024154663
CurrentTrain: epoch  5, batch     2 | loss: 3.7097268Losses:  2.72481369972229 0.5784621238708496
CurrentTrain: epoch  5, batch     3 | loss: 3.3032758Losses:  2.603573799133301 0.604068398475647
CurrentTrain: epoch  5, batch     4 | loss: 3.2076421Losses:  2.5269365310668945 0.708370566368103
CurrentTrain: epoch  5, batch     5 | loss: 3.2353072Losses:  3.0719199180603027 0.2228047251701355
CurrentTrain: epoch  5, batch     6 | loss: 3.2947247Losses:  2.837219715118408 0.7319890260696411
CurrentTrain: epoch  6, batch     0 | loss: 3.5692086Losses:  2.705112934112549 0.9473561644554138
CurrentTrain: epoch  6, batch     1 | loss: 3.6524692Losses:  2.525101661682129 0.4909238815307617
CurrentTrain: epoch  6, batch     2 | loss: 3.0160255Losses:  2.4541351795196533 0.6442127227783203
CurrentTrain: epoch  6, batch     3 | loss: 3.0983479Losses:  2.315195083618164 0.715631365776062
CurrentTrain: epoch  6, batch     4 | loss: 3.0308266Losses:  2.4027438163757324 0.6171653866767883
CurrentTrain: epoch  6, batch     5 | loss: 3.0199091Losses:  3.2980031967163086 0.3501294255256653
CurrentTrain: epoch  6, batch     6 | loss: 3.6481326Losses:  2.2232131958007812 0.6519481539726257
CurrentTrain: epoch  7, batch     0 | loss: 2.8751614Losses:  2.4486243724823 0.5121058225631714
CurrentTrain: epoch  7, batch     1 | loss: 2.9607301Losses:  2.9332666397094727 0.9083138704299927
CurrentTrain: epoch  7, batch     2 | loss: 3.8415804Losses:  2.130082368850708 0.405778706073761
CurrentTrain: epoch  7, batch     3 | loss: 2.5358610Losses:  2.3517508506774902 0.45389923453330994
CurrentTrain: epoch  7, batch     4 | loss: 2.8056500Losses:  2.562356948852539 0.7910157442092896
CurrentTrain: epoch  7, batch     5 | loss: 3.3533726Losses:  1.9255986213684082 0.1462637484073639
CurrentTrain: epoch  7, batch     6 | loss: 2.0718625Losses:  2.2970829010009766 0.5338446497917175
CurrentTrain: epoch  8, batch     0 | loss: 2.8309276Losses:  2.109530448913574 0.4395771324634552
CurrentTrain: epoch  8, batch     1 | loss: 2.5491076Losses:  2.0376715660095215 0.4663107991218567
CurrentTrain: epoch  8, batch     2 | loss: 2.5039823Losses:  2.7171740531921387 0.6083248853683472
CurrentTrain: epoch  8, batch     3 | loss: 3.3254991Losses:  1.9048911333084106 0.3681844472885132
CurrentTrain: epoch  8, batch     4 | loss: 2.2730756Losses:  2.3565876483917236 0.5313799381256104
CurrentTrain: epoch  8, batch     5 | loss: 2.8879676Losses:  2.943082332611084 0.18977561593055725
CurrentTrain: epoch  8, batch     6 | loss: 3.1328580Losses:  2.556375503540039 0.4820074141025543
CurrentTrain: epoch  9, batch     0 | loss: 3.0383830Losses:  2.19980525970459 0.5433947443962097
CurrentTrain: epoch  9, batch     1 | loss: 2.7432001Losses:  2.2534008026123047 0.5866631269454956
CurrentTrain: epoch  9, batch     2 | loss: 2.8400640Losses:  1.8409563302993774 0.36698049306869507
CurrentTrain: epoch  9, batch     3 | loss: 2.2079368Losses:  2.159052848815918 0.542687177658081
CurrentTrain: epoch  9, batch     4 | loss: 2.7017400Losses:  2.151796340942383 0.5430328249931335
CurrentTrain: epoch  9, batch     5 | loss: 2.6948292Losses:  1.9064419269561768 0.1087331548333168
CurrentTrain: epoch  9, batch     6 | loss: 2.0151751
Losses:  5.77879524230957 0.4498879611492157
MemoryTrain:  epoch  0, batch     0 | loss: 6.2286830Losses:  9.0361328125 0.6139947175979614
MemoryTrain:  epoch  0, batch     1 | loss: 9.6501274Losses:  10.138036727905273 0.41816604137420654
MemoryTrain:  epoch  0, batch     2 | loss: 10.5562029Losses:  10.942483901977539 0.42170485854148865
MemoryTrain:  epoch  0, batch     3 | loss: 11.3641891Losses:  0.5812687873840332 0.551388680934906
MemoryTrain:  epoch  1, batch     0 | loss: 1.1326575Losses:  1.3867249488830566 0.4517976641654968
MemoryTrain:  epoch  1, batch     1 | loss: 1.8385227Losses:  0.5864667296409607 0.42938554286956787
MemoryTrain:  epoch  1, batch     2 | loss: 1.0158522Losses:  0.5151106715202332 0.3539522588253021
MemoryTrain:  epoch  1, batch     3 | loss: 0.8690629Losses:  0.685382604598999 0.5884779691696167
MemoryTrain:  epoch  2, batch     0 | loss: 1.2738606Losses:  0.6109250783920288 0.45670685172080994
MemoryTrain:  epoch  2, batch     1 | loss: 1.0676320Losses:  0.2776804566383362 0.34423667192459106
MemoryTrain:  epoch  2, batch     2 | loss: 0.6219171Losses:  0.4523780047893524 0.47421973943710327
MemoryTrain:  epoch  2, batch     3 | loss: 0.9265977Losses:  0.3088754415512085 0.35005056858062744
MemoryTrain:  epoch  3, batch     0 | loss: 0.6589260Losses:  0.2797989547252655 0.3332197666168213
MemoryTrain:  epoch  3, batch     1 | loss: 0.6130188Losses:  0.3484002351760864 0.3777604103088379
MemoryTrain:  epoch  3, batch     2 | loss: 0.7261606Losses:  0.5666877627372742 0.7242233157157898
MemoryTrain:  epoch  3, batch     3 | loss: 1.2909111Losses:  0.2864609956741333 0.7182050943374634
MemoryTrain:  epoch  4, batch     0 | loss: 1.0046661Losses:  0.3145354688167572 0.39260461926460266
MemoryTrain:  epoch  4, batch     1 | loss: 0.7071401Losses:  0.22739851474761963 0.35243451595306396
MemoryTrain:  epoch  4, batch     2 | loss: 0.5798330Losses:  0.3146480321884155 0.2612040936946869
MemoryTrain:  epoch  4, batch     3 | loss: 0.5758522Losses:  0.3114502727985382 0.6131503582000732
MemoryTrain:  epoch  5, batch     0 | loss: 0.9246006Losses:  0.28061842918395996 0.37838298082351685
MemoryTrain:  epoch  5, batch     1 | loss: 0.6590014Losses:  0.20148949325084686 0.27778753638267517
MemoryTrain:  epoch  5, batch     2 | loss: 0.4792770Losses:  0.22096022963523865 0.29760652780532837
MemoryTrain:  epoch  5, batch     3 | loss: 0.5185667Losses:  0.29455822706222534 0.3793349266052246
MemoryTrain:  epoch  6, batch     0 | loss: 0.6738932Losses:  0.1978597193956375 0.3244974911212921
MemoryTrain:  epoch  6, batch     1 | loss: 0.5223572Losses:  0.21811947226524353 0.42467808723449707
MemoryTrain:  epoch  6, batch     2 | loss: 0.6427976Losses:  0.26280105113983154 0.5815607905387878
MemoryTrain:  epoch  6, batch     3 | loss: 0.8443618Losses:  0.15158945322036743 0.19037926197052002
MemoryTrain:  epoch  7, batch     0 | loss: 0.3419687Losses:  0.272003173828125 0.7093223929405212
MemoryTrain:  epoch  7, batch     1 | loss: 0.9813256Losses:  0.2177010178565979 0.3057626485824585
MemoryTrain:  epoch  7, batch     2 | loss: 0.5234637Losses:  0.27359718084335327 0.2660706639289856
MemoryTrain:  epoch  7, batch     3 | loss: 0.5396678Losses:  0.1989378184080124 0.3674558699131012
MemoryTrain:  epoch  8, batch     0 | loss: 0.5663937Losses:  0.24312983453273773 0.5183663368225098
MemoryTrain:  epoch  8, batch     1 | loss: 0.7614962Losses:  0.2519599497318268 0.3966810703277588
MemoryTrain:  epoch  8, batch     2 | loss: 0.6486410Losses:  0.20789554715156555 0.2076273262500763
MemoryTrain:  epoch  8, batch     3 | loss: 0.4155229Losses:  0.14143522083759308 0.22131234407424927
MemoryTrain:  epoch  9, batch     0 | loss: 0.3627476Losses:  0.21270744502544403 0.4410431981086731
MemoryTrain:  epoch  9, batch     1 | loss: 0.6537507Losses:  0.184829980134964 0.36908942461013794
MemoryTrain:  epoch  9, batch     2 | loss: 0.5539194Losses:  0.24057376384735107 0.30576586723327637
MemoryTrain:  epoch  9, batch     3 | loss: 0.5463396
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 60.08%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 61.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 69.68%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.04%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.10%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.21%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 90.43%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 89.23%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 88.16%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 86.58%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 85.96%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.98%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 85.65%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 85.27%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 85.39%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 85.26%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.08%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 84.83%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 84.34%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 83.85%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 83.38%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 82.78%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 82.47%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 82.83%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 84.28%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 83.82%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 82.86%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 82.27%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.81%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.19%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 81.36%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 81.35%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 81.20%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 80.95%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.85%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 80.75%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 80.56%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 80.32%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 80.05%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 79.96%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 80.32%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 80.37%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 80.13%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.78%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 79.39%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 79.01%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.76%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 78.47%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 78.89%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 78.37%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.90%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 77.60%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 77.14%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 76.72%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 76.74%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 76.91%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 77.42%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 77.92%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 77.80%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 77.64%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.52%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 77.40%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 77.39%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.36%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.35%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 77.37%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 77.42%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.47%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 77.49%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 77.47%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 77.26%   
cur_acc:  ['0.9474', '0.7490', '0.7014']
his_acc:  ['0.9474', '0.8470', '0.7726']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15  2  1 12  1 10  0  4  1  1  1 17 16  2  1
  1  5  1 18  1  8  1 14  1  1  7  1  1  1  3  6]
Losses:  6.920994758605957 1.2752633094787598
CurrentTrain: epoch  0, batch     0 | loss: 8.1962585Losses:  8.653907775878906 1.0401513576507568
CurrentTrain: epoch  0, batch     1 | loss: 9.6940594Losses:  10.113260269165039 1.217017412185669
CurrentTrain: epoch  0, batch     2 | loss: 11.3302774Losses:  8.022454261779785 1.1222176551818848
CurrentTrain: epoch  0, batch     3 | loss: 9.1446724Losses:  7.698326110839844 1.038238525390625
CurrentTrain: epoch  0, batch     4 | loss: 8.7365646Losses:  6.173796653747559 1.2263996601104736
CurrentTrain: epoch  0, batch     5 | loss: 7.4001961Losses:  9.177969932556152 0.24059896171092987
CurrentTrain: epoch  0, batch     6 | loss: 9.4185686Losses:  4.136900901794434 1.2935864925384521
CurrentTrain: epoch  1, batch     0 | loss: 5.4304876Losses:  4.987983226776123 1.114729881286621
CurrentTrain: epoch  1, batch     1 | loss: 6.1027131Losses:  3.780822277069092 0.7437033653259277
CurrentTrain: epoch  1, batch     2 | loss: 4.5245256Losses:  3.747260808944702 1.174428939819336
CurrentTrain: epoch  1, batch     3 | loss: 4.9216900Losses:  3.059846878051758 0.9513418078422546
CurrentTrain: epoch  1, batch     4 | loss: 4.0111885Losses:  3.3384406566619873 0.7561948299407959
CurrentTrain: epoch  1, batch     5 | loss: 4.0946355Losses:  4.843068599700928 0.4405127763748169
CurrentTrain: epoch  1, batch     6 | loss: 5.2835813Losses:  3.032142162322998 0.9210021495819092
CurrentTrain: epoch  2, batch     0 | loss: 3.9531443Losses:  3.5609474182128906 0.8563718795776367
CurrentTrain: epoch  2, batch     1 | loss: 4.4173193Losses:  4.463620185852051 0.9353498816490173
CurrentTrain: epoch  2, batch     2 | loss: 5.3989701Losses:  3.204418182373047 0.7248456478118896
CurrentTrain: epoch  2, batch     3 | loss: 3.9292638Losses:  2.9840853214263916 1.0345828533172607
CurrentTrain: epoch  2, batch     4 | loss: 4.0186682Losses:  4.155605316162109 1.2075774669647217
CurrentTrain: epoch  2, batch     5 | loss: 5.3631830Losses:  2.679018020629883 0.2737703323364258
CurrentTrain: epoch  2, batch     6 | loss: 2.9527884Losses:  3.8870198726654053 1.2671215534210205
CurrentTrain: epoch  3, batch     0 | loss: 5.1541414Losses:  2.939159870147705 0.9722387194633484
CurrentTrain: epoch  3, batch     1 | loss: 3.9113986Losses:  3.1710293292999268 1.0315101146697998
CurrentTrain: epoch  3, batch     2 | loss: 4.2025394Losses:  2.9596757888793945 0.7106720209121704
CurrentTrain: epoch  3, batch     3 | loss: 3.6703477Losses:  3.508441209793091 0.8731950521469116
CurrentTrain: epoch  3, batch     4 | loss: 4.3816361Losses:  2.720766067504883 0.6616688370704651
CurrentTrain: epoch  3, batch     5 | loss: 3.3824348Losses:  2.417097568511963 0.23476597666740417
CurrentTrain: epoch  3, batch     6 | loss: 2.6518636Losses:  2.404967784881592 0.6258199214935303
CurrentTrain: epoch  4, batch     0 | loss: 3.0307877Losses:  3.251840591430664 1.0120090246200562
CurrentTrain: epoch  4, batch     1 | loss: 4.2638497Losses:  3.3284854888916016 1.0377386808395386
CurrentTrain: epoch  4, batch     2 | loss: 4.3662243Losses:  2.6457653045654297 0.47146040201187134
CurrentTrain: epoch  4, batch     3 | loss: 3.1172256Losses:  2.408247470855713 0.5546565651893616
CurrentTrain: epoch  4, batch     4 | loss: 2.9629040Losses:  3.497607946395874 0.9899539351463318
CurrentTrain: epoch  4, batch     5 | loss: 4.4875617Losses:  1.9789561033248901 0.22729769349098206
CurrentTrain: epoch  4, batch     6 | loss: 2.2062538Losses:  2.9666292667388916 0.7789046764373779
CurrentTrain: epoch  5, batch     0 | loss: 3.7455339Losses:  3.213893175125122 0.7313439249992371
CurrentTrain: epoch  5, batch     1 | loss: 3.9452372Losses:  2.5325675010681152 0.762102484703064
CurrentTrain: epoch  5, batch     2 | loss: 3.2946701Losses:  2.3645405769348145 0.468972384929657
CurrentTrain: epoch  5, batch     3 | loss: 2.8335130Losses:  2.802401542663574 0.4823628067970276
CurrentTrain: epoch  5, batch     4 | loss: 3.2847643Losses:  2.094048023223877 0.555744469165802
CurrentTrain: epoch  5, batch     5 | loss: 2.6497924Losses:  2.381176233291626 0.15923887491226196
CurrentTrain: epoch  5, batch     6 | loss: 2.5404150Losses:  2.194027900695801 0.6281002759933472
CurrentTrain: epoch  6, batch     0 | loss: 2.8221283Losses:  2.366616725921631 0.6723270416259766
CurrentTrain: epoch  6, batch     1 | loss: 3.0389438Losses:  2.3760340213775635 0.5737799406051636
CurrentTrain: epoch  6, batch     2 | loss: 2.9498138Losses:  2.265704870223999 0.7123892307281494
CurrentTrain: epoch  6, batch     3 | loss: 2.9780941Losses:  2.9618759155273438 0.890004575252533
CurrentTrain: epoch  6, batch     4 | loss: 3.8518806Losses:  2.414417028427124 0.6777845025062561
CurrentTrain: epoch  6, batch     5 | loss: 3.0922015Losses:  1.776533603668213 0.026357393711805344
CurrentTrain: epoch  6, batch     6 | loss: 1.8028910Losses:  2.085489273071289 0.47204136848449707
CurrentTrain: epoch  7, batch     0 | loss: 2.5575306Losses:  1.9727661609649658 0.43743565678596497
CurrentTrain: epoch  7, batch     1 | loss: 2.4102018Losses:  1.823830246925354 0.3205801248550415
CurrentTrain: epoch  7, batch     2 | loss: 2.1444104Losses:  2.3378028869628906 0.30636686086654663
CurrentTrain: epoch  7, batch     3 | loss: 2.6441698Losses:  2.820784091949463 0.8700826168060303
CurrentTrain: epoch  7, batch     4 | loss: 3.6908667Losses:  2.3767471313476562 0.746160626411438
CurrentTrain: epoch  7, batch     5 | loss: 3.1229076Losses:  2.705554962158203 0.24765175580978394
CurrentTrain: epoch  7, batch     6 | loss: 2.9532068Losses:  2.0365257263183594 0.47661298513412476
CurrentTrain: epoch  8, batch     0 | loss: 2.5131388Losses:  2.635254383087158 0.5950462818145752
CurrentTrain: epoch  8, batch     1 | loss: 3.2303007Losses:  2.020357608795166 0.7155578136444092
CurrentTrain: epoch  8, batch     2 | loss: 2.7359154Losses:  2.0614867210388184 0.5440502166748047
CurrentTrain: epoch  8, batch     3 | loss: 2.6055369Losses:  2.0804359912872314 0.41267141699790955
CurrentTrain: epoch  8, batch     4 | loss: 2.4931073Losses:  2.0055899620056152 0.5071319341659546
CurrentTrain: epoch  8, batch     5 | loss: 2.5127220Losses:  1.8034567832946777 0.15789787471294403
CurrentTrain: epoch  8, batch     6 | loss: 1.9613546Losses:  2.1048216819763184 0.4897003769874573
CurrentTrain: epoch  9, batch     0 | loss: 2.5945220Losses:  2.1634573936462402 0.3830093741416931
CurrentTrain: epoch  9, batch     1 | loss: 2.5464668Losses:  2.0406925678253174 0.5934687852859497
CurrentTrain: epoch  9, batch     2 | loss: 2.6341615Losses:  1.9446437358856201 0.5167367458343506
CurrentTrain: epoch  9, batch     3 | loss: 2.4613805Losses:  2.0064847469329834 0.4277914762496948
CurrentTrain: epoch  9, batch     4 | loss: 2.4342761Losses:  2.102013111114502 0.6108336448669434
CurrentTrain: epoch  9, batch     5 | loss: 2.7128468Losses:  1.785836935043335 0.02912306785583496
CurrentTrain: epoch  9, batch     6 | loss: 1.8149600
Losses:  5.850627899169922 0.4700624346733093
MemoryTrain:  epoch  0, batch     0 | loss: 6.3206902Losses:  8.64392375946045 0.3998199701309204
MemoryTrain:  epoch  0, batch     1 | loss: 9.0437441Losses:  9.496295928955078 0.4843125343322754
MemoryTrain:  epoch  0, batch     2 | loss: 9.9806080Losses:  10.589919090270996 0.2708345055580139
MemoryTrain:  epoch  0, batch     3 | loss: 10.8607540Losses:  10.579657554626465 0.4408435523509979
MemoryTrain:  epoch  0, batch     4 | loss: 11.0205011Losses:  1.6380977630615234 0.5103123188018799
MemoryTrain:  epoch  1, batch     0 | loss: 2.1484101Losses:  1.1963640451431274 0.4472774863243103
MemoryTrain:  epoch  1, batch     1 | loss: 1.6436415Losses:  0.7656803131103516 0.43888401985168457
MemoryTrain:  epoch  1, batch     2 | loss: 1.2045643Losses:  0.6260968446731567 0.5032790899276733
MemoryTrain:  epoch  1, batch     3 | loss: 1.1293759Losses:  1.3559271097183228 0.5238333940505981
MemoryTrain:  epoch  1, batch     4 | loss: 1.8797605Losses:  0.5932087898254395 0.34738218784332275
MemoryTrain:  epoch  2, batch     0 | loss: 0.9405910Losses:  1.3418686389923096 0.4343133866786957
MemoryTrain:  epoch  2, batch     1 | loss: 1.7761821Losses:  1.204471230506897 0.5277948379516602
MemoryTrain:  epoch  2, batch     2 | loss: 1.7322661Losses:  0.8240172266960144 0.5668491125106812
MemoryTrain:  epoch  2, batch     3 | loss: 1.3908663Losses:  0.5270459055900574 0.4420766830444336
MemoryTrain:  epoch  2, batch     4 | loss: 0.9691226Losses:  0.5840474367141724 0.5786408185958862
MemoryTrain:  epoch  3, batch     0 | loss: 1.1626883Losses:  0.9916741251945496 0.4165712893009186
MemoryTrain:  epoch  3, batch     1 | loss: 1.4082454Losses:  0.7174360156059265 0.6726495623588562
MemoryTrain:  epoch  3, batch     2 | loss: 1.3900856Losses:  0.4095984995365143 0.32432830333709717
MemoryTrain:  epoch  3, batch     3 | loss: 0.7339268Losses:  0.2607473134994507 0.3238884508609772
MemoryTrain:  epoch  3, batch     4 | loss: 0.5846357Losses:  0.2759925127029419 0.4030632972717285
MemoryTrain:  epoch  4, batch     0 | loss: 0.6790558Losses:  0.6529324054718018 0.5369483232498169
MemoryTrain:  epoch  4, batch     1 | loss: 1.1898807Losses:  0.5419732332229614 0.40845417976379395
MemoryTrain:  epoch  4, batch     2 | loss: 0.9504274Losses:  0.3701465427875519 0.4306783080101013
MemoryTrain:  epoch  4, batch     3 | loss: 0.8008249Losses:  0.3856128454208374 0.5768641829490662
MemoryTrain:  epoch  4, batch     4 | loss: 0.9624770Losses:  0.4054405093193054 0.6155941486358643
MemoryTrain:  epoch  5, batch     0 | loss: 1.0210347Losses:  0.23824641108512878 0.3439595401287079
MemoryTrain:  epoch  5, batch     1 | loss: 0.5822060Losses:  0.2329431176185608 0.20140434801578522
MemoryTrain:  epoch  5, batch     2 | loss: 0.4343475Losses:  0.27269530296325684 0.3482685685157776
MemoryTrain:  epoch  5, batch     3 | loss: 0.6209639Losses:  0.6989856958389282 0.7290712594985962
MemoryTrain:  epoch  5, batch     4 | loss: 1.4280570Losses:  0.36881309747695923 0.5290461778640747
MemoryTrain:  epoch  6, batch     0 | loss: 0.8978593Losses:  0.26678264141082764 0.3519044518470764
MemoryTrain:  epoch  6, batch     1 | loss: 0.6186871Losses:  0.3909739851951599 0.4463305175304413
MemoryTrain:  epoch  6, batch     2 | loss: 0.8373045Losses:  0.2473503202199936 0.4300285875797272
MemoryTrain:  epoch  6, batch     3 | loss: 0.6773789Losses:  0.3063555657863617 0.34771037101745605
MemoryTrain:  epoch  6, batch     4 | loss: 0.6540660Losses:  0.41927701234817505 0.43856844305992126
MemoryTrain:  epoch  7, batch     0 | loss: 0.8578454Losses:  0.2512904107570648 0.4302205443382263
MemoryTrain:  epoch  7, batch     1 | loss: 0.6815109Losses:  0.28485211730003357 0.33632320165634155
MemoryTrain:  epoch  7, batch     2 | loss: 0.6211753Losses:  0.3785454332828522 0.38883176445961
MemoryTrain:  epoch  7, batch     3 | loss: 0.7673772Losses:  0.2690892517566681 0.3098990321159363
MemoryTrain:  epoch  7, batch     4 | loss: 0.5789883Losses:  0.22233235836029053 0.3698867857456207
MemoryTrain:  epoch  8, batch     0 | loss: 0.5922191Losses:  0.43126970529556274 0.5449401140213013
MemoryTrain:  epoch  8, batch     1 | loss: 0.9762098Losses:  0.26281607151031494 0.6523982286453247
MemoryTrain:  epoch  8, batch     2 | loss: 0.9152143Losses:  0.18459142744541168 0.19475558400154114
MemoryTrain:  epoch  8, batch     3 | loss: 0.3793470Losses:  0.28312936425209045 0.3576415479183197
MemoryTrain:  epoch  8, batch     4 | loss: 0.6407709Losses:  0.25280672311782837 0.30046960711479187
MemoryTrain:  epoch  9, batch     0 | loss: 0.5532763Losses:  0.23691609501838684 0.2607494592666626
MemoryTrain:  epoch  9, batch     1 | loss: 0.4976656Losses:  0.256483793258667 0.3753107786178589
MemoryTrain:  epoch  9, batch     2 | loss: 0.6317946Losses:  0.32779374718666077 0.4402867257595062
MemoryTrain:  epoch  9, batch     3 | loss: 0.7680805Losses:  0.278143048286438 0.5272598266601562
MemoryTrain:  epoch  9, batch     4 | loss: 0.8054029
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 70.67%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 69.89%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.15%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 92.43%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 90.57%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 90.10%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 88.10%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 86.91%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 85.87%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 83.00%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 82.41%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 82.31%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 81.85%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 81.93%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 81.98%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.89%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 81.72%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 81.48%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 80.95%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 80.28%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 79.85%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 79.29%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 78.95%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 81.31%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 81.13%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 80.61%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 80.28%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 79.77%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.45%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 79.13%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 78.25%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 77.77%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 77.18%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 76.71%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 76.25%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 76.13%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 75.98%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 76.01%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 76.35%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 76.30%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.03%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 75.62%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 75.26%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.04%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.78%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 75.33%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 74.88%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 74.43%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 74.15%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 73.71%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.28%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 73.38%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 73.47%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 74.52%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 74.16%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.99%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 73.61%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 73.54%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 73.63%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 74.00%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 73.97%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 73.68%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 73.42%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 73.17%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 72.95%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.77%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 72.52%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 72.98%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 72.98%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 72.68%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 72.44%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 72.21%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 71.98%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 71.78%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 72.93%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 73.40%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 73.53%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 73.35%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 73.10%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 73.02%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 73.55%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 73.48%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 73.29%   [EVAL] batch:  241 | acc: 43.75%,  total acc: 73.17%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 73.05%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 72.91%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 72.80%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 72.95%   
cur_acc:  ['0.9474', '0.7490', '0.7014', '0.6915']
his_acc:  ['0.9474', '0.8470', '0.7726', '0.7295']
Clustering into  24  clusters
Clusters:  [ 0 23 13  0  0  0 21  0 12 15 20  0 14  0 22 17 11  0  0  0 19 18  1  0
  0  6  0  9  0 10  0 16  0  5  8  0  0  0  2  7  3  0  0  1  1  4  0  0
  0  0]
Losses:  7.24510383605957 1.2982884645462036
CurrentTrain: epoch  0, batch     0 | loss: 8.5433922Losses:  9.999914169311523 1.6901291608810425
CurrentTrain: epoch  0, batch     1 | loss: 11.6900434Losses:  8.460342407226562 1.348657488822937
CurrentTrain: epoch  0, batch     2 | loss: 9.8090000Losses:  9.184906959533691 1.400115966796875
CurrentTrain: epoch  0, batch     3 | loss: 10.5850229Losses:  7.935194969177246 1.3427176475524902
CurrentTrain: epoch  0, batch     4 | loss: 9.2779121Losses:  6.338126182556152 1.6508888006210327
CurrentTrain: epoch  0, batch     5 | loss: 7.9890151Losses:  6.151787281036377 0.5148379802703857
CurrentTrain: epoch  0, batch     6 | loss: 6.6666250Losses:  4.210165023803711 1.2248914241790771
CurrentTrain: epoch  1, batch     0 | loss: 5.4350567Losses:  4.152286052703857 1.4624704122543335
CurrentTrain: epoch  1, batch     1 | loss: 5.6147566Losses:  4.435324668884277 1.1552646160125732
CurrentTrain: epoch  1, batch     2 | loss: 5.5905895Losses:  3.9605541229248047 1.252880573272705
CurrentTrain: epoch  1, batch     3 | loss: 5.2134347Losses:  3.547996997833252 1.1584393978118896
CurrentTrain: epoch  1, batch     4 | loss: 4.7064362Losses:  3.9766080379486084 1.2957122325897217
CurrentTrain: epoch  1, batch     5 | loss: 5.2723203Losses:  3.873119831085205 0.465467631816864
CurrentTrain: epoch  1, batch     6 | loss: 4.3385873Losses:  3.066901683807373 0.7890990972518921
CurrentTrain: epoch  2, batch     0 | loss: 3.8560009Losses:  3.453904628753662 0.969826877117157
CurrentTrain: epoch  2, batch     1 | loss: 4.4237313Losses:  3.5140156745910645 1.4449909925460815
CurrentTrain: epoch  2, batch     2 | loss: 4.9590068Losses:  3.5256075859069824 1.1238963603973389
CurrentTrain: epoch  2, batch     3 | loss: 4.6495037Losses:  3.2603542804718018 1.252580165863037
CurrentTrain: epoch  2, batch     4 | loss: 4.5129347Losses:  4.016634941101074 1.2150559425354004
CurrentTrain: epoch  2, batch     5 | loss: 5.2316909Losses:  4.900939464569092 0.4833827018737793
CurrentTrain: epoch  2, batch     6 | loss: 5.3843222Losses:  3.9438886642456055 1.32558274269104
CurrentTrain: epoch  3, batch     0 | loss: 5.2694712Losses:  2.700796127319336 0.9867249727249146
CurrentTrain: epoch  3, batch     1 | loss: 3.6875210Losses:  3.826899766921997 0.8453383445739746
CurrentTrain: epoch  3, batch     2 | loss: 4.6722383Losses:  3.236225128173828 0.8555667400360107
CurrentTrain: epoch  3, batch     3 | loss: 4.0917921Losses:  3.0681564807891846 0.955894947052002
CurrentTrain: epoch  3, batch     4 | loss: 4.0240517Losses:  2.507951498031616 0.8721271753311157
CurrentTrain: epoch  3, batch     5 | loss: 3.3800788Losses:  2.6937484741210938 0.1858251690864563
CurrentTrain: epoch  3, batch     6 | loss: 2.8795736Losses:  3.1157259941101074 0.8874319195747375
CurrentTrain: epoch  4, batch     0 | loss: 4.0031581Losses:  2.9136343002319336 0.9885263442993164
CurrentTrain: epoch  4, batch     1 | loss: 3.9021606Losses:  2.946885108947754 0.7864670753479004
CurrentTrain: epoch  4, batch     2 | loss: 3.7333522Losses:  2.6344447135925293 1.0302882194519043
CurrentTrain: epoch  4, batch     3 | loss: 3.6647329Losses:  2.9498963356018066 1.0422742366790771
CurrentTrain: epoch  4, batch     4 | loss: 3.9921706Losses:  2.528010845184326 1.095430612564087
CurrentTrain: epoch  4, batch     5 | loss: 3.6234415Losses:  2.2940433025360107 2.9802322387695312e-08
CurrentTrain: epoch  4, batch     6 | loss: 2.2940433Losses:  2.936007022857666 1.1734944581985474
CurrentTrain: epoch  5, batch     0 | loss: 4.1095014Losses:  2.344287395477295 1.0034325122833252
CurrentTrain: epoch  5, batch     1 | loss: 3.3477199Losses:  2.1255316734313965 0.520335853099823
CurrentTrain: epoch  5, batch     2 | loss: 2.6458676Losses:  2.2729971408843994 0.8433272838592529
CurrentTrain: epoch  5, batch     3 | loss: 3.1163244Losses:  3.0875134468078613 0.5104623436927795
CurrentTrain: epoch  5, batch     4 | loss: 3.5979757Losses:  2.7686073780059814 0.6906993389129639
CurrentTrain: epoch  5, batch     5 | loss: 3.4593067Losses:  3.9523305892944336 0.4082528054714203
CurrentTrain: epoch  5, batch     6 | loss: 4.3605833Losses:  2.7103114128112793 0.7214168906211853
CurrentTrain: epoch  6, batch     0 | loss: 3.4317284Losses:  2.419269561767578 0.7565448880195618
CurrentTrain: epoch  6, batch     1 | loss: 3.1758144Losses:  2.768091917037964 0.8195497989654541
CurrentTrain: epoch  6, batch     2 | loss: 3.5876417Losses:  2.3785367012023926 0.8471640348434448
CurrentTrain: epoch  6, batch     3 | loss: 3.2257009Losses:  2.3548269271850586 0.5765713453292847
CurrentTrain: epoch  6, batch     4 | loss: 2.9313984Losses:  2.137220621109009 1.016137719154358
CurrentTrain: epoch  6, batch     5 | loss: 3.1533585Losses:  2.3979945182800293 0.16039246320724487
CurrentTrain: epoch  6, batch     6 | loss: 2.5583870Losses:  2.6673812866210938 0.8280749320983887
CurrentTrain: epoch  7, batch     0 | loss: 3.4954562Losses:  2.4260103702545166 0.8041423559188843
CurrentTrain: epoch  7, batch     1 | loss: 3.2301526Losses:  2.608987331390381 0.6935354471206665
CurrentTrain: epoch  7, batch     2 | loss: 3.3025227Losses:  2.2636666297912598 0.8301345109939575
CurrentTrain: epoch  7, batch     3 | loss: 3.0938010Losses:  1.8812358379364014 0.5109172463417053
CurrentTrain: epoch  7, batch     4 | loss: 2.3921530Losses:  2.2367846965789795 0.7098085880279541
CurrentTrain: epoch  7, batch     5 | loss: 2.9465933Losses:  1.7725794315338135 0.11471398174762726
CurrentTrain: epoch  7, batch     6 | loss: 1.8872935Losses:  1.8898372650146484 0.4769788682460785
CurrentTrain: epoch  8, batch     0 | loss: 2.3668160Losses:  2.9484362602233887 0.7031702995300293
CurrentTrain: epoch  8, batch     1 | loss: 3.6516066Losses:  2.071828842163086 0.7906061410903931
CurrentTrain: epoch  8, batch     2 | loss: 2.8624349Losses:  2.2713847160339355 0.7548757195472717
CurrentTrain: epoch  8, batch     3 | loss: 3.0262604Losses:  1.948331356048584 0.780701756477356
CurrentTrain: epoch  8, batch     4 | loss: 2.7290330Losses:  2.1986265182495117 0.41782146692276
CurrentTrain: epoch  8, batch     5 | loss: 2.6164479Losses:  1.900067687034607 0.2709345817565918
CurrentTrain: epoch  8, batch     6 | loss: 2.1710024Losses:  2.758258104324341 0.544143557548523
CurrentTrain: epoch  9, batch     0 | loss: 3.3024015Losses:  1.9261767864227295 0.5478451251983643
CurrentTrain: epoch  9, batch     1 | loss: 2.4740219Losses:  1.9318876266479492 0.5564665198326111
CurrentTrain: epoch  9, batch     2 | loss: 2.4883542Losses:  2.179384708404541 0.6711148023605347
CurrentTrain: epoch  9, batch     3 | loss: 2.8504996Losses:  1.78619384765625 0.405079185962677
CurrentTrain: epoch  9, batch     4 | loss: 2.1912730Losses:  2.1547040939331055 0.7407598495483398
CurrentTrain: epoch  9, batch     5 | loss: 2.8954639Losses:  1.985308051109314 0.19066622853279114
CurrentTrain: epoch  9, batch     6 | loss: 2.1759744
Losses:  6.073944091796875 0.37395304441452026
MemoryTrain:  epoch  0, batch     0 | loss: 6.4478970Losses:  8.431319236755371 0.47310906648635864
MemoryTrain:  epoch  0, batch     1 | loss: 8.9044285Losses:  8.968603134155273 0.48643457889556885
MemoryTrain:  epoch  0, batch     2 | loss: 9.4550381Losses:  10.142845153808594 0.5010053515434265
MemoryTrain:  epoch  0, batch     3 | loss: 10.6438503Losses:  10.566446304321289 0.4992031753063202
MemoryTrain:  epoch  0, batch     4 | loss: 11.0656490Losses:  10.550336837768555 0.5725145936012268
MemoryTrain:  epoch  0, batch     5 | loss: 11.1228514Losses:  10.905670166015625 0.03272782638669014
MemoryTrain:  epoch  0, batch     6 | loss: 10.9383984Losses:  1.4359694719314575 0.49010026454925537
MemoryTrain:  epoch  1, batch     0 | loss: 1.9260697Losses:  1.3950027227401733 0.6558132171630859
MemoryTrain:  epoch  1, batch     1 | loss: 2.0508161Losses:  0.5669423341751099 0.3508322834968567
MemoryTrain:  epoch  1, batch     2 | loss: 0.9177746Losses:  0.486025333404541 0.6036379337310791
MemoryTrain:  epoch  1, batch     3 | loss: 1.0896633Losses:  0.5360840559005737 0.3858572840690613
MemoryTrain:  epoch  1, batch     4 | loss: 0.9219413Losses:  0.9667239785194397 0.5810643434524536
MemoryTrain:  epoch  1, batch     5 | loss: 1.5477884Losses:  0.530784547328949 0.041364941745996475
MemoryTrain:  epoch  1, batch     6 | loss: 0.5721495Losses:  0.3815746307373047 0.4230991005897522
MemoryTrain:  epoch  2, batch     0 | loss: 0.8046737Losses:  0.48497411608695984 0.5019309520721436
MemoryTrain:  epoch  2, batch     1 | loss: 0.9869051Losses:  0.5993755459785461 0.4939282536506653
MemoryTrain:  epoch  2, batch     2 | loss: 1.0933038Losses:  0.387944757938385 0.5033449530601501
MemoryTrain:  epoch  2, batch     3 | loss: 0.8912897Losses:  0.6252236366271973 0.3466235101222992
MemoryTrain:  epoch  2, batch     4 | loss: 0.9718472Losses:  1.6379263401031494 0.5894361734390259
MemoryTrain:  epoch  2, batch     5 | loss: 2.2273626Losses:  0.42398321628570557 0.08853286504745483
MemoryTrain:  epoch  2, batch     6 | loss: 0.5125161Losses:  0.2419099509716034 0.4098655581474304
MemoryTrain:  epoch  3, batch     0 | loss: 0.6517755Losses:  0.43214863538742065 0.4299679696559906
MemoryTrain:  epoch  3, batch     1 | loss: 0.8621166Losses:  0.3731408715248108 0.4110581874847412
MemoryTrain:  epoch  3, batch     2 | loss: 0.7841991Losses:  0.8396638631820679 0.5049917101860046
MemoryTrain:  epoch  3, batch     3 | loss: 1.3446555Losses:  0.7178184986114502 0.5761886835098267
MemoryTrain:  epoch  3, batch     4 | loss: 1.2940072Losses:  0.36785024404525757 0.3914978504180908
MemoryTrain:  epoch  3, batch     5 | loss: 0.7593481Losses:  2.1091301441192627 0.03888564929366112
MemoryTrain:  epoch  3, batch     6 | loss: 2.1480157Losses:  0.34316474199295044 0.5979595184326172
MemoryTrain:  epoch  4, batch     0 | loss: 0.9411243Losses:  0.4056929349899292 0.41752374172210693
MemoryTrain:  epoch  4, batch     1 | loss: 0.8232167Losses:  0.41711533069610596 0.6023352146148682
MemoryTrain:  epoch  4, batch     2 | loss: 1.0194505Losses:  0.5392588973045349 0.4032224416732788
MemoryTrain:  epoch  4, batch     3 | loss: 0.9424813Losses:  0.6267343759536743 0.4992135465145111
MemoryTrain:  epoch  4, batch     4 | loss: 1.1259480Losses:  0.2442978024482727 0.24948987364768982
MemoryTrain:  epoch  4, batch     5 | loss: 0.4937877Losses:  0.8565900921821594 0.2986486554145813
MemoryTrain:  epoch  4, batch     6 | loss: 1.1552387Losses:  0.40168797969818115 0.5856993794441223
MemoryTrain:  epoch  5, batch     0 | loss: 0.9873874Losses:  0.34954720735549927 0.4394051730632782
MemoryTrain:  epoch  5, batch     1 | loss: 0.7889524Losses:  0.32162147760391235 0.30789536237716675
MemoryTrain:  epoch  5, batch     2 | loss: 0.6295168Losses:  0.6062107682228088 0.37651681900024414
MemoryTrain:  epoch  5, batch     3 | loss: 0.9827276Losses:  0.346683531999588 0.39511314034461975
MemoryTrain:  epoch  5, batch     4 | loss: 0.7417967Losses:  0.5840756297111511 0.3545230031013489
MemoryTrain:  epoch  5, batch     5 | loss: 0.9385986Losses:  0.3696432113647461 0.09879570454359055
MemoryTrain:  epoch  5, batch     6 | loss: 0.4684389Losses:  0.4145832657814026 0.4666332006454468
MemoryTrain:  epoch  6, batch     0 | loss: 0.8812165Losses:  0.3047384023666382 0.4603627920150757
MemoryTrain:  epoch  6, batch     1 | loss: 0.7651012Losses:  0.5094234943389893 0.6705986857414246
MemoryTrain:  epoch  6, batch     2 | loss: 1.1800222Losses:  0.29414814710617065 0.29784733057022095
MemoryTrain:  epoch  6, batch     3 | loss: 0.5919955Losses:  0.3669222593307495 0.2737216651439667
MemoryTrain:  epoch  6, batch     4 | loss: 0.6406440Losses:  0.2811315059661865 0.5394155383110046
MemoryTrain:  epoch  6, batch     5 | loss: 0.8205470Losses:  0.2693650722503662 0.03436507657170296
MemoryTrain:  epoch  6, batch     6 | loss: 0.3037302Losses:  0.2474571317434311 0.34669309854507446
MemoryTrain:  epoch  7, batch     0 | loss: 0.5941502Losses:  0.43577295541763306 0.6259498596191406
MemoryTrain:  epoch  7, batch     1 | loss: 1.0617228Losses:  0.24754737317562103 0.29364654421806335
MemoryTrain:  epoch  7, batch     2 | loss: 0.5411939Losses:  0.3158002495765686 0.3810698390007019
MemoryTrain:  epoch  7, batch     3 | loss: 0.6968701Losses:  0.3902718424797058 0.6976674199104309
MemoryTrain:  epoch  7, batch     4 | loss: 1.0879393Losses:  0.2777857482433319 0.3494518995285034
MemoryTrain:  epoch  7, batch     5 | loss: 0.6272377Losses:  0.43658822774887085 0.07203209400177002
MemoryTrain:  epoch  7, batch     6 | loss: 0.5086203Losses:  0.26643192768096924 0.4128422141075134
MemoryTrain:  epoch  8, batch     0 | loss: 0.6792741Losses:  0.39851146936416626 0.5008169412612915
MemoryTrain:  epoch  8, batch     1 | loss: 0.8993284Losses:  0.3090022802352905 0.4884389638900757
MemoryTrain:  epoch  8, batch     2 | loss: 0.7974412Losses:  0.33139318227767944 0.39693015813827515
MemoryTrain:  epoch  8, batch     3 | loss: 0.7283233Losses:  0.29870492219924927 0.4727920889854431
MemoryTrain:  epoch  8, batch     4 | loss: 0.7714970Losses:  0.3019716143608093 0.30266043543815613
MemoryTrain:  epoch  8, batch     5 | loss: 0.6046320Losses:  0.22979111969470978 0.04201939329504967
MemoryTrain:  epoch  8, batch     6 | loss: 0.2718105Losses:  0.4211530387401581 0.6665313243865967
MemoryTrain:  epoch  9, batch     0 | loss: 1.0876844Losses:  0.2424599826335907 0.3534718453884125
MemoryTrain:  epoch  9, batch     1 | loss: 0.5959318Losses:  0.2837745249271393 0.395813524723053
MemoryTrain:  epoch  9, batch     2 | loss: 0.6795881Losses:  0.28506776690483093 0.2985915541648865
MemoryTrain:  epoch  9, batch     3 | loss: 0.5836593Losses:  0.2904626131057739 0.4954586923122406
MemoryTrain:  epoch  9, batch     4 | loss: 0.7859213Losses:  0.2793341279029846 0.2757616639137268
MemoryTrain:  epoch  9, batch     5 | loss: 0.5550958Losses:  0.2540636658668518 0.064150869846344
MemoryTrain:  epoch  9, batch     6 | loss: 0.3182145
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 49.06%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 48.21%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 48.86%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 49.18%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 48.96%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 49.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 47.84%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.06%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 44.42%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 42.89%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 41.67%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 40.52%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.21%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 42.61%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 45.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 46.35%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 47.47%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 48.68%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 49.68%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 51.98%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 53.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 54.83%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 54.86%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 54.48%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 54.26%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 54.43%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 54.21%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 54.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 54.41%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 55.05%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 55.19%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 55.90%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 56.14%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 56.58%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 56.58%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 56.47%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 56.14%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 56.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 56.45%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 56.65%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 56.15%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 89.87%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 89.30%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.52%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 87.80%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 86.91%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 85.63%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 84.51%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.46%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 84.33%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.94%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 83.90%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.78%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.69%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 83.28%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 82.53%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 81.85%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 81.40%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 80.81%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.39%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 80.33%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.55%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.84%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.38%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 82.55%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 82.54%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 82.34%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 82.43%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.01%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.73%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.23%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.84%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.41%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.73%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 79.17%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 78.56%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 78.07%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 77.49%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 77.02%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 76.60%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 76.59%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 76.38%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 76.22%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 76.16%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 76.29%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 76.59%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 76.48%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 76.12%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 75.71%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 75.31%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 75.04%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.78%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 75.29%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 74.84%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 74.39%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 74.15%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 73.67%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.24%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.25%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 73.39%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 73.85%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 73.91%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 73.77%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 73.71%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 73.32%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 73.34%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.43%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 73.70%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 73.45%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 73.26%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 73.00%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 72.79%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.60%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 72.39%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 72.89%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 72.56%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 72.32%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 72.12%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 71.89%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 71.69%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 71.77%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 72.65%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 72.78%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 73.37%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 73.13%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 73.11%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 72.91%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 72.86%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 72.93%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.22%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 73.25%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 73.15%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 73.03%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 72.84%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 72.67%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 72.63%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 72.41%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 72.17%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 71.94%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 71.52%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 71.29%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 71.36%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 71.47%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 71.39%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  266 | acc: 25.00%,  total acc: 71.21%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 71.08%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 70.91%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 70.86%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 70.27%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 70.01%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 69.76%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 69.51%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 69.06%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  293 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 69.78%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 69.70%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 69.67%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 69.59%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 69.52%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 69.63%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 69.46%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 69.43%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 69.43%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 69.29%   
cur_acc:  ['0.9474', '0.7490', '0.7014', '0.6915', '0.5615']
his_acc:  ['0.9474', '0.8470', '0.7726', '0.7295', '0.6929']
Clustering into  29  clusters
Clusters:  [ 0 19 15  0  0  0 24  0 27 28 20  0 17  0 14  9 18  0  0  0 23 21  2  0
  0 16  0 22  0 25  0 11  0 13 26  0  0  0  6  8 10  0  0  2  2  7  0  0
  0  0  0  0  0  4  0  3  0 12  5  1]
Losses:  6.679164886474609 0.7107323408126831
CurrentTrain: epoch  0, batch     0 | loss: 7.3898973Losses:  7.625826835632324 0.661852240562439
CurrentTrain: epoch  0, batch     1 | loss: 8.2876787Losses:  7.964158058166504 1.0456461906433105
CurrentTrain: epoch  0, batch     2 | loss: 9.0098038Losses:  7.490789890289307 0.9953665137290955
CurrentTrain: epoch  0, batch     3 | loss: 8.4861565Losses:  7.559230327606201 0.8373665809631348
CurrentTrain: epoch  0, batch     4 | loss: 8.3965969Losses:  7.355436325073242 0.9181371927261353
CurrentTrain: epoch  0, batch     5 | loss: 8.2735739Losses:  5.221912384033203 0.81574946641922
CurrentTrain: epoch  0, batch     6 | loss: 6.0376620Losses:  3.4464821815490723 0.6017941236495972
CurrentTrain: epoch  1, batch     0 | loss: 4.0482764Losses:  4.6579718589782715 0.8594212532043457
CurrentTrain: epoch  1, batch     1 | loss: 5.5173931Losses:  3.172201633453369 0.587870717048645
CurrentTrain: epoch  1, batch     2 | loss: 3.7600722Losses:  3.3390259742736816 0.6358239650726318
CurrentTrain: epoch  1, batch     3 | loss: 3.9748499Losses:  3.0760483741760254 0.6716917753219604
CurrentTrain: epoch  1, batch     4 | loss: 3.7477403Losses:  3.6414055824279785 0.7917201519012451
CurrentTrain: epoch  1, batch     5 | loss: 4.4331255Losses:  3.069377899169922 0.3659089207649231
CurrentTrain: epoch  1, batch     6 | loss: 3.4352868Losses:  2.429671287536621 0.6101621389389038
CurrentTrain: epoch  2, batch     0 | loss: 3.0398335Losses:  3.3380274772644043 0.36946994066238403
CurrentTrain: epoch  2, batch     1 | loss: 3.7074974Losses:  3.408801317214966 0.6882359385490417
CurrentTrain: epoch  2, batch     2 | loss: 4.0970373Losses:  3.4399237632751465 0.39134901762008667
CurrentTrain: epoch  2, batch     3 | loss: 3.8312728Losses:  2.3063368797302246 0.4307083487510681
CurrentTrain: epoch  2, batch     4 | loss: 2.7370453Losses:  2.582075834274292 0.5859697461128235
CurrentTrain: epoch  2, batch     5 | loss: 3.1680455Losses:  3.0988235473632812 0.226954847574234
CurrentTrain: epoch  2, batch     6 | loss: 3.3257785Losses:  2.595567226409912 0.42868685722351074
CurrentTrain: epoch  3, batch     0 | loss: 3.0242541Losses:  2.7245664596557617 0.5058196187019348
CurrentTrain: epoch  3, batch     1 | loss: 3.2303860Losses:  2.4325854778289795 0.369717001914978
CurrentTrain: epoch  3, batch     2 | loss: 2.8023024Losses:  2.5303313732147217 0.4966660737991333
CurrentTrain: epoch  3, batch     3 | loss: 3.0269976Losses:  2.467132568359375 0.5549705624580383
CurrentTrain: epoch  3, batch     4 | loss: 3.0221031Losses:  2.3448824882507324 0.4864421486854553
CurrentTrain: epoch  3, batch     5 | loss: 2.8313246Losses:  3.02423095703125 0.07003837823867798
CurrentTrain: epoch  3, batch     6 | loss: 3.0942693Losses:  2.4416182041168213 0.5748389363288879
CurrentTrain: epoch  4, batch     0 | loss: 3.0164571Losses:  2.552825450897217 0.4926130771636963
CurrentTrain: epoch  4, batch     1 | loss: 3.0454385Losses:  2.3810768127441406 0.3892015516757965
CurrentTrain: epoch  4, batch     2 | loss: 2.7702785Losses:  2.3761980533599854 0.6521683931350708
CurrentTrain: epoch  4, batch     3 | loss: 3.0283666Losses:  2.1417760848999023 0.5879048109054565
CurrentTrain: epoch  4, batch     4 | loss: 2.7296810Losses:  2.0393686294555664 0.23810474574565887
CurrentTrain: epoch  4, batch     5 | loss: 2.2774734Losses:  2.0260143280029297 5.960464477539063e-08
CurrentTrain: epoch  4, batch     6 | loss: 2.0260143Losses:  2.1448209285736084 0.4972459077835083
CurrentTrain: epoch  5, batch     0 | loss: 2.6420670Losses:  2.050900459289551 0.3089563250541687
CurrentTrain: epoch  5, batch     1 | loss: 2.3598568Losses:  2.0355405807495117 0.48908084630966187
CurrentTrain: epoch  5, batch     2 | loss: 2.5246215Losses:  2.347402334213257 0.5364199876785278
CurrentTrain: epoch  5, batch     3 | loss: 2.8838224Losses:  1.8842875957489014 0.2846698462963104
CurrentTrain: epoch  5, batch     4 | loss: 2.1689575Losses:  1.9697035551071167 0.43287140130996704
CurrentTrain: epoch  5, batch     5 | loss: 2.4025750Losses:  1.8673193454742432 0.0
CurrentTrain: epoch  5, batch     6 | loss: 1.8673193Losses:  2.0297117233276367 0.48904937505722046
CurrentTrain: epoch  6, batch     0 | loss: 2.5187612Losses:  1.7400221824645996 0.27058926224708557
CurrentTrain: epoch  6, batch     1 | loss: 2.0106115Losses:  2.0796923637390137 0.31023913621902466
CurrentTrain: epoch  6, batch     2 | loss: 2.3899314Losses:  1.861340045928955 0.28343716263771057
CurrentTrain: epoch  6, batch     3 | loss: 2.1447773Losses:  1.8408560752868652 0.38268765807151794
CurrentTrain: epoch  6, batch     4 | loss: 2.2235436Losses:  2.1061277389526367 0.4172624945640564
CurrentTrain: epoch  6, batch     5 | loss: 2.5233903Losses:  1.8373661041259766 0.27988189458847046
CurrentTrain: epoch  6, batch     6 | loss: 2.1172481Losses:  1.9719889163970947 0.3472166061401367
CurrentTrain: epoch  7, batch     0 | loss: 2.3192055Losses:  1.806241750717163 0.37388110160827637
CurrentTrain: epoch  7, batch     1 | loss: 2.1801229Losses:  1.7578210830688477 0.35774755477905273
CurrentTrain: epoch  7, batch     2 | loss: 2.1155686Losses:  2.114335298538208 0.3841007351875305
CurrentTrain: epoch  7, batch     3 | loss: 2.4984360Losses:  1.866927146911621 0.19570206105709076
CurrentTrain: epoch  7, batch     4 | loss: 2.0626292Losses:  1.900400996208191 0.2702259123325348
CurrentTrain: epoch  7, batch     5 | loss: 2.1706269Losses:  1.7447361946105957 0.11965961009263992
CurrentTrain: epoch  7, batch     6 | loss: 1.8643959Losses:  2.0450286865234375 0.18300074338912964
CurrentTrain: epoch  8, batch     0 | loss: 2.2280295Losses:  1.7631990909576416 0.30181434750556946
CurrentTrain: epoch  8, batch     1 | loss: 2.0650134Losses:  1.880493402481079 0.24416565895080566
CurrentTrain: epoch  8, batch     2 | loss: 2.1246591Losses:  1.7043964862823486 0.18018153309822083
CurrentTrain: epoch  8, batch     3 | loss: 1.8845780Losses:  1.759958028793335 0.3781147003173828
CurrentTrain: epoch  8, batch     4 | loss: 2.1380727Losses:  1.7238616943359375 0.23106899857521057
CurrentTrain: epoch  8, batch     5 | loss: 1.9549307Losses:  1.890917420387268 0.014124099165201187
CurrentTrain: epoch  8, batch     6 | loss: 1.9050416Losses:  1.7336605787277222 0.3160555362701416
CurrentTrain: epoch  9, batch     0 | loss: 2.0497160Losses:  1.80318021774292 0.357597291469574
CurrentTrain: epoch  9, batch     1 | loss: 2.1607776Losses:  1.7689058780670166 0.3780220150947571
CurrentTrain: epoch  9, batch     2 | loss: 2.1469278Losses:  1.7076425552368164 0.26139575242996216
CurrentTrain: epoch  9, batch     3 | loss: 1.9690382Losses:  1.7585382461547852 0.1930883228778839
CurrentTrain: epoch  9, batch     4 | loss: 1.9516265Losses:  1.839996576309204 0.2502029538154602
CurrentTrain: epoch  9, batch     5 | loss: 2.0901995Losses:  1.6819217205047607 0.22166338562965393
CurrentTrain: epoch  9, batch     6 | loss: 1.9035851
Losses:  5.679778099060059 0.3394060730934143
MemoryTrain:  epoch  0, batch     0 | loss: 6.0191841Losses:  7.880887031555176 0.4808107614517212
MemoryTrain:  epoch  0, batch     1 | loss: 8.3616982Losses:  9.543071746826172 0.5032463073730469
MemoryTrain:  epoch  0, batch     2 | loss: 10.0463181Losses:  9.604486465454102 0.32247665524482727
MemoryTrain:  epoch  0, batch     3 | loss: 9.9269629Losses:  10.79926586151123 0.7170431017875671
MemoryTrain:  epoch  0, batch     4 | loss: 11.5163088Losses:  10.149381637573242 0.374083548784256
MemoryTrain:  epoch  0, batch     5 | loss: 10.5234652Losses:  11.821109771728516 0.3695160746574402
MemoryTrain:  epoch  0, batch     6 | loss: 12.1906261Losses:  11.27515697479248 0.26900362968444824
MemoryTrain:  epoch  0, batch     7 | loss: 11.5441608Losses:  1.3173104524612427 0.36333948373794556
MemoryTrain:  epoch  1, batch     0 | loss: 1.6806500Losses:  1.8819464445114136 0.41973018646240234
MemoryTrain:  epoch  1, batch     1 | loss: 2.3016768Losses:  0.9385461807250977 0.2889348268508911
MemoryTrain:  epoch  1, batch     2 | loss: 1.2274810Losses:  0.9388471841812134 0.5949598550796509
MemoryTrain:  epoch  1, batch     3 | loss: 1.5338070Losses:  1.359926462173462 0.4010559618473053
MemoryTrain:  epoch  1, batch     4 | loss: 1.7609824Losses:  0.8136506676673889 0.4497707486152649
MemoryTrain:  epoch  1, batch     5 | loss: 1.2634214Losses:  0.8562468886375427 0.28533482551574707
MemoryTrain:  epoch  1, batch     6 | loss: 1.1415818Losses:  0.6534221172332764 0.24455563724040985
MemoryTrain:  epoch  1, batch     7 | loss: 0.8979778Losses:  1.1462101936340332 0.47476792335510254
MemoryTrain:  epoch  2, batch     0 | loss: 1.6209781Losses:  0.802726149559021 0.4795389175415039
MemoryTrain:  epoch  2, batch     1 | loss: 1.2822651Losses:  0.9442369937896729 0.4822952449321747
MemoryTrain:  epoch  2, batch     2 | loss: 1.4265323Losses:  1.012987494468689 0.5402169823646545
MemoryTrain:  epoch  2, batch     3 | loss: 1.5532045Losses:  0.659192681312561 0.4517761170864105
MemoryTrain:  epoch  2, batch     4 | loss: 1.1109688Losses:  0.36006873846054077 0.32161253690719604
MemoryTrain:  epoch  2, batch     5 | loss: 0.6816813Losses:  0.8479986190795898 0.27060985565185547
MemoryTrain:  epoch  2, batch     6 | loss: 1.1186085Losses:  0.40271762013435364 0.15058061480522156
MemoryTrain:  epoch  2, batch     7 | loss: 0.5532982Losses:  0.9698755741119385 0.5472633838653564
MemoryTrain:  epoch  3, batch     0 | loss: 1.5171390Losses:  0.6738918423652649 0.318228155374527
MemoryTrain:  epoch  3, batch     1 | loss: 0.9921200Losses:  0.4880436956882477 0.49988052248954773
MemoryTrain:  epoch  3, batch     2 | loss: 0.9879242Losses:  0.4908786416053772 0.4390185475349426
MemoryTrain:  epoch  3, batch     3 | loss: 0.9298972Losses:  0.4350065588951111 0.42133715748786926
MemoryTrain:  epoch  3, batch     4 | loss: 0.8563437Losses:  0.5163443684577942 0.46306875348091125
MemoryTrain:  epoch  3, batch     5 | loss: 0.9794132Losses:  0.3364766240119934 0.1977199763059616
MemoryTrain:  epoch  3, batch     6 | loss: 0.5341966Losses:  0.6804822683334351 0.196867436170578
MemoryTrain:  epoch  3, batch     7 | loss: 0.8773497Losses:  0.48926204442977905 0.44677165150642395
MemoryTrain:  epoch  4, batch     0 | loss: 0.9360337Losses:  0.5921358466148376 0.5320910811424255
MemoryTrain:  epoch  4, batch     1 | loss: 1.1242269Losses:  0.5314821004867554 0.5618934035301208
MemoryTrain:  epoch  4, batch     2 | loss: 1.0933754Losses:  0.47144508361816406 0.3789396584033966
MemoryTrain:  epoch  4, batch     3 | loss: 0.8503847Losses:  0.7540327310562134 0.4314422607421875
MemoryTrain:  epoch  4, batch     4 | loss: 1.1854750Losses:  0.2939029335975647 0.3205645978450775
MemoryTrain:  epoch  4, batch     5 | loss: 0.6144675Losses:  0.3811780512332916 0.35954615473747253
MemoryTrain:  epoch  4, batch     6 | loss: 0.7407242Losses:  0.19121889770030975 0.07431859523057938
MemoryTrain:  epoch  4, batch     7 | loss: 0.2655375Losses:  0.46287801861763 0.4819411635398865
MemoryTrain:  epoch  5, batch     0 | loss: 0.9448192Losses:  0.49444520473480225 0.46342796087265015
MemoryTrain:  epoch  5, batch     1 | loss: 0.9578732Losses:  0.35302072763442993 0.4474203586578369
MemoryTrain:  epoch  5, batch     2 | loss: 0.8004411Losses:  0.31331345438957214 0.33939021825790405
MemoryTrain:  epoch  5, batch     3 | loss: 0.6527036Losses:  0.3951056897640228 0.5207393169403076
MemoryTrain:  epoch  5, batch     4 | loss: 0.9158450Losses:  0.47256022691726685 0.33004093170166016
MemoryTrain:  epoch  5, batch     5 | loss: 0.8026012Losses:  0.4144064784049988 0.3034626245498657
MemoryTrain:  epoch  5, batch     6 | loss: 0.7178691Losses:  0.41299009323120117 0.16361524164676666
MemoryTrain:  epoch  5, batch     7 | loss: 0.5766053Losses:  0.3826167583465576 0.45481008291244507
MemoryTrain:  epoch  6, batch     0 | loss: 0.8374268Losses:  0.44518059492111206 0.46308499574661255
MemoryTrain:  epoch  6, batch     1 | loss: 0.9082656Losses:  0.389723539352417 0.28488868474960327
MemoryTrain:  epoch  6, batch     2 | loss: 0.6746122Losses:  0.3963041305541992 0.4638087749481201
MemoryTrain:  epoch  6, batch     3 | loss: 0.8601129Losses:  0.3489716351032257 0.36206746101379395
MemoryTrain:  epoch  6, batch     4 | loss: 0.7110391Losses:  0.27555280923843384 0.2547280192375183
MemoryTrain:  epoch  6, batch     5 | loss: 0.5302808Losses:  0.5051932334899902 0.48586440086364746
MemoryTrain:  epoch  6, batch     6 | loss: 0.9910576Losses:  0.3620375692844391 0.12793835997581482
MemoryTrain:  epoch  6, batch     7 | loss: 0.4899759Losses:  0.3662896752357483 0.4465174674987793
MemoryTrain:  epoch  7, batch     0 | loss: 0.8128071Losses:  0.29284632205963135 0.3024156391620636
MemoryTrain:  epoch  7, batch     1 | loss: 0.5952619Losses:  0.3565771281719208 0.4432643949985504
MemoryTrain:  epoch  7, batch     2 | loss: 0.7998415Losses:  0.5116424560546875 0.48260730504989624
MemoryTrain:  epoch  7, batch     3 | loss: 0.9942498Losses:  0.4344726800918579 0.3622642457485199
MemoryTrain:  epoch  7, batch     4 | loss: 0.7967370Losses:  0.3297969102859497 0.32974550127983093
MemoryTrain:  epoch  7, batch     5 | loss: 0.6595424Losses:  0.38092106580734253 0.41543048620224
MemoryTrain:  epoch  7, batch     6 | loss: 0.7963516Losses:  0.28543397784233093 0.15340843796730042
MemoryTrain:  epoch  7, batch     7 | loss: 0.4388424Losses:  0.35557544231414795 0.25994229316711426
MemoryTrain:  epoch  8, batch     0 | loss: 0.6155177Losses:  0.4504951536655426 0.571687638759613
MemoryTrain:  epoch  8, batch     1 | loss: 1.0221828Losses:  0.2946404814720154 0.19415104389190674
MemoryTrain:  epoch  8, batch     2 | loss: 0.4887915Losses:  0.3205511271953583 0.3423210382461548
MemoryTrain:  epoch  8, batch     3 | loss: 0.6628722Losses:  0.3407745063304901 0.29719144105911255
MemoryTrain:  epoch  8, batch     4 | loss: 0.6379659Losses:  0.3885630667209625 0.4318586587905884
MemoryTrain:  epoch  8, batch     5 | loss: 0.8204217Losses:  0.5653747320175171 0.6051033735275269
MemoryTrain:  epoch  8, batch     6 | loss: 1.1704781Losses:  0.31354138255119324 0.18817539513111115
MemoryTrain:  epoch  8, batch     7 | loss: 0.5017168Losses:  0.27359211444854736 0.33274906873703003
MemoryTrain:  epoch  9, batch     0 | loss: 0.6063412Losses:  0.45079824328422546 0.47510096430778503
MemoryTrain:  epoch  9, batch     1 | loss: 0.9258992Losses:  0.3409225046634674 0.310576856136322
MemoryTrain:  epoch  9, batch     2 | loss: 0.6514994Losses:  0.37477564811706543 0.45277154445648193
MemoryTrain:  epoch  9, batch     3 | loss: 0.8275472Losses:  0.4120088517665863 0.37659746408462524
MemoryTrain:  epoch  9, batch     4 | loss: 0.7886063Losses:  0.36321794986724854 0.3520393371582031
MemoryTrain:  epoch  9, batch     5 | loss: 0.7152573Losses:  0.41502124071121216 0.39889833331108093
MemoryTrain:  epoch  9, batch     6 | loss: 0.8139195Losses:  0.3416334390640259 0.24345415830612183
MemoryTrain:  epoch  9, batch     7 | loss: 0.5850876
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 62.37%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.76%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.54%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.04%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.03%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.68%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.77%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 86.96%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 86.23%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 85.62%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 84.94%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 84.58%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 83.01%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 82.79%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 82.58%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 82.18%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 81.71%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 81.34%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 80.90%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 80.38%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 79.97%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 79.65%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 79.85%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 79.63%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 79.45%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 79.27%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 78.77%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 77.72%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 77.11%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 76.80%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.78%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.89%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 79.13%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 79.09%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 79.23%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.86%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.24%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 77.64%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.05%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.63%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 76.17%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.58%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 75.99%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 75.46%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 74.44%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 73.89%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 73.40%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 73.31%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 73.13%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 72.90%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.47%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 73.34%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.04%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 72.65%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 72.27%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 71.98%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.43%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.00%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.53%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 71.27%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 70.81%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 70.39%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 71.22%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 71.24%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 71.26%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.99%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 70.80%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 70.48%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 70.98%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 70.73%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 70.22%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 69.95%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  200 | acc: 18.75%,  total acc: 69.96%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 69.40%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 69.18%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 68.96%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 68.78%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 69.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  225 | acc: 18.75%,  total acc: 70.60%   [EVAL] batch:  226 | acc: 6.25%,  total acc: 70.32%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 70.03%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 69.81%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 69.67%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 70.27%   [EVAL] batch:  238 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  239 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:  241 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 69.50%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 69.34%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 68.90%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 68.63%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 68.38%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 68.11%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 67.85%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.04%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 67.73%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 67.57%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 66.16%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 65.92%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 65.47%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 65.34%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  293 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 66.12%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 65.79%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 65.79%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  335 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 66.52%   [EVAL] batch:  344 | acc: 43.75%,  total acc: 66.45%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 66.23%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 66.17%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 65.78%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 65.67%   [EVAL] batch:  359 | acc: 25.00%,  total acc: 65.56%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 66.38%   
cur_acc:  ['0.9474', '0.7490', '0.7014', '0.6915', '0.5615', '0.6776']
his_acc:  ['0.9474', '0.8470', '0.7726', '0.7295', '0.6929', '0.6638']
Clustering into  34  clusters
Clusters:  [ 0  1 17  0  0  0 29  0 31 32 26  0 20  0 33 23 19  0  0  0 21 24  2  0
  0 18  0 25  0 30  0 12  0 16 14  0  0  0 11 22 27  0  0  2  2 15  0  0
  0  0  0  0  0 10  0  9  0  5 28 13  1  8  0  0  0  4  0  7  6  3]
Losses:  6.402383804321289 1.1343340873718262
CurrentTrain: epoch  0, batch     0 | loss: 7.5367179Losses:  8.007831573486328 1.1792734861373901
CurrentTrain: epoch  0, batch     1 | loss: 9.1871052Losses:  7.786055564880371 0.9172245860099792
CurrentTrain: epoch  0, batch     2 | loss: 8.7032804Losses:  6.386173725128174 1.0329933166503906
CurrentTrain: epoch  0, batch     3 | loss: 7.4191670Losses:  6.509954929351807 0.7747477293014526
CurrentTrain: epoch  0, batch     4 | loss: 7.2847028Losses:  7.196593761444092 1.1305090188980103
CurrentTrain: epoch  0, batch     5 | loss: 8.3271027Losses:  4.268074035644531 0.1265154480934143
CurrentTrain: epoch  0, batch     6 | loss: 4.3945894Losses:  2.8814749717712402 0.5169060230255127
CurrentTrain: epoch  1, batch     0 | loss: 3.3983810Losses:  3.0032529830932617 0.7879521250724792
CurrentTrain: epoch  1, batch     1 | loss: 3.7912052Losses:  3.4124953746795654 1.0287292003631592
CurrentTrain: epoch  1, batch     2 | loss: 4.4412246Losses:  3.3300137519836426 0.975698709487915
CurrentTrain: epoch  1, batch     3 | loss: 4.3057127Losses:  3.1089651584625244 0.7336356043815613
CurrentTrain: epoch  1, batch     4 | loss: 3.8426008Losses:  3.6793839931488037 0.8635541200637817
CurrentTrain: epoch  1, batch     5 | loss: 4.5429382Losses:  4.634024143218994 0.22975003719329834
CurrentTrain: epoch  1, batch     6 | loss: 4.8637743Losses:  3.1517040729522705 0.6931670904159546
CurrentTrain: epoch  2, batch     0 | loss: 3.8448710Losses:  2.7185540199279785 0.80107581615448
CurrentTrain: epoch  2, batch     1 | loss: 3.5196300Losses:  3.256530284881592 0.577240526676178
CurrentTrain: epoch  2, batch     2 | loss: 3.8337708Losses:  2.3732991218566895 0.42558830976486206
CurrentTrain: epoch  2, batch     3 | loss: 2.7988875Losses:  3.356245517730713 0.8120163083076477
CurrentTrain: epoch  2, batch     4 | loss: 4.1682620Losses:  2.9552907943725586 0.8104578852653503
CurrentTrain: epoch  2, batch     5 | loss: 3.7657487Losses:  2.0612802505493164 0.1565198302268982
CurrentTrain: epoch  2, batch     6 | loss: 2.2178001Losses:  2.8586926460266113 0.5209988951683044
CurrentTrain: epoch  3, batch     0 | loss: 3.3796916Losses:  2.5717952251434326 0.5553106069564819
CurrentTrain: epoch  3, batch     1 | loss: 3.1271057Losses:  3.0566346645355225 0.6622599959373474
CurrentTrain: epoch  3, batch     2 | loss: 3.7188947Losses:  2.5251195430755615 0.6546851992607117
CurrentTrain: epoch  3, batch     3 | loss: 3.1798048Losses:  2.0093421936035156 0.35635989904403687
CurrentTrain: epoch  3, batch     4 | loss: 2.3657022Losses:  2.2752461433410645 0.6792850494384766
CurrentTrain: epoch  3, batch     5 | loss: 2.9545312Losses:  2.1660900115966797 0.0863315612077713
CurrentTrain: epoch  3, batch     6 | loss: 2.2524216Losses:  2.4292092323303223 0.807123064994812
CurrentTrain: epoch  4, batch     0 | loss: 3.2363324Losses:  1.8553276062011719 0.2879684567451477
CurrentTrain: epoch  4, batch     1 | loss: 2.1432960Losses:  2.6235785484313965 0.45292216539382935
CurrentTrain: epoch  4, batch     2 | loss: 3.0765007Losses:  2.5450143814086914 0.5991489887237549
CurrentTrain: epoch  4, batch     3 | loss: 3.1441634Losses:  2.3431758880615234 0.5754587054252625
CurrentTrain: epoch  4, batch     4 | loss: 2.9186347Losses:  1.942124843597412 0.36036399006843567
CurrentTrain: epoch  4, batch     5 | loss: 2.3024888Losses:  2.07254695892334 0.1568862795829773
CurrentTrain: epoch  4, batch     6 | loss: 2.2294333Losses:  1.9084522724151611 0.3422623872756958
CurrentTrain: epoch  5, batch     0 | loss: 2.2507148Losses:  2.2427735328674316 0.46414682269096375
CurrentTrain: epoch  5, batch     1 | loss: 2.7069204Losses:  2.443664073944092 0.4202399253845215
CurrentTrain: epoch  5, batch     2 | loss: 2.8639040Losses:  2.4118475914001465 0.5377631187438965
CurrentTrain: epoch  5, batch     3 | loss: 2.9496107Losses:  1.9718172550201416 0.31212228536605835
CurrentTrain: epoch  5, batch     4 | loss: 2.2839396Losses:  1.8803142309188843 0.35035067796707153
CurrentTrain: epoch  5, batch     5 | loss: 2.2306650Losses:  1.824202060699463 0.058518268167972565
CurrentTrain: epoch  5, batch     6 | loss: 1.8827204Losses:  2.1606760025024414 0.5131829977035522
CurrentTrain: epoch  6, batch     0 | loss: 2.6738591Losses:  2.162668228149414 0.4425007998943329
CurrentTrain: epoch  6, batch     1 | loss: 2.6051691Losses:  1.8131840229034424 0.3105200529098511
CurrentTrain: epoch  6, batch     2 | loss: 2.1237040Losses:  2.2886548042297363 0.4814678132534027
CurrentTrain: epoch  6, batch     3 | loss: 2.7701225Losses:  1.7693326473236084 0.2615606188774109
CurrentTrain: epoch  6, batch     4 | loss: 2.0308933Losses:  1.9842301607131958 0.3615304231643677
CurrentTrain: epoch  6, batch     5 | loss: 2.3457606Losses:  1.8700971603393555 0.030260566622018814
CurrentTrain: epoch  6, batch     6 | loss: 1.9003577Losses:  1.942570447921753 0.3541254997253418
CurrentTrain: epoch  7, batch     0 | loss: 2.2966959Losses:  1.9584343433380127 0.4780772924423218
CurrentTrain: epoch  7, batch     1 | loss: 2.4365115Losses:  1.7555816173553467 0.3266872465610504
CurrentTrain: epoch  7, batch     2 | loss: 2.0822690Losses:  1.9025577306747437 0.45480263233184814
CurrentTrain: epoch  7, batch     3 | loss: 2.3573604Losses:  2.1941332817077637 0.3953869342803955
CurrentTrain: epoch  7, batch     4 | loss: 2.5895202Losses:  1.6937305927276611 0.28379154205322266
CurrentTrain: epoch  7, batch     5 | loss: 1.9775221Losses:  1.7837727069854736 0.027009595185518265
CurrentTrain: epoch  7, batch     6 | loss: 1.8107823Losses:  1.7112566232681274 0.2370760142803192
CurrentTrain: epoch  8, batch     0 | loss: 1.9483327Losses:  1.7169361114501953 0.3076557517051697
CurrentTrain: epoch  8, batch     1 | loss: 2.0245919Losses:  1.8558615446090698 0.4765351414680481
CurrentTrain: epoch  8, batch     2 | loss: 2.3323967Losses:  1.7497175931930542 0.22965063154697418
CurrentTrain: epoch  8, batch     3 | loss: 1.9793682Losses:  2.0643858909606934 0.2883729338645935
CurrentTrain: epoch  8, batch     4 | loss: 2.3527589Losses:  1.9609127044677734 0.3666120767593384
CurrentTrain: epoch  8, batch     5 | loss: 2.3275247Losses:  1.6711187362670898 0.10330969095230103
CurrentTrain: epoch  8, batch     6 | loss: 1.7744284Losses:  1.728820562362671 0.2750755548477173
CurrentTrain: epoch  9, batch     0 | loss: 2.0038962Losses:  1.7824785709381104 0.27644872665405273
CurrentTrain: epoch  9, batch     1 | loss: 2.0589273Losses:  1.7325875759124756 0.19848676025867462
CurrentTrain: epoch  9, batch     2 | loss: 1.9310744Losses:  1.7100274562835693 0.24787497520446777
CurrentTrain: epoch  9, batch     3 | loss: 1.9579024Losses:  1.8903584480285645 0.49075737595558167
CurrentTrain: epoch  9, batch     4 | loss: 2.3811159Losses:  1.837470531463623 0.35080182552337646
CurrentTrain: epoch  9, batch     5 | loss: 2.1882725Losses:  1.7220648527145386 8.94069742685133e-08
CurrentTrain: epoch  9, batch     6 | loss: 1.7220650
Losses:  6.027364730834961 0.31669914722442627
MemoryTrain:  epoch  0, batch     0 | loss: 6.3440638Losses:  8.016446113586426 0.35664471983909607
MemoryTrain:  epoch  0, batch     1 | loss: 8.3730907Losses:  9.160429000854492 0.3799365758895874
MemoryTrain:  epoch  0, batch     2 | loss: 9.5403652Losses:  9.721406936645508 0.36990857124328613
MemoryTrain:  epoch  0, batch     3 | loss: 10.0913153Losses:  11.249460220336914 0.2843288481235504
MemoryTrain:  epoch  0, batch     4 | loss: 11.5337887Losses:  9.886192321777344 0.5243692398071289
MemoryTrain:  epoch  0, batch     5 | loss: 10.4105616Losses:  11.76382064819336 0.5021069049835205
MemoryTrain:  epoch  0, batch     6 | loss: 12.2659273Losses:  11.13049602508545 0.40416908264160156
MemoryTrain:  epoch  0, batch     7 | loss: 11.5346651Losses:  11.160070419311523 0.360036700963974
MemoryTrain:  epoch  0, batch     8 | loss: 11.5201073Losses:  1.9595608711242676 0.6556285619735718
MemoryTrain:  epoch  1, batch     0 | loss: 2.6151896Losses:  1.24949049949646 0.6122795343399048
MemoryTrain:  epoch  1, batch     1 | loss: 1.8617700Losses:  1.0911222696304321 0.4732521176338196
MemoryTrain:  epoch  1, batch     2 | loss: 1.5643744Losses:  0.95030677318573 0.42567598819732666
MemoryTrain:  epoch  1, batch     3 | loss: 1.3759828Losses:  1.6392865180969238 0.24822202324867249
MemoryTrain:  epoch  1, batch     4 | loss: 1.8875085Losses:  0.7130478620529175 0.3259003162384033
MemoryTrain:  epoch  1, batch     5 | loss: 1.0389482Losses:  1.6726548671722412 0.245186910033226
MemoryTrain:  epoch  1, batch     6 | loss: 1.9178418Losses:  1.1036409139633179 0.2723592221736908
MemoryTrain:  epoch  1, batch     7 | loss: 1.3760002Losses:  0.6886106729507446 0.332225501537323
MemoryTrain:  epoch  1, batch     8 | loss: 1.0208361Losses:  0.6760890483856201 0.4268741309642792
MemoryTrain:  epoch  2, batch     0 | loss: 1.1029632Losses:  2.037168264389038 0.3729463815689087
MemoryTrain:  epoch  2, batch     1 | loss: 2.4101148Losses:  0.701280951499939 0.31779393553733826
MemoryTrain:  epoch  2, batch     2 | loss: 1.0190749Losses:  1.2319705486297607 0.3407600522041321
MemoryTrain:  epoch  2, batch     3 | loss: 1.5727305Losses:  0.46205592155456543 0.4632379412651062
MemoryTrain:  epoch  2, batch     4 | loss: 0.9252939Losses:  0.9410192370414734 0.4267422556877136
MemoryTrain:  epoch  2, batch     5 | loss: 1.3677615Losses:  1.44669508934021 0.5676366090774536
MemoryTrain:  epoch  2, batch     6 | loss: 2.0143318Losses:  0.6013349890708923 0.38007545471191406
MemoryTrain:  epoch  2, batch     7 | loss: 0.9814104Losses:  0.3090352416038513 0.1948891580104828
MemoryTrain:  epoch  2, batch     8 | loss: 0.5039244Losses:  0.9004906415939331 0.39394479990005493
MemoryTrain:  epoch  3, batch     0 | loss: 1.2944355Losses:  0.9772727489471436 0.3927503824234009
MemoryTrain:  epoch  3, batch     1 | loss: 1.3700231Losses:  0.5070321559906006 0.5017219185829163
MemoryTrain:  epoch  3, batch     2 | loss: 1.0087540Losses:  0.6956290602684021 0.2171606421470642
MemoryTrain:  epoch  3, batch     3 | loss: 0.9127897Losses:  0.6224314570426941 0.4980621933937073
MemoryTrain:  epoch  3, batch     4 | loss: 1.1204937Losses:  0.4247514605522156 0.3980695605278015
MemoryTrain:  epoch  3, batch     5 | loss: 0.8228210Losses:  0.3970998525619507 0.422910213470459
MemoryTrain:  epoch  3, batch     6 | loss: 0.8200101Losses:  0.9284926056861877 0.4407603144645691
MemoryTrain:  epoch  3, batch     7 | loss: 1.3692529Losses:  0.9692013263702393 0.23036694526672363
MemoryTrain:  epoch  3, batch     8 | loss: 1.1995683Losses:  0.49656611680984497 0.6150763034820557
MemoryTrain:  epoch  4, batch     0 | loss: 1.1116424Losses:  0.3751141428947449 0.3788774609565735
MemoryTrain:  epoch  4, batch     1 | loss: 0.7539916Losses:  0.645922064781189 0.5366226434707642
MemoryTrain:  epoch  4, batch     2 | loss: 1.1825447Losses:  0.7575109601020813 0.3639524579048157
MemoryTrain:  epoch  4, batch     3 | loss: 1.1214634Losses:  0.6620957255363464 0.2781701683998108
MemoryTrain:  epoch  4, batch     4 | loss: 0.9402659Losses:  0.9441601037979126 0.4349313974380493
MemoryTrain:  epoch  4, batch     5 | loss: 1.3790915Losses:  0.6078128814697266 0.4073503613471985
MemoryTrain:  epoch  4, batch     6 | loss: 1.0151632Losses:  0.33194035291671753 0.2660885453224182
MemoryTrain:  epoch  4, batch     7 | loss: 0.5980289Losses:  0.3052999973297119 0.1571720838546753
MemoryTrain:  epoch  4, batch     8 | loss: 0.4624721Losses:  0.40448087453842163 0.34037458896636963
MemoryTrain:  epoch  5, batch     0 | loss: 0.7448555Losses:  0.31764623522758484 0.2855175733566284
MemoryTrain:  epoch  5, batch     1 | loss: 0.6031638Losses:  0.5879389047622681 0.3973371386528015
MemoryTrain:  epoch  5, batch     2 | loss: 0.9852760Losses:  0.5025163292884827 0.41173142194747925
MemoryTrain:  epoch  5, batch     3 | loss: 0.9142478Losses:  0.504826009273529 0.41618889570236206
MemoryTrain:  epoch  5, batch     4 | loss: 0.9210149Losses:  0.39391911029815674 0.2875117063522339
MemoryTrain:  epoch  5, batch     5 | loss: 0.6814308Losses:  0.4358903765678406 0.33254608511924744
MemoryTrain:  epoch  5, batch     6 | loss: 0.7684364Losses:  0.7868150472640991 0.5510634779930115
MemoryTrain:  epoch  5, batch     7 | loss: 1.3378785Losses:  0.8027458190917969 0.4305214583873749
MemoryTrain:  epoch  5, batch     8 | loss: 1.2332673Losses:  0.43530386686325073 0.32083460688591003
MemoryTrain:  epoch  6, batch     0 | loss: 0.7561384Losses:  0.46312791109085083 0.4771239757537842
MemoryTrain:  epoch  6, batch     1 | loss: 0.9402519Losses:  0.33785730600357056 0.35837653279304504
MemoryTrain:  epoch  6, batch     2 | loss: 0.6962339Losses:  0.40620458126068115 0.4066653847694397
MemoryTrain:  epoch  6, batch     3 | loss: 0.8128700Losses:  0.6945284605026245 0.39177507162094116
MemoryTrain:  epoch  6, batch     4 | loss: 1.0863035Losses:  0.3367369771003723 0.2957615852355957
MemoryTrain:  epoch  6, batch     5 | loss: 0.6324986Losses:  0.4442559480667114 0.36579662561416626
MemoryTrain:  epoch  6, batch     6 | loss: 0.8100526Losses:  0.5824138522148132 0.4314475655555725
MemoryTrain:  epoch  6, batch     7 | loss: 1.0138614Losses:  0.5607971549034119 0.3484231233596802
MemoryTrain:  epoch  6, batch     8 | loss: 0.9092203Losses:  0.48727157711982727 0.5084017515182495
MemoryTrain:  epoch  7, batch     0 | loss: 0.9956733Losses:  0.6038076877593994 0.4250936508178711
MemoryTrain:  epoch  7, batch     1 | loss: 1.0289013Losses:  0.4003959894180298 0.4074764549732208
MemoryTrain:  epoch  7, batch     2 | loss: 0.8078724Losses:  0.4620105028152466 0.45489129424095154
MemoryTrain:  epoch  7, batch     3 | loss: 0.9169018Losses:  0.32624363899230957 0.29670286178588867
MemoryTrain:  epoch  7, batch     4 | loss: 0.6229465Losses:  0.3284016251564026 0.2169272005558014
MemoryTrain:  epoch  7, batch     5 | loss: 0.5453289Losses:  0.4135436713695526 0.25658825039863586
MemoryTrain:  epoch  7, batch     6 | loss: 0.6701319Losses:  0.4692952632904053 0.48337870836257935
MemoryTrain:  epoch  7, batch     7 | loss: 0.9526740Losses:  0.4616049528121948 0.3490747809410095
MemoryTrain:  epoch  7, batch     8 | loss: 0.8106797Losses:  0.49083465337753296 0.3707227408885956
MemoryTrain:  epoch  8, batch     0 | loss: 0.8615574Losses:  0.34597182273864746 0.283707857131958
MemoryTrain:  epoch  8, batch     1 | loss: 0.6296797Losses:  0.39957934617996216 0.3687971830368042
MemoryTrain:  epoch  8, batch     2 | loss: 0.7683765Losses:  0.33899375796318054 0.26683467626571655
MemoryTrain:  epoch  8, batch     3 | loss: 0.6058284Losses:  0.37515807151794434 0.2859967350959778
MemoryTrain:  epoch  8, batch     4 | loss: 0.6611548Losses:  0.430307537317276 0.39775097370147705
MemoryTrain:  epoch  8, batch     5 | loss: 0.8280585Losses:  0.6038850545883179 0.5985672473907471
MemoryTrain:  epoch  8, batch     6 | loss: 1.2024523Losses:  0.33896124362945557 0.2154145985841751
MemoryTrain:  epoch  8, batch     7 | loss: 0.5543758Losses:  0.492023229598999 0.42299801111221313
MemoryTrain:  epoch  8, batch     8 | loss: 0.9150212Losses:  0.28331220149993896 0.3407963216304779
MemoryTrain:  epoch  9, batch     0 | loss: 0.6241086Losses:  0.4541718065738678 0.5539028644561768
MemoryTrain:  epoch  9, batch     1 | loss: 1.0080746Losses:  0.3559929430484772 0.3481431007385254
MemoryTrain:  epoch  9, batch     2 | loss: 0.7041360Losses:  0.4667831361293793 0.476295530796051
MemoryTrain:  epoch  9, batch     3 | loss: 0.9430786Losses:  0.46033498644828796 0.38295096158981323
MemoryTrain:  epoch  9, batch     4 | loss: 0.8432859Losses:  0.32843899726867676 0.2738422453403473
MemoryTrain:  epoch  9, batch     5 | loss: 0.6022812Losses:  0.49757811427116394 0.4234291613101959
MemoryTrain:  epoch  9, batch     6 | loss: 0.9210073Losses:  0.38079971075057983 0.36375975608825684
MemoryTrain:  epoch  9, batch     7 | loss: 0.7445595Losses:  0.3503867983818054 0.30022236704826355
MemoryTrain:  epoch  9, batch     8 | loss: 0.6506091
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 65.83%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 63.42%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.63%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 62.24%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.67%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 84.26%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 81.67%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 81.15%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 79.37%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 78.22%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 76.70%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 76.03%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 75.46%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 74.91%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 74.48%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 74.14%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 74.18%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 74.36%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 74.47%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 74.02%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 73.51%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 72.27%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 75.55%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.18%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.59%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 73.52%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 73.20%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 72.82%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.86%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 73.18%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 73.16%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 72.66%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 72.11%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 71.67%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 71.14%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 70.61%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 69.73%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 70.20%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 70.28%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 70.19%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 69.91%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 69.55%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 69.19%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 69.01%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.79%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 69.24%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 68.83%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.10%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.67%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 68.71%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 68.29%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 68.19%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 67.82%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.52%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 68.52%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 67.81%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 68.13%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 67.79%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 67.58%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 67.34%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 67.10%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 66.93%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 68.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 68.67%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 68.59%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 68.45%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 68.23%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 68.62%   [EVAL] batch:  238 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  239 | acc: 25.00%,  total acc: 68.28%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 68.13%   [EVAL] batch:  241 | acc: 18.75%,  total acc: 67.92%   [EVAL] batch:  242 | acc: 25.00%,  total acc: 67.75%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 67.55%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 67.46%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 67.21%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 66.71%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.45%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 66.32%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 66.00%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 65.26%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 65.05%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 64.83%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 64.37%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 64.14%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 63.93%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 63.72%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 63.74%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 63.85%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 64.05%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  294 | acc: 18.75%,  total acc: 64.56%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 64.34%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 64.18%   [EVAL] batch:  297 | acc: 25.00%,  total acc: 64.05%   [EVAL] batch:  298 | acc: 12.50%,  total acc: 63.88%   [EVAL] batch:  299 | acc: 12.50%,  total acc: 63.71%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 63.68%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 63.76%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 63.79%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 63.83%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 63.78%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:  308 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 63.66%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 63.61%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 63.59%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 63.58%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 63.58%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 63.58%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.93%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 64.05%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 64.07%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 64.10%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 64.30%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 64.30%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 64.35%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 64.36%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 64.34%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 64.40%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 64.37%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 64.32%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  349 | acc: 43.75%,  total acc: 64.23%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 64.12%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 64.07%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 63.84%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 63.74%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 63.59%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 63.49%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 63.45%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 64.46%   [EVAL] batch:  376 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 64.61%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 64.57%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 64.60%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 64.55%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 64.52%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 64.47%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.59%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  400 | acc: 31.25%,  total acc: 64.93%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 64.83%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 64.65%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 64.47%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:  408 | acc: 31.25%,  total acc: 64.36%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 64.28%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 64.19%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  415 | acc: 56.25%,  total acc: 64.14%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 64.23%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 64.20%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 65.10%   
cur_acc:  ['0.9474', '0.7490', '0.7014', '0.6915', '0.5615', '0.6776', '0.6895']
his_acc:  ['0.9474', '0.8470', '0.7726', '0.7295', '0.6929', '0.6638', '0.6510']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0 32 27  1  0
  0 37  0 29  0 34  2 31  0  0 36  0  0  0 20 23 17  0  0  1  1  9  0  0
  0  0  0  0  0 12  0 16  0 11 15 30  5 14  0  0  0  2  0 18 28 10  8  4
  0  0 13  7  6  0  0  3]
Losses:  6.351475715637207 1.5664130449295044
CurrentTrain: epoch  0, batch     0 | loss: 7.9178886Losses:  8.233874320983887 1.0966341495513916
CurrentTrain: epoch  0, batch     1 | loss: 9.3305082Losses:  8.285937309265137 1.0578933954238892
CurrentTrain: epoch  0, batch     2 | loss: 9.3438311Losses:  6.6186113357543945 1.2881474494934082
CurrentTrain: epoch  0, batch     3 | loss: 7.9067588Losses:  6.604568958282471 1.343876600265503
CurrentTrain: epoch  0, batch     4 | loss: 7.9484453Losses:  5.6703596115112305 1.2755498886108398
CurrentTrain: epoch  0, batch     5 | loss: 6.9459095Losses:  5.2809014320373535 0.7387217283248901
CurrentTrain: epoch  0, batch     6 | loss: 6.0196233Losses:  3.7000784873962402 1.1246962547302246
CurrentTrain: epoch  1, batch     0 | loss: 4.8247747Losses:  2.63748836517334 0.6453243494033813
CurrentTrain: epoch  1, batch     1 | loss: 3.2828126Losses:  2.933129072189331 0.9410427808761597
CurrentTrain: epoch  1, batch     2 | loss: 3.8741717Losses:  2.322598457336426 1.1100369691848755
CurrentTrain: epoch  1, batch     3 | loss: 3.4326353Losses:  2.199078321456909 0.7688713073730469
CurrentTrain: epoch  1, batch     4 | loss: 2.9679496Losses:  2.4374594688415527 0.7096596956253052
CurrentTrain: epoch  1, batch     5 | loss: 3.1471190Losses:  2.8949456214904785 0.13081367313861847
CurrentTrain: epoch  1, batch     6 | loss: 3.0257592Losses:  2.658414363861084 1.0030311346054077
CurrentTrain: epoch  2, batch     0 | loss: 3.6614456Losses:  2.3620355129241943 0.599748969078064
CurrentTrain: epoch  2, batch     1 | loss: 2.9617844Losses:  2.1301450729370117 0.6580593585968018
CurrentTrain: epoch  2, batch     2 | loss: 2.7882044Losses:  2.5603206157684326 0.6924424171447754
CurrentTrain: epoch  2, batch     3 | loss: 3.2527630Losses:  2.5826079845428467 0.6373106837272644
CurrentTrain: epoch  2, batch     4 | loss: 3.2199187Losses:  2.598994493484497 1.3278186321258545
CurrentTrain: epoch  2, batch     5 | loss: 3.9268131Losses:  1.7954517602920532 0.024846717715263367
CurrentTrain: epoch  2, batch     6 | loss: 1.8202984Losses:  2.2513067722320557 0.6438788175582886
CurrentTrain: epoch  3, batch     0 | loss: 2.8951855Losses:  2.4084129333496094 0.5451952219009399
CurrentTrain: epoch  3, batch     1 | loss: 2.9536080Losses:  2.6070661544799805 1.2506842613220215
CurrentTrain: epoch  3, batch     2 | loss: 3.8577504Losses:  2.0371649265289307 0.4933398962020874
CurrentTrain: epoch  3, batch     3 | loss: 2.5305047Losses:  2.0654096603393555 0.45098263025283813
CurrentTrain: epoch  3, batch     4 | loss: 2.5163922Losses:  1.9458543062210083 0.7714956402778625
CurrentTrain: epoch  3, batch     5 | loss: 2.7173500Losses:  1.906569242477417 1.1920928955078125e-07
CurrentTrain: epoch  3, batch     6 | loss: 1.9065694Losses:  1.9533100128173828 0.6147252321243286
CurrentTrain: epoch  4, batch     0 | loss: 2.5680351Losses:  1.8164761066436768 0.40818172693252563
CurrentTrain: epoch  4, batch     1 | loss: 2.2246578Losses:  2.6766605377197266 0.5263500213623047
CurrentTrain: epoch  4, batch     2 | loss: 3.2030106Losses:  1.9892199039459229 0.31934595108032227
CurrentTrain: epoch  4, batch     3 | loss: 2.3085659Losses:  1.9720854759216309 0.4327390789985657
CurrentTrain: epoch  4, batch     4 | loss: 2.4048245Losses:  2.108243465423584 0.6603978276252747
CurrentTrain: epoch  4, batch     5 | loss: 2.7686412Losses:  1.955870270729065 0.10269969701766968
CurrentTrain: epoch  4, batch     6 | loss: 2.0585699Losses:  2.1486079692840576 0.5207053422927856
CurrentTrain: epoch  5, batch     0 | loss: 2.6693134Losses:  1.8642977476119995 0.61618971824646
CurrentTrain: epoch  5, batch     1 | loss: 2.4804873Losses:  2.397348403930664 0.4991811513900757
CurrentTrain: epoch  5, batch     2 | loss: 2.8965297Losses:  1.8013315200805664 0.2801418900489807
CurrentTrain: epoch  5, batch     3 | loss: 2.0814734Losses:  1.998674988746643 0.5370403528213501
CurrentTrain: epoch  5, batch     4 | loss: 2.5357153Losses:  1.75014328956604 0.39415666460990906
CurrentTrain: epoch  5, batch     5 | loss: 2.1443000Losses:  1.7184020280838013 0.017767351120710373
CurrentTrain: epoch  5, batch     6 | loss: 1.7361693Losses:  2.3947134017944336 0.40379786491394043
CurrentTrain: epoch  6, batch     0 | loss: 2.7985113Losses:  1.8225284814834595 0.4253860116004944
CurrentTrain: epoch  6, batch     1 | loss: 2.2479146Losses:  1.9467315673828125 0.33371785283088684
CurrentTrain: epoch  6, batch     2 | loss: 2.2804494Losses:  1.7468903064727783 0.26499655842781067
CurrentTrain: epoch  6, batch     3 | loss: 2.0118868Losses:  1.7648545503616333 0.49675145745277405
CurrentTrain: epoch  6, batch     4 | loss: 2.2616060Losses:  1.9901049137115479 0.20122739672660828
CurrentTrain: epoch  6, batch     5 | loss: 2.1913323Losses:  1.7862825393676758 0.06777988374233246
CurrentTrain: epoch  6, batch     6 | loss: 1.8540624Losses:  1.8929712772369385 0.3951262831687927
CurrentTrain: epoch  7, batch     0 | loss: 2.2880976Losses:  1.7650206089019775 0.36218929290771484
CurrentTrain: epoch  7, batch     1 | loss: 2.1272099Losses:  1.8762681484222412 0.4730812609195709
CurrentTrain: epoch  7, batch     2 | loss: 2.3493495Losses:  1.7719650268554688 0.43620797991752625
CurrentTrain: epoch  7, batch     3 | loss: 2.2081730Losses:  1.7153229713439941 0.3403024971485138
CurrentTrain: epoch  7, batch     4 | loss: 2.0556254Losses:  2.2275853157043457 0.28372809290885925
CurrentTrain: epoch  7, batch     5 | loss: 2.5113134Losses:  2.1313347816467285 0.038563452661037445
CurrentTrain: epoch  7, batch     6 | loss: 2.1698983Losses:  2.151665210723877 0.48657742142677307
CurrentTrain: epoch  8, batch     0 | loss: 2.6382427Losses:  1.7243839502334595 0.19044218957424164
CurrentTrain: epoch  8, batch     1 | loss: 1.9148262Losses:  1.8209025859832764 0.3969065845012665
CurrentTrain: epoch  8, batch     2 | loss: 2.2178092Losses:  1.6971690654754639 0.1527872085571289
CurrentTrain: epoch  8, batch     3 | loss: 1.8499563Losses:  1.9140293598175049 0.32137835025787354
CurrentTrain: epoch  8, batch     4 | loss: 2.2354078Losses:  1.7557423114776611 0.2450452744960785
CurrentTrain: epoch  8, batch     5 | loss: 2.0007875Losses:  2.064563274383545 0.08021610230207443
CurrentTrain: epoch  8, batch     6 | loss: 2.1447794Losses:  1.8061449527740479 0.33145150542259216
CurrentTrain: epoch  9, batch     0 | loss: 2.1375964Losses:  2.026214599609375 0.4534422755241394
CurrentTrain: epoch  9, batch     1 | loss: 2.4796569Losses:  1.6796284914016724 0.17844459414482117
CurrentTrain: epoch  9, batch     2 | loss: 1.8580731Losses:  1.6861259937286377 0.16853612661361694
CurrentTrain: epoch  9, batch     3 | loss: 1.8546622Losses:  1.8188663721084595 0.2422889918088913
CurrentTrain: epoch  9, batch     4 | loss: 2.0611553Losses:  1.8942070007324219 0.2494453489780426
CurrentTrain: epoch  9, batch     5 | loss: 2.1436524Losses:  1.6781036853790283 0.018278125673532486
CurrentTrain: epoch  9, batch     6 | loss: 1.6963818
Losses:  5.94143009185791 0.4033483564853668
MemoryTrain:  epoch  0, batch     0 | loss: 6.3447785Losses:  8.293596267700195 0.37298858165740967
MemoryTrain:  epoch  0, batch     1 | loss: 8.6665850Losses:  8.613286972045898 0.3423483967781067
MemoryTrain:  epoch  0, batch     2 | loss: 8.9556351Losses:  9.689142227172852 0.3355831205844879
MemoryTrain:  epoch  0, batch     3 | loss: 10.0247250Losses:  9.006576538085938 0.2590373754501343
MemoryTrain:  epoch  0, batch     4 | loss: 9.2656136Losses:  10.365053176879883 0.6059015989303589
MemoryTrain:  epoch  0, batch     5 | loss: 10.9709549Losses:  10.326370239257812 0.4171786308288574
MemoryTrain:  epoch  0, batch     6 | loss: 10.7435493Losses:  10.59859848022461 0.37358593940734863
MemoryTrain:  epoch  0, batch     7 | loss: 10.9721842Losses:  10.843648910522461 0.549383819103241
MemoryTrain:  epoch  0, batch     8 | loss: 11.3930330Losses:  10.913915634155273 0.3854357600212097
MemoryTrain:  epoch  0, batch     9 | loss: 11.2993517Losses:  0.5801786184310913 0.36187708377838135
MemoryTrain:  epoch  1, batch     0 | loss: 0.9420557Losses:  0.7235516309738159 0.27442651987075806
MemoryTrain:  epoch  1, batch     1 | loss: 0.9979782Losses:  0.7057883143424988 0.46803751587867737
MemoryTrain:  epoch  1, batch     2 | loss: 1.1738259Losses:  0.6663658618927002 0.40820497274398804
MemoryTrain:  epoch  1, batch     3 | loss: 1.0745709Losses:  0.6982486844062805 0.46626389026641846
MemoryTrain:  epoch  1, batch     4 | loss: 1.1645126Losses:  1.0860342979431152 0.4109838902950287
MemoryTrain:  epoch  1, batch     5 | loss: 1.4970182Losses:  0.9747674465179443 0.6127960681915283
MemoryTrain:  epoch  1, batch     6 | loss: 1.5875635Losses:  0.9746279120445251 0.4856005907058716
MemoryTrain:  epoch  1, batch     7 | loss: 1.4602284Losses:  1.2487938404083252 0.3696519732475281
MemoryTrain:  epoch  1, batch     8 | loss: 1.6184459Losses:  0.33183562755584717 0.36163026094436646
MemoryTrain:  epoch  1, batch     9 | loss: 0.6934659Losses:  1.0228986740112305 0.4045126736164093
MemoryTrain:  epoch  2, batch     0 | loss: 1.4274113Losses:  0.5177632570266724 0.41487860679626465
MemoryTrain:  epoch  2, batch     1 | loss: 0.9326419Losses:  0.5235689282417297 0.4589020907878876
MemoryTrain:  epoch  2, batch     2 | loss: 0.9824710Losses:  0.46553540229797363 0.22474008798599243
MemoryTrain:  epoch  2, batch     3 | loss: 0.6902755Losses:  0.38720235228538513 0.4041179418563843
MemoryTrain:  epoch  2, batch     4 | loss: 0.7913203Losses:  0.4614461660385132 0.2872050702571869
MemoryTrain:  epoch  2, batch     5 | loss: 0.7486513Losses:  0.6131781339645386 0.39162182807922363
MemoryTrain:  epoch  2, batch     6 | loss: 1.0048000Losses:  0.5140673518180847 0.350838303565979
MemoryTrain:  epoch  2, batch     7 | loss: 0.8649057Losses:  0.6523507237434387 0.4043043851852417
MemoryTrain:  epoch  2, batch     8 | loss: 1.0566552Losses:  0.8343918919563293 0.44108691811561584
MemoryTrain:  epoch  2, batch     9 | loss: 1.2754788Losses:  0.9880043268203735 0.551703691482544
MemoryTrain:  epoch  3, batch     0 | loss: 1.5397080Losses:  0.3456997573375702 0.21061556041240692
MemoryTrain:  epoch  3, batch     1 | loss: 0.5563153Losses:  0.5137733817100525 0.4665467143058777
MemoryTrain:  epoch  3, batch     2 | loss: 0.9803201Losses:  0.4226159155368805 0.3587173521518707
MemoryTrain:  epoch  3, batch     3 | loss: 0.7813333Losses:  0.36801236867904663 0.32525283098220825
MemoryTrain:  epoch  3, batch     4 | loss: 0.6932652Losses:  0.6785838007926941 0.5190504789352417
MemoryTrain:  epoch  3, batch     5 | loss: 1.1976342Losses:  0.4459688663482666 0.36452358961105347
MemoryTrain:  epoch  3, batch     6 | loss: 0.8104925Losses:  0.3590993285179138 0.29037773609161377
MemoryTrain:  epoch  3, batch     7 | loss: 0.6494771Losses:  0.45547598600387573 0.4058263897895813
MemoryTrain:  epoch  3, batch     8 | loss: 0.8613024Losses:  0.53609299659729 0.4429050087928772
MemoryTrain:  epoch  3, batch     9 | loss: 0.9789980Losses:  0.39915308356285095 0.34868481755256653
MemoryTrain:  epoch  4, batch     0 | loss: 0.7478379Losses:  0.3307880759239197 0.2721827030181885
MemoryTrain:  epoch  4, batch     1 | loss: 0.6029708Losses:  0.47654303908348083 0.4618832468986511
MemoryTrain:  epoch  4, batch     2 | loss: 0.9384263Losses:  0.41642940044403076 0.24887791275978088
MemoryTrain:  epoch  4, batch     3 | loss: 0.6653073Losses:  0.4068291187286377 0.35709255933761597
MemoryTrain:  epoch  4, batch     4 | loss: 0.7639217Losses:  0.3948138654232025 0.3106570839881897
MemoryTrain:  epoch  4, batch     5 | loss: 0.7054709Losses:  0.4532925486564636 0.4498384892940521
MemoryTrain:  epoch  4, batch     6 | loss: 0.9031310Losses:  0.4723966717720032 0.38114020228385925
MemoryTrain:  epoch  4, batch     7 | loss: 0.8535368Losses:  0.6014769077301025 0.351487934589386
MemoryTrain:  epoch  4, batch     8 | loss: 0.9529648Losses:  0.5351652503013611 0.44872134923934937
MemoryTrain:  epoch  4, batch     9 | loss: 0.9838866Losses:  0.39154303073883057 0.29430463910102844
MemoryTrain:  epoch  5, batch     0 | loss: 0.6858476Losses:  0.35396432876586914 0.3265226483345032
MemoryTrain:  epoch  5, batch     1 | loss: 0.6804870Losses:  0.433513879776001 0.4205191731452942
MemoryTrain:  epoch  5, batch     2 | loss: 0.8540331Losses:  0.3201892077922821 0.20865368843078613
MemoryTrain:  epoch  5, batch     3 | loss: 0.5288429Losses:  0.3613642454147339 0.3153691291809082
MemoryTrain:  epoch  5, batch     4 | loss: 0.6767334Losses:  0.42294466495513916 0.31416648626327515
MemoryTrain:  epoch  5, batch     5 | loss: 0.7371112Losses:  0.6492749452590942 0.5337327718734741
MemoryTrain:  epoch  5, batch     6 | loss: 1.1830077Losses:  0.51524817943573 0.39811909198760986
MemoryTrain:  epoch  5, batch     7 | loss: 0.9133673Losses:  0.5595802664756775 0.42001673579216003
MemoryTrain:  epoch  5, batch     8 | loss: 0.9795970Losses:  0.3263595402240753 0.238319531083107
MemoryTrain:  epoch  5, batch     9 | loss: 0.5646791Losses:  0.5117664933204651 0.2864781320095062
MemoryTrain:  epoch  6, batch     0 | loss: 0.7982446Losses:  0.48750385642051697 0.5140800476074219
MemoryTrain:  epoch  6, batch     1 | loss: 1.0015839Losses:  0.2742135524749756 0.1859484612941742
MemoryTrain:  epoch  6, batch     2 | loss: 0.4601620Losses:  0.3668484091758728 0.355419397354126
MemoryTrain:  epoch  6, batch     3 | loss: 0.7222678Losses:  0.4739302396774292 0.48418521881103516
MemoryTrain:  epoch  6, batch     4 | loss: 0.9581155Losses:  0.4076150357723236 0.3636358976364136
MemoryTrain:  epoch  6, batch     5 | loss: 0.7712510Losses:  0.4070243239402771 0.35131144523620605
MemoryTrain:  epoch  6, batch     6 | loss: 0.7583358Losses:  0.39319807291030884 0.32536429166793823
MemoryTrain:  epoch  6, batch     7 | loss: 0.7185624Losses:  0.392600417137146 0.32552218437194824
MemoryTrain:  epoch  6, batch     8 | loss: 0.7181226Losses:  0.32957395911216736 0.2497369647026062
MemoryTrain:  epoch  6, batch     9 | loss: 0.5793109Losses:  0.3958672881126404 0.38696128129959106
MemoryTrain:  epoch  7, batch     0 | loss: 0.7828286Losses:  0.5267429351806641 0.3859865069389343
MemoryTrain:  epoch  7, batch     1 | loss: 0.9127294Losses:  0.3617238700389862 0.336639940738678
MemoryTrain:  epoch  7, batch     2 | loss: 0.6983638Losses:  0.525733232498169 0.5078229904174805
MemoryTrain:  epoch  7, batch     3 | loss: 1.0335562Losses:  0.39542800188064575 0.26057037711143494
MemoryTrain:  epoch  7, batch     4 | loss: 0.6559983Losses:  0.36792027950286865 0.2968518137931824
MemoryTrain:  epoch  7, batch     5 | loss: 0.6647721Losses:  0.3555019497871399 0.2871750295162201
MemoryTrain:  epoch  7, batch     6 | loss: 0.6426769Losses:  0.4451659321784973 0.3540506958961487
MemoryTrain:  epoch  7, batch     7 | loss: 0.7992166Losses:  0.2800617218017578 0.19436725974082947
MemoryTrain:  epoch  7, batch     8 | loss: 0.4744290Losses:  0.5319870710372925 0.5379046201705933
MemoryTrain:  epoch  7, batch     9 | loss: 1.0698917Losses:  0.37575626373291016 0.24819821119308472
MemoryTrain:  epoch  8, batch     0 | loss: 0.6239545Losses:  0.2997426986694336 0.2650163173675537
MemoryTrain:  epoch  8, batch     1 | loss: 0.5647590Losses:  0.40703439712524414 0.2860344648361206
MemoryTrain:  epoch  8, batch     2 | loss: 0.6930689Losses:  0.2757737636566162 0.22683733701705933
MemoryTrain:  epoch  8, batch     3 | loss: 0.5026111Losses:  0.4366844892501831 0.28542640805244446
MemoryTrain:  epoch  8, batch     4 | loss: 0.7221109Losses:  0.44551920890808105 0.3859587609767914
MemoryTrain:  epoch  8, batch     5 | loss: 0.8314780Losses:  0.6300266981124878 0.5386566519737244
MemoryTrain:  epoch  8, batch     6 | loss: 1.1686833Losses:  0.47209542989730835 0.4356907606124878
MemoryTrain:  epoch  8, batch     7 | loss: 0.9077862Losses:  0.2971547842025757 0.26875102519989014
MemoryTrain:  epoch  8, batch     8 | loss: 0.5659058Losses:  0.46629610657691956 0.3562326729297638
MemoryTrain:  epoch  8, batch     9 | loss: 0.8225288Losses:  0.3856945037841797 0.2610050439834595
MemoryTrain:  epoch  9, batch     0 | loss: 0.6466995Losses:  0.3735576868057251 0.3436743915081024
MemoryTrain:  epoch  9, batch     1 | loss: 0.7172321Losses:  0.37169310450553894 0.39144977927207947
MemoryTrain:  epoch  9, batch     2 | loss: 0.7631429Losses:  0.3766806125640869 0.17749515175819397
MemoryTrain:  epoch  9, batch     3 | loss: 0.5541757Losses:  0.46506404876708984 0.3481045067310333
MemoryTrain:  epoch  9, batch     4 | loss: 0.8131685Losses:  0.3811599910259247 0.2954031825065613
MemoryTrain:  epoch  9, batch     5 | loss: 0.6765631Losses:  0.42523449659347534 0.37295225262641907
MemoryTrain:  epoch  9, batch     6 | loss: 0.7981868Losses:  0.29517313838005066 0.230180025100708
MemoryTrain:  epoch  9, batch     7 | loss: 0.5253532Losses:  0.3397568464279175 0.20431233942508698
MemoryTrain:  epoch  9, batch     8 | loss: 0.5440692Losses:  0.5522881150245667 0.4077042043209076
MemoryTrain:  epoch  9, batch     9 | loss: 0.9599923
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 82.72%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 82.33%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 81.92%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.54%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 80.79%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 78.34%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 78.07%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.22%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 76.49%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.49%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 74.81%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 74.05%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 73.41%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 72.79%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 71.50%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 72.06%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 71.50%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 71.18%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.64%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 70.33%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 73.95%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 73.73%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 73.94%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.66%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.09%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.59%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 72.16%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 71.85%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 71.54%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.60%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 71.51%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 70.97%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 70.49%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 69.97%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 69.46%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 69.00%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 68.46%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 68.00%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 67.61%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 67.40%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.46%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 68.09%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 67.28%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 67.00%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 66.61%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 67.18%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 66.58%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 66.37%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 66.19%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 66.10%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 65.95%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  200 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 66.03%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 65.83%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 65.56%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 65.34%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 67.09%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 67.00%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 66.87%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  238 | acc: 31.25%,  total acc: 67.05%   [EVAL] batch:  239 | acc: 12.50%,  total acc: 66.82%   [EVAL] batch:  240 | acc: 25.00%,  total acc: 66.65%   [EVAL] batch:  241 | acc: 12.50%,  total acc: 66.43%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 66.31%   [EVAL] batch:  243 | acc: 12.50%,  total acc: 66.09%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 66.05%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 66.01%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 65.64%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 65.40%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.14%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 64.89%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 64.63%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 64.38%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 64.45%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 64.47%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 64.33%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 64.14%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 63.92%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 63.80%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 63.62%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 63.41%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.20%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 62.97%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 62.52%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 62.12%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 62.12%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 62.26%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 62.41%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.59%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.69%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:  294 | acc: 6.25%,  total acc: 62.97%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 62.58%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 62.44%   [EVAL] batch:  298 | acc: 6.25%,  total acc: 62.25%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 62.12%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 62.13%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 62.31%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 62.36%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 62.30%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 62.28%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 62.24%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 62.28%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 62.20%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 62.18%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 62.16%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 62.16%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 62.18%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 62.19%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 62.19%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 62.73%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 62.75%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 62.69%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 62.63%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:  331 | acc: 50.00%,  total acc: 62.61%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 62.59%   [EVAL] batch:  333 | acc: 75.00%,  total acc: 62.63%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 62.63%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 62.67%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 62.65%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 62.70%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 62.76%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 62.77%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 62.72%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 62.68%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 62.61%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 62.63%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 62.57%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 62.59%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 62.52%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 62.46%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 62.45%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 62.43%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 62.43%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 62.39%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 62.38%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 62.26%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 62.15%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 62.01%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 61.91%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 61.86%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 62.40%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 62.62%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 62.85%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 62.92%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 62.91%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 62.90%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 62.94%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 62.96%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:  384 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 62.90%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 62.92%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 62.95%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 62.95%   [EVAL] batch:  390 | acc: 37.50%,  total acc: 62.88%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 62.87%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 62.80%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 62.94%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  400 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 63.25%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 63.18%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 63.13%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  405 | acc: 37.50%,  total acc: 62.99%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.01%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 62.87%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 62.79%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 62.68%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 62.59%   [EVAL] batch:  412 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  413 | acc: 18.75%,  total acc: 62.39%   [EVAL] batch:  414 | acc: 12.50%,  total acc: 62.27%   [EVAL] batch:  415 | acc: 25.00%,  total acc: 62.18%   [EVAL] batch:  416 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:  417 | acc: 31.25%,  total acc: 62.01%   [EVAL] batch:  418 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:  419 | acc: 43.75%,  total acc: 61.89%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 61.89%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 61.87%   [EVAL] batch:  424 | acc: 62.50%,  total acc: 61.87%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 62.40%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 62.49%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 62.54%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 62.62%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 62.76%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 62.98%   [EVAL] batch:  439 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:  440 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  443 | acc: 93.75%,  total acc: 63.29%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 63.52%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  450 | acc: 56.25%,  total acc: 63.53%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  453 | acc: 43.75%,  total acc: 63.48%   [EVAL] batch:  454 | acc: 37.50%,  total acc: 63.42%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  457 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:  458 | acc: 81.25%,  total acc: 63.45%   [EVAL] batch:  459 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:  460 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  469 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 64.04%   [EVAL] batch:  472 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 64.08%   [EVAL] batch:  474 | acc: 56.25%,  total acc: 64.07%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 64.94%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.42%   
cur_acc:  ['0.9474', '0.7490', '0.7014', '0.6915', '0.5615', '0.6776', '0.6895', '0.8254']
his_acc:  ['0.9474', '0.8470', '0.7726', '0.7295', '0.6929', '0.6638', '0.6510', '0.6542']
----------END
his_acc mean:  [0.9463 0.8305 0.7687 0.7397 0.7267 0.6978 0.6754 0.657 ]
